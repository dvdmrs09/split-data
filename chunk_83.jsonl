{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the angular separation of two points in the Meeus sky.", "response": "def anglesep_meeus(lon0: float, lat0: float,\n                   lon1: float, lat1: float, deg: bool = True) -> float:\n    \"\"\"\n    Parameters\n    ----------\n\n    lon0 : float or numpy.ndarray of float\n        longitude of first point\n    lat0 : float or numpy.ndarray of float\n        latitude of first point\n    lon1 : float or numpy.ndarray of float\n        longitude of second point\n    lat1 : float or numpy.ndarray of float\n        latitude of second point\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n\n    sep_rad : float or numpy.ndarray of float\n        angular separation\n\n\n    Meeus p. 109\n\n    from \"Astronomical Algorithms\" by Jean Meeus Ch. 16 p. 111 (16.5)\n    gives angular distance in degrees between two rightAscension,Declination\n    points in the sky.  Neglecting atmospheric effects, of course.\n\n    Meeus haversine method is stable all the way to exactly 0 deg.\n\n    either the arrays must be the same size, or one of them must be a scalar\n    \"\"\"\n\n    if deg:\n        lon0 = radians(lon0)\n        lat0 = radians(lat0)\n        lon1 = radians(lon1)\n        lat1 = radians(lat1)\n\n    sep_rad = 2 * arcsin(sqrt(haversine(lat0 - lat1) +\n                              cos(lat0) * cos(lat1) * haversine(lon0 - lon1)))\n\n    if deg:\n        return degrees(sep_rad)\n    else:\n        return sep_rad"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef anglesep(lon0: float, lat0: float,\n             lon1: float, lat1: float, deg: bool = True) -> float:\n    \"\"\"\n    Parameters\n    ----------\n\n    lon0 : float\n        longitude of first point\n    lat0 : float\n        latitude of first point\n    lon1 : float\n        longitude of second point\n    lat1 : float\n        latitude of second point\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n\n    sep_rad : float or numpy.ndarray of float\n        angular separation\n\n    For reference, this is from astropy astropy/coordinates/angle_utilities.py\n    Angular separation between two points on a sphere.\n    \"\"\"\n    if angular_separation is None:\n        raise ImportError('angledist requires AstroPy. Try angledis_meeus')\n\n    if deg:\n        lon0 = radians(lon0)\n        lat0 = radians(lat0)\n        lon1 = radians(lon1)\n        lat1 = radians(lat1)\n\n    sep_rad = angular_separation(lon0, lat0, lon1, lat1)\n\n    if deg:\n        return degrees(sep_rad)\n    else:\n        return sep_rad", "response": "Returns the angular separation between two points on a sphere."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef azel2radec(az_deg: float, el_deg: float,\n               lat_deg: float, lon_deg: float,\n               time: datetime, usevallado: bool = False) -> Tuple[float, float]:\n    \"\"\"\n    viewing angle (az, el) to sky coordinates (ra, dec)\n\n    Parameters\n    ----------\n    az_deg : float or numpy.ndarray of float\n         azimuth [degrees clockwize from North]\n    el_deg : float or numpy.ndarray of float\n             elevation [degrees above horizon (neglecting aberration)]\n    lat_deg : float\n              observer latitude [-90, 90]\n    lon_deg : float\n              observer longitude [-180, 180] (degrees)\n    time : datetime.datetime\n           time of observation\n    usevallado : bool, optional\n                 default use astropy. If true, use Vallado algorithm\n\n    Returns\n    -------\n    ra_deg : float or numpy.ndarray of float\n         ecliptic right ascension (degress)\n    dec_deg : float or numpy.ndarray of float\n         ecliptic declination (degrees)\n    \"\"\"\n\n    if usevallado or Time is None:  # non-AstroPy method, less accurate\n        return vazel2radec(az_deg, el_deg, lat_deg, lon_deg, time)\n\n    obs = EarthLocation(lat=lat_deg * u.deg, lon=lon_deg * u.deg)\n\n    direc = AltAz(location=obs, obstime=Time(str2dt(time)),\n                  az=az_deg * u.deg, alt=el_deg * u.deg)\n\n    sky = SkyCoord(direc.transform_to(ICRS()))\n\n    return sky.ra.deg, sky.dec.deg", "response": "Convert azimuth elevation to sky coordinates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef radec2azel(ra_deg: float, dec_deg: float,\n               lat_deg: float, lon_deg: float,\n               time: datetime, usevallado: bool = False) -> Tuple[float, float]:\n    \"\"\"\n    sky coordinates (ra, dec) to viewing angle (az, el)\n\n    Parameters\n    ----------\n    ra_deg : float or numpy.ndarray of float\n         ecliptic right ascension (degress)\n    dec_deg : float or numpy.ndarray of float\n         ecliptic declination (degrees)\n    lat_deg : float\n              observer latitude [-90, 90]\n    lon_deg : float\n              observer longitude [-180, 180] (degrees)\n    time : datetime.datetime\n           time of observation\n    usevallado : bool, optional\n                 default use astropy. If true, use Vallado algorithm\n\n    Returns\n    -------\n    az_deg : float or numpy.ndarray of float\n             azimuth [degrees clockwize from North]\n    el_deg : float or numpy.ndarray of float\n             elevation [degrees above horizon (neglecting aberration)]\n    \"\"\"\n\n    if usevallado or Time is None:\n        return vradec2azel(ra_deg, dec_deg, lat_deg, lon_deg, time)\n# %% input trapping\n    lat = np.atleast_1d(lat_deg)\n    lon = np.atleast_1d(lon_deg)\n    ra = np.atleast_1d(ra_deg)\n    dec = np.atleast_1d(dec_deg)\n\n    obs = EarthLocation(lat=lat * u.deg,\n                        lon=lon * u.deg)\n\n    points = SkyCoord(Angle(ra, unit=u.deg),\n                      Angle(dec, unit=u.deg),\n                      equinox='J2000.0')\n\n    altaz = points.transform_to(AltAz(location=obs, obstime=Time(str2dt(time))))\n\n    return altaz.az.degree, altaz.alt.degree", "response": "Convert from a ride to a la turbine sky coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lookAtSpheroid(lat0: float, lon0: float, h0: float, az: float, tilt: float,\n                   ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Calculates line-of-sight intersection with Earth (or other ellipsoid) surface from above surface / orbit\n\n    Parameters\n    ----------\n\n    lat0 : float\n           observer geodetic latitude\n    lon0 : float\n           observer geodetic longitude\n    h0 : float\n        observer altitude (meters)  Must be non-negative since this function doesn't consider terrain\n    az : float or numpy.ndarray of float\n        azimuth angle of line-of-sight, clockwise from North\n    tilt : float or numpy.ndarray of float\n        tilt angle of line-of-sight with respect to local vertical (nadir = 0)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    lat0 : float or numpy.ndarray of float\n           geodetic latitude where the line-of-sight intersects with the Earth ellipsoid\n    lon0 : float or numpy.ndarray of float\n           geodetic longitude where the line-of-sight intersects with the Earth ellipsoid\n    d : float or numpy.ndarray of float\n        slant range (meters) from starting point to intersect point\n\n    Values will be NaN if the line of sight does not intersect.\n\n    Algorithm based on https://medium.com/@stephenhartzell/satellite-line-of-sight-intersection-with-earth-d786b4a6a9b6 Stephen Hartzell\n    \"\"\"\n\n    if (np.asarray(h0) < 0).any():\n        raise ValueError('Intersection calculation requires altitude  [0, Infinity)')\n\n    if ell is None:\n        ell = Ellipsoid()\n\n    tilt = np.asarray(tilt)\n\n    a = ell.a\n    b = ell.a\n    c = ell.b\n\n    el = tilt - 90. if deg else tilt - pi / 2\n\n    e, n, u = aer2enu(az, el, srange=1., deg=deg)  # fixed 1 km slant range\n    u, v, w = enu2uvw(e, n, u, lat0, lon0, deg=deg)\n    x, y, z = geodetic2ecef(lat0, lon0, h0, deg=deg)\n\n    value = -a**2 * b**2 * w * z - a**2 * c**2 * v * y - b**2 * c**2 * u * x\n    radical = (a**2 * b**2 * w**2 + a**2 * c**2 * v**2 - a**2 * v**2 * z**2 + 2 * a**2 * v * w * y * z -\n               a**2 * w**2 * y**2 + b**2 * c**2 * u**2 - b**2 * u**2 * z**2 + 2 * b**2 * u * w * x * z -\n               b**2 * w**2 * x**2 - c**2 * u**2 * y**2 + 2 * c**2 * u * v * x * y - c**2 * v**2 * x**2)\n\n    magnitude = a**2 * b**2 * w**2 + a**2 * c**2 * v**2 + b**2 * c**2 * u**2\n\n# %%   Return nan if radical < 0 or d < 0 because LOS vector does not point towards Earth\n    with np.errstate(invalid='ignore'):\n        d = np.where(radical > 0,\n                     (value - a * b * c * np.sqrt(radical)) / magnitude,\n                     np.nan)\n        d[d < 0] = np.nan\n# %% cartesian to ellipsodal\n    lat, lon, _ = ecef2geodetic(x + d * u, y + d * v, z + d * w, deg=deg)\n\n    return lat, lon, d", "response": "This function calculates the line - of - sight intersection with the specified Earth."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts Azimuth Elevation and Slant range to target to East North and Up ENU.", "response": "def aer2enu(az: float, el: float, srange: float, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Azimuth, Elevation, Slant range to target to East, north, Up\n\n    Parameters\n    ----------\n    azimuth : float or np.ndarray of float\n            azimuth clockwise from north (degrees)\n    elevation : float or np.ndarray of float\n        elevation angle above horizon, neglecting aberattions (degrees)\n    srange : float or np.ndarray of float\n        slant range [meters]\n    deg : bool, optional\n        degrees input/output  (False: radians in/out)\n\n    Returns\n    --------\n    e : float or np.ndarray of float\n        East ENU coordinate (meters)\n    n : float or np.ndarray of float\n        North ENU coordinate (meters)\n    u : float or np.ndarray of float\n        Up ENU coordinate (meters)\n    \"\"\"\n    if deg:\n        el = radians(el)\n        az = radians(az)\n\n    with np.errstate(invalid='ignore'):\n        if (np.asarray(srange) < 0).any():\n            raise ValueError('Slant range  [0, Infinity)')\n\n    r = srange * cos(el)\n\n    return r * sin(az), r * cos(az), srange * sin(el)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enu2geodetic(e: float, n: float, u: float,\n                 lat0: float, lon0: float, h0: float,\n                 ell=None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    East, North, Up to target to geodetic coordinates\n\n    Parameters\n    ----------\n    e : float or np.ndarray of float\n        East ENU coordinate (meters)\n    n : float or np.ndarray of float\n        North ENU coordinate (meters)\n    u : float or np.ndarray of float\n        Up ENU coordinate (meters)\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n\n    Results\n    -------\n    lat : float or np.ndarray of float\n          geodetic latitude\n    lon : float or np.ndarray of float\n          geodetic longitude\n    alt : float or np.ndarray of float\n          altitude above ellipsoid  (meters)\n    \"\"\"\n\n    x, y, z = enu2ecef(e, n, u, lat0, lon0, h0, ell, deg=deg)\n\n    return ecef2geodetic(x, y, z, ell, deg=deg)", "response": "This function converts an ENU to geodetic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef geodetic2enu(lat: float, lon: float, h: float,\n                 lat0: float, lon0: float, h0: float,\n                 ell=None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Parameters\n    ----------\n    lat : float or np.ndarray of float\n          target geodetic latitude\n    lon : float or np.ndarray of float\n          target geodetic longitude\n    h : float or np.ndarray of float\n          target altitude above ellipsoid  (meters)\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n\n    Results\n    -------\n    e : float or np.ndarray of float\n        East ENU\n    n : float or np.ndarray of float\n        North ENU\n    u : float or np.ndarray of float\n        Up ENU\n    \"\"\"\n    x1, y1, z1 = geodetic2ecef(lat, lon, h, ell, deg=deg)\n    x2, y2, z2 = geodetic2ecef(lat0, lon0, h0, ell, deg=deg)\n    dx = x1 - x2\n    dy = y1 - y2\n    dz = z1 - z2\n\n    return uvw2enu(dx, dy, dz, lat0, lon0, deg=deg)", "response": "Returns the uvw of the target geodetic at lat lon h0 in the base ellipsoid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vdist(Lat1: float, Lon1: float, Lat2: float, Lon2: float, ell: Ellipsoid = None) -> Tuple[float, float, float]:\n    if ell is None:\n        ell = Ellipsoid()\n# %% prepare inputs\n    lat1 = np.atleast_1d(Lat1)\n    lat2 = np.atleast_1d(Lat2)\n    lon1 = np.atleast_1d(Lon1)\n    lon2 = np.atleast_1d(Lon2)\n\n    assert lat1.shape == lon1.shape and lat2.shape == lon2.shape\n\n    if lat1.shape != lat2.shape:\n        if lat1.size == 1:\n            lat1 = np.broadcast_to(lat1, lat2.shape)\n            lon1 = np.broadcast_to(lon1, lon2.shape)\n\n        if lat2.size == 1:\n            lat2 = np.broadcast_to(lat2, lat1.shape)\n            lon2 = np.broadcast_to(lon2, lon1.shape)\n# %% Input check:\n    if ((abs(lat1) > 90) | (abs(lat2) > 90)).any():\n        raise ValueError('Input latitudes must be in [-90, 90] degrees.')\n# %% Supply WGS84 earth ellipsoid axis lengths in meters:\n    a = ell.a\n    b = ell.b\n# %% preserve true input latitudes:\n    lat1tr = lat1.copy()\n    lat2tr = lat2.copy()\n# %% convert inputs in degrees to np.radians:\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n# %% correct for errors at exact poles by adjusting 0.6 millimeters:\n    kidx = abs(pi / 2 - abs(lat1)) < 1e-10\n    if kidx.any():\n        lat1[kidx] = sign(lat1[kidx]) * (pi / 2 - (1e-10))\n\n    kidx = abs(pi / 2 - abs(lat2)) < 1e-10\n    if kidx.any():\n        lat2[kidx] = sign(lat2[kidx]) * (pi / 2 - (1e-10))\n\n    f = (a - b) / a\n    U1 = arctan((1 - f) * tan(lat1))\n    U2 = arctan((1 - f) * tan(lat2))\n    lon1 = lon1 % (2 * pi)\n    lon2 = lon2 % (2 * pi)\n    L = abs(lon2 - lon1)\n    kidx = L > pi\n    if kidx.any():\n        L[kidx] = 2 * pi - L[kidx]\n\n    lamb = L.copy()  # NOTE: program will fail without copy!\n    lambdaold = np.zeros(lat1.shape)\n    itercount = 0\n    notdone = np.ones(lat1.shape, dtype=bool)\n    alpha = np.zeros(lat1.shape)\n    sigma = np.zeros(lat1.shape)\n    cos2sigmam = np.zeros(lat1.shape)\n    C = np.zeros(lat1.shape)\n    warninggiven = False\n    sinsigma = np.empty(notdone.shape)\n    cossigma = np.empty(notdone.shape)\n    while notdone.any():  # force at least one execution\n        itercount += 1\n        if itercount > 50:\n            if not warninggiven:\n                logging.warning('Essentially antipodal points--precision may be reduced slightly.')\n\n            lamb[notdone] = pi\n            break\n\n        lambdaold[notdone] = lamb[notdone]\n\n        sinsigma[notdone] = sqrt(\n            (cos(U2[notdone]) * sin(lamb[notdone]))**2 +\n            (cos(U1[notdone]) * sin(U2[notdone]) - sin(U1[notdone]) *\n             cos(U2[notdone]) * cos(lamb[notdone]))**2)\n\n        cossigma[notdone] = (sin(U1[notdone]) * sin(U2[notdone]) +\n                             cos(U1[notdone]) * cos(U2[notdone]) *\n                             cos(lamb[notdone]))\n        # eliminate rare imaginary portions at limit of numerical precision:\n        sinsigma[notdone] = sinsigma[notdone].real\n        cossigma[notdone] = cossigma[notdone].real\n\n        sigma[notdone] = arctan2(sinsigma[notdone], cossigma[notdone])\n\n        alpha[notdone] = (arcsin(cos(U1[notdone]) * cos(U2[notdone]) *\n                                 sin(lamb[notdone]) / sin(sigma[notdone])))\n\n        cos2sigmam[notdone] = (cos(sigma[notdone]) - 2 * sin(U1[notdone]) *\n                               sin(U2[notdone]) / cos(alpha[notdone])**2)\n\n        C[notdone] = (f / 16 * cos(alpha[notdone])**2 *\n                      (4 + f * (4 - 3 * cos(alpha[notdone])**2)))\n\n        lamb[notdone] = (L[notdone] + (1 - C[notdone]) * f * sin(alpha[notdone]) *\n                         (sigma[notdone] + C[notdone] * sin(sigma[notdone]) *\n                          (cos2sigmam[notdone] + C[notdone] * cos(sigma[notdone]) *\n                           (-1 + 2. * cos2sigmam[notdone]**2))))\n        # print(f'then, lambda(21752) = {lamb[21752],20})\n        # correct for convergence failure for essentially antipodal points\n        if (lamb[notdone] > pi).any():\n            logging.warning('Essentially antipodal points encountered. Precision may be reduced slightly.')\n            warninggiven = True\n            lambdaold[lamb > pi] = pi\n            lamb[lamb > pi] = pi\n\n        notdone = abs(lamb - lambdaold) > 1e-12\n\n    u2 = cos(alpha)**2 * (a**2 - b**2) / b**2\n    A = 1 + u2 / 16384 * (4096 + u2 * (-768 + u2 * (320 - 175 * u2)))\n    B = u2 / 1024 * (256 + u2 * (-128 + u2 * (74 - 47 * u2)))\n    deltasigma = (B * sin(sigma) *\n                  (cos2sigmam + B / 4 * (cos(sigma) * (-1 + 2 * cos2sigmam**2) -\n                                         B / 6 * cos2sigmam * (-3 + 4 * sin(sigma)**2) * (-3 + 4 * cos2sigmam**2))))\n\n    dist_m = (b * A * (sigma - deltasigma))\n\n# %% From point #1 to point #2\n    # correct sign of lambda for azimuth calcs:\n    lamb = abs(lamb)\n    kidx = sign(sin(lon2 - lon1)) * sign(sin(lamb)) < 0\n    lamb[kidx] = -lamb[kidx]\n    numer = cos(U2) * sin(lamb)\n    denom = cos(U1) * sin(U2) - sin(U1) * cos(U2) * cos(lamb)\n    a12 = arctan2(numer, denom)\n    kidx = a12 < 0\n    a12[kidx] = a12[kidx] + 2 * pi\n    # %% from poles\n    a12[lat1tr <= -90] = 0\n    a12[lat1tr >= 90] = pi\n    az = np.degrees(a12)\n\n# %% From point #2 to point #1\n    # correct sign of lambda for azimuth calcs:\n    lamb = abs(lamb)\n    kidx = sign(sin(lon1 - lon2)) * sign(sin(lamb)) < 0\n    lamb[kidx] = -lamb[kidx]\n    numer = cos(U1) * sin(lamb)\n    denom = sin(U1) * cos(U2) - cos(U1) * sin(U2) * cos(lamb)\n    a21 = arctan2(numer, denom)\n    kidx = a21 < 0\n    a21[kidx] = a21[kidx] + 2 * pi\n    # %% backwards from poles:\n    a21[lat2tr >= 90] = pi\n    a21[lat2tr <= -90] = 0.\n    backaz = np.degrees(a21)\n\n    return dist_m.squeeze()[()], az.squeeze()[()], backaz.squeeze()[()]", "response": "Compute the distance between two points on the reference ellipsoid and compute forward and backward azimuth of the first point on the reference ellipsoid and compute backward azimuth of the first point on the reference ellipsoid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef track2(lat1: float, lon1: float, lat2: float, lon2: float,\n           ell: Ellipsoid = None, npts: int = 100, deg: bool = True):\n    \"\"\"\n    computes great circle tracks starting at the point lat1, lon1 and ending at lat2, lon2\n\n    Parameters\n    ----------\n\n    Lat1 : float or numpy.ndarray of float\n        Geodetic latitude of first point (degrees)\n    Lon1 : float or numpy.ndarray of float\n        Geodetic longitude of first point (degrees)\n    Lat2 : float or numpy.ndarray of float\n        Geodetic latitude of second point (degrees)\n    Lon2 : float or numpy.ndarray of float\n        Geodetic longitude of second point (degrees)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    npts : int, optional\n        number of points (default is 100)\n    deg : bool, optional\n        degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    lats : numpy.ndarray of float\n        latitudes of points along track\n    lons : numpy.ndarray of float\n        longitudes of points along track\n\n    Based on code posted to the GMT mailing list in Dec 1999 by Jim Levens and by Jeff Whitaker <jeffrey.s.whitaker@noaa.gov>\n    \"\"\"\n\n    if ell is None:\n        ell = Ellipsoid()\n\n    if npts <= 1:\n        raise ValueError('npts must be greater than 1')\n\n    if npts == 2:\n        return [lat1, lat2], [lon1, lon2]\n\n    if deg is True:\n        rlat1, rlon1, rlat2, rlon2 = np.radians([lat1, lon1, lat2, lon2])\n    else:\n        rlat1, rlon1, rlat2, rlon2 = lat1, lon1, lat2, lon2\n\n    gcarclen = 2. * np.arcsin(np.sqrt((np.sin((rlat1 - rlat2) / 2))**2 +\n                                      np.cos(rlat1) * np.cos(rlat2) * (np.sin((rlon1 - rlon2) / 2))**2))\n    # check to see if points are antipodal (if so, route is undefined).\n    if np.allclose(gcarclen, pi):\n        raise ValueError('cannot compute intermediate points on a great circle whose endpoints are antipodal')\n\n    distance, azimuth, _ = vdist(lat1, lon1, lat2, lon2)\n    incdist = distance / (npts - 1)\n\n    latpt = lat1\n    lonpt = lon1\n    lons = [lonpt]\n    lats = [latpt]\n    for n in range(npts - 2):\n        latptnew, lonptnew, _ = vreckon(latpt, lonpt, incdist, azimuth)\n        _, azimuth, _ = vdist(latptnew, lonptnew, lat2, lon2, ell=ell)\n        lats.append(latptnew)\n        lons.append(lonptnew)\n        latpt = latptnew\n        lonpt = lonptnew\n    lons.append(lon2)\n    lats.append(lat2)\n\n    if not deg:\n        lats = np.radians(lats)\n        lons = np.radians(lons)\n\n    return lats, lons", "response": "Returns a new track that is a great circle track between two points."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef datetime2sidereal(time: datetime,\n                      lon_radians: float,\n                      usevallado: bool = True) -> float:\n    \"\"\"\n    Convert ``datetime`` to sidereal time\n\n    from D. Vallado \"Fundamentals of Astrodynamics and Applications\"\n\n\n    time : datetime.datetime\n        time to convert\n    lon_radians : float\n        longitude (radians)\n    usevallado : bool, optional\n        use vallado instead of AstroPy (default is Vallado)\n\n    Results\n    -------\n\n    tsr : float\n        Sidereal time\n    \"\"\"\n    usevallado = usevallado or Time is None\n    if usevallado:\n        jd = juliandate(str2dt(time))\n# %% Greenwich Sidereal time RADIANS\n        gst = julian2sidereal(jd)\n# %% Algorithm 15 p. 188 rotate GST to LOCAL SIDEREAL TIME\n        tsr = gst + lon_radians\n    else:\n        tsr = Time(time).sidereal_time(kind='apparent',\n                                       longitude=Longitude(lon_radians, unit=u.radian)).radian\n\n    return tsr", "response": "Convert datetime to sidereal time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a Python datetime to Julian time.", "response": "def juliandate(time: datetime) -> float:\n    \"\"\"\n    Python datetime to Julian time\n\n    from D.Vallado Fundamentals of Astrodynamics and Applications p.187\n     and J. Meeus Astronomical Algorithms 1991 Eqn. 7.1 pg. 61\n\n    Parameters\n    ----------\n\n    time : datetime.datetime\n        time to convert\n\n    Results\n    -------\n\n    jd : float\n        Julian date\n    \"\"\"\n\n    times = np.atleast_1d(time)\n    assert times.ndim == 1\n\n    jd = np.empty(times.size)\n    for i, t in enumerate(times):\n        if t.month < 3:\n            year = t.year - 1\n            month = t.month + 12\n        else:\n            year = t.year\n            month = t.month\n\n        A = int(year / 100.0)\n        B = 2 - A + int(A / 4.)\n        C = ((t.second / 60. + t.minute) / 60. + t.hour) / 24.\n\n        jd[i] = (int(365.25 * (year + 4716)) +\n                 int(30.6001 * (month + 1)) + t.day + B - 1524.5 + C)\n\n    return jd.squeeze()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef julian2sidereal(Jdate: float) -> float:\n\n    jdate = np.atleast_1d(Jdate)\n    assert jdate.ndim == 1\n\n    tsr = np.empty(jdate.size)\n    for i, jd in enumerate(jdate):\n        # %% Vallado Eq. 3-42 p. 184, Seidelmann 3.311-1\n        tUT1 = (jd - 2451545.0) / 36525.\n\n        # Eqn. 3-47 p. 188\n        gmst_sec = (67310.54841 + (876600 * 3600 + 8640184.812866) *\n                    tUT1 + 0.093104 * tUT1**2 - 6.2e-6 * tUT1**3)\n\n        # 1/86400 and %(2*pi) implied by units of radians\n        tsr[i] = gmst_sec * (2 * pi) / 86400. % (2 * pi)\n\n    return tsr.squeeze()", "response": "Convert Julian time to sidereal time."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_radius_normal(lat_radians: float, ell: Ellipsoid = None) -> float:\n    if ell is None:\n        ell = Ellipsoid()\n\n    a = ell.a\n    b = ell.b\n\n    return a**2 / sqrt(a**2 * cos(lat_radians)**2 + b**2 * sin(lat_radians)**2)", "response": "Compute the radius of the normal of the resource in the specified ellipsoid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npoint transformation from Geodetic of specified ellipsoid (default WGS-84) to ECEF Parameters ---------- lat : float or numpy.ndarray of float target geodetic latitude lon : float or numpy.ndarray of float target geodetic longitude h : float or numpy.ndarray of float target altitude above geodetic ellipsoid (meters) ell : Ellipsoid, optional reference ellipsoid deg : bool, optional degrees input/output (False: radians in/out) Returns ------- ECEF (Earth centered, Earth fixed) x,y,z x : float or numpy.ndarray of float target x ECEF coordinate (meters) y : float or numpy.ndarray of float target y ECEF coordinate (meters) z : float or numpy.ndarray of float target z ECEF coordinate (meters)", "response": "def geodetic2ecef(lat: float, lon: float, alt: float,\n                  ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    point transformation from Geodetic of specified ellipsoid (default WGS-84) to ECEF\n\n    Parameters\n    ----------\n\n    lat : float or numpy.ndarray of float\n           target geodetic latitude\n    lon : float or numpy.ndarray of float\n           target geodetic longitude\n    h : float or numpy.ndarray of float\n         target altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n\n    Returns\n    -------\n\n    ECEF (Earth centered, Earth fixed)  x,y,z\n\n    x : float or numpy.ndarray of float\n        target x ECEF coordinate (meters)\n    y : float or numpy.ndarray of float\n        target y ECEF coordinate (meters)\n    z : float or numpy.ndarray of float\n        target z ECEF coordinate (meters)\n    \"\"\"\n    if ell is None:\n        ell = Ellipsoid()\n\n    if deg:\n        lat = radians(lat)\n        lon = radians(lon)\n\n    with np.errstate(invalid='ignore'):\n        # need np.any() to handle scalar and array cases\n        if np.any((lat < -pi / 2) | (lat > pi / 2)):\n            raise ValueError('-90 <= lat <= 90')\n\n    # radius of curvature of the prime vertical section\n    N = get_radius_normal(lat, ell)\n    # Compute cartesian (geocentric) coordinates given  (curvilinear) geodetic\n    # coordinates.\n    x = (N + alt) * cos(lat) * cos(lon)\n    y = (N + alt) * cos(lat) * sin(lon)\n    z = (N * (ell.b / ell.a)**2 + alt) * sin(lat)\n\n    return x, y, z"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ecef2geodetic(x: float, y: float, z: float,\n                  ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    convert ECEF (meters) to geodetic coordinates\n\n    Parameters\n    ----------\n    x : float or numpy.ndarray of float\n        target x ECEF coordinate (meters)\n    y : float or numpy.ndarray of float\n        target y ECEF coordinate (meters)\n    z : float or numpy.ndarray of float\n        target z ECEF coordinate (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n    lat : float or numpy.ndarray of float\n           target geodetic latitude\n    lon : float or numpy.ndarray of float\n           target geodetic longitude\n    h : float or numpy.ndarray of float\n         target altitude above geodetic ellipsoid (meters)\n\n    based on:\n    You, Rey-Jer. (2000). Transformation of Cartesian to Geodetic Coordinates without Iterations.\n    Journal of Surveying Engineering. doi: 10.1061/(ASCE)0733-9453\n    \"\"\"\n    if ell is None:\n        ell = Ellipsoid()\n\n    x = np.asarray(x)\n    y = np.asarray(y)\n    z = np.asarray(z)\n\n    r = sqrt(x**2 + y**2 + z**2)\n\n    E = sqrt(ell.a**2 - ell.b**2)\n\n    # eqn. 4a\n    u = sqrt(0.5 * (r**2 - E**2) + 0.5 * sqrt((r**2 - E**2)**2 + 4 * E**2 * z**2))\n\n    Q = hypot(x, y)\n\n    huE = hypot(u, E)\n\n    # eqn. 4b\n    with np.errstate(divide='ignore'):\n        Beta = arctan(huE / u * z / hypot(x, y))\n\n    # eqn. 13\n    eps = ((ell.b * u - ell.a * huE + E**2) * sin(Beta)) / (ell.a * huE * 1 / cos(Beta) - E**2 * cos(Beta))\n\n    Beta += eps\n# %% final output\n    lat = arctan(ell.a / ell.b * tan(Beta))\n\n    lon = arctan2(y, x)\n\n    # eqn. 7\n    alt = hypot(z - ell.b * sin(Beta),\n                Q - ell.a * cos(Beta))\n\n    # inside ellipsoid?\n    with np.errstate(invalid='ignore'):\n        inside = x**2 / ell.a**2 + y**2 / ell.a**2 + z**2 / ell.b**2 < 1\n    if isinstance(inside, np.ndarray):\n        alt[inside] = -alt[inside]\n    elif inside:\n        alt = -alt\n\n    if deg:\n        lat = degrees(lat)\n        lon = degrees(lon)\n\n    return lat, lon, alt", "response": "convert ECEF coordinates to geodetic coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ecef2enuv(u: float, v: float, w: float,\n              lat0: float, lon0: float, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    VECTOR from observer to target  ECEF => ENU\n\n    Parameters\n    ----------\n    u : float or numpy.ndarray of float\n        target x ECEF coordinate (meters)\n    v : float or numpy.ndarray of float\n        target y ECEF coordinate (meters)\n    w : float or numpy.ndarray of float\n        target z ECEF coordinate (meters)\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n    uEast : float or numpy.ndarray of float\n        target east ENU coordinate (meters)\n    vNorth : float or numpy.ndarray of float\n        target north ENU coordinate (meters)\n    wUp : float or numpy.ndarray of float\n        target up ENU coordinate (meters)\n\n    \"\"\"\n    if deg:\n        lat0 = radians(lat0)\n        lon0 = radians(lon0)\n\n    t = cos(lon0) * u + sin(lon0) * v\n    uEast = -sin(lon0) * u + cos(lon0) * v\n    wUp = cos(lat0) * t + sin(lat0) * w\n    vNorth = -sin(lat0) * t + cos(lat0) * w\n\n    return uEast, vNorth, wUp", "response": "This function converts from observer to target ethernet ECEF to ENU"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a tuple of x y z coordinates of the base ENU segment from observer to target ECEF coordinates.", "response": "def ecef2enu(x: float, y: float, z: float,\n             lat0: float, lon0: float, h0: float,\n             ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    from observer to target, ECEF => ENU\n\n    Parameters\n    ----------\n    x : float or numpy.ndarray of float\n        target x ECEF coordinate (meters)\n    y : float or numpy.ndarray of float\n        target y ECEF coordinate (meters)\n    z : float or numpy.ndarray of float\n        target z ECEF coordinate (meters)\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n    East : float or numpy.ndarray of float\n        target east ENU coordinate (meters)\n    North : float or numpy.ndarray of float\n        target north ENU coordinate (meters)\n    Up : float or numpy.ndarray of float\n        target up ENU coordinate (meters)\n\n    \"\"\"\n    x0, y0, z0 = geodetic2ecef(lat0, lon0, h0, ell, deg=deg)\n\n    return uvw2enu(x - x0, y - y0, z - z0, lat0, lon0, deg=deg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enu2uvw(east: float, north: float, up: float,\n            lat0: float, lon0: float, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Parameters\n    ----------\n\n    e1 : float or numpy.ndarray of float\n        target east ENU coordinate (meters)\n    n1 : float or numpy.ndarray of float\n        target north ENU coordinate (meters)\n    u1 : float or numpy.ndarray of float\n        target up ENU coordinate (meters)\n\n    Results\n    -------\n\n    u : float or numpy.ndarray of float\n    v : float or numpy.ndarray of float\n    w : float or numpy.ndarray of float\n    \"\"\"\n\n    if deg:\n        lat0 = radians(lat0)\n        lon0 = radians(lon0)\n\n    t = cos(lat0) * up - sin(lat0) * north\n    w = sin(lat0) * up + cos(lat0) * north\n\n    u = cos(lon0) * t - sin(lon0) * east\n    v = sin(lon0) * t + cos(lon0) * east\n\n    return u, v, w", "response": "This function converts an ENU to a U V and W coordinates."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef uvw2enu(u: float, v: float, w: float,\n            lat0: float, lon0: float, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Parameters\n    ----------\n\n    u : float or numpy.ndarray of float\n    v : float or numpy.ndarray of float\n    w : float or numpy.ndarray of float\n\n\n    Results\n    -------\n\n    East : float or numpy.ndarray of float\n        target east ENU coordinate (meters)\n    North : float or numpy.ndarray of float\n        target north ENU coordinate (meters)\n    Up : float or numpy.ndarray of float\n        target up ENU coordinate (meters)\n    \"\"\"\n    if deg:\n        lat0 = radians(lat0)\n        lon0 = radians(lon0)\n\n    t = cos(lon0) * u + sin(lon0) * v\n    East = -sin(lon0) * u + cos(lon0) * v\n    Up = cos(lat0) * t + sin(lat0) * w\n    North = -sin(lat0) * t + cos(lat0) * w\n\n    return East, North, Up", "response": "This function converts UVV V W to North and Up coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert ECI to geodetic coordinates", "response": "def eci2geodetic(eci: np.ndarray, t: datetime,\n                 useastropy: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    convert ECI to geodetic coordinates\n\n    Parameters\n    ----------\n    eci : tuple of float\n          [meters] Nx3 target ECI location (x,y,z)\n    t : datetime.datetime, float\n          length N vector of datetime OR greenwich sidereal time angle [radians].\n\n    Results\n    -------\n    lat : float\n          geodetic latitude\n    lon : float\n          geodetic longitude\n    alt : float\n          altitude above ellipsoid  (meters)\n\n    Notes\n    -----\n\n    Conversion is idealized: doesn't consider nutations, perterbations,\n    etc. like the IAU-76/FK5 or IAU-2000/2006 model-based conversions\n    from ECI to ECEF\n\n    eci2geodetic() a.k.a. eci2lla()\n    \"\"\"\n    ecef = np.atleast_2d(eci2ecef(eci, t, useastropy=useastropy))\n\n    return np.asarray(ecef2geodetic(ecef[:, 0], ecef[:, 1], ecef[:, 2])).squeeze()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enu2ecef(e1: float, n1: float, u1: float,\n             lat0: float, lon0: float, h0: float,\n             ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    ENU to ECEF\n\n    Parameters\n    ----------\n\n    e1 : float or numpy.ndarray of float\n        target east ENU coordinate (meters)\n    n1 : float or numpy.ndarray of float\n        target north ENU coordinate (meters)\n    u1 : float or numpy.ndarray of float\n        target up ENU coordinate (meters)\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n\n    Results\n    -------\n    x : float or numpy.ndarray of float\n        target x ECEF coordinate (meters)\n    y : float or numpy.ndarray of float\n        target y ECEF coordinate (meters)\n    z : float or numpy.ndarray of float\n        target z ECEF coordinate (meters)\n    \"\"\"\n    x0, y0, z0 = geodetic2ecef(lat0, lon0, h0, ell, deg=deg)\n    dx, dy, dz = enu2uvw(e1, n1, u1, lat0, lon0, deg=deg)\n\n    return x0 + dx, y0 + dy, z0 + dz", "response": "This function converts ENU to ECEF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert azimuth, elevation, range to target from observer to North, East, Down Parameters ----------- az : float or numpy.ndarray of float azimuth elev : float or numpy.ndarray of float elevation slantRange : float or numpy.ndarray of float slant range [meters] deg : bool, optional degrees input/output (False: radians in/out) Results ------- n : float or numpy.ndarray of float North NED coordinate (meters) e : float or numpy.ndarray of float East NED coordinate (meters) d : float or numpy.ndarray of float Down NED coordinate (meters)", "response": "def aer2ned(az: float, elev: float, slantRange: float,\n            deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    converts azimuth, elevation, range to target from observer to North, East, Down\n\n    Parameters\n    -----------\n\n    az : float or numpy.ndarray of float\n         azimuth\n    elev : float or numpy.ndarray of float\n         elevation\n    slantRange : float or numpy.ndarray of float\n         slant range [meters]\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n    n : float or numpy.ndarray of float\n        North NED coordinate (meters)\n    e : float or numpy.ndarray of float\n        East NED coordinate (meters)\n    d : float or numpy.ndarray of float\n        Down NED coordinate (meters)\n    \"\"\"\n    e, n, u = aer2enu(az, elev, slantRange, deg=deg)\n\n    return n, e, -u"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts North, East, Down to azimuth, elevation, range Parameters ---------- n : float or numpy.ndarray of float North NED coordinate (meters) e : float or numpy.ndarray of float East NED coordinate (meters) d : float or numpy.ndarray of float Down NED coordinate (meters) deg : bool, optional degrees input/output (False: radians in/out) Results ------- az : float or numpy.ndarray of float azimuth elev : float or numpy.ndarray of float elevation slantRange : float or numpy.ndarray of float slant range [meters]", "response": "def ned2aer(n: float, e: float, d: float,\n            deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    converts North, East, Down to azimuth, elevation, range\n\n    Parameters\n    ----------\n\n    n : float or numpy.ndarray of float\n        North NED coordinate (meters)\n    e : float or numpy.ndarray of float\n        East NED coordinate (meters)\n    d : float or numpy.ndarray of float\n        Down NED coordinate (meters)\n    deg : bool, optional\n        degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    az : float or numpy.ndarray of float\n         azimuth\n    elev : float or numpy.ndarray of float\n         elevation\n    slantRange : float or numpy.ndarray of float\n         slant range [meters]\n    \"\"\"\n    return enu2aer(e, n, -d, deg=deg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert North East Down to target latitude longitude altitude and optional reference ellipsoid.", "response": "def ned2geodetic(n: float, e: float, d: float,\n                 lat0: float, lon0: float, h0: float,\n                 ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Converts North, East, Down to target latitude, longitude, altitude\n\n    Parameters\n    ----------\n\n    n : float or numpy.ndarray of float\n        North NED coordinate (meters)\n    e : float or numpy.ndarray of float\n        East NED coordinate (meters)\n    d : float or numpy.ndarray of float\n        Down NED coordinate (meters)\n    lat0 : float\n        Observer geodetic latitude\n    lon0 : float\n        Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    lat : float\n        target geodetic latitude\n    lon : float\n        target geodetic longitude\n    h : float\n        target altitude above geodetic ellipsoid (meters)\n\n    \"\"\"\n    x, y, z = enu2ecef(e, n, -d, lat0, lon0, h0, ell, deg=deg)\n\n    return ecef2geodetic(x, y, z, ell, deg=deg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn North East Down to target ECEF coordinates", "response": "def ned2ecef(n: float, e: float, d: float,\n             lat0: float, lon0: float, h0: float,\n             ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    North, East, Down to target ECEF coordinates\n\n    Parameters\n    ----------\n\n    n : float or numpy.ndarray of float\n        North NED coordinate (meters)\n    e : float or numpy.ndarray of float\n        East NED coordinate (meters)\n    d : float or numpy.ndarray of float\n        Down NED coordinate (meters)\n    lat0 : float\n        Observer geodetic latitude\n    lon0 : float\n        Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    x : float or numpy.ndarray of float\n        ECEF x coordinate (meters)\n    y : float or numpy.ndarray of float\n        ECEF y coordinate (meters)\n    z : float or numpy.ndarray of float\n        ECEF z coordinate (meters)\n    \"\"\"\n    return enu2ecef(e, n, -d, lat0, lon0, h0, ell, deg=deg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ecef2ned(x: float, y: float, z: float,\n             lat0: float, lon0: float, h0: float,\n             ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Convert ECEF x,y,z to North, East, Down\n\n    Parameters\n    ----------\n\n    x : float or numpy.ndarray of float\n        ECEF x coordinate (meters)\n    y : float or numpy.ndarray of float\n        ECEF y coordinate (meters)\n    z : float or numpy.ndarray of float\n        ECEF z coordinate (meters)\n    lat0 : float\n        Observer geodetic latitude\n    lon0 : float\n        Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    n : float or numpy.ndarray of float\n        North NED coordinate (meters)\n    e : float or numpy.ndarray of float\n        East NED coordinate (meters)\n    d : float or numpy.ndarray of float\n        Down NED coordinate (meters)\n\n    \"\"\"\n    e, n, u = ecef2enu(x, y, z, lat0, lon0, h0, ell, deg=deg)\n\n    return n, e, -u", "response": "Convert ECEF x y z to North East Down NED"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ecef2nedv(x: float, y: float, z: float,\n              lat0: float, lon0: float,\n              deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    for VECTOR between two points\n\n    Parameters\n    ----------\n    x : float or numpy.ndarray of float\n        ECEF x coordinate (meters)\n    y : float or numpy.ndarray of float\n        ECEF y coordinate (meters)\n    z : float or numpy.ndarray of float\n        ECEF z coordinate (meters)\n    lat0 : float\n        Observer geodetic latitude\n    lon0 : float\n        Observer geodetic longitude\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    (Vector)\n\n    n : float or numpy.ndarray of float\n        North NED coordinate (meters)\n    e : float or numpy.ndarray of float\n        East NED coordinate (meters)\n    d : float or numpy.ndarray of float\n        Down NED coordinate (meters)\n    \"\"\"\n    e, n, u = ecef2enuv(x, y, z, lat0, lon0, deg=deg)\n\n    return n, e, -u", "response": "This function converts ECEF coordinates to NED coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting azimuth, elevation to right ascension, declination Parameters ---------- az_deg : float or numpy.ndarray of float azimuth (clockwise) to point [degrees] el_deg : float or numpy.ndarray of float elevation above horizon to point [degrees] lat_deg : float observer WGS84 latitude [degrees] lon_deg : float observer WGS84 longitude [degrees] time : datetime.datetime time of observation Results ------- ra_deg : float or numpy.ndarray of float right ascension to target [degrees] dec_deg : float or numpy.ndarray of float declination of target [degrees] from D.Vallado Fundamentals of Astrodynamics and Applications p.258-259", "response": "def azel2radec(az_deg: float, el_deg: float,\n               lat_deg: float, lon_deg: float,\n               time: datetime) -> Tuple[float, float]:\n    \"\"\"\n    converts azimuth, elevation to right ascension, declination\n\n    Parameters\n    ----------\n\n    az_deg : float or numpy.ndarray of float\n        azimuth (clockwise) to point [degrees]\n\n    el_deg : float or numpy.ndarray of float\n        elevation above horizon to point [degrees]\n\n    lat_deg : float\n        observer WGS84 latitude [degrees]\n\n    lon_deg : float\n        observer WGS84 longitude [degrees]\n\n    time : datetime.datetime\n        time of observation\n\n    Results\n    -------\n\n    ra_deg : float or numpy.ndarray of float\n        right ascension to target [degrees]\n\n    dec_deg : float or numpy.ndarray of float\n        declination of target [degrees]\n\n    from D.Vallado Fundamentals of Astrodynamics and Applications\n    p.258-259\n    \"\"\"\n    az = atleast_1d(az_deg)\n    el = atleast_1d(el_deg)\n    lat = atleast_1d(lat_deg)\n    lon = atleast_1d(lon_deg)\n\n    if az.shape != el.shape:\n        raise ValueError('az and el must be same shape ndarray')\n    if not(lat.size == 1 and lon.size == 1):\n        raise ValueError('need one observer and one or more  (az,el).')\n    if ((lat < -90) | (lat > 90)).any():\n        raise ValueError('-90 <= lat <= 90')\n\n    az = radians(az)\n    el = radians(el)\n    lat = radians(lat)\n    lon = radians(lon)\n# %% Vallado \"algorithm 28\" p 268\n    dec = arcsin(sin(el) * sin(lat) + cos(el) * cos(lat) * cos(az))\n\n    lha = arctan2(-(sin(az) * cos(el)) / cos(dec),\n                  (sin(el) - sin(lat) * sin(dec)) / (cos(dec) * cos(lat)))\n\n    lst = datetime2sidereal(time, lon)  # lon, ra in RADIANS\n\n    \"\"\" by definition right ascension [0, 360) degrees \"\"\"\n    return degrees(lst - lha) % 360, degrees(dec)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert right ascension, declination to azimuth, elevation Parameters ---------- ra_deg : float or numpy.ndarray of float right ascension to target [degrees] dec_deg : float or numpy.ndarray of float declination to target [degrees] lat_deg : float observer WGS84 latitude [degrees] lon_deg : float observer WGS84 longitude [degrees] time : datetime.datetime time of observation Results ------- az_deg : float or numpy.ndarray of float azimuth clockwise from north to point [degrees] el_deg : float or numpy.ndarray of float elevation above horizon to point [degrees] from D. Vallado \"Fundamentals of Astrodynamics and Applications \" 4th Edition Ch. 4.4 pg. 266-268", "response": "def radec2azel(ra_deg: float, dec_deg: float,\n               lat_deg: float, lon_deg: float,\n               time: datetime) -> Tuple[float, float]:\n    \"\"\"\n    converts right ascension, declination to azimuth, elevation\n\n    Parameters\n    ----------\n\n    ra_deg : float or numpy.ndarray of float\n        right ascension to target [degrees]\n\n    dec_deg : float or numpy.ndarray of float\n        declination to target [degrees]\n\n    lat_deg : float\n        observer WGS84 latitude [degrees]\n\n    lon_deg : float\n        observer WGS84 longitude [degrees]\n\n    time : datetime.datetime\n        time of observation\n\n    Results\n    -------\n\n    az_deg : float or numpy.ndarray of float\n        azimuth clockwise from north to point [degrees]\n\n    el_deg : float or numpy.ndarray of float\n        elevation above horizon to point [degrees]\n\n\n    from D. Vallado \"Fundamentals of Astrodynamics and Applications \"\n       4th Edition Ch. 4.4 pg. 266-268\n    \"\"\"\n    ra = atleast_1d(ra_deg)\n    dec = atleast_1d(dec_deg)\n    lat = atleast_1d(lat_deg)\n    lon = atleast_1d(lon_deg)\n\n    if ra.shape != dec.shape:\n        raise ValueError('az and el must be same shape ndarray')\n    if not(lat.size == 1 and lon.size == 1):\n        raise ValueError('need one observer and one or more  (az,el).')\n    if ((lat < -90) | (lat > 90)).any():\n        raise ValueError('-90 <= lat <= 90')\n\n    ra = radians(ra)\n    dec = radians(dec)\n    lat = radians(lat)\n    lon = radians(lon)\n\n    lst = datetime2sidereal(time, lon)  # RADIANS\n# %% Eq. 4-11 p. 267 LOCAL HOUR ANGLE\n    lha = lst - ra\n# %% #Eq. 4-12 p. 267\n    el = arcsin(sin(lat) * sin(dec) + cos(lat) * cos(dec) * cos(lha))\n# %% combine Eq. 4-13 and 4-14 p. 268\n    az = arctan2(-sin(lha) * cos(dec) / cos(el),\n                 (sin(dec) - sin(el) * sin(lat)) / (cos(el) * cos(lat)))\n\n    return degrees(az) % 360.0, degrees(el)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts ECEF coordinates to geodetic coordinates", "response": "def ecef2geodetic_old(x: float, y: float, z: float,\n                      ell: Ellipsoid = None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    convert ECEF (meters) to geodetic coordinates\n\n    input\n    -----\n    x,y,z  [meters] target ECEF location                             [0,Infinity)\n    ell    reference ellipsoid\n    deg    degrees input/output  (False: radians in/out)\n\n    output\n    ------\n    lat,lon   (degrees/radians)\n    alt  (meters)\n\n    Algorithm is based on\n    http://www.astro.uni.torun.pl/~kb/Papers/geod/Geod-BG.htm\n    This algorithm provides a converging solution to the latitude equation\n    in terms of the parametric or reduced latitude form (v)\n    This algorithm provides a uniform solution over all latitudes as it does\n    not involve division by cos(phi) or sin(phi)\n    \"\"\"\n    if ell is None:\n        ell = Ellipsoid()\n\n    ea = ell.a\n    eb = ell.b\n    rad = hypot(x, y)\n# Constant required for Latitude equation\n    rho = arctan2(eb * z, ea * rad)\n# Constant required for latitude equation\n    c = (ea**2 - eb**2) / hypot(ea * rad, eb * z)\n# Starter for the Newtons Iteration Method\n    vnew = arctan2(ea * z, eb * rad)\n# Initializing the parametric latitude\n    v = 0\n    for _ in range(5):\n        v = deepcopy(vnew)\n# %% Newtons Method for computing iterations\n        vnew = v - ((2 * sin(v - rho) - c * sin(2 * v)) /\n                    (2 * (cos(v - rho) - c * cos(2 * v))))\n\n        if allclose(v, vnew):\n            break\n# %% Computing latitude from the root of the latitude equation\n    lat = arctan2(ea * tan(vnew), eb)\n    # by inspection\n    lon = arctan2(y, x)\n\n    alt = (((rad - ea * cos(vnew)) * cos(lat)) +\n           ((z - eb * sin(vnew)) * sin(lat)))\n\n    with np.errstate(invalid='ignore'):\n        # NOTE: need np.any() to handle scalar and array cases\n        if np.any((lat < -pi / 2) | (lat > pi / 2)):\n            raise ValueError('-90 <= lat <= 90')\n\n        if np.any((lon < -pi) | (lon > 2 * pi)):\n            raise ValueError('-180 <= lat <= 360')\n\n    if deg:\n        return degrees(lat), degrees(lon), alt\n    else:\n        return lat, lon, alt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving azimuth, elevation and slant range from an Observer to a Point with ECEF coordinates. ECEF input location is with units of meters Parameters ---------- x : float or numpy.ndarray of float ECEF x coordinate (meters) y : float or numpy.ndarray of float ECEF y coordinate (meters) z : float or numpy.ndarray of float ECEF z coordinate (meters) lat0 : float Observer geodetic latitude lon0 : float Observer geodetic longitude h0 : float observer altitude above geodetic ellipsoid (meters) ell : Ellipsoid, optional reference ellipsoid deg : bool, optional degrees input/output (False: radians in/out) Returns ------- az : float or numpy.ndarray of float azimuth to target el : float or numpy.ndarray of float elevation to target srange : float or numpy.ndarray of float slant range [meters]", "response": "def ecef2aer(x: float, y: float, z: float,\n             lat0: float, lon0: float, h0: float,\n             ell=None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    gives azimuth, elevation and slant range from an Observer to a Point with ECEF coordinates.\n\n    ECEF input location is with units of meters\n\n    Parameters\n    ----------\n\n    x : float or numpy.ndarray of float\n        ECEF x coordinate (meters)\n    y : float or numpy.ndarray of float\n        ECEF y coordinate (meters)\n    z : float or numpy.ndarray of float\n        ECEF z coordinate (meters)\n    lat0 : float\n        Observer geodetic latitude\n    lon0 : float\n        Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n        reference ellipsoid\n    deg : bool, optional\n        degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n    az : float or numpy.ndarray of float\n         azimuth to target\n    el : float or numpy.ndarray of float\n         elevation to target\n    srange : float or numpy.ndarray of float\n         slant range [meters]\n    \"\"\"\n    xEast, yNorth, zUp = ecef2enu(x, y, z, lat0, lon0, h0, ell, deg=deg)\n\n    return enu2aer(xEast, yNorth, zUp, deg=deg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef geodetic2aer(lat: float, lon: float, h: float,\n                 lat0: float, lon0: float, h0: float,\n                 ell=None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    gives azimuth, elevation and slant range from an Observer to a Point with geodetic coordinates.\n\n\n    Parameters\n    ----------\n\n    lat : float or numpy.ndarray of float\n        target geodetic latitude\n    lon : float or numpy.ndarray of float\n        target geodetic longitude\n    h : float or numpy.ndarray of float\n        target altitude above geodetic ellipsoid (meters)\n    lat0 : float\n        Observer geodetic latitude\n    lon0 : float\n        Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n    az : float or numpy.ndarray of float\n         azimuth\n    el : float or numpy.ndarray of float\n         elevation\n    srange : float or numpy.ndarray of float\n         slant range [meters]\n    \"\"\"\n    e, n, u = geodetic2enu(lat, lon, h, lat0, lon0, h0, ell, deg=deg)\n\n    return enu2aer(e, n, u, deg=deg)", "response": "Converts a geodetic location to a Point with geodetic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aer2geodetic(az: float, el: float, srange: float,\n                 lat0: float, lon0: float, h0: float,\n                 ell=None,\n                 deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    gives geodetic coordinates of a point with az, el, range\n    from an observer at lat0, lon0, h0\n\n    Parameters\n    ----------\n    az : float or numpy.ndarray of float\n         azimuth to target\n    el : float or numpy.ndarray of float\n         elevation to target\n    srange : float or numpy.ndarray of float\n         slant range [meters]\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n\n    In reference ellipsoid system:\n\n    lat : float or numpy.ndarray of float\n          geodetic latitude\n    lon : float or numpy.ndarray of float\n          geodetic longitude\n    alt : float or numpy.ndarray of float\n          altitude above ellipsoid  (meters)\n    \"\"\"\n    x, y, z = aer2ecef(az, el, srange, lat0, lon0, h0, ell=ell, deg=deg)\n\n    return ecef2geodetic(x, y, z, ell=ell, deg=deg)", "response": "Returns the geodetic coordinates of a point with az el range lat0 lon0 h0."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple of azimuth el h0 from an observer s ECI coordinates and a range from an observer s ECI location.", "response": "def eci2aer(eci: Tuple[float, float, float],\n            lat0: float, lon0: float, h0: float,\n            t: datetime,\n            useastropy: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    takes ECI coordinates of point and gives az, el, slant range from Observer\n\n    Parameters\n    ----------\n\n    eci : tuple\n          [meters] Nx3 target ECI location (x,y,z)\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    t : datetime.datetime\n        Observation time\n\n\n    Returns\n    -------\n    az : float\n         azimuth to target\n    el : float\n         elevation to target\n    srange : float\n         slant range [meters]\n    \"\"\"\n    ecef = np.atleast_2d(eci2ecef(eci, t, useastropy))\n\n    return ecef2aer(ecef[:, 0], ecef[:, 1], ecef[:, 2], lat0, lon0, h0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive ECI of a point from an observer at az, el, slant range Parameters ---------- az : float or numpy.ndarray of float azimuth to target el : float or numpy.ndarray of float elevation to target srange : float or numpy.ndarray of float slant range [meters] lat0 : float Observer geodetic latitude lon0 : float Observer geodetic longitude h0 : float observer altitude above geodetic ellipsoid (meters) ell : Ellipsoid, optional reference ellipsoid deg : bool, optional degrees input/output (False: radians in/out) t : datetime.datetime Observation time Returns ------- Earth Centered Inertial x,y,z x : float or numpy.ndarray of float ECEF x coordinate (meters) y : float or numpy.ndarray of float ECEF y coordinate (meters) z : float or numpy.ndarray of float ECEF z coordinate (meters)", "response": "def aer2eci(az: float, el: float, srange: float,\n            lat0: float, lon0: float, h0: float, t: datetime,\n            ell=None, deg: bool = True,\n            useastropy: bool = True) -> np.ndarray:\n    \"\"\"\n    gives ECI of a point from an observer at az, el, slant range\n\n    Parameters\n    ----------\n    az : float or numpy.ndarray of float\n         azimuth to target\n    el : float or numpy.ndarray of float\n         elevation to target\n    srange : float or numpy.ndarray of float\n         slant range [meters]\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n    t : datetime.datetime\n        Observation time\n\n    Returns\n    -------\n\n    Earth Centered Inertial x,y,z\n\n    x : float or numpy.ndarray of float\n        ECEF x coordinate (meters)\n    y : float or numpy.ndarray of float\n        ECEF y coordinate (meters)\n    z : float or numpy.ndarray of float\n        ECEF z coordinate (meters)\n    \"\"\"\n    x, y, z = aer2ecef(az, el, srange, lat0, lon0, h0, ell, deg)\n\n    return ecef2eci(np.column_stack((x, y, z)), t, useastropy)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting target azimuth elevation range from observer at lat0 lon0 alt0 to ECEF coordinates.", "response": "def aer2ecef(az: float, el: float, srange: float,\n             lat0: float, lon0: float, alt0: float,\n             ell=None, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    converts target azimuth, elevation, range from observer at lat0,lon0,alt0 to ECEF coordinates.\n\n    Parameters\n    ----------\n    az : float or numpy.ndarray of float\n         azimuth to target\n    el : float or numpy.ndarray of float\n         elevation to target\n    srange : float or numpy.ndarray of float\n         slant range [meters]\n    lat0 : float\n           Observer geodetic latitude\n    lon0 : float\n           Observer geodetic longitude\n    h0 : float\n         observer altitude above geodetic ellipsoid (meters)\n    ell : Ellipsoid, optional\n          reference ellipsoid\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n\n    ECEF (Earth centered, Earth fixed)  x,y,z\n\n    x : float or numpy.ndarray of float\n        ECEF x coordinate (meters)\n    y : float or numpy.ndarray of float\n        ECEF y coordinate (meters)\n    z : float or numpy.ndarray of float\n        ECEF z coordinate (meters)\n\n\n    Notes\n    ------\n    if srange==NaN, z=NaN\n    \"\"\"\n    # Origin of the local system in geocentric coordinates.\n    x0, y0, z0 = geodetic2ecef(lat0, lon0, alt0, ell, deg=deg)\n    # Convert Local Spherical AER to ENU\n    e1, n1, u1 = aer2enu(az, el, srange, deg=deg)\n    # Rotating ENU to ECEF\n    dx, dy, dz = enu2uvw(e1, n1, u1, lat0, lon0, deg=deg)\n    # Origin + offset from origin equals position in ECEF\n    return x0 + dx, y0 + dy, z0 + dz"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the isometric latitude of a point on an ellipsoidal mercator projection.", "response": "def isometric(lat: float, ell: Ellipsoid = None, deg: bool = True):\n    \"\"\"\n     computes isometric latitude of a point on an ellipsoid\n\n     Parameters\n     ----------\n\n     lat : float or numpy.ndarray of float\n         geodetic latitude\n     ell : Ellipsoid, optional\n         reference ellipsoid (default WGS84)\n     deg : bool, optional\n         degrees input/output  (False: radians in/out)\n\n     Returns\n     -------\n\n     isolat : float or numpy.ndarray of float\n         isometric latiude\n\n     Notes\n     -----\n\n     Isometric latitude is an auxiliary latitude proportional to the spacing\n     of parallels of latitude on an ellipsoidal mercator projection.\n\n     Based on Deakin, R.E., 2010, 'The Loxodrome on an Ellipsoid', Lecture Notes,\n     School of Mathematical and Geospatial Sciences, RMIT University,\n     January 2010\n    \"\"\"\n\n    if ell is None:\n        ell = Ellipsoid()\n\n    f = ell.f  # flattening of ellipsoid\n\n    if deg is True:\n        lat = np.deg2rad(lat)\n\n    e2 = f * (2 - f)  # eccentricity-squared\n    e = np.sqrt(e2)  # eccentricity of ellipsoid\n\n    x = e * np.sin(lat)\n    y = (1 - x) / (1 + x)\n    z = np.pi / 4 + lat / 2\n\n#   calculate the isometric latitude\n    isolat = np.log(np.tan(z) * (y**(e / 2)))\n\n    if deg is True:\n        isolat = np.degrees(isolat)\n\n    return isolat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the meridian distance of a given latitude.", "response": "def meridian_dist(lat: float, ell: Ellipsoid = None, deg: bool = True):\n    \"\"\"\n    computes the meridian distance on an ellipsoid *from the equator* to a latitude.\n\n    Parameters\n    ----------\n\n    lat : float or numpy.ndarray of float\n        geodetic latitude\n    ell : Ellipsoid, optional\n         reference ellipsoid (default WGS84)\n    deg : bool, optional\n         degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    mdist : float or numpy.ndarray of float\n         meridian distance (degrees/radians)\n\n\n    Notes\n    -----\n\n    Formula given Baeschlin, C.F., 1948,\n    \"Lehrbuch Der Geodasie\", Orell Fussli Verlag, Zurich, pp.47-50.\n\n    Based on Deakin, R.E., 2010, 'The Loxodrome on an Ellipsoid', Lecture Notes,\n    School of Mathematical and Geospatial Sciences, RMIT University, January 2010\n    \"\"\"\n\n    if deg is True:\n        lat = np.radians(lat)\n\n    #   set ellipsoid parameters\n    if ell is None:\n        ell = Ellipsoid()\n\n    a = ell.a\n    f = ell.f  # flattening of ellipsoid\n\n    e2 = f * (2 - f)  # eccentricity-squared\n\n    # powers of eccentricity\n    e4 = e2 * e2\n    e6 = e4 * e2\n    e8 = e6 * e2\n    e10 = e8 * e2\n\n    # coefficients of series expansion for meridian distance\n    A = 1 + (3 / 4) * e2 + (45 / 64) * e4 + (175 / 256) * e6 + (11025 / 16384) * e8 + (43659 / 65536) * e10\n    B = (3 / 4) * e2 + (15 / 16) * e4 + (525 / 512) * e6 + (2205 / 2048) * e8 + (72765 / 65536) * e10\n    C = (15 / 64) * e4 + (105 / 256) * e6 + (2205 / 4096) * e8 + (10395 / 16384) * e10\n    D = (35 / 512) * e6 + (315 / 2048) * e8 + (31185 / 131072) * e10\n    E = (315 / 16384) * e8 + (3465 / 65536) * e10\n    F = (693 / 131072) * e10\n\n    term1 = A * lat\n    term2 = (B / 2) * np.sin(2 * lat)\n    term3 = (C / 4) * np.sin(4 * lat)\n    term4 = (D / 6) * np.sin(6 * lat)\n    term5 = (E / 8) * np.sin(8 * lat)\n    term6 = (F / 10) * np.sin(10 * lat)\n\n    mdist = a * (1 - e2) * (term1 - term2 + term3 - term4 + term5 - term6)\n\n    return mdist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the arc length and azimuth of the loxodrome between two points on the surface of the reference ellipsoid Parameters ---------- lat1 : float or numpy.ndarray of float geodetic latitude of first point lon1 : float or numpy.ndarray of float geodetic longitude of first point lat2 : float or numpy.ndarray of float geodetic latitude of second point lon2 : float or numpy.ndarray of float geodetic longitude of second point ell : Ellipsoid, optional reference ellipsoid (default WGS84) deg : bool, optional degrees input/output (False: radians in/out) Results ------- lox_s : float or numpy.ndarray of float distance along loxodrome az12 : float or numpy.ndarray of float azimuth of loxodrome (degrees/radians) Based on Deakin, R.E., 2010, 'The Loxodrome on an Ellipsoid', Lecture Notes, School of Mathematical and Geospatial Sciences, RMIT University, January 2010 [1] Bowring, B.R., 1985, 'The geometry of the loxodrome on the ellipsoid', The Canadian Surveyor, Vol. 39, No. 3, Autumn 1985, pp.223-230. [2] Snyder, J.P., 1987, Map Projections-A Working Manual. U.S. Geological Survey Professional Paper 1395. Washington, DC: U.S. Government Printing Office, pp.15-16 and pp. 44-45. [3] Thomas, P.D., 1952, Conformal Projections in Geodesy and Cartography, Special Publication No. 251, Coast and Geodetic Survey, U.S. Department of Commerce, Washington, DC: U.S. Government Printing Office, p. 66.", "response": "def loxodrome_inverse(lat1: float, lon1: float, lat2: float, lon2: float,\n                      ell: Ellipsoid = None, deg: bool = True):\n    \"\"\"\n    computes the arc length and azimuth of the loxodrome\n    between two points on the surface of the reference ellipsoid\n\n    Parameters\n    ----------\n\n    lat1 : float or numpy.ndarray of float\n        geodetic latitude of first point\n    lon1 : float or numpy.ndarray of float\n        geodetic longitude of first point\n    lat2 : float or numpy.ndarray of float\n        geodetic latitude of second point\n    lon2 : float or numpy.ndarray of float\n        geodetic longitude of second point\n    ell : Ellipsoid, optional\n         reference ellipsoid (default WGS84)\n    deg : bool, optional\n         degrees input/output  (False: radians in/out)\n\n    Results\n    -------\n\n    lox_s : float or numpy.ndarray of float\n        distance along loxodrome\n    az12 : float or numpy.ndarray of float\n        azimuth of loxodrome (degrees/radians)\n\n    Based on Deakin, R.E., 2010, 'The Loxodrome on an Ellipsoid', Lecture Notes,\n    School of Mathematical and Geospatial Sciences, RMIT University, January 2010\n\n    [1] Bowring, B.R., 1985, 'The geometry of the loxodrome on the\n    ellipsoid', The Canadian Surveyor, Vol. 39, No. 3, Autumn 1985,\n    pp.223-230.\n    [2] Snyder, J.P., 1987, Map Projections-A Working Manual. U.S.\n    Geological Survey Professional Paper 1395. Washington, DC: U.S.\n    Government Printing Office, pp.15-16 and pp. 44-45.\n    [3] Thomas, P.D., 1952, Conformal Projections in Geodesy and\n    Cartography, Special Publication No. 251, Coast and Geodetic\n    Survey, U.S. Department of Commerce, Washington, DC: U.S.\n    Government Printing Office, p. 66.\n    \"\"\"\n\n    #   set ellipsoid parameters\n    if ell is None:\n        ell = Ellipsoid()\n\n    if deg is True:\n        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n\n    # compute isometric latitude of P1 and P2\n    isolat1 = isometric(lat1, deg=False, ell=ell)\n    isolat2 = isometric(lat2, deg=False, ell=ell)\n\n    # compute changes in isometric latitude and longitude between points\n    disolat = isolat2 - isolat1\n    dlon = lon2 - lon1\n\n    # compute azimuth\n    az12 = np.arctan2(dlon, disolat)\n\n    # compute distance along loxodromic curve\n    m1 = meridian_dist(lat1, deg=False, ell=ell)\n    m2 = meridian_dist(lat2, deg=False, ell=ell)\n    dm = m2 - m1\n    lox_s = dm / np.cos(az12)\n\n    if deg is True:\n        az12 = np.degrees(az12) % 360.\n\n    return lox_s, az12"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eci2ecef(eci: np.ndarray,\n             time: datetime,\n             useastropy: bool = True) -> np.ndarray:\n    \"\"\"\n    Observer => Point  ECI  =>  ECEF\n\n    Parameters\n    ----------\n    eci : tuple of float\n        Nx3 target ECI location (x,y,z) [meters]\n    time : datetime.datetime\n        time of obsevation (UTC)\n    useastropy : bool, optional\n        use AstroPy for conversion\n\n    Results\n    -------\n    x : float\n        target x ECEF coordinate\n    y : float\n        target y ECEF coordinate\n    z : float\n        target z ECEF coordinate\n    \"\"\"\n    useastropy = useastropy and Time\n\n    if useastropy:\n        gst = Time(time).sidereal_time('apparent', 'greenwich').radian\n    else:\n        gst = datetime2sidereal(time, 0.)\n\n    gst = np.atleast_1d(gst)\n    assert gst.ndim == 1 and isinstance(gst[0], float)  # must be in radians!\n\n    eci = np.atleast_2d(eci)\n    assert eci.shape[0] == gst.size, 'length of time does not match number of ECI positions'\n\n    N, trip = eci.shape\n    if eci.ndim > 2 or trip != 3:\n        raise ValueError('eci triplets must be shape (N,3)')\n\n    ecef = np.empty_like(eci)\n\n    for i in range(N):\n        ecef[i, :] = _rottrip(gst[i]) @ eci[i, :]\n\n    return ecef.squeeze()", "response": "Convert an ECI triplet into an ECEF."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _rottrip(ang: np.ndarray) -> np.ndarray:\n    ang = ang.squeeze()\n    if ang.size > 1:\n        raise ValueError('only one angle allowed at a time')\n\n    return np.array([[np.cos(ang), np.sin(ang), 0],\n                     [-np.sin(ang), np.cos(ang), 0],\n                     [0, 0, 1]])", "response": "This function transforms the matrix from radians to radians"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef str2dt(time: datetime) -> np.ndarray:\n    if isinstance(time, datetime):\n        return time\n    elif isinstance(time, str):\n        return parse(time)\n    elif isinstance(time, np.datetime64):\n        return time.astype(datetime)\n    else:  # some sort of iterable\n        try:\n            if isinstance(time[0], datetime):\n                return time\n            elif isinstance(time[0], np.datetime64):\n                return time.astype(datetime)\n            elif isinstance(time[0], str):\n                return [parse(t) for t in time]\n        except (IndexError, TypeError):\n            pass\n\n        # last resort--assume pandas/xarray\n\n        return time.values.astype('datetime64[us]').astype(datetime)", "response": "Converts times in string or list of strings to datetime"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the event that is generated by the DynamoDB differ.", "response": "def handler(event, context):  # pylint: disable=W0613\n    \"\"\"\n    Historical security group event differ.\n\n    Listens to the Historical current table and determines if there are differences that need to be persisted in the\n    historical record.\n    \"\"\"\n    # De-serialize the records:\n    records = deserialize_records(event['Records'])\n\n    for record in records:\n        process_dynamodb_differ_record(record, CurrentVPCModel, DurableVPCModel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(self, account, group, region):\n        return self.dumps({\n            'account': account,\n            'detail': {\n                'request_parameters': {\n                    'groupId': group['GroupId']\n                },\n                'region': region,\n                'collected': group\n            }\n        }).data", "response": "Serializes the JSON for the Polling Event Model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to describe vpc ids.", "response": "def describe_vpc(record):\n    \"\"\"Attempts to describe vpc ids.\"\"\"\n    account_id = record['account']\n    vpc_name = cloudwatch.filter_request_parameters('vpcName', record)\n    vpc_id = cloudwatch.filter_request_parameters('vpcId', record)\n\n    try:\n        if vpc_id and vpc_name:  # pylint: disable=R1705\n            return describe_vpcs(\n                account_number=account_id,\n                assume_role=HISTORICAL_ROLE,\n                region=CURRENT_REGION,\n                Filters=[\n                    {\n                        'Name': 'vpc-id',\n                        'Values': [vpc_id]\n                    }\n                ]\n            )\n        elif vpc_id:\n            return describe_vpcs(\n                account_number=account_id,\n                assume_role=HISTORICAL_ROLE,\n                region=CURRENT_REGION,\n                VpcIds=[vpc_id]\n            )\n        else:\n            raise Exception('[X] Describe requires VpcId.')\n    except ClientError as exc:\n        if exc.response['Error']['Code'] == 'InvalidVpc.NotFound':\n            return []\n        raise exc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a vpc model from a record.", "response": "def create_delete_model(record):\n    \"\"\"Create a vpc model from a record.\"\"\"\n    data = cloudwatch.get_historical_base_info(record)\n\n    vpc_id = cloudwatch.filter_request_parameters('vpcId', record)\n\n    arn = get_arn(vpc_id, cloudwatch.get_region(record), record['account'])\n\n    LOG.debug(F'[-] Deleting Dynamodb Records. Hash Key: {arn}')\n\n    # tombstone these records so that the deletion event time can be accurately tracked.\n    data.update({\n        'configuration': {}\n    })\n\n    items = list(CurrentVPCModel.query(arn, limit=1))\n\n    if items:\n        model_dict = items[0].__dict__['attribute_values'].copy()\n        model_dict.update(data)\n        model = CurrentVPCModel(**model_dict)\n        model.save()\n        return model\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites all of our delete events to DynamoDB.", "response": "def capture_delete_records(records):\n    \"\"\"Writes all of our delete events to DynamoDB.\"\"\"\n    for record in records:\n        model = create_delete_model(record)\n        if model:\n            try:\n                model.delete(condition=(CurrentVPCModel.eventTime <= record['detail']['eventTime']))\n            except DeleteError:\n                LOG.warning(f'[?] Unable to delete VPC. VPC does not exist. Record: {record}')\n        else:\n            LOG.warning(f'[?] Unable to delete VPC. VPC does not exist. Record: {record}')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites all updated configuration info to DynamoDB", "response": "def capture_update_records(records):\n    \"\"\"Writes all updated configuration info to DynamoDB\"\"\"\n    for record in records:\n        data = cloudwatch.get_historical_base_info(record)\n        vpc = describe_vpc(record)\n\n        if len(vpc) > 1:\n            raise Exception(f'[X] Multiple vpcs found. Record: {record}')\n\n        if not vpc:\n            LOG.warning(f'[?] No vpc information found. Record: {record}')\n            continue\n\n        vpc = vpc[0]\n\n        # determine event data for vpc\n        LOG.debug(f'Processing vpc. VPC: {vpc}')\n        data.update({\n            'VpcId': vpc.get('VpcId'),\n            'arn': get_arn(vpc['VpcId'], cloudwatch.get_region(record), data['accountId']),\n            'configuration': vpc,\n            'State': vpc.get('State'),\n            'IsDefault': vpc.get('IsDefault'),\n            'CidrBlock': vpc.get('CidrBlock'),\n            'Name': get_vpc_name(vpc),\n            'Region': cloudwatch.get_region(record),\n            'version': VERSION\n        })\n\n        data['Tags'] = pull_tag_dict(vpc)\n\n        LOG.debug(f'[+] Writing DynamoDB Record. Records: {data}')\n\n        current_revision = CurrentVPCModel(**data)\n        current_revision.save()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handler(event, context):  # pylint: disable=W0613\n    records = deserialize_records(event['Records'])\n\n    # Split records into two groups, update and delete.\n    # We don't want to query for deleted records.\n    update_records, delete_records = group_records_by_type(records, UPDATE_EVENTS)\n    capture_delete_records(delete_records)\n\n    # filter out error events\n    update_records = [e for e in update_records if not e['detail'].get('errorCode')]  # pylint: disable=C0103\n\n    # group records by account for more efficient processing\n    LOG.debug(f'[@] Update Records: {records}')\n\n    capture_update_records(update_records)", "response": "This collector is responsible for processing Cloudwatch events and polling events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining if two revisions have actually changed.", "response": "def default_diff(latest_config, current_config):\n    \"\"\"Determine if two revisions have actually changed.\"\"\"\n    # Pop off the fields we don't care about:\n    pop_no_diff_fields(latest_config, current_config)\n\n    diff = DeepDiff(\n        latest_config,\n        current_config,\n        ignore_order=True\n    )\n    return diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pop_no_diff_fields(latest_config, current_config):\n    for field in ['userIdentity', 'principalId', 'userAgent', 'sourceIpAddress', 'requestParameters', 'eventName']:\n        latest_config.pop(field, None)\n        current_config.pop(field, None)", "response": "Pops off fields that should not be included in the diff."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all fields that belong to the Current table and that don t belong to the Durable table.", "response": "def remove_current_specific_fields(obj):\n    \"\"\"Remove all fields that belong to the Current table -- that don't belong in the Durable table\"\"\"\n    obj = remove_global_dynamo_specific_fields(obj)\n\n    obj.pop(\"ttl\", None)\n    obj.pop(\"eventSource\", None)\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle a DynamoDB MODIFY event type.", "response": "def modify_record(durable_model, current_revision, arn, event_time, diff_func):\n    \"\"\"Handles a DynamoDB MODIFY event type.\"\"\"\n    # We want the newest items first.\n    # See: http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html\n    items = list(durable_model.query(\n        arn,\n        (durable_model.eventTime <= event_time),\n        scan_index_forward=False,\n        limit=1,\n        consistent_read=True))\n\n    if items:\n        latest_revision = items[0]\n\n        latest_config = latest_revision._get_json()[1]['attributes']    # pylint: disable=W0212\n        current_config = current_revision._get_json()[1]['attributes']  # pylint: disable=W0212\n\n        # Determine if there is truly a difference, disregarding Ephemeral Paths\n        diff = diff_func(latest_config, current_config)\n        if diff:\n            LOG.debug(\n                f'[~] Difference found saving new revision to durable table. Arn: {arn} LatestConfig: {latest_config} '\n                f'CurrentConfig: {json.dumps(current_config)}')\n            current_revision.save()\n    else:\n        current_revision.save()\n        LOG.info(f'[?] Got modify event but no current revision found. Arn: {arn}')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles a DynamoDB DELETE event type.", "response": "def delete_differ_record(old_image, durable_model):\n    \"\"\"Handles a DynamoDB DELETE event type -- For the Differ.\"\"\"\n    data = {}\n    for item in old_image:\n        data[item] = DESER.deserialize(old_image[item])\n\n    data['configuration'] = {}\n    # we give our own timestamps for TTL deletions\n    del data['eventTime']\n    durable_model(**data).save()\n    LOG.debug('[+] Adding deletion marker.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_full_current_object(arn, current_model):\n    LOG.debug(f'[-->] Item with ARN: {arn} was too big for SNS -- fetching it from the Current table...')\n    item = list(current_model.query(arn))\n\n    # If for whatever reason, the item *cannot* be found, then this record should be skipped over.\n    # This will happen if this event came in and got processed right after the item was deleted\n    # from the Current table. If so, then do nothing -- the deletion event will be processed later.\n    if not item:\n        return None\n\n    # We need to place the real configuration data into the record so it can be deserialized into\n    # the current model correctly:\n    return item[0]", "response": "Utility method to fetch the full current object from the Current table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deserialize_current_record_to_durable_model(record, current_model, durable_model):\n    # Was the item in question too big for SNS? If so, then we need to fetch the item from the current Dynamo table:\n    if record.get(EVENT_TOO_BIG_FLAG):\n        record = get_full_current_object(record['dynamodb']['Keys']['arn']['S'], current_model)\n\n        if not record:\n            return None\n\n        serialized = record._serialize()  # pylint: disable=W0212\n\n        record = {\n            'dynamodb': {\n                'NewImage': serialized['attributes']\n            }\n        }\n\n        # The ARN isn't added because it's in the HASH key section:\n        record['dynamodb']['NewImage']['arn'] = {'S': serialized['HASH']}\n\n    new_image = remove_current_specific_fields(record['dynamodb']['NewImage'])\n    data = {}\n\n    for item, value in new_image.items():\n        # This could end up as loss of precision\n        data[item] = DESER.deserialize(value)\n\n    return durable_model(**data)", "response": "Utility function that will take a dynamo event record and turn it into a proper pynamo object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deserialize_current_record_to_current_model(record, current_model):\n    # Was the item in question too big for SNS? If so, then we need to fetch the item from the current Dynamo table:\n    if record.get(EVENT_TOO_BIG_FLAG):\n        return get_full_current_object(record['dynamodb']['Keys']['arn']['S'], current_model)\n\n    new_image = remove_global_dynamo_specific_fields(record['dynamodb']['NewImage'])\n    data = {}\n\n    for item, value in new_image.items():\n        # This could end up as loss of precision\n        data[item] = DESER.deserialize(value)\n\n    return current_model(**data)", "response": "Utility function that will take a Dynamo event record and turn it into a proper Current Dynamo object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_dynamodb_differ_record(record, current_model, durable_model, diff_func=None):\n    diff_func = diff_func or default_diff\n\n    # Nothing special needs to be done for deletions as far as items that are too big for SNS are concerned.\n    # This is because the deletion will remove the `configuration` field and save the item without it.\n    if record['eventName'] == 'REMOVE':\n        # We are *ONLY* tracking the deletions from the DynamoDB TTL service.\n        # Why? Because when we process deletion records, we are first saving a new \"empty\" revision to the \"Current\"\n        # table. The \"empty\" revision will then trigger this Lambda as a \"MODIFY\" event. Then, right after it saves\n        # the \"empty\" revision, it will then delete the item from the \"Current\" table. At that point,\n        # we have already saved the \"deletion revision\" to the \"Durable\" table. Thus, no need to process\n        # the deletion events -- except for TTL expirations (which should never happen -- but if they do, you need\n        # to investigate why...)\n        if record.get('userIdentity'):\n            if record['userIdentity']['type'] == 'Service':\n                if record['userIdentity']['principalId'] == 'dynamodb.amazonaws.com':\n                    LOG.error(f\"[TTL] We received a TTL delete. Old Image: {record['dynamodb']['OldImage']}\")\n                    old_image = remove_current_specific_fields(record['dynamodb']['OldImage'])\n                    delete_differ_record(old_image, durable_model)\n\n    if record['eventName'] in ['INSERT', 'MODIFY']:\n        arn = record['dynamodb']['Keys']['arn']['S']\n        current_revision = deserialize_current_record_to_durable_model(record, current_model, durable_model)\n\n        if not current_revision:\n            LOG.error(f'[?] Received item too big for SNS, and was not able to find the original item with ARN: {arn}')\n            return\n\n        if record['eventName'] == 'INSERT':\n            current_revision.save()\n            LOG.debug('[+] Saving new revision to durable table.')\n\n        elif record['eventName'] == 'MODIFY':\n            modify_record(durable_model, current_revision, arn, current_revision.eventTime, diff_func)", "response": "Processes a DynamoDB NewImage record for Differ events."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serialize_me(self, account_id, region, next_token=None):\n        payload = {\n            'account_id': account_id,\n            'region': region\n        }\n\n        if next_token:\n            payload['next_token'] = next_token\n\n        return self.dumps(payload).data", "response": "Dumps the proper JSON for the schema."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndumping the proper JSON for the schema.", "response": "def serialize_me(self, arn, event_time, tech, item=None):\n        \"\"\"Dumps the proper JSON for the schema. If the event is too big, then don't include the item.\n\n        :param arn:\n        :param event_time:\n        :param tech:\n        :param item:\n        :return:\n        \"\"\"\n        payload = {\n            'arn': arn,\n            'event_time': event_time,\n            'tech': tech\n        }\n\n        if item:\n            payload['item'] = item\n\n        else:\n            payload['event_too_big'] = True\n\n        return self.dumps(payload).data.replace('<empty>', '')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the stupid Decimals AttributeNames from the object", "response": "def fix_decimals(obj):\n    \"\"\"Removes the stupid Decimals\n\n    See: https://github.com/boto/boto3/issues/369#issuecomment-302137290\n    \"\"\"\n    if isinstance(obj, list):\n        for i in range(len(obj)):\n            obj[i] = fix_decimals(obj[i])\n        return obj\n\n    elif isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = fix_decimals(value)\n        return obj\n\n    elif isinstance(obj, decimal.Decimal):\n        if obj % 1 == 0:\n            return int(obj)\n        else:\n            return float(obj)\n\n    else:\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a datetime object and returns a string", "response": "def serialize(self, value):\n        \"\"\"Takes a datetime object and returns a string\"\"\"\n        if isinstance(value, str):\n            return value\n        return value.strftime(DATETIME_FORMAT)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pull_tag_dict(data):\n    # If there are tags, set them to a normal dict, vs. a list of dicts:\n    tags = data.pop('Tags', {}) or {}\n    if tags:\n        proper_tags = {}\n        for tag in tags:\n            proper_tags[tag['Key']] = tag['Value']\n\n        tags = proper_tags\n\n    return tags", "response": "This will pull out a list of Tag Name - Value objects and return it as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_delete_model(record):\n    arn = f\"arn:aws:s3:::{cloudwatch.filter_request_parameters('bucketName', record)}\"\n    LOG.debug(f'[-] Deleting Dynamodb Records. Hash Key: {arn}')\n\n    data = {\n        'arn': arn,\n        'principalId': cloudwatch.get_principal(record),\n        'userIdentity': cloudwatch.get_user_identity(record),\n        'accountId': record['account'],\n        'eventTime': record['detail']['eventTime'],\n        'BucketName': cloudwatch.filter_request_parameters('bucketName', record),\n        'Region': cloudwatch.get_region(record),\n        'Tags': {},\n        'configuration': {},\n        'eventSource': record['detail']['eventSource'],\n        'version': VERSION\n    }\n\n    return CurrentS3Model(**data)", "response": "Create an S3 model from a record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_delete_records(delete_records):\n    for rec in delete_records:\n        arn = f\"arn:aws:s3:::{rec['detail']['requestParameters']['bucketName']}\"\n\n        # Need to check if the event is NEWER than the previous event in case\n        # events are out of order. This could *possibly* happen if something\n        # was deleted, and then quickly re-created. It could be *possible* for the\n        # deletion event to arrive after the creation event. Thus, this will check\n        # if the current event timestamp is newer and will only delete if the deletion\n        # event is newer.\n        try:\n            LOG.debug(f'[-] Deleting bucket: {arn}')\n            model = create_delete_model(rec)\n            model.save(condition=(CurrentS3Model.eventTime <= rec['detail']['eventTime']))\n            model.delete()\n\n        except PynamoDBConnectionError as pdce:\n            LOG.warning(f\"[?] Unable to delete bucket: {arn}. Either it doesn't exist, or this deletion event is stale \"\n                        f\"(arrived before a NEWER creation/update). The specific exception is: {pdce}\")", "response": "Process the delete records for S3 bucket deletions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_update_records(update_records):\n    events = sorted(update_records, key=lambda x: x['account'])\n\n    # Group records by account for more efficient processing\n    for account_id, events in groupby(events, lambda x: x['account']):\n        events = list(events)\n\n        # Grab the bucket names (de-dupe events):\n        buckets = {}\n        for event in events:\n            # If the creation date is present, then use it:\n            bucket_event = buckets.get(event['detail']['requestParameters']['bucketName'], {\n                'creationDate': event['detail']['requestParameters'].get('creationDate')\n            })\n            bucket_event.update(event['detail']['requestParameters'])\n\n            buckets[event['detail']['requestParameters']['bucketName']] = bucket_event\n            buckets[event['detail']['requestParameters']['bucketName']]['eventDetails'] = event\n\n        # Query AWS for current configuration\n        for b_name, item in buckets.items():\n            LOG.debug(f'[~] Processing Create/Update for: {b_name}')\n            # If the bucket does not exist, then simply drop the request --\n            # If this happens, there is likely a Delete event that has occurred and will be processed soon.\n            try:\n                bucket_details = get_bucket(b_name,\n                                            account_number=account_id,\n                                            include_created=(item.get('creationDate') is None),\n                                            assume_role=HISTORICAL_ROLE,\n                                            region=CURRENT_REGION)\n                if bucket_details.get('Error'):\n                    LOG.error(f\"[X] Unable to fetch details about bucket: {b_name}. \"\n                              f\"The error details are: {bucket_details['Error']}\")\n                    continue\n\n            except ClientError as cerr:\n                if cerr.response['Error']['Code'] == 'NoSuchBucket':\n                    LOG.warning(f'[?] Received update request for bucket: {b_name} that does not '\n                                'currently exist. Skipping.')\n                    continue\n\n                # Catch Access Denied exceptions as well:\n                if cerr.response['Error']['Code'] == 'AccessDenied':\n                    LOG.error(f'[X] Unable to fetch details for S3 Bucket: {b_name} in {account_id}. Access is Denied. '\n                              'Skipping...')\n                    continue\n                raise Exception(cerr)\n\n            # Pull out the fields we want:\n            data = {\n                'arn': f'arn:aws:s3:::{b_name}',\n                'principalId': cloudwatch.get_principal(item['eventDetails']),\n                'userIdentity': cloudwatch.get_user_identity(item['eventDetails']),\n                'userAgent': item['eventDetails']['detail'].get('userAgent'),\n                'sourceIpAddress': item['eventDetails']['detail'].get('sourceIPAddress'),\n                'requestParameters': item['eventDetails']['detail'].get('requestParameters'),\n                'accountId': account_id,\n                'eventTime': item['eventDetails']['detail']['eventTime'],\n                'BucketName': b_name,\n                'Region': bucket_details.pop('Region'),\n                # Duplicated in top level and configuration for secondary index\n                'Tags': bucket_details.pop('Tags', {}) or {},\n                'eventSource': item['eventDetails']['detail']['eventSource'],\n                'eventName': item['eventDetails']['detail']['eventName'],\n                'version': VERSION\n            }\n\n            # Remove the fields we don't care about:\n            del bucket_details['Arn']\n            del bucket_details['GrantReferences']\n            del bucket_details['_version']\n            del bucket_details['Name']\n\n            if not bucket_details.get('CreationDate'):\n                bucket_details['CreationDate'] = item['creationDate']\n\n            data['configuration'] = bucket_details\n\n            current_revision = CurrentS3Model(**data)\n            current_revision.save()", "response": "Process the update records for S3 bucket updates."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handler(event, context):  # pylint: disable=W0613\n    records = deserialize_records(event['Records'])\n\n    # Split records into two groups, update and delete.\n    # We don't want to query for deleted records.\n    update_records, delete_records = group_records_by_type(records, UPDATE_EVENTS)\n\n    LOG.debug('[@] Processing update records...')\n    process_update_records(update_records)\n    LOG.debug('[@] Completed processing of update records.')\n\n    LOG.debug('[@] Processing delete records...')\n    process_delete_records(delete_records)\n    LOG.debug('[@] Completed processing of delete records.')\n\n    LOG.debug('[@] Successfully updated current Historical table')", "response": "This collector is responsible for processing CloudWatch events and polling events."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handler(event, context):\n    log.debug('Running poller. Configuration: {}'.format(event))\n\n    for account in get_historical_accounts():\n        try:\n            # TODO describe all items\n            # Example::\n            #\n            # groups = describe_security_groups(\n            #     account_number=account['id'],\n            #     assume_role=HISTORICAL_ROLE,\n            #     region=CURRENT_REGION\n            # )\n            # events = [security_group_polling_schema.serialize(account['id'], g) for g in groups['SecurityGroups']]\n            events = []\n            produce_events(events, os.environ.get('HISTORICAL_STREAM', 'Historical{{cookiecutter.technology_slug | titlecase }}PollerStream'))\n            log.debug('Finished generating polling events. Account: {} Events Created: {}'.format(account['id'], len(events)))\n        except ClientError as e:\n            log.warning('Unable to generate events for account. AccountId: {account_id} Reason: {reason}'.format(\n                account_id=account['id'],\n                reason=e\n            ))", "response": "This function is called by the event poller when the event is generated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching valid accounts from SWAG if enabled or a list accounts.", "response": "def get_historical_accounts():\n    \"\"\"Fetches valid accounts from SWAG if enabled or a list accounts.\"\"\"\n    if os.environ.get('SWAG_BUCKET', False):\n        swag_opts = {\n            'swag.type': 's3',\n            'swag.bucket_name': os.environ['SWAG_BUCKET'],\n            'swag.data_file': os.environ.get('SWAG_DATA_FILE', 'accounts.json'),\n            'swag.region': os.environ.get('SWAG_REGION', 'us-east-1')\n        }\n        swag = SWAGManager(**parse_swag_config_options(swag_opts))\n        search_filter = f\"[?provider=='aws' && owner=='{os.environ['SWAG_OWNER']}' && account_status!='deleted'\"\n\n        if parse_boolean(os.environ.get('TEST_ACCOUNTS_ONLY')):\n            search_filter += \" && environment=='test'\"\n\n        search_filter += ']'\n\n        accounts = swag.get_service_enabled('historical', search_filter=search_filter)\n    else:\n        accounts = [{'id': account_id} for account_id in os.environ['ENABLED_ACCOUNTS'].split(',')]\n\n    return accounts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef poller_processor_handler(event, context):  # pylint: disable=W0613\n    LOG.debug('[@] Running Poller...')\n\n    queue_url = get_queue_url(os.environ.get('POLLER_QUEUE_NAME', 'HistoricalS3Poller'))\n\n    records = deserialize_records(event['Records'])\n\n    for record in records:\n        # Skip accounts that have role assumption errors:\n        try:\n            # List all buckets in the account:\n            all_buckets = list_buckets(account_number=record['account_id'],\n                                       assume_role=HISTORICAL_ROLE,\n                                       session_name=\"historical-cloudwatch-s3list\",\n                                       region=record['region'])[\"Buckets\"]\n\n            events = [S3_POLLING_SCHEMA.serialize_me(record['account_id'], bucket) for bucket in all_buckets]\n            produce_events(events, queue_url, randomize_delay=RANDOMIZE_POLLER)\n        except ClientError as exc:\n            LOG.error(f\"[X] Unable to generate events for account. Account Id: {record['account_id']} Reason: {exc}\")\n\n        LOG.debug(f\"[@] Finished generating polling events for account: {record['account_id']}. Events Created:\"\n                  f\" {len(record['account_id'])}\")", "response": "This is the main entry point for the Historical S3 Poller Tasker. It will generate the events for all objects in an anonymized technology."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshrinking a blob to be sent to SNS or SNS", "response": "def shrink_blob(record, deletion):\n    \"\"\"\n    Makes a shrunken blob to be sent to SNS/SQS (due to the 256KB size limitations of SNS/SQS messages).\n    This will essentially remove the \"configuration\" field such that the size of the SNS/SQS message remains under\n    256KB.\n    :param record:\n    :return:\n    \"\"\"\n    item = {\n        \"eventName\": record[\"eventName\"],\n        EVENT_TOO_BIG_FLAG: (not deletion)\n    }\n\n    # To handle TTLs (if they happen)\n    if record.get(\"userIdentity\"):\n        item[\"userIdentity\"] = record[\"userIdentity\"]\n\n    # Remove the 'configuration' and 'requestParameters' fields from new and old images if applicable:\n    if not deletion:\n        # Only remove it from non-deletions:\n        if record['dynamodb'].get('NewImage'):\n            record['dynamodb']['NewImage'].pop('configuration', None)\n            record['dynamodb']['NewImage'].pop('requestParameters', None)\n\n    if record['dynamodb'].get('OldImage'):\n        record['dynamodb']['OldImage'].pop('configuration', None)\n        record['dynamodb']['OldImage'].pop('requestParameters', None)\n\n    item['dynamodb'] = record['dynamodb']\n\n    return item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handler(event, context):  # pylint: disable=W0613\n    queue_url = os.environ.get('PROXY_QUEUE_URL')\n    topic_arn = os.environ.get('PROXY_TOPIC_ARN')\n\n    if not queue_url and not topic_arn:\n        raise MissingProxyConfigurationException('[X] Must set the `PROXY_QUEUE_URL` or the `PROXY_TOPIC_ARN` vars.')\n\n    items_to_ship = []\n\n    # Must ALWAYS shrink for SQS because of 256KB limit of sending batched messages\n    force_shrink = True if queue_url else False\n\n    # Is this a \"Simple Durable Proxy\" -- that is -- are we stripping out all of the DynamoDB data from\n    # the Differ?\n    record_maker = make_proper_simple_record if SIMPLE_DURABLE_PROXY else make_proper_dynamodb_record\n\n    for record in event['Records']:\n        # We should NOT be processing this if the item in question does not\n        # reside in the PROXY_REGIONS\n        correct_region = True\n        for img in ['NewImage', 'OldImage']:\n            if record['dynamodb'].get(img):\n                if record['dynamodb'][img][REGION_ATTR]['S'] not in PROXY_REGIONS:\n                    LOG.debug(f\"[/] Not processing record -- record event took place in:\"\n                              f\" {record['dynamodb'][img][REGION_ATTR]['S']}\")\n                    correct_region = False\n                    break\n\n        if not correct_region:\n            continue\n\n        # Global DynamoDB tables will update a record with the global table specific fields. This creates 2 events\n        # whenever there is an update. The second update, which is a MODIFY event is not relevant and noise. This\n        # needs to be skipped over to prevent duplicated events. This is a \"gotcha\" in Global DynamoDB tables.\n        if detect_global_table_updates(record):\n            continue\n\n        items_to_ship.append(record_maker(record, force_shrink=force_shrink))\n\n    if items_to_ship:\n        # SQS:\n        if queue_url:\n            produce_events(items_to_ship, queue_url, batch_size=int(os.environ.get('PROXY_BATCH_SIZE', 10)))\n\n        # SNS:\n        else:\n            client = boto3.client(\"sns\", region_name=CURRENT_REGION)\n            for i in items_to_ship:\n                _publish_sns_message(client, i, topic_arn)", "response": "This function is called when the event is received from the DynamoDB stream. It will create a new record and add it to the Differ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare and ships an individual DynamoDB record over to SNS or SQS for future processing.", "response": "def make_proper_dynamodb_record(record, force_shrink=False):\n    \"\"\"Prepares and ships an individual DynamoDB record over to SNS/SQS for future processing.\n\n    :param record:\n    :param force_shrink:\n    :return:\n    \"\"\"\n    # Get the initial blob and determine if it is too big for SNS/SQS:\n    blob = json.dumps(record)\n    size = math.ceil(sys.getsizeof(blob) / 1024)\n\n    # If it is too big, then we need to send over a smaller blob to inform the recipient that it needs to go out and\n    # fetch the item from the Historical table!\n    if size >= 200 or force_shrink:\n        deletion = False\n        # ^^ However -- deletions need to be handled differently, because the Differ won't be able to find a\n        # deleted record. For deletions, we will only shrink the 'OldImage', but preserve the 'NewImage' since that is\n        # \"already\" shrunken.\n        if record['dynamodb'].get('NewImage'):\n            # Config will be empty if there was a deletion:\n            if not (record['dynamodb']['NewImage'].get('configuration', {}) or {}).get('M'):\n                deletion = True\n\n        blob = json.dumps(shrink_blob(record, deletion))\n\n    return blob"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing and ships an individual simplified durable table record over to SNS or SQS for future processing.", "response": "def make_proper_simple_record(record, force_shrink=False):\n    \"\"\"Prepares and ships an individual simplified durable table record over to SNS/SQS for future processing.\n\n    :param record:\n    :param force_shrink:\n    :return:\n    \"\"\"\n    # Convert to a simple object\n    item = {\n        'arn': record['dynamodb']['Keys']['arn']['S'],\n        'event_time': record['dynamodb']['NewImage']['eventTime']['S'],\n        'tech': HISTORICAL_TECHNOLOGY\n    }\n\n    # We need to de-serialize the raw DynamoDB object into the proper PynamoDB obj:\n    prepped_new_record = _get_durable_pynamo_obj(record['dynamodb']['NewImage'],\n                                                 DURABLE_MAPPING.get(HISTORICAL_TECHNOLOGY))\n\n    item['item'] = dict(prepped_new_record)\n\n    # Get the initial blob and determine if it is too big for SNS/SQS:\n    blob = json.dumps(item)\n    size = math.ceil(sys.getsizeof(blob) / 1024)\n\n    # If it is too big, then we need to send over a smaller blob to inform the recipient that it needs to go out and\n    # fetch the item from the Historical table!\n    if size >= 200 or force_shrink:\n        del item['item']\n\n        item[EVENT_TOO_BIG_FLAG] = True\n\n        blob = json.dumps(item)\n\n    return blob.replace('<empty>', '')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handler(event, context):  # pylint: disable=W0613\n    # De-serialize the records:\n    records = deserialize_records(event['Records'])\n\n    for record in records:\n        process_dynamodb_differ_record(record, CurrentS3Model, DurableS3Model)", "response": "Handle the event that is triggered by the DynamoDB DynamoDB"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new historical technology.", "response": "def new():\n    \"\"\"Creates a new historical technology.\"\"\"\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    cookiecutter(os.path.join(dir_path, 'historical-cookiecutter/'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef poller_processor_handler(event, context):  # pylint: disable=W0613\n    LOG.debug('[@] Running Poller...')\n\n    queue_url = get_queue_url(os.environ.get('POLLER_QUEUE_NAME', 'HistoricalVPCPoller'))\n\n    records = deserialize_records(event['Records'])\n\n    for record in records:\n        # Skip accounts that have role assumption errors:\n        try:\n            vpcs = describe_vpcs(\n                account_number=record['account_id'],\n                assume_role=HISTORICAL_ROLE,\n                region=record['region']\n            )\n\n            events = [VPC_POLLING_SCHEMA.serialize(record['account_id'], v) for v in vpcs]\n            produce_events(events, queue_url, randomize_delay=RANDOMIZE_POLLER)\n            LOG.debug(f\"[@] Finished generating polling events. Account: {record['account_id']}/{record['region']} \"\n                      f\"Events Created: {len(events)}\")\n        except ClientError as exc:\n            LOG.error(f\"[X] Unable to generate events for account/region. Account Id/Region: {record['account_id']}\"\n                      f\"/{record['region']} Reason: {exc}\")", "response": "This is the main entry point for the Historical Security Group Poller Processor. It generates the polling events for all the accounts in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield successive n - sized chunks from the event list.", "response": "def chunks(event_list, chunk_size):\n    \"\"\"Yield successive n-sized chunks from the event list.\"\"\"\n    for i in range(0, len(event_list), chunk_size):\n        yield event_list[i:i + chunk_size]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the URL of the SQS queue to send events to.", "response": "def get_queue_url(queue_name):\n    \"\"\"Get the URL of the SQS queue to send events to.\"\"\"\n    client = boto3.client(\"sqs\", CURRENT_REGION)\n    queue = client.get_queue_url(QueueName=queue_name)\n\n    return queue[\"QueueUrl\"]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef produce_events(events, queue_url, batch_size=10, randomize_delay=0):\n    client = boto3.client('sqs', region_name=CURRENT_REGION)\n\n    for chunk in chunks(events, batch_size):\n        records = [make_sqs_record(event, delay_seconds=get_random_delay(randomize_delay)) for event in chunk]\n\n        client.send_message_batch(Entries=records, QueueUrl=queue_url)", "response": "Efficiently sends events to the SQS event queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbreaks records into two lists ; create update events and delete events.", "response": "def group_records_by_type(records, update_events):\n    \"\"\"Break records into two lists; create/update events and delete events.\n\n    :param records:\n    :param update_events:\n    :return update_records, delete_records:\n    \"\"\"\n    update_records, delete_records = [], []\n    for record in records:\n        if record.get(\"detail-type\", \"\") == \"Scheduled Event\":\n            LOG.error(\"[X] Received a Scheduled Event in the Queue... Please check that your environment is set up\"\n                      \" correctly.\")\n            continue\n\n        # Ignore SQS junk messages (like subscription notices and things):\n        if not record.get(\"detail\"):\n            continue\n\n        # Do not capture error events:\n        if not record[\"detail\"].get(\"errorCode\"):\n            if record['detail']['eventName'] in update_events:\n                update_records.append(record)\n            else:\n                delete_records.append(record)\n\n    return update_records, delete_records"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the Event of Durable Security Group Event differ.", "response": "def handler(event, context):  # pylint: disable=W0613\n    \"\"\"\n    Historical security group event differ.\n\n    Listens to the Historical current table and determines if there are differences that need to be persisted in the\n    historical record.\n    \"\"\"\n    # De-serialize the records:\n    records = deserialize_records(event['Records'])\n\n    for record in records:\n        process_dynamodb_differ_record(record, CurrentSecurityGroupModel, DurableSecurityGroupModel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the Event of Durable Table Event differ.", "response": "def handler(event, context):\n    \"\"\"\n    Historical security group event differ.\n\n    Listens to the Historical current table and determines if there are differences that need to be persisted in the\n    historical record.\n    \"\"\"\n    for record in event['Records']:\n        process_dynamodb_differ_record(record, Durable{{cookiecutter.technology_slug | titlecase}}Model)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef group_records_by_type(records):\n    update_records, delete_records = [], []\n    for r in records:\n        if isinstance(r, str):\n            break\n\n        if r['detail']['eventName'] in UPDATE_EVENTS:\n            update_records.append(r)\n        else:\n            delete_records.append(r)\n    return update_records, delete_records", "response": "Break records into two lists ; create update events and delete events."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new model from a record.", "response": "def create_delete_model(record):\n    \"\"\"Create a {{cookiecutter.technology_name}} model from a record.\"\"\"\n    data = cloudwatch.get_historical_base_info(record)\n\n    # TODO get tech ID\n    # Example::\n    #    group_id = cloudwatch.filter_request_parameters('groupId', record)\n    #    vpc_id = cloudwatch.filter_request_parameters('vpcId', record)\n    #    group_name = cloudwatch.filter_request_parameters('groupName', record)\n\n    tech_id = None\n    arn = get_arn(tech_id, record['account'])\n\n    log.debug('Deleting Dynamodb Records. Hash Key: {arn}'.format(arn=arn))\n\n    # tombstone these records so that the deletion event time can be accurately tracked.\n    data.update({\n        'configuration': {}\n    })\n\n    items = list(Current{{cookiecutter.technology_slug | titlecase}}Model.query(arn, limit=1))\n\n    if items:\n        model_dict = items[0].__dict__['attribute_values'].copy()\n        model_dict.update(data)\n        model = Current{{cookiecutter.technology_slug | titlecase }}Model(**model_dict)\n        model.save()\n        return model"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef capture_delete_records(records):\n    for r in records:\n        model = create_delete_model(r)\n        if model:\n            try:\n                model.delete(eventTime__le=r['detail']['eventTime'])\n            except DeleteError as e:\n                log.warning('Unable to delete {{cookiecutter.technology_name}}. {{cookiecutter.technology_name}} does not exist. Record: {record}'.format(\n                    record=r\n                ))\n        else:\n            log.warning('Unable to delete {{cookiecutter.technology_name}}. {{cookiecutter.technology_name}} does not exist. Record: {record}'.format(\n                record=r\n            ))", "response": "Write all of our delete events to DynamoDB."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef capture_update_records(records):\n    for record in records:\n        data = cloudwatch.get_historical_base_info(record)\n        items = describe_technology(record)\n\n        if len(items) > 1:\n            raise Exception('Multiple items found. Record: {record}'.format(record=record))\n\n        if not items:\n            log.warning('No technology information found. Record: {record}'.format(record=record))\n            continue\n\n        item = items[0]\n\n        # determine event data for group\n        log.debug('Processing item. Group: {}'.format(item))\n\n        # TODO update data\n        # Example::\n        # data.update({\n        #     'GroupId': item['GroupId'],\n        #     'GroupName': item['GroupName'],\n        #     'Description': item['Description'],\n        #     'VpcId': item.get('VpcId'),\n        #     'Tags': item.get('Tags', []),\n        #     'arn': get_arn(item['GroupId'], item['OwnerId']),\n        #     'OwnerId': item['OwnerId'],\n        #     'configuration': item,\n        #     'Region': cloudwatch.get_region(record)\n        # })\n\n        log.debug('Writing Dynamodb Record. Records: {record}'.format(record=data))\n\n        current_revision = Current{{cookiecutter.technology_slug | titlecase}}Model(**data)\n        current_revision.save()", "response": "Capture all updated configuration info for a list of records and write them to DynamoDB"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef describe_group(record, region):\n    account_id = record['account']\n    group_name = cloudwatch.filter_request_parameters('groupName', record)\n    vpc_id = cloudwatch.filter_request_parameters('vpcId', record)\n    group_id = cloudwatch.filter_request_parameters('groupId', record, look_in_response=True)\n\n    # Did this get collected already by the poller?\n    if cloudwatch.get_collected_details(record):\n        LOG.debug(f\"[<--] Received already collected security group data: {record['detail']['collected']}\")\n        return [record['detail']['collected']]\n\n    try:\n        # Always depend on Group ID first:\n        if group_id:  # pylint: disable=R1705\n            return describe_security_groups(\n                account_number=account_id,\n                assume_role=HISTORICAL_ROLE,\n                region=region,\n                GroupIds=[group_id]\n            )['SecurityGroups']\n\n        elif vpc_id and group_name:\n            return describe_security_groups(\n                account_number=account_id,\n                assume_role=HISTORICAL_ROLE,\n                region=region,\n                Filters=[\n                    {\n                        'Name': 'group-name',\n                        'Values': [group_name]\n                    },\n                    {\n                        'Name': 'vpc-id',\n                        'Values': [vpc_id]\n                    }\n                ]\n            )['SecurityGroups']\n\n        else:\n            raise Exception('[X] Did not receive Group ID or VPC/Group Name pairs. '\n                            f'We got: ID: {group_id} VPC/Name: {vpc_id}/{group_name}.')\n    except ClientError as exc:\n        if exc.response['Error']['Code'] == 'InvalidGroup.NotFound':\n            return []\n        raise exc", "response": "Attempts to describe group ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_delete_model(record):\n    data = cloudwatch.get_historical_base_info(record)\n\n    group_id = cloudwatch.filter_request_parameters('groupId', record)\n    # vpc_id = cloudwatch.filter_request_parameters('vpcId', record)\n    # group_name = cloudwatch.filter_request_parameters('groupName', record)\n\n    arn = get_arn(group_id, cloudwatch.get_region(record), record['account'])\n\n    LOG.debug(f'[-] Deleting Dynamodb Records. Hash Key: {arn}')\n\n    # Tombstone these records so that the deletion event time can be accurately tracked.\n    data.update({'configuration': {}})\n\n    items = list(CurrentSecurityGroupModel.query(arn, limit=1))\n\n    if items:\n        model_dict = items[0].__dict__['attribute_values'].copy()\n        model_dict.update(data)\n        model = CurrentSecurityGroupModel(**model_dict)\n        model.save()\n        return model\n\n    return None", "response": "Create a security group model from a record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef capture_delete_records(records):\n    for rec in records:\n        model = create_delete_model(rec)\n        if model:\n            try:\n                model.delete(condition=(CurrentSecurityGroupModel.eventTime <= rec['detail']['eventTime']))\n            except DeleteError:\n                LOG.warning(f'[X] Unable to delete security group. Security group does not exist. Record: {rec}')\n        else:\n            LOG.warning(f'[?] Unable to delete security group. Security group does not exist. Record: {rec}')", "response": "Write all of our delete events to DynamoDB."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef capture_update_records(records):\n    for rec in records:\n        data = cloudwatch.get_historical_base_info(rec)\n        group = describe_group(rec, cloudwatch.get_region(rec))\n\n        if len(group) > 1:\n            raise Exception(f'[X] Multiple groups found. Record: {rec}')\n\n        if not group:\n            LOG.warning(f'[?] No group information found. Record: {rec}')\n            continue\n\n        group = group[0]\n\n        # Determine event data for group - and pop off items that are going to the top-level:\n        LOG.debug(f'Processing group. Group: {group}')\n        data.update({\n            'GroupId': group['GroupId'],\n            'GroupName': group.pop('GroupName'),\n            'VpcId': group.pop('VpcId', None),\n            'arn': get_arn(group.pop('GroupId'), cloudwatch.get_region(rec), group.pop('OwnerId')),\n            'Region': cloudwatch.get_region(rec)\n        })\n\n        data['Tags'] = pull_tag_dict(group)\n\n        # Set the remaining items to the configuration:\n        data['configuration'] = group\n\n        # Set the version:\n        data['version'] = VERSION\n\n        LOG.debug(f'[+] Writing Dynamodb Record. Records: {data}')\n        current_revision = CurrentSecurityGroupModel(**data)\n        current_revision.save()", "response": "Write all updated configuration info to DynamoDB"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serialize_me(self, account, bucket_details):\n        return self.dumps({\n            \"account\": account,\n            \"detail\": {\n                \"request_parameters\": {\n                    \"bucket_name\": bucket_details[\"Name\"],\n                    \"creation_date\": bucket_details[\"CreationDate\"].replace(\n                        tzinfo=None, microsecond=0).isoformat() + \"Z\"\n                }\n            }\n        }).data", "response": "Serializes the JSON for the Polling Event Model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the log level from the environment variable.", "response": "def extract_log_level_from_environment(k, default):\n    \"\"\"Gets the log level from the environment variable.\"\"\"\n    return LOG_LEVELS.get(os.environ.get(k)) or int(os.environ.get(k, default))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_request_parameters(field_name, msg, look_in_response=False):\n    val = msg['detail'].get(field_name, None)\n    try:\n        if not val:\n            val = msg['detail'].get('requestParameters', {}).get(field_name, None)\n\n        # If we STILL didn't find it -- check if it's in the response element (default off)\n        if not val and look_in_response:\n            if msg['detail'].get('responseElements'):\n                val = msg['detail']['responseElements'].get(field_name, None)\n\n    # Just in case... We didn't find the value, so just make it None:\n    except AttributeError:\n        val = None\n\n    return val", "response": "Filter the request parameters from an event message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_historical_base_info(event):\n    data = {\n        'principalId': get_principal(event),\n        'userIdentity': get_user_identity(event),\n        'accountId': event['account'],\n        'userAgent': event['detail'].get('userAgent'),\n        'sourceIpAddress': event['detail'].get('sourceIPAddress'),\n        'requestParameters': event['detail'].get('requestParameters')\n    }\n\n    if event['detail'].get('eventTime'):\n        data['eventTime'] = event['detail']['eventTime']\n\n    if event['detail'].get('eventSource'):\n        data['eventSource'] = event['detail']['eventSource']\n\n    if event['detail'].get('eventName'):\n        data['eventName'] = event['detail']['eventName']\n\n    return data", "response": "Gets the base details from the CloudWatch Event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findLastCharIndexMatching(text, func):\n    for i in range(len(text) - 1, -1, -1):\n      if func(text[i]):\n        return i", "response": "Find the index of the last character in the string for which func evaluates to True."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit text into sub segments of size not bigger than MAX_SEGMENT_SIZE.", "response": "def splitText(text):\n    \"\"\" Split text into sub segments of size not bigger than MAX_SEGMENT_SIZE. \"\"\"\n    segments = []\n    remaining_text = __class__.cleanSpaces(text)\n\n    while len(remaining_text) > __class__.MAX_SEGMENT_SIZE:\n      cur_text = remaining_text[:__class__.MAX_SEGMENT_SIZE]\n\n      # try to split at punctuation\n      split_idx = __class__.findLastCharIndexMatching(cur_text,\n                                                      # https://en.wikipedia.org/wiki/Unicode_character_property#General_Category\n                                                      lambda x: unicodedata.category(x) in (\"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"))\n      if split_idx is None:\n        # try to split at whitespace\n        split_idx = __class__.findLastCharIndexMatching(cur_text,\n                                                        lambda x: unicodedata.category(x).startswith(\"Z\"))\n      if split_idx is None:\n        # try to split at anything not a letter or number\n        split_idx = __class__.findLastCharIndexMatching(cur_text,\n                                                        lambda x: not (unicodedata.category(x)[0] in (\"L\", \"N\")))\n      if split_idx is None:\n        # split at the last char\n        split_idx = __class__.MAX_SEGMENT_SIZE - 1\n\n      new_segment = cur_text[:split_idx + 1].rstrip()\n      segments.append(new_segment)\n      remaining_text = remaining_text[split_idx + 1:].lstrip(string.whitespace + string.punctuation)\n\n    if remaining_text:\n      segments.append(remaining_text)\n\n    return segments"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if audio data for this segment is present in cache False otherwise.", "response": "def isInCache(self):\n    \"\"\" Return True if audio data for this segment is present in cache, False otherwise. \"\"\"\n    url = self.buildUrl(cache_friendly=True)\n    return url in __class__.cache"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the audio data for fast playback.", "response": "def preLoad(self):\n    \"\"\" Store audio data in cache for fast playback. \"\"\"\n    logging.getLogger().debug(\"Preloading segment '%s'\" % (self))\n    real_url = self.buildUrl()\n    cache_url = self.buildUrl(cache_friendly=True)\n    audio_data = self.download(real_url)\n    assert(audio_data)\n    __class__.cache[cache_url] = audio_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching the audio data from the audio archive.", "response": "def getAudioData(self):\n    \"\"\" Fetch the audio data. \"\"\"\n    with self.preload_mutex:\n      cache_url = self.buildUrl(cache_friendly=True)\n      if cache_url in __class__.cache:\n        logging.getLogger().debug(\"Got data for URL '%s' from cache\" % (cache_url))\n        audio_data = __class__.cache[cache_url]\n        assert(audio_data)\n      else:\n        real_url = self.buildUrl()\n        audio_data = self.download(real_url)\n        assert(audio_data)\n        __class__.cache[cache_url] = audio_data\n    return audio_data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef buildUrl(self, cache_friendly=False):\n    params = collections.OrderedDict()\n    params[\"client\"] = \"tw-ob\"\n    params[\"ie\"] = \"UTF-8\"\n    params[\"idx\"] = str(self.segment_num)\n    if self.segment_count is not None:\n      params[\"total\"] = str(self.segment_count)\n    params[\"textlen\"] = str(len(self.text))\n    params[\"tl\"] = self.lang\n    lower_text = self.text.lower()\n    params[\"q\"] = lower_text\n    return \"%s?%s\" % (__class__.BASE_URL, urllib.parse.urlencode(params))", "response": "Construct the URL to get the sound from Goggle API."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download(self, url):\n    logging.getLogger().debug(\"Downloading '%s'...\" % (url))\n    response = __class__.session.get(url,\n                                     headers={\"User-Agent\": \"Mozilla/5.0\"},\n                                     timeout=3.1)\n    response.raise_for_status()\n    return response.content", "response": "Download a sound file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_param_dict(self):\r\n        param_dict = {}\r\n        for index, dictionary in enumerate(self.value):\r\n            for key, value in dictionary.items():\r\n                param_name = '{param_name}[{index}][{key}]'.format(\r\n                                                    param_name=self.param_name,\r\n                                                    index=index,\r\n                                                    key=key)\r\n                param_dict[param_name] = value\r\n        return OrderedDict(sorted(param_dict.items()))", "response": "Returns a dict of parameters that can be used to create a new parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _discover_params(cls):\r\n\r\n        try:\r\n            return cls.filters\r\n        except AttributeError:\r\n            filters = {}\r\n            for param_class_name in dir(cls):\r\n                param_class = getattr(cls, param_class_name)\r\n                if hasattr(param_class, 'kwarg'):\r\n                    filters[param_class.kwarg] = param_class\r\n                    filters[param_class.param_name] = param_class\r\n            cls.filters = filters\r\n        return cls.filters", "response": "Discovers the parameters of the given class and returns a dict where the key is the key and the value is the value of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Param Class Instance by its kwarg or param name", "response": "def _get(cls, kwarg_name):\r\n        \"\"\" Returns a Param Class Instance, by its kwarg or param name \"\"\"\r\n        param_classes = cls._discover_params()\r\n        try:\r\n            param_class = param_classes[kwarg_name]\r\n        except KeyError:\r\n            raise ValueError('invalid param keyword {}'.format(kwarg_name))\r\n        else:\r\n            return param_class"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_params(self, params):\r\n        new_params = OrderedDict()\r\n        for param_name, param_value in sorted(params.items()):\r\n            param_value = params[param_name]\r\n            ParamClass = AirtableParams._get(param_name)\r\n            new_params.update(ParamClass(param_value).to_param_dict())\r\n        return new_params", "response": "Process the params as needed using filters\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, record_id):\r\n        record_url = self.record_url(record_id)\r\n        return self._get(record_url)", "response": "Retrieves a record by its id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an iterator that returns records from the given airtable.", "response": "def get_iter(self, **options):\r\n        \"\"\"\r\n        Record Retriever Iterator\r\n\r\n        Returns iterator with lists in batches according to pageSize.\r\n        To get all records at once use :any:`get_all`\r\n\r\n        >>> for page in airtable.get_iter():\r\n        ...     for record in page:\r\n        ...         print(record)\r\n        [{'fields': ... }, ...]\r\n\r\n        Keyword Args:\r\n            max_records (``int``, optional): The maximum total number of\r\n                records that will be returned. See :any:`MaxRecordsParam`\r\n            view (``str``, optional): The name or ID of a view.\r\n                See :any:`ViewParam`.\r\n            page_size (``int``, optional ): The number of records returned\r\n                in each request. Must be less than or equal to 100.\r\n                Default is 100. See :any:`PageSizeParam`.\r\n            fields (``str``, ``list``, optional): Name of field or fields to\r\n                be retrieved. Default is all fields. See :any:`FieldsParam`.\r\n            sort (``list``, optional): List of fields to sort by.\r\n                Default order is ascending. See :any:`SortParam`.\r\n            formula (``str``, optional): Airtable formula.\r\n                See :any:`FormulaParam`.\r\n\r\n        Returns:\r\n            iterator (``list``): List of Records, grouped by pageSize\r\n\r\n        \"\"\"\r\n        offset = None\r\n        while True:\r\n            data = self._get(self.url_table, offset=offset, **options)\r\n            records = data.get('records', [])\r\n            time.sleep(self.API_LIMIT)\r\n            yield records\r\n            offset = data.get('offset')\r\n            if not offset:\r\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all(self, **options):\r\n        all_records = []\r\n        for records in self.get_iter(**options):\r\n            all_records.extend(records)\r\n        return all_records", "response": "Retrieves all records repetitively and returns a single list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn first match found in get_all and returns the first record that matches the field_value", "response": "def match(self, field_name, field_value, **options):\r\n        \"\"\"\r\n        Returns first match found in :any:`get_all`\r\n\r\n        >>> airtable.match('Name', 'John')\r\n        {'fields': {'Name': 'John'} }\r\n\r\n        Args:\r\n            field_name (``str``): Name of field to match (column name).\r\n            field_value (``str``): Value of field to match.\r\n\r\n        Keyword Args:\r\n            max_records (``int``, optional): The maximum total number of\r\n                records that will be returned. See :any:`MaxRecordsParam`\r\n            view (``str``, optional): The name or ID of a view.\r\n                See :any:`ViewParam`.\r\n            fields (``str``, ``list``, optional): Name of field or fields to\r\n                be retrieved. Default is all fields. See :any:`FieldsParam`.\r\n            sort (``list``, optional): List of fields to sort by.\r\n                Default order is ascending. See :any:`SortParam`.\r\n\r\n        Returns:\r\n            record (``dict``): First record to match the field_value provided\r\n        \"\"\"\r\n        from_name_and_value = AirtableParams.FormulaParam.from_name_and_value\r\n        formula = from_name_and_value(field_name, field_value)\r\n        options['formula'] = formula\r\n        for record in self.get_all(**options):\r\n            return record\r\n        else:\r\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search(self, field_name, field_value, record=None, **options):\r\n        records = []\r\n        from_name_and_value = AirtableParams.FormulaParam.from_name_and_value\r\n        formula = from_name_and_value(field_name, field_value)\r\n        options['formula'] = formula\r\n        records = self.get_all(**options)\r\n        return records", "response": "Returns all matching records in the Airtable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert(self, fields, typecast=False):\r\n        return self._post(self.url_table, json_data={\"fields\": fields, \"typecast\": typecast})", "response": "Insert a record into the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _batch_request(self, func, iterable):\r\n        responses = []\r\n        for item in iterable:\r\n            responses.append(func(item))\r\n            time.sleep(self.API_LIMIT)\r\n        return responses", "response": "Internal function to limit batch calls to API limit"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batch_insert(self, records, typecast=False):\r\n        return self._batch_request(self.insert, records)", "response": "Calls the airtable. insert method repetitively"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates a record by its record id.", "response": "def update(self, record_id, fields, typecast=False):\r\n        \"\"\"\r\n        Updates a record by its record id.\r\n        Only Fields passed are updated, the rest are left as is.\r\n\r\n        >>> record = airtable.match('Employee Id', 'DD13332454')\r\n        >>> fields = {'Status': 'Fired'}\r\n        >>> airtable.update(record['id'], fields)\r\n\r\n        Args:\r\n            record_id(``str``): Id of Record to update\r\n            fields(``dict``): Fields to update.\r\n                Must be dictionary with Column names as Key\r\n            typecast(``boolean``): Automatic data conversion from string values.\r\n\r\n        Returns:\r\n            record (``dict``): Updated record\r\n        \"\"\"\r\n        record_url = self.record_url(record_id)\r\n        return self._patch(record_url, json_data={\"fields\": fields, \"typecast\": typecast})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_by_field(self, field_name, field_value, fields, typecast=False, **options):\r\n        record = self.match(field_name, field_value, **options)\r\n        return {} if not record else self.update(record['id'], fields, typecast)", "response": "Updates the first record to match field name and value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace a record by its record id.", "response": "def replace(self, record_id, fields, typecast=False):\r\n        \"\"\"\r\n        Replaces a record by its record id.\r\n        All Fields are updated to match the new ``fields`` provided.\r\n        If a field is not included in ``fields``, value will bet set to null.\r\n        To update only selected fields, use :any:`update`.\r\n\r\n        >>> record = airtable.match('Seat Number', '22A')\r\n        >>> fields = {'PassangerName': 'Mike', 'Passport': 'YASD232-23'}\r\n        >>> airtable.replace(record['id'], fields)\r\n\r\n        Args:\r\n            record_id(``str``): Id of Record to update\r\n            fields(``dict``): Fields to replace with.\r\n                Must be dictionary with Column names as Key.\r\n            typecast(``boolean``): Automatic data conversion from string values.\r\n\r\n        Returns:\r\n            record (``dict``): New record\r\n        \"\"\"\r\n        record_url = self.record_url(record_id)\r\n        return self._put(record_url, json_data={\"fields\": fields, \"typecast\": typecast})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace_by_field(self, field_name, field_value, fields, typecast=False, **options):\r\n        record = self.match(field_name, field_value, **options)\r\n        return {} if not record else self.replace(record['id'], fields, typecast)", "response": "Replaces the first record with the given field name and value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a record by its id", "response": "def delete(self, record_id):\r\n        \"\"\"\r\n        Deletes a record by its id\r\n\r\n        >>> record = airtable.match('Employee Id', 'DD13332454')\r\n        >>> airtable.delete(record['id'])\r\n\r\n        Args:\r\n            record_id(``str``): Airtable record id\r\n\r\n        Returns:\r\n            record (``dict``): Deleted Record\r\n        \"\"\"\r\n        record_url = self.record_url(record_id)\r\n        return self._delete(record_url)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting first record to match provided field_name and field_value.", "response": "def delete_by_field(self, field_name, field_value, **options):\r\n        \"\"\"\r\n        Deletes first record  to match provided ``field_name`` and\r\n        ``field_value``.\r\n\r\n        >>> record = airtable.delete_by_field('Employee Id', 'DD13332454')\r\n\r\n        Args:\r\n            field_name (``str``): Name of field to match (column name).\r\n            field_value (``str``): Value of field to match.\r\n\r\n        Keyword Args:\r\n            view (``str``, optional): The name or ID of a view.\r\n                See :any:`ViewParam`.\r\n            sort (``list``, optional): List of fields to sort by.\r\n                Default order is ascending. See :any:`SortParam`.\r\n\r\n        Returns:\r\n            record (``dict``): Deleted Record\r\n        \"\"\"\r\n        record = self.match(field_name, field_value, **options)\r\n        record_url = self.record_url(record['id'])\r\n        return self._delete(record_url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mirror(self, records, **options):\r\n\r\n        all_record_ids = [r['id'] for r in self.get_all(**options)]\r\n        deleted_records = self.batch_delete(all_record_ids)\r\n        new_records = self.batch_insert(records)\r\n        return (new_records, deleted_records)", "response": "Mirrors the specified records on the table or view and replaces with records."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd members and subcategories to data", "response": "def _add_members(self, catmembers):\n        \"\"\"\n        Adds category members and subcategories to data\n        \"\"\"\n        members = [x for x in catmembers if x['ns'] == 0]\n        subcats = [x for x in catmembers if x['ns'] == 14]\n\n        if 'members' in self.data:\n            self.data['members'].extend(members)\n        else:\n            self.data.update({'members': members})\n\n        if subcats:\n            if 'subcategories' in self.data:\n                self.data['subcategories'].extend(subcats)\n            else:\n                self.data.update({'subcategories': subcats})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying the user s log entries.", "response": "def _query(self, action, qobj):\n        \"\"\"\n        Form query to enumerate category\n        \"\"\"\n        title = self.params.get('title')\n        pageid = self.params.get('pageid')\n\n        if action == 'random':\n            return qobj.random(namespace=14)\n        elif action == 'category':\n            return qobj.category(title, pageid, self._continue_params())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting data from API response", "response": "def _set_data(self, action):\n        \"\"\"\n        Set category member data from API response\n        \"\"\"\n        data = self._load_response(action)\n\n        self._handle_continuations(data, 'category')\n\n        if action == 'category':\n            members = data.get('query').get('categorymembers')\n            if members:\n                self._add_members(members)\n\n        if action == 'random':\n            rand = data['query']['random'][0]\n            data = {'pageid': rand.get('id'),\n                    'title': rand.get('title')}\n            self.data.update(data)\n            self.params.update(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the category members of the current article", "response": "def get_members(self, show=True, proxy=None, timeout=0):\n        \"\"\"\n        GET Mediawiki:API (action=query) category members\n        https://www.mediawiki.org/wiki/API:Categorymembers\n\n        Required {params}: title OR pageid\n        - title: <str> article title\n        - pageid: <int> Wikipedia database ID\n\n        Optional arguments:\n        - [show]: <bool> echo page data if true\n        - [proxy]: <str> use this HTTP proxy\n        - [timeout]: <int> timeout in seconds (0=wait forever)\n\n        Data captured:\n        - members: <list> category members [{ns, pageid, title}]\n        \"\"\"\n        title = self.params.get('title')\n        pageid = self.params.get('pageid')\n\n        if not title and not pageid:\n            raise LookupError(\"needs category title or pageid\")\n\n        self._get('category', show, proxy, timeout)\n\n        while self.data.get('continue'):\n            self._get('category', show, proxy, timeout)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a random category from MediaWiki", "response": "def get_random(self, show=True, proxy=None, timeout=0):\n        \"\"\"\n        GET MediaWiki:API (action=query) for random category\n        https://www.mediawiki.org/wiki/API:Random\n\n        Required {params}: None\n\n        Optional arguments:\n        - [show]: <bool> echo page data if true\n        - [proxy]: <str> use this HTTP proxy\n        - [timeout]: <int> timeout in seconds (0=wait forever)\n\n        Data captured:\n        - pageid: <int> Wikipedia database ID\n        - title: <str> article title\n        \"\"\"\n        self._get('random', show, proxy, timeout)\n\n        # flush cache to allow repeated random requests\n        del self.cache['random']\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_data(self, action):\n        if action == 'siteinfo':\n            self._set_siteinfo()\n        elif action == 'sitematrix':\n            self._set_sitematrix()\n        elif action == 'sitevisitors':\n            self._set_sitevisitors()", "response": "Set the data for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_siteinfo(self):\n        data = self._load_response('siteinfo').get('query')\n\n        mostviewed = data.get('mostviewed')\n        self.data['mostviewed'] = []\n        for item in mostviewed[1:]:\n            if item['ns'] == 0:\n                self.data['mostviewed'].append(item)\n\n        general = data.get('general')\n\n        self.params.update({'title': general.get('sitename')})\n        self.params.update({'lang': general.get('lang')})\n        self.data['site'] = general.get('wikiid')\n\n        info = {}\n        for item in general:\n            ginfo = general.get(item)\n            if ginfo:\n                info[item] = ginfo\n        self.data['info'] = info\n\n        siteviews = data.get('siteviews')\n        if siteviews:\n            values = [x for x in siteviews.values() if x]\n            if values:\n                self.data['siteviews'] = int(sum(values) / len(values))\n            else:\n                self.data['siteviews'] = 0\n\n        stats = data.get('statistics')\n        for item in stats:\n            self.data[item] = stats[item]", "response": "Set the site info from the API response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the sitematrix of the current user.", "response": "def _set_sitematrix(self):\n        \"\"\"\n        capture API sitematrix data in data attribute\n        \"\"\"\n        data = self._load_response('sitematrix')\n\n        self.params.update({'title': self.COMMONS})\n\n        matrix = data.get('sitematrix')\n        if matrix:\n            self.data['sites'] = self._sitelist(matrix)\n            self.data['random'] = random.choice(self.data['sites'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_sitevisitors(self):\n        data = self._load_response('sitevisitors').get('query')\n\n        siteviews = data.get('siteviews')\n        if siteviews:\n            values = [x for x in siteviews.values() if x]\n            if values:\n                self.data['visitors'] = int(sum(values) / len(values))\n            else:\n                self.data['visitors'] = 0", "response": "Set the available visitors for this item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of sites from a SiteMatrix optionally filtered by domain param", "response": "def _sitelist(self, matrix):\n        \"\"\"\n        Returns a list of sites from a SiteMatrix, optionally filtered\n        by 'domain' param\n        \"\"\"\n        _list = []\n        for item in matrix:\n            sites = []\n\n            if isinstance(matrix[item], list):\n                sites = matrix[item]\n            elif isinstance(matrix[item], dict):\n                sites = matrix[item]['site']\n\n            for site in sites:\n                if len(site.keys()) > 4:  # closed, fishbowl, private\n                    continue\n                domain = self.params.get('domain')\n                if domain:\n                    if domain in site['url']:\n                        _list.append(site['url'])\n                else:\n                    _list.append(site['url'])\n\n        return _list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_info(self, wiki=None, show=True, proxy=None, timeout=0):\n        if wiki:\n            self.params.update({'wiki': wiki})\n\n        self._get('siteinfo', show=False, proxy=proxy, timeout=timeout)\n        self._get('sitevisitors', show, proxy, timeout)\n\n        return self", "response": "Get site info via the API and add some parameters to the object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sites(self, domain=None, show=True, proxy=None, timeout=0):\n        if domain:\n            self.params.update({'domain': domain})\n\n        self.params.update({'wiki': self.COMMONS})\n\n        self._get('sitematrix', show, proxy, timeout)\n\n        del self.params['wiki']\n\n        return self", "response": "Get Wikimedia sites from Commons SiteMatrix"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef top(self, wiki=None, limit=25):\n        if wiki:\n            self.params.update({'wiki': wiki})\n\n        if 'siteinfo' not in self.cache:\n            self.get_info(show=False)\n\n        print(\"%s mostviewed articles:\" % (self.data['site']))\n\n        count = 0\n        for item in self.data['mostviewed']:\n            if item['ns'] == 0:\n                count += 1\n                print(\"%d. %s (%s)\" % (count, item['title'],\n                                       \"{:,}\".format(item['count'])))\n            if count >= limit:\n                break", "response": "Print the most viewed articles over the last WEEK"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting page data strings to stderr", "response": "def prettyprint(datastr):\n    \"\"\"\n    Print page data strings to stderr\n    \"\"\"\n    maxwidth = WPToolsQuery.MAXWIDTH\n    rpad = WPToolsQuery.RPAD\n\n    extent = maxwidth - (rpad + 2)\n    for line in datastr:\n        if len(line) >= maxwidth:\n            line = line[:extent] + '...'\n        utils.stderr(line)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_showstr(self, seed):\n        output = [\"%s (%s) data\" % (seed, self.params['lang'])]\n\n        output.append('{')\n\n        maxwidth = WPToolsQuery.MAXWIDTH\n\n        for item in sorted(self.data):\n\n            if self.data[item] is None:\n                continue\n\n            prefix = item\n            value = self.data[item]\n\n            if isinstance(value, dict):\n                prefix = \"%s: <dict(%d)>\" % (prefix, len(value))\n                value = ', '.join(value.keys())\n            elif isinstance(value, int):\n                prefix = \"%s:\" % prefix\n                if 'pageid' not in prefix:\n                    value = \"{:,}\".format(value)\n            elif isinstance(value, list):\n                prefix = \"%s: <list(%d)>\" % (prefix, len(value))\n                value = ', '.join((safestr(x) for x in value if x))\n            elif isinstance(value, tuple):\n                prefix = \"%s: <tuple(%d)>\" % (prefix, len(value))\n                value = ', '.join((safestr(x) for x in value if x))\n            elif utils.is_text(value):\n                value = value.strip().replace('\\n', '')\n                if len(value) > (maxwidth - len(prefix)):\n                    prefix = \"%s: <str(%d)>\" % (prefix, len(value))\n                else:\n                    prefix = \"%s:\" % prefix\n\n            output.append(\"  %s %s\" % (prefix, value))\n\n        output.append('}')\n\n        return output", "response": "Builds a show string for the data attribute"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _continue_params(self):\n        if not self.data.get('continue'):\n            return\n\n        params = []\n        for item in self.data['continue']:\n            params.append(\"&%s=%s\" % (item, self.data['continue'][item]))\n\n        return ''.join(params)", "response": "Returns the continue parameters of the url fragment"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles continuations from the cache.", "response": "def _handle_continuations(self, response, cache_key):\n        \"\"\"\n        Select continue params and clear cache or last continue params\n        \"\"\"\n        rcontinue = response.get('continue')\n        listen = ['blcontinue', 'cmcontinue', 'plcontinue']\n        cparams = {}\n\n        if rcontinue:\n            for flag in listen:\n                if rcontinue.get(flag):\n                    cparams[flag] = rcontinue.get(flag)\n\n        if cparams:\n            self.data['continue'] = cparams\n            del self.cache[cache_key]\n        else:  # no more continuations\n            if 'continue' in self.data:\n                del self.data['continue']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get(self, action, show, proxy, timeout):\n        silent = self.flags['silent']\n\n        if action in self.cache:\n            if action != 'imageinfo' and action != 'labels':\n                utils.stderr(\"+ %s results in cache\" % action, silent)\n                return\n        else:\n            self.cache[action] = {}\n\n        if self.flags.get('skip') and action in self.flags['skip']:\n            if not self.flags['silent']:\n                utils.stderr(\"+ skipping %s\" % action)\n            return\n\n        if 'requests' not in self.data:\n            self.data['requests'] = []\n\n        if len(self.data['requests']) >= self.REQUEST_LIMIT:\n            raise StopIteration(\"Hit REQUEST_LIMIT = %d\" % self.REQUEST_LIMIT)\n\n        if self.data['requests'] and self.REQUEST_DELAY:\n            utils.stderr(\"REQUEST_DELAY = %d seconds\" % self.REQUEST_DELAY)\n            sleep(self.REQUEST_DELAY)\n\n        # make the request\n        qobj = WPToolsQuery(lang=self.params['lang'],\n                            variant=self.params.get('variant'),\n                            wiki=self.params.get('wiki'),\n                            endpoint=self.params.get('endpoint'))\n        qstr = self._query(action, qobj)\n        req = self._request(proxy, timeout)\n        response = req.get(qstr, qobj.status)\n\n        self.cache[action]['query'] = qstr\n        self.cache[action]['response'] = response\n        self.cache[action]['info'] = req.info\n\n        self.data['requests'].append(action)\n\n        self._set_data(action)\n\n        if show and not self.flags.get('silent'):\n            self.show()", "response": "Get a specific entry from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading API response from cache or raises LookupError", "response": "def _load_response(self, action):\n        \"\"\"\n        returns API reponse from cache or raises ValueError\n        \"\"\"\n        _query = self.cache[action]['query'].replace('&format=json', '')\n        response = self.cache[action]['response']\n\n        if not response:\n            raise ValueError(\"Empty response: %s\" % self.params)\n\n        try:\n            data = utils.json_loads(response)\n        except ValueError:\n            raise ValueError(_query)\n\n        if data.get('warnings'):\n            if 'WARNINGS' in self.data:\n                self.data['WARNINGS'].update(data['warnings'])\n            else:\n                self.data['WARNINGS'] = data['warnings']\n\n        if data.get('error'):\n            utils.stderr(\"API error: %s\" % data.get('error'))\n            raise LookupError(_query)\n\n        if 'query' in action and data.get('query'):\n            if data['query'].get('pages'):\n                if data['query']['pages'][0].get('missing'):\n                    raise LookupError(_query)\n\n        if action == 'parse' and not data.get('parse'):\n            raise LookupError(_query)\n\n        if action == 'wikidata':\n            handle_wikidata_errors(data, _query)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a WPToolsRequest object", "response": "def _request(self, proxy, timeout):\n        \"\"\"\n        Returns WPToolsRequest object\n        \"\"\"\n        return request.WPToolsRequest(self.flags['silent'],\n                                      self.flags['verbose'],\n                                      proxy, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef info(self, action=None):\n        if action in self.cache:\n            return self.cache[action]['info']\n        return self.cache.keys() or None", "response": "returns the list of cached request info for given action"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of cached query strings for given action", "response": "def query(self, action=None):\n        \"\"\"\n        returns cached query string (without &format=json) for given action,\n        or list of cached actions\n        \"\"\"\n        if action in self.cache:\n            return self.cache[action]['query'].replace('&format=json', '')\n        return self.cache.keys() or None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the response for given action", "response": "def response(self, action=None):\n        \"\"\"\n        returns cached response (as dict) for given action,\n        or list of cached actions\n        \"\"\"\n        if action in self.cache:\n            return utils.json_loads(self.cache[action]['response'])\n        return self.cache.keys() or None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show(self):\n        if not self.data:\n            return\n\n        if self.data.get('continue'):\n            return\n\n        ptitle = self.params.get('title')\n        dtitle = self.data.get('title')\n        pageid = self.params.get('pageid')\n\n        seed = dtitle or ptitle or pageid\n        if utils.is_text(seed):\n            seed = seed.replace('_', ' ')\n\n        prettyprint(self._build_showstr(seed))", "response": "Pretty - print instance data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict with claims as reduced", "response": "def reduce_claims(query_claims):\n    \"\"\"\n    returns claims as reduced dict {P: [Q's or values]}\n        P = property\n        Q = item\n    \"\"\"\n    claims = collections.defaultdict(list)\n\n    for claim, entities in query_claims.items():\n\n        for ent in entities:\n\n            try:\n                snak = ent.get('mainsnak')\n                snaktype = snak.get('snaktype')\n                value = snak.get('datavalue').get('value')\n            except AttributeError:\n                claims[claim] = []\n\n            try:\n                if snaktype != 'value':\n                    val = snaktype\n                elif value.get('id'):\n                    val = value.get('id')\n                elif value.get('text'):\n                    val = value.get('text')\n                elif value.get('time'):\n                    val = value.get('time')\n                else:\n                    val = value\n            except AttributeError:\n                val = value\n\n            if not val or not [x for x in val if x]:\n                raise ValueError(\"%s %s\" % (claim, ent))\n\n            claims[claim].append(val)\n\n    return dict(claims)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of a Wikidata entity property", "response": "def _get_entity_prop(self, entity, prop):\n        \"\"\"\n        returns Wikidata entity property value\n        \"\"\"\n        variant = self.params.get('variant')\n        lang = self.params.get('lang')\n\n        if entity.get(prop):\n            ent = entity[prop]\n            try:\n                return ent[variant or lang].get('value')\n            except AttributeError:\n                return ent.get('value')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmarshaling claims into the internal data structure", "response": "def _marshal_claims(self, query_claims):\n        \"\"\"\n        set Wikidata entities from query claims\n        \"\"\"\n        claims = reduce_claims(query_claims)\n        # self.data['claimq'] = query_claims\n        self.data['claims'] = claims\n\n        entities = set()\n        for eid in claims:\n            if self.user_labels:\n                if eid in self.user_labels or eid == 'P31':\n                    entities.add(eid)  # P (property)\n                else:\n                    continue  # get only wanted entities\n            else:\n                entities.add(eid)  # P (property)\n\n            for val in claims[eid]:\n                if utils.is_text(val) and re.match(r'^Q\\d+$', val):\n                    entities.add(val)  # Q (item)\n\n        self.data['entities'] = list(entities)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopping up to limit entities and return them off the list", "response": "def _pop_entities(self, limit=50):\n        \"\"\"\n        returns up to limit entities and pops them off the list\n        \"\"\"\n        pop = self.data['entities'][:limit]\n        del self.data['entities'][:limit]\n        return pop"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_data(self, action):\n        if action == 'labels':\n            self._set_labels()\n\n        if action == 'wikidata':\n            self._set_wikidata()\n            self.get_labels(show=False)", "response": "Capture Wikidata API response data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_labels(self):\n        data = self._load_response('labels')\n        entities = data.get('entities') or []\n\n        for ent in entities:\n            label = self._get_entity_prop(entities[ent], 'labels')\n            self.data['labels'][ent] = label", "response": "set entity labels from get_labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_title(self, item):\n        title = None\n        lang = self.params['lang']\n        label = self.data['label']\n\n        if item.get('sitelinks'):\n            for link in item['sitelinks']:\n                if link == \"%swiki\" % lang:\n                    title = item['sitelinks'][link]['title']\n                    self.data['title'] = title.replace(' ', '_')\n\n        if not self.data.get('title') and label:\n            self.data['title'] = label.replace(' ', '_')\n\n        if self.data.get('title') and not self.params.get('title'):\n            self.params['title'] = self.data['title']", "response": "set title from wikidata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset attributes derived from Wikidata", "response": "def _set_wikidata(self):\n        \"\"\"\n        set attributes derived from Wikidata (action=wbentities)\n        \"\"\"\n        self.data['labels'] = {}\n        self.data['wikidata'] = {}\n\n        data = self._load_response('wikidata')\n        entities = data.get('entities')\n        item = entities.get(next(iter(entities)))\n\n        self.data['wikidata_pageid'] = item.get('pageid')\n\n        aliases = item.get('aliases')\n        if aliases:\n            aliases = [x['value'] for x in aliases[self.params['lang']]]\n            self.data['aliases'] = aliases\n\n        modified = item.get('modified')\n        try:\n            self.data['modified'].update({'wikidata': modified})\n        except KeyError:\n            self.data['modified'] = {'wikidata': modified}\n\n        wikibase = item.get('id')\n        if wikibase:\n            self.data['wikibase'] = wikibase\n            self.data['wikidata_url'] = utils.wikidata_url(wikibase)\n\n        self.data['description'] = self._get_entity_prop(item, 'descriptions')\n        self.data['label'] = self._get_entity_prop(item, 'labels')\n\n        self._marshal_claims(item.get('claims'))\n        self._set_title(item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the image field in the data object.", "response": "def _update_images(self):\n        \"\"\"\n        add images from Wikidata\n        \"\"\"\n        wd_images = self.data['claims'].get('P18')  # image\n\n        if wd_images:\n            if not isinstance(wd_images, list):\n                wd_images = [wd_images]\n\n            if 'image' not in self.data:\n                self.data['image'] = []\n\n            for img_file in wd_images:\n                self.data['image'].append({'file': img_file,\n                                           'kind': 'wikidata-image'})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates what field in the data object.", "response": "def _update_what(self):\n        \"\"\"\n        set what this thing is! \"instance of (P31)\"\n        \"\"\"\n        if 'P31' not in self.data['claims']:  # missing Wikidata\n            msg = (\"Note: Wikidata item %s\" % self.data['wikibase'],\n                   \"missing 'instance of' (P31)\")\n            utils.stderr(\" \".join(msg))\n            return\n\n        instance_of = self.data['claims']['P31'][0]\n        labels = self.data['labels']\n\n        if instance_of in labels:\n            self.data['what'] = labels[instance_of]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_wikidata(self):\n        claims = self.data['claims']\n\n        for ent in claims:\n\n            plabel = self.data['labels'].get(ent)  # P (property) label\n            if plabel:\n                plabel = \"%s (%s)\" % (plabel, ent)\n\n            claim = []\n            for item in claims[ent]:\n                ilabel = item\n                if utils.is_text(item) and re.match(r'^Q\\d+$', item):\n                    ilabel = self.data['labels'].get(item)  # Q (item) label\n                    if ilabel:\n                        ilabel = \"%s (%s)\" % (ilabel, item)\n\n                if len(claims[ent]) == 1:\n                    claim = ilabel\n                else:\n                    claim.append(ilabel)\n\n            if plabel and ilabel:\n                self.data['wikidata'][plabel] = claim", "response": "Update wikidata from claims and labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_labels(self, show=False, proxy=None, timeout=0):\n        if 'entities' not in self.data:\n            utils.stderr(\"No entities found.\")\n            return\n\n        skip_flag = False\n        if 'skip' in self.flags and 'labels' in self.flags['skip']:\n            skip_flag = True\n\n        while 'entities' in self.data and self.data['entities']:\n            if skip_flag:\n                break\n            self._get('labels', show, proxy, timeout)\n\n        if 'entities' in self.data:\n            del self.data['entities']\n\n        self._post_labels_updates()\n\n        return self", "response": "Get the labels of the current user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget Wikidata data from Mediawiki.", "response": "def get_wikidata(self, show=True, proxy=None, timeout=0):\n        \"\"\"\n        GET Wikidata:API (action=wbgetentities) wikidata\n        https://www.wikidata.org/w/api.php?action=help&modules=wbgetentities\n\n        Required {params}: title OR wikibase\n        - title: <str> Mediawiki page title, file, category, etc.\n        - wikibase: <str> Wikidata item ID\n\n        Optional {params}:\n        - [lang]: <str> Mediawiki language code (default='en')\n        - [variant]: <str> Mediawiki language variant\n\n        Optional arguments:\n        - [show]: <bool> echo page data if true\n        - [proxy]: <str> use this HTTP proxy\n        - [timeout]: <int> timeout in seconds (0=wait forever)\n\n        Data captured:\n        - aliases: <list> list of \"also known as\"\n        - claims: <dict> intermediate Wikidata claims (compare to cache)\n        - description: <str> Wikidata description\n        - image: <dict> {wikidata-image} Wikidata Property:P18\n        - label: <str> Wikidata label\n        - labels: <str> list of resolved labels\n        - modified (wikidata): <str> ISO8601 date and time\n        - pageid: <int> Wikipedia database ID\n        - requests: list of request actions made\n        - title: <str> article title\n        - what: <str> Wikidata Property:P31 \"instance of\"\n        - wikibase: <str> Wikidata item ID\n        - wikidata: <dict> resolved Wikidata claims\n        - wikidata_url: <str> Wikidata URL\n        \"\"\"\n        title = self.params.get('title')\n        wikibase = self.params.get('wikibase')\n\n        if not wikibase and not title:\n            err = \"get_wikidata needs wikibase or title\"\n            raise LookupError(err)\n\n        self._get('wikidata', show, proxy, timeout)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wanted_labels(self, labels):\n        if not isinstance(labels, list):\n            raise ValueError(\"Input labels must be a list.\")\n\n        self.user_labels = labels", "response": "Set the list of WANTED labels to minimize get_labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _html_image(page):\n    source = _image(page)\n    if not source:\n        return\n    alt = page.data.get('label') or page.data.get('title')\n    img = \"<img src=\\\"%s\\\"\" % source\n    img += \" alt=\\\"%s\\\" title=\\\"%s\\\" \" % (alt, alt)\n    img += \"align=\\\"right\\\" width=\\\"240\\\">\"\n    return img", "response": "Returns HTML img tag for a page"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning Wiki - linked HTML title", "response": "def _html_title(page):\n    \"\"\"\n    returns Wiki-linked HTML title\n    \"\"\"\n    link = \"<a href=\\\"%s\\\">%s</a>\" % (page.data.get('url'),\n                                      page.data.get('title'))\n    desc = page.data.get('description')\n    if desc:\n        link += \"&mdash;<i>%s</i>\" % desc\n    else:\n        link += \"&mdash;<i>description</i>\"\n    if link:\n        return \"<p>%s</p>\" % link"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns assembled HTML output for a page.", "response": "def _page_html(page):\n    \"\"\"\n    returns assembled HTML output\n    \"\"\"\n    out = []\n    out.append(_html_title(page))\n    out.append(_html_image(page))\n    out.append(page.data.get('extract'))\n    return \"\\n\".join([x for x in out if x])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _page_text(page, nowrap=False):\n    title = page.data['title']\n    title = \"%s\\n%s\" % (title, \"=\" * len(title))\n\n    desc = page.data.get('description')\n    if desc:\n        desc = \"_%s_\" % desc\n\n    img = _text_image(page)\n\n    pars = page.data.get('extext')\n\n    if pars:\n        # pars = pars.replace(' * ', '\\n * ')\n        pars = re.sub(r'[ ]+\\*[ ]+', '* ', pars)\n\n    if pars and not nowrap:\n        parlist = []\n        for par in pars.split(\"\\n\\n\"):\n            parlist.append(\"\\n\".join(textwrap.wrap(par)))\n\n        disambiguation = page.data.get('disambiguation')\n        if disambiguation:\n            parlist.append(' * ' + \"\\n * \".join(page.data.get('links')))\n\n        pars = \"\\n\\n\".join(parlist)\n\n    url = '<%s>' % page.data['url']\n\n    txt = []\n    txt.append(title)\n    txt.append(desc)\n    txt.append(url)\n    txt.append(pars)\n    txt.append(img)\n\n    return \"\\n\\n\".join([x for x in txt if x])", "response": "returns assembled text for a page"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _safe_exit(start, output):\n    try:\n        sys.stdout.write(output)\n        sys.stdout.flush()\n    except TypeError:  # python3\n        sys.stdout.write(str(output, 'utf-8'))\n        sys.stdout.flush()\n    except IOError:\n        pass\n\n    seconds = time.time() - start\n    print(\"\\n\\n%5.3f seconds\" % (seconds), file=sys.stderr)", "response": "Exit with breaking pipes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning text image URL", "response": "def _text_image(page):\n    \"\"\"\n    returns text image URL\n    \"\"\"\n    img = None\n    alt = page.data.get('label') or page.data.get('title')\n    source = _image(page)\n    if source:\n        img = \"![%s](%s)\" % (alt, source)\n    return img"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninvoke wptools and assemble selected output", "response": "def get(args):\n    \"\"\"\n    invoke wptools and assemble selected output\n    \"\"\"\n\n    html = args.H\n    lang = args.l\n    nowrap = args.n\n    query = args.q\n    silent = args.s\n    title = args.t\n    verbose = args.v\n    wiki = args.w\n\n    if query:\n        qobj = WPToolsQuery(lang=lang, wiki=wiki)\n        if title:\n            return qobj.query(title)\n        return qobj.random()\n\n    page = wptools.page(title, lang=lang, silent=silent,\n                        verbose=verbose, wiki=wiki)\n\n    try:\n        page.get_query()\n    except (StandardError, ValueError, LookupError):\n        return \"NOT_FOUND\"\n\n    if not page.data.get('extext'):\n        out = page.cache['query']['query']\n\n    out = _page_text(page, nowrap)\n    if html:\n        out = _page_html(page)\n\n    try:\n        return out.encode('utf-8')\n    except KeyError:\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse command line arguments", "response": "def parse_args():\n    \"\"\"\n    parse main() args\n    \"\"\"\n    description = (\n        \"Get Wikipedia article info and Wikidata via MediaWiki APIs.\\n\\n\"\n        \"Gets a random English Wikipedia article by default, or in the\\n\"\n        \"language -lang, or from the wikisite -wiki, or by specific\\n\"\n        \"title -title. The output is a plain text extract unless -HTML.\")\n    epilog = (\"Powered by https://github.com/siznax/wptools/ %s\"\n              % wptools.__version__)\n    argp = argparse.ArgumentParser(\n        description=description,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=epilog)\n    argp.add_argument(\"-H\", \"-HTML\", action='store_true',\n                      help=\"output HTML extract\")\n    argp.add_argument(\"-l\", \"-lang\", default='en',\n                      help=\"language code\")\n    argp.add_argument(\"-n\", \"-nowrap\", action='store_true',\n                      help=\"do not wrap text\")\n    argp.add_argument(\"-q\", \"-query\", action='store_true',\n                      help=\"show query and exit\")\n    argp.add_argument(\"-s\", \"-silent\", action='store_true',\n                      help=\"quiet output to stderr\")\n    argp.add_argument(\"-t\", \"-title\", help=\"get a specific title\")\n    argp.add_argument(\"-v\", \"-verbose\", action='store_true',\n                      help=\"HTTP status to stderr\")\n    argp.add_argument(\"-w\", \"-wiki\",\n                      help=\"use alternative wikisite\")\n    return argp.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvoke wptools and exit safely", "response": "def main(args):\n    \"\"\"\n    invoke wptools and exit safely\n    \"\"\"\n    start = time.time()\n    output = get(args)\n    _safe_exit(start, output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __insert_image_info(self, title, _from, info):\n        for img in self.data['image']:\n            if 'url' not in img:\n                if title == img['file']:  # matching title/file\n                    img.update(info)\n                elif _from == img['file']:  # matching from/file\n                    img.update(info)", "response": "Insert API image INFO into matching image dict\nCOOKIENAME"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __pull_image_info(self, title, imageinfo, normalized):\n        for info in imageinfo:\n            info.update({'title': title})\n\n            # get API normalized \"from\" filename for matching\n            _from = None\n            for norm in normalized:\n                if title == norm['to']:\n                    _from = norm['from']\n\n            # let's put all \"metadata\" in one member\n            info['metadata'] = {}\n            extmetadata = info.get('extmetadata')\n            if extmetadata:\n                info['metadata'].update(extmetadata)\n                del info['extmetadata']\n\n            self.__insert_image_info(title, _from, info)", "response": "Pull image INFO from API response and insert it into the database"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extend_data(self, datapoint, new_data):\n        if new_data:\n            try:\n                self.data[datapoint].extend(new_data)\n            except KeyError:\n                self.data[datapoint] = new_data", "response": "extend or assign new data to datapoint"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _missing_imageinfo(self):\n        if 'image' not in self.data:\n            return\n        missing = []\n        for img in self.data['image']:\n            if 'url' not in img:\n                missing.append(img['file'])\n        return list(set(missing))", "response": "returns list of image filenames that are missing info"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnormalizes image filenames by prepending File :", "response": "def _normalize_images(self):\n        \"\"\"\n        normalizes image filenames by prepending 'File:' if needed\n        \"\"\"\n        if 'image' not in self.data:\n            return\n        for img in self.data['image']:\n            fname = img['file'].replace('_', ' ')\n            fstart = fname.startswith('File:')\n            istart = fname.startswith('Image:')\n            if not fstart and not istart:\n                fname = 'File:' + fname\n                img['orig'] = img['file']\n                img['file'] = fname"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the data for the specified action.", "response": "def _set_data(self, action):\n        \"\"\"\n        marshals response data into page data\n        \"\"\"\n        if 'query' in action:\n            self._set_query_data(action)\n        elif action == 'imageinfo':\n            self._set_imageinfo_data()\n        elif action == 'parse':\n            self._set_parse_data()\n        elif action == 'random':\n            self._set_random_data()\n        elif action == 'labels':\n            self._set_labels()\n        elif action == 'wikidata':\n            self._set_wikidata()\n            self.get_labels()\n        elif action == 'restbase':\n            self._set_restbase_data()\n\n        self._update_imageinfo()\n        self._update_params()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_imageinfo_data(self):\n        data = self._load_response('imageinfo')\n        pages = data['query'].get('pages')\n\n        normalized = []\n        if 'normalized' in data['query']:\n            normalized = data['query']['normalized']\n\n        for page in pages:\n            title = page.get('title')\n            imageinfo = page.get('imageinfo')\n            if imageinfo:\n                self.__pull_image_info(title, imageinfo, normalized)\n\n        # Mark missing imageinfo to prevent duplicate requests\n        for img in self.data['image']:\n            if 'url' not in img:\n                img['url'] = 'MISSED'", "response": "Set image attributes from MediaWiki API response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset attributes derived from MediaWiki (action=parse)", "response": "def _set_parse_data(self):\n        \"\"\"\n        set attributes derived from MediaWiki (action=parse)\n        \"\"\"\n        pdata = self._load_response('parse')['parse']\n\n        self.data['iwlinks'] = utils.get_links(pdata.get('iwlinks'))\n        self.data['pageid'] = pdata.get('pageid')\n        self.data['wikitext'] = pdata.get('wikitext')\n\n        parsetree = pdata.get('parsetree')\n        self.data['parsetree'] = parsetree\n\n        boxterm = self.params.get('boxterm')\n        if boxterm:\n            infobox = utils.get_infobox(parsetree, boxterm)\n        else:\n            infobox = utils.get_infobox(parsetree)\n        self.data['infobox'] = infobox\n\n        title = pdata.get('title')\n        if title:\n            self.data['title'] = title\n            if not self.params.get('title'):\n                self.params['title'] = title\n\n        wikibase = pdata.get('properties').get('wikibase_item')\n        if wikibase:\n            self.data['wikibase'] = wikibase\n            self.data['wikidata_url'] = utils.wikidata_url(wikibase)\n\n        if self.data['infobox']:\n            self._set_parse_image(self.data['infobox'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting parse - image data from action = parse response", "response": "def _set_parse_image(self, infobox):\n        \"\"\"\n        set image data from action=parse response\n        \"\"\"\n        image = infobox.get('image')\n        cover = infobox.get('Cover') or infobox.get('cover')\n\n        if image or cover:\n            if 'image' not in self.data:\n                self.data['image'] = []\n\n        if image and utils.isfilename(image):\n            self.data['image'].append({'kind': 'parse-image', 'file': image})\n\n        if cover and utils.isfilename(cover):\n            self.data['image'].append({'kind': 'parse-cover', 'file': cover})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the data derived from MediaWiki s query response.", "response": "def _set_query_data(self, action='query'):\n        \"\"\"\n        set attributes derived from MediaWiki (action=query)\n        \"\"\"\n        data = self._load_response(action)\n        page = data['query']['pages'][0]\n\n        self._handle_continuations(data, action)\n\n        if action == 'query':\n            self.data['random'] = data['query']['random'][0][\"title\"]\n\n        self._extend_data('backlinks', data['query'].get('backlinks'))\n\n        self.data['redirected'] = data['query'].get('redirects')\n\n        self._set_query_data_fast_1(page)  # avoid pylint too-many-branches\n        self._set_query_data_fast_2(page)\n        self._set_query_data_slow(page)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_query_data_fast_1(self, page):\n        self.data['pageid'] = page.get('pageid')\n\n        assessments = page.get('pageassessments')\n        if assessments:\n            self.data['assessments'] = assessments\n\n        extract = page.get('extract')\n        if extract:\n            self.data['extract'] = extract\n            extext = html2text.html2text(extract)\n            if extext:\n                self.data['extext'] = extext.strip()\n\n        fullurl = page.get('fullurl')\n        if fullurl:\n            self.data['url'] = fullurl\n            self.data['url_raw'] = fullurl + '?action=raw'\n\n        length = page.get('length')\n        if length:\n            self.data['length'] = length\n\n        self._extend_data('links', utils.get_links(page.get('links')))\n\n        self._update_data('modified', 'page', page.get('touched'))\n\n        pageprops = page.get('pageprops')\n        if pageprops:\n            wikibase = pageprops.get('wikibase_item')\n            if wikibase:\n                self.data['wikibase'] = wikibase\n                self.data['wikidata_url'] = utils.wikidata_url(wikibase)\n\n            if 'disambiguation' in pageprops:\n                self.data['disambiguation'] = len(self.data['links'])", "response": "set less expensive action = query response data PART 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting less expensive action = query response data PART 2", "response": "def _set_query_data_fast_2(self, page):\n        \"\"\"\n        set less expensive action=query response data PART 2\n        \"\"\"\n        self.data['pageid'] = page.get('pageid')\n\n        redirects = page.get('redirects')\n        if redirects:\n            self.data['redirects'] = redirects\n\n        terms = page.get('terms')\n        if terms:\n            if terms.get('alias'):\n                self.data['aliases'] = terms['alias']\n\n            if terms.get('description'):\n                self.data['description'] = next(iter(terms['description']),\n                                                None)\n            if terms.get('label'):\n                self.data['label'] = next(iter(terms['label']), None)\n\n        title = page.get('title')\n        self.data['title'] = title\n        if not self.params.get('title'):\n            self.params['title'] = title\n\n        watchers = page.get('watchers')\n        if watchers:\n            self.data['watchers'] = watchers\n\n        self._set_query_image(page)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset more expensive action = query response data", "response": "def _set_query_data_slow(self, page):\n        \"\"\"\n        set more expensive action=query response data\n        \"\"\"\n        categories = page.get('categories')\n        if categories:\n            self.data['categories'] = [x['title'] for x in categories]\n\n        if page.get('contributors'):\n            contributors = page.get('contributors') or 0\n            anoncontributors = page.get('anoncontributors') or 0\n            if isinstance(contributors, list):\n                contributors = len(contributors)\n            self.data['contributors'] = contributors + anoncontributors\n\n        files = page.get('images')  # really, these are FILES\n        if files:\n            self.data['files'] = [x['title'] for x in files]\n\n        languages = page.get('langlinks')\n        if languages:\n            self.data['languages'] = languages\n\n        pageviews = page.get('pageviews')\n        if pageviews:\n            values = [x for x in pageviews.values() if x]\n            if values:\n                self.data['views'] = int(sum(values) / len(values))\n            else:\n                self.data['views'] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_query_image(self, page):\n        pageimage = page.get('pageimage')\n        thumbnail = page.get('thumbnail')\n\n        if pageimage or thumbnail:\n            if 'image' not in self.data:\n                self.data['image'] = []\n\n        if pageimage:\n            self.data['image'].append({\n                'kind': 'query-pageimage',\n                'file': pageimage})\n\n        if thumbnail:\n            qthumb = {'kind': 'query-thumbnail'}\n            qthumb.update(thumbnail)\n            qthumb['url'] = thumbnail.get('source')\n            del qthumb['source']\n            qthumb['file'] = qthumb['url'].split('/')[-2]\n            self.data['image'].append(qthumb)", "response": "set image data from action = query response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting page data from random request", "response": "def _set_random_data(self):\n        \"\"\"\n        sets page data from random request\n        \"\"\"\n        rdata = self._load_response('random')\n        rdata = rdata['query']['random'][0]\n\n        pageid = rdata.get('id')\n        title = rdata.get('title')\n\n        self.data.update({'pageid': pageid,\n                          'title': title})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_data(self, datapoint, key, new_data):\n        if new_data:\n            try:\n                self.data[datapoint].update({key: new_data})\n            except KeyError:\n                self.data[datapoint] = {key: new_data}", "response": "Update or assign new data to datapoint"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling get_imageinfo if data image missing info", "response": "def _update_imageinfo(self):\n        \"\"\"\n        calls get_imageinfo() if data image missing info\n        \"\"\"\n        missing = self._missing_imageinfo()\n        deferred = self.flags.get('defer_imageinfo')\n        continuing = self.data.get('continue')\n\n        if missing and not deferred and not continuing:\n            self.get_imageinfo(show=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate params from response data", "response": "def _update_params(self):\n        \"\"\"\n        update params from response data\n        \"\"\"\n        if self.data.get('title'):\n            self.params['title'] = self.data.get('title')\n        if self.data.get('pageid'):\n            self.params['pageid'] = self.data.get('pageid')\n        if self.data.get('wikibase'):\n            self.params['wikibase'] = self.data.get('wikibase')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef skip_action(self, action):\n        if 'skip' not in self.flags:\n            self.flags['skip'] = []\n        self.flags['skip'].append(action)", "response": "append action to skip flag"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, show=True, proxy=None, timeout=0):\n        wikibase = self.params.get('wikibase')\n\n        if wikibase:\n\n            self.flags['defer_imageinfo'] = True\n\n            self.get_wikidata(False, proxy, timeout)\n            self.get_query(False, proxy, timeout)\n            self.get_parse(False, proxy, timeout)\n\n            self.flags['defer_imageinfo'] = False\n\n            self.get_restbase('/page/summary/', False, proxy, timeout)\n\n            if show and not self.flags.get('silent'):\n                self.show()\n\n        else:\n\n            self.flags['defer_imageinfo'] = True\n\n            self.get_query(False, proxy, timeout)\n            self.get_parse(False, proxy, timeout)\n\n            if not self.data.get('wikibase'):\n                self.skip_action('wikidata')\n\n            self.get_wikidata(False, proxy, timeout)\n\n            self.flags['defer_imageinfo'] = False\n\n            wiki = self.params.get('wiki')\n            if wiki and 'wikipedia.org' not in wiki:\n                self.skip_action('restbase')\n\n            self.get_restbase('/page/summary/', False, proxy, timeout)\n\n            if show and not self.flags.get('silent'):\n                self.show()\n\n        return self", "response": "Get the related object and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_imageinfo(self, show=True, proxy=None, timeout=0):\n        if not self.data.get('image'):\n            raise ValueError(\"get_imageinfo needs a page image\")\n\n        if not self._missing_imageinfo() and 'imageinfo' in self.cache:\n            utils.stderr(\"complete imageinfo in cache\", self.flags['silent'])\n            return\n\n        self._normalize_images()\n        self._get('imageinfo', show, proxy, timeout)\n\n        return self", "response": "Get the imageinfo from MediaWiki"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_more(self, show=True, proxy=None, timeout=0):\n        return self.get_querymore(show, proxy, timeout)", "response": "Get more information about the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget parse data from MediaWiki", "response": "def get_parse(self, show=True, proxy=None, timeout=0):\n        \"\"\"\n        GET MediaWiki:API action=parse request\n        https://en.wikipedia.org/w/api.php?action=help&modules=parse\n\n        Required {params}: title OR pageid\n        - title: <str> article title\n        - pageid: <int> Wikipedia database ID\n\n        Optional arguments:\n        - [show]: <bool> echo page data if true\n        - [proxy]: <str> use this HTTP proxy\n        - [timeout]: <int> timeout in seconds (0=wait forever)\n\n        Data captured:\n        - image: <dict> {parse-image, parse-cover}\n        - infobox: <dict> Infobox data as python dictionary\n        - iwlinks: <list> interwiki links\n        - pageid: <int> Wikipedia database ID\n        - parsetree: <str> XML parse tree\n        - requests: list of request actions made\n        - wikibase: <str> Wikidata entity ID or wikidata URL\n        - wikitext: <str> raw wikitext URL\n        \"\"\"\n        if not self.params.get('title') and not self.params.get('pageid'):\n            raise ValueError(\"get_parse needs title or pageid\")\n\n        self._get('parse', show, proxy, timeout)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the current query from MediaWiki", "response": "def get_query(self, show=True, proxy=None, timeout=0):\n        \"\"\"\n        GET MediaWiki:API action=query selected data\n        https://en.wikipedia.org/w/api.php?action=help&modules=query\n\n        Required {params}: title OR pageid\n        - title: <str> article title\n        - pageid: <int> Wikipedia database ID\n\n        Optional arguments:\n        - [show]: <bool> echo page data if true\n        - [proxy]: <str> use this HTTP proxy\n        - [timeout]: <int> timeout in seconds (0=wait forever)\n\n        Data captured:\n        - description: <str> Wikidata description (via pageterms)\n        - extext: <str> plain text (Markdown) extract\n        - extract: <str> HTML extract from Extension:TextExtract\n        - image: <dict> {query-pageimage, query-thumbnail}\n        - label: <str> Wikidata label (via pageterms)\n        - modified (page): <str> ISO8601 date and time\n        - pageid: <int> Wikipedia database ID\n        - random: <str> a random article title with every request!\n        - requests: list of request actions made\n        - url: <str> the canonical wiki URL\n        - url_raw: <str> ostensible raw wikitext URL\n        - watchers: <int> number of people watching this page\n        \"\"\"\n        if not self.params.get('title') and not self.params.get('pageid'):\n            raise ValueError(\"get_query needs title or pageid\")\n\n        self._get('query', show, proxy, timeout)\n\n        while self.data.get('continue'):\n            self._get('query', show, proxy, timeout)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef curl_info(crl):\n    kbps = crl.getinfo(crl.SPEED_DOWNLOAD) / 1000.0\n    url = crl.getinfo(crl.EFFECTIVE_URL)\n    url = url.replace(\"&format=json\", '').replace(\"&formatversion=2\", '')\n\n    # https://travis-ci.org/siznax/wptools/jobs/401109410\n    try:\n        content_type = crl.getinfo(crl.CONTENT_TYPE)\n    except TypeError:\n        content_type = None\n\n    return {\"url\": url,\n            \"user-agent\": user_agent(),\n            \"content-type\": content_type,\n            \"status\": crl.getinfo(crl.RESPONSE_CODE),\n            \"bytes\": crl.getinfo(crl.SIZE_DOWNLOAD),\n            \"seconds\": \"%5.3f\" % crl.getinfo(crl.TOTAL_TIME),\n            \"kB/s\": \"%3.1f\" % kbps}", "response": "Returns curl info from Pycurl object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a single object from the specified URL.", "response": "def get(self, url, status):\n        \"\"\"\n        in favor of python-requests for speed\n        \"\"\"\n\n        # consistently faster than requests by 3x\n        #\n        # r = requests.get(url,\n        #                  headers={'User-Agent': self.user_agent})\n        # return r.text\n\n        crl = self.cobj\n\n        try:\n            crl.setopt(pycurl.URL, url)\n        except UnicodeEncodeError:\n            crl.setopt(pycurl.URL, url.encode('utf-8'))\n\n        if not self.silent:\n            print(status, file=sys.stderr)\n\n        if self.DISABLED:\n            print(\"Requests DISABLED\", file=sys.stderr)\n        else:\n            return self.curl_perform(crl)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef curl_perform(self, crl):\n        bfr = BytesIO()\n        crl.setopt(crl.WRITEFUNCTION, bfr.write)\n        crl.perform()\n        info = curl_info(crl)\n        if info:\n            if self.verbose and not self.silent:\n                for item in sorted(info):\n                    print(\"  %s: %s\" % (item, info[item]), file=sys.stderr)\n            self.info = info\n        body = bfr.getvalue()\n        bfr.close()\n        return body", "response": "Perform HTTP GET and returns body of response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef curl_setup(self, proxy=None, timeout=0):\n\n        crl = pycurl.Curl()\n        crl.setopt(pycurl.USERAGENT, user_agent())\n        crl.setopt(pycurl.FOLLOWLOCATION, True)\n        crl.setopt(pycurl.CAINFO, certifi.where())\n\n        if isinstance(proxy, str):\n            crl.setopt(pycurl.PROXY, proxy)\n        if isinstance(proxy, dict):\n            if proxy.get('PROXY'):\n                crl.setopt(pycurl.PROXY, proxy['PROXY'])\n            if proxy.get('PORT'):\n                crl.setopt(pycurl.PROXYPORT, proxy['PORT'])\n            if proxy.get('USERPWD'):\n                crl.setopt(pycurl.PROXYUSERPWD, proxy['USERPWD'])\n\n        if timeout:  # 0 = wait forever\n            crl.setopt(pycurl.CONNECTTIMEOUT, timeout)\n            crl.setopt(pycurl.TIMEOUT, timeout)\n        if self.verbose and not self.silent:\n            crl.setopt(pycurl.VERBOSE, True)\n\n        self.cobj = crl", "response": "Setup curl options for a specific object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn category query string", "response": "def category(self, title, pageid=None, cparams=None, namespace=None):\n        \"\"\"\n        Returns category query string\n        \"\"\"\n        query = self.LIST.substitute(\n            WIKI=self.uri,\n            ENDPOINT=self.endpoint,\n            LIST='categorymembers')\n        status = pageid or title\n\n        query += \"&cmlimit=500\"\n\n        if namespace is not None:\n            query += \"&cmnamespace=%d\" % namespace\n\n        if title and pageid:\n            title = None\n\n        if title:\n            query += \"&cmtitle=\" + safequote(title)\n\n        if pageid:\n            query += \"&cmpageid=%d\" % pageid\n\n        if cparams:\n            query += cparams\n            status += ' (%s)' % cparams\n\n        self.set_status('categorymembers', status)\n\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef labels(self, qids):\n        if len(qids) > 50:\n            raise ValueError(\"The limit is 50.\")\n\n        self.domain = 'www.wikidata.org'\n        self.uri = self.wiki_uri(self.domain)\n\n        query = self.WIKIDATA.substitute(\n            WIKI=self.uri,\n            ENDPOINT=self.endpoint,\n            LANG=self.variant or self.lang,\n            PROPS='labels')\n\n        qids = '|'.join(qids)\n        query += \"&ids=%s\" % qids\n\n        self.set_status('labels', qids)\n\n        return query", "response": "Returns Wikidata labels query string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef imageinfo(self, files):\n        files = '|'.join([safequote(x) for x in files])\n\n        self.set_status('imageinfo', files)\n\n        return self.IMAGEINFO.substitute(\n            WIKI=self.uri,\n            ENDPOINT=self.endpoint,\n            FILES=files)", "response": "Returns imageinfo query string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning Mediawiki action = parse query string", "response": "def parse(self, title, pageid=None):\n        \"\"\"\n        Returns Mediawiki action=parse query string\n        \"\"\"\n        qry = self.PARSE.substitute(\n            WIKI=self.uri,\n            ENDPOINT=self.endpoint,\n            PAGE=safequote(title) or pageid)\n\n        if pageid and not title:\n            qry = qry.replace('&page=', '&pageid=').replace('&redirects', '')\n\n        if self.variant:\n            qry += '&variant=' + self.variant\n\n        self.set_status('parse', pageid or title)\n\n        return qry"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(self, titles, pageids=None, cparams=None):\n        query = self.QUERY.substitute(\n            WIKI=self.uri,\n            ENDPOINT=self.endpoint,\n            TITLES=safequote(titles) or pageids)\n        status = titles or pageids\n\n        if pageids and not titles:\n            query = query.replace('&titles=', '&pageids=')\n\n        if cparams:\n            query += cparams\n            status += \" (%s)\" % cparams\n\n        if self.variant:\n            query += '&variant=' + self.variant\n\n        self.set_status('query', status)\n\n        return query", "response": "Returns MediaWiki action = query string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns query string for random page", "response": "def random(self, namespace=0):\n        \"\"\"\n        Returns query string for random page\n        \"\"\"\n        query = self.LIST.substitute(\n            WIKI=self.uri,\n            ENDPOINT=self.endpoint,\n            LIST='random')\n        query += \"&rnlimit=1&rnnamespace=%d\" % namespace\n\n        emoji = [\n            u'\\U0001f32f',  # burrito or wrap\n            u'\\U0001f355',  # slice of pizza\n            u'\\U0001f35c',  # steaming bowl of ramen\n            u'\\U0001f363',  # sushi\n            u'\\U0001f369',  # doughnut\n            u'\\U0001f36a',  # cookie\n            u'\\U0001f36d',  # lollipop\n            u'\\U0001f370',  # strawberry shortcake\n        ]\n\n        action = 'random'\n        if namespace:\n            action = 'random:%d' % namespace\n\n        self.set_status(action, random.choice(emoji))\n\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns RESTBase query string", "response": "def restbase(self, endpoint, title):\n        \"\"\"\n        Returns RESTBase query string\n        \"\"\"\n        if not endpoint:\n            raise ValueError(\"invalid endpoint: %s\" % endpoint)\n\n        route = endpoint\n        if title and endpoint != '/page/':\n            route = endpoint + safequote_restbase(title)\n\n        self.set_status('restbase', route)\n\n        return \"%s/api/rest_v1/%s\" % (self.uri, route[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_status(self, action, target):\n        try:\n            target = unquote(target)\n        except (AttributeError, TypeError):\n            pass\n\n        status = \"%s (%s) %s\" % (self.domain, action, target)\n        status = status.strip().replace('\\n', '')\n\n        if len(status) >= self.MAXWIDTH:\n            tail = '...'\n            extent = self.MAXWIDTH - (len(tail) + self.RPAD)\n            self.status = status[:extent] + tail\n        else:\n            self.status = status", "response": "Sets the status of the current object based on the action and target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wikidata(self, title, wikibase=None):\n        self.domain = 'www.wikidata.org'\n        self.uri = self.wiki_uri(self.domain)\n\n        query = self.WIKIDATA.substitute(\n            WIKI=self.uri,\n            ENDPOINT=self.endpoint,\n            LANG=self.variant or self.lang,\n            PROPS=\"aliases|info|claims|descriptions|labels|sitelinks\")\n\n        if wikibase:\n            query += \"&ids=%s\" % wikibase\n        elif title:\n            title = safequote(title)\n            query += \"&sites=%swiki\" % self.lang\n            query += \"&titles=%s\" % title\n\n        self.set_status('wikidata', wikibase or title)\n\n        return query", "response": "Returns Wikidata query string for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning RESTBase response if appropriate", "response": "def _handle_response(self):\n        \"\"\"\n        returns RESTBase response if appropriate\n        \"\"\"\n        content = self.cache['restbase']['info']['content-type']\n        if content.startswith('text/html'):\n            html = self.cache['restbase']['response']\n            if isinstance(html, bytes):\n                html = html.decode('utf-8')\n            self.data['html'] = html\n            return\n\n        response = self._load_response('restbase')\n\n        http_status = self.cache['restbase']['info']['status']\n        if http_status == 404:\n            raise LookupError(self.cache['restbase']['query'])\n\n        if self.params.get('endpoint') == '/page/':\n            msg = \"RESTBase /page/ entry points: %s\" % response.get('items')\n            utils.stderr(msg)\n            del self.cache['restbase']\n            return\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _query(self, action, qobj):\n        return qobj.restbase(self.params['rest_endpoint'],\n                             self.params.get('title'))", "response": "returns WPToolsQuery string from action\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unpack_images(self, rdata):\n        image = rdata.get('image')  # /page/mobile-sections-lead\n        originalimage = rdata.get('originalimage')  # /page/summary\n        thumbnail = rdata.get('thumbnail')  # /page/summary\n\n        if image or originalimage or thumbnail:\n            if 'image' not in self.data:\n                self.data['image'] = []\n\n        def file_url(info):\n            \"\"\"\n            put image source in url and set file key\n            \"\"\"\n            if 'source' in info:\n                info['url'] = info['source']\n                info['file'] = info['source'].split('/')[-1]\n                del info['source']\n            return info\n\n        if image:\n            img = {'kind': 'restbase-image'}\n            img.update(image)\n            self.data['image'].append(file_url(img))\n\n        if originalimage:\n            img = {'kind': 'restbase-original'}\n            img.update(originalimage)\n            self.data['image'].append(file_url(img))\n\n        if thumbnail:\n            img = {'kind': 'restbase-thumb'}\n            img.update(thumbnail)\n            self.data['image'].append(file_url(img))", "response": "Unpacks image data from RESTBase response and sets the related object attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_restbase(self, endpoint='/page/',\n                     show=True, proxy=None, timeout=0):\n        \"\"\"\n        GET RESTBase /page/ endpoints needing only {title}\n        https://en.wikipedia.org/api/rest_v1/\n\n        for example:\n            /page/\n            /page/html/{title}\n            /page/summary/{title}\n            /page/mobile-sections-lead/{title}\n\n        Required {params}: None\n        Without arguments, lists RESTBase /page/ entry points\n\n        Optional {params}:\n        - [title]: <str> Mediawiki page title, file, category, etc.\n        - [lang]: <str> Mediawiki language code (default=en)\n\n        Optional arguments:\n        - [endpoint]: RESTBase entry point (default=/page/)\n        - [show]: <bool> echo page data if true\n        - [proxy]: <str> use this HTTP proxy\n        - [timeout]: <int> timeout in seconds (0=wait forever)\n\n        Data captured:\n        - exhtml: <str> \"extract_html\" from /page/summary\n        - exrest: <str> \"extract\" from /page/summary\n        - html: <str> from /page/html\n        - image: <dict> {rest-image, rest-thumb}\n        - lead: <str> section[0] from /page/mobile-sections-lead\n        - modified (page): <str> ISO8601 date and time\n        - title: <str> the article title\n        - url: <str> the canonical wiki URL\n        - url_raw: <str> probable raw wikitext URL\n        - wikibase: <str> Wikidata item ID\n        - wikidata_url: <str> Wikidata URL\n        \"\"\"\n        if endpoint != '/page/' and not self.params.get('title'):\n            raise StandardError(\"endpoint %s needs a title\" % endpoint)\n\n        self.params.update({'rest_endpoint': endpoint})\n\n        self._get('restbase', show, proxy, timeout)\n\n        return self", "response": "Get the restbase for a given entry point"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning parse tree template with title containing boxterm as dict", "response": "def get_infobox(ptree, boxterm=\"box\"):\n    \"\"\"\n    Returns parse tree template with title containing <boxterm> as dict:\n\n        <box> = {<name>: <value>, ...}\n\n    If simple transform fails, attempts more general assembly:\n\n        <box> = {'boxes': [{<title>: <parts>}, ...],\n                 'count': <len(boxes)>}\n    \"\"\"\n    boxes = []\n    for item in lxml.etree.fromstring(ptree).xpath(\"//template\"):\n\n        title = item.find('title').text\n        if title and boxterm in title:\n\n            box = template_to_dict(item)\n            if box:\n                return box\n\n            alt = template_to_dict_alt(item, title)\n            if alt:\n                boxes.append(alt)\n\n    if boxes:\n        return {'boxes': boxes, 'count': len(boxes)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns list of titles urls from query / parse links response returns list of titles only if no links are found", "response": "def get_links(rlinks):\n    \"\"\"\n    returns list of titles/urls from query/parse links response\n    \"\"\"\n    if rlinks is None:\n        return\n    links = []\n    for item in rlinks:\n        if 'url' in item:\n            links.append(item['url'])\n        if 'title' in item and 'ns' in item:\n            if item['ns'] == 0:  # articles only\n                links.append(item['title'])\n    return sorted(links) if links else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if object is text - like", "response": "def is_text(obj, name=None):\n    \"\"\"\n    returns True if object is text-like\n    \"\"\"\n    try:  # python2\n        ans = isinstance(obj, basestring)\n    except NameError:  # python3\n        ans = isinstance(obj, str)\n    if name:\n        print(\"is_text: (%s) %s = %s\" % (ans, name, obj.__class__),\n              file=sys.stderr)\n    return ans"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite msg to stderr if not silent", "response": "def stderr(msg, silent=False):\n    \"\"\"\n    write msg to stderr if not silent\n    \"\"\"\n    if not silent:\n        print(msg, file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns wikitext template as dict", "response": "def template_to_dict(tree, debug=0, find=False):\n    \"\"\"\n    returns wikitext template as dict\n\n    debug = 1\n        prints minimal debug info to stdout\n    debug > 1\n        compares _iter() versus _find() results\n    find = True\n        sets values from _find() algorithm (default _iter())\n    \"\"\"\n\n    # you can compare (most) raw Infobox wikitext like this:\n    # https://en.wikipedia.org/wiki/TITLE?action=raw&section=0\n\n    obj = defaultdict(str)\n    errors = []\n    for item in tree:\n        try:\n            name = item.findtext('name').strip()\n\n            if debug:\n                template_to_dict_debug(name, item, debug)\n\n            find_val = template_to_dict_find(item, debug)  # DEPRECATED\n            iter_val = template_to_dict_iter(item, debug)\n\n            value = iter_val\n            if find:\n                value = find_val\n\n            if name and value:\n                obj[name] = value.strip()\n\n        except AttributeError:\n\n            if isinstance(item, lxml.etree.ElementBase):\n                name = item.tag.strip()\n                text = item.text.strip()\n                if item.tag == 'title':\n                    obj['infobox'] = text\n                else:\n                    obj[name] = text\n\n        except:\n            errors.append(lxml.etree.tostring(item))\n\n    if errors:\n        obj['errors'] = errors\n\n    return dict(obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn parse tree template as dict.", "response": "def template_to_dict_alt(tree, title):\n    \"\"\"\n    Returns parse tree template as {<title>: <parts>}\n    This is a more general parse tree infobox template parser.\n    \"\"\"\n    box = []\n    part = []\n\n    for item in tree.iter():\n\n        if item.tag == 'part':\n            if part:\n                box.append(part)\n            part = []\n\n        if item.tag == 'name' or item.tag == 'value':\n            for attr in item.keys():\n                part.append({attr: item.get(attr)})\n\n            if item.text:\n                part.append(item.text.strip())\n\n            if item.tail:\n                part.append(item.tail.strip())\n\n    if part:\n        box.append(part)\n\n    return {title.strip(): box}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef template_to_dict_debug(name, item, debug):\n    if debug == 1:\n        print(\"\\n%s = \" % name)\n    elif debug > 1:\n        print(\"\\n%s\" % name)\n        print(\"=\" * 64)\n        print(lxml.etree.tostring(item))\n        print()", "response": "Print debug statements to compare algorithms\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns infobox parsetree value using etree. find", "response": "def template_to_dict_find(item, debug=0):\n    \"\"\"\n    DEPRECATED: Returns infobox parsetree value using etree.find()\n\n    Older template_to_dict() algorithm, uses etree.xpath() to \"lookup\"\n    or find specific elements, but fails to include tail text in the\n    order it is found, and does not _exclude_ <ext> tags (references,\n    etc.). Compare to template_to_dict_iter().\n    \"\"\"\n    if debug > 1:\n        print(\"template_to_dict_find:\")\n\n    tmpl = item.find('value').find('template')\n\n    if tmpl is not None:\n        value = template_to_text(tmpl, debug)\n    else:\n        value = text_with_children(item.find('value'), debug)\n\n    if debug:\n        print(\"  find: %s\" % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef template_to_dict_iter(item, debug=0):\n    valarr = []\n    found_template = False\n\n    if debug > 1:\n        print(\"template_to_dict_iter:\")\n\n    for elm in item.iter():\n\n        if debug > 1:\n            template_to_dict_iter_debug(elm)\n\n        if elm.tag == 'value' and not found_template:\n            valarr.append(elm.text.strip())\n\n        if elm.tag == 'template':\n            found_template = True\n            valarr.append(template_to_text(elm, debug).strip())\n\n        if elm.tail:\n            valarr.append(elm.tail.strip())\n\n    value = \" \".join([x for x in valarr if x])\n\n    if debug:\n        print(\"  iter: %s\" % value)\n\n    return value", "response": "Returns infobox parsetree value using etree. iter"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint expanded element on stdout for debugging", "response": "def template_to_dict_iter_debug(elm):\n    \"\"\"\n    Print expanded element on stdout for debugging\n    \"\"\"\n    if elm.text is not None:\n        print(\"    <%s>%s</%s>\" % (elm.tag, elm.text, elm.tag), end='')\n        if elm.tail is not None:\n            print(elm.tail)\n        else:\n            print()\n    else:\n        if elm.tail is not None:\n            print(\"    <%s>%s\" % (elm.tag, elm.tail))\n        else:\n            print(\"    <%s>\" % elm.tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef template_to_text(tmpl, debug=0):\n    tarr = []\n    for item in tmpl.itertext():\n        tarr.append(item)\n\n    text = \"{{%s}}\" % \"|\".join(tarr).strip()\n\n    if debug > 1:\n        print(\"+ template_to_text:\")\n        print(\"  %s\" % text)\n\n    return text", "response": "convert parse tree template to text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the text content with children.", "response": "def text_with_children(node, debug=0):\n    \"\"\"\n    DEPRECATED: return text content with children (#62), sub-elements (#66)\n\n    Only used by deprecated template_to_dict_find(), and suffers from\n    copypasta code smell.\n    \"\"\"\n\n    # https://stackoverflow.com/questions/4624062/get-all-text-inside-a-tag-in-lxml\n\n    if sys.version.startswith('3'):  # py3 needs encoding=str\n        parts = ([node.text] +\n                 list(chain(\n                     *([tostring(c, with_tail=False, encoding=str),\n                        c.tail] for c in node.getchildren())))\n                 + [node.tail])\n    else:\n        parts = ([node.text] +\n                 list(chain(\n                     *([tostring(c, with_tail=False),\n                        c.tail] for c in node.getchildren())))\n                 + [node.tail])\n\n    value = ''.join(filter(lambda x: x or isinstance(x, str), parts)).strip()\n\n    if debug > 1:\n        print(\"+ text_with_children:\")\n        print(\"  %s\" % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall implicitly before a packet is sent.", "response": "def post_build(self, p, pay):\n    \"\"\"Called implicitly before a packet is sent.\n    \"\"\"\n    p += pay\n    if self.auxdlen != 0:\n      print(\"NOTICE: A properly formatted and complaint V3 Group Record should have an Auxiliary Data length of zero (0).\")\n      print(\"        Subsequent Group Records are lost!\")\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the integer value to its IGMPv3 encoded time value if needed.", "response": "def float_encode(self, value):\n    \"\"\"Convert the integer value to its IGMPv3 encoded time value if needed.\n   \n    If value < 128, return the value specified. If >= 128, encode as a floating \n    point value. Value can be 0 - 31744.\n    \"\"\"\n    if value < 128:\n      code = value\n    elif value > 31743:\n      code = 255\n    else:\n      exp=0\n      value>>=3\n      while(value>31):\n        exp+=1\n        value>>=1\n      exp<<=4\n      code = 0x80 | exp | (value & 0x0F)\n    return code"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post_build(self, p, pay):\n    p += pay\n    if self.type in [0, 0x31, 0x32, 0x22]:   # for these, field is reserved (0)\n      p = p[:1]+chr(0)+p[2:]\n    if self.chksum is None:\n      ck = checksum(p[:2]+p[4:])\n      p = p[:2]+ck.to_bytes(2, 'big')+p[4:]\n    return p", "response": "Called implicitly before a packet is sent to compute and place checksum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mysummary(self):\n\n    if isinstance(self.underlayer, IP):\n      return self.underlayer.sprintf(\"IGMPv3: %IP.src% > %IP.dst% %IGMPv3.type% %IGMPv3.gaddr%\")\n    else:\n      return self.sprintf(\"IGMPv3 %IGMPv3.type% %IGMPv3.gaddr%\")", "response": "Display a summary of the IGMPv3 object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef adjust_ether (self, ip=None, ether=None):\n# The rules are:\n#   1. send to the group mac address address corresponding to the IP.dst\n    if ip != None and ip.haslayer(IP) and ether != None and ether.haslayer(Ether):\n      iplong = atol(ip.dst)\n      ether.dst = \"01:00:5e:%02x:%02x:%02x\" % ( (iplong>>16)&0x7F, (iplong>>8)&0xFF, (iplong)&0xFF )\n      # print \"igmpize ip \" + ip.dst + \" as mac \" + ether.dst \n      return True\n    else:\n      return False", "response": "This function adjusts the ethernet header destination MAC address based on an associated Ethernet header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling to explicitely fixup an associated IP header The function adjusts the IP header based on conformance rules and the group address encoded in the IGMP message. The rules are: 1. Send General Group Query to 224.0.0.1 (all systems) 2. Send Leave Group to 224.0.0.2 (all routers) 3a.Otherwise send the packet to the group address 3b.Send reports/joins to the group address 4. ttl = 1 (RFC 2236, section 2) 5. send the packet with the router alert IP option (RFC 2236, section 2)", "response": "def adjust_ip (self, ip=None):\n    \"\"\"Called to explicitely fixup an associated IP header\n\n    The function adjusts the IP header based on conformance rules \n    and the group address encoded in the IGMP message.\n    The rules are:\n    1. Send General Group Query to 224.0.0.1 (all systems)\n    2. Send Leave Group to 224.0.0.2 (all routers)\n    3a.Otherwise send the packet to the group address\n    3b.Send reports/joins to the group address\n    4. ttl = 1 (RFC 2236, section 2)\n    5. send the packet with the router alert IP option (RFC 2236, section 2)\n    \"\"\"\n    if ip != None and ip.haslayer(IP):\n      if (self.type == 0x11):\n        if (self.gaddr == \"0.0.0.0\"):\n          ip.dst = \"224.0.0.1\"                   # IP rule 1\n          retCode = True                     \n        elif isValidMCAddr(self.gaddr):\n          ip.dst = self.gaddr                    # IP rule 3a\n          retCode = True\n        else:\n          print(\"Warning: Using invalid Group Address\")\n          retCode = False\n      elif ((self.type == 0x17) and isValidMCAddr(self.gaddr)):\n          ip.dst = \"224.0.0.2\"                   # IP rule 2\n          retCode = True\n      elif ((self.type == 0x12) or (self.type == 0x16)) and (isValidMCAddr(self.gaddr)):\n          ip.dst = self.gaddr                    # IP rule 3b\n          retCode = True\n      else:\n        print(\"Warning: Using invalid IGMP Type\")\n        retCode = False\n    else:\n      print(\"Warning: No IGMP Group Address set\")\n      retCode = False\n    if retCode == True:\n       ip.ttl=1                                  # IP Rule 4\n       ip.options=[IPOption_Router_Alert()]      # IP rule 5\n    return retCode"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ospf_lsa_checksum(lsa):\n    # This is based on the GPLed C implementation in Zebra <http://www.zebra.org/>\n\n    CHKSUM_OFFSET = 16\n\n    if len(lsa) < CHKSUM_OFFSET:\n        raise Exception(\"LSA Packet too short (%s bytes)\" % len(lsa))\n\n    c0 = c1 = 0\n    # Calculation is done with checksum set to zero\n    lsa = lsa[:CHKSUM_OFFSET] + lsa[CHKSUM_OFFSET + 2:]\n    for char in lsa[2:]:  #  leave out age\n        c0 += char\n        c1 += c0\n\n    c0 %= 255\n    c1 %= 255\n\n    x = ((len(lsa) - CHKSUM_OFFSET - 1) * c0 - c1) % 255\n\n    if (x <= 0):\n        x += 255\n\n    y = 510 - c0 - x\n\n    if (y > 255):\n        y -= 255\n\n    checksum = (x << 8) + y\n\n    return checksum", "response": "Calculate the Fletcher checksum for OSPF LSAs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _LSAGuessPayloadClass(p, **kargs):\n    # This is heavily based on scapy-cdp.py by Nicolas Bareil and Arnaud Ebalard\n    # XXX: This only works if all payload\n    cls = conf.raw_layer\n    if len(p) >= 4:\n        typ = p[3]\n        clsname = _OSPF_LSclasses.get(typ, \"Raw\")\n        cls = globals()[clsname]\n    return cls(p, **kargs)", "response": "Guess the correct LSA class for a given payload"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef airpwn(iffrom, ifto, replace, pattern=\"\", ignorepattern=\"\"):\n    \n    ptrn = re.compile(pattern)\n    iptrn = re.compile(ignorepattern)\n    def do_airpwn(p, ifto=ifto, replace=replace, ptrn=ptrn, iptrn=iptrn):\n        if not isinstance(p,Dot11):\n            return\n        if not p.FCfield & 1:\n            return\n        if not p.haslayer(TCP):\n            return\n        ip = p.getlayer(IP)\n        tcp = p.getlayer(TCP)\n        pay = str(tcp.payload)\n#        print \"got tcp\"\n        if not ptrn.match(pay):\n            return\n#        print \"match 1\"\n        if iptrn.match(pay):\n            return\n#        print \"match 2\"\n        del(p.payload.payload.payload)\n        p.FCfield=\"from-DS\"\n        p.addr1,p.addr2 = p.addr2,p.addr1\n        q = p.copy()\n        p /= IP(src=ip.dst,dst=ip.src)\n        p /= TCP(sport=tcp.dport, dport=tcp.sport,\n                 seq=tcp.ack, ack=tcp.seq+len(pay),\n                 flags=\"PA\")\n        q = p.copy()\n        p /= replace\n        q.ID += 1\n        q.getlayer(TCP).flags=\"RA\"\n        q.getlayer(TCP).seq+=len(replace)\n        \n        sendp([p,q], iface=ifto, verbose=0)\n#        print \"send\",repr(p)        \n#        print \"send\",repr(q)\n        print(p.sprintf(\"Sent %IP.src%:%IP.sport% > %IP.dst%:%TCP.dport%\"))\n\n    sniff(iface=iffrom,prn=do_airpwn)", "response": "A function that initializes the airpwn of the network."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef locate_ip(ip):\n    ip=map(int,ip.split(\".\"))\n    ip = ip[3]+(ip[2]<<8)+(ip[1]<<16)+(ip[0]<<24)\n\n    cloc = country_loc_kdb.get_base()\n    db = IP_country_kdb.get_base()\n\n    d=0\n    f=len(db)-1\n    while (f-d) > 1:\n        guess = (d+f)/2\n        if ip > db[guess][0]:\n            d = guess\n        else:\n            f = guess\n    s,e,c = db[guess]\n    if  s <= ip and ip <= e:\n        return cloc.get(c,None)", "response": "Locate geographic coordinates from IP using geoip database"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getmacbyip6(ip6, chainCC=0):\n\n    if in6_ismaddr(ip6): # Multicast \n        mac = in6_getnsmac(inet_pton(socket.AF_INET6, ip6))\n        return mac\n\n    iff,a,nh = conf.route6.route(ip6, dev=conf.iface6)\n\n    if iff == LOOPBACK_NAME:\n        return \"ff:ff:ff:ff:ff:ff\"\n\n    if nh != '::': \n        ip6 = nh # Found next hop\n\n    mac = conf.netcache.in6_neighbor.get(ip6)\n    if mac:\n        return mac\n\n    res = neighsol(ip6, a, iff, chainCC=chainCC)\n\n    if res is not None:\n        if ICMPv6NDOptDstLLAddr in res:\n            mac = res[ICMPv6NDOptDstLLAddr].lladdr\n        else:\n            mac = res.src\n        conf.netcache.in6_neighbor[ip6] = mac\n        return mac\n\n    return None", "response": "Returns the mac address for provided ip6 peer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of IPv6 packets in a new order.", "response": "def defragment6(pktlist):\n    \"\"\"\n    Performs defragmentation of a list of IPv6 packets. Packets are reordered.\n    Crap is dropped. What lacks is completed by 'X' characters.\n    \"\"\"\n    \n    l = [ x for x in pktlist if IPv6ExtHdrFragment in x ] # remove non fragments\n    if not l:\n        return []\n\n    id = l[0][IPv6ExtHdrFragment].id \n\n    llen = len(l)\n    l = [ x for x in l if x[IPv6ExtHdrFragment].id == id ]\n    if len(l) != llen:\n        warning(\"defragment6: some fragmented packets have been removed from list\")\n    llen = len(l)\n\n    # reorder fragments \n    i = 0 \n    res = []\n    while l:\n        min_pos = 0\n        min_offset  = l[0][IPv6ExtHdrFragment].offset\n        for p in l:\n            cur_offset = p[IPv6ExtHdrFragment].offset\n            if cur_offset < min_offset:\n                min_pos = 0\n                min_offset  = cur_offset\n        res.append(l[min_pos])\n        del(l[min_pos])\n\n    # regenerate the fragmentable part\n    fragmentable = b\"\"\n    for p in res:\n        q=p[IPv6ExtHdrFragment]\n        offset = 8*q.offset\n        if offset != len(fragmentable):\n            warning(\"Expected an offset of %d. Found %d. Padding with XXXX\" % (len(fragmentable), offset))\n        fragmentable += b\"X\"*(offset - len(fragmentable))\n        fragmentable += bytes(q.payload)\n\n    # Regenerate the unfragmentable part.\n    q = res[0]\n    nh = q[IPv6ExtHdrFragment].nh\n    q[IPv6ExtHdrFragment].underlayer.nh = nh\n    q[IPv6ExtHdrFragment].underlayer.payload = None\n    q /= conf.raw_layer(load=fragmentable)\n    \n    return IPv6(bytes(q))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef names2dnsrepr(x):\n    \n    if type(x) is str:\n        if x and x[-1] == '\\x00': # stupid heuristic\n            return x.encode('ascii')\n        x = [x.encode('ascii')]\n    elif type(x) is bytes:\n        if x and x[-1] == 0:\n            return x\n        x = [x]\n\n    res = []\n    for n in x:\n        if type(n) is str:\n            n = n.encode('ascii')\n        termin = b\"\\x00\"\n        if n.count(b'.') == 0: # single-component gets one more\n            termin += bytes([0]) \n        n = b\"\".join(map(lambda y: chr(len(y)).encode('ascii')+y, n.split(b\".\"))) + termin\n        res.append(n)\n    return b\"\".join(res)", "response": "Takes as input a list of DNS names or a single DNS name and returns a DNS name in DNS format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dnsrepr2names(x):\n    res = []\n    cur = b\"\"\n    if type(x) is str:\n        x = x.encode('ascii')\n    while x:\n        #l = ord(x[0])\n        l = x[0]\n        x = x[1:]\n        if l == 0:\n            if cur and cur[-1] == ord('.'):\n                cur = cur[:-1]\n            res.append(cur)\n            cur = b\"\"\n            #if x and ord(x[0]) == 0: # single component\n            if x and x[0] == 0: # single component\n                x = x[1:]\n            continue\n        if l & 0xc0: # XXX TODO : work on that -- arno\n            raise Exception(\"DNS message can't be compressed at this point!\")\n        else:\n            cur += x[:l]+b\".\"\n            x = x[l:]\n    return res", "response": "Takes as input a DNS encoded string possibly compressed and returns a list of DNS names contained in it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _resolve_one(self, ip):\n\n        if in6_isaddr6to4(ip): # for 6to4, use embedded @\n            tmp = inet_pton(socket.AF_INET6, ip)\n            addr = inet_ntop(socket.AF_INET, tmp[2:6])\n        elif in6_isaddrTeredo(ip): # for Teredo, use mapped address\n            addr = teredoAddrExtractInfo(ip)[2]\n        else:\n            addr = ip\n        \n        _, asn, desc = AS_resolver_riswhois._resolve_one(self, addr)\n\n        return ip,asn,desc", "response": "This function is used to provide a Whois resolution on the IPv4 address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in6_getifaddr():\n    ret = []\n    try:\n        f = open(\"/proc/net/if_inet6\",\"rb\")\n    except IOError as err:\n        return ret\n    l = f.readlines()\n    for i in l:\n        # addr, index, plen, scope, flags, ifname\n        tmp = i.split()\n        addr = struct.unpack('4s4s4s4s4s4s4s4s', tmp[0])\n        addr = kamene.utils6.in6_ptop(b':'.join(addr).decode('ascii'))\n        ret.append((addr, int(tmp[3], 16), tmp[5].decode('ascii'))) # (addr, scope, iface)\n    f.close()\n    return ret", "response": "Returns a list of 3 - tuples of the form addr scope iface where addr is the address of scope iface where iface is the iface associated to the interface\n ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef construct_source_candidate_set(addr, plen, laddr, loname):\n    def cset_sort(x,y):\n        x_global = 0\n        if in6_isgladdr(x):\n            x_global = 1\n        y_global = 0\n        if in6_isgladdr(y):\n            y_global = 1\n        res = y_global - x_global\n        if res != 0 or y_global != 1:\n            return res\n        # two global addresses: if one is native, it wins.\n        if not in6_isaddr6to4(x):\n            return -1;\n        return -res\n\n    cset = []\n    if in6_isgladdr(addr) or in6_isuladdr(addr):\n        cset = [ x for x in laddr if x[1] == IPV6_ADDR_GLOBAL ]\n    elif in6_islladdr(addr):\n        cset = [ x for x in laddr if x[1] == IPV6_ADDR_LINKLOCAL ]\n    elif in6_issladdr(addr):\n        cset = [ x for x in laddr if x[1] == IPV6_ADDR_SITELOCAL ]\n    elif in6_ismaddr(addr):\n        if in6_ismnladdr(addr):\n            cset = [('::1', 16, loname)]\n        elif in6_ismgladdr(addr):\n            cset = [ x for x in laddr if x[1] == IPV6_ADDR_GLOBAL ]\n        elif in6_ismlladdr(addr):\n            cset = [ x for x in laddr if x[1] == IPV6_ADDR_LINKLOCAL ]\n        elif in6_ismsladdr(addr):\n            cset = [ x for x in laddr if x[1] == IPV6_ADDR_SITELOCAL ]\n    elif addr == '::' and plen == 0:\n        cset = [ x for x in laddr if x[1] == IPV6_ADDR_GLOBAL ]\n    cset = [ x[0] for x in cset ]\n    cset.sort(key = cmp_to_key(cset_sort)) # Sort with global addresses first\n    return cset", "response": "This function returns the source candidate set for a specific interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in6_ifaceidtomac(ifaceid): # TODO: finish commenting function behavior\n    try:\n        ifaceid = inet_pton(socket.AF_INET6, \"::\"+ifaceid)[8:16]\n    except:\n        return None\n    if ifaceid[3:5] != b'\\xff\\xfe':\n        return None\n    first = struct.unpack(\"B\", ifaceid[:1])[0]\n    ulbit = 2*[1,'-',0][first & 0x02]\n    first = struct.pack(\"B\", ((first & 0xFD) | ulbit))\n    oui = first + ifaceid[1:3]\n    end = ifaceid[5:]\n    #l = map(lambda x: \"%.02x\" % struct.unpack(\"B\", x)[0], list(oui+end))\n    l = map(lambda x: \"%.02x\" % x, list(oui+end))\n    return \":\".join(l)", "response": "Extract the MAC address from a given iface ID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the IPv6 6to4 prefix associated with provided IPv4 address", "response": "def in6_get6to4Prefix(addr):\n    \"\"\"\n    Returns the /48 6to4 prefix associated with provided IPv4 address\n    On error, None is returned. No check is performed on public/private\n    status of the address\n    \"\"\"\n    try:\n        addr = inet_pton(socket.AF_INET, addr)\n        addr = inet_ntop(socket.AF_INET6, b'\\x20\\x02'+addr+b'\\x00'*10)\n    except:\n        return None\n    return addr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting IPv4 address from 6to4 address.", "response": "def in6_6to4ExtractAddr(addr):\n    \"\"\"\n    Extract IPv4 address embbeded in 6to4 address. Passed address must be\n    a 6to4 addrees. None is returned on error.\n    \"\"\"\n    try:\n        addr = inet_pton(socket.AF_INET6, addr)\n    except:\n        return None\n    if addr[:2] != b\" \\x02\":\n        return None\n    return inet_ntop(socket.AF_INET, addr[2:6])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in6_getLocalUniquePrefix():\n    # Extracted from RFC 1305 (NTP) :\n    # NTP timestamps are represented as a 64-bit unsigned fixed-point number, \n    # in seconds relative to 0h on 1 January 1900. The integer part is in the \n    # first 32 bits and the fraction part in the last 32 bits.\n\n    # epoch = (1900, 1, 1, 0, 0, 0, 5, 1, 0) \n    # x = time.time()\n    # from time import gmtime, strftime, gmtime, mktime\n    # delta = mktime(gmtime(0)) - mktime(self.epoch)\n    # x = x-delta\n\n    tod = time.time() # time of day. Will bother with epoch later\n    i = int(tod)\n    j = int((tod - i)*(2**32))\n    tod = struct.pack(\"!II\", i,j)\n    # TODO: Add some check regarding system address gathering\n    rawmac = get_if_raw_hwaddr(conf.iface6)\n    mac = b\":\".join(map(lambda x: b\"%.02x\" % ord(x), list(rawmac)))\n    # construct modified EUI-64 ID\n    eui64 = inet_pton(socket.AF_INET6, '::' + in6_mactoifaceid(mac))[8:] \n    import sha\n    globalid = sha.new(tod+eui64).digest()[:5]\n    return inet_ntop(socket.AF_INET6, b'\\xfd' + globalid + b'\\x00'*10)", "response": "Returns a pseudo - randomly generated Local Unique prefix."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimplements the interface ID generation algorithm described in RFC 3041. The function takes the Modified EUI-64 interface identifier generated as described in RFC 4291 and an optional previous history value (the first element of the output of this function). If no previous interface identifier is provided, a random one is generated. The function returns a tuple containing the randomized interface identifier and the history value (for possible future use). Input and output values are provided in a \"printable\" format as depicted below. ex: >>> in6_getRandomizedIfaceId('20b:93ff:feeb:2d3') ('4c61:76ff:f46a:a5f3', 'd006:d540:db11:b092') >>> in6_getRandomizedIfaceId('20b:93ff:feeb:2d3', previous='d006:d540:db11:b092') ('fe97:46fe:9871:bd38', 'eeed:d79c:2e3f:62e')", "response": "def in6_getRandomizedIfaceId(ifaceid, previous=None):\n    \"\"\"\n    Implements the interface ID generation algorithm described in RFC 3041.\n    The function takes the Modified EUI-64 interface identifier generated\n    as described in RFC 4291 and an optional previous history value (the\n    first element of the output of this function). If no previous interface\n    identifier is provided, a random one is generated. The function returns\n    a tuple containing the randomized interface identifier and the history\n    value (for possible future use). Input and output values are provided in\n    a \"printable\" format as depicted below.\n    \n    ex: \n\n    >>> in6_getRandomizedIfaceId('20b:93ff:feeb:2d3')\n    ('4c61:76ff:f46a:a5f3', 'd006:d540:db11:b092')\n\n    >>> in6_getRandomizedIfaceId('20b:93ff:feeb:2d3',\n                                 previous='d006:d540:db11:b092')\n    ('fe97:46fe:9871:bd38', 'eeed:d79c:2e3f:62e')\n    \"\"\"\n\n    s = []\n    if previous is None:\n        #d = b\"\".join(map(chr, range(256)))\n        d = list(range(256))\n        for i in range(8):\n            s.append(random.choice(d))\n        s = bytes(s)\n        previous = s\n    s = inet_pton(socket.AF_INET6, \"::\"+ifaceid)[8:] + previous\n    import hashlib\n    s = hashlib.md5(s).digest()\n    s1,s2 = s[:8],s[8:]\n    s1 = bytes([(s1[0]) | 0x04]) + s1[1:]  \n    s1 = inet_ntop(socket.AF_INET6, b\"\\xff\"*8 + s1)[20:]\n    s2 = inet_ntop(socket.AF_INET6, b\"\\xff\"*8 + s2)[20:]    \n    return (s1, s2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in6_ctop(addr):\n    #if len(addr) != 20 or not reduce(lambda x,y: x and y, map(lambda x: x in _rfc1924map, addr)):\n    if len(addr) != 20 or not all(map(lambda x: x in _rfc1924map, addr)):\n        return None\n    i = 0\n    for c in addr:\n        j = _rfc1924map.index(c)\n        i = 85*i + j\n    res = []\n    for j in range(4):\n        res.append(struct.pack(\"!I\", i%2**32))\n        i = i//(2**32)\n    res.reverse()\n    return inet_ntop(socket.AF_INET6, b\"\".join(res))", "response": "Convert an IPv6 address in Compact Representation Notation \n    ( RFC 1924 ) to printable representation ; - ) Returns None on error."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef in6_cidr2mask(m):\n    if m > 128 or m < 0:\n        raise Kamene_Exception(\"value provided to in6_cidr2mask outside [0, 128] domain (%d)\" % m)\n\n    t = []\n    for i in range(0, 4):\n        t.append(max(0, 2**32  - 2**(32-min(32, m))))\n        m -= 32\n\n    return b\"\".join([ struct.pack('!I', i) for i in t ])", "response": "Convert a length \n    value to a mask."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef in6_getnsmac(a): # return multicast Ethernet address associated with multicast v6 destination\n\n    a = struct.unpack('16B', a)[-4:]\n    mac = '33:33:'\n    mac += (':'.join(map(lambda x: '%.2x' %x, a)))\n    return mac", "response": "Returns the multicast mac address associated with provided IPv6 address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind file in current dir or system path", "response": "def _where(filename, dirs=[], env=\"PATH\"):\n    \"\"\"Find file in current dir or system path\"\"\"\n    if not isinstance(dirs, list):\n        dirs = [dirs]\n    if glob(filename):\n        return filename\n    paths = [os.curdir] + os.environ[env].split(os.path.pathsep) + dirs\n    for path in paths:\n        for match in glob(os.path.join(path, filename)):\n            if match:\n                return os.path.normpath(match)\n    raise IOError(\"File not found: %s\" % filename)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef win_find_exe(filename, installsubdir=None, env=\"ProgramFiles\"):\n    for fn in [filename, filename+\".exe\"]:\n        try:\n            if installsubdir is None:\n                path = _where(fn)\n            else:\n                path = _where(fn, dirs=[os.path.join(os.environ[env], installsubdir)])\n        except IOError:\n            path = filename\n        else:\n            break        \n    return path", "response": "Find executable in current dir system path or given ProgramFiles subdir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsniffing packets on the specified interface.", "response": "def sniff(count=0, store=1, offline=None, prn = None, lfilter=None, L2socket=None, timeout=None, stop_callback=None, *arg, **karg):\n    \"\"\"Sniff packets\nsniff([count=0,] [prn=None,] [store=1,] [offline=None,] [lfilter=None,] + L2ListenSocket args) -> list of packets\nSelect interface to sniff by setting conf.iface. Use show_interfaces() to see interface names.\n  count: number of packets to capture. 0 means infinity\n  store: wether to store sniffed packets or discard them\n    prn: function to apply to each packet. If something is returned,\n         it is displayed. Ex:\n         ex: prn = lambda x: x.summary()\nlfilter: python function applied to each packet to determine\n         if further action may be done\n         ex: lfilter = lambda x: x.haslayer(Padding)\noffline: pcap file to read packets from, instead of sniffing them\ntimeout: stop sniffing after a given time (default: None)\nL2socket: use the provided L2socket\nstop_callback: Call every loop to determine if we need\n               to stop the capture\n    \"\"\"\n    c = 0\n\n    if offline is None:\n        log_runtime.info('Sniffing on %s' % conf.iface)\n        if L2socket is None:\n            L2socket = conf.L2listen\n        s = L2socket(type=ETH_P_ALL, *arg, **karg)\n    else:\n        s = PcapReader(offline)\n\n    lst = []\n    if timeout is not None:\n        stoptime = time.time()+timeout\n    remain = None\n    while 1:\n        try:\n            if timeout is not None:\n                remain = stoptime-time.time()\n                if remain <= 0:\n                    break\n\n            if stop_callback and stop_callback():\n                break\n            try:\n                p = s.recv(MTU)\n            except PcapTimeoutElapsed:\n                continue\n            if p is None:\n                break\n            if lfilter and not lfilter(p):\n                continue\n            if store:\n                lst.append(p)\n            c += 1\n            if prn:\n                r = prn(p)\n                if r is not None:\n                    print(r)\n            if count > 0 and c >= count:\n                break\n        except KeyboardInterrupt:\n            break\n    s.close()\n    return plist.PacketList(lst,\"Sniffed\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_loopback(self, data):\n        self.name = data[\"name\"]\n        self.description = data['description']\n        self.win_index = data['win_index']\n        self.mac = data[\"mac\"]\n        self.guid = data[\"guid\"]\n        self.ip = \"127.0.0.1\"", "response": "Initialize the object for our Pseudo Loopback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the info about the network interface according to given dnet dictionary", "response": "def update(self, data):\n        \"\"\"Update info about network interface according to given dnet dictionary\"\"\"\n        self.name = data[\"name\"]\n        self.description = data['description']\n        self.win_index = data['win_index']\n        # Other attributes are optional\n        if conf.use_winpcapy:\n            self._update_pcapdata()\n        try:\n            self.ip = socket.inet_ntoa(get_if_raw_addr(data['guid']))\n        except (KeyError, AttributeError, NameError):\n            pass\n        try:\n            self.mac = data['mac']\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pcap_name(self, devname):\n\n        try:\n            pcap_name = self.data[devname].pcap_name\n        except KeyError:\n            raise ValueError(\"Unknown network interface %r\" % devname)\n        else:\n            return pcap_name", "response": "Return pcap device name for given Windows device name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef devname(self, pcap_name):\n        \n        for devname, iface in self.items():\n            if iface.pcap_name == pcap_name:\n                return iface.name\n        raise ValueError(\"Unknown pypcap network interface %r\" % pcap_name)", "response": "Return Windows device name for given pcap device name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef devname_from_index(self, if_index):\n        for devname, iface in self.items():\n            if iface.win_index == if_index:\n                return iface.name\n        raise ValueError(\"Unknown network interface index %r\" % if_index)", "response": "Return interface name from interface index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint list of available network interfaces in human readable form", "response": "def show(self, resolve_mac=True):\n        \"\"\"Print list of available network interfaces in human readable form\"\"\"\n\n        print(\"%s  %s  %s  %s\" % (\"INDEX\".ljust(5), \"IFACE\".ljust(35), \"IP\".ljust(15), \"MAC\"))\n        for iface_name in sorted(self.data.keys()):\n            dev = self.data[iface_name]\n            mac = dev.mac\n            if resolve_mac and iface_name != LOOPBACK_NAME:\n                mac = conf.manufdb._resolve_MAC(mac)\n            print(\"%s  %s  %s  %s\" % (str(dev.win_index).ljust(5), str(dev.name).ljust(35), str(dev.ip).ljust(15), mac)     )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p0f(pkt):\n    db, sig = packet2p0f(pkt)\n    if db:\n        pb = db.get_base()\n    else:\n        pb = []\n    if not pb:\n        warning(\"p0f base empty.\")\n        return []\n    #s = len(pb[0][0])\n    r = []\n    max = len(sig[4].split(\",\")) + 5\n    for b in pb:\n        d = p0f_correl(sig,b)\n        if d == max:\n            r.append((b[6], b[7], b[1] - pkt[IP].ttl))\n    return r", "response": "Passive OS fingerprinting for this TCP packet?\np0f ( packet ) -> accuracy list of guesses"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p0f_impersonate(pkt, osgenre=None, osdetails=None, signature=None,\n                    extrahops=0, mtu=1500, uptime=None):\n    \"\"\"Modifies pkt so that p0f will think it has been sent by a\nspecific OS.  If osdetails is None, then we randomly pick up a\npersonality matching osgenre. If osgenre and signature are also None,\nwe use a local signature (using p0f_getlocalsigs). If signature is\nspecified (as a tuple), we use the signature.\n\nFor now, only TCP Syn packets are supported.\nSome specifications of the p0f.fp file are not (yet) implemented.\"\"\"\n    pkt = pkt.copy()\n    #pkt = pkt.__class__(str(pkt))\n    while pkt.haslayer(IP) and pkt.haslayer(TCP):\n        pkt = pkt.getlayer(IP)\n        if isinstance(pkt.payload, TCP):\n            break\n        pkt = pkt.payload\n    \n    if not isinstance(pkt, IP) or not isinstance(pkt.payload, TCP):\n        raise TypeError(\"Not a TCP/IP packet\")\n    \n    if uptime is None:\n        uptime = random.randint(120,100*60*60*24*365)\n    \n    db = p0f_selectdb(pkt.payload.flags)\n    if osgenre:\n        pb = db.get_base()\n        if pb is None:\n            pb = []\n        #pb = filter(lambda x: x[6] == osgenre, pb)\n        pb = [ x for x in pb if x[6] == osgenre ]\n        if osdetails:\n            #pb = filter(lambda x: x[7] == osdetails, pb)\n            pb = [ x for x in pb if x[7] == osdetails ]\n    elif signature:\n        pb = [signature]\n    else:\n        pb = p0f_getlocalsigs()[db]\n    if db == p0fr_kdb:\n        # 'K' quirk <=> RST+ACK\n        if pkt.payload.flags & 0x4 == 0x4:\n            #pb = filter(lambda x: 'K' in x[5], pb)\n            pb = [ x for x in pb if 'K' in x[5] ]\n        else:\n            #pb = filter(lambda x: 'K' not in x[5], pb)\n            pb = [ x for x in pb if 'K' not in x[5] ]\n    if not pb:\n        raise Kamene_Exception(\"No match in the p0f database\")\n    pers = pb[random.randint(0, len(pb) - 1)]\n    \n    # options (we start with options because of MSS)\n    ## TODO: let the options already set if they are valid\n    options = []\n    if pers[4] != '.':\n        for opt in pers[4].split(','):\n            if opt[0] == 'M':\n                # MSS might have a maximum size because of window size\n                # specification\n                if pers[0][0] == 'S':\n                    maxmss = (2**16-1) / int(pers[0][1:])\n                else:\n                    maxmss = (2**16-1)\n                # If we have to randomly pick up a value, we cannot use\n                # kamene RandXXX() functions, because the value has to be\n                # set in case we need it for the window size value. That's\n                # why we use random.randint()\n                if opt[1:] == '*':\n                    options.append(('MSS', random.randint(1,maxmss)))\n                elif opt[1] == '%':\n                    coef = int(opt[2:])\n                    options.append(('MSS', coef*random.randint(1,maxmss/coef)))\n                else:\n                    options.append(('MSS', int(opt[1:])))\n            elif opt[0] == 'W':\n                if opt[1:] == '*':\n                    options.append(('WScale', RandByte()))\n                elif opt[1] == '%':\n                    coef = int(opt[2:])\n                    options.append(('WScale', coef*RandNum(min=1,\n                                                           max=(2**8-1)/coef)))\n                else:\n                    options.append(('WScale', int(opt[1:])))\n            elif opt == 'T0':\n                options.append(('Timestamp', (0, 0)))\n            elif opt == 'T':\n                if 'T' in pers[5]:\n                    # FIXME: RandInt() here does not work (bug (?) in\n                    # TCPOptionsField.m2i often raises \"OverflowError:\n                    # long int too large to convert to int\" in:\n                    #    oval = struct.pack(ofmt, *oval)\"\n                    # Actually, this is enough to often raise the error:\n                    #    struct.pack('I', RandInt())\n                    options.append(('Timestamp', (uptime, random.randint(1,2**32-1))))\n                else:\n                    options.append(('Timestamp', (uptime, 0)))\n            elif opt == 'S':\n                options.append(('SAckOK', ''))\n            elif opt == 'N':\n                options.append(('NOP', None))\n            elif opt == 'E':\n                options.append(('EOL', None))\n            elif opt[0] == '?':\n                if int(opt[1:]) in TCPOptions[0]:\n                    optname = TCPOptions[0][int(opt[1:])][0]\n                    optstruct = TCPOptions[0][int(opt[1:])][1]\n                    options.append((optname,\n                                    struct.unpack(optstruct,\n                                                  RandString(struct.calcsize(optstruct))._fix())))\n                else:\n                    options.append((int(opt[1:]), ''))\n            ## FIXME: qqP not handled\n            else:\n                warning(\"unhandled TCP option \" + opt)\n            pkt.payload.options = options\n    \n    # window size\n    if pers[0] == '*':\n        pkt.payload.window = RandShort()\n    elif pers[0].isdigit():\n        pkt.payload.window = int(pers[0])\n    elif pers[0][0] == '%':\n        coef = int(pers[0][1:])\n        pkt.payload.window = coef * RandNum(min=1,max=(2**16-1)/coef)\n    elif pers[0][0] == 'T':\n        pkt.payload.window = mtu * int(pers[0][1:])\n    elif pers[0][0] == 'S':\n        ## needs MSS set\n        #MSS = filter(lambda x: x[0] == 'MSS', options)\n        MSS = [ x for x in options if x[0] == 'MSS' ]\n        if not MSS:\n            raise Kamene_Exception(\"TCP window value requires MSS, and MSS option not set\")\n        pkt.payload.window = MSS[0][1] * int(pers[0][1:])\n    else:\n        raise Kamene_Exception('Unhandled window size specification')\n    \n    # ttl\n    pkt.ttl = pers[1]-extrahops\n    # DF flag\n    pkt.flags |= (2 * pers[2])\n    ## FIXME: ss (packet size) not handled (how ? may be with D quirk\n    ## if present)\n    # Quirks\n    if pers[5] != '.':\n        for qq in pers[5]:\n            ## FIXME: not handled: P, I, X, !\n            # T handled with the Timestamp option\n            if qq == 'Z': pkt.id = 0\n            elif qq == 'U': pkt.payload.urgptr = RandShort()\n            elif qq == 'A': pkt.payload.ack = RandInt()\n            elif qq == 'F':\n                #if db == p0fo_kdb:\n                #    pkt.payload.flags |= 0x20 # U\n                #else:\n                    pkt.payload.flags |= RandChoice(8, 32, 40) #P / U / PU\n            elif qq == 'D' and db != p0fo_kdb:\n                pkt /= conf.raw_layer(load=RandString(random.randint(1, 10))) # XXX p0fo.fp\n            elif qq == 'Q': pkt.payload.seq = pkt.payload.ack\n            #elif qq == '0': pkt.payload.seq = 0\n        #if db == p0fr_kdb:\n        # '0' quirk is actually not only for p0fr.fp (see\n        # packet2p0f())\n    if '0' in pers[5]:\n        pkt.payload.seq = 0\n    elif pkt.payload.seq == 0:\n        pkt.payload.seq = RandInt()\n    \n    while pkt.underlayer:\n        pkt = pkt.underlayer\n    return pkt", "response": "Modifies a packet so that it has been sent by a specific OS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bitmap2RRlist(bitmap):\n    # RFC 4034, 4.1.2. The Type Bit Maps Field\n\n    RRlist = []\n\n    while bitmap:\n\n        if len(bitmap) < 2:\n            warning(\"bitmap too short (%i)\" % len(bitmap))\n            return\n   \n        #window_block = ord(bitmap[0]) # window number\n        window_block = (bitmap[0]) # window number\n        offset = 256*window_block # offset of the Ressource Record\n        #bitmap_len = ord(bitmap[0]) # length of the bitmap in bytes\n        bitmap_len = (bitmap[1]) # length of the bitmap in bytes\n        \n        if bitmap_len <= 0 or bitmap_len > 32:\n            warning(\"bitmap length is no valid (%i)\" % bitmap_len)\n            return\n        \n        tmp_bitmap = bitmap[2:2+bitmap_len]\n        \n        # Let's compare each bit of tmp_bitmap and compute the real RR value\n        for b in range(len(tmp_bitmap)):\n            v = 128\n            for i in range(8):\n                #if ord(tmp_bitmap[b]) & v:\n                if (tmp_bitmap[b]) & v:\n                    # each of the RR is encoded as a bit\n                    RRlist += [ offset + b*8 + i ] \n                v = v >> 1\n\n# Next block if any\n        bitmap = bitmap[2+bitmap_len:]\n\n    return RRlist", "response": "Decode the Type Bit Maps field of the NSEC Resource Record into an integer list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes a list of integers representing Resource Records to a bitmap field", "response": "def RRlist2bitmap(lst):\n    \"\"\"\n    Encode a list of integers representing Resource Records to a bitmap field \n    used in the NSEC Resource Record.\n    \"\"\"\n    # RFC 4034, 4.1.2. The Type Bit Maps Field\n\n    import math\n\n    bitmap = b\"\"\n    lst = list(set(lst))\n    lst.sort()\n    \n    #lst = filter(lambda x: x <= 65535, lst)\n    #lst = map(lambda x: abs(x), lst)\n    lst = [ abs(x) for x in lst if x<= 65535 ]\n\n    # number of window blocks\n    max_window_blocks = int(math.ceil(lst[-1] / 256.))\n    min_window_blocks = int(math.floor(lst[0] / 256.))\n    if min_window_blocks == max_window_blocks:\n        max_window_blocks += 1\n\n    for wb in range(min_window_blocks, max_window_blocks+1):\n        # First, filter out RR not encoded in the current window block\n        # i.e. keep everything between 256*wb <= 256*(wb+1)\n        #rrlist = filter(lambda x: 256*wb <= x and x < 256*(wb+1), lst)\n        rrlist = [ x for x in lst if 256*wb <= x and x < 256*(wb+1) ]\n        rrlist.sort()\n        if rrlist == []:\n            continue\n     \n        # Compute the number of bytes used to store the bitmap\n        if rrlist[-1] == 0: # only one element in the list\n            bs = 1\n        else:\n            max = rrlist[-1] - 256*wb\n            #bs = int(math.ceil(max / 8)) + 1  # use at least 1 byte\n            bs = int(max // 8) + 1  # use at least 1 byte\n        if bs > 32: # Don't encode more than 256 bits / values\n            bs = 32\n\n        bitmap += struct.pack(\"B\", wb)\n        bitmap += struct.pack(\"B\", bs)\n\n        # Generate the bitmap\n        for tmp in range(bs):\n            v = 0\n            # Remove out of range Ressource Records\n            #tmp_rrlist = filter(lambda x: 256*wb+8*tmp <= x and x < 256*wb+8*tmp+8, rrlist)\n            tmp_rrlist = [ x for x in rrlist if 256*wb+8*tmp <= x and x < 256*wb+8*tmp+8 ]\n            if not tmp_rrlist == []:\n                # 1. rescale to fit into 8 bits\n                tmp_rrlist = map(lambda x: (x-256*wb)-(tmp*8), tmp_rrlist)\n                # 2. x gives the bit position ; compute the corresponding value\n                tmp_rrlist = map(lambda x: 2**(7-x) , tmp_rrlist)\n                # 3. sum everything\n                #v = reduce(lambda x,y: x+y, tmp_rrlist)\n                v = sum(tmp_rrlist)\n            bitmap += struct.pack(\"B\", v)\n   \n    return bitmap"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef IE_Dispatcher(s):\n\n  if len(s) < 1:\n    return Raw(s)\n\n  # Get the IE type\n  ietype = ord(s[0])\n  cls = ietypecls.get(ietype, Raw)\n\n  # if ietype greater than 128 are TLVs\n  if cls == Raw and ietype & 128 == 128:\n    cls = IE_NotImplementedTLV\n\n  return cls(s)", "response": "Choose the correct Information Element class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nsummary(self,prn=None, lfilter=None):\n        for i, p in enumerate(self.res):\n            if lfilter is not None:\n                if not lfilter(p):\n                    continue\n            print(conf.color_theme.id(i,fmt=\"%04i\"), end = \" \")\n            if prn is None:\n                print(self._elt2sum(p))\n            else:\n                print(prn(p))", "response": "prints a summary of each packet with the packet s number\nprn and n\nlfilter functions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter(self, func):\n        return self.__class__(list(filter(func,self.res)),\n                              name=\"filtered %s\"%self.listname)", "response": "Returns a new packet list filtered by a truth function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self, f, lfilter=None,**kargs):\n\n        return plt.plot([ f(i) for i in self.res if not lfilter or lfilter(i) ], **kargs)", "response": "Applies a function to each packet to get a value that will be plotted with matplotlib. A matplotlib object is returned by matplotlib."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diffplot(self, f, delay=1, lfilter=None, **kargs):\n\n        return plt.plot([ f(i, j) for i in self.res[:-delay] for j in self.res[delay:] if not lfilter or (lfilter(i) and lfilter(j))], \n            **kargs)", "response": "Plots a function f on the two couples."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses a function that returns a label and a value for this label then plots all the values label by label", "response": "def multiplot(self, f, lfilter=None, **kargs):\n        \"\"\"Uses a function that returns a label and a value for this label, then plots all the values label by label\"\"\"\n\n        d = defaultdict(list)\n        for i in self.res:\n            if lfilter and not lfilter(i):\n                continue \n            k, v = f(i)\n            d[k].append(v)\n\n        figure = plt.figure()\n        ax = figure.add_axes(plt.axes())\n        for i in d:\n            ax.plot(d[i], **kargs)\n        return figure"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef conversations(self, getsrcdst=None, draw = True, **kargs):\n        if getsrcdst is None:\n            getsrcdst = lambda x:(x['IP'].src, x['IP'].dst)\n        conv = {}\n        for p in self.res:\n            p = self._elt2pkt(p)\n            try:\n                c = getsrcdst(p)\n            except:\n                #XXX warning()\n                continue\n            conv[c] = conv.get(c,0)+1\n\n        if NETWORKX: # networkx is available\n            gr = nx.DiGraph()\n            for s,d in conv:\n                if s not in gr:\n                    gr.add_node(s)\n                if d not in gr:\n                    gr.add_node(d)\n                gr.add_edge(s, d)\n            if draw:\n                return do_graph(gr, **kargs)\n            else:\n                return gr\n        else:\n            gr = 'digraph \"conv\" {\\n'\n            for s,d in conv:\n                gr += '\\t \"%s\" -> \"%s\"\\n' % (s,d)\n            gr += \"}\\n\"        \n            return do_graph(gr, **kargs)", "response": "Graphes a conversations between source and destination IP\n            and displays it in a graphviz format"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef afterglow(self, src=None, event=None, dst=None, **kargs):\n        if src is None:\n            src = lambda x: x['IP'].src\n        if event is None:\n            event = lambda x: x['IP'].dport\n        if dst is None:\n            dst = lambda x: x['IP'].dst\n        sl = {}\n        el = {}\n        dl = {}\n        for i in self.res:\n            try:\n                s,e,d = src(i),event(i),dst(i)\n                if s in sl:\n                    n,l = sl[s]\n                    n += 1\n                    if e not in l:\n                        l.append(e)\n                    sl[s] = (n,l)\n                else:\n                    sl[s] = (1,[e])\n                if e in el:\n                    n,l = el[e]\n                    n+=1\n                    if d not in l:\n                        l.append(d)\n                    el[e] = (n,l)\n                else:\n                    el[e] = (1,[d])\n                dl[d] = dl.get(d,0)+1\n            except:\n                continue\n\n        import math\n        def normalize(n):\n            return 2+math.log(n)/4.0\n\n        def minmax(x):\n            m,M = min(x),max(x)\n            if m == M:\n                m = 0\n            if M == 0:\n                M = 1\n            return m,M\n\n        #mins,maxs = minmax(map(lambda (x,y): x, sl.values()))\n        mins,maxs = minmax([ a[0] for a in sl.values()])\n        #mine,maxe = minmax(map(lambda (x,y): x, el.values()))\n        mine,maxe = minmax([ a[0] for a in el.values()])\n        mind,maxd = minmax(dl.values())\n    \n        gr = 'digraph \"afterglow\" {\\n\\tedge [len=2.5];\\n'\n\n        gr += \"# src nodes\\n\"\n        for s in sl:\n            n,l = sl[s]; n = 1+(n-mins)/(maxs-mins)\n            gr += '\"src.%s\" [label = \"%s\", shape=box, fillcolor=\"#FF0000\", style=filled, fixedsize=1, height=%.2f,width=%.2f];\\n' % (repr(s),repr(s),n,n)\n        gr += \"# event nodes\\n\"\n        for e in el:\n            n,l = el[e]; n = n = 1+(n-mine)/(maxe-mine)\n            gr += '\"evt.%s\" [label = \"%s\", shape=circle, fillcolor=\"#00FFFF\", style=filled, fixedsize=1, height=%.2f, width=%.2f];\\n' % (repr(e),repr(e),n,n)\n        for d in dl:\n            n = dl[d]; n = n = 1+(n-mind)/(maxd-mind)\n            gr += '\"dst.%s\" [label = \"%s\", shape=triangle, fillcolor=\"#0000ff\", style=filled, fixedsize=1, height=%.2f, width=%.2f];\\n' % (repr(d),repr(d),n,n)\n\n        gr += \"###\\n\"\n        for s in sl:\n            n,l = sl[s]\n            for e in l:\n                gr += ' \"src.%s\" -> \"evt.%s\";\\n' % (repr(s),repr(e)) \n        for e in el:\n            n,l = el[e]\n            for d in l:\n                gr += ' \"evt.%s\" -> \"dst.%s\";\\n' % (repr(e),repr(d)) \n            \n        gr += \"}\"\n        return do_graph(gr, **kargs)", "response": "Experimental clone attempt of http://sourceforge. net / projects / afterglow\n        each datum is reduced as src event -> dst and the data are graphed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a multipage poscript file with a psdump of every packet", "response": "def psdump(self, filename = None, **kargs):\n        \"\"\"Creates a multipage poscript file with a psdump of every packet\n        filename: name of the file to write to. If empty, a temporary file is used and\n                  conf.prog.psreader is called\"\"\"\n        d = self._dump_document(**kargs)\n        if filename is None:\n            filename = get_temp_file(autoext=\".ps\")\n            d.writePSfile(filename)\n            subprocess.Popen([conf.prog.psreader, filename+\".ps\"])\n        else:\n            d.writePSfile(filename)\n        print"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a PDF file with a psdump of every packet", "response": "def pdfdump(self, filename = None, **kargs):\n        \"\"\"Creates a PDF file with a psdump of every packet\n        filename: name of the file to write to. If empty, a temporary file is used and\n                  conf.prog.pdfreader is called\"\"\"\n        d = self._dump_document(**kargs)\n        if filename is None:\n            filename = get_temp_file(autoext=\".pdf\")\n            d.writePDFfile(filename)\n            subprocess.Popen([conf.prog.pdfreader, filename+\".pdf\"])\n        else:\n            d.writePDFfile(filename)\n        print"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a summary of each SndRcv packet pair pair prn lfilter function to apply to each packet pair and print the summary of each SndRcv packet pair pair lfilter", "response": "def summary(self, prn=None, lfilter=None):\n        \"\"\"prints a summary of each SndRcv packet pair\nprn:     function to apply to each packet pair instead of lambda s, r: \"%s ==> %s\" % (s.summary(),r.summary())\nlfilter: truth function to apply to each packet pair to decide whether it will be displayed\"\"\"\n        for s, r in self.res:\n            if lfilter is not None:\n                if not lfilter(s, r):\n                    continue\n            if prn is None:\n                print(self._elt2sum((s, r)))\n            else:\n                print(prn(s, r))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints a summary of each SndRcv packet pair with the pair s number prn and number lfilter functions to apply to each SndRcv packet pair with the pair s number prn and number lfilter respectively", "response": "def nsummary(self,prn=None, lfilter=None):\n        \"\"\"prints a summary of each SndRcv packet pair with the pair's number\nprn:     function to apply to each packet pair instead of lambda s, r: \"%s ==> %s\" % (s.summary(),r.summary()) \nlfilter: truth function to apply to each packet pair to decide whether it will be displayed\"\"\"\n        for i, (s, r) in enumerate(self.res):\n            if lfilter is not None:\n                if not lfilter(s, r):\n                    continue\n            print(conf.color_theme.id(i,fmt=\"%04i\"), end = \" \")\n            if prn is None:\n                print(self._elt2sum((s, r)))\n            else:\n                print(prn(s, r))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, func):\n        return self.__class__( [ i for i in self.res if func(*i) ], name='filtered %s'%self.listname)", "response": "Returns a SndRcv list filtered by a truth function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _PPIGuessPayloadClass(p, **kargs):\r\n    if len(p) >= 4:\r\n        t,pfh_len = struct.unpack(\"<HH\", p[:4])\r\n        # Find out if the value t is in the dict _ppi_types.\r\n        # If not, return the default TLV class\r\n        cls = getPPIType(t, \"default\")\r\n        pfh_len += 4\r\n        out = cls(p[:pfh_len], **kargs)\r\n        if (out.payload):\r\n            out.payload = conf.raw_layer(out.payload.load)\r\n            if (len(p) > pfh_len):\r\n                out.payload.payload = conf.padding_layer(p[pfh_len:])\r\n        elif (len(p) > pfh_len):\r\n            out.payload = conf.padding_layer(p[pfh_len:])\r\n        \r\n    else:\r\n        out = conf.raw_layer(p, **kargs)\r\n    return out", "response": "This function tries to extract the appropriate class from the PacketListField. If the PacketListField is not in the dict _ppi_types then it will return the default class. If the PacketListField is not in the dict _ppi_types then it will return the default class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef i2b(self, pkt, x):\n        if type(x) is str:\n          x = bytes([ ord(i) for i in x ])\n        return x", "response": "Convert internal value to internal value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a volatile object whose value is both random and suitable for this field", "response": "def randval(self):\n        \"\"\"Return a volatile object whose value is both random and suitable for this field\"\"\"\n        fmtt = self.fmt[-1]\n        if fmtt in \"BHIQ\":\n            return {\"B\":RandByte,\"H\":RandShort,\"I\":RandInt, \"Q\":RandLong}[fmtt]()\n        elif fmtt == \"s\":\n            if self.fmt[0] in \"0123456789\":\n                l = int(self.fmt[:-1])\n            else:\n                l = int(self.fmt[1:-1])\n            return RandBin(l)\n        else:\n            warning(\"no random class for [%s] (fmt=%s).\" % (self.name, self.fmt))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef srcanloop(pkts, *args, **kargs):\n    return sendrecv.__sr_loop(srcan, pkts, *args, **kargs)", "response": "Send a packet at can layer in loop and print the answer each time\nsrloop"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef str2bytes(x):\n  if type(x) is bytes:\n    return x\n  elif type(x) is str:\n    return bytes([ ord(i) for i in x ])\n  else:\n    return str2bytes(str(x))", "response": "Convert input argument to bytes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the IPv4 Address is an RFC 1918 private address.", "response": "def is_private_addr(x):\n    \"\"\"Returns True if the IPv4 Address is an RFC 1918 private address.\"\"\"\n    paddrs = ['10.0.0.0/8', '172.16.0.0/12', '192.168.0.0/16']\n    found = False\n    for ipr in paddrs:\n        try:\n            if ipaddress.ip_address(x) in ipaddress.ip_network(ipr):\n                found = True\n                continue\n        except:\n            break\n    return found"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a generator that generates the color of the given quantities forever.", "response": "def colgen(*lstcol,**kargs):\n    \"\"\"Returns a generator that mixes provided quantities forever\n    trans: a function to convert the three arguments into a color. lambda x,y,z:(x,y,z) by default\"\"\"\n    if len(lstcol) < 2:\n        lstcol *= 2\n    trans = kargs.get(\"trans\", lambda x,y,z: (x,y,z))\n    while 1:\n        for i in range(len(lstcol)):\n            for j in range(len(lstcol)):\n                for k in range(len(lstcol)):\n                    if i != j or j != k or k != i:\n                        yield trans(lstcol[(i+j)%len(lstcol)],lstcol[(j+k)%len(lstcol)],lstcol[(k+i)%len(lstcol)])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef corrupt_bytes(s, p=0.01, n=None):\n    s = bytes(s)\n    s = array.array(\"B\",s)\n    l = len(s)\n    if n is None:\n        n = max(1,int(l*p))\n    for i in random.sample(range(l), n):\n        s[i] = (s[i]+random.randint(1,255))%256\n    return s.tobytes()", "response": "Corrupt a given percentage or number of bytes from bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrpcap(filename, pkt, *args, **kargs):\n    with PcapWriter(filename, *args, **kargs) as pcap:\n        pcap.write(pkt)", "response": "Write a list of packets to a pcap file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rdpcap(filename, count=-1):\n    with PcapReader(filename) as pcap:\n        return pcap.read_all(count=count)", "response": "Read a pcap file and return a list of count packets"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun wireshark on a list of packets", "response": "def wireshark(pktlist, *args):\n    \"\"\"Run wireshark on a list of packets\"\"\"\n    fname = get_temp_file()\n    wrpcap(fname, pktlist)\n    subprocess.Popen([conf.prog.wireshark, \"-r\", fname] + list(args))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun tshark to decode and display the packet.", "response": "def tdecode(pkt, *args):\n    \"\"\"Run tshark to decode and display the packet. If no args defined uses -V\"\"\"\n    if not args:\n      args = [ \"-V\" ]\n    fname = get_temp_file()\n    wrpcap(fname,[pkt])\n    subprocess.call([\"tshark\", \"-r\", fname] + list(args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hexedit(x):\n    x = bytes(x)\n    fname = get_temp_file()\n    with open(fname,\"wb\") as f:\n      f.write(x)\n    subprocess.call([conf.prog.hexedit, fname])\n    with open(fname, \"rb\") as f:\n      x = f.read()\n    return x", "response": "Run external hex editor on a packet or bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a single packet from the file", "response": "def read_packet(self, size=MTU):\n        \"\"\"return a single packet read from the file\n        bytes, (sec, #timestamp seconds\n                usec, #timestamp microseconds\n                wirelen) #actual length of packet\n        returns None when no more packets are available\n        \"\"\"\n        hdr = self.f.read(16)\n        if len(hdr) < 16:\n            return None\n        sec,usec,caplen,wirelen = struct.unpack(self.endian+\"IIII\", hdr)\n        s = self.f.read(caplen)[:MTU]\n        return s, 0, (sec,usec,wirelen)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naccept a single packet or a list of packets to be written to the dumpfile", "response": "def write(self, pkt):\n        \"\"\"accepts a either a single packet or a list of packets\n        to be written to the dumpfile\n        \"\"\"\n        if not self.header_present:\n            self._write_header(pkt)\n        if type(pkt) is bytes:\n            self._write_packet(pkt)\n        else:\n            for p in pkt:\n                self._write_packet(p)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_packet(self, packet, sec=None, usec=None, caplen=None, wirelen=None):\n        if caplen is None:\n            caplen = len(packet)\n        if wirelen is None:\n            wirelen = caplen\n        if sec is None or usec is None:\n            t=time.time()\n            it = int(t)\n            if sec is None:\n                sec = it\n            if usec is None:\n                usec = int(round((t-it)*1000000))\n        self.f.write(struct.pack(self.endian+\"IIII\", sec, usec, caplen, wirelen))\n        self.f.write(packet)\n        if self.gz and self.sync:\n            self.f.flush()", "response": "writes a single packet to the pcap file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the second and micro - second timestamp components for a packet.", "response": "def get_packet_time(self, pkt):\n        \"\"\"Return the second and micro-second timestamp components for a packet.\"\"\"\n        if pkt.sent_time:\n          t = pkt.sent_time\n          sec = int(t)\n        else:\n          t = pkt.time\n          sec = int(t)\n        usec = int(round((t-sec)*1000000))\n        return (sec,usec)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a single or list of packets to the dumpfile", "response": "def write(self, pkt):\n        \"\"\"accepts a either a single packet or a list of packets\n        to be written to the dumpfile\n        \"\"\"\n        if not self.header_present:\n            self._write_header(pkt)\n        if isinstance(pkt, BasePacket):\n          self._write_packet(pkt)\n        else:\n            for p in pkt:\n                self._write_packet(p)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend and receive using a bluetooth socket", "response": "def srbt(peer, pkts, inter=0.1, *args, **kargs):\n    \"\"\"send and receive using a bluetooth socket\"\"\"\n    s = conf.BTsocket(peer=peer)\n    a,b = sndrcv(s,pkts,inter=inter,*args,**kargs)\n    s.close()\n    return a,b"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_if_raw_addr6(iff):\n    #r = filter(lambda x: x[2] == iff and x[1] == IPV6_ADDR_GLOBAL, in6_getifaddr())\n    r = [ x for x in in6_getifaddr() if x[2] == iff and x[1] == IPV6_ADDR_GLOBAL]\n    if len(r) == 0:\n        return None\n    else:\n        r = r[0][0] \n    return inet_pton(socket.AF_INET6, r)", "response": "Returns the main unicast address associated with provided \n    interface in network format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Cipher object for this algo.", "response": "def new_cipher(self, key, iv, digest=None):\n        \"\"\"\n        @param key:    the secret key, a byte string\n        @param iv:     the initialization vector, a byte string. Used as the\n                       initial nonce in counter mode\n        @param digest: also known as tag or icv. A byte string containing the\n                       digest of the encrypted data. Only use this during\n                       decryption!\n\n        @return:    an initialized cipher object for this algo\n        \"\"\"\n        if type(key) is str:\n            key = key.encode('ascii')\n        if self.is_aead and digest is not None:\n            # With AEAD, the mode needs the digest during decryption.\n            return Cipher(\n                self.cipher(key),\n                self.mode(iv, digest, len(digest)),\n                default_backend(),\n            )\n        else:\n            return Cipher(\n                self.cipher(key),\n                self.mode(iv),\n                default_backend(),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pad(self, esp):\n        # 2 extra bytes for padlen and nh\n        data_len = len(esp.data) + 2\n\n        # according to the RFC4303, section 2.4. Padding (for Encryption)\n        # the size of the ESP payload must be a multiple of 32 bits\n        align = _lcm(self.block_size, 4)\n\n        # pad for block size\n        esp.padlen = -data_len % align\n\n        # padding must be an array of bytes starting from 1 to padlen\n        esp.padding = ''\n        for b in range(1, esp.padlen + 1):\n            esp.padding += bytes([b])\n\n        # If the following test fails, it means that this algo does not comply\n        # with the RFC\n        payload_len = len(esp.iv) + len(esp.data) + len(esp.padding) + 2\n        if payload_len % 4 != 0:\n            raise ValueError('The size of the ESP data is not aligned to 32 bits after padding.')\n\n        return esp", "response": "Add the correct amount of padding so that the data to encrypt is\n        exactly a multiple of the algorithm s block size."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decrypt(self, esp, key, icv_size=None):\n        if icv_size is None:\n            icv_size = self.icv_size if self.is_aead else 0\n\n        iv = esp.data[:self.iv_size]\n        data = esp.data[self.iv_size:len(esp.data) - icv_size]\n        icv = esp.data[len(esp.data) - icv_size:]\n\n        if self.cipher:\n            cipher = self.new_cipher(key, iv, icv)\n            decryptor = cipher.decryptor()\n\n            if self.is_aead:\n                # Tag value check is done during the finalize method\n                decryptor.authenticate_additional_data(\n                    struct.pack('!LL', esp.spi, esp.seq)\n                )\n\n            try:\n                data = decryptor.update(data) + decryptor.finalize()\n            except InvalidTag as err:\n                raise IPSecIntegrityError(err)\n\n        # extract padlen and nh\n        padlen = (data[-2])\n        nh = data[-1]\n\n        # then use padlen to determine data and padding\n        data = data[:len(data) - padlen - 2]\n        padding = data[len(data) - padlen - 2: len(data) - 2]\n\n        return _ESPPlain(spi=esp.spi,\n                        seq=esp.seq,\n                        iv=iv,\n                        data=data,\n                        padding=padding,\n                        padlen=padlen,\n                        nh=nh,\n                        icv=icv)", "response": "Decrypt an ESP packet with the specified secret key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef igmpize(self, ip=None, ether=None):\n\n# The rules are:\n#   1.  the Max Response time is meaningful only in Membership Queries and should be zero \n#       otherwise (RFC 2236, section 2.2)\n\n    if (self.type != 0x11):         #rule 1\n      self.mrtime = 0\n      \n    if (self.adjust_ip(ip) == True):\n      if (self.adjust_ether(ip, ether) == True): return True\n    return False", "response": "This function will attempt to fixup the IP and Ethernet headers for a IGMP packet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef channelModeModify(VgcsTargetModeIdentication_presence=0,\n                      MultiRateConfiguration_presence=0):\n    \"\"\"CHANNEL MODE MODIFY Section 9.1.5\"\"\"\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x8)  # 0001000\n    c = ChannelDescription2()\n    d = ChannelMode()\n    packet = a / b / c / d\n    if VgcsTargetModeIdentication is 1:\n        e = VgcsTargetModeIdenticationHdr(ieiVTMI=0x01, eightBitVTMI=0x0)\n        packet = packet / e\n    if MultiRateConfiguration is 1:\n        f = MultiRateConfigurationHdr(ieiMRC=0x03, eightBitMRC=0x0)\n        packet = packet / f\n    return packet", "response": "CHANNEL MODE MODIFY Section 9. 1. 5"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef channelModeModifyAcknowledge():\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x17)  # 00010111\n    c = ChannelDescription2()\n    d = ChannelMode()\n    packet = a / b / c / d\n    return packet", "response": "CHANNEL MODE MODIFY ACKNOWLEDGE Section 9. 1. 6"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchannels RELEASE Section 9. 1. 7", "response": "def channelRelease(BaRange_presence=0, GroupChannelDescription_presence=0,\n                   GroupCipherKeyNumber_presence=0, GprsResumption_presence=0,\n                   BaListPref_presence=0):\n    \"\"\"CHANNEL RELEASE  Section 9.1.7\"\"\"\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0xD)  # 00001101\n    c = RrCause()\n    packet = a / b / c\n    if BaRange_presence is 1:\n        d = BaRangeHdr(ieiBR=0x73, eightBitBR=0x0)\n        packet = packet / d\n    if GroupChannelDescription_presence is 1:\n        e = GroupChannelDescriptionHdr(ieiGCD=0x74, eightBitGCD=0x0)\n        packet = packet / e\n    if GroupCipherKeyNumber_presence is 1:\n        f = GroupCipherKeyNumber(ieiGCKN=0x8)\n        packet = packet / f\n    if GprsResumption_presence is 1:\n        g = GprsResumptionHdr(ieiGR=0xC, eightBitGR=0x0)\n        packet = packet / g\n    if BaListPref_presence is 1:\n        h = BaListPrefHdr(ieiBLP=0x75, eightBitBLP=0x0)\n        packet = packet / h\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cipheringModeCommand():\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x35)  # 00110101\n    c = RrCause()\n #d=cipherModeSetting()\n #e=cipherResponse()\n # FIX\n    d = CipherModeSettingAndcipherResponse()\n    packet = a / b / c / d\n    return packet", "response": "CIPHERING MODE COMMAND Section 9. 1. 9"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cipheringModeComplete(MobileId_presence=0):\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x32)  # 00110010\n    packet = a / b\n    if MobileId_presence is 1:\n        c = MobileIdHdr(ieiMI=0x17, eightBitMI=0x0)\n        packet = packet / c\n    return packet", "response": "CIPHERING MODE COMPLETE Section 9. 1. 10"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef classmarkEnquiry():\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x13)  # 00010011\n    packet = a / b\n    return packet", "response": "CLASSMARK ENQUIRY Section 9. 1. 12"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handoverCommand(SynchronizationIndication_presence=0,\n                    FrequencyShortList_presence=0, FrequencyList_presence=0,\n                    CellChannelDescription_presence=0,\n                    MultislotAllocation_presence=0,\n                    ChannelMode_presence=0, ChannelMode_presence1=0,\n                    ChannelMode_presence2=0,\n                    ChannelMode_presence3=0, ChannelMode_presence4=0,\n                    ChannelMode_presence5=0,\n                    ChannelMode_presence6=0, ChannelMode_presence7=0,\n                    ChannelDescription_presence1=0, ChannelMode2_presence=0,\n                    FrequencyChannelSequence_presence=0,\n                    MobileAllocation_presence=0,\n                    StartingTime_presence=0, TimeDifference_presence=0,\n                    TimingAdvance_presence=0,\n                    FrequencyShortList_presence1=0,\n                    FrequencyList_presence1=0,\n                    ChannelDescription2_presence=0,\n                    ChannelDescription_presence2=0,\n                    FrequencyChannelSequence_presence1=0,\n                    MobileAllocation_presence1=0,\n                    CipherModeSetting_presence=0,\n                    VgcsTargetModeIdentication_presence=0,\n                    MultiRateConfiguration_presence=0):\n    \"\"\"HANDOVER COMMAND Section 9.1.15\"\"\"\n    name = \"Handover Command\"\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x2b)  # 00101011\n    c = CellDescription()\n    d = ChannelDescription2()\n    e = HandoverReference()\n    f = PowerCommandAndAccessType()\n    packet = a / b / c / d / e / f\n    if SynchronizationIndication_presence is 1:\n        g = SynchronizationIndicationHdr(ieiSI=0xD, eightBitSI=0x0)\n        packet = packet / g\n    if FrequencyShortList_presence is 1:\n        h = FrequencyShortListHdr(ieiFSL=0x02)\n        packet = packet / h\n    if FrequencyList_presence is 1:\n        i = FrequencyListHdr(ieiFL=0x05, eightBitFL=0x0)\n        packet = packet / i\n    if CellChannelDescription_presence is 1:\n        j = CellChannelDescriptionHdr(ieiCCD=0x62, eightBitCCD=0x0)\n        packet = packet / j\n    if MultislotAllocation_presence is 1:\n        k = MultislotAllocationHdr(ieiMSA=0x10, eightBitMSA=0x0)\n        packet = packet / k\n    if ChannelMode_presence is 1:\n        l = ChannelModeHdr(ieiCM=0x63, eightBitCM=0x0)\n        packet = packet / l\n    if ChannelMode_presence1 is 1:\n        m = ChannelModeHdr(ieiCM=0x11, eightBitCM=0x0)\n        packet = packet / m\n    if ChannelMode_presence2 is 1:\n        n = ChannelModeHdr(ieiCM=0x13, eightBitCM=0x0)\n        packet = packet / n\n    if ChannelMode_presence3 is 1:\n        o = ChannelModeHdr(ieiCM=0x14, eightBitCM=0x0)\n        packet = packet / o\n    if ChannelMode_presence4 is 1:\n        p = ChannelModeHdr(ieiCM=0x15, eightBitCM=0x0)\n        packet = packet / p\n    if ChannelMode_presence5 is 1:\n        q = ChannelModeHdr(ieiCM=0x16, eightBitCM=0x0)\n        packet = packet / q\n    if ChannelMode_presence6 is 1:\n        r = ChannelModeHdr(ieiCM=0x17, eightBitCM=0x0)\n        packet = packet / r\n    if ChannelMode_presence7 is 1:\n        s = ChannelModeHdr(ieiCM=0x18, eightBitCM=0x0)\n        packet = packet / s\n    if ChannelDescription_presence1 is 1:\n        s1 = ChannelDescriptionHdr(ieiCD=0x64, eightBitCD=0x0)\n        packet = packet / s1\n    if ChannelMode2_presence is 1:\n        t = ChannelMode2Hdr(ieiCM2=0x66, eightBitCM2=0x0)\n        packet = packet / t\n    if FrequencyChannelSequence_presence is 1:\n        u = FrequencyChannelSequenceHdr(ieiFCS=0x69, eightBitFCS=0x0)\n        packet = packet / u\n    if MobileAllocation_presence is 1:\n        v = MobileAllocationHdr(ieiMA=0x72, eightBitMA=0x0)\n        packet = packet / v\n    if StartingTime_presence is 1:\n        w = StartingTimeHdr(ieiST=0x7C, eightBitST=0x0)\n        packet = packet / w\n    if TimeDifference_presence is 1:\n        x = TimeDifferenceHdr(ieiTD=0x7B, eightBitTD=0x0)\n        packet = packet / x\n    if TimingAdvance_presence is 1:\n        y = TimingAdvanceHdr(ieiTA=0x7D, eightBitTA=0x0)\n        packet = packet / y\n    if FrequencyShortList_presence1 is 1:\n        z = FrequencyShortListHdr(ieiFSL=0x12)\n        packet = packet / z\n    if FrequencyList_presence1 is 1:\n        aa = FrequencyListHdr(ieiFL=0x19, eightBitFL=0x0)\n        packet = packet / aa\n    if ChannelDescription2_presence is 1:\n        ab = ChannelDescription2Hdr(ieiCD2=0x1C, eightBitCD2=0x0)\n        packet = packet / ab\n    if ChannelDescription_presence2 is 1:\n        ac = ChannelDescriptionHdr(ieiCD=0x1D, eightBitCD=0x0)\n        packet = packet / ac\n    if FrequencyChannelSequence_presence1 is 1:\n        ad = FrequencyChannelSequenceHdr(ieiFCS=0x1E, eightBitFCS=0x0)\n        packet = packet / ad\n    if MobileAllocation_presence1 is 1:\n        ae = MobileAllocationHdr(ieiMA=0x21, eightBitMA=0x0)\n        packet = packet / ae\n    if CipherModeSetting_presence is 1:\n        af = CipherModeSettingHdr(ieiCMS=0x9, eightBitCMS=0x0)\n        packet = packet / af\n    if VgcsTargetModeIdentication_presence is 1:\n        ag = VgcsTargetModeIdenticationHdr(ieiVTMI=0x01, eightBitVTMI=0x0)\n        packet = packet / ag\n    if MultiRateConfiguration_presence is 1:\n        ah = MultiRateConfigurationHdr(ieiMRC=0x03, eightBitMRC=0x0)\n        packet = packet / ah\n    return packet", "response": "HANDOVER COMMAND Section 9. 1. 15"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handoverComplete(MobileTimeDifference_presence=0):\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x2c)  # 00101100\n    c = RrCause()\n    packet = a / b / c\n    if MobileTimeDifference_presence is 1:\n        d = MobileTimeDifferenceHdr(ieiMTD=0x77, eightBitMTD=0x0)\n        packet = packet / d\n    return packet", "response": "HANDOVER COMPLETE Section 9. 1. 16"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef immediateAssignment(ChannelDescription_presence=0,\n                        PacketChannelDescription_presence=0,\n                        StartingTime_presence=0):\n    \"\"\"IMMEDIATE ASSIGNMENT Section 9.1.18\"\"\"\n    a = L2PseudoLength()\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x3F)  # 00111111\n    d = PageModeAndDedicatedModeOrTBF()\n    packet = a / b / c / d\n    if ChannelDescription_presence is 1:\n        f = ChannelDescription()\n        packet = packet / f\n    if PacketChannelDescription_presence is 1:\n        g = PacketChannelDescription()\n        packet = packet / g\n    h = RequestReference()\n    i = TimingAdvance()\n    j = MobileAllocation()\n    packet = packet / h / i / j\n    if StartingTime_presence is 1:\n        k = StartingTimeHdr(ieiST=0x7C, eightBitST=0x0)\n        packet = packet / k\n    l = IaRestOctets()\n    packet = packet / l\n    return packet", "response": "IMMEDIATE ASSIGNMENT Section 9. 1. 18"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef immediateAssignmentExtended(StartingTime_presence=0):\n    a = L2PseudoLength()\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x39)  # 00111001\n    d = PageModeAndSpareHalfOctets()\n    f = ChannelDescription()\n    g = RequestReference()\n    h = TimingAdvance()\n    i = MobileAllocation()\n    packet = a / b / c / d / f / g / h / i\n    if StartingTime_presence is 1:\n        j = StartingTimeHdr(ieiST=0x7C, eightBitST=0x0)\n        packet = packet / j\n    k = IaxRestOctets()\n    packet = packet / k\n    return packet", "response": "IMMEDIATE ASSIGNMENT EXTENDED Section 9. 1. 19"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef immediateAssignmentReject():\n    a = L2PseudoLength(l2pLength=0x13)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x3a)  # 00111010\n    d = PageModeAndSpareHalfOctets()\n    f = RequestReference()\n    g = WaitIndication()\n    h = RequestReference()\n    i = WaitIndication()\n    j = RequestReference()\n    k = WaitIndication()\n    l = RequestReference()\n    m = WaitIndication()\n    n = IraRestOctets()\n    packet = a / b / c / d / f / g / h / i / j / k / l / m / n\n    return packet", "response": "IMMEDIATE ASSIGNMENT REJECT Section 9. 1. 20"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notificationNch():\n    a = L2PseudoLength(l2pLength=0x01)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x20)  # 00100000\n    d = NtNRestOctets()\n    packet = a / b / c / d\n    return packet", "response": "NOTIFICATION NCH Section 9. 1. 21b"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef notificationResponse():\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x26)  # 00100110\n    c = MobileStationClassmark2()\n    d = MobileId()\n    e = DescriptiveGroupOrBroadcastCallReference()\n    packet = a / b / c / d / e\n    return packet", "response": "NOTIFICATION RESPONSE Section 9. 1. 21d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rrCellChangeOrder():\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x8)  # 00001000\n    c = CellDescription()\n    d = NcModeAndSpareHalfOctets()\n    packet = a / b / c / d\n    return packet", "response": "RR - CELL CHANGE ORDER Section 9. 1. 21e"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pagingRequestType1(MobileId_presence=0):\n #The L2 pseudo length of this message is the sum of lengths of all\n #information elements present in the message except\n #the P1 Rest Octets and L2 Pseudo Length information elements.\n    a = L2PseudoLength()\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x21)  # 00100001\n    d = PageModeAndChannelNeeded()\n    f = MobileId()\n    packet = a / b / c / d / f\n    if MobileId_presence is 1:\n        g = MobileIdHdr(ieiMI=0x17, eightBitMI=0x0)\n        packet = packet / g\n    h = P1RestOctets()\n    packet = packet / h\n    return packet", "response": "PAGING REQUEST TYPE 1 Section 9. 1. 22"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npaging REQUEST TYPE 2 Section 9. 1. 23", "response": "def pagingRequestType2(MobileId_presence=0):\n    \"\"\"PAGING REQUEST TYPE 2  Section 9.1.23\"\"\"\n    a = L2PseudoLength()\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x22)  # 00100010\n    d = PageModeAndChannelNeeded()\n    f = MobileId()\n    g = MobileId()\n    packet = a / b / c / d / f / g\n    if MobileId_presence is 1:\n        h = MobileIdHdr(ieiMI=0x17, eightBitMI=0x0)\n        packet = packet / h\n    i = P2RestOctets()\n    packet = packet / i\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npages REQUEST TYPE 3 Section 9. 1. 24", "response": "def pagingRequestType3():\n    \"\"\"PAGING REQUEST TYPE 3 Section 9.1.24\"\"\"\n# This message has a L2 Pseudo Length of 19\n    a = L2PseudoLength(l2pLength=0x13)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x24)  # 00100100\n    d = PageModeAndChannelNeeded()\n    e = TmsiPTmsi()\n    f = TmsiPTmsi()\n    g = TmsiPTmsi()\n    h = TmsiPTmsi()\n    i = P3RestOctets()\n    packet = a / b / c / d / e / f / g / h / i\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pagingResponse():\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x27)  # 00100111\n    c = CiphKeySeqNrAndSpareHalfOctets()\n    d = MobileStationClassmark2()\n    e = MobileId()\n    packet = a / b / c / d / e\n    return packet", "response": "PAGING RESPONSE Section 9. 1. 25"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partialReleaseComplete():\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0xf)  # 00001111\n    packet = a / b\n    return packet", "response": "PARTIAL RELEASE COMPLETE Section 9. 1. 27"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef systemInformationType1():\n    a = L2PseudoLength(l2pLength=0x15)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x19)  # 00011001\n    d = CellChannelDescription()\n    e = RachControlParameters()\n    f = Si1RestOctets()\n    packet = a / b / c / d / e / f\n    return packet", "response": "SYSTEM INFORMATION TYPE 1 Section 9. 1. 31"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef systemInformationType2():\n    a = L2PseudoLength(l2pLength=0x16)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x1a)  # 00011010\n    d = NeighbourCellsDescription()\n    e = NccPermitted()\n    f = RachControlParameters()\n    packet = a / b / c / d / e / f\n    return packet", "response": "SYSTEM INFORMATION TYPE 2 Section 9. 1. 32"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef systemInformationType2bis():\n    a = L2PseudoLength(l2pLength=0x15)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x2)  # 00000010\n    d = NeighbourCellsDescription()\n    e = RachControlParameters()\n    f = Si2bisRestOctets()\n    packet = a / b / c / d / e / f\n    return packet", "response": "SYSTEM INFORMATION TYPE 2bis Section 9. 1. 33"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef systemInformationType5ter():\n    a = L2PseudoLength(l2pLength=0x12)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x6)  # 00000110\n    d = NeighbourCellsDescription2()\n    packet = a / b / c / d\n    return packet", "response": "SYSTEM INFORMATION TYPE 5ter Section 9. 1. 39"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef systemInformationType6():\n    a = L2PseudoLength(l2pLength=0x0b)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x1e)  # 00011011\n    d = CellIdentity()\n    e = LocalAreaId()\n    f = CellOptionsBCCH()\n    g = NccPermitted()\n    h = Si6RestOctets()\n    packet = a / b / c / d / e / f / g\n    return packet", "response": "SYSTEM INFORMATION TYPE 6 Section 9. 1. 40"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef systemInformationType7():\n    a = L2PseudoLength(l2pLength=0x01)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x37)  # 000110111\n    d = Si7RestOctets()\n    packet = a / b / c / d\n    return packet", "response": "SYSTEM INFORMATION TYPE 7 Section 9. 1. 41"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef systemInformationType9():\n    a = L2PseudoLength(l2pLength=0x01)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x4)  # 00000100\n    d = Si9RestOctets()\n    packet = a / b / c / d\n    return packet", "response": "SYSTEM INFORMATION TYPE 9 Section 9. 1. 43"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef systemInformationType13():\n    a = L2PseudoLength(l2pLength=0x00)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x0)  # 00000000\n    d = Si13RestOctets()\n    packet = a / b / c / d\n    return packet", "response": "SYSTEM INFORMATION TYPE 13 Section 9. 1. 43a"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef systemInformationType16():\n    a = L2PseudoLength(l2pLength=0x01)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x3d)  # 00111101\n    d = Si16RestOctets()\n    packet = a / b / c / d\n    return packet", "response": "SYSTEM INFORMATION TYPE 16 Section 9. 1. 43d"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef systemInformationType17():\n    a = L2PseudoLength(l2pLength=0x01)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x3e)  # 00111110\n    d = Si17RestOctets()\n    packet = a / b / c / d\n    return packet", "response": "SYSTEM INFORMATION TYPE 17 Section 9. 1. 43e"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uplinkBusy():\n    name = \"Uplink Busy\"\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x2a)  # 00101010\n    packet = a / b\n    return packet", "response": "UPLINK BUSY Section 9. 1. 46"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extendedMeasurementOrder():\n    a = L2PseudoLength(l2pLength=0x12)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x37)  # 00110111\n    d = ExtendedMeasurementFrequencyList()\n    packet = a / b / c / d\n    return packet", "response": "EXTENDED MEASUREMENT ORDER Section 9. 1. 51"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextending MEASUREMENT REPORT Section 9. 1. 52", "response": "def extendedMeasurementReport():\n    \"\"\"EXTENDED MEASUREMENT REPORT Section 9.1.52\"\"\"\n    a = TpPd(pd=0x6)\n    b = MessageType(mesType=0x36)  # 00110110\n    c = ExtendedMeasurementResults()\n    packet = a / b / c\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authenticationReject():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x11)  # 00010001\n    packet = a / b\n    return packet", "response": "AUTHENTICATION REJECT Section 9. 2. 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authenticationResponse():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x14)  # 00010100\n    c = AuthenticationParameterSRES()\n    packet = a / b / c\n    return packet", "response": "AUTHENTICATION RESPONSE Section 9. 2. 3"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cmServiceAccept():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x21)  # 00100001\n    packet = a / b\n    return packet", "response": "CM SERVICE ACCEPT Section 9. 2. 5"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmServiceAbort():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x23)  # 00100011\n    packet = a / b\n    return packet", "response": "CM SERVICE ABORT Section 9. 2. 7"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef abort():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x29)  # 00101001\n    c = RejectCause()\n    packet = a / b / c\n    return packet", "response": "ABORT Section 9. 2. 8"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cmServiceRequest(PriorityLevel_presence=0):\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x24)  # 00100100\n    c = CmServiceTypeAndCiphKeySeqNr()\n    e = MobileStationClassmark2()\n    f = MobileId()\n    packet = a / b / c / e / f\n    if PriorityLevel_presence is 1:\n        g = PriorityLevelHdr(ieiPL=0x8, eightBitPL=0x0)\n        packet = packet / g\n    return packet", "response": "CM SERVICE REQUEST Section 9. 2. 9"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef identityRequest():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x8)  # 00001000\n    c = IdentityTypeAndSpareHalfOctets()\n    packet = a / b / c\n    return packet", "response": "IDENTITY REQUEST Section 9. 2. 10"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef imsiDetachIndication():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x1)  # 00000001\n    c = MobileStationClassmark1()\n    d = MobileId()\n    packet = a / b / c / d\n    return packet", "response": "IMSI DETACH INDICATION Section 9. 2. 12"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef locationUpdatingAccept(MobileId_presence=0,\n                           FollowOnProceed_presence=0,\n                           CtsPermission_presence=0):\n    \"\"\"LOCATION UPDATING ACCEPT Section 9.2.13\"\"\"\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x02)  # 00000010\n    c = LocalAreaId()\n    packet = a / b / c\n    if MobileId_presence is 1:\n        d = MobileIdHdr(ieiMI=0x17, eightBitMI=0x0)\n        packet = packet / d\n    if FollowOnProceed_presence is 1:\n        e = FollowOnProceed(ieiFOP=0xA1)\n        packet = packet / e\n    if CtsPermission_presence is 1:\n        f = CtsPermissionHdr(ieiCP=0xA2, eightBitCP=0x0)\n        packet = packet / f\n    return packet", "response": "LOCATION UPDATING ACCEPT Section 9. 2. 13"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mmNull():\n    a = TpPd(pd=0x5)\n    b = MessageType(mesType=0x30)  # 00110000\n    packet = a / b\n    return packet", "response": "MM NULL Section 9. 2. 19"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nalert Section 9. 3. 1. 1", "response": "def alertingNetToMs(Facility_presence=0, ProgressIndicator_presence=0,\n                    UserUser_presence=0):\n    \"\"\"ALERTING Section 9.3.1.1\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x1)  # 00000001\n    packet = a / b\n    if Facility_presence is 1:\n        c = FacilityHdr(ieiF=0x1C)\n        packet = packet / c\n    if ProgressIndicator_presence is 1:\n        d = ProgressIndicatorHdr(ieiPI=0x1E)\n        packet = packet / d\n    if UserUser_presence is 1:\n        e = UserUserHdr(ieiUU=0x7E)\n        packet = packet / e\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls CONFIRMED Section 9. 3. 2", "response": "def callConfirmed(RepeatIndicator_presence=0,\n                  BearerCapability_presence=0, BearerCapability_presence1=0,\n                  Cause_presence=0, CallControlCapabilities_presence=0):\n    \"\"\"CALL CONFIRMED Section 9.3.2\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x8)  # 00001000\n    packet = a / b\n    if RepeatIndicator_presence is 1:\n        c = RepeatIndicatorHdr(ieiRI=0xD, eightBitRI=0x0)\n        packet = packet / c\n    if BearerCapability_presence is 1:\n        d = BearerCapabilityHdr(ieiBC=0x04, eightBitBC=0x0)\n        packet = packet / d\n    if BearerCapability_presence1 is 1:\n        e = BearerCapabilityHdr(ieiBC=0x04, eightBitBC=0x0)\n        packet = packet / e\n    if Cause_presence is 1:\n        f = CauseHdr(ieiC=0x08, eightBitC=0x0)\n        packet = packet / f\n    if CallControlCapabilities_presence is 1:\n        g = CallControlCapabilitiesHdr(ieiCCC=0x15, eightBitCCC=0x0)\n        packet = packet / g\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef callProceeding(RepeatIndicator_presence=0,\n                   BearerCapability_presence=0,\n                   BearerCapability_presence1=0,\n                   Facility_presence=0, ProgressIndicator_presence=0,\n                   PriorityLevel_presence=0):\n    \"\"\"CALL PROCEEDING Section 9.3.3\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x2)  # 00000010\n    packet = a / b\n    if RepeatIndicator_presence is 1:\n        c = RepeatIndicatorHdr(ieiRI=0xD, eightBitRI=0x0)\n        packet = packet / c\n    if BearerCapability_presence is 1:\n        d = BearerCapabilityHdr(ieiBC=0x04, eightBitBC=0x0)\n        packet = packet / d\n    if BearerCapability_presence1 is 1:\n        e = BearerCapabilityHdr(ieiBC=0x04, eightBitBC=0x0)\n        packet = packet / e\n    if Facility_presence is 1:\n        f = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n        packet = packet / f\n    if ProgressIndicator_presence is 1:\n        g = ProgressIndicatorHdr(ieiPI=0x1E, eightBitPI=0x0)\n        packet = packet / g\n    if PriorityLevel_presence is 1:\n        h = PriorityLevelHdr(ieiPL=0x80, eightBitPL=0x0)\n        packet = packet / h\n    return packet", "response": "CALL PROCEEDING Section 9. 3. 3"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef congestionControl(Cause_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x39)  # 00111001\n    c = CongestionLevelAndSpareHalfOctets()\n    packet = a / b / c\n    if Cause_presence is 1:\n        e = CauseHdr(ieiC=0x08, eightBitC=0x0)\n        packet = packet / e\n    return packet", "response": "CONGESTION CONTROL Section 9. 3. 4"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect Section 9. 3. 5. 1", "response": "def connectNetToMs(Facility_presence=0, ProgressIndicator_presence=0,\n                   ConnectedNumber_presence=0, ConnectedSubaddress_presence=0,\n                   UserUser_presence=0):\n    \"\"\"CONNECT Section 9.3.5.1\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x7)  # 00000111\n    packet = a / b\n    if Facility_presence is 1:\n        c = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n        packet = packet / c\n    if ProgressIndicator_presence is 1:\n        d = ProgressIndicatorHdr(ieiPI=0x1E, eightBitPI=0x0)\n        packet = packet / d\n    if ConnectedNumber_presence is 1:\n        e = ConnectedNumberHdr(ieiCN=0x4C, eightBitCN=0x0)\n        packet = packet / e\n    if ConnectedSubaddress_presence is 1:\n        f = ConnectedSubaddressHdr(ieiCS=0x4D, eightBitCS=0x0)\n        packet = packet / f\n    if UserUser_presence is 1:\n        g = UserUserHdr(ieiUU=0x7F, eightBitUU=0x0)\n        packet = packet / g\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connectMsToNet(Facility_presence=0, ConnectedSubaddress_presence=0,\n                   UserUser_presence=0, SsVersionIndicator_presence=0):\n    \"\"\"CONNECT Section 9.3.5.2\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x7)  # 00000111\n    packet = a / b\n    if Facility_presence is 1:\n        c = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n        packet = packet / c\n    if ConnectedSubaddress_presence is 1:\n        d = ConnectedSubaddressHdr(ieiCS=0x4D, eightBitCS=0x0)\n        packet = packet / d\n    if UserUser_presence is 1:\n        e = UserUserHdr(ieiUU=0x7F, eightBitUU=0x0)\n        packet = packet / e\n    if SsVersionIndicator_presence is 1:\n        f = SsVersionIndicatorHdr(ieiSVI=0x7F, eightBitSVI=0x0)\n        packet = packet / f\n    return packet", "response": "CONNECT Section 9. 3. 5. 2"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting ACKNOWLEDGE Section 9. 3. 6", "response": "def connectAcknowledge():\n    \"\"\"CONNECT ACKNOWLEDGE Section 9.3.6\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0xf)  # 00001111\n    packet = a / b\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disconnectNetToMs(Facility_presence=0, ProgressIndicator_presence=0,\n                      UserUser_presence=0, AllowedActions_presence=0):\n    \"\"\"DISCONNECT Section 9.3.7.1\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x25)  # 00100101\n    c = Cause()\n    packet = a / b / c\n    if Facility_presence is 1:\n        d = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n        packet = packet / d\n    if ProgressIndicator_presence is 1:\n        e = ProgressIndicatorHdr(ieiPI=0x1E, eightBitPI=0x0)\n        packet = packet / e\n    if UserUser_presence is 1:\n        f = UserUserHdr(ieiUU=0x7E, eightBitUU=0x0)\n        packet = packet / f\n    if AllowedActions_presence is 1:\n        g = AllowedActionsHdr(ieiAA=0x7B, eightBitAA=0x0)\n        packet = packet / g\n    return packet", "response": "DISCONNECT Section 9. 3. 7. 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisconnect Section 9. 3. 7. 2", "response": "def disconnectMsToNet(Facility_presence=0, UserUser_presence=0,\n                      SsVersionIndicator_presence=0):\n    \"\"\"Disconnect Section 9.3.7.2\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x25)  # 00100101\n    c = Cause()\n    packet = a / b / c\n    if Facility_presence is 1:\n        d = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n        packet = packet / d\n    if UserUser_presence is 1:\n        e = UserUserHdr(ieiUU=0x7E, eightBitUU=0x0)\n        packet = packet / e\n    if SsVersionIndicator_presence is 1:\n        f = SsVersionIndicatorHdr(ieiSVI=0x7F, eightBitSVI=0x0)\n        packet = packet / f\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef emergencySetup(BearerCapability_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0xe)  # 00001110\n    packet = a / b\n    if BearerCapability_presence is 1:\n        c = BearerCapabilityHdr(ieiBC=0x04, eightBitBC=0x0)\n        packet = packet / c\n    return packet", "response": "EMERGENCY SETUP Section 9. 3. 8"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef facilityNetToMs():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x3a)  # 00111010\n    c = Facility()\n    packet = a / b / c\n    return packet", "response": "FACILITY Section 9. 3. 9. 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nholds Section 9. 3. 10", "response": "def hold():\n    \"\"\"HOLD Section 9.3.10\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x18)  # 00011000\n    packet = a / b\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef holdAcknowledge():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x19)  # 00011001\n    packet = a / b\n    return packet", "response": "HOLD ACKNOWLEDGE Section 9. 3. 11"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmodify Section 9. 3. 13", "response": "def modify(LowLayerCompatibility_presence=0,\n           HighLayerCompatibility_presence=0,\n           ReverseCallSetupDirection_presence=0):\n    \"\"\"MODIFY Section 9.3.13\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x17)  # 00010111\n    c = BearerCapability()\n    packet = a / b / c\n    if LowLayerCompatibility_presence is 1:\n        d = LowLayerCompatibilityHdr(ieiLLC=0x7C, eightBitLLC=0x0)\n        packet = packet / d\n    if HighLayerCompatibility_presence is 1:\n        e = HighLayerCompatibilityHdr(ieiHLC=0x7D, eightBitHLC=0x0)\n        packet = packet / e\n    if ReverseCallSetupDirection_presence is 1:\n        f = ReverseCallSetupDirectionHdr(ieiRCSD=0xA3)\n        packet = packet / f\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef modifyReject(LowLayerCompatibility_presence=0,\n                 HighLayerCompatibility_presence=0):\n    \"\"\"MODIFY REJECT Section 9.3.15\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x13)  # 00010011\n    c = BearerCapability()\n    d = Cause()\n    packet = a / b / c / d\n    if LowLayerCompatibility_presence is 1:\n        e = LowLayerCompatibilityHdr(ieiLLC=0x7C, eightBitLLC=0x0)\n        packet = packet / e\n    if HighLayerCompatibility_presence is 1:\n        f = HighLayerCompatibilityHdr(ieiHLC=0x7D, eightBitHLC=0x0)\n        packet = packet / f\n    return packet", "response": "MODIFY REJECT Section 9. 3. 15"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notify():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x3e)  # 00111110\n    c = NotificationIndicator()\n    packet = a / b / c\n    return packet", "response": "NOTIFY Section 9. 3. 16"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef progress(UserUser_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x3)  # 00000011\n    c = ProgressIndicator()\n    packet = a / b / c\n    if UserUser_presence is 1:\n        d = UserUserHdr()\n        packet = packet / d\n    return packet", "response": "PROGRESS Section 9. 3. 17"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ccEstablishment():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x4)  # 00000100\n    c = SetupContainer()\n    packet = a / b / c\n    return packet", "response": "CC - ESTABLISHMENT Section 9. 3. 17a"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ccEstablishmentConfirmed(RepeatIndicator_presence=0,\n                             BearerCapability_presence=0,\n                             BearerCapability_presence1=0,\n                             Cause_presence=0):\n    \"\"\"CC-ESTABLISHMENT CONFIRMED Section 9.3.17b\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x6)  # 00000110\n    packet = a / b\n    if RepeatIndicator_presence is 1:\n        c = RepeatIndicatorHdr(ieiRI=0xD, eightBitRI=0x0)\n        packet = packet / c\n    if BearerCapability_presence is 1:\n        d = BearerCapabilityHdr(ieiBC=0x04, eightBitBC=0x0)\n        packet = packet / d\n    if BearerCapability_presence1 is 1:\n        e = BearerCapabilityHdr(ieiBC=0x04, eightBitBC=0x0)\n        packet = packet / e\n    if Cause_presence is 1:\n        f = CauseHdr(ieiC=0x08, eightBitC=0x0)\n        packet = packet / f\n    return packet", "response": "CC - ESTABLISHMENT CONFIRMED Section 9. 3. 17a"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrelease Section 9. 3. 18. 1", "response": "def releaseNetToMs():\n    \"\"\"RELEASE Section 9.3.18.1\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x2d)  # 00101101\n    c = CauseHdr(ieiC=0x08, eightBitC=0x0)\n    d = CauseHdr(ieiC=0x08, eightBitC=0x0)\n    e = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n    f = UserUserHdr(ieiUU=0x7E, eightBitUU=0x0)\n    packet = a / b / c / d / e / f\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrecalling Section 9. 3. 18a", "response": "def recall():\n    \"\"\"RECALL Section 9.3.18a\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0xb)  # 00001011\n    c = RecallType()\n    d = Facility()\n    packet = a / b / c / d\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef releaseCompleteNetToMs(Cause_presence=0, Facility_presence=0,\n                           UserUser_presence=0):\n    \"\"\"RELEASE COMPLETE Section 9.3.19.1\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x2a)  # 00101010\n    packet = a / b\n    if Cause_presence is 1:\n        c = CauseHdr(ieiC=0x08, eightBitC=0x0)\n        packet = packet / c\n    if Facility_presence is 1:\n        d = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n        packet = packet / d\n    if UserUser_presence is 1:\n        e = UserUserHdr(ieiUU=0x7E, eightBitUU=0x0)\n        packet = packet / e\n    return packet", "response": "RELEASE COMPLETE Section 9. 3. 19. 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef releaseCompleteMsToNet(Cause_presence=0, Facility_presence=0,\n                           UserUser_presence=0, SsVersionIndicator_presence=0):\n    \"\"\"RELEASE COMPLETE Section 9.3.19.2\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x2a)  # 00101010\n    packet = a / b\n    if Cause_presence is 1:\n        c = CauseHdr(ieiC=0x08, eightBitC=0x0)\n        packet = packet / c\n    if Facility_presence is 1:\n        d = FacilityHdr(ieiF=0x1C, eightBitF=0x0)\n        packet = packet / d\n    if UserUser_presence is 1:\n        e = UserUserHdr(ieiUU=0x7E, eightBitUU=0x0)\n        packet = packet / e\n    if SsVersionIndicator_presence is 1:\n        f = SsVersionIndicatorHdr(ieiSVI=0x7F, eightBitSVI=0x0)\n        packet = packet / f\n    return packet", "response": "RELEASE COMPLETE Section 9. 3. 19. 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retrieve():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x1c)  # 00011100\n    packet = a / b\n    return packet", "response": "RETRIEVE Section 9. 3. 20"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve ACKNOWLEDGE Section 9. 3. 21", "response": "def retrieveAcknowledge():\n    \"\"\"RETRIEVE ACKNOWLEDGE Section 9.3.21\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x1d)  # 00011101\n    packet = a / b\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting CC Section 9. 3. 23a", "response": "def startCc(CallControlCapabilities_presence=0):\n    \"\"\"START CC Section 9.3.23a\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x9)  # 00001001\n    packet = a / b\n    if CallControlCapabilities_presence is 1:\n        c = CallControlCapabilitiesHdr(ieiCCC=0x15, eightBitCCC=0x0)\n        packet = paclet / c\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef startDtmf():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x35)  # 00110101\n    c = KeypadFacilityHdr(ieiKF=0x2C, eightBitKF=0x0)\n    packet = a / b / c\n    return packet", "response": "START DTMF Section 9. 3. 24"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart DTMF REJECT Section 9. 3. 26", "response": "def startDtmfReject():\n    \"\"\" START DTMF REJECT Section 9.3.26\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x37)  # 00110111\n    c = Cause()\n    packet = a / b / c\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status(AuxiliaryStates_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x3d)  # 00111101\n    c = Cause()\n    d = CallState()\n    packet = a / b / c / d\n    if AuxiliaryStates_presence is 1:\n        e = AuxiliaryStatesHdr(ieiAS=0x24, eightBitAS=0x0)\n        packet = packet / e\n    return packet", "response": "STATUS Section 9. 3. 27"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stopDtmf():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x31)  # 00110001\n    packet = a / b\n    return packet", "response": "STOP DTMF Section 9. 3. 29"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stopDtmfAcknowledge():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x32)  # 00110010\n    packet = a / b\n    return packet", "response": "STOP DTMF ACKNOWLEDGE Section 9. 3. 30"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef userInformation(MoreData_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x20)  # 000100000\n    c = UserUser()\n    packet = a / b / c\n    if MoreData_presence is 1:\n        d = MoreDataHdr(ieiMD=0xA0, eightBitMD=0x0)\n        packet = packet / d\n    return packet", "response": "USER INFORMATION Section 9. 3. 31"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef attachRequest(PTmsiSignature_presence=0, GprsTimer_presence=0,\n                  TmsiStatus_presence=0):\n    \"\"\"ATTACH REQUEST Section 9.4.1\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x1)  # 0000001\n    c = MsNetworkCapability()\n    d = AttachTypeAndCiphKeySeqNr()\n    f = DrxParameter()\n    g = MobileId()\n    h = RoutingAreaIdentification()\n    i = MsRadioAccessCapability()\n    packet = a / b / c / d / f / g / h / i\n    if PTmsiSignature_presence is 1:\n        j = PTmsiSignature(ieiPTS=0x19)\n        packet = packet / j\n    if GprsTimer_presence is 1:\n        k = GprsTimer(ieiGT=0x17)\n        packet = packet / k\n    if TmsiStatus_presence is 1:\n        l = TmsiStatus(ieiTS=0x9)\n        packet = packet / l\n    return packet", "response": "ATTACH REQUEST Section 9. 4. 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attachAccept(PTmsiSignature_presence=0, GprsTimer_presence=0,\n                 MobileId_presence=0, MobileId_presence1=0,\n                 GmmCause_presence=0):\n    \"\"\"ATTACH ACCEPT Section 9.4.2\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x2)  # 00000010\n    c = AttachResult()\n    d = ForceToStandby()\n    e = GprsTimer()\n    f = RadioPriorityAndSpareHalfOctets()\n    h = RoutingAreaIdentification()\n    packet = a / b / c / d / e / f / h\n    if PTmsiSignature_presence is 1:\n        i = PTmsiSignature(ieiPTS=0x19)\n        packet = packet / i\n    if GprsTimer_presence is 1:\n        j = GprsTimer(ieiGT=0x17)\n        packet = packet / j\n    if MobileId_presence is 1:\n        k = MobileIdHdr(ieiMI=0x18, eightBitMI=0x0)\n        packet = packet / k\n    if MobileId_presence1 is 1:\n        l = MobileIdHdr(ieiMI=0x23, eightBitMI=0x0)\n        packet = packet / l\n    if GmmCause_presence is 1:\n        m = GmmCause(ieiGC=0x25)\n        packet = packet / m\n    return packet", "response": "ATTACH ACCEPT Section 9. 4. 2"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattaches COMPLETE Section 9. 4. 3", "response": "def attachComplete():\n    \"\"\"ATTACH COMPLETE Section 9.4.3\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x3)  # 00000011\n    packet = a / b\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef detachRequest(GmmCause_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x5)  # 00000101\n    c = DetachTypeAndForceToStandby()\n    packet = a / b / c\n    if GmmCause_presence is 1:\n        e = GmmCause(ieiGC=0x25)\n        packet = packet / e\n    return packet", "response": "DETACH REQUEST Section 9. 4. 5"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detachRequestMsOriginating():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x5)  # 00000101\n    c = DetachTypeAndSpareHalfOctets()\n    packet = a / b / c\n    return packet", "response": "DETACH REQUEST Section 9. 4. 5. 2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detachAcceptMsTerminated():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x6)  # 00000110\n    packet = a / b\n    return packet", "response": "DETACH ACCEPT Section 9. 4. 6. 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detachAcceptMsOriginating():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x6)  # 00000110\n    c = ForceToStandbyAndSpareHalfOctets()\n    packet = a / b / c\n    return packet", "response": "DETACH ACCEPT Section 9. 4. 6. 2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ptmsiReallocationCommand(PTmsiSignature_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x10)  # 00010000\n    c = MobileId()\n    d = RoutingAreaIdentification()\n    e = ForceToStandbyAndSpareHalfOctets()\n    packet = a / b / c / d / e\n    if PTmsiSignature_presence is 1:\n        g = PTmsiSignature(ieiPTS=0x19)\n        packet = packet / g\n    return packet", "response": "P - TMSI REALLOCATION COMMAND Section 9. 4. 7"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authenticationAndCipheringResponse(\n                                       AuthenticationParameterSRES_presence=0,\n                                       MobileId_presence=0):\n    \"\"\"AUTHENTICATION AND CIPHERING RESPONSE Section 9.4.10\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x13)  # 00010011\n    c = AcReferenceNumberAndSpareHalfOctets()\n    packet = a / b / c\n    if AuthenticationParameterSRES_presence is 1:\n        e = AuthenticationParameterSRES(ieiAPS=0x22)\n        packet = packet / e\n    if MobileId_presence is 1:\n        f = MobileIdHdr(ieiMI=0x23, eightBitMI=0x0)\n        packet = packet / f\n    return packet", "response": "AUTHENTICATION AND CIPHERING RESPONSE Section 9. 4. 10"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrouting AREA UPDATE REQUEST Section 9. 4. 14", "response": "def routingAreaUpdateRequest(PTmsiSignature_presence=0,\n                             GprsTimer_presence=0,\n                             DrxParameter_presence=0,\n                             TmsiStatus_presence=0):\n    \"\"\"ROUTING AREA UPDATE REQUEST Section 9.4.14\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x8)  # 00001000\n    c = UpdateTypeAndCiphKeySeqNr()\n    e = RoutingAreaIdentification()\n    f = MsNetworkCapability()\n    packet = a / b / c / e / f\n    if PTmsiSignature_presence is 1:\n        g = PTmsiSignature(ieiPTS=0x19)\n        packet = packet / g\n    if GprsTimer_presence is 1:\n        h = GprsTimer(ieiGT=0x17)\n        packet = packet / h\n    if DrxParameter_presence is 1:\n        i = DrxParameter(ieiDP=0x27)\n        packet = packet / i\n    if TmsiStatus_presence is 1:\n        j = TmsiStatus(ieiTS=0x9)\n        packet = packet / j\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef routingAreaUpdateAccept(PTmsiSignature_presence=0,\n                            MobileId_presence=0, MobileId_presence1=0,\n                            ReceiveNpduNumbersList_presence=0,\n                            GprsTimer_presence=0, GmmCause_presence=0):\n    \"\"\"ROUTING AREA UPDATE ACCEPT Section 9.4.15\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x9)  # 00001001\n    c = ForceToStandbyAndUpdateResult()\n    e = GprsTimer()\n    f = RoutingAreaIdentification()\n    packet = a / b / c / e / f\n    if PTmsiSignature_presence is 1:\n        g = PTmsiSignature(ieiPTS=0x19)\n        packet = packet / g\n    if MobileId_presence is 1:\n        h = MobileIdHdr(ieiMI=0x18, eightBitMI=0x0)\n        packet = packet / h\n    if MobileId_presence1 is 1:\n        i = MobileIdHdr(ieiMI=0x23, eightBitMI=0x0)\n        packet = packet / i\n    if ReceiveNpduNumbersList_presence is 1:\n        j = ReceiveNpduNumbersList(ieiRNNL=0x26)\n        packet = packet / j\n    if GprsTimer_presence is 1:\n        k = GprsTimer(ieiGT=0x17)\n        packet = packet / k\n    if GmmCause_presence is 1:\n        l = GmmCause(ieiGC=0x25)\n        packet = packet / l\n    return packet", "response": "ROUTING AREA UPDATE ACCEPT Section 9. 4. 15"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef routingAreaUpdateComplete(ReceiveNpduNumbersList_presence=0):\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0xa)  # 00001010\n    packet = a / b\n    if ReceiveNpduNumbersList_presence is 1:\n        c = ReceiveNpduNumbersList(ieiRNNL=0x26)\n        packet = packet / c\n    return packet", "response": "ROUTING AREA UPDATE COMPLETE Section 9. 4. 16"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef routingAreaUpdateReject():\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0xb)  # 00001011\n    c = GmmCause()\n    d = ForceToStandbyAndSpareHalfOctets()\n    packet = a / b / c / d\n    return packet", "response": "ROUTING AREA UPDATE REJECT Section 9. 4. 17"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nactivating PDP CONTEXT REQUEST Section 9. 5. 1", "response": "def activatePdpContextRequest(AccessPointName_presence=0,\n                              ProtocolConfigurationOptions_presence=0):\n    \"\"\"ACTIVATE PDP CONTEXT REQUEST Section 9.5.1\"\"\"\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x41)  # 01000001\n    c = NetworkServiceAccessPointIdentifier()\n    d = LlcServiceAccessPointIdentifier()\n    e = QualityOfService()\n    f = PacketDataProtocolAddress()\n    packet = a / b / c / d / e / f\n    if AccessPointName_presence is 1:\n        g = AccessPointName(ieiAPN=0x28)\n        packet = packet / g\n    if ProtocolConfigurationOptions_presence is 1:\n        h = ProtocolConfigurationOptions(ieiPCO=0x27)\n        packet = packet / h\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nactivating PDP CONTEXT ACCEPT Section 9. 5. 2", "response": "def activatePdpContextAccept(PacketDataProtocolAddress_presence=0,\n                             ProtocolConfigurationOptions_presence=0):\n    \"\"\"ACTIVATE PDP CONTEXT ACCEPT Section 9.5.2\"\"\"\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x42)  # 01000010\n    c = LlcServiceAccessPointIdentifier()\n    d = QualityOfService()\n    e = RadioPriorityAndSpareHalfOctets()\n    packet = a / b / c / d / e\n    if PacketDataProtocolAddress_presence is 1:\n        f = PacketDataProtocolAddress(ieiPDPA=0x2B)\n        packet = packet / f\n    if ProtocolConfigurationOptions_presence is 1:\n        g = ProtocolConfigurationOptions(ieiPCO=0x27)\n        packet = packet / g\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef requestPdpContextActivation(AccessPointName_presence=0):\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x44)  # 01000100\n    c = PacketDataProtocolAddress()\n    packet = a / b / c\n    if AccessPointName_presence is 1:\n        d = AccessPointName(ieiAPN=0x28)\n        packet = packet / d\n    return packet", "response": "REQUEST PDP CONTEXT ACTIVATION Section 9. 5. 4"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef requestPdpContextActivationReject():\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x45)  # 01000101\n    c = SmCause()\n    packet = a / b / c\n    return packet", "response": "REQUEST PDP CONTEXT ACTIVATION REJECT Section 9. 5. 5"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modifyPdpContextRequest():\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x48)  # 01001000\n    c = RadioPriorityAndSpareHalfOctets()\n    d = LlcServiceAccessPointIdentifier()\n    e = QualityOfService()\n    packet = a / b / c / d / e\n    return packet", "response": "MODIFY PDP CONTEXT REQUEST Section 9. 5. 6"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef modifyPdpContextAccept():\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x45)  # 01000101\n    packet = a / b\n    return packet", "response": "MODIFY PDP CONTEXT ACCEPT Section 9. 5. 7"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deactivatePdpContextAccept():\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x47)  # 01000111\n    packet = a / b\n    return packet", "response": "DEACTIVATE PDP CONTEXT ACCEPT Section 9. 5. 9"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef activateAaPdpContextRequest(AccessPointName_presence=0,\n                                ProtocolConfigurationOptions_presence=0,\n                                GprsTimer_presence=0):\n    \"\"\"ACTIVATE AA PDP CONTEXT REQUEST Section 9.5.10\"\"\"\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x50)  # 01010000\n    c = NetworkServiceAccessPointIdentifier()\n    d = LlcServiceAccessPointIdentifier()\n    e = QualityOfService()\n    f = PacketDataProtocolAddress()\n    packet = a / b / c / d / e / f\n    if AccessPointName_presence is 1:\n        g = AccessPointName(ieiAPN=0x28)\n        packet = packet / g\n    if ProtocolConfigurationOptions_presence is 1:\n        h = ProtocolConfigurationOptions(ieiPCO=0x27)\n        packet = packet / h\n    if GprsTimer_presence is 1:\n        i = GprsTimer(ieiGT=0x29)\n        packet = packet / i\n    return packet", "response": "ACTIVATE AA PDP CONTEXT REQUEST Section 9. 5. 10"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef activateAaPdpContextAccept(ProtocolConfigurationOptions_presence=0,\n                               GprsTimer_presence=0):\n    \"\"\"ACTIVATE AA PDP CONTEXT ACCEPT Section 9.5.11\"\"\"\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x51)  # 01010001\n    c = LlcServiceAccessPointIdentifier()\n    d = QualityOfService()\n    e = MobileId()\n    f = PacketDataProtocolAddress()\n    g = RadioPriorityAndSpareHalfOctets()\n    packet = a / b / c / d / e / f / g\n    if ProtocolConfigurationOptions_presence is 1:\n        i = ProtocolConfigurationOptions(ieiPCO=0x27)\n        packet = packet / i\n    if GprsTimer_presence is 1:\n        j = GprsTimer(ieiGT=0x29)\n        packet = packet / j\n    return packet", "response": "ACTIVATE AA PDP CONTEXT ACCEPT Section 9. 5. 11"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactivating AA PDP CONTEXT REJECT Section 9. 5. 12", "response": "def activateAaPdpContextReject(ProtocolConfigurationOptions_presence=0):\n    \"\"\"ACTIVATE AA PDP CONTEXT REJECT Section 9.5.12\"\"\"\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x52)  # 01010010\n    c = SmCause()\n    packet = a / b / c\n    if ProtocolConfigurationOptions_presence is 1:\n        d = ProtocolConfigurationOptions(ieiPCO=0x27)\n        packet = packet / d\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deactivateAaPdpContextRequest():\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x53)  # 01010011\n    c = AaDeactivationCauseAndSpareHalfOctets()\n    packet = a / b / c\n    return packet", "response": "DEACTIVATE AA PDP CONTEXT REQUEST Section 9. 5. 13"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeactivates AA PDP CONTEXT ACCEPT Section 9. 5. 14", "response": "def deactivateAaPdpContextAccept():\n    \"\"\"DEACTIVATE AA PDP CONTEXT ACCEPT Section 9.5.14\"\"\"\n    a = TpPd(pd=0x8)\n    b = MessageType(mesType=0x54)  # 01010100\n    packet = a / b\n    return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists available layers or infos on a given layer", "response": "def ls(obj=None):\n    \"\"\"List  available layers, or infos on a given layer\"\"\"\n    if obj is None:\n        \n        import builtins\n        all = builtins.__dict__.copy()\n        all.update(globals())\n        objlst = sorted(conf.layers, key=lambda x:x.__name__)\n        for o in objlst:\n            print(\"%-10s : %s\" %(o.__name__,o.name))\n    else:\n        if isinstance(obj, type) and issubclass(obj, Packet):\n            for f in obj.fields_desc:\n                print(\"%-10s : %-20s = (%s)\" % (f.name, f.__class__.__name__,  repr(f.default)))\n        elif isinstance(obj, Packet):\n            for f in obj.fields_desc:\n                print(\"%-10s : %-20s = %-15s (%s)\" % (f.name, f.__class__.__name__, repr(getattr(obj,f.name)), repr(f.default)))\n            if not isinstance(obj.payload, NoPayload):\n                print(\"--\")\n                ls(obj.payload)\n                \n\n        else:\n            print(\"Not a packet class. Type 'ls()' to list packet classes.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform a layer into a fuzzy layer by replacing some default values by random objects", "response": "def fuzz(p, _inplace=0):\n    \"\"\"Transform a layer into a fuzzy layer by replacing some default values by random objects\"\"\"\n    if not _inplace:\n        p = p.copy()\n    q = p\n    while not isinstance(q, NoPayload):\n        for f in q.fields_desc:\n            if isinstance(f, PacketListField):\n                for r in getattr(q, f.name):\n                    print(\"fuzzing\", repr(r))\n                    fuzz(r, _inplace=1)\n            elif isinstance(f, FlagsField):\n                rnd = random.randint(0, (2 ^ f.size) - 1)\n                q.default_fields[f.name] = rnd\n            elif f.default is not None:\n                rnd = f.randval()\n                if rnd is not None:\n                    q.default_fields[f.name] = rnd\n        q = q.payload\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self):\n        clone = self.__class__()\n        clone.fields = self.fields.copy()\n        for k in clone.fields:\n            clone.fields[k] = self.get_field(k).do_copy(clone.fields[k])\n        clone.default_fields = self.default_fields.copy()\n        clone.overloaded_fields = self.overloaded_fields.copy()\n        clone.overload_fields = self.overload_fields.copy()\n        clone.underlayer = self.underlayer\n        clone.explicit = self.explicit\n        clone.raw_packet_cache = self.raw_packet_cache\n        clone.post_transforms = self.post_transforms[:]\n        clone.__dict__[\"payload\"] = self.payload.copy()\n        clone.payload.add_underlayer(clone)\n        clone.time = self.time\n        clone.sent_time = self.sent_time\n        return clone", "response": "Returns a deep copy of the instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef psdump(self, filename=None, **kargs):\n        canvas = self.canvas_dump(**kargs)\n        if filename is None:\n            fname = get_temp_file(autoext=\".eps\")\n            canvas.writeEPSfile(fname)\n            subprocess.Popen([conf.prog.psreader, fname+\".eps\"])\n        else:\n            canvas.writeEPSfile(filename)", "response": "Dump an EPS file describing a packet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef guess_payload_class(self, payload):\n        for t in self.aliastypes:\n            for fval, cls in t.payload_guess:\n                ok = 1\n                for k in fval.keys():\n                    if not hasattr(self, k) or fval[k] != self.getfieldval(k):\n                        ok = 0\n                        break\n                if ok:\n                    return cls\n        return self.default_payload_class(payload)", "response": "Guesses the next payload class from the layer bonds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hide_defaults(self):\n        for k in list(self.fields.keys()):\n            if k in self.default_fields:\n                if self.default_fields[k] == self.fields[k]:\n                    del(self.fields[k])\n        self.payload.hide_defaults()", "response": "Removes fields values that are the same as default values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef haslayer(self, cls):\n        if self.__class__ == cls or self.__class__.__name__ == cls:\n            return 1\n        for f in self.packetfields:\n            fvalue_gen = self.getfieldval(f.name)\n            if fvalue_gen is None:\n                continue\n            if not f.islist:\n                fvalue_gen = SetGen(fvalue_gen,_iterpacket=0)\n            for fvalue in fvalue_gen:\n                if isinstance(fvalue, Packet):\n                    ret = fvalue.haslayer(cls)\n                    if ret:\n                        return ret\n        return self.payload.haslayer(cls)", "response": "True if self has a layer that is an instance of cls. Superseded by cls in self syntax."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the nb^th layer that is an instance of cls.", "response": "def getlayer(self, cls, nb=1, _track=None):\n        \"\"\"Return the nb^th layer that is an instance of cls.\"\"\"\n        if type(cls) is int:\n            nb = cls+1\n            cls = None\n        if type(cls) is str and \".\" in cls:\n            ccls,fld = cls.split(\".\",1)\n        else:\n            ccls,fld = cls,None\n        if cls is None or self.__class__ == cls or self.__class__.name == ccls:\n            if nb == 1:\n                if fld is None:\n                    return self\n                else:\n                    return self.getfieldval(fld)\n            else:\n                nb -=1\n        for f in self.packetfields:\n            fvalue_gen = self.getfieldval(f.name)\n            if fvalue_gen is None:\n                continue\n            if not f.islist:\n                fvalue_gen = SetGen(fvalue_gen,_iterpacket=0)\n            for fvalue in fvalue_gen:\n                if isinstance(fvalue, Packet):\n                    track=[]\n                    ret = fvalue.getlayer(cls, nb, _track=track)\n                    if ret is not None:\n                        return ret\n                    nb = track[0]\n        return self.payload.getlayer(cls,nb,_track=_track)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a hierarchical view of the packet. indent gives the size of indentation for each layer. label_lvl gives the size of indentation for each layer.", "response": "def show(self, indent=3, lvl=\"\", label_lvl=\"\"):\n        \"\"\"Prints a hierarchical view of the packet. \"indent\" gives the size of indentation for each layer.\"\"\"\n        ct = conf.color_theme\n        print(\"%s%s %s %s\" % (label_lvl,\n                              ct.punct(\"###[\"),\n                              ct.layer_name(self.name),\n                              ct.punct(\"]###\")))\n        for f in self.fields_desc:\n            if isinstance(f, ConditionalField) and not f._evalcond(self):\n                continue\n            if isinstance(f, Emph) or f in conf.emph:\n                ncol = ct.emph_field_name\n                vcol = ct.emph_field_value\n            else:\n                ncol = ct.field_name\n                vcol = ct.field_value\n            fvalue = self.getfieldval(f.name)\n            if isinstance(fvalue, Packet) or (f.islist and f.holds_packets and type(fvalue) is list):\n                print(\"%s  \\\\%-10s\\\\\" % (label_lvl+lvl, ncol(f.name)))\n                fvalue_gen = SetGen(fvalue,_iterpacket=0)\n                for fvalue in fvalue_gen:\n                    fvalue.show(indent=indent, label_lvl=label_lvl+lvl+\"   |\")\n            else:\n                begn = \"%s  %-10s%s \" % (label_lvl+lvl,\n                                        ncol(f.name),\n                                        ct.punct(\"=\"),)\n                reprval = f.i2repr(self,fvalue)\n                if type(reprval) is str:\n                    reprval = reprval.replace(\"\\n\", \"\\n\"+\" \"*(len(label_lvl)\n                                                              +len(lvl)\n                                                              +len(f.name)\n                                                              +4))\n                print(\"%s%s\" % (begn,vcol(reprval)))\n        self.payload.show(indent=indent, lvl=lvl+(\" \"*indent*self.show_indent), label_lvl=label_lvl)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreassembles the payload and decode it using another packet class", "response": "def decode_payload_as(self,cls):\n        \"\"\"Reassembles the payload and decode it using another packet class\"\"\"\n        s = bytes(self.payload)\n        self.payload = cls(s, _internal=1, _underlayer=self)\n        pp = self\n        while pp.underlayer is not None:\n            pp = pp.underlayer\n        self.payload.dissection_done(pp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef libnet(self):\n        print(\"libnet_build_%s(\" % self.__class__.name.lower())\n        det = self.__class__(str(self))\n        for f in self.fields_desc:\n            val = det.getfieldval(f.name)\n            if val is None:\n                val = 0\n            elif type(val) is int:\n                val = str(val)\n            else:\n                val = '\"%s\"' % str(val)\n            print(\"\\t%s, \\t\\t/* %s */\" % (val,f.name))\n        print(\");\")", "response": "Not ready yet. Should give the necessary C code that interfaces with libnet to recreate the packet"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string representing the command you have to type to obtain the same packet", "response": "def command(self):\n        \"\"\"Returns a string representing the command you have to type to obtain the same packet\"\"\"\n        f = []\n        for fn,fv in self.fields.items():\n            fld = self.get_field(fn)\n            if isinstance(fv, Packet):\n                fv = fv.command()\n            elif fld.islist and fld.holds_packets and type(fv) is list:\n                #fv = \"[%s]\" % \",\".join( map(Packet.command, fv))\n                fv = \"[%s]\" % \",\".join([ Packet.command(i) for i in fv ])\n            else:\n                fv = repr(fv)\n            f.append(\"%s=%s\" % (fn, fv))\n        c = \"%s(%s)\" % (self.__class__.__name__, \", \".join(f))\n        pc = self.payload.command()\n        if pc:\n            c += \"/\"+pc\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dissect(self, b):\n        if len(b) < 8:\n            raise ValueError(\"given packet too short\")\n        return super(DoIPRawPacket, self).dissect(b)", "response": "Dissect an incoming DoIP packet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delt(self, dst, gw=None):\n        tmp = dst+b\"/128\"\n        dst, plen = tmp.split(b'/')[:2]\n        dst = in6_ptop(dst)\n        plen = int(plen)\n        #l = filter(lambda x: in6_ptop(x[0]) == dst and x[1] == plen, self.routes)\n        l = [ x for x in self.routes if in6_ptop(x[0]) == dst and x[1] == plen ]\n        if gw:\n            gw = in6_ptop(gw)\n            #l = filter(lambda x: in6_ptop(x[0]) == gw, self.routes)\n            l = [ x for x in self.routes if in6_ptop(x[0]) == gw ]\n        if len(l) == 0:\n            warning(\"No matching route found\")\n        elif len(l) > 1:\n            warning(\"Found more than one match. Aborting.\")\n        else:\n            i=self.routes.index(l[0])\n            self.invalidate_cache()\n            del(self.routes[i])", "response": "A method to remove a specific entry from a set of entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef route(self, dst, dev=None):\n        # Transform \"2001:db8:cafe:*::1-5:0/120\" to one IPv6 address of the set\n        dst = dst.split(\"/\")[0]\n        savedst = dst # In case following inet_pton() fails \n        dst = dst.replace(\"*\",\"0\")\n        l = dst.find(\"-\")\n        while l >= 0:\n            m = (dst[l:]+\":\").find(\":\")\n            dst = dst[:l]+dst[l+m:]\n            l = dst.find(\"-\")\n            \n        try:\n            inet_pton(socket.AF_INET6, dst)\n        except socket.error:\n            dst = socket.getaddrinfo(savedst, None, socket.AF_INET6)[0][-1][0]\n            # TODO : Check if name resolution went well\n\n        # Deal with dev-specific request for cache search\n        k = dst\n        if dev is not None:\n            k = dst + \"%%\" + dev\n        if k in self.cache:\n            return self.cache[k]\n\n        pathes = []\n\n        # TODO : review all kinds of addresses (scope and *cast) to see\n        #        if we are able to cope with everything possible. I'm convinced \n        #        it's not the case.\n        # -- arnaud\n        for p, plen, gw, iface, cset in self.routes:\n            if dev is not None and iface != dev:\n                continue\n            if in6_isincluded(dst, p, plen):\n                pathes.append((plen, (iface, cset, gw)))\n            elif (in6_ismlladdr(dst) and in6_islladdr(p) and in6_islladdr(cset[0])):\n                pathes.append((plen, (iface, cset, gw)))\n                \n        if not pathes:\n            warning(\"No route found for IPv6 destination %s (no default route?). This affects only IPv6\" % dst)\n            return (LOOPBACK_NAME, \"::\", \"::\") # XXX Linux specific\n\n        # Sort with longest prefix first\n        pathes.sort(reverse=True)\n\n        best_plen = pathes[0][0]\n        pathes = filter(lambda x: x[0] == best_plen, pathes)\n\n        res = []\n        for p in pathes: # Here we select best source address for every route\n            tmp = p[1]\n            srcaddr = get_source_addr_from_candidate_set(dst, p[1][1])\n            if srcaddr is not None:\n                res.append((p[0], (tmp[0], srcaddr, tmp[2])))\n \n        if res == []:\n            warning(\"Found a route for IPv6 destination '%s', but no possible source address. This affects only IPv6\" % dst)\n            return (LOOPBACK_NAME, b\"::\", b\"::\") # XXX Linux specific\n\n        # Symptom  : 2 routes with same weight (our weight is plen)\n        # Solution : \n        #  - dst is unicast global. Check if it is 6to4 and we have a source \n        #    6to4 address in those available\n        #  - dst is link local (unicast or multicast) and multiple output\n        #    interfaces are available. Take main one (conf.iface6)\n        #  - if none of the previous or ambiguity persists, be lazy and keep\n        #    first one\n        #  XXX TODO : in a _near_ future, include metric in the game\n\n        if len(res) > 1:\n            tmp = []\n            if in6_isgladdr(dst) and in6_isaddr6to4(dst):\n                # TODO : see if taking the longest match between dst and\n                #        every source addresses would provide better results\n                #tmp = filter(lambda x: in6_isaddr6to4(x[1][1]), res)\n                tmp = [ x for x in res if in6_isaddr6to4(x[1][1]) ]\n            elif in6_ismaddr(dst) or in6_islladdr(dst):\n                # TODO : I'm sure we are not covering all addresses. Check that\n                #tmp = filter(lambda x: x[1][0] == conf.iface6, res)\n                tmp = [ x for x in res if x[1][0] == conf.iface6 ]\n\n            if tmp:\n                res = tmp\n                \n        # Fill the cache (including dev-specific request)\n        k = dst\n        if dev is not None:\n            k = dst + \"%%\" + dev\n        self.cache[k] = res[0][1]\n\n        return res[0][1]", "response": "Return a set of entries for a given destination address."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef arping(net, timeout=2, cache=0, verbose=None, **kargs):\n    if verbose is None:\n        verbose = conf.verb\n    ans,unans = srp(Ether(dst=\"ff:ff:ff:ff:ff:ff\")/ARP(pdst=net), verbose=verbose,\n                    filter=\"arp and arp[7] = 2\", timeout=timeout, iface_hint=net, **kargs)\n    ans = ARPingResult(ans.res)\n\n    if cache and ans is not None:\n        for pair in ans:\n            conf.netcache.arp_cache[pair[1].psrc] = (pair[1].hwsrc, time.time())\n    if verbose:\n        ans.show()\n    return ans,unans", "response": "Send ARP who - has requests to determine which hosts are up"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inet_pton(af, addr):\n    print('hello')\n    if af == socket.AF_INET:\n        return inet_aton(addr)\n    elif af == socket.AF_INET6:\n        # IPv6: The use of \"::\" indicates one or more groups of 16 bits of zeros.\n        # We deal with this form of wildcard using a special marker. \n        JOKER = b\"*\"\n        while b\"::\" in addr:\n            addr = addr.replace(b\"::\", b\":\" + JOKER + b\":\")\n        joker_pos = None \n        \n        # The last part of an IPv6 address can be an IPv4 address\n        ipv4_addr = None\n        if b\".\" in addr:\n            ipv4_addr = addr.split(b\":\")[-1]\n           \n        result = b\"\"\n        parts = addr.split(b\":\")\n        for part in parts:\n            if part == JOKER:\n                # Wildcard is only allowed once\n                if joker_pos is None:\n                   joker_pos = len(result)\n                else:\n                   raise Exception(\"Illegal syntax for IP address\")\n            elif part == ipv4_addr: # FIXME: Make sure IPv4 can only be last part\n                # FIXME: inet_aton allows IPv4 addresses with less than 4 octets \n                result += socket.inet_aton(ipv4_addr)\n            else:\n                # Each part must be 16bit. Add missing zeroes before decoding. \n                try:\n                    result += part.rjust(4, b\"0\").decode(\"hex\")\n                except TypeError:\n                    raise Exception(\"Illegal syntax for IP address\")\n                    \n        # If there's a wildcard, fill up with zeros to reach 128bit (16 bytes) \n        if JOKER in addr:\n            result = (result[:joker_pos] + b\"\\x00\" * (16 - len(result))\n                      + result[joker_pos:])\n    \n        if len(result) != 16:\n            raise Exception(\"Illegal syntax for IP address\")\n        return result \n    else:\n        raise Exception(\"Address family not supported\")", "response": "Convert an IP address from text representation into binary form."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert an IP address from binary form into text represenation", "response": "def inet_ntop(af, addr):\n    \"\"\"Convert an IP address from binary form into text represenation\"\"\"\n    if af == socket.AF_INET:\n        return inet_ntoa(addr)\n    elif af == socket.AF_INET6:\n        # IPv6 addresses have 128bits (16 bytes)\n        if len(addr) != 16:\n            raise Exception(\"Illegal syntax for IP address\")\n        parts = []\n        for left in [0, 2, 4, 6, 8, 10, 12, 14]:\n            try: \n                value = struct.unpack(\"!H\", addr[left:left+2])[0]\n                hexstr = hex(value)[2:]\n            except TypeError:\n                raise Exception(\"Illegal syntax for IP address\")\n            parts.append(hexstr.lstrip(\"0\").lower())\n        result = b\":\".join(parts)\n        while b\":::\" in result:\n            result = result.replace(b\":::\", b\"::\")\n        # Leaving out leading and trailing zeros is only allowed with ::\n        if result.endswith(b\":\") and not result.endswith(b\"::\"):\n            result = result + b\"0\"\n        if result.startswith(b\":\") and not result.startswith(b\"::\"):\n            result = b\"0\" + result\n        return result\n    else:\n        raise Exception(\"Address family not supported yet\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a random string of length l with zero in it.", "response": "def zerofree_randstring(l):\n    \"\"\"\n    Returns a random string of length l (l >= 0) without zero in it. \n    \"\"\"\n    tmp = map(lambda x: struct.pack(\"B\", random.randrange(1, 256, 1)), [\"\"]*l)\n    return \"\".join(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the binary AND of the 2 provided strings s1 and s2.", "response": "def strand(s1, s2):\n    \"\"\"\n    Returns the binary AND of the 2 provided strings s1 and s2. s1 and s2\n    must be of same length.\n    \"\"\"\n    return \"\".join(map(lambda x,y:chr(ord(x)&ord(y)), s1, s2))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a long (the first parameter) to the associated byte string representation of length l (second parameter). Basically, the length parameters allow the function to perform the associated padding. Input : x nonnegative integer to be converted xLen intended length of the resulting octet string Output: x corresponding nonnegative integer Reverse function is pkcs_os2ip().", "response": "def pkcs_i2osp(x,xLen):\n    \"\"\"\n    Converts a long (the first parameter) to the associated byte string\n    representation of length l (second parameter). Basically, the length\n    parameters allow the function to perform the associated padding.\n\n    Input : x        nonnegative integer to be converted\n            xLen     intended length of the resulting octet string\n\n    Output: x        corresponding nonnegative integer\n\n    Reverse function is pkcs_os2ip().\n    \"\"\"\n    z = number.long_to_bytes(x)\n    padlen = max(0, xLen-len(z))\n    return '\\x00'*padlen + z"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimplement generic MGF1 Mask Generation function as described in Appendix B.2.1 of RFC 3447. The hash function is passed by name. valid values are 'md2', 'md4', 'md5', 'sha1', 'tls, 'sha256', 'sha384' and 'sha512'. Returns None on error. Input: mgfSeed: seed from which mask is generated, an octet string maskLen: intended length in octets of the mask, at most 2^32 * hLen hLen (see below) h : hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls', 'sha256', 'sha384'). hLen denotes the length in octets of the hash function output. Output: an octet string of length maskLen", "response": "def pkcs_mgf1(mgfSeed, maskLen, h):\n    \"\"\"\n    Implements generic MGF1 Mask Generation function as described in\n    Appendix B.2.1 of RFC 3447. The hash function is passed by name.\n    valid values are 'md2', 'md4', 'md5', 'sha1', 'tls, 'sha256',\n    'sha384' and 'sha512'. Returns None on error.\n\n    Input:\n       mgfSeed: seed from which mask is generated, an octet string\n       maskLen: intended length in octets of the mask, at most 2^32 * hLen\n                hLen (see below)\n       h      : hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls',\n                'sha256', 'sha384'). hLen denotes the length in octets of\n                the hash function output.\n\n    Output:\n       an octet string of length maskLen\n    \"\"\"\n\n    # steps are those of Appendix B.2.1\n    if not h in _hashFuncParams:\n        warning(\"pkcs_mgf1: invalid hash (%s) provided\")\n        return None\n    hLen = _hashFuncParams[h][0]\n    hFunc = _hashFuncParams[h][1]\n    if maskLen > 2**32 * hLen:                               # 1)\n        warning(\"pkcs_mgf1: maskLen > 2**32 * hLen\")         \n        return None\n    T = \"\"                                                   # 2)\n    maxCounter = math.ceil(float(maskLen) / float(hLen))     # 3)\n    counter = 0\n    while counter < maxCounter:\n        C = pkcs_i2osp(counter, 4)\n        T += hFunc(mgfSeed + C)\n        counter += 1\n    return T[:maskLen]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimplements EMSA-PSS-ENCODE() function described in Sect. 9.1.1 of RFC 3447 Input: M : message to be encoded, an octet string emBits: maximal bit length of the integer resulting of pkcs_os2ip(EM), where EM is the encoded message, output of the function. h : hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls', 'sha256', 'sha384'). hLen denotes the length in octets of the hash function output. mgf : the mask generation function f : seed, maskLen -> mask sLen : intended length in octets of the salt Output: encoded message, an octet string of length emLen = ceil(emBits/8) On error, None is returned.", "response": "def pkcs_emsa_pss_encode(M, emBits, h, mgf, sLen): \n    \"\"\"\n    Implements EMSA-PSS-ENCODE() function described in Sect. 9.1.1 of RFC 3447\n\n    Input:\n       M     : message to be encoded, an octet string\n       emBits: maximal bit length of the integer resulting of pkcs_os2ip(EM),\n               where EM is the encoded message, output of the function.\n       h     : hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls',\n               'sha256', 'sha384'). hLen denotes the length in octets of\n               the hash function output. \n       mgf   : the mask generation function f : seed, maskLen -> mask\n       sLen  : intended length in octets of the salt\n\n    Output:\n       encoded message, an octet string of length emLen = ceil(emBits/8)\n\n    On error, None is returned.\n    \"\"\"\n\n    # 1) is not done\n    hLen = _hashFuncParams[h][0]                             # 2)\n    hFunc = _hashFuncParams[h][1]\n    mHash = hFunc(M)\n    emLen = int(math.ceil(emBits/8.))\n    if emLen < hLen + sLen + 2:                              # 3)\n        warning(\"encoding error (emLen < hLen + sLen + 2)\")\n        return None\n    salt = randstring(sLen)                                  # 4)\n    MPrime = '\\x00'*8 + mHash + salt                         # 5)\n    H = hFunc(MPrime)                                        # 6)\n    PS = '\\x00'*(emLen - sLen - hLen - 2)                    # 7)\n    DB = PS + '\\x01' + salt                                  # 8)\n    dbMask = mgf(H, emLen - hLen - 1)                        # 9)\n    maskedDB = strxor(DB, dbMask)                            # 10)\n    l = (8*emLen - emBits)/8                                 # 11)\n    rem = 8*emLen - emBits - 8*l # additionnal bits\n    andMask = l*'\\x00'\n    if rem:\n        j = chr(reduce(lambda x,y: x+y, map(lambda x: 1<<x, range(8-rem))))\n        andMask += j\n        l += 1\n    maskedDB = strand(maskedDB[:l], andMask) + maskedDB[l:]\n    EM = maskedDB + H + '\\xbc'                               # 12)\n    return EM"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimplementing EMSA-PSS-VERIFY() function described in Sect. 9.1.2 of RFC 3447 Input: M : message to be encoded, an octet string EM : encoded message, an octet string of length emLen = ceil(emBits/8) emBits: maximal bit length of the integer resulting of pkcs_os2ip(EM) h : hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls', 'sha256', 'sha384'). hLen denotes the length in octets of the hash function output. mgf : the mask generation function f : seed, maskLen -> mask sLen : intended length in octets of the salt Output: True if the verification is ok, False otherwise.", "response": "def pkcs_emsa_pss_verify(M, EM, emBits, h, mgf, sLen):\n    \"\"\"\n    Implements EMSA-PSS-VERIFY() function described in Sect. 9.1.2 of RFC 3447\n\n    Input:\n       M     : message to be encoded, an octet string\n       EM    : encoded message, an octet string of length emLen = ceil(emBits/8)\n       emBits: maximal bit length of the integer resulting of pkcs_os2ip(EM)\n       h     : hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls',\n               'sha256', 'sha384'). hLen denotes the length in octets of\n               the hash function output.\n       mgf   : the mask generation function f : seed, maskLen -> mask\n       sLen  : intended length in octets of the salt\n\n    Output:\n       True if the verification is ok, False otherwise.\n    \"\"\"\n    \n    # 1) is not done\n    hLen = _hashFuncParams[h][0]                             # 2)\n    hFunc = _hashFuncParams[h][1]\n    mHash = hFunc(M)\n    emLen = int(math.ceil(emBits/8.))                        # 3)\n    if emLen < hLen + sLen + 2:\n        return False\n    if EM[-1] != '\\xbc':                                     # 4)\n        return False\n    l = emLen - hLen - 1                                     # 5)\n    maskedDB = EM[:l]\n    H = EM[l:l+hLen]\n    l = (8*emLen - emBits)/8                                 # 6)\n    rem = 8*emLen - emBits - 8*l # additionnal bits\n    andMask = l*'\\xff'\n    if rem:\n        val = reduce(lambda x,y: x+y, map(lambda x: 1<<x, range(8-rem)))\n        j = chr(~val & 0xff)\n        andMask += j\n        l += 1\n    if strand(maskedDB[:l], andMask) != '\\x00'*l:\n        return False\n    dbMask = mgf(H, emLen - hLen - 1)                        # 7)\n    DB = strxor(maskedDB, dbMask)                            # 8)\n    l = (8*emLen - emBits)/8                                 # 9)\n    rem = 8*emLen - emBits - 8*l # additionnal bits\n    andMask = l*'\\x00'\n    if rem:\n        j = chr(reduce(lambda x,y: x+y, map(lambda x: 1<<x, range(8-rem))))\n        andMask += j\n        l += 1\n    DB = strand(DB[:l], andMask) + DB[l:]\n    l = emLen - hLen - sLen - 1                              # 10)\n    if DB[:l] != '\\x00'*(l-1) + '\\x01':\n        return False\n    salt = DB[-sLen:]                                        # 11)\n    MPrime = '\\x00'*8 + mHash + salt                         # 12)\n    HPrime = hFunc(MPrime)                                   # 13)\n    return H == HPrime"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pkcs_emsa_pkcs1_v1_5_encode(M, emLen, h): # section 9.2 of RFC 3447\n    hLen = _hashFuncParams[h][0]                             # 1)\n    hFunc = _hashFuncParams[h][1]\n    H = hFunc(M)\n    hLeadingDigestInfo = _hashFuncParams[h][2]               # 2)\n    T = hLeadingDigestInfo + H\n    tLen = len(T)\n    if emLen < tLen + 11:                                    # 3)\n        warning(\"pkcs_emsa_pkcs1_v1_5_encode: intended encoded message length too short\")\n        return None\n    PS = '\\xff'*(emLen - tLen - 3)                           # 4)\n    EM = '\\x00' + '\\x01' + PS + '\\x00' + T                   # 5)\n    return EM", "response": "This function encodes a message M according to the EMSA - PKCS1 - V1 - 5 - ENCODE function described in Section 9. 2 of RFC 3447."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_ca_file(anchor_list, filename):\n    try:\n        f = open(filename, \"w\")\n        for a in anchor_list:\n            s = a.output(fmt=\"PEM\")\n            f.write(s)\n        f.close()\n    except:\n        return None\n    return filename", "response": "Create a CAfile from a list of anchor_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_temporary_ca_file(anchor_list):\n    try:\n        f, fname = tempfile.mkstemp()\n        for a in anchor_list:\n            s = a.output(fmt=\"PEM\")\n            l = os.write(f, s)\n        os.close(f)\n    except:\n        return None\n    return fname", "response": "Create a temporary CAfile for the given list of anchor_list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_temporary_ca_path(anchor_list, folder):\n    # We should probably avoid writing duplicate anchors and also\n    # check if they are all certs.\n    try:\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n    except:\n        return None\n    \n    l = len(anchor_list)\n    if l == 0:\n        return None\n    fmtstr = \"%%0%sd.pem\" % math.ceil(math.log(l, 10))\n    i = 0\n    try:\n        for a in anchor_list:\n            fname = os.path.join(folder, fmtstr % i)\n            f = open(fname, \"w\")\n            s = a.output(fmt=\"PEM\")\n            f.write(s)\n            f.close()\n            i += 1\n    except:\n        return None\n\n    r,w,e=popen3([\"c_rehash\", folder])\n    r.close(); w.close(); e.close()\n\n    return l", "response": "Create a temporary CA path for the given list of anchors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencrypting message 'm' using 't' encryption scheme where 't' can be: - None: the message 'm' is directly applied the RSAEP encryption primitive, as described in PKCS#1 v2.1, i.e. RFC 3447 Sect 5.1.1. Simply put, the message undergo a modular exponentiation using the public key. Additionnal method parameters are just ignored. - 'pkcs': the message 'm' is applied RSAES-PKCS1-V1_5-ENCRYPT encryption scheme as described in section 7.2.1 of RFC 3447. In that context, other parameters ('h', 'mgf', 'l') are not used. - 'oaep': the message 'm' is applied the RSAES-OAEP-ENCRYPT encryption scheme, as described in PKCS#1 v2.1, i.e. RFC 3447 Sect 7.1.1. In that context, o 'h' parameter provides the name of the hash method to use. Possible values are \"md2\", \"md4\", \"md5\", \"sha1\", \"tls\", \"sha224\", \"sha256\", \"sha384\" and \"sha512\". if none is provided, sha1 is used. o 'mgf' is the mask generation function. By default, mgf is derived from the provided hash function using the generic MGF1 (see pkcs_mgf1() for details). o 'L' is the optional label to be associated with the message. If not provided, the default value is used, i.e the empty string. No check is done on the input limitation of the hash function regarding the size of 'L' (for instance, 2^61 - 1 for SHA-1). You have been warned.", "response": "def encrypt(self, m, t=None, h=None, mgf=None, L=None):\n        \"\"\"\n        Encrypt message 'm' using 't' encryption scheme where 't' can be:\n\n        - None: the message 'm' is directly applied the RSAEP encryption\n                primitive, as described in PKCS#1 v2.1, i.e. RFC 3447\n                Sect 5.1.1. Simply put, the message undergo a modular\n                exponentiation using the public key. Additionnal method\n                parameters are just ignored.\n\n        - 'pkcs': the message 'm' is applied RSAES-PKCS1-V1_5-ENCRYPT encryption\n                scheme as described in section 7.2.1 of RFC 3447. In that\n                context, other parameters ('h', 'mgf', 'l') are not used.\n\n        - 'oaep': the message 'm' is applied the RSAES-OAEP-ENCRYPT encryption\n                scheme, as described in PKCS#1 v2.1, i.e. RFC 3447 Sect\n                7.1.1. In that context,\n\n                o 'h' parameter provides the name of the hash method to use.\n                  Possible values are \"md2\", \"md4\", \"md5\", \"sha1\", \"tls\",\n                  \"sha224\", \"sha256\", \"sha384\" and \"sha512\". if none is provided,\n                  sha1 is used.\n\n                o 'mgf' is the mask generation function. By default, mgf\n                  is derived from the provided hash function using the\n                  generic MGF1 (see pkcs_mgf1() for details).\n\n                o 'L' is the optional label to be associated with the\n                  message. If not provided, the default value is used, i.e\n                  the empty string. No check is done on the input limitation\n                  of the hash function regarding the size of 'L' (for\n                  instance, 2^61 - 1 for SHA-1). You have been warned.\n        \"\"\"\n\n        if h is not None:\n            h = mapHashFunc(h)\n        if t is None: # Raw encryption\n            return self.key.encrypt(\n                m,\n                padding.AssymmetricPadding(),\n            )\n        \n        elif t == \"pkcs\":\n            return self.key.encrypt(\n                m,\n                padding.PKCS1v15(),\n            )\n        \n        elif t == \"oaep\":\n            return self.key.encrypt(\n                m,\n                padding.OAEP(\n                    mgf=mgf(h()),\n                    algorithm=h(),\n                    label=L,\n                ),\n            )\n        else:\n            warning(\"Key.encrypt(): Unknown encryption type (%s) provided\" % t)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify(self, M, S, t=None, h=None, mgf=None, sLen=None):\n        if h is not None:\n            h = mapHashFunc(h)\n        if t is None: #RSAVP1\n            pad_inst = padding.AsymmetricPadding()\n        elif t == \"pkcs\": # RSASSA-PKCS1-v1_5-VERIFY\n            if h is None:\n                h = hashes.SHA1\n            pad_inst = padding.PKCS1v15()\n\n        elif t == \"pss\": # RSASSA-PSS-VERIFY\n            pad_inst = padding.PSS(mgf = mgf, salg_length = sLen)\n\n        else:\n            warning(\"Key.verify(): Unknown signature type (%s) provided\" % t)\n            return None\n        try:\n            self.key.verify(\n                signature=S,\n                data=M,\n                padding=pad_inst,\n                algorithm=h(),\n            )\n            return True\n        except InvalidSignature:\n            return False", "response": "Verify the signature of message M using the public key t and the signature of message S."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _rsaes_pkcs1_v1_5_decrypt(self, C):\n        \n        # 1) Length checking\n        cLen = len(C)\n        k = self.modulusLen / 8\n        if cLen != k or k < 11:\n            warning(\"Key._rsaes_pkcs1_v1_5_decrypt() decryption error \"\n                    \"(cLen != k or k < 11)\")\n            return None\n\n        # 2) RSA decryption\n        c = pkcs_os2ip(C)                           # 2.a)\n        m = self._rsadp(c)                          # 2.b)\n        EM = pkcs_i2osp(m, k)                       # 2.c)\n\n        # 3) EME-PKCS1-v1_5 decoding\n\n        # I am aware of the note at the end of 7.2.2 regarding error\n        # conditions reporting but the one provided below are for _local_\n        # debugging purposes. --arno\n        \n        if EM[0] != '\\x00':\n            warning(\"Key._rsaes_pkcs1_v1_5_decrypt(): decryption error \"\n                    \"(first byte is not 0x00)\")\n            return None\n\n        if EM[1] != '\\x02':\n            warning(\"Key._rsaes_pkcs1_v1_5_decrypt(): decryption error \"\n                    \"(second byte is not 0x02)\")\n            return None\n\n        tmp = EM[2:].split('\\x00', 1)\n        if len(tmp) != 2:\n            warning(\"Key._rsaes_pkcs1_v1_5_decrypt(): decryption error \"\n                    \"(no 0x00 to separate PS from M)\")\n            return None\n\n        PS, M = tmp\n        if len(PS) < 8:\n            warning(\"Key._rsaes_pkcs1_v1_5_decrypt(): decryption error \"\n                    \"(PS is less than 8 byte long)\")\n            return None\n\n        return M", "response": "Internal function to decrypt a message of length 11 using RSAES - PKCS1 - v1 - 5 - DECRYPT."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _rsaes_oaep_decrypt(self, C, h=None, mgf=None, L=None):\n        # The steps below are the one described in Sect. 7.1.2 of RFC 3447.\n\n        # 1) Length Checking\n                                                    # 1.a) is not done\n        if h is None:\n            h = \"sha1\"\n        if not h in _hashFuncParams:\n            warning(\"Key._rsaes_oaep_decrypt(): unknown hash function %s.\", h)\n            return None\n        hLen = _hashFuncParams[h][0]\n        hFun = _hashFuncParams[h][1]\n        k = self.modulusLen / 8\n        cLen = len(C)\n        if cLen != k:                               # 1.b)\n            warning(\"Key._rsaes_oaep_decrypt(): decryption error. \"\n                    \"(cLen != k)\")\n            return None\n        if k < 2*hLen + 2:\n            warning(\"Key._rsaes_oaep_decrypt(): decryption error. \"\n                    \"(k < 2*hLen + 2)\")\n            return None\n\n        # 2) RSA decryption\n        c = pkcs_os2ip(C)                           # 2.a)\n        m = self._rsadp(c)                          # 2.b)\n        EM = pkcs_i2osp(m, k)                       # 2.c)\n\n        # 3) EME-OAEP decoding\n        if L is None:                               # 3.a)\n            L = \"\"\n        lHash = hFun(L)\n        Y = EM[:1]                                  # 3.b)\n        if Y != '\\x00':\n            warning(\"Key._rsaes_oaep_decrypt(): decryption error. \"\n                    \"(Y is not zero)\")\n            return None\n        maskedSeed = EM[1:1+hLen]\n        maskedDB = EM[1+hLen:]\n        if mgf is None:\n            mgf = lambda x,y: pkcs_mgf1(x, y, h)\n        seedMask = mgf(maskedDB, hLen)              # 3.c)\n        seed = strxor(maskedSeed, seedMask)         # 3.d)\n        dbMask = mgf(seed, k - hLen - 1)            # 3.e)\n        DB = strxor(maskedDB, dbMask)               # 3.f)\n\n        # I am aware of the note at the end of 7.1.2 regarding error\n        # conditions reporting but the one provided below are for _local_\n        # debugging purposes. --arno\n\n        lHashPrime = DB[:hLen]                      # 3.g)\n        tmp = DB[hLen:].split('\\x01', 1)\n        if len(tmp) != 2:\n            warning(\"Key._rsaes_oaep_decrypt(): decryption error. \"\n                    \"(0x01 separator not found)\")\n            return None\n        PS, M = tmp\n        if PS != '\\x00'*len(PS):\n            warning(\"Key._rsaes_oaep_decrypt(): decryption error. \"\n                    \"(invalid padding string)\")\n            return None\n        if lHash != lHashPrime:\n            warning(\"Key._rsaes_oaep_decrypt(): decryption error. \"\n                    \"(invalid hash)\")\n            return None            \n        return M", "response": "This method decrypts the ciphertext C using RSAES - OAEP - DECRYPT as defined in Section 3. 1. 2 of RFC 3447."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decrypt(self, C, t=None, h=None, mgf=None, L=None):\n        if t is None:\n            C = pkcs_os2ip(C)\n            c = self._rsadp(C)\n            l = int(math.ceil(math.log(c, 2) / 8.)) # Hack\n            return pkcs_i2osp(c, l)\n\n        elif t == \"pkcs\":\n            return self._rsaes_pkcs1_v1_5_decrypt(C)\n\n        elif t == \"oaep\":\n            return self._rsaes_oaep_decrypt(C, h, mgf, L)\n\n        else:\n            warning(\"Key.decrypt(): Unknown decryption type (%s) provided\" % t)\n            return None", "response": "This method decrypts the ciphertext C using t and h and L."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rsassa_pss_sign(self, M, h=None, mgf=None, sLen=None):\n\n        # Set default parameters if not provided\n        if h is None: # By default, sha1\n            h = \"sha1\"\n        if not h in _hashFuncParams:\n            warning(\"Key._rsassa_pss_sign(): unknown hash function \"\n                    \"provided (%s)\" % h)\n            return None\n        if mgf is None: # use mgf1 with underlying hash function\n            mgf = lambda x,y: pkcs_mgf1(x, y, h)\n        if sLen is None: # use Hash output length (A.2.3 of RFC 3447)\n            hLen = _hashFuncParams[h][0]\n            sLen = hLen\n\n        # 1) EMSA-PSS encoding\n        modBits = self.modulusLen\n        k = modBits / 8\n        EM = pkcs_emsa_pss_encode(M, modBits - 1, h, mgf, sLen)\n        if EM is None:\n            warning(\"Key._rsassa_pss_sign(): unable to encode\")\n            return None\n\n        # 2) RSA signature\n        m = pkcs_os2ip(EM)                          # 2.a)\n        s = self._rsasp1(m)                         # 2.b)\n        S = pkcs_i2osp(s, k)                        # 2.c)\n\n        return S", "response": "Private function to generate a signature for a message M."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimplements RSASSA-PKCS1-v1_5-SIGN() function as described in Sect. 8.2.1 of RFC 3447. Input: M: message to be signed, an octet string h: hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls' 'sha256', 'sha384'). Output: the signature, an octet string.", "response": "def _rsassa_pkcs1_v1_5_sign(self, M, h):\n        \"\"\"\n        Implements RSASSA-PKCS1-v1_5-SIGN() function as described in\n        Sect. 8.2.1 of RFC 3447.\n\n        Input:\n           M: message to be signed, an octet string\n           h: hash function name (in 'md2', 'md4', 'md5', 'sha1', 'tls'\n                'sha256', 'sha384').\n           \n        Output:\n           the signature, an octet string.\n        \"\"\"\n        \n        # 1) EMSA-PKCS1-v1_5 encoding\n        k = self.modulusLen / 8\n        EM = pkcs_emsa_pkcs1_v1_5_encode(M, k, h)\n        if EM is None:\n            warning(\"Key._rsassa_pkcs1_v1_5_sign(): unable to encode\")\n            return None\n\n        # 2) RSA signature\n        m = pkcs_os2ip(EM)                          # 2.a)\n        s = self._rsasp1(m)                         # 2.b)\n        S = pkcs_i2osp(s, k)                        # 2.c)\n\n        return S"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sign(self, M, t=None, h=None, mgf=None, sLen=None):\n\n        if t is None: # RSASP1\n            M = pkcs_os2ip(M)\n            n = self.modulus\n            if M > n-1:\n                warning(\"Message to be signed is too long for key modulus\")\n                return None\n            s = self._rsasp1(M)\n            if s is None:\n                return None\n            return pkcs_i2osp(s, self.modulusLen/8)\n        \n        elif t == \"pkcs\": # RSASSA-PKCS1-v1_5-SIGN\n            if h is None:\n                h = \"sha1\"\n            return self._rsassa_pkcs1_v1_5_sign(M, h)\n        \n        elif t == \"pss\": # RSASSA-PSS-SIGN\n            return self._rsassa_pss_sign(M, h, mgf, sLen)\n\n        else:\n            warning(\"Key.sign(): Unknown signature type (%s) provided\" % t)\n            return None", "response": "Signs a message M using the RSA - SAFE signature scheme."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if self is signed by other False otherwise.", "response": "def isIssuerCert(self, other):\n        \"\"\"\n        True if 'other' issued 'self', i.e.:\n          - self.issuer == other.subject\n          - self is signed by other\n        \"\"\"\n        # XXX should be done on raw values, instead of their textual repr\n        if self.issuer != other.subject:\n            return False\n\n        # Sanity check regarding modulus length and the\n        # signature length\n        keyLen = (other.modulusLen + 7)/8\n        if keyLen != self.sigLen:\n            return False\n\n        unenc = other.encrypt(self.sig) # public key encryption, i.e. decrypt\n\n        # XXX Check block type (00 or 01 and type of padding)\n        unenc = unenc[1:]\n        if not '\\x00' in unenc:\n            return False\n        pos = unenc.index('\\x00')\n        unenc = unenc[pos+1:]\n\n        found = None\n        for k in _hashFuncParams.keys():\n            if self.sigAlg.startswith(k):\n                found = k\n                break\n        if not found:\n            return False\n        hlen, hfunc, digestInfo =  _hashFuncParams[k]\n        \n        if len(unenc) != (hlen+len(digestInfo)):\n            return False\n\n        if not unenc.startswith(digestInfo):\n            return False\n\n        h = unenc[-hlen:]\n        myh = hfunc(self.tbsCertificate)\n\n        return h == myh"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chain(self, certlist):\n        d = {}\n        for c in certlist:\n            # XXX we should check if we have duplicate\n            d[c.subject] = c\n        res = [self]\n        cur = self\n        while not cur.isSelfSigned():\n            if cur.issuer in d:\n                possible_issuer = d[cur.issuer]\n                if cur.isIssuerCert(possible_issuer):\n                    res.append(possible_issuer)\n                    cur = possible_issuer\n                else:\n                    break\n        return res", "response": "Construct the chain of certificates leading from self to the root using the certificate list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number of days the certificate is still valid at the given date.", "response": "def remainingDays(self, now=None):\n        \"\"\"\n        Based on the value of notBefore field, returns the number of\n        days the certificate will still be valid. The date used for the\n        comparison is the current and local date, as returned by \n        time.localtime(), except if 'now' argument is provided another\n        one. 'now' argument can be given as either a time tuple or a string\n        representing the date. Accepted format for the string version\n        are:\n        \n         - '%b %d %H:%M:%S %Y %Z' e.g. 'Jan 30 07:38:59 2008 GMT'\n         - '%m/%d/%y' e.g. '01/30/08' (less precise)\n\n        If the certificate is no more valid at the date considered, then,\n        a negative value is returned representing the number of days\n        since it has expired.\n        \n        The number of days is returned as a float to deal with the unlikely\n        case of certificates that are still just valid.\n        \"\"\"\n        if now is None:\n            now = time.localtime()\n        elif type(now) is str:\n            try:\n                if '/' in now:\n                    now = time.strptime(now, '%m/%d/%y')\n                else:\n                    now = time.strptime(now, '%b %d %H:%M:%S %Y %Z')\n            except:\n                warning(\"Bad time string provided '%s'. Using current time\" % now)\n                now = time.localtime()\n\n        now = time.mktime(now)\n        nft = time.mktime(self.notAfter)\n        diff = (nft - now)/(24.*3600)\n        return diff"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export(self, filename, fmt=\"DER\"):\n        f = open(filename, \"wb\")\n        f.write(self.output(fmt))\n        f.close()", "response": "Export certificate in fmt format to file filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the certificate is self signed.", "response": "def isSelfSigned(self):\n        \"\"\"\n        Return True if the certificate is self signed:\n          - issuer and subject are the same\n          - the signature of the certificate is valid.\n        \"\"\"\n        if self.issuer == self.subject:\n            return self.isIssuerCert(self)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify the certificate chain for a particular set of anchors.", "response": "def verifychain(self, anchors, untrusted=None):\n        \"\"\"\n        Perform verification of certificate chains for that certificate. The\n        behavior of verifychain method is mapped (and also based) on openssl\n        verify userland tool (man 1 verify).\n        A list of anchors is required. untrusted parameter can be provided \n        a list of untrusted certificates that can be used to reconstruct the\n        chain.\n\n        If you have a lot of certificates to verify against the same\n        list of anchor, consider constructing this list as a cafile\n        and use .verifychain_from_cafile() instead.\n        \"\"\"\n        cafile = create_temporary_ca_file(anchors)\n        if not cafile:\n            return False\n        untrusted_file = None\n        if untrusted:\n            untrusted_file = create_temporary_ca_file(untrusted) # hack\n            if not untrusted_file:\n                os.unlink(cafile)\n                return False\n        res = self.verifychain_from_cafile(cafile, \n                                           untrusted_file=untrusted_file)\n        os.unlink(cafile)\n        if untrusted_file:\n            os.unlink(untrusted_file)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying the certificate chain for the current entry in the cafile.", "response": "def verifychain_from_cafile(self, cafile, untrusted_file=None):\n        \"\"\"\n        Does the same job as .verifychain() but using the list of anchors\n        from the cafile. This is useful (because more efficient) if\n        you have a lot of certificates to verify do it that way: it\n        avoids the creation of a cafile from anchors at each call.\n\n        As for .verifychain(), a list of untrusted certificates can be\n        passed (as a file, this time)\n        \"\"\"\n        cmd = [\"openssl\", \"verify\", \"-CAfile\", cafile]\n        if untrusted_file:\n           cmd += [\"-untrusted\", untrusted_file]\n        try:\n            pemcert = self.output(fmt=\"PEM\")\n            cmdres = self._apply_ossl_cmd(cmd, pemcert)\n        except:\n            return False\n        return cmdres.endswith(\"\\nOK\\n\") or cmdres.endswith(\": OK\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying that the capath directory contains all certificates in the current directory.", "response": "def verifychain_from_capath(self, capath, untrusted_file=None):\n        \"\"\"\n        Does the same job as .verifychain_from_cafile() but using the list\n        of anchors in capath directory. The directory should contain\n        certificates files in PEM format with associated links as\n        created using c_rehash utility (man c_rehash).\n\n        As for .verifychain_from_cafile(), a list of untrusted certificates\n        can be passed as a file (concatenation of the certificates in\n        PEM format)\n        \"\"\"\n        cmd = [\"openssl\", \"verify\", \"-CApath\", capath]\n        if untrusted_file:\n            cmd += [\"-untrusted\", untrusted_file]\n        try:\n            pemcert = self.output(fmt=\"PEM\")\n            cmdres = self._apply_ossl_cmd(cmd, pemcert)\n        except:\n            return False\n        return cmdres.endswith(\"\\nOK\\n\") or cmdres.endswith(\": OK\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_revoked(self, crl_list):\n        for c in crl_list:\n            if (self.authorityKeyID is not None and \n                c.authorityKeyID is not None and\n                self.authorityKeyID == c.authorityKeyID):\n                return self.serial in map(lambda x: x[0], c.revoked_cert_serials)\n            elif (self.issuer == c.issuer):\n                return self.serial in map(lambda x: x[0], c.revoked_cert_serials)\n        return False", "response": "This function checks if a certificate is revoked by one of the trusted CRLs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verify(self, anchors):\n        cafile = create_temporary_ca_file(anchors)\n        if cafile is None:\n            return False\n        try:\n            cmd = self.osslcmdbase + [\"-noout\", \"-CAfile\", cafile]\n            cmdres = self._apply_ossl_cmd(cmd, self.rawcrl)\n        except:\n            os.unlink(cafile)\n            return False\n        os.unlink(cafile)\n        return \"verify OK\" in cmdres", "response": "Return True if the CRL is signed by one of the provided\n        anchors. False on error (invalid signature, missing anchorand, ...)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend packets at layer 3.", "response": "def send(x, inter=0, loop=0, count=None, verbose=None, realtime=None, *args, **kargs):\n    \"\"\"Send packets at layer 3\nsend(packets, [inter=0], [loop=0], [verbose=conf.verb]) -> None\"\"\"\n    __gen_send(conf.L3socket(*args, **kargs), x, inter=inter, loop=loop, count=count,verbose=verbose, realtime=realtime)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sendp(x, inter=0, loop=0, iface=None, iface_hint=None, count=None, verbose=None, realtime=None, *args, **kargs):\n    if iface is None and iface_hint is not None:\n        iface = conf.route.route(iface_hint)[0]\n    __gen_send(conf.L2socket(iface=iface, *args, **kargs), x, inter=inter, loop=loop, count=count, verbose=verbose, realtime=realtime)", "response": "Send packets at layer 2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sendpfast(x, pps=None, mbps=None, realtime=None, loop=0, file_cache=False, iface=None, verbose=True):\n    if iface is None:\n        iface = conf.iface\n    argv = [conf.prog.tcpreplay, \"--intf1=%s\" % iface ]\n    if pps is not None:\n        argv.append(\"--pps=%i\" % pps)\n    elif mbps is not None:\n        argv.append(\"--mbps=%f\" % mbps)\n    elif realtime is not None:\n        argv.append(\"--multiplier=%i\" % realtime)\n    else:\n        argv.append(\"--topspeed\")\n    if not verbose:\n        argv.append(\"-q\")\n    if loop:\n        argv.append(\"--loop=%i\" % loop)\n        if file_cache:\n            argv.append(\"--enable-file-cache\")\n\n    f = get_temp_file()\n    argv.append(f)\n    wrpcap(f, x)\n    with open(os.devnull, \"wb\") as null:\n        proc_output = null if not verbose else None\n        try:\n            subprocess.check_call(argv,\n                                  stdout=proc_output,\n                                  stderr=proc_output)\n        except KeyboardInterrupt:\n            log_interactive.info(\"Interrupted by user\")\n        except Exception as e:\n            log_interactive.error(\"while trying to exec [%s]: %s\" % (argv[0],e))\n        finally:\n            os.unlink(f)", "response": "Send packets at layer 2 using tcpreplay for performance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sr(x,filter=None, iface=None, nofilter=0, *args,**kargs):\n    if not \"timeout\" in kargs:\n        kargs[\"timeout\"] = -1\n    s = conf.L3socket(filter=filter, iface=iface, nofilter=nofilter)\n    a,b=sndrcv(s,x,*args,**kargs)\n    s.close()\n    return a,b", "response": "Send and receive packets at layer 3"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending and receive packets at layer 2 and return only the first answered packet", "response": "def srp1(*args,**kargs):\n    \"\"\"Send and receive packets at layer 2 and return only the first answer\nnofilter: put 1 to avoid use of bpf filters\nretry:    if positive, how many times to resend unanswered packets\n          if negative, how many times to retry when no more packets are answered\ntimeout:  how much time to wait after the last packet has been sent\nverbose:  set verbosity level\nmulti:    whether to accept multiple answers for the same stimulus\nfilter:   provide a BPF filter\niface:    work only on the given interface\"\"\"\n    if not \"timeout\" in kargs:\n        kargs[\"timeout\"] = -1\n    a,b=srp(*args,**kargs)\n    if len(a) > 0:\n        return a[0][1]\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsniffing packets sniff([count=0,] [prn=None,] [store=1,] [offline=None,] [lfilter=None,] + L2ListenSocket args) -> list of packets count: number of packets to capture. 0 means infinity store: wether to store sniffed packets or discard them prn: function to apply to each packet. If something is returned, it is displayed. Ex: ex: prn = lambda x: x.summary() lfilter: python function applied to each packet to determine if further action may be done ex: lfilter = lambda x: x.haslayer(Padding) offline: pcap file to read packets from, instead of sniffing them timeout: stop sniffing after a given time (default: None) L2socket: use the provided L2socket opened_socket: provide an object ready to use .recv() on stop_filter: python function applied to each packet to determine if we have to stop the capture after this packet ex: stop_filter = lambda x: x.haslayer(TCP) exceptions: reraise caught exceptions such as KeyboardInterrupt when a user interrupts sniffing stop_callback: Call every loop to determine if we need to stop the capture", "response": "def sniff(count=0, store=1, offline=None, prn = None, lfilter=None, L2socket=None, timeout=None,\n          opened_socket=None, stop_filter=None, exceptions=False, stop_callback=None, *arg, **karg):\n    \"\"\"Sniff packets\nsniff([count=0,] [prn=None,] [store=1,] [offline=None,] [lfilter=None,] + L2ListenSocket args) -> list of packets\n\n  count: number of packets to capture. 0 means infinity\n  store: wether to store sniffed packets or discard them\n    prn: function to apply to each packet. If something is returned,\n         it is displayed. Ex:\n         ex: prn = lambda x: x.summary()\nlfilter: python function applied to each packet to determine\n         if further action may be done\n         ex: lfilter = lambda x: x.haslayer(Padding)\noffline: pcap file to read packets from, instead of sniffing them\ntimeout: stop sniffing after a given time (default: None)\nL2socket: use the provided L2socket\nopened_socket: provide an object ready to use .recv() on\nstop_filter: python function applied to each packet to determine\n             if we have to stop the capture after this packet\n             ex: stop_filter = lambda x: x.haslayer(TCP)\nexceptions: reraise caught exceptions such as KeyboardInterrupt\n            when a user interrupts sniffing\nstop_callback: Call every loop to determine if we need\n               to stop the capture\n    \"\"\"\n    c = 0\n    \n    if opened_socket is not None:\n        s = opened_socket\n    else:\n        if offline is None:\n            if L2socket is None:\n                L2socket = conf.L2listen\n            s = L2socket(type=ETH_P_ALL, *arg, **karg)\n        else:\n            s = PcapReader(offline)\n\n    lst = []\n    if timeout is not None:\n        stoptime = time.time()+timeout\n    remain = None\n    try:\n        while 1:\n            if timeout is not None:\n                remain = stoptime-time.time()\n                if remain <= 0:\n                    break\n            if stop_callback and stop_callback():\n                break\n            sel = select([s],[],[],remain)\n            if s in sel[0]:\n                p = s.recv(MTU)\n                if p is None:\n                    break\n                if lfilter and not lfilter(p):\n                    continue\n                if store:\n                    lst.append(p)\n                c += 1\n                if prn:\n                    r = prn(p)\n                    if r is not None:\n                        print(r)\n                if stop_filter and stop_filter(p):\n                    break\n                if count > 0 and c >= count:\n                    break\n    except KeyboardInterrupt:\n        if exceptions:\n            raise\n        else:\n            pass\n    finally:\n        if opened_socket is None:\n            s.close()\n\n    return plist.PacketList(lst,\"Sniffed\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bridge_and_sniff(if1, if2, count=0, store=1, offline=None, prn = None, lfilter=None, L2socket=None, timeout=None,\n                     stop_filter=None, stop_callback=None, *args, **kargs):\n    \"\"\"Forward traffic between two interfaces and sniff packets exchanged\nbridge_and_sniff([count=0,] [prn=None,] [store=1,] [offline=None,] [lfilter=None,] + L2Socket args) -> list of packets\n\n  count: number of packets to capture. 0 means infinity\n  store: wether to store sniffed packets or discard them\n    prn: function to apply to each packet. If something is returned,\n         it is displayed. Ex:\n         ex: prn = lambda x: x.summary()\nlfilter: python function applied to each packet to determine\n         if further action may be done\n         ex: lfilter = lambda x: x.haslayer(Padding)\ntimeout: stop sniffing after a given time (default: None)\nL2socket: use the provided L2socket\nstop_filter: python function applied to each packet to determine\n             if we have to stop the capture after this packet\n             ex: stop_filter = lambda x: x.haslayer(TCP)\nstop_callback: Call every loop to determine if we need\n               to stop the capture\n    \"\"\"\n    c = 0\n    if L2socket is None:\n        L2socket = conf.L2socket\n    s1 = L2socket(iface=if1)\n    s2 = L2socket(iface=if2)\n    peerof={s1:s2,s2:s1}\n    label={s1:if1, s2:if2}\n    \n    lst = []\n    if timeout is not None:\n        stoptime = time.time()+timeout\n    remain = None\n    try:\n        while True:\n            if timeout is not None:\n                remain = stoptime-time.time()\n                if remain <= 0:\n                    break\n            if stop_callback and stop_callback():\n                break\n            ins,outs,errs = select([s1,s2],[],[], remain)\n            for s in ins:\n                p = s.recv()\n                if p is not None:\n                    peerof[s].send(p.original)\n                    if lfilter and not lfilter(p):\n                        continue\n                    if store:\n                        p.sniffed_on = label[s]\n                        lst.append(p)\n                    c += 1\n                    if prn:\n                        r = prn(p)\n                        if r is not None:\n                            print(\"%s: %s\" % (label[s],r))\n                    if stop_filter and stop_filter(p):\n                        break\n                    if count > 0 and c >= count:\n                        break\n    except KeyboardInterrupt:\n        pass\n    finally:\n        return plist.PacketList(lst,\"Sniffed\")", "response": "Forward traffic between two interfaces and sniff packets exchanged\nBridgeAndSniff"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsniffing packets and print them calling pkt. show", "response": "def tshark(*args,**kargs):\n    \"\"\"Sniff packets and print them calling pkt.show(), a bit like text wireshark\"\"\"\n    sniff(prn=lambda x: x.display(),*args,**kargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fragment(pkt, fragsize=1480):\n    fragsize = (fragsize + 7) // 8 * 8\n    lst = []\n    for p in pkt:\n        s = bytes(p[IP].payload)\n        nb = (len(s) + fragsize - 1) // fragsize\n        for i in range(nb):\n            q = p.copy()\n            del q[IP].payload\n            del q[IP].chksum\n            del q[IP].len\n            if i == nb - 1:\n                q[IP].flags &= ~1\n            else:\n                q[IP].flags |= 1\n            q[IP].frag = i * fragsize // 8\n            r = conf.raw_layer(load=s[i * fragsize:(i + 1) * fragsize])\n            r.overload_fields = p[IP].payload.overload_fields.copy()\n            q.add_payload(r)\n            lst.append(q)\n    return lst", "response": "Fragment a big IP datagram"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _packetlist_timeskew_graph(self, ip, **kargs):\n    res = map(lambda x: self._elt2pkt(x), self.res)\n    b = filter(lambda x: x.haslayer(IP) and x.getlayer(IP).src == ip and x.haslayer(TCP), res)\n    c = []\n    for p in b:\n        opts = p.getlayer(TCP).options\n        for o in opts:\n            if o[0] == \"Timestamp\":\n                c.append((p.time, o[1][0]))\n    if not c:\n        warning(\"No timestamps found in packet list\")\n        return\n    # d = map(lambda (x,y): (x%2000,((x-c[0][0])-((y-c[0][1])/1000.0))),c)\n    d = map(lambda a: (a[0] % 2000, ((a[0] - c[0][0]) - ((a[1] - c[0][1]) / 1000.0))), c)\n    return plt.plot(d, **kargs)", "response": "Tries to graph the timeskew between the timestamps and real time for a given ip"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mtr(target, dport=80, minttl=1, maxttl=30, stype=\"Random\", srcport=50000, iface=None, l4=None, filter=None, timeout=2, verbose=None, gw=None, netproto=\"TCP\", nquery=1, ptype=None, payload=b'', privaddr=0, rasn=1, **kargs):\n    #\n    # Initialize vars...\n    trace = []\t\t\t# Individual trace array\n    #\n    # Range check number of query traces\n    if nquery < 1:\n        nquery = 1\n    #\n    # Create instance of an MTR class...\n    mtrc = MTR(nquery=nquery, target=target)\n    #\n    # Default to network protocol: \"TCP\" if not found in list...\n    plist = [\"TCP\", \"UDP\", \"ICMP\"]\n    netproto = netproto.upper()\n    if netproto not in plist:\n        netproto = \"TCP\"\n    mtrc._netprotocol = netproto\n    #\n    # Default to source type: \"Random\" if not found in list...\n    slist = [\"Random\", \"Increment\"]\n    stype = stype.title()\n    if stype not in slist:\n        stype = \"Random\"\n    if stype == \"Random\":\n        sport = RandShort()  # Random\n    elif stype == \"Increment\":\n        if srcport != None:\n            sport = IncrementalValue(start=(srcport - 1), step=1, restart=65535)  # Increment\n    #\n    # Default to payload type to it's default network protocol value if not found in list...\n    pllist = [\"Disabled\", \"RandStr\", \"RandStrTerm\", \"Custom\"]\n    if ptype is None or (not ptype in pllist):\n        if netproto == \"ICMP\":\n            ptype = \"RandStr\"\t\t# ICMP: A random string payload to fill out the minimum packet size\n        elif netproto == \"UDP\":\n            ptype = \"RandStrTerm\"  # UDP: A random string terminated payload to fill out the minimum packet size\n        elif netproto == \"TCP\":\n            ptype = \"Disabled\"\t\t# TCP: Disabled -> The minimum packet size satisfied - no payload required\n    #\n    # Set trace interface...\n    if not iface is None:\n        mtrc._iface = iface\n    else:\n        mtrc._iface = conf.iface\n    #\n    # Set Default Gateway...\n    if not gw is None:\n        mtrc._gw = gw\n    #\n    # Set default verbosity if no override...\n    if verbose is None:\n        verbose = conf.verb\n    #\n    # Only consider ICMP error packets and TCP packets with at\n    # least the ACK flag set *and* either the SYN or the RST flag set...\n    filterundefined = False\n    if filter is None:\n        filterundefined = True\n        filter = \"(icmp and (icmp[0]=3 or icmp[0]=4 or icmp[0]=5 or icmp[0]=11 or icmp[0]=12)) or (tcp and (tcp[13] & 0x16 > 0x10))\"\n    #\n    # Resolve and expand each target...\n    ntraces = 0\t\t# Total trace count\n    exptrg = []\t\t# Expanded targets\n    for t in target:\n        #\n        # Use kamene's 'Net' function to expand target...\n        et = [ip for ip in iter(Net(t))]\n        exptrg.extend(et)\n        #\n        # Map Host Names to IP Addresses and store...\n        if t in mtrc._host2ip:\n            mtrc._host2ip[t].extend(et)\n        else:\n            mtrc._host2ip[t] = et\n        #\n        # Map IP Addresses to Host Names and store...\n        for a in et:\n            mtrc._ip2host[a] = t\n    #\n    # Store resolved and expanded targets...\n    mtrc._exptrg = exptrg\n    #\n    # Traceroute each expanded target value...\n    if l4 is None:\n        #\n        # Standard Layer: 3 ('TCP', 'UDP' or 'ICMP') tracing...\n        for n in range(0, nquery):\n            for t in exptrg:\n                #\n                # Execute a traceroute based on network protocol setting...\n                if netproto == \"ICMP\":\n                    #\n                    # MTR Network Protocol: 'ICMP'\n                    tid = 8\t\t\t\t        # Use a 'Type: 8 - Echo Request' packet for the trace:\n                    id = 0x8888\t\t\t\t\t# MTR ICMP identifier: '0x8888'\n                    seq = IncrementalValue(start=(minttl - 2), step=1, restart=-10)  # Use a Sequence number in step with TTL value\n                    if filterundefined:\n                        #\n                        # Update Filter -> Allow for ICMP echo-request (8) and ICMP echo-reply (0) packet to be processed...\n                        filter = \"(icmp and (icmp[0]=8 or icmp[0]=0 or icmp[0]=3 or icmp[0]=4 or icmp[0]=5 or icmp[0]=11 or icmp[0]=12))\"\n                    #\n                    # Check payload types:\n                    if ptype == 'Disabled':\n                        a, b = sr(IP(dst=[t], id=RandShort(), ttl=(minttl, maxttl)) / ICMP(type=tid, id=id, seq=seq),\n                                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n                    else:\n                        if ptype == 'RandStr':\n                            #\n                            # Use a random payload string to full out a minimum size PDU of 46 bytes for each ICMP packet:\n                            # Length of 'IP()/ICMP()' = 28, Minimum Protocol Data Unit (PDU) is = 46 -> Therefore a\n                            # payload of 18 octets is required.\n                            pload = RandString(size=18)\n                        elif ptype == 'RandStrTerm':\n                            pload = RandStringTerm(size=17, term=b'\\n')  # Random string terminated\n                        elif ptype == 'Custom':\n                            pload = payload\n                        #\n                        # ICMP trace with payload...\n                        a, b = sr(IP(dst=[t], id=RandShort(), ttl=(minttl, maxttl)) / ICMP(type=tid, id=id, seq=seq) / Raw(load=pload),\n                                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n                elif netproto == \"UDP\":\n                    #\n                    # MTR Network Protocol: 'UDP'\n                    if filterundefined:\n                        filter += \" or udp\"\t\t\t# Update Filter -> Allow for processing UDP packets\n                    #\n                    # Check payload types:\n                    if ptype == 'Disabled':\n                        a, b = sr(IP(dst=[t], id=RandShort(), ttl=(minttl, maxttl)) / UDP(sport=sport, dport=dport),\n                                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n                    else:\n                        if ptype == 'RandStr':\n                            #\n                            # Use a random payload string to full out a minimum size PDU of 46 bytes for each UDP packet:\n                            # Length of 'IP()/UDP()' = 28, Minimum PDU is = 46 -> Therefore a payload of 18 octets is required.\n                            pload = RandString(size=18)\n                        elif ptype == 'RandStrTerm':\n                            pload = RandStringTerm(size=17, term=b'\\n')  # Random string terminated\n                        elif ptype == 'Custom':\n                            pload = payload\n                        #\n                        # UDP trace with payload...\n                        a, b = sr(IP(dst=[t], id=RandShort(), ttl=(minttl, maxttl)) / UDP(sport=sport, dport=dport) / Raw(load=pload),\n                                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n                else:\n                    #\n                    # Default MTR Network Protocol: 'TCP'\n                    #\n                    # Use some TCP options for the trace. Some firewalls will filter\n                    # TCP/IP packets without the 'Timestamp' option set.\n                    #\n                    # Note: The minimum PDU size of 46 is statisfied with the use of TCP options.\n                    #\n                    # Use an integer encoded microsecond timestamp for the TCP option timestamp for each trace sequence.\n                    uts = IntAutoMicroTime()\n                    opts = [('MSS', 1460), ('NOP', None), ('NOP', None), ('Timestamp', (uts, 0)), ('NOP', None), ('WScale', 7)]\n                    seq = RandInt()\t\t# Use a random TCP sequence number\n                    #\n                    # Check payload types:\n                    if ptype == 'Disabled':\n                        a, b = sr(IP(dst=[t], id=RandShort(), ttl=(minttl, maxttl)) / TCP(seq=seq, sport=sport, dport=dport, options=opts),\n                                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n                    else:\n                        if ptype == 'RandStr':\n                            pload = RandString(size=32)\t\t\t# Use a 32 byte random string\n                        elif ptype == 'RandStrTerm':\n                            pload = RandStringTerm(size=32, term=b'\\n')  # Use a 32 byte random string terminated\n                        elif ptype == 'Custom':\n                            pload = payload\n                        #\n                        # TCP trace with payload...\n                        a, b = sr(IP(dst=[t], id=RandShort(),\n                                     ttl=(minttl, maxttl)) / TCP(seq=seq, sport=sport, dport=dport, options=opts) / Raw(load=pload),\n                                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n                #\n                # Create an 'MTracerouteResult' instance for each result packets...\n                trace.append(MTracerouteResult(res=a.res))\n                mtrc._res.append(a)\t\t# Store Response packets\n                mtrc._ures.append(b)\t\t# Store Unresponse packets\n                if verbose:\n                    trace[ntraces].show(ntrace=(ntraces + 1))\n                    print()\n                ntraces += 1\n    else:\n        #\n        # Custom Layer: 4 tracing...\n        filter = \"ip\"\n        for n in range(0, nquery):\n            for t in exptrg:\n                #\n                # Run traceroute...\n                a, b = sr(IP(dst=[t], id=RandShort(), ttl=(minttl, maxttl)) / l4,\n                          timeout=timeout, filter=filter, verbose=verbose, **kargs)\n                trace.append(MTracerouteResult(res=a.res))\n                mtrc._res.append(a)\n                mtrc._ures.append(b)\n                if verbose:\n                    trace[ntraces].show(ntrace=(ntraces + 1))\n                    print()\n                ntraces += 1\n    #\n    # Store total trace run count...\n    mtrc._ntraces = ntraces\n    #\n    # Get the trace components...\n    # for n in range(0, ntraces):\n    for n in range(0, mtrc._ntraces):\n        trace[n].get_trace_components(mtrc, n)\n    #\n    # Compute any Black Holes...\n    mtrc.get_black_holes()\n    #\n    # Compute Trace Hop Ranges...\n    mtrc.compute_hop_ranges()\n    #\n    # Resolve AS Numbers...\n    if rasn:\n        mtrc.get_asns(privaddr)\n        #\n        # Try to guess ASNs for Traceroute 'Unkown Hops'...\n        mtrc.guess_unk_asns()\n    #\n    # Debug: Print object vars at verbose level 8...\n    if verbose == 8:\n        print(\"mtrc._target (User Target(s)):\")\n        print(\"=======================================================\")\n        print(mtrc._target)\n        print(\"\\nmtrc._exptrg (Resolved and Expanded Target(s)):\")\n        print(\"=======================================================\")\n        print(mtrc._exptrg)\n        print(\"\\nmtrc._host2ip (Target Host Name to IP Address):\")\n        print(\"=======================================================\")\n        print(mtrc._host2ip)\n        print(\"\\nmtrc._ip2host (Target IP Address to Host Name):\")\n        print(\"=======================================================\")\n        print(mtrc._ip2host)\n        print(\"\\nmtrc._res (Trace Response Packets):\")\n        print(\"=======================================================\")\n        print(mtrc._res)\n        print(\"\\nmtrc._ures (Trace Unresponse Packets):\")\n        print(\"=======================================================\")\n        print(mtrc._ures)\n        print(\"\\nmtrc._ips (Trace Unique IPv4 Addresses):\")\n        print(\"=======================================================\")\n        print(mtrc._ips)\n        print(\"\\nmtrc._rt (Individual Route Traces):\")\n        print(\"=======================================================\")\n        print(mtrc._rt)\n        print(\"\\nmtrc._rtt (Round Trip Times (msecs) for Trace Nodes):\")\n        print(\"=======================================================\")\n        print(mtrc._rtt)\n        print(\"\\nmtrc._hops (Traceroute Hop Ranges):\")\n        print(\"=======================================================\")\n        print(mtrc._hops)\n        print(\"\\nmtrc._tlblid (Target Trace Label IDs):\")\n        print(\"=======================================================\")\n        print(mtrc._tlblid)\n        print(\"\\nmtrc._ports (Completed Targets & Ports):\")\n        print(\"=======================================================\")\n        print(mtrc._ports)\n        print(\"\\nmtrc._portsdone (Completed Trace Routes & Ports):\")\n        print(\"=======================================================\")\n        print(mtrc._portsdone)\n        print(\"\\nconf.L3socket (Layer 3 Socket Method):\")\n        print(\"=======================================================\")\n        print(conf.L3socket)\n        print(\"\\nconf.AS_resolver Resolver (AS Resolver Method):\")\n        print(\"=======================================================\")\n        print(conf.AS_resolver)\n        print(\"\\nmtrc._asns (AS Numbers):\")\n        print(\"=======================================================\")\n        print(mtrc._asns)\n        print(\"\\nmtrc._asds (AS Descriptions):\")\n        print(\"=======================================================\")\n        print(mtrc._asds)\n        print(\"\\nmtrc._unks (Unknown Hops IP Boundary for AS Numbers):\")\n        print(\"=======================================================\")\n        print(mtrc._unks)\n        print(\"\\nmtrc._iface (Trace Interface):\")\n        print(\"=======================================================\")\n        print(mtrc._iface)\n        print(\"\\nmtrc._gw (Trace Default Gateway IPv4 Address):\")\n        print(\"=======================================================\")\n        print(mtrc._gw)\n\n    return mtrc", "response": "A multi - triply MTR command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a 3D representation of the traceroute.", "response": "def trace3D(self):\n        \"\"\"Give a 3D representation of the traceroute.\n        right button: rotate the scene\n        middle button: zoom\n        left button: move the scene\n        left button on a ball: toggle IP displaying\n        ctrl-left button on a ball: scan ports 21,22,23,25,80 and 443 and display the result\"\"\"\n        trace = self.get_trace()\n        import visual\n\n        class IPsphere(visual.sphere):\n            def __init__(self, ip, **kargs):\n                visual.sphere.__init__(self, **kargs)\n                self.ip = ip\n                self.label = None\n                self.setlabel(self.ip)\n\n            def setlabel(self, txt, visible=None):\n                if self.label is not None:\n                    if visible is None:\n                        visible = self.label.visible\n                    self.label.visible = 0\n                elif visible is None:\n                    visible = 0\n                self.label = visual.label(text=txt, pos=self.pos, space=self.radius, xoffset=10, yoffset=20, visible=visible)\n\n            def action(self):\n                self.label.visible ^= 1\n\n        visual.scene = visual.display()\n        visual.scene.exit = True\n        start = visual.box()\n        rings = {}\n        tr3d = {}\n        for i in trace:\n            tr = trace[i]\n            tr3d[i] = []\n            ttl = tr.keys()\n            for t in range(1, max(ttl) + 1):\n                if t not in rings:\n                    rings[t] = []\n                if t in tr:\n                    if tr[t] not in rings[t]:\n                        rings[t].append(tr[t])\n                    tr3d[i].append(rings[t].index(tr[t]))\n                else:\n                    rings[t].append((\"unk\", -1))\n                    tr3d[i].append(len(rings[t]) - 1)\n        for t in rings:\n            r = rings[t]\n            l = len(r)\n            for i in range(l):\n                if r[i][1] == -1:\n                    col = (0.75, 0.75, 0.75)\n                elif r[i][1]:\n                    col = visual.color.green\n                else:\n                    col = visual.color.blue\n\n                s = IPsphere(pos=((l - 1) * visual.cos(2 * i * visual.pi / l), (l - 1) * visual.sin(2 * i * visual.pi / l), 2 * t),\n                             ip=r[i][0],\n                             color=col)\n                for trlst in tr3d.values():\n                    if t <= len(trlst):\n                        if trlst[t - 1] == i:\n                            trlst[t - 1] = s\n        forecol = colgen(0.625, 0.4375, 0.25, 0.125)\n        for trlst in tr3d.values():\n            col = next(forecol)\n            start = (0, 0, 0)\n            for ip in trlst:\n                visual.cylinder(pos=start, axis=ip.pos - start, color=col, radius=0.2)\n                start = ip.pos\n\n        movcenter = None\n        while 1:\n            visual.rate(50)\n            if visual.scene.kb.keys:\n                k = visual.scene.kb.getkey()\n                if k == \"esc\" or k == \"q\":\n                    break\n            if visual.scene.mouse.events:\n                ev = visual.scene.mouse.getevent()\n                if ev.press == \"left\":\n                    o = ev.pick\n                    if o:\n                        if ev.ctrl:\n                            if o.ip == \"unk\":\n                                continue\n                            savcolor = o.color\n                            o.color = (1, 0, 0)\n                            a, _ = sr(IP(dst=o.ip) / TCP(dport=[21, 22, 23, 25, 80, 443]), timeout=2)\n                            o.color = savcolor\n                            if len(a) == 0:\n                                txt = \"%s:\\nno results\" % o.ip\n                            else:\n                                txt = \"%s:\\n\" % o.ip\n                                for s, r in a:\n                                    txt += r.sprintf(\"{TCP:%IP.src%:%TCP.sport% %TCP.flags%}{TCPerror:%IPerror.dst%:%TCPerror.dport% %IP.src% %ir,ICMP.type%}\\n\")\n                            o.setlabel(txt, visible=1)\n                        else:\n                            if hasattr(o, \"action\"):\n                                o.action()\n                elif ev.drag == \"left\":\n                    movcenter = ev.pos\n                elif ev.drop == \"left\":\n                    movcenter = None\n            if movcenter:\n                visual.scene.center -= visual.scene.mouse.pos - movcenter\n                movcenter = visual.scene.mouse.pos"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nobtain associated AS Numbers for IPv4 Addreses.", "response": "def get_asns(self, privaddr=0):\n        \"\"\"Obtain associated AS Numbers for IPv4 Addreses.\n           privaddr: 0 - Normal display of AS numbers,\n                     1 - Do not show an associated AS Number bound box (cluster) on graph for a private IPv4 Address.\"\"\"\n        ips = {}\n        if privaddr:\n            for k, v in self._ips.items():\n                if not is_private_addr(k):\n                    ips[k] = v\n        else:\n            ips = self._ips\n        #\n        # Special case for the loopback IP Address: 127.0.0.1 - Do not ASN resolve...\n        if '127.0.0.1' in ips:\n            del ips['127.0.0.1']\n        #\n        # ASN Lookup...\n        asnquerylist = dict.fromkeys(map(lambda x: x.rsplit(\" \", 1)[0], ips)).keys()\n        if self._asres is None:\n            asnlist = []\n        else:\n            try:\n                asnlist = self._asres.resolve(*asnquerylist)\n            except:\n                pass\n        for ip, asn, desc, in asnlist:\n            if asn is None:\n                continue\n            iplist = self._asns.get(asn, [])  # Get previous ASN value\n            iplist.append(ip)\t\t\t# Append IP Address to previous ASN\n\n            #\n            # If ASN is a string Convert to a number: (i.e., 'AS3257' => 3257)\n            if type(asn) == str:\n                asn = asn.upper()\n                asn = asn.replace('AS', '')\n                try:\n                    asn = int(asn)\n                    self._asns[asn] = iplist\n                    self._asds[asn] = desc\n                except:\n                    continue\n            else:\n                self._asns[asn] = iplist\n                self._asds[asn] = desc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef graph(self, ASres=None, padding=0, vspread=0.75, title=\"Multi-Traceroute Probe (MTR)\", timestamp=\"\", rtt=1, **kargs):\n        if self._asres is None:\n            self._asres = conf.AS_resolver\n        if (self._graphdef is None or\t\t# Remake the graph if there are any changes\n            self._graphasres != self._asres or\n                self._graphpadding != padding):\n            self.make_dot_graph(ASres, padding, vspread, title, timestamp, rtt)\n\n        return do_graph(self._graphdef, **kargs)", "response": "Generate a graph of the current node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the program s version from a python variable declaration.", "response": "def parse_version(str_):\n    \"\"\"\n    Parses the program's version from a python variable declaration.\n    \"\"\"\n    v = re.findall(r\"\\d+.\\d+.\\d+\", str_)\n    if v:\n        return v[0]\n    else:\n        print(\"cannot parse string {}\".format(str_))\n        raise KeyError"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_session(username=None, password=None, bearer_token=None, extra_headers_dict=None): \n\n    if password is None and bearer_token is None:\n        logger.error(\"No authentication information provided; \"\n                     \"please check your object\")\n        raise KeyError\n\n    session = requests.Session()\n    session.trust_env = False\n    headers = {'Accept-encoding': 'gzip',\n               'User-Agent': 'twitterdev-search-tweets-python/' + VERSION}\n    if bearer_token:\n        logger.info(\"using bearer token for authentication\")\n        headers['Authorization'] = \"Bearer {}\".format(bearer_token)\n        session.headers = headers\n    else:\n        logger.info(\"using username and password for authentication\")\n        session.auth = username, password\n        session.headers = headers\n    if extra_headers_dict:\n        headers.update(extra_headers_dict) \n    return session", "response": "Creates a Requests Session for use."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(session, url, rule_payload, **kwargs):\n    if isinstance(rule_payload, dict):\n        rule_payload = json.dumps(rule_payload)\n    logger.debug(\"sending request\")\n    result = session.post(url, data=rule_payload, **kwargs)\n    return result", "response": "Executes a request to the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef collect_results(rule, max_results=500, result_stream_args=None):\n    if result_stream_args is None:\n        logger.error(\"This function requires a configuration dict for the \"\n                     \"inner ResultStream object.\")\n        raise KeyError\n\n    rs = ResultStream(rule_payload=rule,\n                      max_results=max_results,\n                      **result_stream_args)\n    return list(rs.stream())", "response": "Utility function to quickly get a list of tweets from a ResultStream object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator that yields all the tweets in the current page.", "response": "def stream(self):\n        \"\"\"\n        Main entry point for the data from the API. Will automatically paginate\n        through the results via the ``next`` token and return up to ``max_results``\n        tweets or up to ``max_requests`` API calls, whichever is lower.\n\n        Usage:\n            >>> result_stream = ResultStream(**kwargs)\n            >>> stream = result_stream.stream()\n            >>> results = list(stream)\n            >>> # or for faster usage...\n            >>> results = list(ResultStream(**kwargs).stream())\n        \"\"\"\n        self.init_session()\n        self.check_counts()\n        self.execute_request()\n        self.stream_started = True\n        while True:\n            for tweet in self.current_tweets:\n                if self.total_results >= self.max_results:\n                    break\n                yield self._tweet_func(tweet)\n                self.total_results += 1\n\n            if self.next_token and self.total_results < self.max_results and self.n_requests <= self.max_requests:\n                self.rule_payload = merge_dicts(self.rule_payload,\n                                                {\"next\": self.next_token})\n                logger.info(\"paging; total requests read so far: {}\"\n                            .format(self.n_requests))\n                self.execute_request()\n            else:\n                break\n        logger.info(\"ending stream at {} tweets\".format(self.total_results))\n        self.current_tweets = None\n        self.session.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing a session object for passing requests.", "response": "def init_session(self):\n        \"\"\"\n        Defines a session object for passing requests.\n        \"\"\"\n        if self.session:\n            self.session.close()\n        self.session = make_session(self.username,\n                                    self.password,\n                                    self.bearer_token,\n                                    self.extra_headers_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisabling tweet parsing if the count API is used.", "response": "def check_counts(self):\n        \"\"\"\n        Disables tweet parsing if the count API is used.\n        \"\"\"\n        if \"counts\" in re.split(\"[/.]\", self.endpoint):\n            logger.info(\"disabling tweet parsing due to counts API usage\")\n            self._tweet_func = lambda x: x"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a request to the API and parses the response.", "response": "def execute_request(self):\n        \"\"\"\n        Sends the request to the API and parses the json response.\n        Makes some assumptions about the session length and sets the presence\n        of a \"next\" token.\n        \"\"\"\n        if self.n_requests % 20 == 0 and self.n_requests > 1:\n            logger.info(\"refreshing session\")\n            self.init_session()\n\n        resp = request(session=self.session,\n                       url=self.endpoint,\n                       rule_payload=self.rule_payload)\n        self.n_requests += 1\n        ResultStream.session_request_counter += 1\n        resp = json.loads(resp.content.decode(resp.encoding))\n        self.next_token = resp.get(\"next\", None)\n        self.current_tweets = resp[\"results\"]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_utc_time(datetime_str):\n    if not datetime_str:\n        return None\n    if not set(['-', ':']) & set(datetime_str):\n        _date = datetime.datetime.strptime(datetime_str, \"%Y%m%d%H%M\")\n    else:\n        try:\n            if \"T\" in datetime_str:\n                # command line with 'T'\n                datetime_str = datetime_str.replace('T', ' ')\n            _date = datetime.datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M\")\n        except ValueError:\n            _date = datetime.datetime.strptime(datetime_str, \"%Y-%m-%d\")\n    return _date.strftime(\"%Y%m%d%H%M\")", "response": "Converts a string to the GNIP API format which is\n    YYYYMMDDHHSS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_to_count_endpoint(endpoint):\n\n    tokens = filter(lambda x: x != '', re.split(\"[/:]\", endpoint))\n    filt_tokens = list(filter(lambda x: x != \"https\", tokens))\n    last = filt_tokens[-1].split('.')[0]  # removes .json on the endpoint\n    filt_tokens[-1] = last  # changes from *.json -> '' for changing input\n    if last == 'counts':\n        return endpoint\n    else:\n        return \"https://\" + '/'.join(filt_tokens) + '/' + \"counts.json\"", "response": "Utility function to change a normal endpoint to a count api\n    endpoint. Returns the same endpoint if it s already a count endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_rule_payload(pt_rule, results_per_call=None,\n                     from_date=None, to_date=None, count_bucket=None,\n                     tag=None,\n                     stringify=True):\n\n    \"\"\"\n    Generates the dict or json payload for a PowerTrack rule.\n\n    Args:\n        pt_rule (str): The string version of a powertrack rule,\n            e.g., \"beyonce has:geo\". Accepts multi-line strings\n            for ease of entry.\n        results_per_call (int): number of tweets or counts returned per API\n        call. This maps to the ``maxResults`` search API parameter.\n            Defaults to 500 to reduce API call usage.\n        from_date (str or None): Date format as specified by\n            `convert_utc_time` for the starting time of your search.\n        to_date (str or None): date format as specified by `convert_utc_time`\n            for the end time of your search.\n        count_bucket (str or None): If using the counts api endpoint,\n            will define the count bucket for which tweets are aggregated.\n        stringify (bool): specifies the return type, `dict`\n            or json-formatted `str`.\n\n    Example:\n\n        >>> from searchtweets.utils import gen_rule_payload\n        >>> gen_rule_payload(\"beyonce has:geo\",\n            ...              from_date=\"2017-08-21\",\n            ...              to_date=\"2017-08-22\")\n        '{\"query\":\"beyonce has:geo\",\"maxResults\":100,\"toDate\":\"201708220000\",\"fromDate\":\"201708210000\"}'\n    \"\"\"\n\n    pt_rule = ' '.join(pt_rule.split())  # allows multi-line strings\n    payload = {\"query\": pt_rule}\n    if results_per_call is not None and isinstance(results_per_call, int) is True:\n        payload[\"maxResults\"] = results_per_call\n    if to_date:\n        payload[\"toDate\"] = convert_utc_time(to_date)\n    if from_date:\n        payload[\"fromDate\"] = convert_utc_time(from_date)\n    if count_bucket:\n        if set([\"day\", \"hour\", \"minute\"]) & set([count_bucket]):\n            payload[\"bucket\"] = count_bucket\n            del payload[\"maxResults\"]\n        else:\n            logger.error(\"invalid count bucket: provided {}\"\n                         .format(count_bucket))\n            raise ValueError\n    if tag:\n        payload[\"tag\"] = tag\n\n    return json.dumps(payload) if stringify else payload", "response": "Generates the dict or json payload for a powertrack rule."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a dictionary of parameters for a ResultStream from a dictionary.", "response": "def gen_params_from_config(config_dict):\n    \"\"\"\n    Generates parameters for a ResultStream from a dictionary.\n    \"\"\"\n\n    if config_dict.get(\"count_bucket\"):\n        logger.warning(\"change your endpoint to the count endpoint; this is \"\n                       \"default behavior when the count bucket \"\n                       \"field is defined\")\n        endpoint = change_to_count_endpoint(config_dict.get(\"endpoint\"))\n    else:\n        endpoint = config_dict.get(\"endpoint\")\n\n\n    def intify(arg):\n        if not isinstance(arg, int) and arg is not None:\n            return int(arg)\n        else:\n            return arg\n\n    # this parameter comes in as a string when it's parsed\n    results_per_call = intify(config_dict.get(\"results_per_call\", None))\n\n    rule = gen_rule_payload(pt_rule=config_dict[\"pt_rule\"],\n                            from_date=config_dict.get(\"from_date\", None),\n                            to_date=config_dict.get(\"to_date\", None),\n                            results_per_call=results_per_call,\n                            count_bucket=config_dict.get(\"count_bucket\", None))\n\n    _dict = {\"endpoint\": endpoint,\n             \"username\": config_dict.get(\"username\"),\n             \"password\": config_dict.get(\"password\"),\n             \"bearer_token\": config_dict.get(\"bearer_token\"),\n             \"extra_headers_dict\": config_dict.get(\"extra_headers_dict\",None),\n             \"rule_payload\": rule,\n             \"results_per_file\": intify(config_dict.get(\"results_per_file\")),\n             \"max_results\": intify(config_dict.get(\"max_results\")),\n             \"max_pages\": intify(config_dict.get(\"max_pages\", None))}\n    return _dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninferring which endpoint should be used for a given rule payload.", "response": "def infer_endpoint(rule_payload):\n    \"\"\"\n    Infer which endpoint should be used for a given rule payload.\n    \"\"\"\n    bucket = (rule_payload if isinstance(rule_payload, dict)\n              else json.loads(rule_payload)).get(\"bucket\")\n    return \"counts\" if bucket else \"search\""}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that the counts api is set correctly in a payload.", "response": "def validate_count_api(rule_payload, endpoint):\n    \"\"\"\n    Ensures that the counts api is set correctly in a payload.\n    \"\"\"\n    rule = (rule_payload if isinstance(rule_payload, dict)\n            else json.loads(rule_payload))\n    bucket = rule.get('bucket')\n    counts = set(endpoint.split(\"/\")) & {\"counts.json\"}\n    if len(counts) == 0:\n        if bucket is not None:\n            msg = (\"\"\"There is a count bucket present in your payload,\n                   but you are using not using the counts API.\n                   Please check your endpoints and try again\"\"\")\n            logger.error(msg)\n            raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef partition(iterable, chunk_size, pad_none=False):\n    args = [iter(iterable)] * chunk_size\n    if not pad_none:\n        return zip(*args)\n    else:\n        return it.zip_longest(*args)", "response": "adapted from Toolz. Breaks an iterable into n iterables up to chunk_size with Nones if availble."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the contents of a result stream to a file.", "response": "def write_result_stream(result_stream, filename_prefix=None,\n                        results_per_file=None, **kwargs):\n    \"\"\"\n    Wraps a ``ResultStream`` object to save it to a file. This function will still\n    return all data from the result stream as a generator that wraps the\n    ``write_ndjson`` method.\n\n    Args:\n        result_stream (ResultStream): the unstarted ResultStream object\n        filename_prefix (str or None): the base name for file writing\n        results_per_file (int or None): the maximum number of tweets to write\n        per file. Defaults to having no max, which means one file. Multiple\n        files will be named by datetime, according to\n        ``<prefix>_YYY-mm-ddTHH_MM_SS.json``.\n\n    \"\"\"\n    if isinstance(result_stream, types.GeneratorType):\n        stream = result_stream\n    else:\n        stream = result_stream.stream()\n\n    file_time_formatter = \"%Y-%m-%dT%H_%M_%S\"\n    if filename_prefix is None:\n        filename_prefix = \"twitter_search_results\"\n\n    if results_per_file:\n        logger.info(\"chunking result stream to files with {} tweets per file\"\n                    .format(results_per_file))\n        chunked_stream = partition(stream, results_per_file, pad_none=True)\n        for chunk in chunked_stream:\n            chunk = filter(lambda x: x is not None, chunk)\n            curr_datetime = (datetime.datetime.utcnow()\n                             .strftime(file_time_formatter))\n            _filename = \"{}_{}.json\".format(filename_prefix, curr_datetime)\n            yield from write_ndjson(_filename, chunk)\n\n    else:\n        curr_datetime = (datetime.datetime.utcnow()\n                         .strftime(file_time_formatter))\n        _filename = \"{}.json\".format(filename_prefix)\n        yield from write_ndjson(_filename, stream)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_config(filename):\n    file_type = \"yaml\" if filename.endswith(\".yaml\") else \"config\"\n    config = configparser.ConfigParser()\n\n    if file_type == \"yaml\":\n        with open(os.path.expanduser(filename)) as f:\n            config_dict = yaml.load(f)\n\n        config_dict = merge_dicts(*[dict(config_dict[s]) for s\n                                    in config_dict.keys()])\n\n    elif file_type == \"config\":\n        with open(filename) as f:\n            config.read_file(f)\n            config_dict = merge_dicts(*[dict(config[s]) for s\n                                        in config.sections()])\n    else:\n        logger.error(\"Config files must be either in YAML or Config style.\")\n        raise TypeError\n\n    # ensure args are renamed correctly:\n    config_dict = {k.replace('-', '_'): v for k, v in config_dict.items()}\n    # YAML will parse datestrings as datetimes; we'll convert them here if they\n    # exist\n    if config_dict.get(\"to_date\") is not None:\n        config_dict[\"to_date\"] = str(config_dict[\"to_date\"])\n    if config_dict.get(\"from_date\") is not None:\n        config_dict[\"from_date\"] = str(config_dict[\"from_date\"])\n    return config_dict", "response": "Reads and flattens a configuration file into a single node dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_yaml_credentials(filename=None, yaml_key=None):\n    try:\n        with open(os.path.expanduser(filename)) as f:\n            search_creds = yaml.safe_load(f)[yaml_key]\n    except FileNotFoundError:\n        logger.error(\"cannot read file {}\".format(filename))\n        search_creds = {}\n    except KeyError:\n        logger.error(\"{} is missing the provided key: {}\"\n                     .format(filename, yaml_key))\n        search_creds = {}\n\n    return search_creds", "response": "Loads and parses the credentials in a YAML file. Catches common exceptions\n            and returns an empty dict on error."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the credentials from a YAML file.", "response": "def load_credentials(filename=None, account_type=None,\n                     yaml_key=None, env_overwrite=True):\n    \"\"\"\n    Handles credential management. Supports both YAML files and environment\n    variables. A YAML file is preferred for simplicity and configureability.\n    A YAML credential file should look something like this:\n\n    .. code:: yaml\n\n        <KEY>:\n          endpoint: <FULL_URL_OF_ENDPOINT>\n          username: <USERNAME>\n          password: <PW>\n          consumer_key: <KEY>\n          consumer_secret: <SECRET>\n          bearer_token: <TOKEN>\n          account_type: <enterprise OR premium>\n          extra_headers: \n            <MY_HEADER_KEY>: <MY_HEADER_VALUE>\n\n    with the appropriate fields filled out for your account. The top-level key\n    defaults to ``search_tweets_api`` but can be flexible.\n\n    If a YAML file is not found or is missing keys, this function will check\n    for this information in the environment variables that correspond to\n\n    .. code: yaml\n\n        SEARCHTWEETS_ENDPOINT\n        SEARCHTWEETS_USERNAME\n        SEARCHTWEETS_PASSWORD\n        SEARCHTWEETS_BEARER_TOKEN\n        SEARCHTWEETS_ACCOUNT_TYPE\n        ...\n\n    Again, set the variables that correspond to your account information and\n    type. See the main documentation for details and more examples.\n\n\n    Args:\n        filename (str): pass a filename here if you do not want to use the\n                        default ``~/.twitter_keys.yaml``\n        account_type (str): your account type, \"premium\" or \"enterprise\". We\n            will attempt to infer the account info if left empty.\n        yaml_key (str): the top-level key in the YAML file that has your\n            information. Defaults to ``search_tweets_api``.\n        env_overwrite: any found environment variables will overwrite values\n            found in a YAML file. Defaults to ``True``.\n\n    Returns:\n        dict: your access credentials.\n\n    Example:\n        >>> from searchtweets.api_utils import load_credentials\n        >>> search_args = load_credentials(account_type=\"premium\",\n                env_overwrite=False)\n        >>> search_args.keys()\n        dict_keys(['bearer_token', 'endpoint'])\n        >>> import os\n        >>> os.environ[\"SEARCHTWEETS_ENDPOINT\"] = \"https://endpoint\"\n        >>> os.environ[\"SEARCHTWEETS_USERNAME\"] = \"areallybadpassword\"\n        >>> os.environ[\"SEARCHTWEETS_PASSWORD\"] = \"<PW>\"\n        >>> load_credentials()\n        {'endpoint': 'https://endpoint',\n         'password': '<PW>',\n         'username': 'areallybadpassword'}\n\n    \"\"\"\n    yaml_key = yaml_key if yaml_key is not None else \"search_tweets_api\"\n    filename = \"~/.twitter_keys.yaml\" if filename is None else filename\n\n    yaml_vars = _load_yaml_credentials(filename=filename, yaml_key=yaml_key)\n    if not yaml_vars:\n        logger.warning(\"Error parsing YAML file; searching for \"\n                       \"valid environment variables\")\n    env_vars = _load_env_credentials()\n    merged_vars = (merge_dicts(yaml_vars, env_vars)\n                   if env_overwrite\n                   else merge_dicts(env_vars, yaml_vars))\n    parsed_vars = _parse_credentials(merged_vars, account_type=account_type)\n    return parsed_vars"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a bearer token for a given pair of consumer key and secret values.", "response": "def _generate_bearer_token(consumer_key, consumer_secret):\n    \"\"\"\n    Return the bearer token for a given pair of consumer key and secret values.\n    \"\"\"\n    data = [('grant_type', 'client_credentials')]\n    resp = requests.post(OAUTH_ENDPOINT,\n                         data=data,\n                         auth=(consumer_key, consumer_secret))\n    logger.warning(\"Grabbing bearer token from OAUTH\")\n    if resp.status_code >= 400:\n        logger.error(resp.text)\n        resp.raise_for_status()\n\n    return resp.json()['access_token']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef email(value, whitelist=None):\n\n    if whitelist is None:\n        whitelist = domain_whitelist\n\n    if not value or '@' not in value:\n        return False\n\n    user_part, domain_part = value.rsplit('@', 1)\n\n    if not user_regex.match(user_part):\n        return False\n\n    if domain_part not in whitelist and not domain_regex.match(domain_part):\n        # Try for possible IDN domain-part\n        try:\n            domain_part = domain_part.encode('idna').decode('ascii')\n            return domain_regex.match(domain_part)\n        except UnicodeError:\n            return False\n    return True", "response": "Validates an email address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fi_business_id(business_id):\n    if not business_id or not re.match(business_id_pattern, business_id):\n        return False\n    factors = [7, 9, 10, 5, 8, 4, 2]\n    numbers = map(int, business_id[:7])\n    checksum = int(business_id[8])\n    sum_ = sum(f * n for f, n in zip(factors, numbers))\n    modulo = sum_ % 11\n    return (11 - modulo == checksum) or (modulo == 0 and checksum == 0)", "response": "Validate a Finnish Business ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fi_ssn(ssn, allow_temporal_ssn=True):\n    if not ssn:\n        return False\n\n    result = re.match(ssn_pattern, ssn)\n    if not result:\n        return False\n    gd = result.groupdict()\n    checksum = int(gd['date'] + gd['serial'])\n    return (\n        int(gd['serial']) >= 2 and\n        (allow_temporal_ssn or int(gd['serial']) <= 899) and\n        ssn_checkmarks[checksum % len(ssn_checkmarks)] ==\n        gd['checksum']\n    )", "response": "Validate a Finnish Social Security Number."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the value string passes the mod97 - test.", "response": "def modcheck(value):\n    \"\"\"Check if the value string passes the mod97-test.\n    \"\"\"\n    # move country code and check numbers to end\n    rearranged = value[4:] + value[:4]\n    # convert letters to numbers\n    converted = [char_value(char) for char in rearranged]\n    # interpret as integer\n    integerized = int(''.join([str(i) for i in converted]))\n    return (integerized % 97 == 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef func_args_as_dict(func, args, kwargs):\n    if six.PY2:\n        _getargspec = inspect.getargspec\n    else:\n        _getargspec = inspect.getfullargspec\n\n    arg_names = list(\n        OrderedDict.fromkeys(\n            itertools.chain(\n                _getargspec(func)[0],\n                kwargs.keys()\n            )\n        )\n    )\n    return OrderedDict(\n        list(six.moves.zip(arg_names, args)) +\n        list(kwargs.items())\n    )", "response": "Return given function s positional and key value arguments as an ordered OrderedDict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef length(value, min=None, max=None):\n    if (min is not None and min < 0) or (max is not None and max < 0):\n        raise AssertionError(\n            '`min` and `max` need to be greater than zero.'\n        )\n    return between(len(value), min=min, max=max)", "response": "Return whether or not the length of given string is within a specified range."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns whether or not given value is a valid URL.", "response": "def url(value, public=False):\n    \"\"\"\n    Return whether or not given value is a valid URL.\n\n    If the value is valid URL this function returns ``True``, otherwise\n    :class:`~validators.utils.ValidationFailure`.\n\n    This validator is based on the wonderful `URL validator of dperini`_.\n\n    .. _URL validator of dperini:\n        https://gist.github.com/dperini/729294\n\n    Examples::\n\n        >>> url('http://foobar.dk')\n        True\n\n        >>> url('ftp://foobar.dk')\n        True\n\n        >>> url('http://10.0.0.1')\n        True\n\n        >>> url('http://foobar.d')\n        ValidationFailure(func=url, ...)\n\n        >>> url('http://10.0.0.1', public=True)\n        ValidationFailure(func=url, ...)\n\n    .. versionadded:: 0.2\n\n    .. versionchanged:: 0.10.2\n\n        Added support for various exotic URLs and fixed various false\n        positives.\n\n    .. versionchanged:: 0.10.3\n\n        Added ``public`` parameter.\n\n    .. versionchanged:: 0.11.0\n\n        Made the regular expression this function uses case insensitive.\n\n    .. versionchanged:: 0.11.3\n\n        Added support for URLs containing localhost\n\n    :param value: URL address string to validate\n    :param public: (default=False) Set True to only allow a public IP address\n    \"\"\"\n    result = pattern.match(value)\n    if not public:\n        return result\n\n    return result and not any(\n        (result.groupdict().get(key) for key in ('private_ip', 'private_host'))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef between(value, min=None, max=None):\n    if min is None and max is None:\n        raise AssertionError(\n            'At least one of `min` or `max` must be specified.'\n        )\n    if min is None:\n        min = Min\n    if max is None:\n        max = Max\n    try:\n        min_gt_max = min > max\n    except TypeError:\n        min_gt_max = max < min\n    if min_gt_max:\n        raise AssertionError('`min` cannot be more than `max`.')\n\n    return min <= value and max >= value", "response": "Validate that a number is between minimum and maximum value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating that given value is a valid IP version 4 address.", "response": "def ipv4(value):\n    \"\"\"\n    Return whether or not given value is a valid IP version 4 address.\n\n    This validator is based on `WTForms IPAddress validator`_\n\n    .. _WTForms IPAddress validator:\n       https://github.com/wtforms/wtforms/blob/master/wtforms/validators.py\n\n    Examples::\n\n        >>> ipv4('123.0.0.7')\n        True\n\n        >>> ipv4('900.80.70.11')\n        ValidationFailure(func=ipv4, args={'value': '900.80.70.11'})\n\n    .. versionadded:: 0.2\n\n    :param value: IP address string to validate\n    \"\"\"\n    groups = value.split('.')\n    if len(groups) != 4 or any(not x.isdigit() for x in groups):\n        return False\n    return all(0 <= int(part) < 256 for part in groups)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ipv6(value):\n    ipv6_groups = value.split(':')\n    if len(ipv6_groups) == 1:\n        return False\n    ipv4_groups = ipv6_groups[-1].split('.')\n\n    if len(ipv4_groups) > 1:\n        if not ipv4(ipv6_groups[-1]):\n            return False\n        ipv6_groups = ipv6_groups[:-1]\n    else:\n        ipv4_groups = []\n\n    max_groups = 6 if ipv4_groups else 8\n    if len(ipv6_groups) > max_groups:\n        return False\n\n    count_blank = 0\n    for part in ipv6_groups:\n        if not part:\n            count_blank += 1\n            continue\n        try:\n            num = int(part, 16)\n        except ValueError:\n            return False\n        else:\n            if not 0 <= num <= 65536:\n                return False\n\n    if count_blank < 2:\n        return True\n    elif count_blank == 2 and not ipv6_groups[0] and not ipv6_groups[1]:\n        return True\n    return False", "response": "Validate that given value is a valid IP version 6 address."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the current version as defined by the given module or file.", "response": "def get_version(module_name_or_file=None):\n    \"\"\"Return the current version as defined by the given module/file.\"\"\"\n\n    if module_name_or_file is None:\n        parts = base_module.split('.')\n        module_name_or_file = parts[0] if len(parts) > 1 else \\\n            find_packages(exclude=['test', 'test.*'])[0]\n\n    if os.path.isdir(module_name_or_file):\n        module_name_or_file = os.path.join(module_name_or_file, '__init__.py')\n\n    with open(module_name_or_file, 'r') as f:\n        match = VERSION_PATTERN.search(f.read())\n        return match.group(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nposts coverage report to coveralls. io.", "response": "def post_report(coverage, args):\n    \"\"\"Post coverage report to coveralls.io.\"\"\"\n    response = requests.post(URL, files={'json_file': json.dumps(coverage)},\n                             verify=(not args.skip_ssl_verify))\n    try:\n        result = response.json()\n    except ValueError:\n        result = {'error': 'Failure to submit data. '\n                  'Response [%(status)s]: %(text)s' % {\n                      'status': response.status_code,\n                      'text': response.text}}\n    print(result)\n    if 'error' in result:\n        return result['error']\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns true if the file is a C ++ source file.", "response": "def is_source_file(args, filepath):\n    \"\"\"Returns true if it is a C++ source file.\"\"\"\n    if args.extension:\n        return os.path.splitext(filepath)[1] in args.extension\n    else:\n        return os.path.splitext(filepath)[1] in _CPP_EXTENSIONS"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exclude_paths(args):\n    results = []\n    if args.exclude:\n        for excl_path in args.exclude:\n            results.append(os.path.abspath(os.path.join(args.root, excl_path)))\n    return results", "response": "Returns the absolute paths for excluded paths."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the exlude rules", "response": "def create_exclude_rules(args):\n    \"\"\"Creates the exlude rules\n    \"\"\"\n    global _cached_exclude_rules\n    if _cached_exclude_rules is not None:\n        return _cached_exclude_rules\n    rules = []\n    for excl_path in args.exclude:\n        abspath = os.path.abspath(os.path.join(args.root, excl_path))\n        rules.append((abspath, True))\n    for incl_path in args.include:\n        abspath = os.path.abspath(os.path.join(args.root, incl_path))\n        rules.append((abspath, False))\n    _cached_exclude_rules = sorted(rules, key=lambda p: p[0])\n    return _cached_exclude_rules"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_excluded_path(args, filepath):\n    # Try regular expressions first.\n    for regexp_exclude_path in args.regexp:\n        if re.match(regexp_exclude_path, filepath):\n            return True\n    abspath = os.path.abspath(filepath)\n    if args.include:\n        # If the file is outside of any include directories.\n        out_of_include_dirs = True\n        for incl_path in args.include:\n            absolute_include_path = os.path.abspath(os.path.join(args.root, incl_path))\n            if is_child_dir(absolute_include_path, abspath):\n                out_of_include_dirs = False\n                break\n        if out_of_include_dirs:\n            return True\n\n    excl_rules = create_exclude_rules(args)\n    for i, rule in enumerate(excl_rules):\n        if rule[0] == abspath:\n            return rule[1]\n        if is_child_dir(rule[0], abspath):\n            # continue to try to longest match.\n            last_result = rule[1]\n            for j in range(i + 1, len(excl_rules)):\n                rule_deep = excl_rules[j]\n                if not is_child_dir(rule_deep[0], abspath):\n                    break\n                last_result = rule_deep[1]\n            return last_result\n    return False", "response": "Returns true if the filepath is under the one of the exclude path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_dirs(root, dirs, excl_paths):\n    filtered_dirs = []\n    for dirpath in dirs:\n        abspath = os.path.abspath(os.path.join(root, dirpath))\n        if os.path.basename(abspath) in _SKIP_DIRS:\n            continue\n        if abspath not in excl_paths:\n            filtered_dirs.append(dirpath)\n    return filtered_dirs", "response": "Filter the directory paths based on the exclusion rules defined in excl_paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_gcov_file(args, fobj, filename):\n    coverage = []\n    ignoring = False\n    for line in fobj:\n        report_fields = line.decode('utf-8', 'replace').split(':', 2)\n        if len(report_fields) == 1:\n            continue\n        line_num = report_fields[1].strip()\n        if line_num == '':\n            continue\n\n        cov_num = report_fields[0].strip()\n        line_num = int(line_num)\n        text = report_fields[2]\n        if line_num == 0:\n            continue\n        if re.search(r'\\bLCOV_EXCL_START\\b', text):\n            if ignoring:\n                sys.stderr.write(\"Warning: %s:%d: nested LCOV_EXCL_START, \"\n                                 \"please fix\\n\" % (filename, line_num))\n            ignoring = True\n        elif re.search(r'\\bLCOV_EXCL_(STOP|END)\\b', text):\n            if not ignoring:\n                sys.stderr.write(\"Warning: %s:%d: LCOV_EXCL_STOP outside of \"\n                                 \"exclusion zone, please fix\\n\" % (filename,\n                                                                   line_num))\n            if 'LCOV_EXCL_END' in text:\n                sys.stderr.write(\"Warning: %s:%d: LCOV_EXCL_STOP is the \"\n                                 \"correct keyword\\n\" % (filename, line_num))\n            ignoring = False\n        if cov_num == '-':\n            coverage.append(None)\n        elif cov_num == '#####':\n            # Avoid false positives.\n            if (\n                ignoring or\n                any([re.search(pattern, text) for pattern in args.exclude_lines_pattern])\n            ):\n                coverage.append(None)\n            else:\n                coverage.append(0)\n        elif cov_num == '=====':\n            # This is indicitive of a gcov output parse\n            # error.\n            coverage.append(0)\n        else:\n            coverage.append(int(cov_num.rstrip('*')))\n    return coverage", "response": "Parses the content of a. gcov file and returns a list of the coverage of the files."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse lcov info file", "response": "def parse_lcov_file_info(args, filepath, line_iter, line_coverage_re, file_end_string):\n    \"\"\" Parse the file content in lcov info file\n    \"\"\"\n    coverage = []\n    lines_covered = []\n    for line in line_iter:\n        if line != \"end_of_record\":\n            line_coverage_match = line_coverage_re.match(line)\n            if line_coverage_match:\n                line_no = line_coverage_match.group(1)\n                cov_count = int(line_coverage_match.group(2))\n                if args.max_cov_count:\n                    if cov_count > args.max_cov_count:\n                        cov_count = args.max_cov_count + 1\n                lines_covered.append((line_no, cov_count))\n        else:\n            break\n\n    num_code_lines = len([line.rstrip('\\n') for line in open(filepath, 'r')])\n    coverage = [None] * num_code_lines\n    for line_covered in lines_covered:\n        coverage[int(line_covered[0]) - 1] = line_covered[1]\n\n    return coverage"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef combine_reports(original, new):\n    if original is None:\n        return new\n    report = {}\n    report['name'] = original['name']\n    report['source_digest'] = original['source_digest']\n    coverage = []\n    for original_num, new_num in zip(original['coverage'], new['coverage']):\n        if original_num is None:\n            coverage.append(new_num)\n        elif new_num is None:\n            coverage.append(original_num)\n        else:\n            coverage.append(original_num + new_num)\n\n    report['coverage'] = coverage\n    return report", "response": "Combines two gcov reports for a file into one by adding the number of hits on each line of each line of each line of each line of each line of each line of each line of each line of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect_non_report_files(args, discovered_files):\n    excl_paths = exclude_paths(args)\n    abs_root = os.path.abspath(args.root)\n    non_report_files = []\n    for root, dirs, files in os.walk(args.root, followlinks=args.follow_symlinks):\n        dirs[:] = filter_dirs(root, dirs, excl_paths)\n\n        for filename in files:\n            if not is_source_file(args, filename):\n                continue\n            abs_filepath = os.path.join(os.path.abspath(root), filename)\n            if is_excluded_path(args, abs_filepath):\n                continue\n            filepath = os.path.relpath(abs_filepath, abs_root)\n            if filepath not in discovered_files:\n                src_report = {}\n                src_report['name'] = posix_path(filepath)\n                coverage = []\n                with io.open(abs_filepath, mode='rb') as fobj:\n                    for _ in fobj:\n                        coverage.append(None)\n                    fobj.seek(0)\n                    src_report['source_digest'] = hashlib.md5(fobj.read()).hexdigest()\n                src_report['coverage'] = coverage\n                non_report_files.append(src_report)\n    return non_report_files", "response": "Collect the source files that have no coverage reports."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn hash of Git data that can be used to display more information to users.", "response": "def gitrepo(cwd):\n    \"\"\"Return hash of Git data that can be used to display more information to\n    users.\n\n    Example:\n        \"git\": {\n            \"head\": {\n                \"id\": \"5e837ce92220be64821128a70f6093f836dd2c05\",\n                \"author_name\": \"Wil Gieseler\",\n                \"author_email\": \"wil@example.com\",\n                \"committer_name\": \"Wil Gieseler\",\n                \"committer_email\": \"wil@example.com\",\n                \"message\": \"depend on simplecov >= 0.7\"\n            },\n            \"branch\": \"master\",\n            \"remotes\": [{\n                \"name\": \"origin\",\n                \"url\": \"https://github.com/lemurheavy/coveralls-ruby.git\"\n            }]\n        }\n\n    From https://github.com/coagulant/coveralls-python (with MIT license).\n\n    \"\"\"\n    repo = Repository(cwd)\n    if not repo.valid():\n        return {}\n\n    return {\n        'head': {\n            'id': repo.gitlog('%H'),\n            'author_name': repo.gitlog('%aN'),\n            'author_email': repo.gitlog('%ae'),\n            'committer_name': repo.gitlog('%cN'),\n            'committer_email': repo.gitlog('%ce'),\n            'message': repo.gitlog('%s')\n        },\n        'branch': os.environ.get('TRAVIS_BRANCH',\n                  os.environ.get('APPVEYOR_REPO_BRANCH',\n                                 repo.git('rev-parse', '--abbrev-ref', 'HEAD')[1].strip())),\n        'remotes': [{'name': line.split()[0], 'url': line.split()[1]}\n                    for line in repo.git('remote', '-v')[1] if '(fetch)' in line]\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning exit code and output from git.", "response": "def git(self, *arguments):\n        \"\"\"\n        Return (exit code, output) from git.\n        \"\"\"\n        process = subprocess.Popen(['git'] + list(arguments),\n                                   stdout=subprocess.PIPE,\n                                   cwd=self.cwd)\n        out = process.communicate()[0].decode('UTF-8')\n        code = process.returncode\n        return code, out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclean up the specified pin by closing and unexporting it.", "response": "def cleanup(pin=None, assert_exists=False):\r\n    \"\"\"Cleanup the pin by closing and unexporting it.\r\n\r\n    Args:\r\n        pin (int, optional): either the pin to clean up or None (default).\r\n            If None, clean up all pins.\r\n        assert_exists: if True, raise a ValueError if the pin was not\r\n            setup. Otherwise, this function is a NOOP.\r\n    \"\"\"\r\n    if pin is None:\r\n        # Take a list of keys because we will be deleting from _open\r\n        for pin in list(_open):\r\n            cleanup(pin)\r\n        return\r\n    if not isinstance(pin, int):\r\n        raise TypeError(\"pin must be an int, got: {}\".format(pin))\r\n\r\n    state = _open.get(pin)\r\n    if state is None:\r\n        if assert_exists:\r\n            raise ValueError(\"pin {} was not setup\".format(pin))\r\n        return\r\n    state.value.close()\r\n    state.direction.close()\r\n    if os.path.exists(gpiopath(pin)):\r\n        log.debug(\"Unexporting pin {0}\".format(pin))\r\n        with _export_lock:\r\n            with open(pjoin(gpio_root, 'unexport'), 'w') as f:\r\n                _write(f, pin)\r\n\r\n    del _open[pin]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the value of the next available token", "response": "def read(pin):\r\n    '''read the pin value\r\n\r\n    Returns:\r\n        bool: 0 or 1\r\n    '''\r\n    f = _open[pin].value\r\n    out = int(_read(f))\r\n    log.debug(\"Read {0}: {1}\".format(pin, out))\r\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set(pin, value):\r\n    '''set the pin value to 0 or 1'''\r\n    if value is LOW:\r\n        value = 0\r\n    value = int(bool(value))\r\n    log.debug(\"Write {0}: {1}\".format(pin, value))\r\n    f = _open[pin].value\r\n    _write(f, value)", "response": "set the pin value to 0 or 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tqdm(self, desc, total, leave, initial=0):\n        return tqdm(desc=desc, total=total, leave=leave, file=self.output_file, initial=initial)", "response": "Override to provide custom options to tqdm initializer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a new outer progress bar based on the given description and total number of epochs.", "response": "def build_tqdm_outer(self, desc, total):\n        \"\"\"\n        Extension point. Override to provide custom options to outer progress bars (Epoch loop)\n        :param desc: Description\n        :param total: Number of epochs\n        :return: new progress bar\n        \"\"\"\n        return self.tqdm(desc=desc, total=total, leave=self.leave_outer, initial=self.initial)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_tqdm_inner(self, desc, total):\n        return self.tqdm(desc=desc, total=total, leave=self.leave_inner)", "response": "This method is used to build the inner progress bar for the given description and total number of batches."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride to provide custom options to tqdm_notebook initializer.", "response": "def tqdm(self, desc, total, leave, initial=0):\n        \"\"\"\n        Extension point. Override to provide custom options to tqdm_notebook initializer.\n        :param desc: Description string\n        :param total: Total number of updates\n        :param leave: Leave progress bar when done\n        :return: new progress bar\n        :param initial: Initial counter state\n        \"\"\"\n        return tqdm_notebook(desc=desc, total=total, leave=leave, initial=initial)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclosing the V interface and release the resource tables.", "response": "def end(self):\n        \"\"\"Close the V interface.\n\n        Args::\n\n          No argument\n\n        Returns::\n\n          None\n\n        C library equivalent : Vend\n                                                \"\"\"\n\n        # Note: Vend is just a macro; use 'Vfinish' instead\n        # Note also the the same C function is used to end\n        # the VS interface\n        _checkErr('vend', _C.Vfinish(self._hdf_inst._id),\n                  \"cannot terminate V interface\")\n        self._hdf_inst = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen an existing vgroup given its name or its referenceNumber number or create a new vgroup.", "response": "def attach(self, num_name, write=0):\n        \"\"\"Open an existing vgroup given its name or its reference\n        number, or create a new vgroup, returning a VG instance for\n        that vgroup.\n\n        Args::\n\n          num_name      reference number or name of the vgroup to open,\n                        or -1 to create a new vgroup; vcreate() can also\n                        be called to create and name a new vgroup\n          write         set to non-zero to open the vgroup in write mode\n                        and to 0 to open it in readonly mode (default)\n\n        Returns::\n\n          VG instance for the vgroup\n\n        An exception is raised if an attempt is made to open\n        a non-existent vgroup.\n\n        C library equivalent : Vattach\n                                                \"\"\"\n\n        if isinstance(num_name, bytes):\n            num = self.find(num_name)\n        else:\n            num = num_name\n        vg_id = _C.Vattach(self._hdf_inst._id, num,\n                           write and 'w' or 'r')\n        _checkErr('vattach', vg_id, \"cannot attach Vgroup\")\n        return VG(self, vg_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new vgroup and assign it a name.", "response": "def create(self, name):\n        \"\"\"Create a new vgroup, and assign it a name.\n\n        Args::\n\n          name   name to assign to the new vgroup\n\n        Returns::\n\n          VG instance for the new vgroup\n\n        A create(name) call is equivalent to an attach(-1, 1) call,\n        followed by a call to the setname(name) method of the instance.\n\n        C library equivalent : no equivalent\n                                                    \"\"\"\n\n        vg = self.attach(-1, 1)\n        vg._name = name\n        return vg"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find(self, name):\n\n        refnum = _C.Vfind(self._hdf_inst._id, name)\n        if not refnum:\n            raise HDF4Error(\"vgroup not found\")\n        return refnum", "response": "Find a vgroup given its name returning its reference number if found."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef findclass(self, name):\n\n        refnum = _C.Vfindclass(self._hdf_inst._id, name)\n        if not refnum:\n            raise HDF4Error(\"vgroup not found\")\n        return refnum", "response": "Find a vgroup given its class name returning its reference number if found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, num_name):\n\n        try:\n            vg = self.attach(num_name, 1)\n        except HDF4Error as msg:\n            raise HDF4Error(\"delete: no such vgroup\")\n\n        # ATTENTION: The HDF documentation says that the vgroup_id\n        #            is passed to Vdelete(). This is wrong.\n        #            The vgroup reference number must instead be passed.\n        refnum = vg._refnum\n        vg.detach()\n        _checkErr('delete', _C.Vdelete(self._hdf_inst._id, refnum),\n                  \"error deleting vgroup\")", "response": "Delete from the HDF file the vgroup identified by its\n        reference number or its name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the reference number of the vgroup following the given reference number.", "response": "def getid(self, ref):\n        \"\"\"Obtain the reference number of the vgroup following the\n        vgroup with the given reference number .\n\n        Args::\n\n          ref    reference number of the vgroup after which to search;\n                 set to -1 to start the search at the start of\n                 the HDF file\n\n        Returns::\n\n          reference number of the vgroup past the one identified by 'ref'\n\n        An exception is raised if the end of the vgroup is reached.\n\n        C library equivalent : Vgetid\n                                                    \"\"\"\n\n        num = _C.Vgetid(self._hdf_inst._id, ref)\n        _checkErr('getid', num, \"bad arguments or last vgroup reached\")\n        return num"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting a vdata or a vgroup instance into the vgroup.", "response": "def insert(self, inst):\n        \"\"\"Insert a vdata or a vgroup in the vgroup.\n\n        Args::\n\n          inst  vdata or vgroup instance to add\n\n        Returns::\n\n          index of the inserted vdata or vgroup (0 based)\n\n        C library equivalent : Vinsert\n                                                  \"\"\"\n\n        if isinstance(inst, VD):\n            id = inst._id\n        elif isinstance(inst, VG):\n            id = inst._id\n        else:\n            raise HDF4Error(\"insrt: bad argument\")\n\n        index = _C.Vinsert(self._id, id)\n        _checkErr('insert', index, \"cannot insert in vgroup\")\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an object identified by its tag and ref number to the vgroup.", "response": "def add(self, tag, ref):\n        \"\"\"Add to the vgroup an object identified by its tag and\n        reference number.\n\n        Args::\n\n          tag       tag of the object to add\n          ref       reference number of the object to add\n\n        Returns::\n\n          total number of objects in the vgroup after the addition\n\n        C library equivalent : Vaddtagref\n                                              \"\"\"\n\n        n = _C.Vaddtagref(self._id, tag, ref)\n        _checkErr('addtagref', n, 'invalid arguments')\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, tag, ref):\n\n        _checkErr('delete', _C.Vdeletetagref(self._id, tag, ref),\n                  \"error deleting member\")", "response": "Delete the member identified by its tag and reference number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tagref(self, index):\n\n        status, tag, ref = _C.Vgettagref(self._id, index)\n        _checkErr('tagref', status, \"illegal arguments\")\n        return tag, ref", "response": "Get the tag and reference number of a vgroup member given the index number of that member."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tagrefs(self):\n        n = self._nmembers\n        ret = []\n        if n:\n            tags = _C.array_int32(n)\n            refs = _C.array_int32(n)\n            k = _C.Vgettagrefs(self._id, tags, refs, n)\n            _checkErr('tagrefs', k, \"error getting tags and refs\")\n            for m in xrange(k):\n                ret.append((tags[m], refs[m]))\n\n        return ret", "response": "Get the tags and reference numbers of all the vgroup member\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if an object identified by its tag and referenceVersion number belongs to the vgroup.", "response": "def inqtagref(self, tag, ref):\n        \"\"\"Determines if an object identified by its tag and reference\n        number belongs to the vgroup.\n\n        Args::\n\n          tag      tag of the object to check\n          ref      reference number of the object to check\n\n        Returns::\n\n          False (0) if the object does not belong to the vgroup,\n          True  (1) otherwise\n\n        C library equivalent : Vinqtagref\n                                             \"\"\"\n\n        return _C.Vinqtagref(self._id, tag, ref)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines the number of members of a given type in a given vgroup.", "response": "def nrefs(self, tag):\n        \"\"\"Determine the number of tags of a given type in a vgroup.\n\n        Args::\n\n          tag    tag type to look for in the vgroup\n\n        Returns::\n\n          number of members identified by this tag type\n\n        C library equivalent : Vnrefs\n                                              \"\"\"\n\n        n = _C.Vnrefs(self._id, tag)\n        _checkErr('nrefs', n, \"bad arguments\")\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef attrinfo(self):\n\n        dic = {}\n        for n in range(self._nattrs):\n            att = self.attr(n)\n            name, type, order, size = att.info()\n            dic[name] = (type, order, att.get(), size)\n\n        return dic", "response": "Return info about all the vgroup attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findattr(self, name):\n\n        try:\n            att = self.attr(name)\n            if att._index is None:\n                att = None\n        except HDF4Error:\n            att = None\n        return att", "response": "Search the vgroup for a given attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the value of the attribute in the vgroup.", "response": "def get(self):\n        \"\"\"Retrieve the attribute value.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          attribute value(s); a list is returned if the attribute\n          is made up of more than one value, except in the case of a\n          string-valued attribute (data type HC.CHAR8) where the\n          values are returned as a string\n\n        Note that a vgroup attribute can also be queried like a standard\n        python class attribute by applying the usual \"dot notation\" to a\n        VG instance.\n\n        C library equivalent : Vgetattr\n\n                                                \"\"\"\n        # Make sure the attribute exists.\n        if self._index is None:\n            raise HDF4Error(\"non existent attribute\")\n        # Obtain attribute type and the number of values.\n        status, aName, data_type, n_values, size = \\\n                    _C.Vattrinfo(self._v_inst._id, self._index)\n        _checkErr('get', status, 'illegal parameters')\n\n        # Get attribute value.\n        convert = _array_to_ret\n        if data_type == HC.CHAR8:\n            buf = _C.array_byte(n_values)\n            convert = _array_to_str\n\n        elif data_type in [HC.UCHAR8, HC.UINT8]:\n            buf = _C.array_byte(n_values)\n\n        elif data_type == HC.INT8:\n            buf = _C.array_int8(n_values)\n\n        elif data_type == HC.INT16:\n            buf = _C.array_int16(n_values)\n\n        elif data_type == HC.UINT16:\n            buf = _C.array_uint16(n_values)\n\n        elif data_type == HC.INT32:\n            buf = _C.array_int32(n_values)\n\n        elif data_type == HC.UINT32:\n            buf = _C.array_uint32(n_values)\n\n        elif data_type == HC.FLOAT32:\n            buf = _C.array_float32(n_values)\n\n        elif data_type == HC.FLOAT64:\n            buf = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"get: attribute index %d has an \"\\\n                             \"illegal or unupported type %d\" % \\\n                             (self._index, data_type))\n\n        status = _C.Vgetattr(self._v_inst._id, self._index, buf)\n        _checkErr('get', status, 'illegal attribute ')\n        return convert(buf, n_values)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(self):\n\n        # Make sure the attribute exists.\n        if self._index is None:\n            raise HDF4Error(\"non existent attribute\")\n\n        status, name, type, order, size = \\\n                _C.Vattrinfo(self._v_inst._id, self._index)\n        _checkErr('info', status, \"execution error\")\n        return name, type, order, size", "response": "Retrieve info about the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(self):\n        if self._index is None:\n            try:\n                self._index = self._obj.findattr(self._name)\n            except HDF4Error:\n                raise HDF4Error(\"info: cannot convert name to index\")\n        status, self._name, data_type, n_values = \\\n                              _C.SDattrinfo(self._obj._id, self._index)\n        _checkErr('info', status, 'illegal attribute index')\n        return self._name, data_type, n_values", "response": "Retrieve info about the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef index(self):\n\n        self._index = _C.SDfindattr(self._obj._id, self._name)\n        _checkErr('find', self._index, 'illegal attribute name')\n        return self._index", "response": "Retrieve the attribute index number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the value of the attribute in the current object.", "response": "def get(self):\n        \"\"\"Retrieve the attribute value.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          attribute value(s); a list is returned if the attribute\n          is made up of more than one value, except in the case of a\n          string-valued attribute (data type SDC.CHAR8) where the\n          values are returned as a string\n\n        C library equivalent : SDreadattr\n\n        Attributes can also be read like ordinary python attributes,\n        using the dot notation. See \"High level attribute access\".\n\n                                                \"\"\"\n\n        if self._index is None:\n            try:\n                self._index = self._obj.findattr(self._name)\n            except HDF4Error:\n                raise HDF4Error(\"get: cannot convert name to index\")\n\n        # Obtain attribute type and the number of values.\n        status, self._name, data_type, n_values = \\\n                    _C.SDattrinfo(self._obj._id, self._index)\n        _checkErr('read', status, 'illegal attribute index')\n\n        # Get attribute value.\n        convert = _array_to_ret\n        if data_type == SDC.CHAR8:\n            buf = _C.array_byte(n_values)\n            convert = _array_to_str\n\n        elif data_type in [SDC.UCHAR8, SDC.UINT8]:\n            buf = _C.array_byte(n_values)\n\n        elif data_type == SDC.INT8:\n            buf = _C.array_int8(n_values)\n\n        elif data_type == SDC.INT16:\n            buf = _C.array_int16(n_values)\n\n        elif data_type == SDC.UINT16:\n            buf = _C.array_uint16(n_values)\n\n        elif data_type == SDC.INT32:\n            buf = _C.array_int32(n_values)\n\n        elif data_type == SDC.UINT32:\n            buf = _C.array_uint32(n_values)\n\n        elif data_type == SDC.FLOAT32:\n            buf = _C.array_float32(n_values)\n\n        elif data_type == SDC.FLOAT64:\n            buf = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"read: attribute index %d has an \"\\\n                             \"illegal or unupported type %d\" % \\\n                             (self._index, data_type))\n\n        status = _C.SDreadattr(self._obj._id, self._index, buf)\n        _checkErr('read', status, 'illegal attribute index')\n        return convert(buf, n_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set(self, data_type, values):\n        try:\n            n_values = len(values)\n        except:\n            n_values = 1\n            values = [values]\n        if data_type == SDC.CHAR8:\n            buf = _C.array_byte(n_values)\n            # Allow values to be passed as a string.\n            # Noop if a list is passed.\n            values = list(values)\n            for n in range(n_values):\n                values[n] = ord(values[n])\n\n        elif data_type in [SDC.UCHAR8, SDC.UINT8]:\n            buf = _C.array_byte(n_values)\n\n        elif data_type == SDC.INT8:\n            buf = _C.array_int8(n_values)\n\n        elif data_type == SDC.INT16:\n            buf = _C.array_int16(n_values)\n\n        elif data_type == SDC.UINT16:\n            buf = _C.array_uint16(n_values)\n\n        elif data_type == SDC.INT32:\n            buf = _C.array_int32(n_values)\n\n        elif data_type == SDC.UINT32:\n            buf = _C.array_uint32(n_values)\n\n        elif data_type == SDC.FLOAT32:\n            buf = _C.array_float32(n_values)\n\n        elif data_type == SDC.FLOAT64:\n            buf = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"set: illegal or unimplemented data_type\")\n\n        for n in range(n_values):\n            buf[n] = values[n]\n        status = _C.SDsetattr(self._obj._id, self._name,\n                              data_type, n_values, buf)\n        _checkErr('set', status, 'illegal attribute')\n        # Init index following attribute creation.\n        self._index = _C.SDfindattr(self._obj._id, self._name)\n        _checkErr('find', self._index, 'illegal attribute')", "response": "Update or create a new attribute and set its value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nend access to the SD interface and close the HDF file.", "response": "def end(self):\n        \"\"\"End access to the SD interface and close the HDF file.\n\n        Args::\n\n            no argument\n\n        Returns::\n\n            None\n\n        The instance should not be used afterwards.\n        The 'end()' method is implicitly called when the\n        SD instance is deleted.\n\n        C library equivalent : SDend\n                                                      \"\"\"\n\n        status = _C.SDend(self._id)\n        _checkErr('end', status, \"cannot execute\")\n        self._id = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving information about the SD interface.", "response": "def info(self):\n        \"\"\"Retrieve information about the SD interface.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          2-element tuple holding:\n            number of datasets inside the file\n            number of file attributes\n\n        C library equivalent : SDfileinfo\n                                                  \"\"\"\n\n        status, n_datasets, n_file_attrs = _C.SDfileinfo(self._id)\n        _checkErr('info', status, \"cannot execute\")\n        return n_datasets, n_file_attrs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nametoindex(self, sds_name):\n\n        sds_idx = _C.SDnametoindex(self._id, sds_name)\n        _checkErr('nametoindex', sds_idx, 'non existent SDS')\n        return sds_idx", "response": "Return the index number of a dataset given the dataset name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the index number of a dataset given the dataset s reference number.", "response": "def reftoindex(self, sds_ref):\n        \"\"\"Returns the index number of a dataset given the dataset\n        reference number.\n\n        Args::\n\n          sds_ref : dataset reference number\n\n        Returns::\n\n          dataset index number\n\n        C library equivalent : SDreftoindex\n                                             \"\"\"\n\n        sds_idx = _C.SDreftoindex(self._id, sds_ref)\n        _checkErr('reftoindex', sds_idx, 'illegal SDS ref number')\n        return sds_idx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setfillmode(self, fill_mode):\n\n        if not fill_mode in [SDC.FILL, SDC.NOFILL]:\n            raise HDF4Error(\"bad fill mode\")\n        old_mode = _C.SDsetfillmode(self._id, fill_mode)\n        _checkErr('setfillmode', old_mode, 'cannot execute')\n        return old_mode", "response": "Set the fill mode for all the datasets in the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, name, data_type, dim_sizes):\n\n        # Validate args.\n        if isinstance(dim_sizes, type(1)):  # allow k instead of [k]\n                                        # for a 1-dim arr\n            dim_sizes = [dim_sizes]\n        rank = len(dim_sizes)\n        buf = _C.array_int32(rank)\n        for n in range(rank):\n            buf[n] = dim_sizes[n]\n        id = _C.SDcreate(self._id, name, data_type, rank, buf)\n        _checkErr('CREATE', id, \"cannot execute\")\n        return SDS(self, id)", "response": "Create a new dataset with the specified name data type and dimension sizes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select(self, name_or_index):\n\n        if isinstance(name_or_index, type(1)):\n            idx = name_or_index\n        else:\n            try:\n                idx = self.nametoindex(name_or_index)\n            except HDF4Error:\n                raise HDF4Error(\"select: non-existent dataset\")\n        id = _C.SDselect(self._id, idx)\n        _checkErr('select', id, \"cannot execute\")\n        return SDS(self, id)", "response": "Locate a dataset.\n\n        Args::\n\n          name_or_index  dataset name or index number\n\n        Returns::\n\n          SDS instance for the dataset\n\n        C library equivalent : SDselect"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attributes(self, full=0):\n\n        # Get the number of global attributes.\n        nsds, natts = self.info()\n\n        # Inquire each attribute\n        res = {}\n        for n in range(natts):\n            a = self.attr(n)\n            name, aType, nVal = a.info()\n            if full:\n                res[name] = (a.get(), a.index(), aType, nVal)\n            else:\n                res[name] = a.get()\n\n        return res", "response": "Return a dictionnary describing every global attribute attached to the SD interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef datasets(self):\n        # Get number of datasets\n        nDs = self.info()[0]\n\n        # Inquire each var\n        res = {}\n        for n in range(nDs):\n            # Get dataset info.\n            v = self.select(n)\n            vName, vRank, vLen, vType, vAtt = v.info()\n            if vRank < 2:     # need a sequence\n                vLen = [vLen]\n            # Get dimension info.\n            dimNames = []\n            dimLengths = []\n            for dimNum in range(vRank):\n                d = v.dim(dimNum)\n                dimNames.append(d.info()[0])\n                dimLengths.append(vLen[dimNum])\n            res[vName] = (tuple(dimNames), tuple(dimLengths),\n                         vType, n)\n\n        return res", "response": "Return a dictionnary describing all the file datasets in the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nterminating access to the SDS instance.", "response": "def endaccess(self):\n        \"\"\"Terminates access to the SDS.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          None.\n\n        The SDS instance should not be used afterwards.\n        The 'endaccess()' method is implicitly called when\n        the SDS instance is deleted.\n\n        C library equivalent : SDendaccess\n                                                 \"\"\"\n\n        status = _C.SDendaccess(self._id)\n        _checkErr('endaccess', status, \"cannot execute\")\n        self._id = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dim(self, dim_index):\n        id = _C.SDgetdimid(self._id, dim_index)\n        _checkErr('dim', id, 'invalid SDS identifier or dimension index')\n        return SDim(self, id, dim_index)", "response": "Get an SDim instance given a dimension index number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, start=None, count=None, stride=None):\n\n        # Obtain SDS info.\n        try:\n            sds_name, rank, dim_sizes, data_type, n_attrs = self.info()\n            if isinstance(dim_sizes, type(1)):\n                dim_sizes = [dim_sizes]\n        except HDF4Error:\n            raise HDF4Error('get : cannot execute')\n\n        # Validate args.\n        if start is None:\n            start = [0] * rank\n        elif isinstance(start, type(1)):\n            start = [start]\n        if count is None:\n            count = dim_sizes\n            if count[0] == 0:\n                count[0] = 1\n        elif isinstance(count, type(1)):\n            count = [count]\n        if stride is None:\n            stride = [1] * rank\n        elif isinstance(stride, type(1)):\n            stride = [stride]\n        if len(start) != rank or len(count) != rank or len(stride) != rank:\n            raise HDF4Error('get : start, stride or count ' \\\n                             'do not match SDS rank')\n        for n in range(rank):\n            if start[n] < 0 or start[n] + \\\n                  (abs(count[n]) - 1) * stride[n] >= dim_sizes[n]:\n                raise HDF4Error('get arguments violate ' \\\n                                 'the size (%d) of dimension %d' \\\n                                 % (dim_sizes[n], n))\n        if not data_type in SDC.equivNumericTypes:\n            raise HDF4Error('get cannot currrently deal with '\\\n                             'the SDS data type')\n\n        return _C._SDreaddata_0(self._id, data_type, start, count, stride)", "response": "Read data from the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, data, start=None, count=None, stride=None):\n\n\n        # Obtain SDS info.\n        try:\n            sds_name, rank, dim_sizes, data_type, n_attrs = self.info()\n            if isinstance(dim_sizes, type(1)):\n                dim_sizes = [dim_sizes]\n        except HDF4Error:\n            raise HDF4Error('set : cannot execute')\n\n        # Validate args.\n        if start is None:\n            start = [0] * rank\n        elif isinstance(start, type(1)):\n            start = [start]\n        if count is None:\n            count = dim_sizes\n            if count[0] == 0:\n                count[0] = 1\n        elif isinstance(count, type(1)):\n            count = [count]\n        if stride is None:\n            stride = [1] * rank\n        elif isinstance(stride, type(1)):\n            stride = [stride]\n        if len(start) != rank or len(count) != rank or len(stride) != rank:\n            raise HDF4Error('set : start, stride or count '\\\n                             'do not match SDS rank')\n        unlimited = self.isrecord()\n        for n in range(rank):\n            ok = 1\n            if start[n] < 0:\n                ok = 0\n            elif n > 0 or not unlimited:\n                if start[n] + (abs(count[n]) - 1) * stride[n] >= dim_sizes[n]:\n                    ok = 0\n            if not ok:\n                raise HDF4Error('set arguments violate '\\\n                                 'the size (%d) of dimension %d' \\\n                                 % (dim_sizes[n], n))\n        # ??? Check support for UINT16\n        if not data_type in SDC.equivNumericTypes:\n            raise HDF4Error('set cannot currrently deal '\\\n                             'with the SDS data type')\n\n        _C._SDwritedata_0(self._id, data_type, start, count, data, stride)", "response": "Write data to the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve information about the dataset.", "response": "def info(self):\n        \"\"\"Retrieves information about the dataset.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          5-element tuple holding:\n\n          - dataset name\n          - dataset rank (number of dimensions)\n          - dataset shape, that is a list giving the length of each\n            dataset dimension; if the first dimension is unlimited, then\n            the first value of the list gives the current length of the\n            unlimited dimension\n          - data type (one of the SDC.xxx values)\n          - number of attributes defined for the dataset\n\n        C library equivalent : SDgetinfo\n                                                       \"\"\"\n\n        buf = _C.array_int32(_C.H4_MAX_VAR_DIMS)\n        status, sds_name, rank, data_type, n_attrs = \\\n                _C.SDgetinfo(self._id, buf)\n        _checkErr('info', status, \"cannot execute\")\n        dim_sizes = _array_to_ret(buf, rank)\n        return sds_name, rank, dim_sizes, data_type, n_attrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks whether the dataset is empty.", "response": "def checkempty(self):\n        \"\"\"Determine whether the dataset is empty.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          True(1) if dataset is empty, False(0) if not\n\n        C library equivalent : SDcheckempty\n                                                 \"\"\"\n\n        status, emptySDS = _C.SDcheckempty(self._id)\n        _checkErr('checkempty', status, 'invalid SDS identifier')\n        return emptySDS"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ref(self):\n\n        sds_ref = _C.SDidtoref(self._id)\n        _checkErr('idtoref', sds_ref, 'illegal SDS identifier')\n        return sds_ref", "response": "Get the reference number of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the SDS calibration coefficients for the specified calendar record.", "response": "def getcal(self):\n        \"\"\"Retrieve the SDS calibration coefficients.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          5-element tuple holding:\n\n          - cal: calibration factor (attribute 'scale_factor')\n          - cal_error : calibration factor error\n                        (attribute 'scale_factor_err')\n          - offset: calibration offset (attribute 'add_offset')\n          - offset_err : offset error (attribute 'add_offset_err')\n          - data_type : type of the data resulting from applying\n                        the calibration formula to the dataset values\n                        (attribute 'calibrated_nt')\n\n        An exception is raised if no calibration data are defined.\n\n        Original dataset values 'orival' are converted to calibrated\n        values 'calval' through the formula::\n\n           calval = cal * (orival - offset)\n\n        The calibration coefficients are part of the so-called\n        \"standard\" SDS attributes. The values inside the tuple returned\n        by 'getcal' are those of the following attributes, in order::\n\n          scale_factor, scale_factor_err, add_offset, add_offset_err,\n          calibrated_nt\n\n        C library equivalent: SDgetcal()\n                                               \"\"\"\n\n        status, cal, cal_error, offset, offset_err, data_type = \\\n                         _C.SDgetcal(self._id)\n        _checkErr('getcal', status, 'no calibration record')\n        return cal, cal_error, offset, offset_err, data_type"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getdatastrs(self):\n\n        status, label, unit, format, coord_system = \\\n               _C.SDgetdatastrs(self._id, 128)\n        _checkErr('getdatastrs', status, 'cannot execute')\n        return label, unit, format, coord_system", "response": "Retrieve the standard string attributes of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the minimum and maximum values from the current dataset.", "response": "def getrange(self):\n        \"\"\"Retrieve the dataset min and max values.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          (min, max) tuple (attribute 'valid_range')\n\n          Note that those are the values as stored\n          by the 'setrange' method. 'getrange' does *NOT* compute the\n          min and max from the current dataset contents.\n\n        An exception is raised if the range is not set.\n\n        The range returned by 'getrange' is part of the so-called\n        \"standard\" SDS attributes. It corresponds to the following\n        attribute::\n\n          valid_range\n\n        C library equivalent: SDgetrange\n                                                       \"\"\"\n\n        # Obtain SDS data type.\n        try:\n            sds_name, rank, dim_sizes, data_type, n_attrs = \\\n                               self.info()\n        except HDF4Error:\n            raise HDF4Error('getrange : invalid SDS identifier')\n        n_values = 1\n\n        convert = _array_to_ret\n        if data_type == SDC.CHAR8:\n            buf1 = _C.array_byte(n_values)\n            buf2 = _C.array_byte(n_values)\n            convert = _array_to_str\n\n        elif data_type in  [SDC.UCHAR8, SDC.UINT8]:\n            buf1 = _C.array_byte(n_values)\n            buf2 = _C.array_byte(n_values)\n\n        elif data_type == SDC.INT8:\n            buf1 = _C.array_int8(n_values)\n            buf2 = _C.array_int8(n_values)\n\n        elif data_type == SDC.INT16:\n            buf1 = _C.array_int16(n_values)\n            buf2 = _C.array_int16(n_values)\n\n        elif data_type == SDC.UINT16:\n            buf1 = _C.array_uint16(n_values)\n            buf2 = _C.array_uint16(n_values)\n\n        elif data_type == SDC.INT32:\n            buf1 = _C.array_int32(n_values)\n            buf2 = _C.array_int32(n_values)\n\n        elif data_type == SDC.UINT32:\n            buf1 = _C.array_uint32(n_values)\n            buf2 = _C.array_uint32(n_values)\n\n        elif data_type == SDC.FLOAT32:\n            buf1 = _C.array_float32(n_values)\n            buf2 = _C.array_float32(n_values)\n\n        elif data_type == SDC.FLOAT64:\n            buf1 = _C.array_float64(n_values)\n            buf2 = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"getrange: SDS has an illegal or \" \\\n                             \"unsupported type %d\" % data)\n\n        # Note: The C routine returns the max in buf1 and the min\n        # in buf2. We swap the values returned by the Python\n        # interface, since it is more natural to return\n        # min first, then max.\n        status = _C.SDgetrange(self._id, buf1, buf2)\n        _checkErr('getrange', status, 'range not set')\n        return convert(buf2, n_values), convert(buf1, n_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the dataset calibration coefficients.", "response": "def setcal(self, cal, cal_error, offset, offset_err, data_type):\n        \"\"\"Set the dataset calibration coefficients.\n\n        Args::\n\n          cal         the calibraton factor (attribute 'scale_factor')\n          cal_error   calibration factor error\n                      (attribute 'scale_factor_err')\n          offset      offset value (attribute 'add_offset')\n          offset_err  offset error (attribute 'add_offset_err')\n          data_type   data type of the values resulting from applying the\n                      calibration formula to the dataset values\n                      (one of the SDC.xxx constants)\n                      (attribute 'calibrated_nt')\n\n        Returns::\n\n          None\n\n        See method 'getcal' for the definition of the calibration\n        formula.\n\n        Calibration coefficients are part of the so-called standard\n        SDS attributes. Calling 'setcal' is equivalent to setting\n        the following attributes, which correspond to the method\n        parameters, in order::\n\n          scale_factor, scale_factor_err, add_offset, add_offset_err,\n          calibrated_nt\n\n        C library equivalent: SDsetcal\n                                                      \"\"\"\n\n        status = _C.SDsetcal(self._id, cal, cal_error,\n                             offset, offset_err, data_type)\n        _checkErr('setcal', status, 'cannot execute')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setdatastrs(self, label, unit, format, coord_sys):\n\n        status = _C.SDsetdatastrs(self._id, label, unit, format, coord_sys)\n        _checkErr('setdatastrs', status, 'cannot execute')", "response": "Set the standard string type attributes of the specified dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setfillvalue(self, fill_val):\n\n        # Obtain SDS data type.\n        try:\n            sds_name, rank, dim_sizes, data_type, n_attrs = self.info()\n        except HDF4Error:\n            raise HDF4Error('setfillvalue : cannot execute')\n        n_values = 1   # Fill value stands for 1 value.\n\n        if data_type == SDC.CHAR8:\n            buf = _C.array_byte(n_values)\n\n        elif data_type in [SDC.UCHAR8, SDC.UINT8]:\n            buf = _C.array_byte(n_values)\n\n        elif data_type == SDC.INT8:\n            buf = _C.array_int8(n_values)\n\n        elif data_type == SDC.INT16:\n            buf = _C.array_int16(n_values)\n\n        elif data_type == SDC.UINT16:\n            buf = _C.array_uint16(n_values)\n\n        elif data_type == SDC.INT32:\n            buf = _C.array_int32(n_values)\n\n        elif data_type == SDC.UINT32:\n            buf = _C.array_uint32(n_values)\n\n        elif data_type == SDC.FLOAT32:\n            buf = _C.array_float32(n_values)\n\n        elif data_type == SDC.FLOAT64:\n            buf = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"setfillvalue: SDS has an illegal or \" \\\n                             \"unsupported type %d\" % data_type)\n\n        buf[0] = fill_val\n        status = _C.SDsetfillvalue(self._id, buf)\n        _checkErr('setfillvalue', status, 'cannot execute')", "response": "Set the dataset fill value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setrange(self, min, max):\n\n        # Obtain SDS data type.\n        try:\n            sds_name, rank, dim_sizes, data_type, n_attrs = self.info()\n        except HDF4Error:\n            raise HDF4Error('setrange : cannot execute')\n        n_values = 1\n\n        if data_type == SDC.CHAR8:\n            buf1 = _C.array_byte(n_values)\n            buf2 = _C.array_byte(n_values)\n\n        elif data_type in [SDC.UCHAR8, SDC.UINT8]:\n            buf1 = _C.array_byte(n_values)\n            buf2 = _C.array_byte(n_values)\n\n        elif data_type == SDC.INT8:\n            buf1 = _C.array_int8(n_values)\n            buf2 = _C.array_int8(n_values)\n\n        elif data_type == SDC.INT16:\n            buf1 = _C.array_int16(n_values)\n            buf2 = _C.array_int16(n_values)\n\n        elif data_type == SDC.UINT16:\n            buf1 = _C.array_uint16(n_values)\n            buf2 = _C.array_uint16(n_values)\n\n        elif data_type == SDC.INT32:\n            buf1 = _C.array_int32(n_values)\n            buf2 = _C.array_int32(n_values)\n\n        elif data_type == SDC.UINT32:\n            buf1 = _C.array_uint32(n_values)\n            buf2 = _C.array_uint32(n_values)\n\n        elif data_type == SDC.FLOAT32:\n            buf1 = _C.array_float32(n_values)\n            buf2 = _C.array_float32(n_values)\n\n        elif data_type == SDC.FLOAT64:\n            buf1 = _C.array_float64(n_values)\n            buf2 = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"SDsetrange: SDS has an illegal or \" \\\n                             \"unsupported type %d\" % data_type)\n\n        buf1[0] = max\n        buf2[0] = min\n        status = _C.SDsetrange(self._id, buf1, buf2)\n        _checkErr('setrange', status, 'cannot execute')", "response": "Set the dataset min and max values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getcompress(self):\n\n        status, comp_type, value, v2, v3, v4, v5 = _C._SDgetcompress(self._id)\n        _checkErr('getcompress', status, 'no compression')\n        if comp_type == SDC.COMP_NONE:\n            return (comp_type,)\n        elif comp_type == SDC.COMP_SZIP:\n            return comp_type, value, v2, v3, v4, v5\n        else:\n            return comp_type, value", "response": "Returns info about the compressed dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompresses the dataset using a specified compression method.", "response": "def setcompress(self, comp_type, value=0, v2=0):\n        \"\"\"Compresses the dataset using a specified compression method.\n\n        Args::\n\n          comp_type    compression type, identified by one of the\n                       SDC.COMP_xxx constants\n          value,v2     auxiliary value(s) needed by some compression types\n                         SDC.COMP_SKPHUFF   Skipping-Huffman; compression value=data size in bytes, v2 is ignored\n                         SDC.COMP_DEFLATE   Gzip compression; value=deflate level (1 to 9), v2 is ignored\n                         SDC.COMP_SZIP      Szip compression; value=encoding scheme (SDC.COMP_SZIP_EC or\n                                            SDC.COMP_SZIP_NN), v2=pixels per block (2 to 32)\n\n        Returns::\n\n            None\n\n        .. note::\n             Starting with v0.8, an exception is always raised if\n             pyhdf was installed with the NOCOMPRESS macro set.\n\n        SDC.COMP_DEFLATE applies the GZIP compression to the dataset,\n        and the value varies from 1 to 9, according to the level of\n        compression desired.\n\n        SDC.COMP_SZIP compresses the dataset using the SZIP algorithm. See the HDF User's Guide\n        for details about the encoding scheme and the number of pixels per block. SZIP is new\n        with HDF 4.2.\n\n        'setcompress' must be called before writing to the dataset.\n        The dataset must be written all at once, unless it is\n        appendable (has an unlimited dimension). Updating the dataset\n        in not allowed. Refer to the HDF user's guide for more details\n        on how to use data compression.\n\n        C library equivalent: SDsetcompress\n                                                          \"\"\"\n\n        status = _C._SDsetcompress(self._id, comp_type, value, v2)\n        _checkErr('setcompress', status, 'cannot execute')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setexternalfile(self, filename, offset=0):\n\n        status = _C.SDsetexternalfile(self._id, filename, offset)\n        _checkErr('setexternalfile', status, 'execution error')", "response": "Store the dataset data in an external file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionnary describing every dataset dimension.", "response": "def dimensions(self, full=0):\n        \"\"\"Return a dictionnary describing every dataset dimension.\n\n        Args::\n\n          full      true to get complete info about each dimension\n                    false to report only each dimension length\n\n        Returns::\n\n          Dictionnary where each key is a dimension name. If no name\n          has been given to the dimension, the key is set to\n          'fakeDimx' where 'x' is the dimension index number.\n          If parameter 'full' is false, key value is the dimension\n          length. If 'full' is true, key value is a 5-element tuple\n          with the following elements:\n\n          - dimension length; for an unlimited dimension, the reported\n            length is the current dimension length\n          - dimension index number\n          - 1 if the dimension is unlimited, 0 otherwise\n          - dimension scale type, or 0 if no scale is defined for\n            the dimension\n          - number of attributes defined on the dimension\n\n        C library equivalent : no equivalent\n                                                    \"\"\"\n\n        # Get the number of dimensions and their lengths.\n        nDims, dimLen = self.info()[1:3]\n        if isinstance(dimLen, int):    # need a sequence\n            dimLen = [dimLen]\n        # Check if the dataset is appendable.\n        unlim = self.isrecord()\n\n        # Inquire each dimension\n        res = {}\n        for n in range(nDims):\n            d = self.dim(n)\n            # The length reported by info() is 0 for an unlimited dimension.\n            # Rather use the lengths reported by SDS.info()\n            name, k, scaleType, nAtt = d.info()\n            length = dimLen[n]\n            if full:\n                res[name] = (length, n, unlim and n == 0,\n                             scaleType, nAtt)\n            else:\n                res[name] = length\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef info(self):\n        status, dim_name, dim_size, data_type, n_attrs = \\\n                _C.SDdiminfo(self._id)\n        _checkErr('info', status, 'cannot execute')\n        return dim_name, dim_size, data_type, n_attrs", "response": "Return info about the current dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setname(self, dim_name):\n\n        status = _C.SDsetdimname(self._id, dim_name)\n        _checkErr('setname', status, 'cannot execute')", "response": "Set the name of the dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getscale(self):\n\n        # Get dimension info. If data_type is 0, no scale have been set\n        # on the dimension.\n        status, dim_name, dim_size, data_type, n_attrs = _C.SDdiminfo(self._id)\n        _checkErr('getscale', status, 'cannot execute')\n        if data_type == 0:\n            raise HDF4Error(\"no scale set on that dimension\")\n\n        # dim_size is 0 for an unlimited dimension. The actual length is\n        # obtained through SDgetinfo.\n        if dim_size == 0:\n            dim_size = self._sds.info()[2][self._index]\n\n        # Get scale values.\n        if data_type in [SDC.UCHAR8, SDC.UINT8]:\n            buf = _C.array_byte(dim_size)\n\n        elif data_type == SDC.INT8:\n            buf = _C.array_int8(dim_size)\n\n        elif data_type == SDC.INT16:\n            buf = _C.array_int16(dim_size)\n\n        elif data_type == SDC.UINT16:\n            buf = _C.array_uint16(dim_size)\n\n        elif data_type == SDC.INT32:\n            buf = _C.array_int32(dim_size)\n\n        elif data_type == SDC.UINT32:\n            buf = _C.array_uint32(dim_size)\n\n        elif data_type == SDC.FLOAT32:\n            buf = _C.array_float32(dim_size)\n\n        elif data_type == SDC.FLOAT64:\n            buf = _C.array_float64(dim_size)\n\n        else:\n            raise HDF4Error(\"getscale: dimension has an \"\\\n                             \"illegal or unsupported type %d\" % data_type)\n\n        status = _C.SDgetdimscale(self._id, buf)\n        _checkErr('getscale', status, 'cannot execute')\n        return _array_to_ret(buf, dim_size)", "response": "Get the scale values along a dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setscale(self, data_type, scale):\n\n        try:\n            n_values = len(scale)\n        except:\n            n_values = 1\n\n        # Validate args\n        info = self._sds.info()\n        if info[1] == 1:\n            dim_size = info[2]\n        else:\n            dim_size = info[2][self._index]\n        if n_values != dim_size:\n            raise HDF4Error('number of scale values (%d) does not match ' \\\n                             'dimension size (%d)' % (n_values, dim_size))\n\n        if data_type == SDC.CHAR8:\n            buf = _C.array_byte(n_values)\n            # Allow a string as the scale argument.\n            # Becomes a noop if already a list.\n            scale = list(scale)\n            for n in range(n_values):\n                scale[n] = ord(scale[n])\n\n        elif data_type in [SDC.UCHAR8, SDC.UINT8]:\n            buf = _C.array_byte(n_values)\n\n        elif data_type == SDC.INT8:\n            buf = _C.array_int8(n_values)\n\n        elif data_type == SDC.INT16:\n            buf = _C.array_int16(n_values)\n\n        elif data_type == SDC.UINT16:\n            buf = _C.array_uint16(n_values)\n\n        elif data_type == SDC.INT32:\n            buf = _C.array_int32(n_values)\n\n        elif data_type == SDC.UINT32:\n            buf = _C.array_uint32(n_values)\n\n        elif data_type == SDC.FLOAT32:\n            buf = _C.array_float32(n_values)\n\n        elif data_type == SDC.FLOAT64:\n            buf = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"setscale: illegal or usupported data_type\")\n\n        if n_values == 1:\n            buf[0] = scale\n        else:\n            for n in range(n_values):\n                buf[n] = scale[n]\n        status = _C.SDsetdimscale(self._id, n_values, data_type, buf)\n        _checkErr('setscale', status, 'cannot execute')", "response": "Initialize the scale values along the dimension."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the dimension standard string attributes.", "response": "def getstrs(self):\n        \"\"\"Retrieve the dimension standard string attributes.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          3-element tuple holding:\n            -dimension label  (attribute 'long_name')\n            -dimension unit   (attribute 'units')\n            -dimension format (attribute 'format')\n\n        An exception is raised if the standard attributes have\n        not been set.\n\n        C library equivalent: SDgetdimstrs\n                                                \"\"\"\n\n        status, label, unit, format = _C.SDgetdimstrs(self._id, 128)\n        _checkErr('getstrs', status, 'cannot execute')\n        return label, unit, format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setstrs(self, label, unit, format):\n\n        status = _C.SDsetdimstrs(self._id, label, unit, format)\n        _checkErr('setstrs', status, 'cannot execute')", "response": "Set the dimension standard string attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlocates an existing vdata or create a new vdata in the HDF file.", "response": "def attach(self, num_name, write=0):\n        \"\"\"Locate an existing vdata or create a new vdata in the HDF file,\n        returning a VD instance.\n\n        Args::\n\n          num_name  Name or reference number of the vdata. An existing vdata\n                    can be specified either through its reference number or\n                    its name. Use -1 to create a new vdata.\n                    Note that uniqueness is not imposed on vdatas names,\n                    whereas refnums are guaranteed to be unique. Thus\n                    knowledge of its reference number may be the only way\n                    to get at a wanted vdata.\n\n          write     Set to 0 to open the vdata in read-only mode,\n                    set to 1 to open it in write mode\n\n\n        Returns::\n\n          VD instance representing the vdata\n\n        C library equivalent : VSattach\n\n        After creating a new vdata (num_name == -1), fields must be\n        defined using method fdefine() of the VD instance, and those\n        fields must be allocated to the vdata with method setfields().\n        Same results can be achieved, but more simply, by calling the\n        create() method of the VS instance.\n                                                    \"\"\"\n\n        mode = write and 'w' or 'r'\n        if isinstance(num_name, str):\n            num = self.find(num_name)\n        else:\n            num = num_name\n        vd = _C.VSattach(self._hdf_inst._id, num, mode)\n        if vd < 0:\n            _checkErr('attach', vd, 'cannot attach vdata')\n        return VD(self, vd)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new vdata with the given name and allocating the fields.", "response": "def create(self, name, fields):\n        \"\"\"Create a new vdata, setting its name and allocating\n        its fields.\n\n        Args::\n\n          name     Name to assign to the vdata\n          fields   Sequence of field definitions. Each field definition\n                   is a sequence with the following elements in order:\n\n                   - field name\n                   - field type (one of HC.xxx constants)\n                   - field order (number of values)\n\n                   Fields are allocated to the vdata in the given order\n\n\n        Returns::\n\n          VD instance representing the created vdata\n\n        Calling the create() method is equivalent to the following calls:\n          - vd = attach(-1,1), to create a new vdata and open it in\n                 write mode\n          - vd._name = name, to set the vdata name\n          - vd.fdefine(...), to define the name, type and order of\n                 each field\n          - vd.setfields(...), to allocate fields to the vdata\n\n        C library equivalent : no equivalent\n                                                      \"\"\"\n\n        try:\n            # Create new vdata (-1), open in write mode (1)\n            vd = self.attach(-1, 1)\n            # Set vdata name\n            vd._name = name\n            # Define fields\n            allNames = []\n            for name, type, order in fields:\n                vd.fdefine(name, type, order)\n                allNames.append(name)\n            # Allocate fields to the vdata\n            vd.setfields(*allNames)\n            return vd\n        except HDF4Error as msg:\n            raise HDF4Error(\"error creating vdata (%s)\" % msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find(self, vName):\n\n        refNum = _C.VSfind(self._hdf_inst._id, vName)\n        _checkErr(\"find\", refNum, \"cannot find vdata %s\" % vName)\n        return refNum", "response": "Find the reference number of a vdata given its name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the next vdata in the HDF file.", "response": "def next(self, vRef):\n        \"\"\"Get the reference number of the vdata following a given\n        vdata.\n\n        Args::\n\n          vRef   Reference number of the vdata preceding the one\n                 we require. Set to -1 to get the first vdata in\n                 the HDF file. Knowing its reference number,\n                 the vdata can then be opened (attached) by passing this\n                 reference number to the attach() method.\n\n        Returns::\n\n          Reference number of the vdata following the one given\n          by argument vref\n\n        An exception is raised if no vdata follows the one given by vRef.\n\n        C library equivalent : VSgetid\n                                               \"\"\"\n\n        num = _C.VSgetid(self._hdf_inst._id, vRef)\n        _checkErr('next', num, 'cannot get next vdata')\n        return num"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn info about all the file vdatas in the current directory.", "response": "def vdatainfo(self, listAttr=0):\n        \"\"\"Return info about all the file vdatas.\n\n        Args::\n\n          listAttr   Set to 0 to ignore vdatas used to store attribute\n                     values, 1 to list them (see the VD._isattr readonly\n                     attribute)\n\n        Returns::\n\n          List of vdata descriptions. Each vdata is described as\n          a 9-element tuple, composed of the following:\n\n          - vdata name\n          - vdata class\n          - vdata reference number\n          - vdata number of records\n          - vdata number of fields\n          - vdata number of attributes\n          - vdata record size in bytes\n          - vdata tag number\n          - vdata interlace mode\n\n\n        C library equivalent : no equivalent\n                                                 \"\"\"\n\n        lst = []\n        ref = -1      # start at beginning\n        while True:\n            try:\n                nxtRef = self.next(ref)\n            except HDF4Error:    # no vdata left\n                break\n            # Attach the vdata and check for an \"attribute\" vdata.\n            ref = nxtRef\n            vdObj = self.attach(ref)\n            if listAttr or not vdObj._isattr:\n                # Append a list of vdata properties.\n                lst.append((vdObj._name,\n                            vdObj._class,\n                            vdObj._refnum,\n                            vdObj._nrecs,\n                            vdObj._nfields,\n                            vdObj._nattrs,\n                            vdObj._recsize,\n                            vdObj._tag,\n                            vdObj._interlace))\n            vdObj.detach()\n        return lst"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef storedata(self, fieldName, values, data_type, vName, vClass):\n\n        # See if the field is multi-valued.\n        nrecs = len(values)\n        if type(values[0]) in [list, tuple]:\n            order = len(values[0])\n            # Replace input list with a flattened list.\n            newValues = []\n            for el in values:\n                for e in el:\n                    newValues.append(e)\n            values = newValues\n        else:\n            order = 1\n        n_values = nrecs * order\n        if data_type == HC.CHAR8:\n            buf = _C.array_byte(n_values)\n            # Allow values to be passed as a string.\n            # Noop if a list is passed.\n            values = list(values)\n            for n in range(n_values):\n                values[n] = ord(values[n])\n\n        elif data_type in [HC.UCHAR8, HC.UINT8]:\n            buf = _C.array_byte(n_values)\n\n        elif data_type == HC.INT8:\n            # SWIG refuses negative values here. We found that if we\n            # pass them as byte values, it will work.\n            buf = _C.array_int8(n_values)\n            values = list(values)\n            for n in range(n_values):\n                v = values[n]\n                if v >= 0:\n                    v &= 0x7f\n                else:\n                    v = abs(v) & 0x7f\n                    if v:\n                        v = 256 - v\n                    else:\n                        v = 128         # -128 in 2s complement\n                values[n] = v\n\n        elif data_type == HC.INT16:\n            buf = _C.array_int16(n_values)\n\n        elif data_type == HC.UINT16:\n            buf = _C.array_uint16(n_values)\n\n        elif data_type == HC.INT32:\n            buf = _C.array_int32(n_values)\n\n        elif data_type == HC.UINT32:\n            buf = _C.array_uint32(n_values)\n\n        elif data_type == HC.FLOAT32:\n            buf = _C.array_float32(n_values)\n\n        elif data_type == HC.FLOAT64:\n            buf = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"storedata: illegal or unimplemented data_type\")\n\n        for n in range(n_values):\n            buf[n] = values[n]\n        if order == 1:\n            vd = _C.VHstoredata(self._hdf_inst._id, fieldName, buf,\n                                nrecs, data_type, vName, vClass)\n        else:\n            vd = _C.VHstoredatam(self._hdf_inst._id, fieldName, buf,\n                                nrecs, data_type, vName, vClass, order)\n\n        _checkErr('storedata', vd, 'cannot create vdata')\n\n        return vd", "response": "Create and initialize a single field vdata returning the vdata reference number."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefining a field in the vdata.", "response": "def fdefine(self, name, type, order):\n        \"\"\"Define a field. To initialize a newly created vdata with\n        fields created with fdefine(), assign a tuple of field names\n        to the _fields attribute or call the setfields() method.\n\n        Args::\n\n          name     field name\n          type     field data type (one of HC.xxx)\n          order    field order (number of values in the field)\n\n        Returns::\n\n          None\n\n        C library equivalent : VSfdefine\n                                            \"\"\"\n\n        _checkErr('fdefine', _C.VSfdefine(self._id, name, type, order),\n                  'cannot define field')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setfields(self, *fldNames):\n\n        _checkErr('setfields', _C.VSsetfields(self._id, ','.join(fldNames)),\n                  'cannot execute')\n        self._setfields = fldNames", "response": "Set the names and order of the fields that this vdata contains."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef field(self, name_index):\n\n        # Transform a name to an index number\n        if isinstance(name_index, str):\n            status, index = _C.VSfindex(self._id, name_index)\n            _checkErr('field', status, \"illegal field name: %s\" % name_index)\n        else:\n            n = _C.VFnfields(self._id)\n            _checkErr('field', n, 'cannot execute')\n            index = name_index\n            if index >= n:\n                raise HDF4Error(\"field: illegal index number\")\n        return VDField(self, index)", "response": "Returns a VDField instance representing a field of the vdata."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nseeks to the beginning of the record identified by its record index. A succeeding read will load this record in the memory.", "response": "def seek(self, recIndex):\n        \"\"\"Seek to the beginning of the record identified by its\n        record index. A succeeding read will load this record in\n        memory.\n\n        Args::\n\n          recIndex  index of the record in the vdata; numbering\n                    starts at 0. Legal values range from 0\n                    (start of vdata) to the current number of\n                    records (at end of vdata).\n\n        Returns::\n\n          record index\n\n        An exception is raised if an attempt is made to seek beyond the\n        last record.\n\n        The C API prohibits seeking past the next-to-last record,\n        forcing one to read the last record to advance to the end\n        of the vdata. The python API removes this limitation.\n\n        Seeking to the end of the vdata can also be done by calling\n        method ``seekend()``.\n\n        C library equivalent : VSseek\n                                                \"\"\"\n\n        if recIndex > self._nrecs - 1:\n            if recIndex == self._nrecs:\n                return self.seekend()\n            else:\n                raise HDF4Error(\"attempt to seek past last record\")\n        n = _C.VSseek(self._id, recIndex)\n        _checkErr('seek', n, 'cannot seek')\n        self._offset = n\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef seekend(self):\n\n        try:\n            # Seek to the next-to-last record position\n            n = self.seek(self._nrecs - 1)       # updates _offset\n            # Read last record, ignoring values\n            self.read(1)                         # updates _offset\n            return self._nrecs\n        except HDF4Error:\n            raise HDF4Error(\"seekend: cannot execute\")", "response": "Seek to the end of the last record in the vdata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the values of a number of records from the current record.", "response": "def read(self, nRec=1):\n        \"\"\"Retrieve the values of a number of records, starting\n        at the current record position. The current record position\n        is advanced by the number of records read. Current position\n        is 0 after \"opening\" the vdata with the attach() method.\n\n        Args::\n\n          nRec    number of records to read\n\n\n        Returns::\n\n          2-level list. First level is a sequence of records,\n          second level gives the sequence of values for each record.\n          The values returned for each record are those of the fields\n          specified in the last call to method setfields(), in that\n          order. The complete vdata field set is returned if\n          setfields() has not been called.\n\n        An exception is raised if the current record position is\n        already at the end of the vdata when read() is called. This\n        exception can be caught as an \"end of vdata\" indication to\n        exit a loop which scans each record of the vdata. Otherwise,\n        the number of records to be read is lowered to the number of\n        records remaining in the vdata, if that number is less than\n        the number asked for by parameter 'nRec'. Setting 'nRec' to\n        an arbitrarily large value can thus be used to retrieve the\n        remaining records in the vdata.\n\n        C library equivalent : VSread\n                                                       \"\"\"\n        # Validate number of records to read vs the current offset.\n        # Return \"end of vdata\" exception if already at end of vdata\n        # otherwise \"clip\" the number of records if it exceeds the\n        # number of remaining records in the vdata.\n        n = self._nrecs\n        if self._offset == n:\n            raise HDF4Error(\"end of vdata reached\")\n        if self._offset + nRec > n:\n            nRec = self._offset + nRec - n\n\n        fields = self._setfields or self._fields\n        nFields = len(fields)\n        fieldList = ','.join(fields)\n        _checkErr('read', _C.VSsetfields(self._id, fieldList),\n                  'error defining fields to read')\n\n        # Allocate a buffer to store the packed records.\n        bufSize = self.sizeof(fields) * nRec\n        bigBuf = _C.array_byte(bufSize)\n\n        # Read records\n        nRead = _C.VSread(self._id, bigBuf, nRec, 0)   # 0: FULL_INTERLACE\n        _checkErr('read', nRead, 'read error')\n        self._offset += nRec\n\n        # Allocate an array to store a pointer to the field buffer.\n        fldArr = _C.new_array_voidp(1)\n\n        # Initialize return value\n        values = []\n        for numRec in range(nRead):\n            v = []\n            for numFld in range(nFields):\n                v.append(None)\n            values.append(v)\n\n        # Unpack each field in turn.\n        for numFld in range(nFields):\n            fld = self.field(fields[numFld])\n            data_type = fld._type\n            order = fld._order\n            n_values = order * nRead\n\n            # Allocate a buffer to store the field values.\n            if data_type in [HC.CHAR8, HC.UCHAR8, HC.UINT8]:\n                buf = _C.array_byte(n_values)\n\n            elif data_type == HC.INT8:\n                buf = _C.array_int8(n_values)\n\n            elif data_type == HC.INT16:\n                buf = _C.array_int16(n_values)\n\n            elif data_type == HC.UINT16:\n                buf = _C.array_uint16(n_values)\n\n            elif data_type == HC.INT32:\n                buf = _C.array_int32(n_values)\n\n            elif data_type == HC.UINT32:\n                buf = _C.array_uint32(n_values)\n\n            elif data_type == HC.FLOAT32:\n                buf = _C.array_float32(n_values)\n\n            elif data_type == HC.FLOAT64:\n                buf = _C.array_float64(n_values)\n\n            else:\n                raise HDF4Error(\"read: illegal or unupported type %d\" % \\\n                                 data_type)\n\n            # Unpack the field values.\n            _C.array_voidp_setitem(fldArr, 0, buf)\n            _checkErr('read',\n                      _C.VSfpack(self._id, 1, fieldList, bigBuf, bufSize,\n                                 nRead, fld._name, fldArr),\n                      \"cannot execute\")\n\n            # Extract values from the field buffer.\n            k = 0\n            for numRec in range(nRead):\n                if order == 1:\n                    values[numRec][numFld] = buf[k]\n                    k += 1\n                else:\n                    # Handle strings specially\n                    if data_type == HC.CHAR8:\n                        s = ''\n                        for i in range(order):\n                            v = buf[k]\n                            if v != 0:\n                                s += chr(v)\n                            k += 1\n                        values[numRec][numFld] = s\n                    # Return field values as a list\n                    else:\n                        values[numRec][numFld] = []\n                        for i in range(order):\n                            values[numRec][numFld].append(buf[k])\n                            k += 1\n\n            del buf\n\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, values):\n\n        nFields = self._nfields\n        # Fields give the order the record values, as defined in the\n        # last call to setfields()\n        fields = self._setfields or self._fields\n        # We must pack values using the effective field order in the vdata\n        fieldList = ','.join(self._fields)\n\n        # Validate the values argument.\n        if nFields != len(fields):\n            raise HDF4Error(\"write: must write whole records\")\n        if type(values) not in [list, tuple]:\n            raise HDF4Error(\"write: values must be a sequence\")\n        nRec = len(values)\n        for n in range(nRec):\n            rec = values[n]\n            if type(rec) not in [list, tuple]:\n                raise HDF4Error(\"write: records must be given as sequences\")\n            # Make sure each record is complete.\n            if len(rec) != nFields:\n                raise HDF4Error(\"write: records must specify every field\")\n\n        # Allocate a buffer to store the packed records.\n        bufSize = self._recsize * nRec\n        bigBuf = _C.array_byte(bufSize)\n\n        # Allocate an array to store a pointer to the field buffer.\n        fldArr = _C.new_array_voidp(1)\n\n        # Pack each field in turn.\n        for numFld in range(nFields):\n            fld = self.field(fields[numFld])\n            data_type = fld._type\n            order = fld._order\n            n_values = order * nRec\n\n            # Allocate a buffer to store the field values.\n            if data_type in [HC.CHAR8, HC.UCHAR8, HC.UINT8]:\n                buf = _C.array_byte(n_values)\n\n            elif data_type == HC.INT8:\n                buf = _C.array_int8(n_values)\n\n            elif data_type == HC.INT16:\n                buf = _C.array_int16(n_values)\n\n            elif data_type == HC.UINT16:\n                buf = _C.array_uint16(n_values)\n\n            elif data_type == HC.INT32:\n                buf = _C.array_int32(n_values)\n\n            elif data_type == HC.UINT32:\n                buf = _C.array_uint32(n_values)\n\n            elif data_type == HC.FLOAT32:\n                buf = _C.array_float32(n_values)\n\n            elif data_type == HC.FLOAT64:\n                buf = _C.array_float64(n_values)\n\n            else:\n                raise HDF4Error(\"write: illegal or unupported type %d\" % \\\n                                 data_type)\n\n            # Load the field buffer with values.\n            k = 0\n            for numRec in range(nRec):\n                val = values[numRec][numFld]\n                # Single-valued field\n                if order == 1:\n                    buf[k] = val\n                    k += 1\n                # Multi-valued field\n                else:\n                    # Handle strings specially.\n                    if data_type == HC.CHAR8:\n                        if not isinstance(val, str):\n                            raise HDF4Error(\"char fields must be set with strings\")\n                        n = len(val)\n                        for i in range(order):\n                            buf[k] = i < n and ord(val[i]) or 0\n                            k += 1\n                    # Should validate field elements ...\n                    elif type(val) not in [list, tuple]:\n                        raise HDF4Error(\"multi-values fields must be given as sequences\")\n                    else:\n                        for i in range(order):\n                            buf[k] = val[i]\n                            k += 1\n\n            # Store address of the field buffer in first position\n            # of the field array. Pack the field values.\n            _C.array_voidp_setitem(fldArr, 0, buf) # fldArr[0] = buf\n            _checkErr('write',\n                      _C.VSfpack(self._id, 0, fieldList, bigBuf, bufSize,\n                                 nRec, fld._name, fldArr),\n                      \"cannot execute\")\n            del buf\n\n        # Write the packed records.\n        n = _C.VSwrite(self._id, bigBuf, nRec, 0)   # 0: FULL_INTERLACE\n        _checkErr('write', n, 'cannot execute')\n        self._offset += nRec\n\n        return n", "response": "Write the records to the vdata."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inquire(self):\n\n        status, nRecs, interlace, fldNames, size, vName = \\\n                _C.VSinquire(self._id)\n        _checkErr('inquire', status, \"cannot query vdata info\")\n        return nRecs, interlace, fldNames.split(','), size, vName", "response": "Retrieve info about the vdata."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fieldinfo(self):\n\n        lst = []\n        for n in range(self._nfields):\n            fld = self.field(n)\n            lst.append((fld._name,\n                        fld._type,\n                        fld._order,\n                        fld._nattrs,\n                        fld._index,\n                        fld._esize,\n                        fld._isize))\n\n        return lst", "response": "Retrieve info about all vdata fields."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sizeof(self, fields):\n\n        if type(fields) in [tuple, list]:\n            str = ','.join(fields)\n        else:\n            str = fields\n        n = _C.VSsizeof(self._id, str)\n        _checkErr('sizeof', n, \"cannot retrieve field sizes\")\n        return n", "response": "Retrieve the size in bytes of the given fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a given set of fields in the vdata exists.", "response": "def fexist(self, fields):\n        \"\"\"Check if a vdata contains a given set of fields.\n\n        Args::\n\n          fields   sequence of field names whose presence in the\n                   vdata must be checked\n\n        Returns::\n\n          true  (1) if the given fields are present\n          false (0) otherwise\n\n        C library equivalent : VSfexist\n                                                         \"\"\"\n\n        if type(fields) in [tuple, list]:\n            str = ','.join(fields)\n        else:\n            str = fields\n        ret = _C.VSfexist(self._id, str)\n        if ret < 0:\n            return 0\n        else:\n            return 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch the field for a given attribute.", "response": "def find(self, name):\n        \"\"\"Search the field for a given attribute.\n\n        Args::\n\n          name    attribute name\n\n        Returns::\n\n          if found, VDAttr instance describing the attribute\n          None otherwise\n\n         C library equivalent : VSfindattr\n                                                  \"\"\"\n\n        try:\n            att = self.attr(name)\n            if att._index is None:\n                att = None\n        except HDF4Error:\n            att = None\n        return att"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, data_type, values):\n        try:\n            n_values = len(values)\n        except:\n            values = [values]\n            n_values = 1\n        if data_type == HC.CHAR8:\n            buf = _C.array_byte(n_values)\n            # Allow values to be passed as a string.\n            # Noop if a list is passed.\n            values = list(values)\n            for n in range(n_values):\n                if not isinstance(values[n], int):\n                    values[n] = ord(values[n])\n\n        elif data_type in [HC.UCHAR8, HC.UINT8]:\n            buf = _C.array_byte(n_values)\n\n        elif data_type == HC.INT8:\n            # SWIG refuses negative values here. We found that if we\n            # pass them as byte values, it will work.\n            buf = _C.array_int8(n_values)\n            values = list(values)\n            for n in range(n_values):\n                v = values[n]\n                if v >= 0:\n                    v &= 0x7f\n                else:\n                    v = abs(v) & 0x7f\n                    if v:\n                        v = 256 - v\n                    else:\n                        v = 128         # -128 in 2s complement\n                values[n] = v\n\n        elif data_type == HC.INT16:\n            buf = _C.array_int16(n_values)\n\n        elif data_type == HC.UINT16:\n            buf = _C.array_uint16(n_values)\n\n        elif data_type == HC.INT32:\n            buf = _C.array_int32(n_values)\n\n        elif data_type == HC.UINT32:\n            buf = _C.array_uint32(n_values)\n\n        elif data_type == HC.FLOAT32:\n            buf = _C.array_float32(n_values)\n\n        elif data_type == HC.FLOAT64:\n            buf = _C.array_float64(n_values)\n\n        else:\n            raise HDF4Error(\"set: illegal or unimplemented data_type\")\n\n        for n in range(n_values):\n            buf[n] = values[n]\n        status = _C.VSsetattr(self._vd_inst._id, self._fIndex, self._name,\n                              data_type, n_values, buf)\n        _checkErr('attr', status, 'cannot execute')\n        # Update the attribute index\n        self._index = _C.VSfindattr(self._vd_inst._id, self._fIndex,\n                                    self._name);\n        if self._index < 0:\n            raise HDF4Error(\"set: error retrieving attribute index\")", "response": "Set the value of the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(self):\n\n        # Make sure the attribute exists.\n        if self._index is None:\n            raise HDF4Error(\"non existent attribute\")\n\n        status, name, type, order, size = \\\n                _C.VSattrinfo(self._vd_inst._id, self._fIndex, self._index)\n        _checkErr('info', status, \"execution error\")\n        return name, type, order, size", "response": "Retrieve info about the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getlibversion():\n\n    status, major_v, minor_v, release, info = _C.Hgetlibversion()\n    _checkErr('getlibversion', status, \"cannot get lib version\")\n    return major_v, minor_v, release, info", "response": "Get the library version info."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getfileversion(self):\n\n        status, major_v, minor_v, release, info = _C.Hgetfileversion(self._id)\n        _checkErr('getfileversion', status, \"cannot get file version\")\n        return major_v, minor_v, release, info", "response": "Get the file version info."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef colorize(lead, num, color):\n    if num != 0 and ANSIBLE_COLOR and color is not None:\n        return \"%s%s%-15s\" % (stringc(lead, color), stringc(\"=\", color), stringc(str(num), color))\n    else:\n        return \"%s=%-4s\" % (lead, str(num))", "response": "Colorize a single lead number"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of strings with important stuff for a given model.", "response": "def get_fields(Model, \n               parent_field=\"\",\n               model_stack=None,\n               stack_limit=2,\n               excludes=['permissions', 'comment', 'content_type']):\n    \"\"\"\n    Given a Model, return a list of lists of strings with important stuff:\n    ...\n    ['test_user__user__customuser', 'customuser', 'User', 'RelatedObject']\n    ['test_user__unique_id', 'unique_id', 'TestUser', 'CharField']\n    ['test_user__confirmed', 'confirmed', 'TestUser', 'BooleanField']\n    ...\n\n     \"\"\"\n    out_fields = []\n\n    if model_stack is None:\n        model_stack = []\n\n    # github.com/omab/python-social-auth/commit/d8637cec02422374e4102231488481170dc51057\n    if isinstance(Model, basestring):\n        app_label, model_name = Model.split('.')\n        Model = models.get_model(app_label, model_name)\n\n    fields = Model._meta.fields + Model._meta.many_to_many + Model._meta.get_all_related_objects()\n    model_stack.append(Model)\n\n    # do a variety of checks to ensure recursion isnt being redundant\n\n    stop_recursion = False\n    if len(model_stack) > stack_limit:\n        # rudimentary CustomUser->User->CustomUser->User detection\n        if model_stack[-3] == model_stack[-1]:\n            stop_recursion = True\n\n        # stack depth shouldn't exceed x\n        if len(model_stack) > 5:\n            stop_recursion = True\n\n        # we've hit a point where we are repeating models\n        if len(set(model_stack)) != len(model_stack):\n            stop_recursion = True\n\n    if stop_recursion:\n        return [] # give empty list for \"extend\"\n\n    for field in fields:\n        field_name = field.name\n\n        if isinstance(field, RelatedObject):\n            field_name = field.field.related_query_name()\n\n        if parent_field:\n            full_field = \"__\".join([parent_field, field_name])\n        else:\n            full_field = field_name\n\n        if len([True for exclude in excludes if (exclude in full_field)]):\n            continue\n\n        # add to the list\n        out_fields.append([full_field, field_name, Model, field.__class__])\n\n        if not stop_recursion and \\\n                (isinstance(field, ForeignKey) or isinstance(field, OneToOneField) or \\\n                isinstance(field, RelatedObject) or isinstance(field, ManyToManyField)):\n\n            if isinstance(field, RelatedObject):\n                RelModel = field.model\n                #field_names.extend(get_fields(RelModel, full_field, True))\n            else:\n                RelModel = field.related.parent_model\n\n            out_fields.extend(get_fields(RelModel, full_field, list(model_stack)))\n\n    return out_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef give_model_field(full_field, Model):\n    field_data = get_fields(Model, '', [])\n\n    for full_key, name, _Model, _ModelField in field_data:\n        if full_key == full_field:\n            return full_key, name, _Model, _ModelField\n\n    raise Exception('Field key `{0}` not found on `{1}`.'.format(full_field, Model.__name__))", "response": "Given a field_name and Model returns a full_key name Model and ModelField"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timeline(self, request, drip_id, into_past, into_future):\n        from django.shortcuts import render, get_object_or_404\n\n        drip = get_object_or_404(Drip, id=drip_id)\n\n        shifted_drips = []\n        seen_users = set()\n        for shifted_drip in drip.drip.walk(into_past=int(into_past), into_future=int(into_future)+1):\n            shifted_drip.prune()\n            shifted_drips.append({\n                'drip': shifted_drip,\n                'qs': shifted_drip.get_queryset().exclude(id__in=seen_users)\n            })\n            seen_users.update(shifted_drip.get_queryset().values_list('id', flat=True))\n\n        return render(request, 'drip/timeline.html', locals())", "response": "Return a list of people who should get emails."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef walk(self, into_past=0, into_future=0):\n        walked_range = []\n        for shift in range(-into_past, into_future):\n            kwargs = dict(drip_model=self.drip_model,\n                          name=self.name,\n                          now_shift_kwargs={'days': shift})\n            walked_range.append(self.__class__(**kwargs))\n        return walked_range", "response": "Walk over a date range and create new instances of self with new ranges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply_queryset_rules(self, qs):\n        clauses = {\n            'filter': [],\n            'exclude': []}\n\n        for rule in self.drip_model.queryset_rules.all():\n\n            clause = clauses.get(rule.method_type, clauses['filter'])\n\n            kwargs = rule.filter_kwargs(qs, now=self.now)\n            clause.append(Q(**kwargs))\n\n            qs = rule.apply_any_annotation(qs)\n\n        if clauses['exclude']:\n            qs = qs.exclude(functools.reduce(operator.or_, clauses['exclude']))\n        qs = qs.filter(*clauses['filter'])\n\n        return qs", "response": "Apply all queryset rules to the given queryset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the queryset prune sent people and send it.", "response": "def run(self):\n        \"\"\"\n        Get the queryset, prune sent people, and send it.\n        \"\"\"\n        if not self.drip_model.enabled:\n            return None\n\n        self.prune()\n        count = self.send()\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove all SentDrip entries from the queryset.", "response": "def prune(self):\n        \"\"\"\n        Do an exclude for all Users who have a SentDrip already.\n        \"\"\"\n        target_user_ids = self.get_queryset().values_list('id', flat=True)\n        exclude_user_ids = SentDrip.objects.filter(date__lt=conditional_now(),\n                                                   drip=self.drip_model,\n                                                   user__id__in=target_user_ids)\\\n                                           .values_list('user_id', flat=True)\n        self._queryset = self.get_queryset().exclude(id__in=exclude_user_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend the message to each user on the queryset. Returns the number of SentDrips.", "response": "def send(self):\n        \"\"\"\n        Send the message to each user on the queryset.\n\n        Create SentDrip for each user that gets a message.\n\n        Returns count of created SentDrips.\n        \"\"\"\n\n        if not self.from_email:\n            self.from_email = getattr(settings, 'DRIP_FROM_EMAIL', settings.DEFAULT_FROM_EMAIL)\n        MessageClass = message_class_for(self.drip_model.message_class)\n\n        count = 0\n        for user in self.get_queryset():\n            message_instance = MessageClass(self, user)\n            try:\n                result = message_instance.message.send()\n                if result:\n                    SentDrip.objects.create(\n                        drip=self.drip_model,\n                        user=user,\n                        from_email=self.from_email,\n                        from_email_name=self.from_email_name,\n                        subject=message_instance.subject,\n                        body=message_instance.body\n                    )\n                    count += 1\n            except Exception as e:\n                logging.error(\"Failed to send drip %s to user %s: %s\" % (self.drip_model.id, user, e))\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the angle to the vector other", "response": "def angle(self, other):\n        \"\"\"Return the angle to the vector other\"\"\"\n        return math.acos(self.dot(other) / (self.magnitude() * other.magnitude()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning one vector projected on the vector other", "response": "def project(self, other):\n        \"\"\"Return one vector projected on the vector other\"\"\"\n        n = other.normalized()\n        return self.dot(n) * n"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rotate_around(self, axis, theta):\n\n        # Adapted from equations published by Glenn Murray.\n        # http://inside.mines.edu/~gmurray/ArbitraryAxisRotation/ArbitraryAxisRotation.html\n        x, y, z = self.x, self.y, self.z\n        u, v, w = axis.x, axis.y, axis.z\n\n        # Extracted common factors for simplicity and efficiency\n        r2 = u**2 + v**2 + w**2\n        r = math.sqrt(r2)\n        ct = math.cos(theta)\n        st = math.sin(theta) / r\n        dt = (u * x + v * y + w * z) * (1 - ct) / r2\n        return Vector3((u * dt + x * ct + (-w * y + v * z) * st),\n                       (v * dt + y * ct + (w * x - u * z) * st),\n                       (w * dt + z * ct + (-v * x + u * y) * st))", "response": "Return the vector rotated around axis through angle theta. Right hand rule applies."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_ip(self, values, from_unit):\n        if from_unit == 'Btu/h-ft2-F':\n            return values, from_unit\n        else:\n            return self.to_unit(values, 'Btu/h-ft2-F', from_unit), 'Btu/h-ft2-F'", "response": "Return values in IP and the units to which the values have been converted."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_si(self, values, from_unit):\n        if from_unit == 'W/m2-K':\n            return values, from_unit\n        else:\n            return self.to_unit(values, 'W/m2-K', from_unit), 'W/m2-K'", "response": "Return values in SI and the units to which the values have been converted."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preparedir(target_dir, remove_content=True):\n    if os.path.isdir(target_dir):\n        if remove_content:\n            nukedir(target_dir, False)\n        return True\n    else:\n        try:\n            os.makedirs(target_dir)\n            return True\n        except Exception as e:\n            print(\"Failed to create folder: %s\\n%s\" % (target_dir, e))\n            return False", "response": "Prepare a folder for analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes all the files inside target_dir", "response": "def nukedir(target_dir, rmdir=False):\n    \"\"\"Delete all the files inside target_dir.\n\n    Usage:\n        nukedir(\"c:/ladybug/libs\", True)\n    \"\"\"\n    d = os.path.normpath(target_dir)\n\n    if not os.path.isdir(d):\n        return\n\n    files = os.listdir(d)\n\n    for f in files:\n        if f == '.' or f == '..':\n            continue\n        path = os.path.join(d, f)\n\n        if os.path.isdir(path):\n            nukedir(path)\n        else:\n            try:\n                os.remove(path)\n            except Exception:\n                print(\"Failed to remove %s\" % path)\n\n    if rmdir:\n        try:\n            os.rmdir(d)\n        except Exception:\n            print(\"Failed to remove %s\" % d)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_to_file_by_name(folder, fname, data, mkdir=False):\n    if not os.path.isdir(folder):\n        if mkdir:\n            preparedir(folder)\n        else:\n            created = preparedir(folder, False)\n            if not created:\n                raise ValueError(\"Failed to find %s.\" % folder)\n\n    file_path = os.path.join(folder, fname)\n\n    with open(file_path, writemode) as outf:\n        try:\n            outf.write(str(data))\n            return file_path\n        except Exception as e:\n            raise IOError(\"Failed to write %s to file:\\n\\t%s\" % (fname, str(e)))", "response": "Writes a string of data to file by filename and folder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_to_file(file_path, data, mkdir=False):\n    folder, fname = os.path.split(file_path)\n    return write_to_file_by_name(folder, fname, data, mkdir)", "response": "Write a string of data to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies a list of files to a new target folder.", "response": "def copy_files_to_folder(files, target_folder, overwrite=True):\n    \"\"\"Copy a list of files to a new target folder.\n\n    Returns:\n        A list of fullpath of the new files.\n    \"\"\"\n    if not files:\n        return []\n\n    for f in files:\n        target = os.path.join(target_folder, os.path.split(f)[-1])\n\n        if target == f:\n            # both file path are the same!\n            return target\n\n        if os.path.exists(target):\n            if overwrite:\n                # remove the file before copying\n                try:\n                    os.remove(target)\n                except Exception:\n                    raise IOError(\"Failed to remove %s\" % f)\n                else:\n                    shutil.copy(f, target)\n            else:\n                continue\n        else:\n            print('Copying %s to %s' % (os.path.split(f)[-1],\n                                        os.path.normpath(target_folder)))\n            shutil.copy(f, target)\n\n    return [os.path.join(target_folder, os.path.split(f)[-1]) for f in files]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bat_to_sh(file_path):\n    sh_file = file_path[:-4] + '.sh'\n    with open(file_path, 'rb') as inf, open(sh_file, 'wb') as outf:\n        outf.write('#!/usr/bin/env bash\\n\\n')\n        for line in inf:\n            # pass the path lines, etc to get to the commands\n            if line.strip():\n                continue\n            else:\n                break\n\n        for line in inf:\n            if line.startswith('echo'):\n                continue\n            modified_line = line.replace('c:\\\\radiance\\\\bin\\\\', '').replace('\\\\', '/')\n            outf.write(modified_line)\n\n    print('bash file is created at:\\n\\t%s' % sh_file)\n    # Heroku - Make command.sh executable\n    st = os.stat(sh_file)\n    os.chmod(sh_file, st.st_mode | 0o111)\n    return sh_file", "response": "Convert honeybee. bat file to. sh file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a file from a link in Python 2.", "response": "def _download_py2(link, path, __hdr__):\n    \"\"\"Download a file from a link in Python 2.\"\"\"\n    try:\n        req = urllib2.Request(link, headers=__hdr__)\n        u = urllib2.urlopen(req)\n    except Exception as e:\n        raise Exception(' Download failed with the error:\\n{}'.format(e))\n\n    with open(path, 'wb') as outf:\n        for l in u:\n            outf.write(l)\n    u.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _download_py3(link, path, __hdr__):\n    try:\n        req = urllib.request.Request(link, headers=__hdr__)\n        u = urllib.request.urlopen(req)\n    except Exception as e:\n        raise Exception(' Download failed with the error:\\n{}'.format(e))\n\n    with open(path, 'wb') as outf:\n        for l in u:\n            outf.write(l)\n    u.close()", "response": "Download a file from a link in Python 3."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload a file from a URL to a folder.", "response": "def download_file_by_name(url, target_folder, file_name, mkdir=False):\n    \"\"\"Download a file to a directory.\n\n    Args:\n        url: A string to a valid URL.\n        target_folder: Target folder for download (e.g. c:/ladybug)\n        file_name: File name (e.g. testPts.zip).\n        mkdir: Set to True to create the directory if doesn't exist (Default: False)\n    \"\"\"\n    # headers to \"spoof\" the download as coming from a browser (needed for E+ site)\n    __hdr__ = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 '\n               '(KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n               'Accept': 'text/html,application/xhtml+xml,'\n               'application/xml;q=0.9,*/*;q=0.8',\n               'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n               'Accept-Encoding': 'none',\n               'Accept-Language': 'en-US,en;q=0.8',\n               'Connection': 'keep-alive'}\n\n    # create the target directory.\n    if not os.path.isdir(target_folder):\n        if mkdir:\n            preparedir(target_folder)\n        else:\n            created = preparedir(target_folder, False)\n            if not created:\n                raise ValueError(\"Failed to find %s.\" % target_folder)\n    file_path = os.path.join(target_folder, file_name)\n\n    if (sys.version_info < (3, 0)):\n        _download_py2(url, file_path, __hdr__)\n    else:\n        _download_py3(url, file_path, __hdr__)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_file(url, file_path, mkdir=False):\n    folder, fname = os.path.split(file_path)\n    return download_file_by_name(url, folder, fname, mkdir)", "response": "Download a string of data to file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unzip_file(source_file, dest_dir=None, mkdir=False):\n    # set default dest_dir and create it if need be.\n    if dest_dir is None:\n        dest_dir, fname = os.path.split(source_file)\n    elif not os.path.isdir(dest_dir):\n        if mkdir:\n            preparedir(dest_dir)\n        else:\n            created = preparedir(dest_dir, False)\n            if not created:\n                raise ValueError(\"Failed to find %s.\" % dest_dir)\n\n    # extract files to destination\n    with zipfile.ZipFile(source_file) as zf:\n        for member in zf.infolist():\n            words = member.filename.split('\\\\')\n            for word in words[:-1]:\n                drive, word = os.path.splitdrive(word)\n                head, word = os.path.split(word)\n                if word in (os.curdir, os.pardir, ''):\n                    continue\n                dest_dir = os.path.join(dest_dir, word)\n            zf.extract(member, dest_dir)", "response": "Unzip a compressed file into a new directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a CSV file into a Python matrix of strings.", "response": "def csv_to_matrix(csv_file_path):\n    \"\"\"Load a CSV file into a Python matrix of strings.\n\n    Args:\n        csv_file_path: Full path to a valid CSV file (e.g. c:/ladybug/test.csv)\n    \"\"\"\n    mtx = []\n    with open(csv_file_path) as csv_data_file:\n        for row in csv_data_file:\n            mtx.append(row.split(','))\n    return mtx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a CSV file consisting only of numbers into a Python matrix of floats.", "response": "def csv_to_num_matrix(csv_file_path):\n    \"\"\"Load a CSV file consisting only of numbers into a Python matrix of floats.\n\n    Args:\n        csv_file_path: Full path to a valid CSV file (e.g. c:/ladybug/test.csv)\n    \"\"\"\n    mtx = []\n    with open(csv_file_path) as csv_data_file:\n        for row in csv_data_file:\n            mtx.append([float(val) for val in row.split(',')])\n    return mtx"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_json(cls, data):\n        # Initialize the class with all data missing\n        stat_ob = cls(None)\n\n        # Check required and optional keys\n        option_keys_none = ('ashrae_climate_zone', 'koppen_climate_zone',\n                            'extreme_cold_week', 'extreme_hot_week',\n                            'standard_pressure_at_elev')\n        option_keys_list = ('monthly_db_50', 'monthly_wb_50',\n                            'monthly_db_range_50', 'monthly_wb_range_50',\n                            'monthly_db_100', 'monthly_wb_100', 'monthly_db_20',\n                            'monthly_wb_20', 'monthly_db_04', 'monthly_wb_04',\n                            'monthly_wind', 'monthly_wind_dirs',\n                            'monthly_tau_beam', 'monthly_tau_diffuse')\n        option_keys_dict = ('typical_weeks', 'heating_dict', 'cooling_dict')\n        assert 'location' in data, 'Required key \"location\" is missing!'\n        for key in option_keys_none:\n            if key not in data:\n                data[key] = None\n        for key in option_keys_list:\n            if key not in data:\n                data[key] = []\n        for key in option_keys_dict:\n            if key not in data:\n                data[key] = {}\n\n        # assign the properties of the dictionary to the stat object.\n        stat_ob._location = Location.from_json(data['location'])\n        stat_ob._ashrae_climate_zone = data['ashrae_climate_zone']\n        stat_ob._koppen_climate_zone = data['koppen_climate_zone']\n        stat_ob._extreme_cold_week = AnalysisPeriod.from_json(data['extreme_cold_week'])\\\n            if data['extreme_cold_week'] else None\n        stat_ob._extreme_hot_week = AnalysisPeriod.from_json(data['extreme_hot_week'])\\\n            if data['extreme_hot_week'] else None\n        stat_ob._typical_weeks = {}\n        for key, val in data['typical_weeks'].items():\n            if isinstance(val, list):\n                stat_ob._typical_weeks[key] = [AnalysisPeriod.from_json(v) for v in val]\n            else:\n                stat_ob._typical_weeks[key] = AnalysisPeriod.from_json(val)\n        stat_ob._winter_des_day_dict = data['heating_dict']\n        stat_ob._summer_des_day_dict = data['cooling_dict']\n        stat_ob._monthly_db_50 = data['monthly_db_50']\n        stat_ob._monthly_wb_50 = data['monthly_wb_50']\n        stat_ob._monthly_db_range_50 = data['monthly_db_range_50']\n        stat_ob._monthly_wb_range_50 = data['monthly_wb_range_50']\n        stat_ob._monthly_db_100 = data['monthly_db_100']\n        stat_ob._monthly_wb_100 = data['monthly_wb_100']\n        stat_ob._monthly_db_20 = data['monthly_db_20']\n        stat_ob._monthly_wb_20 = data['monthly_wb_20']\n        stat_ob._monthly_db_04 = data['monthly_db_04']\n        stat_ob._monthly_wb_04 = data['monthly_wb_04']\n        stat_ob._monthly_wind = data['monthly_wind']\n        stat_ob._monthly_wind_dirs = data['monthly_wind_dirs']\n        stat_ob._stand_press_at_elev = data['standard_pressure_at_elev']\n        stat_ob._monthly_tau_beam = data['monthly_tau_beam']\n        stat_ob._monthly_tau_diffuse = data['monthly_tau_diffuse']\n\n        return stat_ob", "response": "Create STAT from json dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports data from a stat file.", "response": "def _import_data(self):\n        \"\"\"Import data from a stat file.\n        \"\"\"\n        # set default state to ironpython for very old ironpython (2.7.0)\n        iron_python = True\n        try:\n            iron_python = True if platform.python_implementation() == 'IronPython' \\\n                else False\n        except ValueError as e:\n            # older versions of IronPython fail to parse version correctly\n            # failed to parse IronPython sys.version: '2.7.5 (IronPython 2.7.5 (2.7.5.0)\n            # on .NET 4.0.30319.42000 (64-bit))'\n            if 'IronPython' in str(e):\n                iron_python = True\n\n        if iron_python:\n            statwin = codecs.open(self.file_path, 'r')\n        else:\n            statwin = codecs.open(self.file_path, 'r', encoding='utf-8', errors='ignore')\n        try:\n            line = statwin.readline()\n            # import header with location\n            self._header = [line] + [statwin.readline() for i in xrange(9)]\n            self._body = statwin.read()\n        except Exception as e:\n            import traceback\n            raise Exception('{}\\n{}'.format(e, traceback.format_exc()))\n        else:\n\n            # import location data\n            loc_name = self._header[2].strip().replace('Location -- ', '')\n            if ' - ' in loc_name:\n                city = ' '.join(loc_name.split(' - ')[:-1])\n            else:\n                # for US stat files it is full name separated by spaces\n                city = ' '.join(loc_name.split()[:-2])\n            country = loc_name.split(' ')[-1]\n            source = self._header[6].strip().replace('Data Source -- ', '')\n            station_id = self._header[8].strip().replace('WMO Station ', '')\n            if iron_python:\n                # IronPython\n                coord_pattern = re.compile(r\"{([NSEW])(\\s*\\d*)deg(\\s*\\d*)\")\n                matches = coord_pattern.findall(self._header[3].replace('\\xb0', 'deg'))\n            else:\n                # CPython\n                coord_pattern = re.compile(r\"{([NSEW])(\\s*\\d*) (\\s*\\d*)\")\n                matches = coord_pattern.findall(self._header[3])\n            lat_sign = -1 if matches[0][0] == 'S' else 1\n            latitude = lat_sign * (float(matches[0][1]) + (float(matches[0][2]) / 60))\n            lon_sign = -1 if matches[1][0] == 'W' else 1\n            longitude = lon_sign * (float(matches[1][1]) + (float(matches[1][2]) / 60))\n            time_zone = self._regex_check(r\"{GMT\\s*(\\S*)\\s*Hours}\", self._header[3])\n            elev_pattern = re.compile(r\"Elevation\\s*[-]*\\s*(\\d*)m\\s*(\\S*)\")\n            elev_matches = elev_pattern.findall(self._header[4])\n            if len(elev_matches) == 0:\n                elev_pattern = re.compile(r\"Elevation\\s*[-]*\\s*(\\d*)\\s*m\\s*(\\S*)\")\n                elev_matches = elev_pattern.findall(self._header[4])\n            elev_sign = -1 if elev_matches[0][-1].lower() == 'below' else 1\n            elevation = elev_sign * float(elev_matches[0][0])\n            self._location = Location()\n            self._location.city = city\n            self._location.country = country\n            self._location.source = source\n            self._location.station_id = station_id\n            self._location.latitude = latitude\n            self._location.longitude = longitude\n            self._location.time_zone = time_zone\n            self._location.elevation = elevation\n\n            # pull out individual properties\n            self._stand_press_at_elev = self._regex_check(\n                r\"Elevation\\s*[-]*\\s*(\\d*)\", self._header[5])\n            self._ashrae_climate_zone = self._regex_check(\n                r'Climate type\\s\"(\\S*)\"\\s\\(A', self._body)\n            self._koppen_climate_zone = self._regex_check(\n                r'Climate type\\s\"(\\S*)\"\\s\\(K', self._body)\n\n            # pull out extreme and seasonal weeks.\n            self._extreme_hot_week = self._regex_week_parse(\n                r\"Extreme Hot Week Period selected:\"\n                \"\\s*(\\w{3})\\s*(\\d{1,2}):\\s*(\\w{3})\\s*(\\d{1,2}),\")\n            self._extreme_cold_week = self._regex_week_parse(\n                r\"Extreme Cold Week Period selected:\"\n                \"\\s*(\\w{3})\\s*(\\d{1,2}):\\s*(\\w{3})\\s*(\\d{1,2}),\")\n            self._typical_weeks = self._regex_typical_week_parse()\n\n            # pull out annual design days\n            winter_vals = self._regex_parse(r\"Heating\\s(\\d.*)\")\n            for key, val in zip(DesignDay.heating_keys, winter_vals):\n                self._winter_des_day_dict[key] = val\n            summer_vals = self._regex_parse(r\"Cooling\\s(\\d.*)\")\n            for key, val in zip(DesignDay.cooling_keys, summer_vals):\n                self._summer_des_day_dict[key] = val\n\n            # Pull out relevant monthly information\n            self._monthly_tau_beam = self._regex_parse(r\"taub \\(beam\\)(.*)\")\n            self._monthly_tau_diffuse = self._regex_parse(r\"taud \\(diffuse\\)(.*)\")\n            self._monthly_db_50 = self._regex_parse(r\"Drybulb 5.0%(.*)\")\n            self._monthly_wb_50 = self._regex_parse(r\"Coincident Wetbulb 5.0%(.*)\")\n            self._monthly_db_100 = self._regex_parse(r\"Drybulb 10.%(.*)\")\n            self._monthly_wb_100 = self._regex_parse(r\"Coincident Wetbulb 10.%(.*)\")\n            self._monthly_db_20 = self._regex_parse(r\"Drybulb 2.0%(.*)\")\n            self._monthly_wb_20 = self._regex_parse(r\"Coincident Wetbulb 2.0%(.*)\")\n            self._monthly_db_04 = self._regex_parse(r\"Drybulb 0.4%(.*)\")\n            self._monthly_wb_04 = self._regex_parse(r\"Coincident Wetbulb 0.4%(.*)\")\n            self._monthly_db_range_50 = self._regex_parse(r\"Drybulb range - DB 5%(.*)\")\n            self._monthly_wb_range_50 = self._regex_parse(r\"Wetbulb range - DB 5%(.*)\")\n            self._monthly_wind = self._regex_parse(\n                r\"Monthly Statistics for Wind Speed[\\s\\S]*Daily Avg(.*)\")\n            for direction in self._wind_dir_names:\n                re_string = r\"Monthly Wind Direction %[\\s\\S]*\" + direction + r\"\\s(.*)\"\n                dirs = self._regex_parse(re_string)\n                if dirs != []:\n                    self._monthly_wind_dirs.append(dirs)\n            if self._monthly_wind_dirs == []:\n                self._monthly_wind_dirs = [[0] * 12 for i in xrange(8)]\n\n        finally:\n            statwin.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef annual_heating_design_day_996(self):\n        if bool(self._winter_des_day_dict) is True:\n            return DesignDay.from_ashrae_dict_heating(\n                self._winter_des_day_dict, self.location, False,\n                self._stand_press_at_elev)\n        else:\n            return None", "response": "A design day object representing the annual 99. 6% heating design day."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef annual_cooling_design_day_004(self):\n        if bool(self._summer_des_day_dict) is True:\n            tau = None\n            month_num = int(self._summer_des_day_dict['Month'])\n            if self._monthly_tau_beam != [] and self._monthly_tau_diffuse != [] \\\n                and self._monthly_tau_beam[month_num - 1] is not None and \\\n                    self._monthly_tau_diffuse[month_num - 1] is not None:\n                        tau = (self._monthly_tau_beam[month_num - 1],\n                               self._monthly_tau_diffuse[month_num - 1])\n            return DesignDay.from_ashrae_dict_cooling(\n                self._summer_des_day_dict, self.location, False,\n                self._stand_press_at_elev, tau)\n        else:\n            return None", "response": "A design day object representing the annual 0. 4% cooling design day."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef monthly_cooling_design_days_020(self):\n        if self.monthly_found is False or self._monthly_db_20 == [] \\\n                or self._monthly_wb_20 == []:\n                    return []\n        else:\n            db_conds = [DryBulbCondition(x, y) for x, y in zip(\n                self._monthly_db_20, self._monthly_db_range_50)]\n            hu_conds = [HumidityCondition(\n                'Wetbulb', x, self._stand_press_at_elev) for x in self._monthly_wb_20]\n            ws_conds = self.monthly_wind_conditions\n            sky_conds = self.monthly_clear_sky_conditions\n            return [DesignDay(\n                '2% Cooling Design Day for {}'.format(self._months[i]),\n                'SummerDesignDay', self._location,\n                db_conds[i], hu_conds[i], ws_conds[i], sky_conds[i])\n                    for i in xrange(12)]", "response": "A list of 12 objects representing monthly 2. 0% cooling design days."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the STAT object to a dictionary.", "response": "def to_json(self):\n        \"\"\"Convert the STAT object to a dictionary.\"\"\"\n        def jsonify_dict(base_dict):\n            new_dict = {}\n            for key, val in base_dict.items():\n                if isinstance(val, list):\n                    new_dict[key] = [v.to_json() for v in val]\n                else:\n                    new_dict[key] = val.to_json()\n            return new_dict\n        return {\n            'location': self.location.to_json(),\n            'ashrae_climate_zone': self.ashrae_climate_zone,\n            'koppen_climate_zone': self.koppen_climate_zone,\n            'extreme_cold_week': self.extreme_cold_week.to_json()\n            if self.extreme_cold_week else None,\n            'extreme_hot_week': self.extreme_hot_week.to_json()\n            if self.extreme_cold_week else None,\n            'typical_weeks': jsonify_dict(self._typical_weeks),\n            'heating_dict': self._winter_des_day_dict,\n            'cooling_dict': self._summer_des_day_dict,\n            \"monthly_db_50\": self._monthly_db_50,\n            \"monthly_wb_50\": self._monthly_wb_50,\n            \"monthly_db_range_50\": self._monthly_db_range_50,\n            \"monthly_wb_range_50\": self._monthly_wb_range_50,\n            \"monthly_db_100\": self._monthly_db_100,\n            \"monthly_wb_100\": self._monthly_wb_100,\n            \"monthly_db_20\": self._monthly_db_20,\n            \"monthly_wb_20\": self._monthly_wb_20,\n            \"monthly_db_04\": self._monthly_db_04,\n            \"monthly_wb_04\": self._monthly_wb_04,\n            \"monthly_wind\": self._monthly_wind,\n            \"monthly_wind_dirs\": self._monthly_wind_dirs,\n            \"standard_pressure_at_elev\": self.standard_pressure_at_elev,\n            \"monthly_tau_beam\": self.monthly_tau_beam,\n            \"monthly_tau_diffuse\": self.monthly_tau_diffuse\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a data type from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Create a data type from a dictionary.\n\n        Args:\n            data: Data as a dictionary.\n                {\n                    \"name\": data type name of the data type as a string\n                    \"data_type\": the class name of the data type as a string\n                    \"base_unit\": the base unit of the data type\n                }\n        \"\"\"\n        assert 'name' in data, 'Required keyword \"name\" is missing!'\n        assert 'data_type' in data, 'Required keyword \"data_type\" is missing!'\n        if cls._type_enumeration is None:\n            cls._type_enumeration = _DataTypeEnumeration(import_modules=False)\n\n        if data['data_type'] == 'GenericType':\n            assert 'base_unit' in data, \\\n                'Keyword \"base_unit\" is missing and is required for GenericType.'\n            return cls._type_enumeration._GENERICTYPE(data['name'], data['base_unit'])\n        elif data['data_type'] in cls._type_enumeration._TYPES:\n            clss = cls._type_enumeration._TYPES[data['data_type']]\n            if data['data_type'] == data['name'].title().replace(' ', ''):\n                return clss()\n            else:\n                instance = clss()\n                instance._name = data['name']\n                return instance\n        else:\n            raise ValueError(\n                'Data Type {} could not be recognized'.format(data['data_type']))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_unit_acceptable(self, unit, raise_exception=True):\n        _is_acceptable = unit in self.units\n\n        if _is_acceptable or raise_exception is False:\n            return _is_acceptable\n        else:\n            raise ValueError(\n                '{0} is not an acceptable unit type for {1}. '\n                'Choose from the following: {2}'.format(\n                    unit, self.__class__.__name__, self.units\n                )\n            )", "response": "Checks if a certain unit is acceptable for the data type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a list of values is within physically or mathematically possible range.", "response": "def is_in_range(self, values, unit=None, raise_exception=True):\n        \"\"\"Check if a list of values is within physically/mathematically possible range.\n\n        Args:\n            values: A list of values.\n            unit: The unit of the values.  If not specified, the default metric\n                unit will be assumed.\n            raise_exception: Set to True to raise an exception if not in range.\n        \"\"\"\n        self._is_numeric(values)\n        if unit is None or unit == self.units[0]:\n            minimum = self.min\n            maximum = self.max\n        else:\n            namespace = {'self': self}\n            self.is_unit_acceptable(unit, True)\n            min_statement = \"self._{}_to_{}(self.min)\".format(\n                self._clean(self.units[0]), self._clean(unit))\n            max_statement = \"self._{}_to_{}(self.max)\".format(\n                self._clean(self.units[0]), self._clean(unit))\n            minimum = eval(min_statement, namespace)\n            maximum = eval(max_statement, namespace)\n\n        for value in values:\n            if value < minimum or value > maximum:\n                if not raise_exception:\n                    return False\n                else:\n                    raise ValueError(\n                        '{0} should be between {1} and {2}. Got {3}'.format(\n                            self.__class__.__name__, self.min, self.max, value\n                        )\n                    )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking to be sure values are numbers before doing numerical operations.", "response": "def _is_numeric(self, values):\n        \"\"\"Check to be sure values are numbers before doing numerical operations.\"\"\"\n        if len(values) > 0:\n            assert isinstance(values[0], (float, int)), \\\n                \"values must be numbers to perform math operations. Got {}\".format(\n                    type(values[0]))\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _to_unit_base(self, base_unit, values, unit, from_unit):\n        self._is_numeric(values)\n        namespace = {'self': self, 'values': values}\n        if not from_unit == base_unit:\n            self.is_unit_acceptable(from_unit, True)\n            statement = '[self._{}_to_{}(val) for val in values]'.format(\n                self._clean(from_unit), self._clean(base_unit))\n            values = eval(statement, namespace)\n            namespace['values'] = values\n        if not unit == base_unit:\n            self.is_unit_acceptable(unit, True)\n            statement = '[self._{}_to_{}(val) for val in values]'.format(\n                self._clean(base_unit), self._clean(unit))\n            values = eval(statement, namespace)\n        return values", "response": "Return values in a given unit given the input from_unit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new object from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Create a header from a dictionary.\n\n        Args:\n            data: {\n                \"data_type\": {}, //Type of data (e.g. Temperature)\n                \"unit\": string,\n                \"analysis_period\": {} // A Ladybug AnalysisPeriod\n                \"metadata\": {}, // A dictionary of metadata\n            }\n        \"\"\"\n        # assign default values\n        assert 'data_type' in data, 'Required keyword \"data_type\" is missing!'\n        keys = ('data_type', 'unit', 'analysis_period', 'metadata')\n        for key in keys:\n            if key not in data:\n                data[key] = None\n\n        data_type = DataTypeBase.from_json(data['data_type'])\n        ap = AnalysisPeriod.from_json(data['analysis_period'])\n        return cls(data_type, data['unit'], ap, data['metadata'])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef duplicate(self):\n        a_per = self.analysis_period.duplicate() if self.analysis_period else None\n        return self.__class__(self.data_type, self.unit,\n                              a_per, deepcopy(self.metadata))", "response": "Return a copy of the header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of values for the Ladybug header as a list.", "response": "def to_tuple(self):\n        \"\"\"Return Ladybug header as a list.\"\"\"\n        return (\n            self.data_type,\n            self.unit,\n            self.analysis_period,\n            self.metadata\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(self):\n        a_per = self.analysis_period.to_json() if self.analysis_period else None\n        return {'data_type': self.data_type.to_json(),\n                'unit': self.unit,\n                'analysis_period': a_per,\n                'metadata': self.metadata}", "response": "Return a header as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ashrae_clear_sky(altitudes, month, sky_clearness=1):\n    # apparent solar irradiation at air mass m = 0\n    MONTHLY_A = [1202, 1187, 1164, 1130, 1106, 1092, 1093, 1107, 1136,\n                 1166, 1190, 1204]\n    # atmospheric extinction coefficient\n    MONTHLY_B = [0.141, 0.142, 0.149, 0.164, 0.177, 0.185, 0.186, 0.182,\n                 0.165, 0.152, 0.144, 0.141]\n\n    dir_norm_rad = []\n    dif_horiz_rad = []\n    for i, alt in enumerate(altitudes):\n        if alt > 0:\n            try:\n                dir_norm = MONTHLY_A[month - 1] / (math.exp(\n                    MONTHLY_B[month - 1] / (math.sin(math.radians(alt)))))\n                diff_horiz = 0.17 * dir_norm * math.sin(math.radians(alt))\n                dir_norm_rad.append(dir_norm * sky_clearness)\n                dif_horiz_rad.append(diff_horiz * sky_clearness)\n            except OverflowError:\n                # very small altitude values\n                dir_norm_rad.append(0)\n                dif_horiz_rad.append(0)\n        else:\n            # night time\n            dir_norm_rad.append(0)\n            dif_horiz_rad.append(0)\n\n    return dir_norm_rad, dif_horiz_rad", "response": "Calculates the solar flux of an original ASHRAE Clear Sky in the given list of solar altitudes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ashrae_revised_clear_sky(altitudes, tb, td, use_2017_model=False):\n    dir_norm_rad = []\n    dif_horiz_rad = []\n\n    if use_2017_model is True:\n        ab = 1.454 - (0.406 * tb) - (0.268 * td) - (0.021 * tb * td)\n        ad = 0.507 + (0.205 * tb) - (0.080 * td) - (0.190 * tb * td)\n    else:\n        ab = 1.219 - (0.043 * tb) - (0.151 * td) - (0.204 * tb * td)\n        ad = 0.202 + (0.852 * tb) - (0.007 * td) - (0.357 * tb * td)\n\n    # compute hourly radiation\n    for alt in altitudes:\n        if alt > 0:\n            # calculate hourly air mass between top of the atmosphere and earth\n            air_mass = get_relative_airmass(alt)\n            dir_norm_rad.append(1415 * math.exp(-tb * math.pow(air_mass, ab)))\n            dif_horiz_rad.append(1415 * math.exp(-td * math.pow(air_mass, ad)))\n        else:\n            dir_norm_rad.append(0)\n            dif_horiz_rad.append(0)\n\n    return dir_norm_rad, dif_horiz_rad", "response": "Calculates solar flux for an ASHRAE Revised Clear Sky."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate global horizontal solar irradiance using the Zhang - Huang model.", "response": "def zhang_huang_solar(alt, cloud_cover, relative_humidity,\n                      dry_bulb_present, dry_bulb_t3_hrs, wind_speed,\n                      irr_0=1355):\n    \"\"\"Calculate global horizontal solar irradiance using the Zhang-Huang model.\n\n    Note:\n        [1] Zhang, Q.Y. and Huang, Y.J. 2002. \"Development of typical year weather files\n        for Chinese locations\", LBNL-51436, ASHRAE Transactions, Vol. 108, Part 2.\n\n    Args:\n        alt: A solar altitude in degrees.\n        cloud_cover: A float value between 0 and 10 that represents the sky cloud cover\n            in tenths (0 = clear; 10 = completely overcast)\n        relative_humidity: A float value between 0 and 100 that represents\n            the relative humidity in percent.\n        dry_bulb_present: A float value that represents the dry bulb\n            temperature at the time of interest (in degrees C).\n        dry_bulb_t3_hrs: A float value that represents the dry bulb\n            temperature at three hours before the time of interest (in degrees C).\n        wind_speed: A float value that represents the wind speed in m/s.\n        irr_0 = Optional extraterrestrial solar constant (W/m2).\n            Default is to use the average value over the earth's orbit (1355).\n\n    Returns:\n        glob_ir: A global horizontall radiation value in W/m2.\n    \"\"\"\n    # zhang-huang solar model regression constants\n    C0, C1, C2, C3, C4, C5, D_COEFF, K_COEFF = 0.5598, 0.4982, \\\n        -0.6762, 0.02842, -0.00317, 0.014, -17.853, 0.843\n\n    # start assuming night time\n    glob_ir = 0\n\n    if alt > 0:\n        # get sin of the altitude\n        sin_alt = math.sin(math.radians(alt))\n\n        # shortened and converted versions of the input parameters\n        cc, rh, n_temp, n3_temp, w_spd = cloud_cover / 10.0, \\\n            relative_humidity, dry_bulb_present, dry_bulb_t3_hrs, wind_speed\n\n        # calculate zhang-huang global radiation\n        glob_ir = ((irr_0 * sin_alt *\n                    (C0 + (C1 * cc) + (C2 * cc**2) +\n                     (C3 * (n_temp - n3_temp)) +\n                     (C4 * rh) + (C5 * w_spd))) + D_COEFF) / K_COEFF\n        if glob_ir < 0:\n            glob_ir = 0\n\n    return glob_ir"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates direct and diffuse solar irradiance using the Zhang-Huang model. By default, this function uses the DIRINT method (aka. Perez split) to split global irradiance into direct and diffuse. This is the same method used by EnergyPlus. Args: altitudes: A list of solar altitudes in degrees. doys: A list of days of the year that correspond to the altitudes. cloud_cover: A list of float values between 0 and 10 that represents cloud cover in tenths (0 = clear; 10 = completely overcast) relative_humidity: A list of float values between 0 and 100 that represents the relative humidity in percent. dry_bulb_present: A list of float values that represents the dry bulb temperature at the time of interest (in degrees C). dry_bulb_t3_hrs: A list of float values that represents the dry bulb temperature at three hours before the time of interest (in degrees C). wind_speed: A list of float values that represents the wind speed in m/s. atm_pressure: A list of float values that represent the atmospheric pressure in Pa. use_disc: Set to True to use the original DISC model as opposed to the newer and more accurate DIRINT model. Default is False. Returns: dir_norm_rad: A list of direct normal radiation values for each of the connected altitudes in W/m2. dif_horiz_rad: A list of diffuse horizontall radiation values for each of the connected altitudes in W/m2.", "response": "def zhang_huang_solar_split(altitudes, doys, cloud_cover, relative_humidity,\n                            dry_bulb_present, dry_bulb_t3_hrs, wind_speed,\n                            atm_pressure, use_disc=False):\n    \"\"\"Calculate direct and diffuse solar irradiance using the Zhang-Huang model.\n\n    By default, this function uses the DIRINT method (aka. Perez split) to split global\n    irradiance into direct and diffuse.  This is the same method used by EnergyPlus.\n\n    Args:\n        altitudes: A list of solar altitudes in degrees.\n        doys: A list of days of the year that correspond to the altitudes.\n        cloud_cover: A list of float values between 0 and 10 that represents cloud cover\n            in tenths (0 = clear; 10 = completely overcast)\n        relative_humidity: A list of float values between 0 and 100 that represents\n            the relative humidity in percent.\n        dry_bulb_present: A list of float values that represents the dry bulb\n            temperature at the time of interest (in degrees C).\n        dry_bulb_t3_hrs: A list of float values that represents the dry bulb\n            temperature at three hours before the time of interest (in degrees C).\n        wind_speed: A list of float values that represents the wind speed in m/s.\n        atm_pressure: A list of float values that represent the\n            atmospheric pressure in Pa.\n        use_disc: Set to True to use the original DISC model as opposed to the\n            newer and more accurate DIRINT model. Default is False.\n\n    Returns:\n        dir_norm_rad: A list of direct normal radiation values for each\n            of the connected altitudes in W/m2.\n        dif_horiz_rad: A list of diffuse horizontall radiation values for each\n            of the connected altitudes in W/m2.\n    \"\"\"\n    # Calculate global horizontal irradiance using the original zhang-huang model\n    glob_ir = []\n    for i in range(len(altitudes)):\n        ghi = zhang_huang_solar(altitudes[i], cloud_cover[i], relative_humidity[i],\n                                dry_bulb_present[i], dry_bulb_t3_hrs[i], wind_speed[i])\n        glob_ir.append(ghi)\n\n    if use_disc is False:\n        # Calculate dew point temperature to improve the splitting of direct + diffuse\n        temp_dew = [dew_point_from_db_rh(dry_bulb_present[i], relative_humidity[i])\n                    for i in range(len(glob_ir))]\n\n        # Split global rad into direct + diffuse using dirint method (aka. Perez split)\n        dir_norm_rad = dirint(glob_ir, altitudes, doys, atm_pressure,\n                              use_delta_kt_prime=True, temp_dew=temp_dew)\n\n        # Calculate diffuse horizontal from dni and ghi.\n        dif_horiz_rad = [glob_ir[i] -\n                         (dir_norm_rad[i] * math.sin(math.radians(altitudes[i])))\n                         for i in range(len(glob_ir))]\n    else:\n        dir_norm_rad = []\n        dif_horiz_rad = []\n        for i in range(len(glob_ir)):\n            dni, kt, am = disc(glob_ir[i], altitudes[i], doys[i], atm_pressure[i])\n            dhi = glob_ir[i] - (dni * math.sin(math.radians(altitudes[i])))\n            dir_norm_rad.append(dni)\n            dif_horiz_rad.append(dhi)\n\n    return dir_norm_rad, dif_horiz_rad"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_horizontal_infrared(sky_cover, dry_bulb, dew_point):\n    # stefan-boltzmann constant\n    SIGMA = 5.6697e-8\n\n    # convert to kelvin\n    db_k = dry_bulb + 273.15\n    dp_k = dew_point + 273.15\n\n    # calculate sky emissivity and horizontal ir\n    sky_emiss = (0.787 + (0.764 * math.log(dp_k / 273.15))) * \\\n        (1 + (0.022 * sky_cover) - (0.0035 * (sky_cover ** 2)) +\n         (0.00028 * (sky_cover ** 3)))\n    horiz_ir = sky_emiss * SIGMA * (db_k ** 4)\n    return horiz_ir", "response": "Calculates the horizontal infrared radiation intensity for a given sky cover in tenths of dry bulb temperature and dew point temperature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the DNI from a given GHI altitude and doys and pressure.", "response": "def dirint(ghi, altitudes, doys, pressures, use_delta_kt_prime=True,\n           temp_dew=None, min_sin_altitude=0.065, min_altitude=3):\n    \"\"\"\n    Determine DNI from GHI using the DIRINT modification of the DISC\n    model.\n\n    Implements the modified DISC model known as \"DIRINT\" introduced in\n    [1]. DIRINT predicts direct normal irradiance (DNI) from measured\n    global horizontal irradiance (GHI). DIRINT improves upon the DISC\n    model by using time-series GHI data and dew point temperature\n    information. The effectiveness of the DIRINT model improves with\n    each piece of information provided.\n\n    The pvlib implementation limits the clearness index to 1.\n    The DIRINT model requires time series data.\n\n    Note:\n        [1] Perez, R., P. Ineichen, E. Maxwell, R. Seals and A. Zelenka,\n        (1992). \"Dynamic Global-to-Direct Irradiance Conversion Models\".\n        ASHRAE Transactions-Research Series, pp. 354-369\n\n        [2] Maxwell, E. L., \"A Quasi-Physical Model for Converting Hourly\n        Global Horizontal to Direct Normal Insolation\", Technical Report No.\n        SERI/TR-215-3087, Golden, CO: Solar Energy Research Institute, 1987.\n\n    Args:\n        ghi: array-like\n            Global horizontal irradiance in W/m^2.\n        altitudes: array-like\n            True (not refraction-corrected) solar altitude angles in decimal\n            degrees.\n        doys: array-like\n            Integers representing the day of the year.\n        pressures: array-like\n            The site pressure in Pascal. Pressure may be measured or an\n            average pressure may be calculated from site altitude.\n        use_delta_kt_prime: bool, default True\n            If True, indicates that the stability index delta_kt_prime is\n            included in the model. The stability index adjusts the estimated\n            DNI in response to dynamics in the time series of GHI. It is\n            recommended that delta_kt_prime is not used if the time between\n            GHI points is 1.5 hours or greater. If use_delta_kt_prime=True,\n            input data must be Series.\n        temp_dew: None or array-like, default None\n            Surface dew point temperatures, in degrees C. Values of temp_dew\n            must be numeric. If temp_dew is not provided, then dew point\n            improvements are not applied.\n        min_sin_altitude: numeric, default 0.065\n            Minimum value of sin(altitude) to allow when calculating global\n            clearness index `kt`. Equivalent to altitude = 3.727 degrees.\n        min_altitude: numeric, default 87\n            Minimum value of altitude to allow in DNI calculation. DNI will be\n            set to 0 for times with altitude values smaller than `min_altitude`.\n\n    Returns:\n        dni: array-like\n            The modeled direct normal irradiance in W/m^2 provided by the\n            DIRINT model.\n    \"\"\"\n    # calculate kt_prime values\n    kt_primes = []\n    disc_dni = []\n    for i in range(len(ghi)):\n        dni, kt, airmass = disc(ghi[i], altitudes[i], doys[i], pressure=pressures[i],\n                                min_sin_altitude=min_sin_altitude,\n                                min_altitude=min_altitude)\n        kt_prime = clearness_index_zenith_independent(\n            kt, airmass, max_clearness_index=1)\n        kt_primes.append(kt_prime)\n        disc_dni.append(dni)\n\n    # calculate delta_kt_prime values\n    if use_delta_kt_prime is True:\n        delta_kt_prime = []\n        for i in range(len(kt_primes)):\n            try:\n                kt_prime_1 = kt_primes[i + 1]\n            except IndexError:\n                # last hour\n                kt_prime_1 = kt_primes[0]\n            delta_kt_prime.append(0.5 * (abs(kt_primes[i] - kt_prime_1) +\n                                         abs(kt_primes[i] - kt_primes[i - 1])))\n    else:\n        delta_kt_prime = [-1] * len(ghi)\n\n    # calculate W values if dew point temperatures have been provided\n    if temp_dew is not None:\n        w = [math.exp(0.07 * td - 0.075) for td in temp_dew]\n    else:\n        w = [-1] * len(ghi)\n\n    # bin the values into appropriate categoreis for lookup in the coefficient matirx.\n    ktp_bin, alt_bin, w_bin, delta_ktp_bin = \\\n        _dirint_bins(kt_primes, altitudes, w, delta_kt_prime)\n\n    # get the dirint coefficient by looking up values in the matrix\n    coeffs = _get_dirint_coeffs()\n    dirint_coeffs = [coeffs[ktp_bin[i]][alt_bin[i]][delta_ktp_bin[i]][w_bin[i]]\n                     for i in range(len(ghi))]\n\n    # Perez eqn 5\n    dni = [disc_d * coef for disc_d, coef in zip(disc_dni, dirint_coeffs)]\n\n    return dni"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining the bin indices for the DIRINT coefficients.", "response": "def _dirint_bins(ktp, alt, w, dktp):\n    \"\"\"\n    Determine the bins for the DIRINT coefficients.\n\n    Args:\n        ktp : Altitude-independent clearness index\n        alt : Solar altitude angle\n        w : precipitable water estimated from surface dew-point temperature\n        dktp : stability index\n\n    Returns:\n        tuple of ktp_bin, alt_bin, w_bin, dktp_bin\n    \"\"\"\n    it = range(len(ktp))\n\n    # Create kt_prime bins\n    ktp_bin = [-1] * len(ktp)\n    ktp_bin = [0 if ktp[i] >= 0 and ktp[i] < 0.24 else ktp_bin[i] for i in it]\n    ktp_bin = [1 if ktp[i] >= 0.24 and ktp[i] < 0.4 else ktp_bin[i] for i in it]\n    ktp_bin = [2 if ktp[i] >= 0.4 and ktp[i] < 0.56 else ktp_bin[i] for i in it]\n    ktp_bin = [3 if ktp[i] >= 0.56 and ktp[i] < 0.7 else ktp_bin[i] for i in it]\n    ktp_bin = [4 if ktp[i] >= 0.7 and ktp[i] < 0.8 else ktp_bin[i] for i in it]\n    ktp_bin = [5 if ktp[i] >= 0.8 and ktp[i] <= 1 else ktp_bin[i] for i in it]\n\n    # Create altitude angle bins\n    alt_bin = [-1] * len(alt)\n    alt_bin = [0 if alt[i] <= 90 and alt[i] > 65 else alt_bin[i] for i in it]\n    alt_bin = [1 if alt[i] <= 65 and alt[i] > 50 else alt_bin[i] for i in it]\n    alt_bin = [2 if alt[i] <= 50 and alt[i] > 35 else alt_bin[i] for i in it]\n    alt_bin = [3 if alt[i] <= 35 and alt[i] > 20 else alt_bin[i] for i in it]\n    alt_bin = [4 if alt[i] <= 20 and alt[i] > 10 else alt_bin[i] for i in it]\n    alt_bin = [5 if alt[i] <= 10 else alt_bin[i] for i in it]\n\n    # Create the bins for w based on dew point temperature\n    w_bin = [-1] * len(w)\n    w_bin = [0 if w[i] >= 0 and w[i] < 1 else w_bin[i] for i in it]\n    w_bin = [1 if w[i] >= 1 and w[i] < 2 else w_bin[i] for i in it]\n    w_bin = [2 if w[i] >= 2 and w[i] < 3 else w_bin[i] for i in it]\n    w_bin = [3 if w[i] >= 3 else w_bin[i] for i in it]\n    w_bin = [4 if w[i] == -1 else w_bin[i] for i in it]\n\n    # Create delta_kt_prime binning.\n    dktp_bin = [-1] * len(dktp)\n    dktp_bin = [0 if dktp[i] >= 0 and dktp[i] < 0.015 else dktp_bin[i] for i in it]\n    dktp_bin = [1 if dktp[i] >= 0.015 and dktp[i] < 0.035 else dktp_bin[i] for i in it]\n    dktp_bin = [2 if dktp[i] >= 0.035 and dktp[i] < 0.07 else dktp_bin[i] for i in it]\n    dktp_bin = [3 if dktp[i] >= 0.07 and dktp[i] < 0.15 else dktp_bin[i] for i in it]\n    dktp_bin = [4 if dktp[i] >= 0.15 and dktp[i] < 0.3 else dktp_bin[i] for i in it]\n    dktp_bin = [5 if dktp[i] >= 0.3 and dktp[i] <= 1 else dktp_bin[i] for i in it]\n    dktp_bin = [6 if dktp[i] == -1 else dktp_bin[i] for i in it]\n\n    return ktp_bin, alt_bin, w_bin, dktp_bin"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disc(ghi, altitude, doy, pressure=101325,\n         min_sin_altitude=0.065, min_altitude=3, max_airmass=12):\n    \"\"\"\n    Estimate Direct Normal Irradiance from Global Horizontal Irradiance\n    using the DISC model.\n\n    The DISC algorithm converts global horizontal irradiance to direct\n    normal irradiance through empirical relationships between the global\n    and direct clearness indices.\n\n    This implementation limits the clearness index to 1 by default.\n\n    The original report describing the DISC model [1]_ uses the\n    relative airmass rather than the absolute (pressure-corrected)\n    airmass. However, the NREL implementation of the DISC model [2]_\n    uses absolute airmass. PVLib Matlab also uses the absolute airmass.\n    pvlib python defaults to absolute airmass, but the relative airmass\n    can be used by supplying `pressure=None`.\n\n    Note:\n        [1] Maxwell, E. L., \"A Quasi-Physical Model for Converting Hourly\n        Global Horizontal to Direct Normal Insolation\", Technical\n        Report No. SERI/TR-215-3087, Golden, CO: Solar Energy Research\n        Institute, 1987.\n\n        [2] Maxwell, E. \"DISC Model\", Excel Worksheet.\n        https://www.nrel.gov/grid/solar-resource/disc.html\n\n    Args:\n        ghi : numeric\n            Global horizontal irradiance in W/m^2.\n        altitude : numeric\n            True (not refraction-corrected) solar altitude angles in decimal\n            degrees.\n        doy : An integer representing the day of the year.\n        pressure : None or numeric, default 101325\n            Site pressure in Pascal. If None, relative airmass is used\n            instead of absolute (pressure-corrected) airmass.\n        min_sin_altitude : numeric, default 0.065\n            Minimum value of sin(altitude) to allow when calculating global\n            clearness index `kt`. Equivalent to altitude = 3.727 degrees.\n        min_altitude : numeric, default 87\n            Minimum value of altitude to allow in DNI calculation. DNI will be\n            set to 0 for times with altitude values smaller than `min_altitude`.\n        max_airmass : numeric, default 12\n            Maximum value of the airmass to allow in Kn calculation.\n            Default value (12) comes from range over which Kn was fit\n            to airmass in the original paper.\n\n    Returns:\n        dni: The modeled direct normal irradiance\n            in W/m^2 provided by the\n            Direct Insolation Simulation Code (DISC) model.\n        kt: Ratio of global to extraterrestrial\n            irradiance on a horizontal plane.\n        am: Airmass\n    \"\"\"\n    if altitude > min_altitude and ghi > 0:\n        # this is the I0 calculation from the reference\n        # SSC uses solar constant = 1367.0 (checked 2018 08 15)\n        I0 = get_extra_radiation(doy, 1370.)\n\n        kt = clearness_index(ghi, altitude, I0, min_sin_altitude=min_sin_altitude,\n                             max_clearness_index=1)\n\n        am = get_relative_airmass(altitude, model='kasten1966')\n        if pressure is not None:\n            am = get_absolute_airmass(am, pressure)\n\n        Kn, am = _disc_kn(kt, am, max_airmass=max_airmass)\n        dni = Kn * I0\n        dni = max(dni, 0)\n\n        return dni, kt, am\n    else:\n        return 0, 0, None", "response": "This function converts a solar altitude angle in solar altitude degrees to direct normal irradiance using the DISC model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating Kn for discourse of a single airmass.", "response": "def _disc_kn(clearness_index, airmass, max_airmass=12):\n    \"\"\"\n    Calculate Kn for `disc`\n\n    Args:\n        clearness_index : numeric\n        airmass : numeric\n        max_airmass : float\n            airmass > max_airmass is set to max_airmass before being used\n            in calculating Kn.\n\n    Returns:\n        Kn : numeric\n        am : numeric\n            airmass used in the calculation of Kn. am <= max_airmass.\n    \"\"\"\n    # short names for equations\n    kt = clearness_index\n    am = airmass\n\n    am = min(am, max_airmass)  # GH 450\n\n    # powers of kt will be used repeatedly, so compute only once\n    kt2 = kt * kt  # about the same as kt ** 2\n    kt3 = kt2 * kt  # 5-10x faster than kt ** 3\n\n    if kt <= 0.6:\n        a = 0.512 - 1.56*kt + 2.286*kt2 - 2.222*kt3\n        b = 0.37 + 0.962*kt\n        c = -0.28 + 0.932*kt - 2.048*kt2\n    else:\n        a = -5.743 + 21.77*kt - 27.49*kt2 + 11.56*kt3\n        b = 41.4 - 118.5*kt + 66.05*kt2 + 31.9*kt3\n        c = -47.01 + 184.2*kt - 222.0*kt2 + 73.81*kt3\n\n    delta_kn = a + b * math.exp(c*am)\n\n    Knc = 0.866 - 0.122*am + 0.0121*am**2 - 0.000653*am**3 + 1.4e-05*am**4\n    Kn = Knc - delta_kn\n    return Kn, am"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_extra_radiation(doy, solar_constant=1366.1):\n    # Calculates the day angle for the Earth's orbit around the Sun.\n    B = (2. * math.pi / 365.) * (doy - 1)\n    # Calculate R over R squared from the angle\n    RoverR0sqrd = (1.00011 + 0.034221 * math.cos(B) + 0.00128 * math.sin(B) +\n                   0.000719 * math.cos(2 * B) + 7.7e-05 * math.sin(2 * B))\n\n    Ea = solar_constant * RoverR0sqrd\n\n    return Ea", "response": "Returns the extra radiation of the current entry in the base class for the given day of year."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the clearness index for a given global irradiance in W.", "response": "def clearness_index(ghi, altitude, extra_radiation, min_sin_altitude=0.065,\n                    max_clearness_index=2.0):\n    \"\"\"\n    Calculate the clearness index.\n\n    The clearness index is the ratio of global to extraterrestrial\n    irradiance on a horizontal plane.\n\n    Note:\n        [1] Maxwell, E. L., \"A Quasi-Physical Model for Converting Hourly\n        Global Horizontal to Direct Normal Insolation\", Technical\n        Report No. SERI/TR-215-3087, Golden, CO: Solar Energy Research\n        Institute, 1987.\n\n    Args:\n        ghi: numeric\n            Global horizontal irradiance in W/m^2.\n        altitude: numeric\n            True (not refraction-corrected) solar altitude angle in decimal\n            degrees.\n        extra_radiation: numeric\n            Irradiance incident at the top of the atmosphere\n        min_sin_altitude: numeric, default 0.065\n            Minimum value of sin(altitude) to allow when calculating global\n            clearness index `kt`. Equivalent to altitude = 3.727 degrees.\n        max_clearness_index: numeric, default 2.0\n            Maximum value of the clearness index. The default, 2.0, allows\n            for over-irradiance events typically seen in sub-hourly data.\n            NREL's SRRL Fortran code used 0.82 for hourly data.\n\n    Returns:\n        kt : numeric\n            Clearness index\n    \"\"\"\n    sin_altitude = math.sin(math.radians(altitude))\n    I0h = extra_radiation * max(sin_altitude, min_sin_altitude)\n    kt = ghi / I0h\n    kt = max(kt, 0)\n    kt = min(kt, max_clearness_index)\n    return kt"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the zenith angle independent clearness index.", "response": "def clearness_index_zenith_independent(clearness_index, airmass,\n                                       max_clearness_index=2.0):\n    \"\"\"\n    Calculate the zenith angle independent clearness index.\n\n    Note:\n        [1] Perez, R., P. Ineichen, E. Maxwell, R. Seals and A. Zelenka,\n        (1992). \"Dynamic Global-to-Direct Irradiance Conversion Models\".\n        ASHRAE Transactions-Research Series, pp. 354-369\n\n    Args:\n        clearness_index: numeric\n            Ratio of global to extraterrestrial irradiance on a horizontal\n            plane\n        airmass: numeric\n            Airmass\n        max_clearness_index: numeric, default 2.0\n            Maximum value of the clearness index. The default, 2.0, allows\n            for over-irradiance events typically seen in sub-hourly data.\n            NREL's SRRL Fortran code used 0.82 for hourly data.\n\n    Returns:\n        kt_prime : numeric\n            Zenith independent clearness index\n    \"\"\"\n    if airmass is not None:\n        # Perez eqn 1\n        kt_prime = clearness_index / (\n            1.031 * math.exp(-1.4 / (0.9 + 9.4 / airmass)) + 0.1)\n        kt_prime = max(kt_prime, 0)\n        kt_prime = min(kt_prime, max_clearness_index)\n        return kt_prime\n    else:\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the relative air mass at sea - level given a sun altitude angle.", "response": "def get_relative_airmass(altitude, model='kastenyoung1989'):\n    \"\"\"\n    Gives the relative (not pressure-corrected) airmass.\n\n    Gives the airmass at sea-level when given a sun altitude angle (in\n    degrees). The ``model`` variable allows selection of different\n    airmass models (described below). If ``model`` is not included or is\n    not valid, the default model is 'kastenyoung1989'.\n\n    Note:\n        [1] Fritz Kasten. \"A New Table and Approximation Formula for the\n        Relative Optical Air Mass\". Technical Report 136, Hanover, N.H.:\n        U.S. Army Material Command, CRREL.\n        [2] A. T. Young and W. M. Irvine, \"Multicolor Photoelectric\n        Photometry of the Brighter Planets,\" The Astronomical Journal, vol.\n        72, pp. 945-950, 1967.\n        [3] Fritz Kasten and Andrew Young. \"Revised optical air mass tables\n        and approximation formula\". Applied Optics 28:4735-4738\n        [4] C. Gueymard, \"Critical analysis and performance assessment of\n        clear sky solar irradiance models using theoretical and measured\n        data,\" Solar Energy, vol. 51, pp. 121-138, 1993.\n        [5] A. T. Young, \"AIR-MASS AND REFRACTION,\" Applied Optics, vol. 33,\n        pp. 1108-1110, Feb 1994.\n        [6] Keith A. Pickering. \"The Ancient Star Catalog\". DIO 12:1, 20,\n        [7] Matthew J. Reno, Clifford W. Hansen and Joshua S. Stein, \"Global\n        Horizontal Irradiance Clear Sky Models: Implementation and Analysis\"\n        Sandia Report, (2012).\n\n    Args:\n        altitude: numeric\n            Altitude angle of the sun in degrees. Note that some models use\n            the apparent (refraction corrected) altitude angle, and some\n            models use the true (not refraction-corrected) altitude angle. See\n            model descriptions to determine which type of altitude angle is\n            required. Apparent altitude angles must be calculated at sea level.\n        model: string, default 'kastenyoung1989'\n            Available models include the following:\n            * 'simple' - secant(apparent altitude angle) -\n              Note that this gives -inf at altitude=0\n            * 'kasten1966' - See reference [1] -\n              requires apparent sun altitude\n            * 'youngirvine1967' - See reference [2] -\n              requires true sun altitude\n            * 'kastenyoung1989' - See reference [3] -\n              requires apparent sun altitude\n            * 'gueymard1993' - See reference [4] -\n              requires apparent sun altitude\n            * 'young1994' - See reference [5] -\n              requries true sun altitude\n            * 'pickering2002' - See reference [6] -\n              requires apparent sun altitude\n\n    Returns:\n        airmass_relative: numeric\n            Relative airmass at sea level. Will return None for any\n            altitude angle smaller than 0 degrees.\n    \"\"\"\n    if altitude < 0:\n        return None\n    else:\n        alt_rad = math.radians(altitude)\n        model = model.lower()\n\n        if 'kastenyoung1989' == model:\n            am = (1.0 / (math.sin(alt_rad) +\n                  0.50572*(((6.07995 + altitude) ** - 1.6364))))\n        elif 'kasten1966' == model:\n            am = 1.0 / (math.sin(alt_rad) + 0.15*((3.885 + altitude) ** - 1.253))\n        elif 'simple' == model:\n            am = 1.0 / math.sin(altitude)\n        elif 'pickering2002' == model:\n            am = (1.0 / (math.sin(math.radians(altitude +\n                  244.0 / (165 + 47.0 * altitude ** 1.1)))))\n        elif 'youngirvine1967' == model:\n            sec_zen = 1.0 / math.sin(alt_rad)\n            am = sec_zen * (1 - 0.0012 * (sec_zen * sec_zen - 1))\n        elif 'young1994' == model:\n            am = ((1.002432*((math.sin(alt_rad)) ** 2) +\n                  0.148386*(math.sin(alt_rad)) + 0.0096467) /\n                  (math.sin(alt_rad) ** 3 +\n                  0.149864*(math.sin(alt_rad) ** 2) +\n                  0.0102963*(math.sin(alt_rad)) + 0.000303978))\n        elif 'gueymard1993' == model:\n            am = (1.0 / (math.sin(alt_rad) +\n                  0.00176759*(90 - altitude)*(\n                      (94.37515 - (90 - altitude)) ** - 1.21563)))\n        else:\n            raise ValueError('%s is not a valid model for relativeairmass', model)\n    return am"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the domain of the colors based on min and max of a list of values.", "response": "def set_domain(self, values):\n        \"\"\"Set domain of the colors based on min and max of a list of values.\"\"\"\n        _flattenedList = sorted(flatten(values))\n        self.domain = tuple(_flattenedList[0] if d == 'min' else d for d in self.domain)\n        self.domain = tuple(_flattenedList[-1] if d == 'max' else d for d in self.domain)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_colors(self, values):\n        # set domain if it is not set\n        _flattenedList = list(flatten(values))\n        if not self.is_domain_set:\n            self.set_domain(_flattenedList)\n\n        _flattenedColors = range(len(_flattenedList))\n        for count, value in enumerate(_flattenedList):\n            _flattenedColors[count] = self.calculate_color(value)\n\n        return unflatten(values, iter(_flattenedColors))", "response": "Return a list of lists of colors based on input values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Data Collection from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Create a Data Collection from a dictionary.\n\n        Args:\n            {\n                \"header\": A Ladybug Header,\n                \"values\": An array of values,\n                \"datetimes\": An array of datetimes,\n                \"validated_a_period\": Boolean for whether header analysis_period is valid\n            }\n        \"\"\"\n        assert 'header' in data, 'Required keyword \"header\" is missing!'\n        assert 'values' in data, 'Required keyword \"values\" is missing!'\n        assert 'datetimes' in data, 'Required keyword \"datetimes\" is missing!'\n        collection = cls(Header.from_json(data['header']), data['values'],\n                         [DateTime.from_json(dat) for dat in data['datetimes']])\n        if 'validated_a_period' in data:\n            collection._validated_a_period = data['validated_a_period']\n        return collection"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timestep_text(self):\n        if self.header.analysis_period.timestep == 1:\n            return 'Hourly'\n        else:\n            return '{} Minute'.format(int(60 / self.header.analysis_period.timestep))", "response": "Return a text string representing the timestep of the collection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of this collection s values where the keys are the moys.", "response": "def moys_dict(self):\n        \"\"\"Return a dictionary of this collection's values where the keys are the moys.\n\n        This is useful for aligning the values with another list of datetimes.\n        \"\"\"\n        moy_dict = {}\n        for val, dt in zip(self.values, self.datetimes):\n            moy_dict[dt.moy] = val\n        return moy_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters a Data Collection based on an analysis period.", "response": "def filter_by_analysis_period(self, analysis_period):\n        \"\"\"\n        Filter a Data Collection based on an analysis period.\n\n        Args:\n           analysis period: A Ladybug analysis period\n\n        Return:\n            A new Data Collection with filtered data\n        \"\"\"\n        self._check_analysis_period(analysis_period)\n        _filtered_data = self.filter_by_moys(analysis_period.moys)\n        _filtered_data.header._analysis_period = analysis_period\n        return _filtered_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_by_hoys(self, hoys):\n        _moys = tuple(int(hour * 60) for hour in hoys)\n        return self.filter_by_moys(_moys)", "response": "Filter the Data Collection based on an analysis period."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters the Data Collection based on a list of minutes of the year.", "response": "def filter_by_moys(self, moys):\n        \"\"\"Filter the Data Collection based on a list of minutes of the year.\n\n        Args:\n           moys: A List of minutes of the year [0..8759 * 60]\n\n        Return:\n            A new Data Collection with filtered data\n        \"\"\"\n        _filt_values, _filt_datetimes = self._filter_by_moys_slow(moys)\n        collection = HourlyDiscontinuousCollection(\n            self.header.duplicate(), _filt_values, _filt_datetimes)\n        collection._validated_a_period = self._validated_a_period\n        return collection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of this collection s values grouped by each day of year.", "response": "def group_by_day(self):\n        \"\"\"Return a dictionary of this collection's values grouped by each day of year.\n\n        Key values are between 1-365.\n        \"\"\"\n        data_by_day = OrderedDict()\n        for d in xrange(1, 366):\n            data_by_day[d] = []\n        for v, dt in zip(self._values, self.datetimes):\n            data_by_day[dt.doy].append(v)\n        return data_by_day"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of this collection s values grouped by each month.", "response": "def group_by_month(self):\n        \"\"\"Return a dictionary of this collection's values grouped by each month.\n\n        Key values are between 1-12.\n        \"\"\"\n        data_by_month = OrderedDict()\n        for d in xrange(1, 13):\n            data_by_month[d] = []\n        for v, dt in zip(self._values, self.datetimes):\n            data_by_month[dt.month].append(v)\n        return data_by_month"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of this collection s values grouped by each month per hour.", "response": "def group_by_month_per_hour(self):\n        \"\"\"Return a dictionary of this collection's values grouped by each month per hour.\n\n        Key values are tuples of 2 integers:\n        The first represents the month of the year between 1-12.\n        The first represents the hour of the day between 0-24.\n        (eg. (12, 23) for December at 11 PM)\n        \"\"\"\n        data_by_month_per_hour = OrderedDict()\n        for m in xrange(1, 13):\n            for h in xrange(0, 24):\n                data_by_month_per_hour[(m, h)] = []\n        for v, dt in zip(self.values, self.datetimes):\n            data_by_month_per_hour[(dt.month, dt.hour)].append(v)\n        return data_by_month_per_hour"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interpolate_holes(self):\n        # validate analysis_period and use the resulting period to generate datetimes\n        assert self.validated_a_period is True, 'validated_a_period property must be' \\\n            ' True to use interpolate_holes(). Run validate_analysis_period().'\n        mins_per_step = int(60 / self.header.analysis_period.timestep)\n        new_datetimes = self.header.analysis_period.datetimes\n        new_values = []\n\n        # if the first steps are a hole, duplicate the first value.\n        i = 0\n        if new_datetimes[0] != self.datetimes[0]:\n            n_steps = int((self.datetimes[0].moy - new_datetimes[0].moy) / mins_per_step)\n            new_values.extend([self._values[0]] * n_steps)\n            i = n_steps - 1\n\n        # go through the values interpolating any holes.\n        for j in xrange(len(self._values)):\n            if new_datetimes[i] == self.datetimes[j]:  # there is no hole.\n                new_values.append(self._values[j])\n                i += 1\n            else:  # there is a hole between this step and the previous step.\n                n_steps = int((self.datetimes[j].moy - new_datetimes[i].moy)\n                              / mins_per_step)\n                intp_vals = self._xxrange(self._values[j - 1], self._values[j], n_steps)\n                new_values.extend(list(intp_vals)[1:] + [self._values[j]])\n                i += n_steps\n\n        # if the last steps are a hole duplicate the last value.\n        if len(new_values) != len(new_datetimes):\n            n_steps = len(new_datetimes) - len(new_values)\n            new_values.extend([self._values[-1]] * n_steps)\n\n        # build the new continuous data collection.\n        return HourlyContinuousCollection(self.header.duplicate(), new_values)", "response": "Linearly interpolate over holes in this collection to make it continuous."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cull_to_timestep(self, timestep=1):\n        valid_s = self.header.analysis_period.VALIDTIMESTEPS.keys()\n        assert timestep in valid_s, \\\n            'timestep {} is not valid. Choose from: {}'.format(timestep, valid_s)\n\n        new_ap, new_values, new_datetimes = self._timestep_cull(timestep)\n        new_header = self.header.duplicate()\n        new_header._analysis_period = new_ap\n        new_coll = HourlyDiscontinuousCollection(\n            new_header, new_values, new_datetimes)\n        new_coll._validated_a_period = True\n        return new_coll", "response": "Get a collection with only datetimes that fit a timestep."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_to_culled_timestep(self, timestep=1):\n        valid_s = self.header.analysis_period.VALIDTIMESTEPS.keys()\n        assert timestep in valid_s, \\\n            'timestep {} is not valid. Choose from: {}'.format(timestep, valid_s)\n\n        new_ap, new_values, new_datetimes = self._timestep_cull(timestep)\n        self.header._analysis_period = new_ap\n        self._values = new_values\n        self._datetimes = new_datetimes", "response": "Convert this collection to one that only has datetimes that fit a timestep."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_analysis_period(self):\n        a_per = self.header.analysis_period\n        n_ap = [a_per.st_month, a_per.st_day, a_per.st_hour, a_per.end_month,\n                a_per.end_day, a_per.end_hour, a_per.timestep, a_per.is_leap_year]\n\n        # make sure that datetimes are all in chronological order.\n        sort_datetimes, sort_values = zip(*sorted(zip(self.datetimes, self.values)))\n        if not a_per.is_reversed and not a_per.is_annual:\n            if sort_datetimes[0].doy < a_per.st_time.doy:\n                n_ap[0] = sort_datetimes[0].month\n                n_ap[1] = sort_datetimes[0].day\n            if sort_datetimes[-1].doy > a_per.end_time.doy:\n                n_ap[3] = sort_datetimes[-1].month\n                n_ap[4] = sort_datetimes[-1].day\n        elif a_per.is_reversed:\n            last_ind = None\n            for i, date_t in enumerate(sort_datetimes):\n                last_ind = i if date_t.moy <= a_per.end_time.moy else last_ind\n            if last_ind is not None:\n                last_ind = last_ind + 1\n                sort_datetimes = sort_datetimes[last_ind:] + sort_datetimes[:last_ind]\n                sort_values = sort_values[last_ind:] + sort_values[:last_ind]\n            # If datetimes are outside the a_period range, just make it annual.\n            # There's no way to know what side of the analysis_period should be etended.\n            if sort_datetimes[0].doy > a_per.end_time.doy and \\\n                    sort_datetimes[0].doy < a_per.st_time.doy:\n                n_ap[0], n_ap[1], n_ap[3], n_ap[4] = 1, 1, 12, 31\n                sort_datetimes, sort_values = zip(*sorted(zip(\n                    self.datetimes, self.values)))\n\n        # check that no hours lie outside of the analysis_period\n        if not a_per.is_annual:\n            if a_per.st_hour != 0:\n                for date_t in sort_datetimes:\n                    n_ap[2] = date_t.hour if date_t.hour < n_ap[2] else n_ap[2]\n            if a_per.end_hour != 23:\n                for date_t in sort_datetimes:\n                    n_ap[5] = date_t.hour if date_t.hour > n_ap[5] else n_ap[5]\n\n        # check that there are no duplicate datetimes.\n        for i in xrange(len(sort_datetimes)):\n            assert sort_datetimes[i] != sort_datetimes[i - 1], 'Duplicate datetime ' \\\n                'was found in the collection: {}'.format(sort_datetimes[i])\n\n        # check that the analysis_period timestep is correct.\n        mins_per_step = int(60 / n_ap[6])\n        for date_t in sort_datetimes:\n            if date_t.moy % mins_per_step != 0:\n                i = 0\n                valid_steps = sorted(a_per.VALIDTIMESTEPS.keys())\n                while date_t.moy % mins_per_step != 0 and i < len(valid_steps):\n                    mins_per_step = int(60 / valid_steps[i])\n                    i += 1\n                n_ap[6] = int(60 / mins_per_step)\n\n        # check that the analysis_period leap_year is correct.\n        if a_per.is_leap_year is False:\n            for date_t in sort_datetimes:\n                if date_t.month == 2 and date_t.day == 29:\n                    n_ap[7] = True\n\n        # build a validated collection.\n        new_ap = AnalysisPeriod(*n_ap)\n        new_header = self.header.duplicate()\n        new_header._analysis_period = new_ap\n        new_coll = HourlyDiscontinuousCollection(new_header, sort_values, sort_datetimes)\n        new_coll._validated_a_period = True\n        return new_coll", "response": "Validate that the header analysis_period aligns with datetimes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate n values between start and end.", "response": "def _xxrange(self, start, end, step_count):\n        \"\"\"Generate n values between start and end.\"\"\"\n        _step = (end - start) / float(step_count)\n        return (start + (i * _step) for i in xrange(int(step_count)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _filter_by_moys_slow(self, moys):\n        _filt_values = []\n        _filt_datetimes = []\n        for i, d in enumerate(self.datetimes):\n            if d.moy in moys:\n                _filt_datetimes.append(d)\n                _filt_values.append(self._values[i])\n        return _filt_values, _filt_datetimes", "response": "Filter the Data Collection with a slow method that always works."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncull out values that do not fit a timestep.", "response": "def _timestep_cull(self, timestep):\n        \"\"\"Cull out values that do not fit a timestep.\"\"\"\n        new_values = []\n        new_datetimes = []\n        mins_per_step = int(60 / timestep)\n        for i, date_t in enumerate(self.datetimes):\n            if date_t.moy % mins_per_step == 0:\n                new_datetimes.append(date_t)\n                new_values.append(self.values[i])\n        a_per = self.header.analysis_period\n        new_ap = AnalysisPeriod(a_per.st_month, a_per.st_day, a_per.st_hour,\n                                a_per.end_month, a_per.end_day, a_per.end_hour,\n                                timestep, a_per.is_leap_year)\n        return new_ap, new_values, new_datetimes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _time_interval_operation(self, interval, operation, percentile=0):\n        # retrive the function that correctly describes the operation\n        if operation == 'average':\n            funct = self._average\n        elif operation == 'total':\n            funct = self._total\n        else:\n            assert 0 <= percentile <= 100, \\\n                'percentile must be between 0 and 100. Got {}'.format(percentile)\n            funct = self._get_percentile_function(percentile)\n\n        # retrive the data that correctly describes the time interval\n        if interval == 'monthly':\n            data_dict = self.group_by_month()\n            dates = self.header.analysis_period.months_int\n        elif interval == 'daily':\n            data_dict = self.group_by_day()\n            dates = self.header.analysis_period.doys_int\n        elif interval == 'monthlyperhour':\n            data_dict = self.group_by_month_per_hour()\n            dates = self.header.analysis_period.months_per_hour\n        else:\n            raise ValueError('Invalid input value for interval: {}'.format(interval))\n        # get the data and header for the new collection\n        new_data, d_times = [], []\n        for i in dates:\n            vals = data_dict[i]\n            if vals != []:\n                new_data.append(funct(vals))\n                d_times.append(i)\n        new_header = self.header.duplicate()\n        if operation == 'percentile':\n            new_header.metadata['operation'] = '{} percentile'.format(percentile)\n        else:\n            new_header.metadata['operation'] = operation\n\n        # build the final data collection\n        if interval == 'monthly':\n            collection = MonthlyCollection(new_header, new_data, d_times)\n        elif interval == 'daily':\n            collection = DailyCollection(new_header, new_data, d_times)\n        elif interval == 'monthlyperhour':\n            collection = MonthlyPerHourCollection(new_header, new_data, d_times)\n\n        collection._validated_a_period = True\n        return collection", "response": "Get a collection of a certain time interval with a given math operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_json(cls, data):\n        assert 'header' in data, 'Required keyword \"header\" is missing!'\n        assert 'values' in data, 'Required keyword \"values\" is missing!'\n        return cls(Header.from_json(data['header']), data['values'])", "response": "Create a Data Collection from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datetimes(self):\n        if self._datetimes is None:\n            self._datetimes = tuple(self.header.analysis_period.datetimes)\n        return self._datetimes", "response": "Return the datetimes for this collection as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef interpolate_to_timestep(self, timestep, cumulative=None):\n        assert timestep % self.header.analysis_period.timestep == 0, \\\n            'Target timestep({}) must be divisable by current timestep({})' \\\n            .format(timestep, self.header.analysis_period.timestep)\n        if cumulative is not None:\n            assert isinstance(cumulative, bool), \\\n                'Expected Boolean. Got {}'.format(type(cumulative))\n\n        # generate new data\n        _new_values = []\n        _data_length = len(self._values)\n        for d in xrange(_data_length):\n            for _v in self._xxrange(self[d], self[(d + 1) % _data_length], timestep):\n                _new_values.append(_v)\n\n        # divide cumulative values by the timestep\n        native_cumulative = self.header.data_type.cumulative\n        if cumulative is True or (cumulative is None and native_cumulative):\n            for i, d in enumerate(_new_values):\n                _new_values[i] = d / timestep\n\n        # shift data by a half-hour if data is averaged or cumulative over an hour\n        if self.header.data_type.point_in_time is False:\n            shift_dist = int(timestep / 2)\n            _new_values = _new_values[-shift_dist:] + _new_values[:-shift_dist]\n\n        # build a new header\n        a_per = self.header.analysis_period\n        _new_a_per = AnalysisPeriod(a_per.st_month, a_per.st_day, a_per.st_hour,\n                                    a_per.end_month, a_per.end_day, a_per.end_hour,\n                                    timestep, a_per.is_leap_year)\n        _new_header = self.header.duplicate()\n        _new_header._analysis_period = _new_a_per\n        return HourlyContinuousCollection(_new_header, _new_values)", "response": "Interpolate data for a given timestep."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters the Data Collection based on a conditional statement.", "response": "def filter_by_conditional_statement(self, statement):\n        \"\"\"Filter the Data Collection based on a conditional statement.\n\n        Args:\n            statement: A conditional statement as a string (e.g. a > 25 and a%5 == 0).\n                The variable should always be named as 'a' (without quotations).\n\n        Return:\n            A new Data Collection containing only the filtered data\n        \"\"\"\n        _filt_values, _filt_datetimes = self._filter_by_statement(statement)\n        collection = HourlyDiscontinuousCollection(\n            self.header.duplicate(), _filt_values, _filt_datetimes)\n        collection._validated_a_period = True\n        return collection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_by_pattern(self, pattern):\n        _filt_values, _filt_datetimes = self._filter_by_pattern(pattern)\n        collection = HourlyDiscontinuousCollection(\n            self.header.duplicate(), _filt_values, _filt_datetimes)\n        collection._validated_a_period = True\n        return collection", "response": "Filter the Data Collection based on a list of booleans."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering the data collection based on an analysis period.", "response": "def filter_by_analysis_period(self, analysis_period):\n        \"\"\"Filter the Data Collection based on an analysis period.\n\n        Args:\n           analysis period: A Ladybug analysis period\n\n        Return:\n            A new Data Collection with filtered data\n        \"\"\"\n        self._check_analysis_period(analysis_period)\n        analysis_period = self._get_analysis_period_subset(analysis_period)\n\n        if analysis_period.st_hour == 0 and analysis_period.end_hour == 23:\n            # We can still return an Hourly Continuous Data Collection\n            t_s = 60 / analysis_period.timestep\n            st_ind = int((analysis_period.st_time.moy / t_s) -\n                         (self.header.analysis_period.st_time.moy / t_s))\n            end_ind = int((analysis_period.end_time.moy / t_s) -\n                          (analysis_period.st_time.moy / t_s) + st_ind + 1)\n            if end_ind > st_ind:\n                _filt_values = self._values[st_ind:end_ind]\n            else:\n                _filt_values = self._values[st_ind:] + self._values[:end_ind]\n            _filt_header = self.header.duplicate()\n            _filt_header._analysis_period = analysis_period\n            return HourlyContinuousCollection(_filt_header, _filt_values)\n        else:\n            # Filter using  HOYs and the result cannot be continuous\n            _filtered_data = self.filter_by_moys(analysis_period.moys)\n            _filtered_data.header._analysis_period = analysis_period\n            return _filtered_data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_by_hoys(self, hoys):\n        existing_hoys = self.header.analysis_period.hoys\n        hoys = [h for h in hoys if h in existing_hoys]\n        _moys = tuple(int(hour * 60) for hour in hoys)\n        return self.filter_by_moys(_moys)", "response": "Filter the Data Collection based on a list of hours of the year 0.. 8759."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_by_moys(self, moys):\n        t_s = 60 / self.header.analysis_period.timestep\n        st_ind = self.header.analysis_period.st_time.moy / t_s\n        if self.header.analysis_period.is_reversed is False:\n            _filt_indices = [int(moy / t_s - st_ind) for moy in moys]\n        else:\n            if self.header.analysis_period.is_leap_year is False:\n                eoy_ind = 8759 * self.header.analysis_period.timestep - st_ind\n            else:\n                eoy_ind = 8783 * self.header.analysis_period.timestep - st_ind\n            _filt_indices = []\n            for moy in moys:\n                ind = moy / t_s\n                if ind > st_ind:\n                    _filt_indices.append(int(ind - st_ind))\n                else:\n                    _filt_indices.append(int(ind + eoy_ind))\n\n        _filt_values = [self._values[i] for i in _filt_indices]\n        _filt_datetimes = [self.datetimes[i] for i in _filt_indices]\n        _filt_header = self.header.duplicate()\n        coll = HourlyDiscontinuousCollection(_filt_header, _filt_values, _filt_datetimes)\n        coll._validated_a_period = True\n        return coll", "response": "Filter the Data Collection based on a list of minutes of the year."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of this collection s values grouped by each day of year.", "response": "def group_by_day(self):\n        \"\"\"Return a dictionary of this collection's values grouped by each day of year.\n\n        Key values are between 1-365.\n        \"\"\"\n        hourly_data_by_day = OrderedDict()\n        for d in xrange(1, 366):\n            hourly_data_by_day[d] = []\n        a_per = self.header.analysis_period\n        indx_per_day = 24 * a_per.timestep\n        start_doy = sum(a_per._num_of_days_each_month[:a_per.st_time.month-1]) \\\n            + a_per.st_time.day\n        if not a_per.is_reversed:\n            for i in range(0, len(self._values), indx_per_day):\n                hourly_data_by_day[start_doy] = self._values[i:i + indx_per_day]\n                start_doy += 1\n        else:\n            end_ind = 24 * a_per.timestep * (365 - start_doy)\n            for i in range(0, end_ind + 1, indx_per_day):\n                hourly_data_by_day[start_doy] = self._values[i:i + indx_per_day]\n                start_doy += 1\n            start_doy = 1\n            for i in range(end_ind, len(self._values), indx_per_day):\n                hourly_data_by_day[start_doy] = self._values[i:i + indx_per_day]\n                start_doy += 1\n        return hourly_data_by_day"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_by_month(self):\n        hourly_data_by_month = OrderedDict()\n        for d in xrange(1, 13):\n            hourly_data_by_month[d] = []\n\n        a_per = self.header.analysis_period\n        a_per_months = a_per.months_int\n        indx = 24 * a_per.timestep * abs(\n            a_per.st_day - 1 - a_per._num_of_days_each_month[a_per_months[0]-1])\n        hourly_data_by_month[a_per_months[0]] = self._values[0:indx + 1]\n\n        if len(a_per_months) > 1:\n            for mon in a_per_months[1:]:\n                interval = a_per._num_of_days_each_month[mon - 1] * 24 * a_per.timestep\n                try:\n                    hourly_data_by_month[mon] = self._values[indx:indx + interval + 1]\n                except IndexError:\n                    hourly_data_by_month[mon] = self._values[indx:]  # last items\n                indx += interval\n        return hourly_data_by_month", "response": "Return a dictionary of this collection s values grouped by each month."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an immutable version of this collection.", "response": "def to_immutable(self):\n        \"\"\"Get an immutable version of this collection.\"\"\"\n        if self._enumeration is None:\n            self._get_mutable_enumeration()\n        col_obj = self._enumeration['immutable'][self._collection_type]\n        return col_obj(self.header, self.values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_collection_aligned(self, data_collection):\n        if self._collection_type != data_collection._collection_type:\n            return False\n        elif len(self.values) != len(data_collection.values):\n            return False\n        elif self.header.analysis_period != data_collection.header.analysis_period:\n                return False\n        return True", "response": "Checks if this Data Collection is aligned with another."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a discontinuous version of the current collection.", "response": "def to_discontinuous(self):\n        \"\"\"Return a discontinuous version of the current collection.\"\"\"\n        collection = HourlyDiscontinuousCollection(self.header.duplicate(),\n                                                   self.values, self.datetimes)\n        collection._validated_a_period = True\n        return collection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an analysis_period always a subset of the Data Collection", "response": "def _get_analysis_period_subset(self, a_per):\n        \"\"\"Return an analysis_period is always a subset of the Data Collection\"\"\"\n        if self.header.analysis_period.is_annual:\n            return a_per\n\n        new_needed = False\n        n_ap = [a_per.st_month, a_per.st_day, a_per.st_hour,\n                a_per.end_month, a_per.end_day, a_per.end_hour,\n                a_per.timestep, a_per.is_leap_year]\n        if a_per.st_hour < self.header.analysis_period.st_hour:\n            n_ap[2] = self.header.analysis_period.st_hour\n            new_needed = True\n        if a_per.end_hour > self.header.analysis_period.end_hour:\n            n_ap[5] = self.header.analysis_period.end_hour\n            new_needed = True\n        if a_per.st_time.doy < self.header.analysis_period.st_time.doy:\n            n_ap[0] = self.header.analysis_period.st_month\n            n_ap[1] = self.header.analysis_period.st_day\n            new_needed = True\n        if a_per.end_time.doy > self.header.analysis_period.end_time.doy:\n            n_ap[3] = self.header.analysis_period.end_month\n            n_ap[4] = self.header.analysis_period.end_day\n            new_needed = True\n        if new_needed is False:\n            return a_per\n        else:\n            return AnalysisPeriod(*n_ap)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_values(self, values):\n        assert isinstance(values, Iterable) and not isinstance(\n            values, (str, dict, bytes, bytearray)), \\\n            'values should be a list or tuple. Got {}'.format(type(values))\n        if self.header.analysis_period.is_annual:\n            a_period_len = 8760 * self.header.analysis_period.timestep\n            if self.header.analysis_period.is_leap_year is True:\n                a_period_len = a_period_len + 24 * self.header.analysis_period.timestep\n        else:\n            a_period_len = len(self.header.analysis_period.moys)\n        assert len(values) == a_period_len, \\\n            'Length of values does not match that expected by the '\\\n            'header analysis_period. {} != {}'.format(len(values), a_period_len)", "response": "Check values whenever they come through the values setter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering the data collection based on an analysis period.", "response": "def filter_by_analysis_period(self, analysis_period):\n        \"\"\"Filter the Data Collection based on an analysis period.\n\n        Args:\n           analysis period: A Ladybug analysis period\n\n        Return:\n            A new Data Collection with filtered data\n        \"\"\"\n        self._check_analysis_period(analysis_period)\n        _filtered_data = self.filter_by_doys(analysis_period.doys_int)\n        _filtered_data.header._analysis_period = analysis_period\n        return _filtered_data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_by_doys(self, doys):\n        _filt_values = []\n        _filt_datetimes = []\n        for i, d in enumerate(self.datetimes):\n            if d in doys:\n                _filt_datetimes.append(d)\n                _filt_values.append(self._values[i])\n        _filt_header = self.header.duplicate()\n        return DailyCollection(_filt_header, _filt_values, _filt_datetimes)", "response": "Filter the Data Collection based on a list of days of the year."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the analysis_period of the data collection.", "response": "def validate_analysis_period(self):\n        \"\"\"Get a collection where the header analysis_period aligns with datetimes.\n\n        This means that checks for four criteria will be performed:\n        1) All days in the data collection are chronological starting from the\n            analysis_period start day to the end day.\n        2) No duplicate days exist in the data collection.\n        3) There are no days that lie outside of the analysis_period time range.\n        4) February 29th is excluded if is_leap_year is False on the analysis_period.\n\n        Note that there is no need to run this check any time that a discontinous\n        data collection has been derived from a continuous one or when the\n        validated_a_period attribute of the collection is True.\n        \"\"\"\n        a_per = self.header.analysis_period\n        n_ap = [a_per.st_month, a_per.st_day, a_per.st_hour, a_per.end_month,\n                a_per.end_day, a_per.end_hour, a_per.timestep, a_per.is_leap_year]\n\n        # check that the analysis_period leap_year is correct.\n        if a_per.is_leap_year is False:\n            for date_t in self.datetimes:\n                if date_t == 366:\n                    n_ap[7] = True\n\n        # make sure that datetimes are all in chronological order.\n        sort_datetimes, sort_values = zip(*sorted(zip(self.datetimes, self.values)))\n        if not a_per.is_reversed and not a_per.is_annual:\n            if sort_datetimes[0] < a_per.st_time.doy:\n                new_start = DateTime.from_hoy((sort_datetimes[0] - 1) * 24, n_ap[7])\n                n_ap[0] = new_start.month\n                n_ap[1] = new_start.day\n            if sort_datetimes[-1] > a_per.end_time.doy:\n                new_end = DateTime.from_hoy((sort_datetimes[-1] - 1) * 24, n_ap[7])\n                n_ap[3] = new_end.month\n                n_ap[4] = new_end.day\n        elif a_per.is_reversed:\n            last_ind = None\n            for i, date_t in enumerate(sort_datetimes):\n                last_ind = i if date_t <= a_per.end_time.doy else last_ind\n            if last_ind is not None:\n                last_ind = last_ind + 1\n                sort_datetimes = sort_datetimes[last_ind:] + sort_datetimes[:last_ind]\n                sort_values = sort_values[last_ind:] + sort_values[:last_ind]\n            # If datetimes are outside the a_period range, just make it annual.\n            # There's no way to know what side of the analysis_period should be etended.\n            if sort_datetimes[0] > a_per.end_time.doy and \\\n                    sort_datetimes[0] < a_per.st_time.doy:\n                n_ap[0], n_ap[1], n_ap[3], n_ap[4] = 1, 1, 12, 31\n                sort_datetimes, sort_values = zip(*sorted(zip(\n                    self.datetimes, self.values)))\n\n        # check that there are no duplicate days.\n        for i in xrange(len(sort_datetimes)):\n            assert sort_datetimes[i] != sort_datetimes[i - 1], 'Duplicate day of year ' \\\n                'was found in the collection: {}'.format(sort_datetimes[i])\n\n        # build a validated collection.\n        new_ap = AnalysisPeriod(*n_ap)\n        new_header = self.header.duplicate()\n        new_header._analysis_period = new_ap\n        new_coll = DailyCollection(new_header, sort_values, sort_datetimes)\n        new_coll._validated_a_period = True\n        return new_coll"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _monthly_operation(self, operation, percentile=0):\n        # Retrive the correct operation.\n        if operation == 'average':\n            funct = self._average\n        elif operation == 'total':\n            funct = self._total\n        else:\n            assert 0 <= percentile <= 100, \\\n                'percentile must be between 0 and 100. Got {}'.format(percentile)\n            funct = self._get_percentile_function(percentile)\n\n        # Get the data for the new collection\n        data_dict = self.group_by_month()\n        new_data, d_times = [], []\n        for i in self.header.analysis_period.months_int:\n            vals = data_dict[i]\n            if vals != []:\n                new_data.append(funct(vals))\n                d_times.append(i)\n\n        # build the new monthly collection\n        new_header = self.header.duplicate()\n        if operation == 'percentile':\n            new_header.metadata['operation'] = '{} percentile'.format(percentile)\n        else:\n            new_header.metadata['operation'] = operation\n        collection = MonthlyCollection(new_header, new_data, d_times)\n        collection._validated_a_period = True\n        return collection", "response": "Get a MonthlyCollection given a certain operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters the data collection based on an analysis period.", "response": "def filter_by_analysis_period(self, analysis_period):\n        \"\"\"Filter the Data Collection based on an analysis period.\n\n        Args:\n           analysis period: A Ladybug analysis period\n\n        Return:\n            A new Data Collection with filtered data\n        \"\"\"\n        _filtered_data = self.filter_by_months(analysis_period.months_int)\n        _filtered_data.header._analysis_period = analysis_period\n        return _filtered_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters the Data Collection based on a list of months of the year.", "response": "def filter_by_months(self, months):\n        \"\"\"Filter the Data Collection based on a list of months of the year (as integers).\n\n        Args:\n           months: A List of months of the year [1..12]\n\n        Return:\n            A new Data Collection with filtered data\n        \"\"\"\n        _filt_values = []\n        _filt_datetimes = []\n        for i, d in enumerate(self.datetimes):\n            if d in months:\n                _filt_datetimes.append(d)\n                _filt_values.append(self._values[i])\n        _filt_header = self.header.duplicate()\n        return MonthlyCollection(_filt_header, _filt_values, _filt_datetimes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_analysis_period(self):\n        a_per = self.header.analysis_period\n        n_ap = [a_per.st_month, a_per.end_month]\n\n        # make sure that months are in chronological order.\n        sort_datetimes, sort_values = zip(*sorted(zip(self.datetimes, self.values)))\n        # check that no datetimes lie outside of the analysis_period\n        if not a_per.is_reversed and not a_per.is_annual:\n            if sort_datetimes[0] < a_per.st_month:\n                n_ap[0] = sort_datetimes[0]\n            if sort_datetimes[-1] > a_per.end_month:\n                n_ap[1] = sort_datetimes[-1]\n        elif a_per.is_reversed:\n            last_ind = None\n            for i, date_t in enumerate(sort_datetimes):\n                last_ind = i if date_t <= a_per.end_time.month else last_ind\n            if last_ind is not None:\n                last_ind = last_ind + 1\n                sort_datetimes = sort_datetimes[last_ind:] + sort_datetimes[:last_ind]\n                sort_values = sort_values[last_ind:] + sort_values[:last_ind]\n            if sort_datetimes[0] > a_per.end_time.month and \\\n                    sort_datetimes[0] < a_per.st_time.month:\n                n_ap = [1, 12]\n                sort_datetimes, sort_values = zip(*sorted(zip(\n                    self.datetimes, self.values)))\n\n        # check that there are no duplicate months.\n        for i in xrange(len(sort_datetimes)):\n            assert sort_datetimes[i] != sort_datetimes[i - 1], 'Duplicate month ' \\\n                'was found in the collection: {}'.format(sort_datetimes[i])\n\n        # build a validated collection.\n        new_ap = AnalysisPeriod(st_month=n_ap[0], end_month=n_ap[1])\n        new_header = self.header.duplicate()\n        new_header._analysis_period = new_ap\n        new_coll = MonthlyCollection(new_header, sort_values, sort_datetimes)\n        new_coll._validated_a_period = True\n        return new_coll", "response": "Validate the analysis_period of the data collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering the data collection based on an analysis period.", "response": "def filter_by_analysis_period(self, analysis_period):\n        \"\"\"Filter the Data Collection based on an analysis period.\n\n        Args:\n           analysis period: A Ladybug analysis period\n\n        Return:\n            A new Data Collection with filtered data\n        \"\"\"\n        _filtered_data = self.filter_by_months_per_hour(\n            analysis_period.months_per_hour)\n        _filtered_data.header._analysis_period = analysis_period\n        return _filtered_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_by_months_per_hour(self, months_per_hour):\n        _filt_values = []\n        _filt_datetimes = []\n        for i, d in enumerate(self.datetimes):\n            if d in months_per_hour:\n                _filt_datetimes.append(d)\n                _filt_values.append(self._values[i])\n        return MonthlyPerHourCollection(\n            self.header.duplicate(), _filt_values, _filt_datetimes)", "response": "Filter the Data Collection based on a list of months per hour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_unit(self, values, unit, from_unit):\n        return self._to_unit_base('degC-days', values, unit, from_unit)", "response": "Return values converted to the unit given the input from_unit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning values in IP and the units to which the values have been converted.", "response": "def to_ip(self, values, from_unit):\n        \"\"\"Return values in IP and the units to which the values have been converted.\"\"\"\n        if from_unit in self._ip_units:\n            return values, from_unit\n        elif from_unit == 'degC-hours':\n            return self.to_unit(values, 'degF-hours', from_unit), 'degF-hours'\n        else:\n            return self.to_unit(values, 'degF-days', from_unit), 'degF-days'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting values in SI and the units to which the values have been converted.", "response": "def to_si(self, values, from_unit):\n        \"\"\"Return values in SI and the units to which the values have been converted.\"\"\"\n        if from_unit in self._si_units:\n            return values, from_unit\n        elif from_unit == 'degF-hours':\n            return self.to_unit(values, 'degC-hours', from_unit), 'degC-hours'\n        else:\n            return self.to_unit(values, 'degC-days', from_unit), 'degC-days'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_json(cls, data):\n        optional_keys = ('city', 'state', 'country', 'latitude', 'longitude',\n                         'time_zone', 'elevation', 'station_id', 'source')\n        for key in optional_keys:\n            if key not in data:\n                data[key] = None\n\n        return cls(data['city'], data['state'], data['country'], data['latitude'],\n                   data['longitude'], data['time_zone'], data['elevation'],\n                   data['station_id'], data['source'])", "response": "Create a location from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to create a Ladybug location from a location string.", "response": "def from_location(cls, location):\n        \"\"\"Try to create a Ladybug location from a location string.\n\n        Args:\n            locationString: Location string\n\n        Usage:\n\n            l = Location.from_location(locationString)\n        \"\"\"\n        if not location:\n            return cls()\n        try:\n            if hasattr(location, 'isLocation'):\n                # Ladybug location\n                return location\n\n            elif hasattr(location, 'Latitude'):\n                # Revit's location\n                return cls(city=str(location.Name.replace(\",\", \" \")),\n                           latitude=location.Latitude,\n                           longitude=location.Longitude)\n\n            elif location.startswith('Site:'):\n                loc, city, latitude, longitude, time_zone, elevation = \\\n                    [x.strip() for x in re.findall(r'\\r*\\n*([^\\r\\n]*)[,|;]',\n                                                   location, re.DOTALL)]\n            else:\n                try:\n                    city, latitude, longitude, time_zone, elevation = \\\n                        [key.split(\":\")[-1].strip()\n                         for key in location.split(\",\")]\n                except ValueError:\n                    # it's just the city name\n                    return cls(city=location)\n\n            return cls(city=city, country=None, latitude=latitude,\n                       longitude=longitude, time_zone=time_zone,\n                       elevation=elevation)\n\n        except Exception as e:\n            raise ValueError(\n                \"Failed to create a Location from %s!\\n%s\" % (location, e))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a copy of the location.", "response": "def duplicate(self):\n        \"\"\"Duplicate location.\"\"\"\n        return Location(self.city, self.state, self.country,\n                        self.latitude, self.longitude, self.time_zone, self.elevation,\n                        self.station_id, self.source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ep_style_location_string(self):\n        return \"Site:Location,\\n  \" + \\\n            self.city + ',\\n  ' + \\\n            str(self.latitude) + ',      !Latitude\\n  ' + \\\n            str(self.longitude) + ',     !Longitude\\n  ' + \\\n            str(self.time_zone) + ',     !Time Zone\\n  ' + \\\n            str(self.elevation) + ';       !Elevation'", "response": "Return EnergyPlus s location string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a location from json.", "response": "def to_json(self):\n        \"\"\"Create a location from json.\n            {\n              \"city\": \"-\",\n              \"latitude\": 0,\n              \"longitude\": 0,\n              \"time_zone\": 0,\n              \"elevation\": 0\n            }\n        \"\"\"\n        return {\n            \"city\": self.city,\n            \"state\": self.state,\n            \"country\": self.country,\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"time_zone\": self.time_zone,\n            \"elevation\": self.elevation,\n            \"station_id\": self.station_id,\n            \"source\": self.source\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing an EPW object with all missing values.", "response": "def from_missing_values(cls, is_leap_year=False):\n        \"\"\"Initalize an EPW object with all data missing or empty.\n\n        Note that this classmethod is intended for workflows where one plans\n        to set all of the data within the EPW object.  The EPW file written\n        out from the use of this method is not simulate-abe or useful since\n        all hourly data slots just possess the missing value for that data\n        type. To obtain a EPW that is simulate-able in EnergyPlus, one must\n        at least set the following properties:\n            location\n            dry_bulb_temperature\n            dew_point_temperature\n            relative_humidity\n            atmospheric_station_pressure\n            direct_normal_radiation\n            diffuse_horizontal_radiation\n            wind_direction\n            wind_speed\n            total_sky_cover\n            opaque_sky_cover or horizontal_infrared_radiation_intensity\n\n        Args:\n            is_leap_year: A boolean to set whether the EPW object is for a leap year.\n\n        Usage:\n            from ladybug.epw import EPW\n            from ladybug.location import Location\n            epw = EPW.from_missing_values()\n            epw.location = Location('Denver Golden','CO','USA',39.74,-105.18,-7.0,1829.0)\n            epw.dry_bulb_temperature.values = [20] * 8760\n        \"\"\"\n        # Initialize the class with all data missing\n        epw_obj = cls(None)\n        epw_obj._is_leap_year = is_leap_year\n        epw_obj._location = Location()\n\n        # create an annual analysis period\n        analysis_period = AnalysisPeriod(is_leap_year=is_leap_year)\n\n        # create headers and an empty list for each field in epw file\n        headers = []\n        for field_number in xrange(epw_obj._num_of_fields):\n            field = EPWFields.field_by_number(field_number)\n            header = Header(data_type=field.name, unit=field.unit,\n                            analysis_period=analysis_period)\n            headers.append(header)\n            epw_obj._data.append([])\n\n        # fill in missing datetime values and uncertainty flags.\n        uncertainty = '?9?9?9?9E0?9?9?9?9?9?9?9?9?9?9?9?9?9?9?9*9*9?9?9?9'\n        for dt in analysis_period.datetimes:\n            hr = dt.hour if dt.hour != 0 else 24\n            epw_obj._data[0].append(dt.year)\n            epw_obj._data[1].append(dt.month)\n            epw_obj._data[2].append(dt.day)\n            epw_obj._data[3].append(hr)\n            epw_obj._data[4].append(0)\n            epw_obj._data[5].append(uncertainty)\n\n        # generate missing hourly data\n        calc_length = len(analysis_period.datetimes)\n        for field_number in xrange(6, epw_obj._num_of_fields):\n            field = EPWFields.field_by_number(field_number)\n            mis_val = field.missing if field.missing is not None else 0\n            for dt in xrange(calc_length):\n                epw_obj._data[field_number].append(mis_val)\n\n        # finally, build the data collection objects from the headers and data\n        for i in xrange(epw_obj._num_of_fields):\n            epw_obj._data[i] = HourlyContinuousCollection(headers[i], epw_obj._data[i])\n\n        epw_obj._is_header_loaded = True\n        epw_obj._is_data_loaded = True\n        return epw_obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_json(cls, data):\n        # Initialize the class with all data missing\n        epw_obj = cls(None)\n        epw_obj._is_header_loaded = True\n        epw_obj._is_data_loaded = True\n\n        # Check required and optional keys\n        required_keys = ('location', 'data_collections')\n        option_keys_dict = ('metadata', 'heating_dict', 'cooling_dict',\n                            'extremes_dict', 'extreme_hot_weeks', 'extreme_cold_weeks',\n                            'typical_weeks', 'monthly_ground_temps')\n        for key in required_keys:\n            assert key in data, 'Required key \"{}\" is missing!'.format(key)\n        assert len(data['data_collections']) == epw_obj._num_of_fields, \\\n            'The number of data_collections must be {}. Got {}.'.format(\n                epw_obj._num_of_fields, len(data['data_collections']))\n        for key in option_keys_dict:\n            if key not in data:\n                data[key] = {}\n\n        # Set the required properties of the EPW object.\n        epw_obj._location = Location.from_json(data['location'])\n        epw_obj._data = [HourlyContinuousCollection.from_json(dc)\n                         for dc in data['data_collections']]\n        if 'is_leap_year' in data:\n            epw_obj._is_leap_year = data['is_leap_year']\n        if 'is_ip' in data:\n            epw_obj._is_ip = data['is_ip']\n\n        # Check that the required properties all make sense.\n        for dc in epw_obj._data:\n            assert isinstance(dc, HourlyContinuousCollection), 'data_collections must ' \\\n                'be of HourlyContinuousCollection schema. Got {}'.format(type(dc))\n            assert dc.header.analysis_period.is_annual, 'data_collections ' \\\n                'analysis_period must be annual.'\n            assert dc.header.analysis_period.is_leap_year == epw_obj._is_leap_year, \\\n                'data_collections is_leap_year is not aligned with that of the EPW.'\n\n        # Set all of the header properties if they exist in the dictionary.\n        epw_obj._metadata = data['metadata']\n        epw_obj.heating_design_condition_dictionary = data['heating_dict']\n        epw_obj.cooling_design_condition_dictionary = data['cooling_dict']\n        epw_obj.extreme_design_condition_dictionary = data['extremes_dict']\n\n        def _dejson(parent_dict, obj):\n            new_dict = {}\n            for key, val in parent_dict.items():\n                new_dict[key] = obj.from_json(val)\n            return new_dict\n        epw_obj.extreme_hot_weeks = _dejson(data['extreme_hot_weeks'], AnalysisPeriod)\n        epw_obj.extreme_cold_weeks = _dejson(data['extreme_cold_weeks'], AnalysisPeriod)\n        epw_obj.typical_weeks = _dejson(data['typical_weeks'], AnalysisPeriod)\n        epw_obj.monthly_ground_temperature = _dejson(\n            data['monthly_ground_temps'], MonthlyCollection)\n\n        if 'daylight_savings_start' in data:\n            epw_obj.daylight_savings_start = data['daylight_savings_start']\n        if 'daylight_savings_end' in data:\n            epw_obj.daylight_savings_end = data['daylight_savings_end']\n        if 'comments_1' in data:\n            epw_obj.comments_1 = data['comments_1']\n        if 'comments_2' in data:\n            epw_obj.comments_2 = data['comments_2']\n\n        return epw_obj", "response": "Create an EPW from a json dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef annual_heating_design_day_990(self):\n        self._load_header_check()\n        if bool(self._heating_dict) is True:\n            avg_press = self.atmospheric_station_pressure.average\n            avg_press = None if avg_press == 999999 else avg_press\n            return DesignDay.from_ashrae_dict_heating(\n                self._heating_dict, self.location, True, avg_press)\n        else:\n            return None", "response": "A design day object representing the annual 99. 0% heating design day."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef annual_cooling_design_day_004(self):\n        self._load_header_check()\n        if bool(self._cooling_dict) is True:\n            avg_press = self.atmospheric_station_pressure.average\n            avg_press = None if avg_press == 999999 else avg_press\n            return DesignDay.from_ashrae_dict_cooling(\n                self._cooling_dict, self.location, False, avg_press)\n        else:\n            return None", "response": "A design day object representing the annual 0. 4% cooling design day."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef annual_cooling_design_day_010(self):\n        self._load_header_check()\n        if bool(self._cooling_dict) is True:\n            avg_press = self.atmospheric_station_pressure.average\n            avg_press = None if avg_press == 999999 else avg_press\n            return DesignDay.from_ashrae_dict_cooling(\n                self._cooling_dict, self.location, True, avg_press)\n        else:\n            return None", "response": "A design day object representing the annual 1. 0% cooling design day."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _des_dict_check(self, des_dict, req_keys, cond_name):\n        assert isinstance(des_dict, dict), '{}' \\\n            ' must be a dictionary. Got {}.'.format(cond_name, type(des_dict))\n        if bool(des_dict) is True:\n            input_keys = list(des_dict.keys())\n            for key in req_keys:\n                assert key in input_keys, 'Required key \"{}\" was not found in ' \\\n                    '{}'.format(key, cond_name)", "response": "Check if an input design condition dictionary is acceptable."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if input for the typical or extreme weeks of the header is correct.", "response": "def _weeks_check(self, data, week_type):\n        \"\"\"Check if input for the typical/extreme weeks of the header is correct.\"\"\"\n        assert isinstance(data, dict), '{}' \\\n            ' must be an OrderedDict. Got {}.'.format(week_type, type(data))\n        if bool(data) is True:\n            for val in data.values():\n                assert isinstance(val, AnalysisPeriod), '{} dictionary must contain' \\\n                    ' AnalysisPeriod objects. Got {}.'.format(week_type, type(val))\n                assert len(val.doys_int) == 7, '{} AnalysisPeriod must be for'\\\n                    ' a week.  Got AnalysisPeriod for {} days.'.format(\n                        week_type, type(val))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport data from an epw file.", "response": "def _import_data(self, import_header_only=False):\n        \"\"\"Import data from an epw file.\n\n        Hourly data will be saved in self.data and the various header data\n        will be saved in the properties above.\n        \"\"\"\n        # perform checks on the file before opening it.\n        assert os.path.isfile(self._file_path), 'Cannot find an epw file at {}'.format(\n            self._file_path)\n        assert self._file_path.lower().endswith('epw'), '{} is not an .epw file. \\n' \\\n            'It does not possess the .epw file extension.'.format(self._file_path)\n\n        with open(self._file_path, readmode) as epwin:\n            line = epwin.readline()\n            original_header_load = bool(self._is_header_loaded)\n\n            if not self._is_header_loaded:\n                # import location data\n                # first line has location data - Here is an example\n                # LOCATION,Denver Golden Nr,CO,USA,TMY3,724666,39.74,-105.18,-7.0,1829.0\n                location_data = line.strip().split(',')\n                self._location = Location()\n                self._location.city = location_data[1].replace('\\\\', ' ') \\\n                    .replace('/', ' ')\n                self._location.state = location_data[2]\n                self._location.country = location_data[3]\n                self._location.source = location_data[4]\n                self._location.station_id = location_data[5]\n                self._location.latitude = location_data[6]\n                self._location.longitude = location_data[7]\n                self._location.time_zone = location_data[8]\n                self._location.elevation = location_data[9]\n\n                # asemble a dictionary of metadata\n                self._metadata = {\n                    'source': self._location.source,\n                    'country': self._location.country,\n                    'city': self._location.city\n                }\n\n                self._header = [line] + [epwin.readline() for i in xrange(7)]\n\n                # parse the heating, cooling and extreme design conditions.\n                dday_data = self._header[1].strip().split(',')\n                if len(dday_data) >= 2 and int(dday_data[1]) == 1:\n                    if dday_data[4] == 'Heating':\n                        for key, val in zip(DesignDay.heating_keys, dday_data[5:20]):\n                            self._heating_dict[key] = val\n                    if dday_data[20] == 'Cooling':\n                        for key, val in zip(DesignDay.cooling_keys, dday_data[21:53]):\n                            self._cooling_dict[key] = val\n                    if dday_data[53] == 'Extremes':\n                        for key, val in zip(DesignDay.extreme_keys, dday_data[54:70]):\n                            self._extremes_dict[key] = val\n\n                # parse typical and extreme periods into analysis periods.\n                week_data = self._header[2].split(',')\n                num_weeks = int(week_data[1]) if len(week_data) >= 2 else 0\n                st_ind = 2\n                for i in xrange(num_weeks):\n                    week_dat = week_data[st_ind:st_ind + 4]\n                    st_ind += 4\n                    st = [int(num) for num in week_dat[2].split('/')]\n                    end = [int(num) for num in week_dat[3].split('/')]\n                    if len(st) == 3:\n                        a_per = AnalysisPeriod(st[1], st[2], 0, end[1], end[2], 23)\n                    elif len(st) == 2:\n                        a_per = AnalysisPeriod(st[0], st[1], 0, end[0], end[1], 23)\n                    if 'Max' in week_dat[0] and week_dat[1] == 'Extreme':\n                        self._extreme_hot_weeks[week_dat[0]] = a_per\n                    elif 'Min' in week_dat[0] and week_dat[1] == 'Extreme':\n                        self._extreme_cold_weeks[week_dat[0]] = a_per\n                    elif week_dat[1] == 'Typical':\n                        self._typical_weeks[week_dat[0]] = a_per\n\n                # parse the monthly ground temperatures in the header.\n                grnd_data = self._header[3].strip().split(',')\n                num_depths = int(grnd_data[1]) if len(grnd_data) >= 2 else 0\n                st_ind = 2\n                for i in xrange(num_depths):\n                    header_meta = dict(self._metadata)  # copying the metadata dictionary\n                    header_meta['depth'] = float(grnd_data[st_ind])\n                    header_meta['soil conductivity'] = grnd_data[st_ind + 1]\n                    header_meta['soil density'] = grnd_data[st_ind + 2]\n                    header_meta['soil specific heat'] = grnd_data[st_ind + 3]\n                    grnd_header = Header(temperature.GroundTemperature(), 'C',\n                                         AnalysisPeriod(), header_meta)\n                    grnd_vlas = [float(x) for x in grnd_data[st_ind + 4: st_ind + 16]]\n                    self._monthly_ground_temps[float(grnd_data[st_ind])] = \\\n                        MonthlyCollection(grnd_header, grnd_vlas, list(xrange(12)))\n                    st_ind += 16\n\n                # parse leap year, daylight savings and comments.\n                leap_dl_sav = self._header[4].strip().split(',')\n                self._is_leap_year = True if leap_dl_sav[1] == 'Yes' else False\n                self.daylight_savings_start = leap_dl_sav[2]\n                self.daylight_savings_end = leap_dl_sav[3]\n                comments_1 = self._header[5].strip().split(',')\n                if len(comments_1) > 0:\n                    self.comments_1 = ','.join(comments_1[1:])\n                comments_2 = self._header[6].strip().split(',')\n                if len(comments_2) > 0:\n                    self.comments_2 = ','.join(comments_2[1:])\n\n                self._is_header_loaded = True\n\n            if import_header_only:\n                return\n\n            # read first line of data to overwrite the number of fields\n            if original_header_load is True:\n                for i in xrange(7):\n                    epwin.readline()\n            line = epwin.readline()\n            self._num_of_fields = min(len(line.strip().split(',')), 35)\n\n            # create an annual analysis period\n            analysis_period = AnalysisPeriod(is_leap_year=self.is_leap_year)\n\n            # create headers and an empty list for each field in epw file\n            headers = []\n            for field_number in xrange(self._num_of_fields):\n                field = EPWFields.field_by_number(field_number)\n                header = Header(data_type=field.name, unit=field.unit,\n                                analysis_period=analysis_period,\n                                metadata=dict(self._metadata))\n                headers.append(header)\n                self._data.append([])\n\n            # collect hourly data\n            while line:\n                data = line.strip().split(',')\n\n                for field_number in xrange(self._num_of_fields):\n                    value_type = EPWFields.field_by_number(field_number).value_type\n                    try:\n                        value = value_type(data[field_number])\n                    except ValueError as e:\n                        # failed to convert the value for the specific TypeError\n                        if value_type != int:\n                            raise ValueError(e)\n                        value = int(round(float(data[field_number])))\n\n                    self._data[field_number].append(value)\n\n                line = epwin.readline()\n\n            # if the first value is at 1 AM, move last item to start position\n            for field_number in xrange(self._num_of_fields):\n                point_in_time = headers[field_number].data_type.point_in_time\n                if point_in_time is True:\n                    # move the last hour to first position\n                    last_hour = self._data[field_number].pop()\n                    self._data[field_number].insert(0, last_hour)\n\n            # finally, build the data collection objects from the headers and data\n            for i in xrange(self._num_of_fields):\n                self._data[i] = HourlyContinuousCollection(headers[i], self._data[i])\n\n            self._is_data_loaded = True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef header(self):\n        self._load_header_check()\n        loc = self.location\n        loc_str = 'LOCATION,{},{},{},{},{},{},{},{},{}\\n'.format(\n            loc.city, loc.state, loc.country, loc.source, loc.station_id, loc.latitude,\n            loc.longitude, loc.time_zone, loc.elevation)\n        winter_found = bool(self._heating_dict)\n        summer_found = bool(self._cooling_dict)\n        extreme_found = bool(self._extremes_dict)\n        if winter_found and summer_found and extreme_found:\n            des_str = 'DESIGN CONDITIONS,1,Climate Design Data 2009 ASHRAE Handbook,,'\n            des_str = des_str + 'Heating,{},Cooling,{},Extremes,{}\\n'.format(\n                ','.join([self._heating_dict[key] for key in DesignDay.heating_keys]),\n                ','.join([self._cooling_dict[key] for key in DesignDay.cooling_keys]),\n                ','.join([self._extremes_dict[key] for key in DesignDay.extreme_keys]))\n        else:\n            des_str = 'DESIGN CONDITIONS,0\\n'\n        weeks = []\n        if bool(self.extreme_hot_weeks) is True:\n            for wk_name, a_per in self.extreme_hot_weeks.items():\n                weeks.append(self._format_week(wk_name, 'Extreme', a_per))\n        if bool(self.extreme_cold_weeks):\n            for wk_name, a_per in self.extreme_cold_weeks.items():\n                weeks.append(self._format_week(wk_name, 'Extreme', a_per))\n        if bool(self.typical_weeks):\n            for wk_name in sorted(self.typical_weeks.keys()):\n                a_per = self.typical_weeks[wk_name]\n                weeks.append(self._format_week(wk_name, 'Typical', a_per))\n        week_str = 'TYPICAL/EXTREME PERIODS,{},{}\\n'.format(len(weeks), ','.join(weeks))\n        grnd_st = 'GROUND TEMPERATURES,{}'.format(len(self._monthly_ground_temps.keys()))\n        for depth in sorted(self._monthly_ground_temps.keys()):\n            grnd_st = grnd_st + ',{},{}'.format(\n                depth, self._format_grndt(self._monthly_ground_temps[depth]))\n        grnd_st = grnd_st + '\\n'\n        leap_yr = 'Yes' if self._is_leap_year is True else 'No'\n        leap_str = 'HOLIDAYS/DAYLIGHT SAVINGS,{},{},{},0\\n'.format(\n            leap_yr, self.daylight_savings_start, self.daylight_savings_end)\n        c_str1 = 'COMMENTS 1,{}\\n'.format(self.comments_1)\n        c_str2 = 'COMMENTS 2,{}\\n'.format(self.comments_2)\n        data_str = 'DATA PERIODS,1,1,Data,Sunday, 1/ 1,12/31\\n'\n        return [loc_str, des_str, week_str, grnd_st, leap_str, c_str1, c_str2, data_str]", "response": "A list of text representing the full header of the EPW."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat an AnalysisPeriod into a string for the EPW header.", "response": "def _format_week(self, name, type, a_per):\n        \"\"\"Format an AnalysisPeriod into string for the EPW header.\"\"\"\n        return '{},{},{}/{},{}/{}'.format(name, type, a_per.st_month, a_per.st_day,\n                                          a_per.end_month, a_per.end_day)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _format_grndt(self, data_c):\n        monthly_str = '{},{},{},{}'.format(\n            data_c.header.metadata['soil conductivity'],\n            data_c.header.metadata['soil density'],\n            data_c.header.metadata['soil specific heat'],\n            ','.join(['%.2f' % x for x in data_c.values]))\n        return monthly_str", "response": "Format monthly ground data collection into string for the EPW header."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the epw object to a file.", "response": "def save(self, file_path):\n        \"\"\"Save epw object as an epw file.\n\n        args:\n            file_path: A string representing the path to write the epw file to.\n        \"\"\"\n        # load data if it's  not loaded convert to SI if it is in IP\n        if not self.is_data_loaded:\n            self._import_data()\n        originally_ip = False\n        if self.is_ip is True:\n            self.convert_to_si()\n            originally_ip = True\n\n        # write the file\n        lines = self.header\n        try:\n            # if the first value is at 1AM, move first item to end position\n            for field in xrange(0, self._num_of_fields):\n                point_in_time = self._data[field].header.data_type.point_in_time\n                if point_in_time is True:\n                    first_hour = self._data[field]._values.pop(0)\n                    self._data[field]._values.append(first_hour)\n\n            annual_a_per = AnalysisPeriod(is_leap_year=self.is_leap_year)\n            for hour in xrange(0, len(annual_a_per.datetimes)):\n                line = []\n                for field in xrange(0, self._num_of_fields):\n                    line.append(str(self._data[field]._values[hour]))\n                lines.append(\",\".join(line) + \"\\n\")\n        except IndexError:\n            # cleaning up\n            length_error_msg = 'Data length is not for a full year and cannot be ' + \\\n                'saved as an EPW file.'\n            raise ValueError(length_error_msg)\n        else:\n            file_data = ''.join(lines)\n            write_to_file(file_path, file_data, True)\n        finally:\n            del(lines)\n            # move last item to start position for fields on the hour\n            for field in xrange(0, self._num_of_fields):\n                point_in_time = self._data[field].header.data_type.point_in_time\n                if point_in_time is True:\n                    last_hour = self._data[field]._values.pop()\n                    self._data[field]._values.insert(0, last_hour)\n\n        if originally_ip is True:\n            self.convert_to_ip()\n\n        return file_path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_to_ip(self):\n        if not self.is_data_loaded:\n            self._import_data()\n        if self.is_ip is False:\n            for coll in self._data:\n                coll.convert_to_ip()\n        self._is_ip = True", "response": "Convert all Data Collections of this EPW object to IP units."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a data field by field number.", "response": "def _get_data_by_field(self, field_number):\n        \"\"\"Return a data field by field number.\n\n        This is a useful method to get the values for fields that Ladybug\n        currently doesn't import by default. You can find list of fields by typing\n        EPWFields.fields\n\n        Args:\n            field_number: a value between 0 to 34 for different available epw fields.\n\n        Returns:\n            An annual Ladybug list\n        \"\"\"\n        if not self.is_data_loaded:\n            self._import_data()\n\n        # check input data\n        if not 0 <= field_number < self._num_of_fields:\n            raise ValueError(\"Field number should be between 0-%d\" % self._num_of_fields)\n\n        return self._data[field_number]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sky_temperature(self):\n        # create sky temperature header\n        sky_temp_header = Header(data_type=temperature.SkyTemperature(), unit='C',\n                                 analysis_period=AnalysisPeriod(),\n                                 metadata=self._metadata)\n\n        # calculate sy temperature for each hour\n        horiz_ir = self._get_data_by_field(12).values\n        sky_temp_data = [calc_sky_temperature(hir) for hir in horiz_ir]\n        return HourlyContinuousCollection(sky_temp_header, sky_temp_data)", "response": "Return annual Sky Temperature as a Ladybug Data Collection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_wea(self, file_path, hoys=None):\n        hoys = hoys or xrange(len(self.direct_normal_radiation.datetimes))\n        if not file_path.lower().endswith('.wea'):\n            file_path += '.wea'\n\n        originally_ip = False\n        if self.is_ip is True:\n            self.convert_to_si()\n            originally_ip = True\n\n        # write header\n        lines = [self._get_wea_header()]\n        # write values\n        datetimes = self.direct_normal_radiation.datetimes\n        for hoy in hoys:\n            dir_rad = self.direct_normal_radiation[hoy]\n            dif_rad = self.diffuse_horizontal_radiation[hoy]\n            line = \"%d %d %.3f %d %d\\n\" \\\n                % (datetimes[hoy].month,\n                   datetimes[hoy].day,\n                   datetimes[hoy].hour + 0.5,\n                   dir_rad, dif_rad)\n            lines.append(line)\n\n        file_data = ''.join(lines)\n        write_to_file(file_path, file_data, True)\n\n        if originally_ip is True:\n            self.convert_to_ip()\n\n        return file_path", "response": "Write an wea file from the epw file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_json(self):\n        # load data if it's not loaded\n        if not self.is_data_loaded:\n            self._import_data()\n\n        def jsonify_dict(base_dict):\n            new_dict = {}\n            for key, val in base_dict.items():\n                new_dict[key] = val.to_json()\n            return new_dict\n        hot_wks = jsonify_dict(self.extreme_hot_weeks)\n        cold_wks = jsonify_dict(self.extreme_cold_weeks)\n        typ_wks = jsonify_dict(self.typical_weeks)\n        grnd_temps = jsonify_dict(self.monthly_ground_temperature)\n        return {\n            'location': self.location.to_json(),\n            'data_collections': [dc.to_json() for dc in self._data],\n            'metadata': self.metadata,\n            'heating_dict': self.heating_design_condition_dictionary,\n            'cooling_dict': self.cooling_design_condition_dictionary,\n            'extremes_dict': self.extreme_design_condition_dictionary,\n            'extreme_hot_weeks': hot_wks,\n            'extreme_cold_weeks': cold_wks,\n            'typical_weeks': typ_wks,\n            \"monthly_ground_temps\": grnd_temps,\n            \"is_ip\": self._is_ip,\n            \"is_leap_year\": self.is_leap_year,\n            \"daylight_savings_start\": self.daylight_savings_start,\n            \"daylight_savings_end\": self.daylight_savings_end,\n            \"comments_1\": self.comments_1,\n            \"comments_2\": self.comments_2\n        }", "response": "Convert the EPW to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_json(cls, data):\n        keys = ('st_month', 'st_day', 'st_hour', 'end_month',\n                'end_day', 'end_hour', 'timestep', 'is_leap_year')\n        for key in keys:\n            if key not in data:\n                data[key] = None\n\n        return cls(\n            data['st_month'], data['st_day'], data['st_hour'], data['end_month'],\n            data['end_day'], data['end_hour'], data['timestep'],\n            data['is_leap_year'])", "response": "Create an analysis period from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_analysis_period(cls, analysis_period=None):\n        if not analysis_period:\n            return cls()\n        elif hasattr(analysis_period, 'isAnalysisPeriod'):\n            return analysis_period\n        elif isinstance(analysis_period, str):\n            try:\n                return cls.from_string(analysis_period)\n            except Exception as e:\n                raise ValueError(\n                    \"{} is not convertable to an AnalysisPeriod: {}\".format(\n                        analysis_period, e)\n                )", "response": "Create and return an AnalysisPeriod object from an analysis period."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_string(cls, analysis_period_string):\n        # %s/%s to %s/%s between %s to %s @%s*\n        is_leap_year = True if analysis_period_string.strip()[-1] == '*' else False\n        ap = analysis_period_string.lower().replace(' ', '') \\\n            .replace('to', ' ') \\\n            .replace('and', ' ') \\\n            .replace('/', ' ') \\\n            .replace('between', ' ') \\\n            .replace('@', ' ') \\\n            .replace('*', '')\n        try:\n            st_month, st_day, end_month, end_day, \\\n                st_hour, end_hour, timestep = ap.split(' ')\n            return cls(st_month, st_day, st_hour, end_month,\n                       end_day, end_hour, int(timestep), is_leap_year)\n        except Exception as e:\n            raise ValueError(str(e))", "response": "Create an Analysis Period object from an analysis period string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hoys_int(self):\n        if self._timestamps_data is None:\n            self._calculate_timestamps()\n        return tuple(int(moy / 60.0) for moy in self._timestamps_data)", "response": "A sorted list of hours of year in this analysis period as integers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef months_int(self):\n        if not self._is_reversed:\n            return list(xrange(self.st_time.month, self.end_time.month + 1))\n        else:\n            months_st = list(xrange(self.st_time.month, 13))\n            months_end = list(xrange(1, self.end_time.month + 1))\n            return months_st + months_end", "response": "A sorted list of months of the year in this analysis period as integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef months_per_hour(self):\n        month_hour = []\n        hour_range = xrange(self.st_hour, self.end_hour + 1)\n        for month in self.months_int:\n            month_hour.extend([(month, hr) for hr in hour_range])\n        return month_hour", "response": "A list of tuples representing months per hour in this analysis period."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if an analysis period is annual.", "response": "def is_annual(self):\n        \"\"\"Check if an analysis period is annual.\"\"\"\n        if (self.st_month, self.st_day, self.st_hour, self.end_month,\n                self.end_day, self.end_hour) == (1, 1, 0, 12, 31, 23):\n                    return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a float hour is a possible hour for this analysis period.", "response": "def is_possible_hour(self, hour):\n        \"\"\"Check if a float hour is a possible hour for this analysis period.\"\"\"\n        if hour > 23 and self.is_possible_hour(0):\n            hour = int(hour)\n        if not self._is_overnight:\n            return self.st_time.hour <= hour <= self.end_time.hour\n        else:\n            return self.st_time.hour <= hour <= 23 or \\\n                0 <= hour <= self.end_time.hour"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_time_included(self, time):\n        if self._timestamps_data is None:\n            self._calculate_timestamps()\n        # time filtering in Ladybug Tools is slightly different than \"normal\"\n        # filtering since start hour and end hour will be applied for every day.\n        # For instance 2/20 9am to 2/22 5pm means hour between 9-17\n        # during 20, 21 and 22 of Feb.\n        return time.moy in self._timestamps_data", "response": "Check if a time is included in this analysis period."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a copy of the analysis period.", "response": "def duplicate(self):\n        \"\"\"Return a copy of the analysis period.\"\"\"\n        return AnalysisPeriod(self.st_month, self.st_day, self.st_hour,\n                              self.end_month, self.end_day, self.end_hour,\n                              self.timestep, self.is_leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the analysis period to a dictionary.", "response": "def to_json(self):\n        \"\"\"Convert the analysis period to a dictionary.\"\"\"\n        return {\n            'st_month': self.st_month,\n            'st_day': self.st_day,\n            'st_hour': self.st_hour,\n            'end_month': self.end_month,\n            'end_day': self.end_day,\n            'end_hour': self.end_hour,\n            'timestep': self.timestep,\n            'is_leap_year': self.is_leap_year\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate timesteps between start time and end time.", "response": "def _calc_timestamps(self, st_time, end_time):\n        \"\"\"Calculate timesteps between start time and end time.\n\n        Use this method only when start time month is before end time month.\n        \"\"\"\n        # calculate based on minutes\n        # I have to convert the object to DateTime because of how Dynamo\n        # works: https://github.com/DynamoDS/Dynamo/issues/6683\n        # Do not modify this line to datetime\n        curr = datetime(st_time.year, st_time.month, st_time.day, st_time.hour,\n                        st_time.minute, self.is_leap_year)\n        end_time = datetime(end_time.year, end_time.month, end_time.day,\n                            end_time.hour, end_time.minute, self.is_leap_year)\n\n        while curr <= end_time:\n            if self.is_possible_hour(curr.hour + (curr.minute / 60.0)):\n                time = DateTime(curr.month, curr.day, curr.hour, curr.minute,\n                                self.is_leap_year)\n                self._timestamps_data.append(time.moy)\n            curr += self.minute_intervals\n\n        if self.timestep != 1 and curr.hour == 23 and self.is_possible_hour(0):\n            # This is for cases that timestep is more than one\n            # and last hour of the day is part of the calculation\n            curr = end_time\n            for i in list(xrange(self.timestep))[1:]:\n                curr += self.minute_intervals\n                time = DateTime(curr.month, curr.day, curr.hour, curr.minute,\n                                self.is_leap_year)\n                self._timestamps_data.append(time.moy)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_timestamps(self):\n        self._timestamps_data = []\n        if not self._is_reversed:\n            self._calc_timestamps(self.st_time, self.end_time)\n        else:\n            self._calc_timestamps(self.st_time, DateTime.from_hoy(8759))\n            self._calc_timestamps(DateTime.from_hoy(0), self.end_time)", "response": "Calculate the timestamps of the class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calc_daystamps(self, st_time, end_time):\n        start_doy = sum(self._num_of_days_each_month[:st_time.month-1]) + st_time.day\n        end_doy = sum(self._num_of_days_each_month[:end_time.month-1]) + end_time.day + 1\n        return list(range(start_doy, end_doy))", "response": "Calculate days of the year between start time and end time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_values(cls, location, direct_normal_irradiance,\n                    diffuse_horizontal_irradiance, timestep=1, is_leap_year=False):\n        \"\"\"Create wea from a list of irradiance values.\n\n        This method converts input lists to data collection.\n        \"\"\"\n        err_message = 'For timestep %d, %d number of data for %s is expected. ' \\\n            '%d is provided.'\n        if len(direct_normal_irradiance) % cls.hour_count(is_leap_year) == 0:\n            # add extra information to err_message\n            err_message = err_message + ' Did you forget to set the timestep to %d?' \\\n                % (len(direct_normal_irradiance) / cls.hour_count(is_leap_year))\n\n        assert len(direct_normal_irradiance) / \\\n            timestep == cls.hour_count(is_leap_year), \\\n            err_message % (timestep, timestep * cls.hour_count(is_leap_year),\n                           'direct normal irradiance', len(\n                               direct_normal_irradiance))\n\n        assert len(diffuse_horizontal_irradiance) / timestep == \\\n            cls.hour_count(is_leap_year), \\\n            err_message % (timestep, timestep * cls.hour_count(is_leap_year),\n                           'diffuse_horizontal_irradiance', len(\n                               direct_normal_irradiance))\n\n        metadata = {'source': location.source, 'country': location.country,\n                    'city': location.city}\n        dnr, dhr = cls._get_data_collections(\n            direct_normal_irradiance, diffuse_horizontal_irradiance,\n            metadata, timestep, is_leap_year)\n        return cls(location, dnr, dhr, timestep, is_leap_year)", "response": "Create wea from a list of irradiance values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a Wea from a json dict.", "response": "def from_json(cls, data):\n        \"\"\" Create Wea from json file\n            {\n            \"location\": {} , // ladybug location schema\n            \"direct_normal_irradiance\": [], // List of hourly direct normal\n                irradiance data points\n            \"diffuse_horizontal_irradiance\": [], // List of hourly diffuse\n                horizontal irradiance data points\n            \"timestep\": float //timestep between measurements, default is 1\n            }\n        \"\"\"\n        required_keys = ('location', 'direct_normal_irradiance',\n                         'diffuse_horizontal_irradiance')\n        optional_keys = ('timestep', 'is_leap_year')\n\n        for key in required_keys:\n            assert key in data, 'Required key \"{}\" is missing!'.format(key)\n\n        for key in optional_keys:\n            if key not in data:\n                data[key] = None\n\n        location = Location.from_json(data['location'])\n        direct_normal_irradiance = \\\n            HourlyContinuousCollection.from_json(data['direct_normal_irradiance'])\n        diffuse_horizontal_irradiance = \\\n            HourlyContinuousCollection.from_json(data['diffuse_horizontal_irradiance'])\n        timestep = data['timestep']\n        is_leap_year = data['is_leap_year']\n\n        return cls(location, direct_normal_irradiance,\n                   diffuse_horizontal_irradiance, timestep, is_leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a wea object from a wea file.", "response": "def from_file(cls, weafile, timestep=1, is_leap_year=False):\n        \"\"\"Create wea object from a wea file.\n\n        Args:\n            weafile:Full path to wea file.\n            timestep: An optional integer to set the number of time steps per hour.\n                Default is 1 for one value per hour. If the wea file has a time step\n                smaller than an hour adjust this input accordingly.\n            is_leap_year: A boolean to indicate if values are representing a leap year.\n                Default is False.\n        \"\"\"\n        assert os.path.isfile(weafile), 'Failed to find {}'.format(weafile)\n        location = Location()\n        with open(weafile, readmode) as weaf:\n            first_line = weaf.readline()\n            assert first_line.startswith('place'), \\\n                'Failed to find place in header. ' \\\n                '{} is not a valid wea file.'.format(weafile)\n            location.city = ' '.join(first_line.split()[1:])\n            # parse header\n            location.latitude = float(weaf.readline().split()[-1])\n            location.longitude = -float(weaf.readline().split()[-1])\n            location.time_zone = -int(weaf.readline().split()[-1]) / 15\n            location.elevation = float(weaf.readline().split()[-1])\n            weaf.readline()  # pass line for weather data units\n\n            # parse irradiance values\n            direct_normal_irradiance = []\n            diffuse_horizontal_irradiance = []\n            for line in weaf:\n                dirn, difh = [int(v) for v in line.split()[-2:]]\n                direct_normal_irradiance.append(dirn)\n                diffuse_horizontal_irradiance.append(difh)\n\n        return cls.from_values(location, direct_normal_irradiance,\n                               diffuse_horizontal_irradiance, timestep, is_leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Wea object from an epw file.", "response": "def from_epw_file(cls, epwfile, timestep=1):\n        \"\"\"Create a wea object using the solar irradiance values in an epw file.\n\n        Args:\n            epwfile: Full path to epw weather file.\n            timestep: An optional integer to set the number of time steps per hour.\n                Default is 1 for one value per hour. Note that this input\n                will only do a linear interpolation over the data in the EPW\n                file.  While such linear interpolations are suitable for most\n                thermal simulations, where thermal lag \"smooths over\" the effect\n                of momentary increases in solar energy, it is not recommended\n                for daylight simulations, where momentary increases in solar\n                energy can mean the difference between glare and visual comfort.\n        \"\"\"\n        is_leap_year = False  # epw file is always for 8760 hours\n\n        epw = EPW(epwfile)\n        direct_normal, diffuse_horizontal = \\\n            cls._get_data_collections(epw.direct_normal_radiation.values,\n                                      epw.diffuse_horizontal_radiation.values,\n                                      epw.metadata, 1, is_leap_year)\n        if timestep != 1:\n            print (\"Note: timesteps greater than 1 on epw-generated Wea's \\n\" +\n                   \"are suitable for thermal models but are not recommended \\n\" +\n                   \"for daylight models.\")\n            # interpolate the data\n            direct_normal = direct_normal.interpolate_to_timestep(timestep)\n            diffuse_horizontal = diffuse_horizontal.interpolate_to_timestep(timestep)\n            # create sunpath to check if the sun is up at a given timestep\n            sp = Sunpath.from_location(epw.location)\n            # add correct values to the emply data collection\n            for i, dt in enumerate(cls._get_datetimes(timestep, is_leap_year)):\n                # set irradiance values to 0 when the sun is not up\n                sun = sp.calculate_sun_from_date_time(dt)\n                if sun.altitude < 0:\n                    direct_normal[i] = 0\n                    diffuse_horizontal[i] = 0\n\n        return cls(epw.location, direct_normal, diffuse_horizontal,\n                   timestep, is_leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an ASHRAE Revised Clear Sky object from the monthly skyoptical depths in a. stat file.", "response": "def from_stat_file(cls, statfile, timestep=1, is_leap_year=False):\n        \"\"\"Create an ASHRAE Revised Clear Sky wea object from the monthly sky\n        optical depths in a .stat file.\n\n        Args:\n            statfile: Full path to the .stat file.\n            timestep: An optional integer to set the number of time steps per\n                hour. Default is 1 for one value per hour.\n            is_leap_year: A boolean to indicate if values are representing a leap year.\n                Default is False.\n        \"\"\"\n        stat = STAT(statfile)\n\n        # check to be sure the stat file does not have missing tau values\n        def check_missing(opt_data, data_name):\n            if opt_data == []:\n                raise ValueError('Stat file contains no optical data.')\n            for i, x in enumerate(opt_data):\n                if x is None:\n                    raise ValueError(\n                        'Missing optical depth data for {} at month {}'.format(\n                            data_name, i)\n                    )\n        check_missing(stat.monthly_tau_beam, 'monthly_tau_beam')\n        check_missing(stat.monthly_tau_diffuse, 'monthly_tau_diffuse')\n\n        return cls.from_ashrae_revised_clear_sky(stat.location, stat.monthly_tau_beam,\n                                                 stat.monthly_tau_diffuse, timestep,\n                                                 is_leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a wea object representing an ASHRAE Revised Clear Sky.", "response": "def from_ashrae_revised_clear_sky(cls, location, monthly_tau_beam,\n                                      monthly_tau_diffuse, timestep=1,\n                                      is_leap_year=False):\n        \"\"\"Create a wea object representing an ASHRAE Revised Clear Sky (\"Tau Model\")\n\n        ASHRAE Revised Clear Skies are intended to determine peak solar load\n        and sizing parmeters for HVAC systems.  The revised clear sky is\n        currently the default recommended sky model used to autosize HVAC\n        systems in EnergyPlus. For more information on the ASHRAE Revised Clear\n        Sky model, see the EnergyPlus Engineering Reference:\n        https://bigladdersoftware.com/epx/docs/8-9/engineering-reference/climate-calculations.html\n\n        Args:\n            location: Ladybug location object.\n            monthly_tau_beam: A list of 12 float values indicating the beam\n                optical depth of the sky at each month of the year.\n            monthly_tau_diffuse: A list of 12 float values indicating the\n                diffuse optical depth of the sky at each month of the year.\n            timestep: An optional integer to set the number of time steps per\n                hour. Default is 1 for one value per hour.\n            is_leap_year: A boolean to indicate if values are representing a leap year.\n                Default is False.\n        \"\"\"\n        # extract metadata\n        metadata = {'source': location.source, 'country': location.country,\n                    'city': location.city}\n\n        # create sunpath and get altitude at every timestep of the year\n        sp = Sunpath.from_location(location)\n        sp.is_leap_year = is_leap_year\n        altitudes = [[] for i in range(12)]\n        dates = cls._get_datetimes(timestep, is_leap_year)\n        for t_date in dates:\n            sun = sp.calculate_sun_from_date_time(t_date)\n            altitudes[sun.datetime.month - 1].append(sun.altitude)\n\n        # run all of the months through the ashrae_revised_clear_sky model\n        direct_norm, diffuse_horiz = [], []\n        for i_mon, alt_list in enumerate(altitudes):\n            dir_norm_rad, dif_horiz_rad = ashrae_revised_clear_sky(\n                alt_list, monthly_tau_beam[i_mon], monthly_tau_diffuse[i_mon])\n            direct_norm.extend(dir_norm_rad)\n            diffuse_horiz.extend(dif_horiz_rad)\n\n        direct_norm_rad, diffuse_horiz_rad = \\\n            cls._get_data_collections(direct_norm, diffuse_horiz,\n                                      metadata, timestep, is_leap_year)\n\n        return cls(location, direct_norm_rad, diffuse_horiz_rad, timestep, is_leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a wea object from climate data using Zhang - Huang model.", "response": "def from_zhang_huang_solar(cls, location, cloud_cover,\n                               relative_humidity, dry_bulb_temperature,\n                               wind_speed, atmospheric_pressure=None,\n                               timestep=1, is_leap_year=False, use_disc=False):\n        \"\"\"Create a wea object from climate data using the Zhang-Huang model.\n\n        The Zhang-Huang solar model was developed to estimate solar\n        irradiance for weather stations that lack such values, which are\n        typically colleted with a pyranometer. Using total cloud cover,\n        dry-bulb temperature, relative humidity, and wind speed as\n        inputs the Zhang-Huang estimates global horizontal irradiance\n        by means of a regression model across these variables.\n        For more information on the Zhang-Huang model, see the\n        EnergyPlus Engineering Reference:\n        https://bigladdersoftware.com/epx/docs/8-7/engineering-reference/climate-calculations.html#zhang-huang-solar-model\n\n        Args:\n            location: Ladybug location object.\n            cloud_cover: A list of annual float values between 0 and 1\n                that represent the fraction of the sky dome covered\n                in clouds (0 = clear; 1 = completely overcast)\n            relative_humidity: A list of annual float values between\n                0 and 100 that represent the relative humidity in percent.\n            dry_bulb_temperature: A list of annual float values that\n                represent the dry bulb temperature in degrees Celcius.\n            wind_speed: A list of annual float values that\n                represent the wind speed in meters per second.\n            atmospheric_pressure: An optional list of float values that\n                represent the atmospheric pressure in Pa.  If None or\n                left blank, pressure at sea level will be used (101325 Pa).\n            timestep: An optional integer to set the number of time steps per\n                hour. Default is 1 for one value per hour.\n            is_leap_year: A boolean to indicate if values are representing a leap year.\n                Default is False.\n            use_disc: Set to True to use the original DISC model as opposed to the\n                newer and more accurate DIRINT model. Default is False.\n        \"\"\"\n        # check input data\n        assert len(cloud_cover) == len(relative_humidity) == \\\n            len(dry_bulb_temperature) == len(wind_speed), \\\n            'lengths of input climate data must match.'\n        assert len(cloud_cover) / timestep == cls.hour_count(is_leap_year), \\\n            'input climate data must be annual.'\n        assert isinstance(timestep, int), 'timestep must be an' \\\n            ' integer. Got {}'.format(type(timestep))\n        if atmospheric_pressure is not None:\n            assert len(atmospheric_pressure) == len(cloud_cover), \\\n                'length pf atmospheric_pressure must match the other input lists.'\n        else:\n            atmospheric_pressure = [101325] * cls.hour_count(is_leap_year) * timestep\n\n        # initiate sunpath based on location\n        sp = Sunpath.from_location(location)\n        sp.is_leap_year = is_leap_year\n\n        # calculate parameters needed for zhang-huang irradiance\n        date_times = []\n        altitudes = []\n        doys = []\n        dry_bulb_t3_hrs = []\n        for count, t_date in enumerate(cls._get_datetimes(timestep, is_leap_year)):\n            date_times.append(t_date)\n            sun = sp.calculate_sun_from_date_time(t_date)\n            altitudes.append(sun.altitude)\n            doys.append(sun.datetime.doy)\n            dry_bulb_t3_hrs.append(dry_bulb_temperature[count - (3 * timestep)])\n\n        # calculate zhang-huang irradiance\n        dir_ir, diff_ir = zhang_huang_solar_split(altitudes, doys, cloud_cover,\n                                                  relative_humidity,\n                                                  dry_bulb_temperature,\n                                                  dry_bulb_t3_hrs, wind_speed,\n                                                  atmospheric_pressure, use_disc)\n\n        # assemble the results into DataCollections\n        metadata = {'source': location.source, 'country': location.country,\n                    'city': location.city}\n        direct_norm_rad, diffuse_horiz_rad = \\\n            cls._get_data_collections(dir_ir, diff_ir, metadata, timestep, is_leap_year)\n\n        return cls(location, direct_norm_rad, diffuse_horiz_rad, timestep, is_leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the global horizontal irradiance at each timestep.", "response": "def global_horizontal_irradiance(self):\n        \"\"\"Returns the global horizontal irradiance at each timestep.\"\"\"\n        analysis_period = AnalysisPeriod(timestep=self.timestep,\n                                         is_leap_year=self.is_leap_year)\n        header_ghr = Header(data_type=GlobalHorizontalIrradiance(),\n                            unit='W/m2',\n                            analysis_period=analysis_period,\n                            metadata=self.metadata)\n        glob_horiz = []\n        sp = Sunpath.from_location(self.location)\n        sp.is_leap_year = self.is_leap_year\n        for dt, dnr, dhr in zip(self.datetimes, self.direct_normal_irradiance,\n                                self.diffuse_horizontal_irradiance):\n            sun = sp.calculate_sun_from_date_time(dt)\n            glob_horiz.append(dhr + dnr * math.sin(math.radians(sun.altitude)))\n        return HourlyContinuousCollection(header_ghr, glob_horiz)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the direct irradiance on a horizontal surface at each timestep.", "response": "def direct_horizontal_irradiance(self):\n        \"\"\"Returns the direct irradiance on a horizontal surface at each timestep.\n\n        Note that this is different from the direct_normal_irradiance needed\n        to construct a Wea, which is NORMAL and not HORIZONTAL.\"\"\"\n        analysis_period = AnalysisPeriod(timestep=self.timestep,\n                                         is_leap_year=self.is_leap_year)\n        header_dhr = Header(data_type=DirectHorizontalIrradiance(),\n                            unit='W/m2',\n                            analysis_period=analysis_period,\n                            metadata=self.metadata)\n        direct_horiz = []\n        sp = Sunpath.from_location(self.location)\n        sp.is_leap_year = self.is_leap_year\n        for dt, dnr in zip(self.datetimes, self.direct_normal_irradiance):\n            sun = sp.calculate_sun_from_date_time(dt)\n            direct_horiz.append(dnr * math.sin(math.radians(sun.altitude)))\n        return HourlyContinuousCollection(header_dhr, direct_horiz)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a tuple of datetimes based on timestep.", "response": "def _get_datetimes(timestep, is_leap_year):\n        \"\"\"List of datetimes based on timestep.\n\n        This method should only be used for classmethods. For datetimes use datetiems or\n        hoys methods.\n        \"\"\"\n        hour_count = 8760 + 24 if is_leap_year else 8760\n        adjust_time = 30 if timestep == 1 else 0\n        return tuple(\n            DateTime.from_moy(60.0 * count / timestep + adjust_time, is_leap_year)\n            for count in xrange(hour_count * timestep)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_data_collections(dnr_values, dhr_values, metadata, timestep, is_leap_year):\n        analysis_period = AnalysisPeriod(timestep=timestep, is_leap_year=is_leap_year)\n        dnr_header = Header(data_type=DirectNormalIrradiance(),\n                            unit='W/m2',\n                            analysis_period=analysis_period,\n                            metadata=metadata)\n        direct_norm_rad = HourlyContinuousCollection(dnr_header, dnr_values)\n        dhr_header = Header(data_type=DiffuseHorizontalIrradiance(),\n                            unit='W/m2',\n                            analysis_period=analysis_period,\n                            metadata=metadata)\n        diffuse_horiz_rad = HourlyContinuousCollection(dhr_header, dhr_values)\n\n        return direct_norm_rad, diffuse_horiz_rad", "response": "Return two data collections for Direct Normal Diffuse Horizontal and Direct Horiz."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_irradiance_value(self, month, day, hour):\n        dt = DateTime(month, day, hour, leap_year=self.is_leap_year)\n        count = int(dt.hoy * self.timestep)\n        return self.direct_normal_irradiance[count], \\\n            self.diffuse_horizontal_irradiance[count]", "response": "Get direct and diffuse irradiance values for a point in time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_irradiance_value_for_hoy(self, hoy):\n        count = int(hoy * self.timestep)\n        return self.direct_normal_irradiance[count], \\\n            self.diffuse_horizontal_irradiance[count]", "response": "Get direct and diffuse irradiance values for an hoy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the irradiance components facing a given altitude and azimuth. This method computes unobstructed solar flux facing a given altitude and azimuth. The default is set to return the golbal horizontal irradiance, assuming an altitude facing straight up (90 degrees). Args: altitude: A number between -90 and 90 that represents the altitude at which irradiance is being evaluated in degrees. azimuth: A number between 0 and 360 that represents the azimuth at wich irradiance is being evaluated in degrees. ground_reflectance: A number between 0 and 1 that represents the reflectance of the ground. Default is set to 0.2. Some common ground reflectances are: urban: 0.18 grass: 0.20 fresh grass: 0.26 soil: 0.17 sand: 0.40 snow: 0.65 fresh_snow: 0.75 asphalt: 0.12 concrete: 0.30 sea: 0.06 isotrophic: A boolean value that sets whether an istotrophic sky is used (as opposed to an anisotrophic sky). An isotrophic sky assummes an even distribution of diffuse irradiance across the sky while an anisotrophic sky places more diffuse irradiance near the solar disc. Default is set to True for isotrophic Returns: total_irradiance: A data collection of total solar irradiance. direct_irradiance: A data collection of direct solar irradiance. diffuse_irradiance: A data collection of diffuse sky solar irradiance. reflected_irradiance: A data collection of ground reflected solar irradiance.", "response": "def directional_irradiance(self, altitude=90, azimuth=180,\n                               ground_reflectance=0.2, isotrophic=True):\n        \"\"\"Returns the irradiance components facing a given altitude and azimuth.\n\n        This method computes unobstructed solar flux facing a given\n        altitude and azimuth. The default is set to return the golbal horizontal\n        irradiance, assuming an altitude facing straight up (90 degrees).\n\n        Args:\n            altitude: A number between -90 and 90 that represents the\n                altitude at which irradiance is being evaluated in degrees.\n            azimuth: A number between 0 and 360 that represents the\n                azimuth at wich irradiance is being evaluated in degrees.\n            ground_reflectance: A number between 0 and 1 that represents the\n                reflectance of the ground. Default is set to 0.2. Some\n                common ground reflectances are:\n                    urban: 0.18\n                    grass: 0.20\n                    fresh grass: 0.26\n                    soil: 0.17\n                    sand: 0.40\n                    snow: 0.65\n                    fresh_snow: 0.75\n                    asphalt: 0.12\n                    concrete: 0.30\n                    sea: 0.06\n            isotrophic: A boolean value that sets whether an istotrophic sky is\n                used (as opposed to an anisotrophic sky). An isotrophic sky\n                assummes an even distribution of diffuse irradiance across the\n                sky while an anisotrophic sky places more diffuse irradiance\n                near the solar disc. Default is set to True for isotrophic\n\n        Returns:\n            total_irradiance: A data collection of total solar irradiance.\n            direct_irradiance: A data collection of direct solar irradiance.\n            diffuse_irradiance: A data collection of diffuse sky solar irradiance.\n            reflected_irradiance: A data collection of ground reflected solar irradiance.\n        \"\"\"\n        # function to convert polar coordinates to xyz.\n        def pol2cart(phi, theta):\n            mult = math.cos(theta)\n            x = math.sin(phi) * mult\n            y = math.cos(phi) * mult\n            z = math.sin(theta)\n            return Vector3(x, y, z)\n\n        # convert the altitude and azimuth to a normal vector\n        normal = pol2cart(math.radians(azimuth), math.radians(altitude))\n\n        # create sunpath and get altitude at every timestep of the year\n        direct_irr, diffuse_irr, reflected_irr, total_irr = [], [], [], []\n        sp = Sunpath.from_location(self.location)\n        sp.is_leap_year = self.is_leap_year\n        for dt, dnr, dhr in zip(self.datetimes, self.direct_normal_irradiance,\n                                self.diffuse_horizontal_irradiance):\n            sun = sp.calculate_sun_from_date_time(dt)\n            sun_vec = pol2cart(math.radians(sun.azimuth),\n                               math.radians(sun.altitude))\n            vec_angle = sun_vec.angle(normal)\n\n            # direct irradiance on surface\n            srf_dir = 0\n            if sun.altitude > 0 and vec_angle < math.pi / 2:\n                srf_dir = dnr * math.cos(vec_angle)\n\n            # diffuse irradiance on surface\n            if isotrophic is True:\n                srf_dif = dhr * ((math.sin(math.radians(altitude)) / 2) + 0.5)\n            else:\n                y = max(0.45, 0.55 + (0.437 * math.cos(vec_angle)) + 0.313 *\n                        math.cos(vec_angle) * 0.313 * math.cos(vec_angle))\n                srf_dif = self.dhr * (y * (\n                    math.sin(math.radians(abs(90 - altitude)))) +\n                    math.cos(math.radians(abs(90 - altitude))))\n\n            # reflected irradiance on surface.\n            e_glob = dhr + dnr * math.cos(math.radians(90 - sun.altitude))\n            srf_ref = e_glob * ground_reflectance * (0.5 - (math.sin(\n                math.radians(altitude)) / 2))\n\n            # add it all together\n            direct_irr.append(srf_dir)\n            diffuse_irr.append(srf_dif)\n            reflected_irr.append(srf_ref)\n            total_irr.append(srf_dir + srf_dif + srf_ref)\n\n        # create the headers\n        a_per = AnalysisPeriod(timestep=self.timestep, is_leap_year=self.is_leap_year)\n        direct_hea = diffuse_hea = reflected_hea = total_hea = \\\n            Header(Irradiance(), 'W/m2', a_per, self.metadata)\n\n        # create the data collections\n        direct_irradiance = HourlyContinuousCollection(direct_hea, direct_irr)\n        diffuse_irradiance = HourlyContinuousCollection(diffuse_hea, diffuse_irr)\n        reflected_irradiance = HourlyContinuousCollection(reflected_hea, reflected_irr)\n        total_irradiance = HourlyContinuousCollection(total_hea, total_irr)\n\n        return total_irradiance, direct_irradiance, \\\n            diffuse_irradiance, reflected_irradiance"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the wea header.", "response": "def header(self):\n        \"\"\"Wea header.\"\"\"\n        return \"place %s\\n\" % self.location.city + \\\n            \"latitude %.2f\\n\" % self.location.latitude + \\\n            \"longitude %.2f\\n\" % -self.location.longitude + \\\n            \"time_zone %d\\n\" % (-self.location.time_zone * 15) + \\\n            \"site_elevation %.1f\\n\" % self.location.elevation + \\\n            \"weather_data_file_units 1\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_json(self):\n        return {\n            'location': self.location.to_json(),\n            'direct_normal_irradiance':\n                self.direct_normal_irradiance.to_json(),\n            'diffuse_horizontal_irradiance':\n                self.diffuse_horizontal_irradiance.to_json(),\n            'timestep': self.timestep,\n            'is_leap_year': self.is_leap_year\n        }", "response": "Write wea to json file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the wea file.", "response": "def write(self, file_path, hoys=None, write_hours=False):\n        \"\"\"Write the wea file.\n\n        WEA carries irradiance values from epw and is what gendaymtx uses to\n        generate the sky.\n        \"\"\"\n        if not file_path.lower().endswith('.wea'):\n            file_path += '.wea'\n\n        # generate hoys in wea file based on timestep\n        full_wea = False\n        if not hoys:\n            hoys = self.hoys\n            full_wea = True\n\n        # write header\n        lines = [self.header]\n        if full_wea:\n            # there is no user input for hoys, write it for all the hours\n            for dir_rad, dif_rad, dt in zip(self.direct_normal_irradiance,\n                                            self.diffuse_horizontal_irradiance,\n                                            self.datetimes):\n                line = \"%d %d %.3f %d %d\\n\" \\\n                    % (dt.month, dt.day, dt.float_hour, dir_rad, dif_rad)\n                lines.append(line)\n        else:\n            # output wea based on user request\n            for hoy in hoys:\n                try:\n                    dir_rad, dif_rad = self.get_irradiance_value_for_hoy(hoy)\n                except IndexError:\n                    print('Warn: Wea data for {} is not available!'.format(dt))\n                    continue\n\n                dt = DateTime.from_hoy(hoy)\n                dt = dt.add_minute(30) if self.timestep == 1 else dt\n                line = \"%d %d %.3f %d %d\\n\" \\\n                    % (dt.month, dt.day, dt.float_hour, dir_rad, dif_rad)\n\n                lines.append(line)\n        file_data = ''.join(lines)\n        write_to_file(file_path, file_data, True)\n\n        if write_hours:\n            hrs_file_path = file_path[:-4] + '.hrs'\n            hrs_data = ','.join(str(h) for h in hoys) + '\\n'\n            write_to_file(hrs_file_path, hrs_data, True)\n\n        return file_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a flattened genertor from an input list.", "response": "def flatten(input_list):\n    \"\"\"Return a flattened genertor from an input list.\n\n    Usage:\n\n        input_list = [['a'], ['b', 'c', 'd'], [['e']], ['f']]\n        list(flatten(input_list))\n        >> ['a', 'b', 'c', 'd', 'e', 'f']\n    \"\"\"\n    for el in input_list:\n        if isinstance(el, collections.Iterable) \\\n                and not isinstance(el, basestring):\n            for sub in flatten(el):\n                yield sub\n        else:\n            yield el"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef color(self, value):\n        assert self._is_domain_set, \\\n            \"Domain is not set. Use self.domain to set the domain.\"\n\n        if self._ctype == 2:\n            # if ordinal map the value and color\n            try:\n                return self._colors[self._domain.index(value)]\n            except ValueError:\n                raise ValueError(\n                    \"%s is not a valid input for ordinal type.\\n\" % str(value) +\n                    \"List of valid values are %s\" % \";\".join(map(str, self._domain))\n                )\n\n        if value < self._domain[0]:\n            return self._colors[0]\n        if value > self._domain[-1]:\n            return self._colors[-1]\n\n        # find the index of the value in domain\n        for count, d in enumerate(self._domain):\n            if d <= value <= self._domain[count + 1]:\n                if self._ctype == 0:\n                    return self._cal_color(value, count)\n                if self._ctype == 1:\n                    return self._colors[count + 1]", "response": "Return color for an input value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cal_color(self, value, color_index):\n        range_min_p = self._domain[color_index]\n        range_p = self._domain[color_index + 1] - range_min_p\n        try:\n            factor = (value - range_min_p) / range_p\n        except ZeroDivisionError:\n            factor = 0\n\n        min_color = self.colors[color_index]\n        max_color = self.colors[color_index + 1]\n        red = round(factor * (max_color.r - min_color.r) + min_color.r)\n        green = round(factor * (max_color.g - min_color.g) + min_color.g)\n        blue = round(factor * (max_color.b - min_color.b) + min_color.b)\n\n        return Color(red, green, blue)", "response": "Blend between two colors based on input value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_location(cls, location, north_angle=0, daylight_saving_period=None):\n        location = Location.from_location(location)\n        return cls(location.latitude, location.longitude,\n                   location.time_zone, north_angle, daylight_saving_period)", "response": "Create a sun path from a LBlocation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the latitude value.", "response": "def latitude(self, value):\n        \"\"\"Set latitude value.\"\"\"\n        self._latitude = math.radians(float(value))\n        assert -self.PI / 2 <= self._latitude <= self.PI / 2, \\\n            \"latitude value should be between -90..90.\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef longitude(self, value):\n        self._longitude = math.radians(float(value))\n\n        # update time_zone\n        if abs((value / 15.0) - self.time_zone) > 1:\n            # if time_zone doesn't match the longitude update the time_zone\n            self.time_zone = value / 15.0", "response": "Set the longitude value in degrees."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_daylight_saving_hour(self, datetime):\n        if not self.daylight_saving_period:\n            return False\n        return self.daylight_saving_period.isTimeIncluded(datetime.hoy)", "response": "Check if a datetime is a daylight saving time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates sun data for a specific hour of the year.", "response": "def calculate_sun(self, month, day, hour, is_solar_time=False):\n        \"\"\"Get Sun data for an hour of the year.\n\n        Args:\n            month: An integer between 1-12\n            day: An integer between 1-31\n            hour: A positive number between 0..23\n            is_solar_time: A boolean to indicate if the input hour is solar time.\n                (Default: False)\n\n        Returns:\n            A sun object for this particular time\n        \"\"\"\n        datetime = DateTime(month, day, *self._calculate_hour_and_minute(hour),\n                            leap_year=self.is_leap_year)\n        return self.calculate_sun_from_date_time(datetime, is_solar_time)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the sun data for an hour of the year.", "response": "def calculate_sun_from_hoy(self, hoy, is_solar_time=False):\n        \"\"\"Get Sun data for an hour of the year.\n\n        Args:\n            datetime: Ladybug datetime\n            is_solar_time: A boolean to indicate if the input hour is solar time\n                (Default: False).\n\n        Returns:\n            A sun object for this particular time\n        \"\"\"\n        datetime = DateTime.from_hoy(hoy, self.is_leap_year)\n        return self.calculate_sun_from_date_time(datetime, is_solar_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_sun_from_date_time(self, datetime, is_solar_time=False):\n        # TODO(mostapha): This should be more generic and based on a method\n        if datetime.year != 2016 and self.is_leap_year:\n            datetime = DateTime(datetime.month, datetime.day, datetime.hour,\n                                datetime.minute, True)\n\n        sol_dec, eq_of_time = self._calculate_solar_geometry(datetime)\n\n        hour = datetime.float_hour\n\n        is_daylight_saving = self.is_daylight_saving_hour(datetime.hoy)\n\n        hour = hour + 1 if self.is_daylight_saving_hour(datetime.hoy) else hour\n\n        # minutes\n        sol_time = self._calculate_solar_time(hour, eq_of_time, is_solar_time) * 60\n\n        # degrees\n        if sol_time / 4 < 0:\n            hour_angle = sol_time / 4 + 180\n        else:\n            hour_angle = sol_time / 4 - 180\n\n        # Degrees\n        zenith = math.degrees(math.acos\n                              (math.sin(self._latitude) *\n                               math.sin(math.radians(sol_dec)) +\n                               math.cos(self._latitude) *\n                               math.cos(math.radians(sol_dec)) *\n                               math.cos(math.radians(hour_angle))))\n\n        altitude = 90 - zenith\n\n        # Approx Atmospheric Refraction\n        if altitude > 85:\n            atmos_refraction = 0\n        else:\n            if altitude > 5:\n                atmos_refraction = 58.1 / math.tan(math.radians(altitude))\n\n                - 0.07 / (math.tan(math.radians(altitude)))**3\n                + 0.000086 / (math.tan(math.radians(altitude)))**5\n            else:\n                if altitude > -0.575:\n                    atmos_refraction = 1735\n\n                    + altitude * (-518.2 + altitude *\n                                  (103.4 + altitude *\n                                   (-12.79 + altitude * 0.711)))\n                else:\n\n                    atmos_refraction = -20.772 / math.tan(\n                        math.radians(altitude))\n\n        atmos_refraction /= 3600\n\n        altitude += atmos_refraction\n\n        # Degrees\n        if hour_angle > 0:\n            azimuth = (math.degrees(\n                math.acos(\n                    (\n                        (math.sin(self._latitude) *\n                         math.cos(math.radians(zenith))) -\n                        math.sin(math.radians(sol_dec))) /\n                    (math.cos(self._latitude) *\n                     math.sin(math.radians(zenith)))\n                )\n            ) + 180) % 360\n        else:\n            azimuth = (540 - math.degrees(math.acos((\n                (math.sin(self._latitude) *\n                 math.cos(math.radians(zenith))) -\n                math.sin(math.radians(sol_dec))) /\n                (math.cos(self._latitude) *\n                 math.sin(math.radians(zenith))))\n            )) % 360\n\n        altitude = math.radians(altitude)\n        azimuth = math.radians(azimuth)\n        # create the sun for this hour\n        return Sun(datetime, altitude, azimuth, is_solar_time, is_daylight_saving,\n                   self.north_angle)", "response": "Calculate the sun for a particular date."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates sunrise sunset from a date and time.", "response": "def calculate_sunrise_sunset(self, month, day, depression=0.833,\n                                 is_solar_time=False):\n        \"\"\"Calculate sunrise, noon and sunset.\n\n        Return:\n            A dictionary. Keys are (\"sunrise\", \"noon\", \"sunset\")\n        \"\"\"\n        datetime = DateTime(month, day, hour=12, leap_year=self.is_leap_year)\n\n        return self.calculate_sunrise_sunset_from_datetime(datetime,\n                                                           depression,\n                                                           is_solar_time)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating sunrise sunset and noon for a given date.", "response": "def calculate_sunrise_sunset_from_datetime(self, datetime, depression=0.833,\n                                               is_solar_time=False):\n        \"\"\"Calculate sunrise, sunset and noon for a day of year.\"\"\"\n        # TODO(mostapha): This should be more generic and based on a method\n        if datetime.year != 2016 and self.is_leap_year:\n            datetime = DateTime(datetime.month, datetime.day, datetime.hour,\n                                datetime.minute, True)\n        sol_dec, eq_of_time = self._calculate_solar_geometry(datetime)\n        # calculate sunrise and sunset hour\n        if is_solar_time:\n            noon = .5\n        else:\n            noon = (720 -\n                    4 * math.degrees(self._longitude) -\n                    eq_of_time +\n                    self.time_zone * 60\n                    ) / 1440.0\n\n        try:\n            sunrise_hour_angle = self._calculate_sunrise_hour_angle(\n                sol_dec, depression)\n        except ValueError:\n            # no sun rise and sunset at this hour\n            noon = 24 * noon\n            return {\n                \"sunrise\": None,\n                \"noon\": DateTime(datetime.month, datetime.day,\n                                 *self._calculate_hour_and_minute(noon),\n                                 leap_year=self.is_leap_year),\n                \"sunset\": None\n            }\n        else:\n            sunrise = noon - sunrise_hour_angle * 4 / 1440.0\n            sunset = noon + sunrise_hour_angle * 4 / 1440.0\n            noon = 24 * noon\n            sunrise = 24 * sunrise\n            sunset = 24 * sunset\n\n            return {\n                \"sunrise\": DateTime(datetime.month, datetime.day,\n                                    *self._calculate_hour_and_minute(sunrise),\n                                    leap_year=self.is_leap_year),\n                \"noon\": DateTime(datetime.month, datetime.day,\n                                 *self._calculate_hour_and_minute(noon),\n                                 leap_year=self.is_leap_year),\n                \"sunset\": DateTime(datetime.month, datetime.day,\n                                   *self._calculate_hour_and_minute(sunset),\n                                   leap_year=self.is_leap_year)\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the solar geometry for an hour of the year.", "response": "def _calculate_solar_geometry(self, datetime):\n        \"\"\"Calculate Solar geometry for an hour of the year.\n\n        Attributes:\n            datetime: A Ladybug datetime\n\n        Returns:\n            Solar declination: Solar declination in radians\n            eq_of_time: Equation of time as minutes\n        \"\"\"\n        month = datetime.month\n        day = datetime.day\n        hour = datetime.hour\n        minute = datetime.minute\n        year = 2016 if self.is_leap_year else 2017\n\n        def find_fraction_of_24(hour, minute):\n            \"\"\"\n            This function calculates the fraction of the 24 hour\n            the provided time represents\n            1440 is total the number of minutes in a 24 hour cycle.\n            args\n                hour: Integer. Hour between 0 - 23\n                minute: Integer. Minute between 0 - 59\n            return: Float.\n                The fraction of the 24 hours the provided time represents\n            \"\"\"\n            return round((minute + hour * 60) / 1440.0, 2)\n\n        def days_from_010119(year, month, day):\n            \"\"\"\n            This function calculates the number of days from 01-01-1900 \\\n            to the provided date\n            args :\n                year: Integer. The year in the date\n                month: Integer. The month in the date\n                day: Integer. The date\n            return: The number of days from 01-01-1900 to the date provided\n            \"\"\"\n\n            # Making a list of years from the year 1900\n            years = range(1900, year)\n\n            def is_leap_year(year):\n                \"\"\"Determine whether a year is a leap year.\"\"\"\n                return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\n            # Number of days in a year are 366 if it is a leap year\n            days_in_year = []\n            for item in years:\n                if is_leap_year(item):\n                    days_in_year.append(366)\n                else:\n                    days_in_year.append(365)\n\n            # Making the total of all the days in preceding years\n            days_in_precending_years = 0\n            for days in days_in_year:\n                days_in_precending_years += days\n\n            if is_leap_year(year):\n                month_dict = {1: 31, 2: 29, 3: 31, 4: 30, 5: 31, 6: 30,\n                              7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n            else:\n                month_dict = {1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30,\n                              7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n\n            \"\"\"Making the total of all the days in preceding months\\\n            in the same year\"\"\"\n            keys = tuple(month_dict.keys())\n            days_in_precending_months = 0\n            for i in range(month - 1):\n                days_in_precending_months += month_dict[keys[i]]\n\n            return days_in_precending_years + days_in_precending_months + day + 1\n\n        julian_day = days_from_010119(year, month, day) + 2415018.5 + \\\n            find_fraction_of_24(hour, minute) - (float(self.time_zone) / 24)\n\n        julian_century = (julian_day - 2451545) / 36525\n\n        # degrees\n        geom_mean_long_sun = (280.46646 + julian_century *\n                              (36000.76983 + julian_century * 0.0003032)\n                              ) % 360\n        # degrees\n        geom_mean_anom_sun = 357.52911 + julian_century * \\\n            (35999.05029 - 0.0001537 * julian_century)\n\n        eccent_orbit = 0.016708634 - julian_century * \\\n            (0.000042037 + 0.0000001267 * julian_century)\n\n        sun_eq_of_ctr = math.sin(\n            math.radians(geom_mean_anom_sun)) * \\\n            (1.914602 - julian_century * (0.004817 + 0.000014 * julian_century)\n             ) +\\\n            math.sin(math.radians(2 * geom_mean_anom_sun)) * \\\n            (0.019993 - 0.000101 * julian_century) + \\\n            math.sin(math.radians(3 * geom_mean_anom_sun)) * \\\n            0.000289\n\n        # degrees\n        sun_true_long = geom_mean_long_sun + sun_eq_of_ctr\n\n        # degrees\n        sun_app_long = sun_true_long - 0.00569 - 0.00478 * \\\n            math.sin(math.radians(125.04 - 1934.136 * julian_century))\n\n        # degrees\n        mean_obliq_ecliptic = 23 + \\\n            (26 + ((21.448 - julian_century * (46.815 + julian_century *\n                                               (0.00059 - julian_century *\n                                                0.001813)))) / 60) / 60\n\n        # degrees\n        oblique_corr = mean_obliq_ecliptic + 0.00256 * \\\n            math.cos(math.radians(125.04 - 1934.136 * julian_century))\n\n        # RADIANS\n        sol_dec = math.degrees(math.asin(math.sin(math.radians(oblique_corr)) *\n                                         math.sin(math.radians(sun_app_long))))\n\n        var_y = math.tan(math.radians(oblique_corr / 2)) * \\\n            math.tan(math.radians(oblique_corr / 2))\n\n        # minutes\n        eq_of_time = 4 \\\n            * math.degrees(\n                var_y * math.sin(2 * math.radians(geom_mean_long_sun)) -\n                2 * eccent_orbit * math.sin(math.radians(geom_mean_anom_sun)) +\n                4 * eccent_orbit * var_y *\n                math.sin(math.radians(geom_mean_anom_sun)) *\n                math.cos(2 * math.radians(geom_mean_long_sun)) -\n                0.5 * (var_y ** 2) *\n                math.sin(4 * math.radians(geom_mean_long_sun)) -\n                1.25 * (eccent_orbit ** 2) *\n                math.sin(2 * math.radians(geom_mean_anom_sun))\n            )\n\n        return sol_dec, eq_of_time"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates hour angle for sunrise time in degrees.", "response": "def _calculate_sunrise_hour_angle(self, solar_dec, depression=0.833):\n        \"\"\"Calculate hour angle for sunrise time in degrees.\"\"\"\n\n        hour_angle_arg = math.degrees(math.acos(\n            math.cos(math.radians(90 + depression)) /\n            (math.cos(math.radians(self.latitude)) * math.cos(\n                math.radians(solar_dec))) -\n            math.tan(math.radians(self.latitude)) *\n            math.tan(math.radians(solar_dec))\n        ))\n\n        return hour_angle_arg"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calculate_solar_time(self, hour, eq_of_time, is_solar_time):\n        if is_solar_time:\n            return hour\n\n        return (\n            (hour * 60 + eq_of_time + 4 * math.degrees(self._longitude) -\n             60 * self.time_zone) % 1440) / 60", "response": "Calculate Solar time for an hour."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_sunpath(self,\n                     hoys=None,\n                     origin=None,\n                     scale=1, sun_scale=1, annual=True, rem_night=True):\n        \"\"\"Create sunpath geometry. \\\n        This method should only be used from the + libraries.\n\n        Args:\n            hoys: An optional list of hours of the year(default: None).\n            origin: Sunpath origin(default: (0, 0, 0)).\n            scale: Sunpath scale(default: 1).\n            sun_scale: Scale for the sun spheres(default: 1).\n            annual: Set to True to draw an annual sunpath.\n                Otherwise a daily sunpath is drawn.\n            rem_night: Remove suns which are under the horizon(night!).\n        Returns:\n            base_curves: A collection of curves for base plot.\n            analemma_curves: A collection of analemma_curves.\n            daily_curves: A collection of daily_curves.\n            suns: A list of suns.\n        \"\"\"\n        # check and make sure the call is coming from inside a plus library\n        assert ladybug.isplus, \\\n            '\"draw_sunpath\" method can only be used in the [+] libraries.'\n        hoys = hoys or ()\n        origin = origin or (0, 0, 0)\n        try:\n            origin = tuple(origin)\n        except TypeError as e:\n            # dynamo\n            try:\n                origin = origin.X, origin.Y, origin.Z\n            except AttributeError:\n                raise TypeError(str(e))\n\n        scale = scale or 1\n        sun_scale = sun_scale or 1\n        assert annual or hoys, 'For daily sunpath you need to provide at least one hour.'\n\n        radius = 200 * scale\n\n        # draw base circles and lines\n        base_curves = plus.base_curves(origin, radius, self.north_angle)\n        # draw analemma\n        # calculate date times for analemma curves\n        if annual:\n            asuns = self._analemma_suns()\n            analemma_curves = plus.analemma_curves(asuns, origin, radius)\n        else:\n            analemma_curves = ()\n\n        # add sun spheres\n        if hoys:\n            suns = tuple(self.calculate_sun_from_hoy(hour) for hour in hoys)\n        else:\n            suns = ()\n\n        if rem_night:\n            suns = tuple(sun for sun in suns if sun.is_during_day)\n\n        sun_geos = plus.sun_geometry(suns, origin, radius)\n\n        # draw daily sunpath\n        if annual:\n            dts = (DateTime(m, 21) for m in xrange(1, 13))\n        else:\n            dts = (sun.datetime for sun in suns)\n\n        dsuns = self._daily_suns(dts)\n        daily_curves = plus.daily_curves(dsuns, origin, radius)\n\n        SPGeo = namedtuple(\n            'SunpathGeo',\n            ('compass_curves',\n             'analemma_curves',\n             'daily_curves',\n             'suns',\n             'sun_geos'))\n\n        # return outputs\n        return SPGeo(base_curves, analemma_curves, daily_curves, suns, sun_geos)", "response": "This method creates a sunpath from the base curves and analemma curves."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck what the analemma position is for an hour.", "response": "def _analemma_position(self, hour):\n        \"\"\"Check what the analemma position is for an hour.\n\n        This is useful for calculating hours of analemma curves.\n\n        Returns:\n            -1 if always night,\n            0 if both day and night,\n            1 if always day.\n        \"\"\"\n        # check for 21 dec and 21 jun\n        low = self.calculate_sun(12, 21, hour).is_during_day\n        high = self.calculate_sun(6, 21, hour).is_during_day\n\n        if low and high:\n            return 1\n        elif low or high:\n            return 0\n        else:\n            return -1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _analemma_suns(self):\n        for h in xrange(0, 24):\n            if self._analemma_position(h) < 0:\n                continue\n            elif self._analemma_position(h) == 0:\n                chours = []\n                # this is an hour that not all the hours are day or night\n                prevhour = self.latitude <= 0\n                num_of_days = 8760 if not self.is_leap_year else 8760 + 24\n                for hoy in xrange(h, num_of_days, 24):\n                    thishour = self.calculate_sun_from_hoy(hoy).is_during_day\n                    if thishour != prevhour:\n                        if not thishour:\n                            hoy -= 24\n                        dt = DateTime.from_hoy(hoy, self.is_leap_year)\n                        chours.append((dt.month, dt.day, dt.hour))\n                    prevhour = thishour\n                tt = []\n                for hcount in range(int(len(chours) / 2)):\n                    st = chours[2 * hcount]\n                    en = chours[2 * hcount + 1]\n                    if self.latitude >= 0:\n                        tt = [self.calculate_sun(*st)] + \\\n                            [self.calculate_sun(st[0], d, h)\n                             for d in xrange(st[1] + 1, 29, 7)] + \\\n                            [self.calculate_sun(m, d, h)\n                             for m in xrange(st[0] + 1, en[0])\n                             for d in xrange(3, 29, 7)] + \\\n                            [self.calculate_sun(en[0], d, h)\n                             for d in xrange(3, en[1], 7)] + \\\n                            [self.calculate_sun(*en)]\n                    else:\n                        tt = [self.calculate_sun(*en)] + \\\n                            [self.calculate_sun(en[0], d, h)\n                             for d in xrange(en[1] + 1, 29, 7)] + \\\n                            [self.calculate_sun(m, d, h) for m in xrange(en[0] +\n                                                                         1, 13)\n                             for d in xrange(3, 29, 7)] + \\\n                            [self.calculate_sun(m, d, h) for m in xrange(1, st[0])\n                             for d in xrange(3, 29, 7)] + \\\n                            [self.calculate_sun(st[0], d, h)\n                             for d in xrange(3, st[1], 7)] + \\\n                            [self.calculate_sun(*st)]\n                    yield tt\n            else:\n                yield tuple(self.calculate_sun((m % 12) + 1, d, h)\n                            for m in xrange(0, 13) for d in (7, 14, 21))[:-2]", "response": "Calculate times that should be used for drawing analemma curves."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget sun curves for multiple days of the year.", "response": "def _daily_suns(self, datetimes):\n        \"\"\"Get sun curve for multiple days of the year.\"\"\"\n        for dt in datetimes:\n            # calculate sunrise sunset and noon\n            nss = self.calculate_sunrise_sunset(dt.month, dt.day)\n            dts = tuple(nss[k] for k in ('sunrise', 'noon', 'sunset'))\n            if dts[0] is None:\n                # circle\n                yield (self.calculate_sun(dt.month, dt.day, h) for h in (0, 12,\n                                                                         15)), \\\n                    False\n            else:\n                # Arc\n                yield (self.calculate_sun_from_date_time(dt) for dt in dts), True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _calculate_sun_vector(self):\n        z_axis = Vector3(0., 0., -1.)\n        x_axis = Vector3(1., 0., 0.)\n        north_vector = Vector3(0., 1., 0.)\n\n        # rotate north vector based on azimuth, altitude, and north\n        _sun_vector = north_vector \\\n            .rotate_around(x_axis, self.altitude_in_radians) \\\n            .rotate_around(z_axis, self.azimuth_in_radians) \\\n            .rotate_around(z_axis, math.radians(-1 * self.north_angle))\n\n        _sun_vector.normalize()\n        try:\n            _sun_vector.flip()\n        except AttributeError:\n            # euclid3\n            _sun_vector = Vector3(-1 * _sun_vector.x,\n                                  -1 * _sun_vector.y,\n                                  -1 * _sun_vector.z)\n\n        self._sun_vector = _sun_vector", "response": "Calculate sun vector for this sun."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef saturated_vapor_pressure(t_kelvin):\n\n    if t_kelvin >= 273.15:\n        # Calculate saturation vapor pressure above freezing\n        sig = 1 - (t_kelvin / 647.096)\n        sig_polynomial = (-7.85951783 * sig) + (1.84408259 * sig ** 1.5) + \\\n            (-11.7866487 * sig ** 3) + (22.6807411 * sig ** 3.5) + \\\n            (-15.9618719 * sig ** 4) + (1.80122502 * sig ** 7.5)\n        crit_temp = 647.096 / t_kelvin\n        exponent = crit_temp * sig_polynomial\n        p_ws = math.exp(exponent) * 22064000\n    else:\n        # Calculate saturation vapor pressure below freezing\n        theta = t_kelvin / 273.15\n        exponent = -13.928169 * (1 - theta ** -1.5) + \\\n            34.707823 * (1 - theta ** -1.25)\n        p_ws = math.exp(exponent) * 611.657\n    return p_ws", "response": "This function calculates the saturation vapor pressure at t_kelvin."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the humidity ratio at a given db temperature rel_humid and b_press.", "response": "def humid_ratio_from_db_rh(db_temp, rel_humid, b_press=101325):\n    \"\"\"Humidity Ratio (kg water/kg air) at a db_temp (C),\n    rel_humid (%) and Pressure b_press (Pa).\n\n    Note:\n        [1] Vaisala. (2013) Humidity Conversion Formulas:\n        Calculation Formulas for Humidity.\n        www.vaisala.com/Vaisala%20Documents/Application%20notes/Humidity_Conversion_Formulas_B210973EN-F.pdf\n    \"\"\"\n    # Find saturation pressure\n    p_ws = saturated_vapor_pressure(db_temp + 273.15)\n    # Calculate partial pressure\n    decrh = rel_humid * 0.01\n    p_w = decrh * p_ws\n    # Calculate humidity ratio\n    press_differ = b_press - p_w\n    constant = p_w * 0.621991\n    humidity_ratio = constant / press_differ\n    return humidity_ratio"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwets Bulb Temperature (C) at db_temp (C), Relative Humidity rh (%), and Pressure b_press (Pa). Note: [1] J. Sullivan and L. D. Sanders. \"Method for obtaining wet-bulb temperatures by modifying the psychrometric formula.\" Center for Experiment Design and Data Analysis. NOAA - National Oceanic and Atmospheric Administration. http://www.srh.noaa.gov/epz/?n=wxcalc_rh", "response": "def wet_bulb_from_db_rh(db_temp, rh, b_press=101325):\n    \"\"\"Wet Bulb Temperature (C) at db_temp (C),\n    Relative Humidity rh (%), and Pressure b_press (Pa).\n\n    Note:\n        [1] J. Sullivan and L. D. Sanders. \"Method for obtaining wet-bulb temperatures by\n        modifying the psychrometric formula.\" Center for Experiment Design and Data\n        Analysis. NOAA - National Oceanic and Atmospheric Administration.\n        http://www.srh.noaa.gov/epz/?n=wxcalc_rh\n    \"\"\"\n    es = 6.112 * math.e**((17.67 * db_temp) / (db_temp + 243.5))\n    e = (es * rh) / 100\n    t_w = 0\n    increse = 10.0\n    previoussign = 1\n    e_d = 1\n    while math.fabs(e_d) > 0.005:\n        e_wg = 6.112 * (math.e**((17.67 * t_w) / (t_w + 243.5)))\n        eg = e_wg - (b_press/100) * (db_temp - t_w) * 0.00066 * (1 + (0.00155 * t_w))\n        e_d = e - eg\n        if e_d == 0:\n            break\n        else:\n            if e_d < 0:\n                cursign = -1\n                if cursign != previoussign:\n                    previoussign = cursign\n                    increse = increse / 10\n                else:\n                    increse = increse\n            else:\n                cursign = 1\n                if cursign != previoussign:\n                    previoussign = cursign\n                    increse = increse/10\n                else:\n                    increse = increse\n        t_w = t_w + increse * previoussign\n    return t_w"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dew_point_from_db_rh(db_temp, rh):\n    es = 6.112 * math.e**((17.67 * db_temp) / (db_temp + 243.5))\n    e = (es * rh) / 100\n    td = (243.5 * math.log(e / 6.112)) / (17.67 - math.log(e / 6.112))\n    return td", "response": "This function calculates the dew point from the given temperature and relative humidity."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rel_humid_from_db_hr(db_temp, humid_ratio, b_press=101325):\n    pw = (humid_ratio * 1000 * b_press) / (621.9907 + (humid_ratio * 1000))\n    pws = saturated_vapor_pressure(db_temp + 273.15)\n    rel_humid = (pw / pws) * 100\n    return rel_humid", "response": "Calculate relative humid from db temperature and humid ratio."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate relative humidity at db_temp and enthalpy.", "response": "def rel_humid_from_db_enth(db_temp, enthalpy, b_press=101325):\n    \"\"\"Relative Humidity (%) at db_temp (C), enthalpy (kJ/kg)\n    and Pressure b_press (Pa).\n    \"\"\"\n    assert enthalpy >= 0, 'enthalpy must be' \\\n        'greater than 0. Got {}'.format(str(enthalpy))\n    hr = (enthalpy - (1.006 * db_temp)) / ((1.84 * db_temp) + 2501)\n    rel_humid = rel_humid_from_db_hr(db_temp, hr, b_press)\n    return rel_humid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rel_humid_from_db_dpt(db_temp, dew_pt):\n    pws_ta = saturated_vapor_pressure(db_temp + 273.15)\n    pws_td = saturated_vapor_pressure(dew_pt + 273.15)\n    rh = 100 * (pws_td / pws_ta)\n    return rh", "response": "Calculate relative humid from db temperature and dew_pt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rel_humid_from_db_wb(db_temp, wet_bulb, b_press=101325):\n    # Calculate saturation pressure.\n    p_ws = saturated_vapor_pressure(db_temp + 273.15)\n    p_ws_wb = saturated_vapor_pressure(wet_bulb + 273.15)\n    # calculate partial vapor pressure\n    p_w = p_ws_wb - (b_press * 0.000662 * (db_temp - wet_bulb))\n    # Calculate the relative humidity.\n    rel_humid = (p_w / p_ws) * 100\n    return rel_humid", "response": "Calculate relative humidity at db_temp wet_bulb and b_press."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dew_point_from_db_hr(db_temp, hr, b_press=101325):\n    rh = rel_humid_from_db_hr(db_temp, hr, b_press)\n    td = dew_point_from_db_rh(db_temp, rh)\n    return td", "response": "Returns the dew point at Temperature db_temp Humidity Ratio hr and Pressure b_press."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dew_point_from_db_enth(db_temp, enthlpy, b_press=101325):\n    rh = rel_humid_from_db_enth(db_temp, enthlpy, b_press)\n    td = dew_point_from_db_rh(db_temp, rh)\n    return td", "response": "Returns the dew point at Temperature db_temp enthlpy and Pressure b_press."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the dew point at Temperature db_temp wet_bulb and b_press.", "response": "def dew_point_from_db_wb(db_temp, wet_bulb, b_press=101325):\n    \"\"\"Dew Point Temperature (C) at Temperature db_temp (C), wet_bulb (C)\n    and Pressure b_press (Pa).\n    \"\"\"\n    rh = rel_humid_from_db_wb(db_temp, wet_bulb, b_press)\n    td = dew_point_from_db_rh(db_temp, rh)\n    return td"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning dry bulb temperature and humidity ratio at wet_bulb.", "response": "def db_temp_from_wb_rh(wet_bulb, rel_humid, b_press=101325):\n    \"\"\"Dry Bulb Temperature (C) and humidity_ratio at at wet_bulb (C),\n    rel_humid (%) and Pressure b_press (Pa).\n\n    Formula is only valid for rel_humid == 0 or rel_humid == 100.\n    \"\"\"\n    assert rel_humid == 0 or rel_humid == 100, 'formula is only valid for' \\\n        ' rel_humid == 0 or rel_humid == 100'\n    humidity_ratio = humid_ratio_from_db_rh(wet_bulb, rel_humid, b_press)\n    hr_saturation = humid_ratio_from_db_rh(wet_bulb, 100, b_press)\n    db_temp = wet_bulb + (((hr_saturation - humidity_ratio) * 2260000) / (1005))\n    return db_temp, humidity_ratio"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_json(cls, data):\n        required_keys = ('location', 'design_days')\n        for key in required_keys:\n            assert key in data, 'Required key \"{}\" is missing!'.format(key)\n\n        return cls(Location.from_json(data['location']),\n                   [DesignDay.from_json(des_day) for des_day in data['design_days']])", "response": "Create a DDY from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, file_path):\n        # write all data into the file\n        # write the file\n        data = self.location.ep_style_location_string + '\\n\\n'\n        for d_day in self.design_days:\n            data = data + d_day.ep_style_string + '\\n\\n'\n        write_to_file(file_path, data, True)", "response": "Save the ddy object as a. ddy file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of ddys that have a certain keyword in their name.", "response": "def filter_by_keyword(self, keyword):\n        \"\"\"Return a list of ddys that have a certain keyword in their name.\n\n        This is useful for selecting out design days from a ddy file that are\n        for a specific type of condition (for example, .4% cooling design days)\n        \"\"\"\n        filtered_days = []\n        for des_day in self.design_days:\n            if keyword in des_day.name:\n                filtered_days.append(des_day)\n        return filtered_days"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_json(self):\n        return {\n            'location': self.location.to_json(),\n            'design_days': [des_d.to_json() for des_d in self.design_days]\n        }", "response": "Convert the Design Day to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a Design Day from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Create a Design Day from a dictionary.\n\n        Args:\n            data = {\n            \"name\": string,\n            \"day_type\": string,\n            \"location\": ladybug Location schema,\n            \"dry_bulb_condition\": ladybug DryBulbCondition schema,\n            \"humidity_condition\": ladybug HumidityCondition schema,\n            \"wind_condition\": ladybug WindCondition schema,\n            \"sky_condition\": ladybug SkyCondition schema}\n        \"\"\"\n        required_keys = ('name', 'day_type', 'location', 'dry_bulb_condition',\n                         'humidity_condition', 'wind_condition', 'sky_condition')\n        for key in required_keys:\n            assert key in data, 'Required key \"{}\" is missing!'.format(key)\n\n        return cls(data['name'], data['day_type'], Location.from_json(data['location']),\n                   DryBulbCondition.from_json(data['dry_bulb_condition']),\n                   HumidityCondition.from_json(data['humidity_condition']),\n                   WindCondition.from_json(data['wind_condition']),\n                   SkyCondition.from_json(data['sky_condition']))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_design_day_properties(cls, name, day_type, location, analysis_period,\n                                   dry_bulb_max, dry_bulb_range, humidity_type,\n                                   humidity_value, barometric_p, wind_speed, wind_dir,\n                                   sky_model, sky_properties):\n        \"\"\"Create a design day object from various key properties.\n\n        Args:\n            name: A text string to set the name of the design day\n            day_type: Choose from 'SummerDesignDay', 'WinterDesignDay' or other\n                EnergyPlus days\n            location: Location for the design day\n            analysis_period: Analysis period for the design day\n            dry_bulb_max: Maximum dry bulb temperature over the design day (in C).\n            dry_bulb_range: Dry bulb range over the design day (in C).\n            humidity_type: Type of humidity to use. Choose from\n                Wetbulb, Dewpoint, HumidityRatio, Enthalpy\n            humidity_value: The value of the condition above.\n            barometric_p: Barometric pressure in Pa.\n            wind_speed: Wind speed over the design day in m/s.\n            wind_dir: Wind direction over the design day in degrees.\n            sky_model: Type of solar model to use.  Choose from\n                ASHRAEClearSky, ASHRAETau\n            sky_properties: A list of properties describing the sky above.\n                For ASHRAEClearSky this is a single value for clearness\n                For ASHRAETau, this is the tau_beam and tau_diffuse\n        \"\"\"\n        dry_bulb_condition = DryBulbCondition(\n            dry_bulb_max, dry_bulb_range)\n        humidity_condition = HumidityCondition(\n            humidity_type, humidity_value, barometric_p)\n        wind_condition = WindCondition(\n            wind_speed, wind_dir)\n        if sky_model == 'ASHRAEClearSky':\n            sky_condition = OriginalClearSkyCondition.from_analysis_period(\n                analysis_period, sky_properties[0])\n        elif sky_model == 'ASHRAETau':\n            sky_condition = RevisedClearSkyCondition.from_analysis_period(\n                analysis_period, sky_properties[0], sky_properties[-1])\n        return cls(name, day_type, location, dry_bulb_condition,\n                   humidity_condition, wind_condition, sky_condition)", "response": "Create a new DesignDay object from various key properties."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_ashrae_dict_heating(cls, ashrae_dict, location,\n                                 use_990=False, pressure=None):\n        \"\"\"Create a heating design day object from a ASHRAE HOF dictionary.\n\n        Args:\n            ashrae_dict: A dictionary with 15 keys that match those in the\n                heating_keys property of this object. Each key should\n                correspond to a value.\n            location: Location object for the design day\n            use_990: Boolean to denote what type of design day to create\n                (wether it is a 99.0% or a 99.6% design day).  Default is\n                False for a 99.6% annual heating design day\n            pressure: Atmospheric pressure in Pa that should be used in the\n                creation of the humidity condition. Default is 101325 Pa\n                for pressure at sea level.\n        \"\"\"\n        db_key = 'DB996' if use_990 is False else 'DB990'\n        perc_str = '99.6' if use_990 is False else '99.0'\n        pressure = pressure if pressure is not None else 101325\n        db_cond = DryBulbCondition(float(ashrae_dict[db_key]), 0)\n        hu_cond = HumidityCondition('Wetbulb', float(ashrae_dict[db_key]), pressure)\n        ws_cond = WindCondition(float(ashrae_dict['WS_DB996']),\n                                float(ashrae_dict['WD_DB996']))\n        sky_cond = OriginalClearSkyCondition(int(ashrae_dict['Month']), 21, 0)\n        name = '{}% Heating Design Day for {}'.format(perc_str, location.city)\n        return cls(name, 'WinterDesignDay', location,\n                   db_cond, hu_cond, ws_cond, sky_cond)", "response": "Create a Heating Design Day object from an ASHRAE HOF dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_ashrae_dict_cooling(cls, ashrae_dict, location,\n                                 use_010=False, pressure=None, tau=None):\n        \"\"\"Create a heating design day object from a ASHRAE HOF dictionary.\n\n        Args:\n            ashrae_dict: A dictionary with 32 keys that match those in the\n                cooling_keys property of this object. Each key should\n                correspond to a value.\n            location: Location object for the design day\n            use_010: Boolean to denote what type of design day to create\n                (wether it is a 1.0% or a 0.4% design day).  Default is\n                False for a 0.4% annual cooling design day\n            pressure: Atmospheric pressure in Pa that should be used in the\n                creation of the humidity condition. Default is 101325 Pa\n                for pressure at sea level.\n            tau: Optional tuple containing two values, which will set the\n                sky condition to be a revised ASHRAE clear sky (Tau model).\n                The first item of the tuple should be the tau beam value\n                and the second is for the tau diffuse value.  Default is\n                None, which will default to the original ASHRAE Clear Sky.\n        \"\"\"\n        db_key = 'DB004' if use_010 is False else 'DB010'\n        wb_key = 'WB_DB004' if use_010 is False else 'WB_DB010'\n        perc_str = '0.4' if use_010 is False else '1.0'\n        pressure = pressure if pressure is not None else 101325\n        db_cond = DryBulbCondition(float(ashrae_dict[db_key]), float(ashrae_dict['DBR']))\n        hu_cond = HumidityCondition('Wetbulb', float(ashrae_dict[wb_key]), pressure)\n        ws_cond = WindCondition(float(ashrae_dict['WS_DB004']),\n                                float(ashrae_dict['WD_DB004']))\n        month_num = int(ashrae_dict['Month'])\n        if tau is not None:\n            sky_cond = RevisedClearSkyCondition(month_num, 21, tau[0], tau[1])\n        else:\n            sky_cond = OriginalClearSkyCondition(month_num, 21)\n        name = '{}% Cooling Design Day for {}'.format(perc_str, location.city)\n        return cls(name, 'SummerDesignDay', location,\n                   db_cond, hu_cond, ws_cond, sky_cond)", "response": "Create an ASHRAE Heating Design Day object from an ASHRAE HOF dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserializing object to an EnerygPlus SizingPeriod string.", "response": "def ep_style_string(self):\n        \"\"\"Serialize object to an EnerygPlus SizingPeriod:DesignDay.\n\n        Returns:\n            ep_string: A full string representing a SizingPeriod:DesignDay.\n        \"\"\"\n        # Put together the values in the order that they exist in the ddy file\n        ep_vals = [self.name,\n                   self.sky_condition.month,\n                   self.sky_condition.day_of_month,\n                   self.day_type,\n                   self.dry_bulb_condition.dry_bulb_max,\n                   self.dry_bulb_condition.dry_bulb_range,\n                   self.dry_bulb_condition.modifier_type,\n                   self.dry_bulb_condition.modifier_schedule,\n                   self.humidity_condition.hum_type, '',\n                   self.humidity_condition.schedule, '', '',\n                   self.humidity_condition.wet_bulb_range,\n                   self.humidity_condition.barometric_pressure,\n                   self.wind_condition.wind_speed,\n                   self.wind_condition.wind_direction,\n                   self.wind_condition.rain,\n                   self.wind_condition.snow_on_ground,\n                   self.sky_condition.daylight_savings_indicator,\n                   self.sky_condition.solar_model,\n                   self.sky_condition.beam_shced,\n                   self.sky_condition.diff_sched, '', '', '']\n\n        # assign humidity values based on the type of criteria\n        if self.humidity_condition.hum_type == 'Wetbulb' or \\\n                self.humidity_condition.hum_type == 'Dewpoint':\n                    ep_vals[9] = self.humidity_condition.hum_value\n        elif self.humidity_condition.hum_type == 'HumidityRatio':\n            ep_vals[11] = self.humidity_condition.hum_value\n        elif self.humidity_condition.hum_type == 'Enthalpy':\n            ep_vals[12] = self.humidity_condition.hum_value\n\n        # assign sky condition values based on the solar model\n        if self.sky_condition.solar_model == 'ASHRAEClearSky':\n            ep_vals[25] = self.sky_condition.clearness\n        if self.sky_condition.solar_model == 'ASHRAETau':\n            ep_vals[23] = self.sky_condition.tau_b\n            ep_vals[24] = self.sky_condition._tau_d\n            ep_vals.pop()\n\n        # put everything together into one string\n        comented_str = ['  {},{}{}\\n'.format(\n            str(val), ' ' * (60 - len(str(val))), self.comments[i])\n                        for i, val in enumerate(ep_vals)]\n        comented_str[-1] = comented_str[-1].replace(',', ';')\n        comented_str.insert(0, 'SizingPeriod:DesignDay,\\n')\n        comented_str.append('\\n')\n\n        return ''.join(comented_str)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analysis_period(self):\n        return AnalysisPeriod(\n            self.sky_condition.month,\n            self.sky_condition.day_of_month,\n            0,\n            self.sky_condition.month,\n            self.sky_condition.day_of_month,\n            23)", "response": "The analysisperiod of the design day."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hourly_dew_point(self):\n        dpt_data = self._humidity_condition.hourly_dew_point_values(\n            self._dry_bulb_condition)\n        return self._get_daily_data_collections(\n            temperature.DewPointTemperature(), 'C', dpt_data)", "response": "A data collection containing hourly dew points over they day."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn three data collections containing hourly direct normal diffuse horizontal and global horizontal radiation.", "response": "def hourly_solar_radiation(self):\n        \"\"\"Three data collections containing hourly direct normal, diffuse horizontal,\n        and global horizontal radiation.\n        \"\"\"\n        dir_norm, diff_horiz, glob_horiz = \\\n            self._sky_condition.radiation_values(self._location)\n\n        dir_norm_data = self._get_daily_data_collections(\n            energyintensity.DirectNormalRadiation(), 'Wh/m2', dir_norm)\n        diff_horiz_data = self._get_daily_data_collections(\n            energyintensity.DiffuseHorizontalRadiation(), 'Wh/m2', diff_horiz)\n        glob_horiz_data = self._get_daily_data_collections(\n            energyintensity.GlobalHorizontalRadiation(), 'Wh/m2', glob_horiz)\n\n        return dir_norm_data, diff_horiz_data, glob_horiz_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hourly_horizontal_infrared(self):\n        sky_cover = self._sky_condition.hourly_sky_cover\n        db_temp = self._dry_bulb_condition.hourly_values\n        dp_temp = self._humidity_condition.hourly_dew_point_values(\n            self._dry_bulb_condition)\n\n        horiz_ir = []\n        for i in xrange(len(sky_cover)):\n            horiz_ir.append(\n                calc_horizontal_infrared(sky_cover[i], db_temp[i], dp_temp[i]))\n\n        return self._get_daily_data_collections(\n            energyflux.HorizontalInfraredRadiationIntensity(), 'W/m2', horiz_ir)", "response": "A data collection containing hourly horizontal infrared intensity in W / m2."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an empty data collection.", "response": "def _get_daily_data_collections(self, data_type, unit, values):\n        \"\"\"Return an empty data collection.\"\"\"\n        data_header = Header(data_type=data_type, unit=unit,\n                             analysis_period=self.analysis_period,\n                             metadata={'source': self._location.source,\n                                       'country': self._location.country,\n                                       'city': self._location.city})\n        return HourlyContinuousCollection(data_header, values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(self):\n        return {\n            'name': self.name,\n            'day_type': self.day_type,\n            'location': self.location.to_json(),\n            'dry_bulb_condition': self.dry_bulb_condition.to_json(),\n            'humidity_condition': self.humidity_condition.to_json(),\n            'wind_condition': self.wind_condition.to_json(),\n            'sky_condition': self.sky_condition.to_json()\n        }", "response": "Convert the Design Day to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_json(self):\n        return {\n            'dry_bulb_max': self.dry_bulb_max,\n            'dry_bulb_range': self.dry_bulb_range,\n            'modifier_type': self.modifier_type,\n            'modifier_schedule': self.modifier_schedule\n        }", "response": "Convert the Dry Bulb Condition to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_json(cls, data):\n        # Check required and optional keys\n        required_keys = ('hum_type', 'hum_value')\n        optional_keys = {'barometric_pressure': 101325,\n                         'schedule': '', 'wet_bulb_range': ''}\n        for key in required_keys:\n            assert key in data, 'Required key \"{}\" is missing!'.format(key)\n        for key, val in optional_keys.items():\n            if key not in data:\n                data[key] = val\n\n        return cls(data['hum_type'], data['hum_value'], data['barometric_pressure'],\n                   data['schedule'], data['wet_bulb_range'])", "response": "Create a Humidity Condition from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hourly_dew_point_values(self, dry_bulb_condition):\n        hourly_dew_point = []\n        max_dpt = self.dew_point(dry_bulb_condition.dry_bulb_max)\n        for db in dry_bulb_condition.hourly_values:\n            if db >= max_dpt:\n                hourly_dew_point.append(max_dpt)\n            else:\n                hourly_dew_point.append(db)\n        return hourly_dew_point", "response": "Get a list of dew points ( C ) at each hour over the design day."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dew_point(self, db):\n        if self._hum_type == 'Dewpoint':\n            return self._hum_value\n        elif self._hum_type == 'Wetbulb':\n            return dew_point_from_db_wb(\n                db, self._hum_value, self._barometric_pressure)\n        elif self._hum_type == 'HumidityRatio':\n            return dew_point_from_db_hr(\n                db, self._hum_value, self._barometric_pressure)\n        elif self._hum_type == 'Enthalpy':\n            return dew_point_from_db_enth(\n                db, self._hum_value / 1000,  self._barometric_pressure)", "response": "Get the dew point from the humidity ratio and humidity ratio of the humidity ratio of the humidity ratio over the day."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_json(self):\n        return {\n            'hum_type': self.hum_type,\n            'hum_value': self.hum_value,\n            'barometric_pressure': self.barometric_pressure,\n            'schedule': self.schedule,\n            'wet_bulb_range': self.wet_bulb_range,\n        }", "response": "Convert the Humidity Condition to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_json(cls, data):\n        # Check required and optional keys\n        optional_keys = {'wind_direction': 0, 'rain': False, 'snow_on_ground': False}\n        assert 'wind_speed' in data, 'Required key \"wind_speed\" is missing!'\n        for key, val in optional_keys.items():\n            if key not in data:\n                data[key] = val\n\n        return cls(data['wind_speed'], data['wind_direction'], data['rain'],\n                   data['snow_on_ground'])", "response": "Create a Wind Condition from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_json(self):\n        return {\n            'wind_speed': self.wind_speed,\n            'wind_direction': self.wind_direction,\n            'rain': self.rain,\n            'snow_on_ground': self.snow_on_ground\n        }", "response": "Convert the Wind Condition to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a Sky Condition from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Create a Sky Condition from a dictionary.\n\n        Args:\n            data = {\n            \"solar_model\": string,\n            \"month\": int,\n            \"day_of_month\": int,\n            \"daylight_savings_indicator\": string // \"Yes\" or \"No\"}\n        \"\"\"\n        # Check required and optional keys\n        required_keys = ('solar_model', 'month', 'day_of_month')\n        for key in required_keys:\n            assert key in data, 'Required key \"{}\" is missing!'.format(key)\n\n        if data['solar_model'] == 'ASHRAEClearSky':\n            return OriginalClearSkyCondition.from_json(data)\n        if data['solar_model'] == 'ASHRAETau':\n            return RevisedClearSkyCondition.from_json(data)\n\n        if 'daylight_savings_indicator' not in data:\n            data['daylight_savings_indicator'] = 'No'\n        optional_keys = ('beam_shced', 'diff_sched')\n        for key in optional_keys:\n            if key not in data:\n                data[key] = ''\n\n        return cls(data['month'], data['day_of_month'], data['clearness'],\n                   data['daylight_savings_indicator'],\n                   data['beam_shced'], data['diff_sched'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist of datetimes based on design day date and timestep.", "response": "def _get_datetimes(self, timestep=1):\n        \"\"\"List of datetimes based on design day date and timestep.\"\"\"\n        start_moy = DateTime(self._month, self._day_of_month).moy\n        if timestep == 1:\n            start_moy = start_moy + 30\n        num_moys = 24 * timestep\n        return tuple(\n            DateTime.from_moy(start_moy + (i * (1 / timestep) * 60))\n            for i in xrange(num_moys)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the Sky Condition to a dictionary.", "response": "def to_json(self):\n        \"\"\"Convert the Sky Condition to a dictionary.\"\"\"\n        return {\n            'solar_model': self.solar_model,\n            'month': self.month,\n            'day_of_month': self.day_of_month,\n            'daylight_savings_indicator': self.daylight_savings_indicator,\n            'beam_shced': self.beam_shced,\n            'diff_sched': self.diff_sched\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize a OriginalClearSkyCondition from an analysis_period", "response": "def from_analysis_period(cls, analysis_period, clearness=1,\n                             daylight_savings_indicator='No'):\n        \"\"\"\"Initialize a OriginalClearSkyCondition from an analysis_period\"\"\"\n        _check_analysis_period(analysis_period)\n        return cls(analysis_period.st_month, analysis_period.st_day, clearness,\n                   daylight_savings_indicator)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Sky Condition from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Create a Sky Condition from a dictionary.\n\n        Args:\n            data = {\n            \"solar_model\": string,\n            \"month\": int,\n            \"day_of_month\": int,\n            \"clearness\": float,\n            \"daylight_savings_indicator\": string // \"Yes\" or \"No\"}\n        \"\"\"\n        # Check required and optional keys\n        required_keys = ('solar_model', 'month', 'day_of_month', 'clearness')\n        for key in required_keys:\n            assert key in data, 'Required key \"{}\" is missing!'.format(key)\n        if 'daylight_savings_indicator' not in data:\n            data['daylight_savings_indicator'] = 'No'\n\n        return cls(data['month'], data['day_of_month'], data['clearness'],\n                   data['daylight_savings_indicator'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of driect normal diffuse horiz and global horiz rad at each timestep.", "response": "def radiation_values(self, location, timestep=1):\n        \"\"\"Lists of driect normal, diffuse horiz, and global horiz rad at each timestep.\n        \"\"\"\n        # create sunpath and get altitude at every timestep of the design day\n        sp = Sunpath.from_location(location)\n        altitudes = []\n        dates = self._get_datetimes(timestep)\n        for t_date in dates:\n            sun = sp.calculate_sun_from_date_time(t_date)\n            altitudes.append(sun.altitude)\n        dir_norm, diff_horiz = ashrae_clear_sky(\n            altitudes, self._month, self._clearness)\n        glob_horiz = [dhr + dnr * math.sin(math.radians(alt)) for\n                      alt, dnr, dhr in zip(altitudes, dir_norm, diff_horiz)]\n        return dir_norm, diff_horiz, glob_horiz"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the Sky Condition to a dictionary.", "response": "def to_json(self):\n        \"\"\"Convert the Sky Condition to a dictionary.\"\"\"\n        return {\n            'solar_model': self.solar_model,\n            'month': self.month,\n            'day_of_month': self.day_of_month,\n            'clearness': self.clearness,\n            'daylight_savings_indicator': self.daylight_savings_indicator\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing a RevisedClearSkyCondition from an analysis_period", "response": "def from_analysis_period(cls, analysis_period, tau_b, tau_d,\n                             daylight_savings_indicator='No'):\n        \"\"\"\"Initialize a RevisedClearSkyCondition from an analysis_period\"\"\"\n        _check_analysis_period(analysis_period)\n        return cls(analysis_period.st_month, analysis_period.st_day, tau_b, tau_d,\n                   daylight_savings_indicator)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_json(self):\n        return {\n            'solar_model': self.solar_model,\n            'month': self.month,\n            'day_of_month': self.day_of_month,\n            'tau_b': self.tau_b,\n            'tau_d': self.tau_d,\n            'daylight_savings_indicator': self.daylight_savings_indicator\n        }", "response": "Convert the Sky Condition to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Data Collection from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Create a Data Collection from a dictionary.\n\n        Args:\n            {\n                \"header\": A Ladybug Header,\n                \"values\": An array of values,\n                \"datetimes\": An array of datetimes,\n                \"validated_a_period\": Boolean for whether header analysis_period is valid\n            }\n        \"\"\"\n        assert 'header' in data, 'Required keyword \"header\" is missing!'\n        assert 'values' in data, 'Required keyword \"values\" is missing!'\n        assert 'datetimes' in data, 'Required keyword \"datetimes\" is missing!'\n        coll = cls(Header.from_json(data['header']), data['values'], data['datetimes'])\n        if 'validated_a_period' in data:\n            coll._validated_a_period = data['validated_a_period']\n        return coll"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the Data Collection to the input unit.", "response": "def convert_to_unit(self, unit):\n        \"\"\"Convert the Data Collection to the input unit.\"\"\"\n        self._values = self._header.data_type.to_unit(\n            self._values, unit, self._header.unit)\n        self._header._unit = unit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the Data Collection to IP units.", "response": "def convert_to_ip(self):\n        \"\"\"Convert the Data Collection to IP units.\"\"\"\n        self._values, self._header._unit = self._header.data_type.to_ip(\n                self._values, self._header.unit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the Data Collection to SI units.", "response": "def convert_to_si(self):\n        \"\"\"Convert the Data Collection to SI units.\"\"\"\n        self._values, self._header._unit = self._header.data_type.to_si(\n                self._values, self._header.unit)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Data Collection in the input unit.", "response": "def to_unit(self, unit):\n        \"\"\"Return a Data Collection in the input unit.\"\"\"\n        new_data_c = self.duplicate()\n        new_data_c.convert_to_unit(unit)\n        return new_data_c"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_in_data_type_range(self, raise_exception=True):\n        return self._header.data_type.is_in_range(\n            self._values, self._header.unit, raise_exception)", "response": "Check if the values are in the physically possible ranges for the data_type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of the x highest values of the Data Collection and their indices.", "response": "def get_highest_values(self, count):\n        \"\"\"Get a list of the the x highest values of the Data Collection and their indices.\n\n        This is useful for situations where one needs to know the times of\n        the year when the largest values of a data collection occur.  For example,\n        there is a European dayight code that requires an analysis for the hours\n        of the year with the greatest exterior illuminance level.  This method\n        can be used to help build a shcedule for such a study.\n\n        Args:\n            count: Integer representing the number of highest values to account for.\n\n        Returns:\n            highest_values: The n highest values in data list, ordered from\n                highest to lowest.\n            highest_values_index: Indicies of the n highest values in data\n                list, ordered from highest to lowest.\n        \"\"\"\n        count = int(count)\n        assert count <= len(self._values), \\\n            'count must be smaller than or equal to values length. {} > {}.'.format(\n                count, len(self._values))\n        assert count > 0, \\\n            'count must be greater than 0. Got {}.'.format(count)\n        highest_values = sorted(self._values, reverse=True)[0:count]\n        highest_values_index = sorted(list(xrange(len(self._values))),\n                                      key=lambda k: self._values[k],\n                                      reverse=True)[0:count]\n        return highest_values, highest_values_index"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_lowest_values(self, count):\n        count = int(count)\n        assert count <= len(self._values), \\\n            'count must be <= to Data Collection len. {} > {}.'.format(\n                count, len(self._values))\n        assert count > 0, \\\n            'count must be greater than 0. Got {}.'.format(count)\n        lowest_values = sorted(self._values)[0:count]\n        lowest_values_index = sorted(list(xrange(len(self._values))),\n                                     key=lambda k: self._values[k])[0:count]\n        return lowest_values, lowest_values_index", "response": "Get a list of the x lowest values of the Data Collection and their indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_percentile(self, percentile):\n        assert 0 <= percentile <= 100, \\\n            'percentile must be between 0 and 100. Got {}'.format(percentile)\n        return self._percentile(self._values, percentile)", "response": "Get a value representing a the input percentile of the data collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter the Data Collection based on a conditional statement.", "response": "def filter_by_conditional_statement(self, statement):\n        \"\"\"Filter the Data Collection based on a conditional statement.\n\n        Args:\n            statement: A conditional statement as a string (e.g. a > 25 and a%5 == 0).\n                The variable should always be named as 'a' (without quotations).\n\n        Return:\n            A new Data Collection containing only the filtered data\n        \"\"\"\n        _filt_values, _filt_datetimes = self._filter_by_statement(statement)\n        if self._enumeration is None:\n            self._get_mutable_enumeration()\n        col_obj = self._enumeration['mutable'][self._collection_type]\n        collection = col_obj(self.header.duplicate(), _filt_values, _filt_datetimes)\n        collection._validated_a_period = self._validated_a_period\n        return collection"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_by_pattern(self, pattern):\n        _filt_values, _filt_datetimes = self._filter_by_pattern(pattern)\n        if self._enumeration is None:\n            self._get_mutable_enumeration()\n        col_obj = self._enumeration['mutable'][self._collection_type]\n        collection = col_obj(self.header.duplicate(), _filt_values, _filt_datetimes)\n        collection._validated_a_period = self._validated_a_period\n        return collection", "response": "Filter the Data Collection based on a list of booleans."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if this Data Collection is aligned with another.", "response": "def is_collection_aligned(self, data_collection):\n        \"\"\"Check if this Data Collection is aligned with another.\n\n        Aligned Data Collections are of the same Data Collection class, have the\n        same number of values and have matching datetimes.\n\n        Args:\n            data_collection: The Data Collection which you want to test if this\n                collection is aligned with.\n\n        Return:\n            True if collections are aligned, False if not aligned\n        \"\"\"\n        if self._collection_type != data_collection._collection_type:\n            return False\n        elif len(self.values) != len(data_collection.values):\n            return False\n        elif self.datetimes != data_collection.datetimes:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_aligned_collection(self, value=0, data_type=None, unit=None, mutable=None):\n        # set up the header of the new collection\n        header = self._check_aligned_header(data_type, unit)\n\n        # set up the values of the new collection\n        values = self._check_aligned_value(value)\n\n        # get the correct base class for the aligned collection (mutable or immutable)\n        if mutable is None:\n            collection = self.__class__(header, values, self.datetimes)\n        else:\n            if self._enumeration is None:\n                self._get_mutable_enumeration()\n            if mutable is False:\n                col_obj = self._enumeration['immutable'][self._collection_type]\n            else:\n                col_obj = self._enumeration['mutable'][self._collection_type]\n            collection = col_obj(header, values, self.datetimes)\n        collection._validated_a_period = self._validated_a_period\n        return collection", "response": "Returns a new aligned data collection with this one composed of one repeated value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef duplicate(self):\n        collection = self.__class__(self.header.duplicate(), self.values, self.datetimes)\n        collection._validated_a_period = self._validated_a_period\n        return collection", "response": "Return a copy of the current Data Collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_json(self):\n        return {\n            'header': self.header.to_json(),\n            'values': self._values,\n            'datetimes': self.datetimes,\n            'validated_a_period': self._validated_a_period\n        }", "response": "Convert the object to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering a list of aligned data collections according to a conditional statement.", "response": "def filter_collections_by_statement(data_collections, statement):\n        \"\"\"Generate a filtered data collections according to a conditional statement.\n\n        Args:\n            data_collections: A list of aligned Data Collections to be evaluated\n                against the statement.\n            statement: A conditional statement as a string (e.g. a>25 and a%5==0).\n                The variable should always be named as 'a' (without quotations).\n\n        Return:\n            collections: A list of Data Collections that have been filtered based\n                on the statement.\n        \"\"\"\n        pattern = BaseCollection.pattern_from_collections_and_statement(\n            data_collections, statement)\n        collections = [coll.filter_by_pattern(pattern) for coll in data_collections]\n        return collections"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a list of booleans from a list of data collections and a conditional statement.", "response": "def pattern_from_collections_and_statement(data_collections, statement):\n        \"\"\"Generate a list of booleans from data collections and a conditional statement.\n\n        Args:\n            data_collections: A list of aligned Data Collections to be evaluated\n                against the statement.\n            statement: A conditional statement as a string (e.g. a>25 and a%5==0).\n                The variable should always be named as 'a' (without quotations).\n\n        Return:\n            pattern: A list of True/False booleans with the length of the\n                Data Collections where True meets the conditional statement\n                and False does not.\n        \"\"\"\n        BaseCollection.are_collections_aligned(data_collections)\n        correct_var = BaseCollection._check_conditional_statement(\n            statement, len(data_collections))\n\n        # replace the operators of the statement with non-alphanumeric characters\n        # necessary to avoid replacing the characters of the operators\n        num_statement_clean = BaseCollection._replace_operators(statement)\n\n        pattern = []\n        for i in xrange(len(data_collections[0])):\n            num_statement = num_statement_clean\n            # replace the variable names with their numerical values\n            for j, coll in enumerate(data_collections):\n                var = correct_var[j]\n                num_statement = num_statement.replace(var, str(coll[i]))\n            # put back the operators\n            num_statement = BaseCollection._restore_operators(num_statement)\n            pattern.append(eval(num_statement, {}))\n        return pattern"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntesting if a series of Data Collections are aligned with one another.", "response": "def are_collections_aligned(data_collections, raise_exception=True):\n        \"\"\"Test if a series of Data Collections are aligned with one another.\n\n        Aligned Data Collections are of the same Data Collection class, have the\n        same number of values and have matching datetimes.\n\n        Args:\n            data_collections: A list of Data Collections for which you want to\n                test if they are al aligned with one another.\n\n        Return:\n            True if collections are aligned, False if not aligned\n        \"\"\"\n        if len(data_collections) > 1:\n            first_coll = data_collections[0]\n            for coll in data_collections[1:]:\n                if not first_coll.is_collection_aligned(coll):\n                    if raise_exception is True:\n                        error_msg = '{} Data Collection is not aligned with '\\\n                            '{} Data Collection.'.format(\n                                first_coll.header.data_type, coll.header.data_type)\n                        raise ValueError(error_msg)\n                    return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_function_aligned(funct, data_collections, data_type, unit):\n        # check that all inputs are either data collections or floats\n        data_colls = []\n        for i, func_input in enumerate(data_collections):\n            if isinstance(func_input, BaseCollection):\n                data_colls.append(func_input)\n            else:\n                try:\n                    data_collections[i] = float(func_input)\n                except ValueError:\n                    raise TypeError('Expected a number or a Data Colleciton. '\n                                    'Got {}'.format(type(func_input)))\n\n        # run the function and return the result\n        if len(data_colls) == 0:\n            return funct(*data_collections)\n        else:\n            BaseCollection.are_collections_aligned(data_colls)\n            val_len = len(data_colls[0].values)\n            for i, col in enumerate(data_collections):\n                data_collections[i] = [col] * val_len if isinstance(col, float) else col\n            result = data_colls[0].get_aligned_collection(data_type=data_type, unit=unit)\n            for i in xrange(val_len):\n                result[i] = funct(*[col[i] for col in data_collections])\n            return result", "response": "Compute a function with a list of aligned data collections or individual values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_conditional_statement(statement, num_collections):\n        # Determine what the list of variables should be based on the num_collections\n        correct_var = list(ascii_lowercase)[:num_collections]\n\n        # Clean out the operators of the statement\n        st_statement = BaseCollection._remove_operators(statement)\n        parsed_st = [s for s in st_statement if s.isalpha()]\n\n        # Perform the check\n        for var in parsed_st:\n            if var not in correct_var:\n                raise ValueError(\n                    'Invalid conditional statement: {}\\n '\n                    'Statement should be a valid Python statement'\n                    ' and the variables should be named as follows: {}'.format(\n                        statement, ', '.join(correct_var))\n                    )\n        return correct_var", "response": "Method to check that the conditional statement is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter the data collection based on a conditional statement.", "response": "def _filter_by_statement(self, statement):\n        \"\"\"Filter the data collection based on a conditional statement.\"\"\"\n        self.__class__._check_conditional_statement(statement, 1)\n        _filt_values, _filt_datetimes = [], []\n        for i, a in enumerate(self._values):\n            if eval(statement, {'a': a}):\n                _filt_values.append(a)\n                _filt_datetimes.append(self.datetimes[i])\n        return _filt_values, _filt_datetimes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter the Data Collection based on a list of booleans.", "response": "def _filter_by_pattern(self, pattern):\n        \"\"\"Filter the Filter the Data Collection based on a list of booleans.\"\"\"\n        try:\n            _len = len(pattern)\n        except TypeError:\n            raise TypeError(\"pattern is not a list of Booleans. Got {}\".format(\n                type(pattern)))\n        _filt_values = [d for i, d in enumerate(self._values) if pattern[i % _len]]\n        _filt_datetimes = [d for i, d in enumerate(self.datetimes) if pattern[i % _len]]\n        return _filt_values, _filt_datetimes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_values(self, values):\n        assert isinstance(values, Iterable) and not \\\n            isinstance(values, (str, dict, bytes, bytearray)), \\\n            'values should be a list or tuple. Got {}'.format(type(values))\n        assert len(values) == len(self.datetimes), \\\n            'Length of values list must match length of datetimes list. {} != {}'.format(\n                len(values), len(self.datetimes))\n        assert len(values) > 0, 'Data Collection must include at least one value'", "response": "Check values whenever they come through the values setter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the header inputs whenever get_aligned_collection is called.", "response": "def _check_aligned_header(self, data_type, unit):\n        \"\"\"Check the header inputs whenever get_aligned_collection is called.\"\"\"\n        if data_type is not None:\n            assert isinstance(data_type, DataTypeBase), \\\n                'data_type must be a Ladybug DataType. Got {}'.format(type(data_type))\n            if unit is None:\n                unit = data_type.units[0]\n        else:\n            data_type = self.header.data_type\n            unit = unit or self.header.unit\n        return Header(data_type, unit, self.header.analysis_period, self.header.metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_aligned_value(self, value):\n        if isinstance(value, Iterable) and not isinstance(\n                value, (str, dict, bytes, bytearray)):\n            assert len(value) == len(self._values), \"Length of value ({}) must match \"\\\n                \"the length of this collection's values ({})\".format(\n                    len(value), len(self._values))\n            values = value\n        else:\n            values = [value] * len(self._values)\n        return values", "response": "Check the value input whenever get_aligned_collection is called."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the percentile of a list of values.", "response": "def _percentile(self, values, percent, key=lambda x: x):\n        \"\"\"Find the percentile of a list of values.\n\n        Args:\n            values: A list of values for which percentiles are desired\n            percent: A float value from 0 to 100 representing the requested percentile.\n            key: optional key function to compute value from each element of N.\n\n        Return:\n            The percentile of the values\n        \"\"\"\n        vals = sorted(values)\n        k = (len(vals) - 1) * (percent / 100)\n        f = math.floor(k)\n        c = math.ceil(k)\n        if f == c:\n            return key(vals[int(k)])\n        d0 = key(vals[int(f)]) * (c - k)\n        d1 = key(vals[int(c)]) * (k - f)\n        return d0 + d1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_ip(self, values, from_unit):\n        if from_unit in self.ip_units:\n            return values, from_unit\n        elif from_unit == 'tonne':\n            return self.to_unit(values, 'ton', from_unit), 'ton'\n        else:\n            return self.to_unit(values, 'lb', from_unit), 'lb'", "response": "Return values in IP and the units to which the values have been converted."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning values in SI and the units to which the values have been converted.", "response": "def to_si(self, values, from_unit):\n        \"\"\"Return values in SI and the units to which the values have been converted.\"\"\"\n        if from_unit in self.si_units:\n            return values, from_unit\n        elif from_unit == 'ton':\n            return self.to_unit(values, 'tonne', from_unit), 'tonne'\n        else:\n            return self.to_unit(values, 'kg', from_unit), 'kg'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new instance of the class from a dictionary.", "response": "def from_json(cls, data):\n        \"\"\"Creat datetime from a dictionary.\n\n        Args:\n            data: {\n                'month': A value for month between 1-12. (Defualt: 1)\n                'day': A value for day between 1-31. (Defualt: 1)\n                'hour': A value for hour between 0-23. (Defualt: 0)\n                'minute': A value for month between 0-59. (Defualt: 0)\n            }\n        \"\"\"\n        if 'month' not in data:\n            data['month'] = 1\n\n        if 'day' not in data:\n            data['day'] = 1\n\n        if 'hour' not in data:\n            data['hour'] = 0\n\n        if 'minute' not in data:\n            data['minute'] = 0\n\n        if 'year' not in data:\n            data['year'] = 2017\n\n        leap_year = True if int(data['year']) == 2016 else False\n        return cls(data['month'], data['day'], data['hour'], data['minute'], leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Ladybug Datetime object from an hour of the year.", "response": "def from_hoy(cls, hoy, leap_year=False):\n        \"\"\"Create Ladybug Datetime from an hour of the year.\n\n        Args:\n            hoy: A float value 0 <= and < 8760\n        \"\"\"\n        return cls.from_moy(round(hoy * 60), leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_moy(cls, moy, leap_year=False):\n        if not leap_year:\n            num_of_minutes_until_month = (0, 44640, 84960, 129600, 172800, 217440,\n                                          260640, 305280, 349920, 393120, 437760,\n                                          480960, 525600)\n        else:\n            num_of_minutes_until_month = (0, 44640, 84960 + 1440, 129600 + 1440,\n                                          172800 + 1440, 217440 + 1440, 260640 + 1440,\n                                          305280 + 1440, 349920 + 1440, 393120 + 1440,\n                                          437760 + 1440, 480960 + 1440, 525600 + 1440)\n        # find month\n        for monthCount in range(12):\n            if int(moy) < num_of_minutes_until_month[monthCount + 1]:\n                month = monthCount + 1\n                break\n        try:\n            day = int((moy - num_of_minutes_until_month[month - 1]) / (60 * 24)) + 1\n        except UnboundLocalError:\n            raise ValueError(\n                \"moy must be positive and smaller than 525600. Invalid input %d\" % (moy)\n            )\n        else:\n            hour = int((moy / 60) % 24)\n            minute = int(moy % 60)\n\n            return cls(month, day, hour, minute, leap_year)", "response": "Create a Ladybug Datetime object from a minute of the year."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Ladybug DateTime object from a string.", "response": "def from_date_time_string(cls, datetime_string, leap_year=False):\n        \"\"\"Create Ladybug DateTime from a DateTime string.\n\n        Usage:\n\n            dt = DateTime.from_date_time_string(\"31 Dec 12:00\")\n        \"\"\"\n        dt = datetime.strptime(datetime_string, '%d %b %H:%M')\n        return cls(dt.month, dt.day, dt.hour, dt.minute, leap_year)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate hour and minutes as integers from a float hour.", "response": "def _calculate_hour_and_minute(float_hour):\n        \"\"\"Calculate hour and minutes as integers from a float hour.\"\"\"\n        hour, minute = int(float_hour), int(round((float_hour - int(float_hour)) * 60))\n        if minute == 60:\n            return hour + 1, 0\n        else:\n            return hour, minute"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_minute(self, minute):\n        _moy = self.moy + int(minute)\n        return self.__class__.from_moy(_moy)", "response": "Create a new DateTime after the minutes are added."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting date time as a dictionary.", "response": "def to_json(self):\n        \"\"\"Get date time as a dictionary.\"\"\"\n        return {'year': self.year,\n                'month': self.month,\n                'day': self.day,\n                'hour': self.hour,\n                'minute': self.minute}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a scalebar to axesax", "response": "def add_scalebar(ax, matchx=True, matchy=True, hidex=True, hidey=True, unitsx='', unitsy='', scalex=1, scaley=1, **kwargs):\n    \"\"\" Add scalebars to axes\n    Adds a set of scale bars to *ax*, matching the size to the ticks of the plot\n    and optionally hiding the x and y axes\n    - ax : the axis to attach ticks to\n    - matchx,matchy : if True, set size of scale bars to spacing between ticks\n                    if False, size should be set using sizex and sizey params\n    - hidex,hidey : if True, hide x-axis and y-axis of parent\n    - **kwargs : additional arguments passed to AnchoredScaleBars\n    Returns created scalebar object\n    \"\"\"\n    def f(axis):\n        l = axis.get_majorticklocs()\n        return len(l)>1 and (l[1] - l[0])\n    \n    if matchx: kwargs['sizex'] = f(ax.xaxis)      \n    if matchy: kwargs['sizey'] = f(ax.yaxis)\n\n    if 'labelx' not in kwargs or kwargs['labelx'] is None:\n        kwargs['labelx'] = '%.3g %s'%(kwargs['sizex']*scalex,unitsx)\n    if 'labely' not in kwargs or kwargs['labely'] is None:\n        kwargs['labely'] = '%.3g %s'%(kwargs['sizey']*scaley,unitsy)\n\n    sb = AnchoredScaleBar(ax.transData, **kwargs)\n    ax.add_artist(sb)\n\n    if hidex : ax.xaxis.set_visible(False)\n    if hidey : ax.yaxis.set_visible(False)\n    if hidex and hidey: ax.set_frame_on(False)\n\n    return sb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plotTraces (include = None, timeRange = None, overlay = False, oneFigPer = 'cell', rerun = False, colors = None, ylim = None, axis='on', fontSize=12,\n    figSize = (10,8), saveData = None, saveFig = None, showFig = True): \n    ''' \n    Plot recorded traces\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of cells for which to plot \n            the recorded traces (default: [])\n        - timeRange ([start:stop]): Time range of spikes shown; if None shows all (default: None)\n        - overlay (True|False): Whether to overlay the data lines or plot in separate subplots (default: False)\n        - oneFigPer ('cell'|'trace'): Whether to plot one figure per cell (showing multiple traces) \n            or per trace (showing multiple cells) (default: 'cell')\n        - rerun (True|False): rerun simulation so new set of cells gets recorded (default: False)\n        - colors (list): List of normalized RGB colors to use for traces\n        - ylim (list): Y-axis limits\n        - axis ('on'|'off'): Whether to show axis or not; if not, then a scalebar is included (default: 'on')\n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure; \n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figure handles\n    '''\n    from .. import sim\n\n    print('Plotting recorded cell traces ...',oneFigPer)\n\n    if include is None:  # if none, record from whatever was recorded\n        if 'plotTraces' in sim.cfg.analysis and 'include' in sim.cfg.analysis['plotTraces']:\n            include = sim.cfg.analysis['plotTraces']['include'] + sim.cfg.recordCells\n        else:\n            include = sim.cfg.recordCells\n            \n    global colorList\n    if isinstance(colors, list): \n        colorList2 = colors\n    else:\n        colorList2 = colorList\n\n    # rerun simulation so new include cells get recorded from\n    if rerun: \n        cellsRecord = [cell.gid for cell in sim.getCellsList(include)]\n        for cellRecord in cellsRecord:\n            if cellRecord not in sim.cfg.recordCells:\n                sim.cfg.recordCells.append(cellRecord)\n        sim.setupRecording()\n        sim.simulate()\n\n    tracesList = list(sim.cfg.recordTraces.keys())\n    tracesList.sort()\n    cells, cellGids, _ = getCellsInclude(include)\n    gidPops = {cell['gid']: cell['tags']['pop'] for cell in cells}\n\n    # time range\n    if timeRange is None:\n        timeRange = [0,sim.cfg.duration]\n\n    recordStep = sim.cfg.recordStep\n\n    figs = {}\n    tracesData = []\n\n    # set font size\n    plt.rcParams.update({'font.size': fontSize})\n\n    # Plot one fig per trace for given cell list\n    def plotFigPerTrace(subGids):\n        for itrace, trace in enumerate(tracesList):\n            figs['_trace_'+str(trace)] = plt.figure(figsize=figSize) # Open a new figure\n            for igid, gid in enumerate(subGids):\n                # print('recordStep',recordStep)\n                if 'cell_'+str(gid) in sim.allSimData[trace]:\n                    fullTrace = sim.allSimData[trace]['cell_'+str(gid)]\n                    if isinstance(fullTrace, dict):\n                        if recordStep == 'adaptive':\n                            t = []\n                            t_indexes = []\n                            data = []\n                            lenData = []\n                            for key in list(fullTrace.keys()):\n                                t.append(np.array(sim.allSimData[trace]['cell_time_'+str(gid)][key]))\n                                t_indexes.append(t[-1].__ge__(timeRange[0]).__and__(t[-1].__le__(timeRange[1])))\n                                data.append(np.array(fullTrace[key])[t_indexes[-1]])\n                                lenData = len(data[-1])\n                                t[-1] = t[-1][t_indexes[-1]]\n                        else:\n                            data = [fullTrace[key][int(timeRange[0]/recordStep):int(timeRange[1]/recordStep)] for key in list(fullTrace.keys())]\n                            lenData = len(data[0])\n                            data = np.transpose(np.array(data))\n                            t = np.arange(timeRange[0], timeRange[1]+recordStep, recordStep)\n                    else:\n                        if recordStep == 'adaptive':\n                            t = np.array(sim.allSimData[trace]['cell_time_'+str(gid)])\n                            t_indexes = t.__ge__(timeRange[0]).__and__(t.__le__(timeRange[1]))\n                            data = np.array(sim.allSimData[trace]['cell_'+str(gid)])[t_indexes]\n                            lenData = len(data)\n                            t = t[t_indexes]\n                        else:\n                            data = np.array(fullTrace[int(timeRange[0]/recordStep):int(timeRange[1]/recordStep)])\n                            lenData = len(data)\n                            t = np.arange(timeRange[0], timeRange[1]+recordStep, recordStep)\n                    tracesData.append({'t': t, 'cell_'+str(gid)+'_'+trace: data})\n                    color = colorList2[igid%len(colorList2)]\n                    if not overlay:\n                        plt.subplot(len(subGids),1,igid+1)\n                        plt.ylabel(trace, fontsize=fontsiz)\n                    if recordStep == 'adaptive':\n                        if isinstance(data, list):\n                            for tl,dl in zip(t,data):\n                                plt.plot(tl[:len(dl)], dl, linewidth=1.5,\n                                         color=color, label='Cell %d, Pop %s '%(int(gid), gidPops[gid]))\n                        else:\n                            plt.plot(t, data, linewidth=1.5,\n                                     color=color, label='Cell %d, Pop %s '%(int(gid), gidPops[gid]))\n                            plt.plot(t, data, linewidth=1.5, color=color, label=trace)\n                    else:\n                        if isinstance(data, list):\n                            for tl,dl in zip(t,data):\n                                plt.plot(tl[:len(dl)], dl, linewidth=1.5,\n                                         color=color, label='Cell %d, Pop %s '%(int(gid), gidPops[gid]))\n                        else:\n                            plt.plot(t[:len(data)], data, linewidth=1.5,\n                                     color=color, label='Cell %d, Pop %s '%(int(gid), gidPops[gid]))\n                    plt.xlabel('Time (ms)', fontsize=fontsiz)\n                    plt.xlim(timeRange)\n                    if ylim: plt.ylim(ylim)\n                    plt.title('%s '%(trace))\n                    \n            if axis == 'off':  # if no axis, add scalebar\n                ax = plt.gca()\n                sizex =  (timeRange[1]-timeRange[0])/20.0\n                # yl = plt.ylim()\n                # plt.ylim(yl[0]-0.2*(yl[1]-yl[0]), yl[1])\n                add_scalebar(ax, hidex=False, hidey=True, matchx=False, matchy=True, sizex=sizex, sizey=None, \n                    unitsx='ms', unitsy='mV', scalex=1, scaley=1, loc=1, pad=-1, borderpad=0.5, sep=4, prop=None, barcolor=\"black\", barwidth=3)   \n                plt.axis(axis) \n\n            if overlay and len(subGids) < 20:\n                #maxLabelLen = 10\n                #plt.subplots_adjust(right=(0.9-0.012*maxLabelLen)) \n                #plt.legend(fontsize=fontsiz, bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0.)\n                plt.legend()  # PUT BACK!!!!!!\n                pass\n\n\n    # Plot one fig per cell\n    if oneFigPer == 'cell':\n        for gid in cellGids:\n            figs['_gid_'+str(gid)] = plt.figure(figsize=figSize) # Open a new figure\n            fontsiz = 12\n            for itrace, trace in enumerate(tracesList):\n                if 'cell_'+str(gid) in sim.allSimData[trace]:\n                    fullTrace = sim.allSimData[trace]['cell_'+str(gid)]\n                    if isinstance(fullTrace, dict):\n                        if recordStep == 'adaptive':\n                            t = []\n                            t_indexes = []\n                            data = []\n                            lenData = []\n                            for key in list(fullTrace.keys()):\n                                t.append(np.array(sim.allSimData[trace]['cell_time_'+str(gid)][key]))\n                                t_indexes.append(t[-1].__ge__(timeRange[0]).__and__(t[-1].__le__(timeRange[1])))\n                                data.append(np.array(fullTrace[key])[t_indexes[-1]])\n                                lenData = len(data[-1])\n                                t[-1] = t[-1][t_indexes[-1]]\n                        else:\n                                data = [fullTrace[key][int(timeRange[0]/recordStep):int(timeRange[1]/recordStep)] for key in list(fullTrace.keys())]\n                                lenData = len(data[0])\n                                data = np.transpose(np.array(data))\n                                t = np.arange(timeRange[0], timeRange[1]+recordStep, recordStep)\n                    else:\n                        if recordStep == 'adaptive':\n                            t = np.array(sim.allSimData[trace]['cell_time_'+str(gid)])\n                            t_indexes = t.__ge__(timeRange[0]).__and__(t.__le__(timeRange[1]))\n                            data = np.array(sim.allSimData[trace]['cell_'+str(gid)])[t_indexes]\n                            lenData = len(data)\n                            t = t[t_indexes]\n                        else:\n                            data = fullTrace[int(timeRange[0]/recordStep):int(timeRange[1]/recordStep)]\n                            lenData = len(data)\n                            t = np.arange(timeRange[0], timeRange[1]+recordStep, recordStep)\n                    tracesData.append({'t': t, 'cell_'+str(gid)+'_'+trace: data})\n                    color = colorList2[itrace%len(colorList2)]\n                    if not overlay:\n                        plt.subplot(len(tracesList),1,itrace+1)\n                        color = 'blue'\n                    if recordStep == 'adaptive':\n                        if isinstance(data, list) and isinstance(data[0], (list, np.array)):\n                            for tl,dl in zip(t,data):\n                                plt.plot(tl, dl, linewidth=1.5, color=color, label=trace)\n                        else:\n                            plt.plot(t, data, linewidth=1.5, color=color, label=trace)\n                    else:\n                        plt.plot(t[:lenData], data, linewidth=1.5, color=color, label=trace)\n                    plt.xlabel('Time (ms)', fontsize=fontsiz)\n                    plt.ylabel(trace, fontsize=fontsiz)\n                    plt.xlim(timeRange)\n                    if ylim: plt.ylim(ylim)\n                    if itrace==0: plt.title('Cell %d, Pop %s '%(int(gid), gidPops[gid]))\n                    \n            if overlay: \n                #maxLabelLen = 10\n                #plt.subplots_adjust(right=(0.9-0.012*maxLabelLen))\n                plt.legend()#fontsize=fontsiz, bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0.)\n\n            if axis == 'off':  # if no axis, add scalebar\n                ax = plt.gca()\n                sizex = timeRange[1]-timeRange[0]/20\n                yl = plt.ylim()\n                plt.ylim(yl[0]-0.2*(yl[1]-yl[0]), yl[1])  # leave space for scalebar?\n                add_scalebar(ax, hidex=False, hidey=True, matchx=False, matchy=True,  sizex=sizex, sizey=None, \n                    unitsx='ms', unitsy='mV', scalex=1, scaley=1, loc=4, pad=10, borderpad=0.5, sep=3, prop=None, barcolor=\"black\", barwidth=2)   \n                plt.axis(axis)                 \n            \n    # Plot one fig per trace\n    elif oneFigPer == 'trace':\n        plotFigPerTrace(cellGids)\n\n    # Plot one fig per trace for each population\n    elif oneFigPer == 'popTrace':\n        allPopGids = invertDictMapping(gidPops)\n        for popLabel, popGids in allPopGids.items():\n            plotFigPerTrace(popGids)\n\n\n    try:\n        plt.tight_layout()\n    except:\n        pass\n\n    #save figure data\n    if saveData:\n        figData = {'tracesData': tracesData, 'include': include, 'timeRange': timeRange, 'oneFigPer': oneFigPer,\n         'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n    \n        _saveFigData(figData, saveData, 'traces')\n \n    # save figure\n    if saveFig: \n        if isinstance(saveFig, basestring):\n            filename = saveFig\n        else:\n            filename = sim.cfg.filename+'_'+'traces.png'\n        if len(figs) > 1:\n            for figLabel, figObj in figs.items():\n                plt.figure(figObj.number)\n                plt.savefig(filename[:-4]+figLabel+filename[-4:])\n        else:\n            plt.savefig(filename)\n\n    # show fig \n    if showFig: _showFigure()\n\n    return figs, {'tracesData': tracesData, 'include': include}", "response": "Plot the recorded cell traces in a single figure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating all - to - all connections between all pre and post syn cells.", "response": "def fullConn (self, preCellsTags, postCellsTags, connParam):\n    from .. import sim\n\n    ''' Generates connections between all pre and post-syn cells '''\n    if sim.cfg.verbose: print('Generating set of all-to-all connections (rule: %s) ...' % (connParam['label']))\n\n    # get list of params that have a lambda function\n    paramsStrFunc = [param for param in [p+'Func' for p in self.connStringFuncParams] if param in connParam] \n\n    for paramStrFunc in paramsStrFunc:\n        # replace lambda function (with args as dict of lambda funcs) with list of values\n        connParam[paramStrFunc[:-4]+'List'] = {(preGid,postGid): connParam[paramStrFunc](**{k:v if isinstance(v, Number) else v(preCellTags,postCellTags) for k,v in connParam[paramStrFunc+'Vars'].items()})  \n            for preGid,preCellTags in preCellsTags.items() for postGid,postCellTags in postCellsTags.items()}\n    \n    for postCellGid in postCellsTags:  # for each postsyn cell\n        if postCellGid in self.gid2lid:  # check if postsyn is in this node's list of gids\n            for preCellGid, preCellTags in preCellsTags.items():  # for each presyn cell\n                self._addCellConn(connParam, preCellGid, postCellGid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef probConn (self, preCellsTags, postCellsTags, connParam):\n    from .. import sim\n\n    ''' Generates connections between all pre and post-syn cells based on probability values'''\n    if sim.cfg.verbose: print('Generating set of probabilistic connections (rule: %s) ...' % (connParam['label']))\n\n    allRands = self.generateRandsPrePost(preCellsTags, postCellsTags)\n\n    # get list of params that have a lambda function\n    paramsStrFunc = [param for param in [p+'Func' for p in self.connStringFuncParams] if param in connParam]\n\n    # copy the vars into args immediately and work out which keys are associated with lambda functions only once per method\n    funcKeys = {}\n    for paramStrFunc in paramsStrFunc:\n        connParam[paramStrFunc + 'Args'] = connParam[paramStrFunc + 'Vars'].copy()\n        funcKeys[paramStrFunc] = [key for key in connParam[paramStrFunc + 'Vars'] if callable(connParam[paramStrFunc + 'Vars'][key])]\n\n    # probabilistic connections with disynapticBias (deprecated)\n    if isinstance(connParam.get('disynapticBias', None), Number):  \n        allPreGids = sim._gatherAllCellConnPreGids()\n        prePreGids = {gid: allPreGids[gid] for gid in preCellsTags}\n        postPreGids = {gid: allPreGids[gid] for gid in postCellsTags}\n        \n        probMatrix = {(preCellGid,postCellGid): connParam['probabilityFunc'][preCellGid,postCellGid] if 'probabilityFunc' in connParam else connParam['probability']\n                                            for postCellGid,postCellTags in postCellsTags.items() # for each postsyn cell\n                                            for preCellGid, preCellTags in preCellsTags.items()  # for each presyn cell\n                                            if postCellGid in self.gid2lid}  # check if postsyn is in this node\n        \n        connGids = self._disynapticBiasProb2(probMatrix, allRands, connParam['disynapticBias'], prePreGids, postPreGids)\n        for preCellGid, postCellGid in connGids:\n            for paramStrFunc in paramsStrFunc: # call lambda functions to get weight func args\n                connParam[paramStrFunc+'Args'] = {k:v if isinstance(v, Number) else v(preCellsTags[preCellGid],postCellsTags[postCellGid]) for k,v in connParam[paramStrFunc+'Vars'].items()}  \n            self._addCellConn(connParam, preCellGid, postCellGid) # add connection\n\n    # standard probabilistic conenctions   \n    else:\n        # calculate the conn preGids of the each pre and post cell\n        # for postCellGid,postCellTags in sorted(postCellsTags.items()):  # for each postsyn cell\n        for postCellGid,postCellTags in postCellsTags.items():  # for each postsyn cell  # for each postsyn cell\n            if postCellGid in self.gid2lid:  # check if postsyn is in this node\n                for preCellGid, preCellTags in preCellsTags.items(): # for each presyn cell\n                    probability = connParam['probabilityFunc'][preCellGid,postCellGid] if 'probabilityFunc' in connParam else connParam['probability']\n                    if probability >= allRands[preCellGid,postCellGid]:\n                        for paramStrFunc in paramsStrFunc: # call lambda functions to get weight func args\n                            # update the relevant FuncArgs dict where lambda functions are known to exist in the corresponding FuncVars dict\n                            for funcKey in funcKeys[paramStrFunc]:\n                                connParam[paramStrFunc + 'Args'][funcKey] = connParam[paramStrFunc + 'Vars'][funcKey](preCellTags, postCellTags)\n                            # connParam[paramStrFunc+'Args'] = {k:v if isinstance(v, Number) else v(preCellTags,postCellTags) for k,v in connParam[paramStrFunc+'Vars'].items()}\n                        self._addCellConn(connParam, preCellGid, postCellGid)", "response": "Generates a new random undirected synphot connection between all pre and post - syn cells based on probability values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convConn (self, preCellsTags, postCellsTags, connParam):\n    from .. import sim\n\n    ''' Generates connections between all pre and post-syn cells based on probability values'''\n    if sim.cfg.verbose: print('Generating set of convergent connections (rule: %s) ...' % (connParam['label']))\n           \n    # get list of params that have a lambda function\n    paramsStrFunc = [param for param in [p+'Func' for p in self.connStringFuncParams] if param in connParam] \n\n    # copy the vars into args immediately and work out which keys are associated with lambda functions only once per method\n    funcKeys = {}\n    for paramStrFunc in paramsStrFunc:\n        connParam[paramStrFunc + 'Args'] = connParam[paramStrFunc + 'Vars'].copy()\n        funcKeys[paramStrFunc] = [key for key in connParam[paramStrFunc + 'Vars'] if callable(connParam[paramStrFunc + 'Vars'][key])]\n\n    # converted to list only once \n    preCellsTagsKeys = sorted(preCellsTags)\n\n    # calculate hash for post cell gids\n    hashPreCells = sim.hashList(preCellsTagsKeys)\n\n    for postCellGid,postCellTags in postCellsTags.items():  # for each postsyn cell\n        if postCellGid in self.gid2lid:  # check if postsyn is in this node\n            convergence = connParam['convergenceFunc'][postCellGid] if 'convergenceFunc' in connParam else connParam['convergence']  # num of presyn conns / postsyn cell\n            convergence = max(min(int(round(convergence)), len(preCellsTags)-1), 0)\n            self.rand.Random123(hashPreCells, postCellGid, sim.cfg.seeds['conn'])  # init randomizer\n            randSample = self.randUniqueInt(self.rand, convergence+1, 0, len(preCellsTags)-1)             \n\n            # note: randSample[divergence] is an extra value used only if one of the random postGids coincided with the preGid \n            preCellsSample = {preCellsTagsKeys[randSample[convergence]] if preCellsTagsKeys[i]==postCellGid else preCellsTagsKeys[i]:0\n                                   for i in randSample[0:convergence]}  # dict of selected gids of postsyn cells with removed post gid\n            preCellsConv = {k:v for k,v in preCellsTags.items() if k in preCellsSample}  # dict of selected presyn cells tags\n\n            for preCellGid, preCellTags in preCellsConv.items():  # for each presyn cell\n         \n                for paramStrFunc in paramsStrFunc: # call lambda functions to get weight func args\n                    # update the relevant FuncArgs dict where lambda functions are known to exist in the corresponding FuncVars dict\n                    for funcKey in funcKeys[paramStrFunc]:\n                        connParam[paramStrFunc + 'Args'][funcKey] = connParam[paramStrFunc+'Vars'][funcKey](preCellTags,postCellTags)\n\n                if preCellGid != postCellGid: # if not self-connection   \n                    self._addCellConn(connParam, preCellGid, postCellGid)", "response": "Generates a set of connections between all pre - syn cells and all post - syn cells based on probability values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef divConn (self, preCellsTags, postCellsTags, connParam):\n    from .. import sim\n\n    ''' Generates connections between all pre and post-syn cells based on probability values'''\n    if sim.cfg.verbose: print('Generating set of divergent connections (rule: %s) ...' % (connParam['label']))\n     \n    # get list of params that have a lambda function\n    paramsStrFunc = [param for param in [p+'Func' for p in self.connStringFuncParams] if param in connParam] \n\n    # copy the vars into args immediately and work out which keys are associated with lambda functions only once per method\n    funcKeys = {}\n    for paramStrFunc in paramsStrFunc:\n        connParam[paramStrFunc + 'Args'] = connParam[paramStrFunc + 'Vars'].copy()\n        funcKeys[paramStrFunc] = [key for key in connParam[paramStrFunc + 'Vars'] if callable(connParam[paramStrFunc + 'Vars'][key])]\n\n    # converted to list only once \n    postCellsTagsKeys = sorted(postCellsTags)    \n\n    # calculate hash for post cell gids\n    hashPostCells = sim.hashList(postCellsTagsKeys)\n\n    for preCellGid, preCellTags in preCellsTags.items():  # for each presyn cell\n        divergence = connParam['divergenceFunc'][preCellGid] if 'divergenceFunc' in connParam else connParam['divergence']  # num of presyn conns / postsyn cell\n        divergence = max(min(int(round(divergence)), len(postCellsTags)-1), 0)\n        self.rand.Random123(hashPostCells, preCellGid, sim.cfg.seeds['conn'])  # init randomizer\n        randSample = self.randUniqueInt(self.rand, divergence+1, 0, len(postCellsTags)-1)\n        \n        # note: randSample[divergence] is an extra value used only if one of the random postGids coincided with the preGid \n        postCellsSample = {postCellsTagsKeys[randSample[divergence]] if postCellsTagsKeys[i]==preCellGid else postCellsTagsKeys[i]: 0\n                               for i in randSample[0:divergence]}  # dict of selected gids of postsyn cells with removed pre gid\n\n        for postCellGid in [c for c in postCellsSample if c in self.gid2lid]:            \n            postCellTags = postCellsTags[postCellGid]\n            for paramStrFunc in paramsStrFunc: # call lambda functions to get weight func args\n                # update the relevant FuncArgs dict where lambda functions are known to exist in the corresponding FuncVars dict\n                for funcKey in funcKeys[paramStrFunc]:\n                    connParam[paramStrFunc + 'Args'][funcKey] = connParam[paramStrFunc+'Vars'][funcKey](preCellTags,postCellTags)\n\n            if preCellGid != postCellGid: # if not self-connection\n                self._addCellConn(connParam, preCellGid, postCellGid)", "response": "Generates a set of connections between all pre - syn cells and all post - syn cells based on probability values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fromListConn (self, preCellsTags, postCellsTags, connParam):\n    from .. import sim\n\n    ''' Generates connections between all pre and post-syn cells based list of relative cell ids'''\n    if sim.cfg.verbose: print('Generating set of connections from list (rule: %s) ...' % (connParam['label']))\n\n    orderedPreGids = sorted(preCellsTags)\n    orderedPostGids = sorted(postCellsTags)\n\n    # list of params that can have a lambda function\n    paramsStrFunc = [param for param in [p+'Func' for p in self.connStringFuncParams] if param in connParam] \n    for paramStrFunc in paramsStrFunc:\n        # replace lambda function (with args as dict of lambda funcs) with list of values\n        connParam[paramStrFunc[:-4]+'List'] = {(orderedPreGids[preId],orderedPostGids[postId]): \n            connParam[paramStrFunc](**{k:v if isinstance(v, Number) else v(preCellsTags[orderedPreGids[preId]], postCellsTags[orderedPostGids[postId]]) \n            for k,v in connParam[paramStrFunc+'Vars'].items()}) for preId,postId in connParam['connList']}\n\n    if 'weight' in connParam and isinstance(connParam['weight'], list): \n        connParam['weightFromList'] = list(connParam['weight'])  # if weight is a list, copy to weightFromList\n    if 'delay' in connParam and isinstance(connParam['delay'], list): \n        connParam['delayFromList'] = list(connParam['delay'])  # if delay is a list, copy to delayFromList\n    if 'loc' in connParam and isinstance(connParam['loc'], list): \n        connParam['locFromList'] = list(connParam['loc'])  # if delay is a list, copy to locFromList\n\n\n    for iconn, (relativePreId, relativePostId) in enumerate(connParam['connList']):  # for each postsyn cell\n        preCellGid = orderedPreGids[relativePreId]     \n        postCellGid = orderedPostGids[relativePostId]\n        if postCellGid in self.gid2lid:  # check if postsyn is in this node's list of gids\n            \n            if 'weightFromList' in connParam: connParam['weight'] = connParam['weightFromList'][iconn] \n            if 'delayFromList' in connParam: connParam['delay'] = connParam['delayFromList'][iconn]\n            if 'locFromList' in connParam: connParam['loc'] = connParam['locFromList'][iconn]\n    \n            if preCellGid != postCellGid: # if not self-connection\n                self._addCellConn(connParam, preCellGid, postCellGid)", "response": "Generate a new set of unique identifiers from a list of relative cell ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates average and peak firing rates for a specific time period.", "response": "def calculateRate (include = ['allCells', 'eachPop'], peakBin = 5, timeRange = None): \n    ''' \n    Calculate avg and peak rate of different subsets of cells for specific time period\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of data series to include. \n            Note: one line per item, not grouped (default: ['allCells', 'eachPop'])\n        - timeRange ([start:stop]): Time range of spikes shown; if None shows all (default: None)\n        - peakBin (int): Histogram bin size used to calculate peak firing rate; if None, peak rate not calculated (default: 5)\n        - Returns list with rates\n    '''\n\n    from .. import sim\n\n    print('Calculating avg and peak firing rates ...')\n\n    # Replace 'eachPop' with list of pops\n    if 'eachPop' in include: \n        include.remove('eachPop')\n        for pop in sim.net.allPops: include.append(pop)\n\n    # time range\n    if timeRange is None:\n        timeRange = [0,sim.cfg.duration]\n\n    avg, peak, histData = [], [], []\n\n    # Plot separate line for each entry in include\n    for iplot,subset in enumerate(include):\n        cells, cellGids, netStimLabels = getCellsInclude([subset])\n        numNetStims = 0\n\n        # Select cells to include\n        if len(cellGids) > 0:\n            try:\n                spkinds,spkts = list(zip(*[(spkgid,spkt) for spkgid,spkt in zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]))\n            except:\n                spkinds,spkts = [],[]\n        else: \n            spkinds,spkts = [],[]\n\n        # Add NetStim spikes\n        spkts, spkinds = list(spkts), list(spkinds)\n        numNetStims = 0\n        if 'stims' in sim.allSimData:\n            for netStimLabel in netStimLabels:\n                netStimSpks = [spk for cell,stims in sim.allSimData['stims'].items() \\\n                for stimLabel,stimSpks in stims.items() for spk in stimSpks if stimLabel == netStimLabel]\n                if len(netStimSpks) > 0:\n                    lastInd = max(spkinds) if len(spkinds)>0 else 0\n                    spktsNew = netStimSpks \n                    spkindsNew = [lastInd+1+i for i in range(len(netStimSpks))]\n                    spkts.extend(spktsNew)\n                    spkinds.extend(spkindsNew)\n                    numNetStims += 1\n\n        if peakBin:\n            histo = np.histogram(spkts, bins = np.arange(timeRange[0], timeRange[1], peakBin))\n            histoT = histo[1][:-1]+peakBin/2\n            histoCount = histo[0] \n\n            histData.append(histoCount)\n\n            histoCount = histoCount * float((1000.0 / peakBin)) / float((len(cellGids)+numNetStims)) # convert to firing rate\n            peak.append(float(max(histoCount)))\n\n        spktsRange = [spkt for spkt in spkts if timeRange[0] <= spkt <= timeRange[1]]\n        avg.append(float(len(spktsRange)) / float((len(cellGids)+numNetStims)) / float((timeRange[1]-timeRange[0])) * 1000.0)\n\n    return include, avg, peak"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting average and peak firing rates for a specific time period.", "response": "def plotRates (include =['allCells', 'eachPop'], peakBin = 5, timeRanges = None, timeRangeLabels = None, colors = None, figSize = ((5,5)), saveData = None, \n        ylim = None, saveFig = None, showFig = True):\n    ''' \n    Calculate avg and peak rate of different subsets of cells for specific time period\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of data series to include. \n            Note: one line per item, not grouped (default: ['allCells', 'eachPop'])\n        - timeRanges ([[start1:stop1], [start2:stop2]]): List of time range of spikes shown; if None shows all (default: None)\n        - timeRangeLabels (['preStim', 'postStim']): List of labels for each time range period (default: None)\n        - peakBin (int): Histogram bin size used to calculate peak firing rate; if None, peak rate not calculated (default: 5)\n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure; \n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure (default: None)\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figs\n    '''\n    from .. import sim\n\n    if not colors: colors = colorList\n\n    avgs = []\n    peaks = []\n    if not timeRangeLabels:\n        timeRangeLabels = ['%f-%f ms'%(t[0], t[1]) for t in timeRanges] #['period '+i for i in range(len(timeRanges))]\n\n    for i, timeRange in enumerate(timeRanges):\n        labels, avg, peak = sim.analysis.calculateRate(include=include, peakBin=peakBin, timeRange=timeRange)\n        avgs.append(avg)\n        peaks.append(peak)\n\n    if showFig or saveFig:\n        fig1,ax1 = plt.subplots(figsize=figSize)\n\n        # avg\n        fontsiz=14\n        ax1.set_color_cycle(colors)\n        ax1.plot(avgs, marker='o')\n        #ax1.set_xlabel('Time period', fontsize=fontsiz)\n        ax1.set_ylabel('Avg firing rate', fontsize=fontsiz)\n        ax1.set_xticks(list(range(len(timeRangeLabels))))\n        ax1.set_xticklabels(timeRangeLabels, fontsize=fontsiz)\n        ax1.set_xlim(-0.5, len(avgs)-0.5)\n        if ylim: ax1.set_ylim(ylim)\n        ax1.legend(include)\n\n        try:\n            plt.tight_layout()\n        except:\n            pass\n\n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_'+'avgRates.png'\n            plt.savefig(filename)\n\n        # show fig \n        if showFig: _showFigure()\n\n        # peak\n        fig2,ax2 = plt.subplots(figsize=figSize)\n        ax2.set_color_cycle(colors)\n        ax2.plot(peaks, marker='o')\n        #ax2.set_xlabel('Time period', fontsize=fontsiz)\n        ax2.set_ylabel('Peak firing rate', fontsize=fontsiz)\n        ax2.set_xticks(list(range(len(timeRangeLabels))))\n        ax2.set_xticklabels(timeRangeLabels)\n        ax2.set_xlim(-0.5, len(peaks)-0.5)\n        if ylim: ax2.set_ylim(ylim)\n        ax2.legend(include)\n\n        try:\n            plt.tight_layout()\n        except:\n            pass\n\n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_'+'peakRates.png'\n            plt.savefig(filename)\n\n        # show fig \n        if showFig: _showFigure()\n    else:\n        fig1, fig2 = None, None\n\n        \n    # save figure data\n    if saveData:\n        figData = {'includeList': includeList, 'timeRanges': timeRanges, 'avgs': avgs, 'peaks': peaks}\n\n        _saveFigData(figData, saveData, 'raster')\n\n    return fig1, fig2, avgs, peaks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the syncs of the items in the same time range.", "response": "def plotSyncs (include =['allCells', 'eachPop'], timeRanges = None, timeRangeLabels = None, colors = None, figSize = ((5,5)), saveData = None, \n        saveFig = None, showFig = True):\n    ''' \n    Calculate avg and peak rate of different subsets of cells for specific time period\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of data series to include. \n            Note: one line per item, not grouped (default: ['allCells', 'eachPop'])\n        - timeRanges ([[start1:stop1], [start2:stop2]]): List of time range of spikes shown; if None shows all (default: None)\n        - timeRangeLabels (['preStim', 'postStim']): List of labels for each time range period (default: None)\n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure; \n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure (default: None)\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figs\n    '''\n    from .. import sim\n\n    if not colors: colors = colorList\n\n    syncs = []\n    if not timeRangeLabels:\n        timeRangeLabels = ['%f-%f ms'%(t[0], t[1]) for t in timeRanges] #['period '+i for i in range(len(timeRanges))]\n\n    for i, timeRange in enumerate(timeRanges):\n        print(timeRange)\n        _, sync = sim.analysis.plotSpikeStats (include = include, timeRange = timeRange, stats = ['sync'], saveFig = False, showFig =False)\n        print(sync)\n        sync = [s[0] for s in sync]\n        syncs.append(sync)\n\n    fig1,ax1 = plt.subplots(figsize=figSize)\n\n    # avg\n    fontsiz=14\n    ax1.set_color_cycle(colors)\n    ax1.plot(syncs, marker='o')\n    ax1.set_xlabel('Time period', fontsize=fontsiz)\n    ax1.set_ylabel('Spiking synchrony', fontsize=fontsiz)\n    ax1.set_xticks(list(range(len(timeRangeLabels))))\n    ax1.set_xticklabels(timeRangeLabels)\n    ax1.set_xlim(-0.5, len(syncs)-0.5)\n    ax1.legend(include)\n\n    # save figure\n    if saveFig: \n        if isinstance(saveFig, basestring):\n            filename = saveFig\n        else:\n            filename = sim.cfg.filename+'_'+'sync.png'\n        plt.savefig(filename)\n\n    # show fig \n    if showFig: _showFigure()\n\n    # save figure data\n    if saveData:\n        figData = {'includeList': includeList, 'timeRanges': timeRanges, 'syncs': syncs}\n\n        _saveFigData(figData, saveData, 'raster')\n \n\n\n    return fig1, syncs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots a single raster of the network cells.", "response": "def plotRaster (include = ['allCells'], timeRange = None, maxSpikes = 1e8, orderBy = 'gid', orderInverse = False, labels = 'legend', popRates = False,\n        spikeHist=None, spikeHistBin=5, syncLines=False, lw=2, marker='|', markerSize=5, popColors=None, figSize=(10, 8), fontSize=12,\n        dpi = 100, saveData = None, saveFig = None, showFig = True):\n    '''\n    Raster plot of network cells\n        - include (['all',|'allCells',|'allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Cells to include (default: 'allCells')\n        - timeRange ([start:stop]): Time range of spikes shown; if None shows all (default: None)\n        - maxSpikes (int): maximum number of spikes that will be plotted  (default: 1e8)\n        - orderBy ('gid'|'y'|'ynorm'|...): Unique numeric cell property to order y-axis by, e.g. 'gid', 'ynorm', 'y' (default: 'gid')\n        - orderInverse (True|False): Invert the y-axis order (default: False)\n        - labels = ('legend', 'overlay'): Show population labels in a legend or overlayed on one side of raster (default: 'legend')\n        - popRates = (True|False): Include population rates (default: False)\n        - spikeHist (None|'overlay'|'subplot'): overlay line over raster showing spike histogram (spikes/bin) (default: False)\n        - spikeHistBin (int): Size of bin in ms to use for histogram (default: 5)\n        - syncLines (True|False): calculate synchorny measure and plot vertical lines for each spike to evidence synchrony (default: False)\n        - lw (integer): Line width for each spike (default: 2)\n        - marker (char): Marker for each spike (default: '|')\n        - popColors (odict): Dictionary with color (value) used for each population (key) (default: None)\n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - dpi (int): Dots per inch to save fig (default: 100)\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure;\n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure (default: None)\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n        - Returns figure handle\n    '''\n\n    from .. import sim\n\n    print('Plotting raster...')\n\n    # Select cells to include\n    cells, cellGids, netStimLabels = getCellsInclude(include)\n\n    df = pd.DataFrame.from_records(cells)\n    df = pd.concat([df.drop('tags', axis=1), pd.DataFrame.from_records(df['tags'].tolist())], axis=1)\n\n    keep = ['pop', 'gid', 'conns']\n\n    if isinstance(orderBy, basestring) and orderBy not in cells[0]['tags']:  # if orderBy property doesn't exist or is not numeric, use gid\n        orderBy = 'gid'\n    elif orderBy is 'pop':\n        df['popInd'] = df['pop'].astype('category')\n        df['popInd'].cat.set_categories(sim.net.pops.keys(), inplace=True)\n        orderBy='popInd'\n    elif isinstance(orderBy, basestring) and not isinstance(cells[0]['tags'][orderBy], Number):\n        orderBy = 'gid'\n        \n    if isinstance(orderBy, list):\n        if 'pop' in orderBy:\n            df['popInd'] = df['pop'].astype('category')\n            df['popInd'].cat.set_categories(sim.net.pops.keys(), inplace=True)\n            orderBy[orderBy.index('pop')] = 'popInd'\n        keep = keep + list(set(orderBy) - set(keep))\n    elif orderBy not in keep:\n        keep.append(orderBy)\n    df = df[keep]\n\n    popLabels = [pop for pop in sim.net.allPops if pop in df['pop'].unique()] #preserves original ordering\n    if netStimLabels: popLabels.append('NetStims')\n    popColorsTmp = {popLabel: colorList[ipop%len(colorList)] for ipop,popLabel in enumerate(popLabels)} # dict with color for each pop\n    if popColors: popColorsTmp.update(popColors)\n    popColors = popColorsTmp\n    if len(cellGids) > 0:\n        gidColors = {cell['gid']: popColors[cell['tags']['pop']] for cell in cells}  # dict with color for each gid\n        try:\n            sel, spkts,spkgids = getSpktSpkid(cellGids=cellGids, timeRange=timeRange, allCells=(include == ['allCells']))\n        except:\n            import sys\n            print((sys.exc_info()))\n            spkgids, spkts = [], []\n            sel = pd.DataFrame(columns=['spkt', 'spkid'])\n        sel['spkgidColor'] = sel['spkid'].map(gidColors)\n        df['gidColor'] = df['pop'].map(popColors)\n        df.set_index('gid', inplace=True)\n\n\n    # Order by\n    if len(df) > 0:\n        ylabelText = 'Cells (ordered by %s)'%(orderBy)\n        df = df.sort_values(by=orderBy)\n        sel['spkind'] = sel['spkid'].apply(df.index.get_loc)\n\n    else:\n        sel = pd.DataFrame(columns=['spkt', 'spkid', 'spkind'])\n        ylabelText = ''\n\n    # Add NetStim spikes\n    numCellSpks = len(sel)\n    numNetStims = 0\n    for netStimLabel in netStimLabels:\n        netStimSpks = [spk for cell,stims in sim.allSimData['stims'].items() \\\n            for stimLabel,stimSpks in stims.items() for spk in stimSpks if stimLabel == netStimLabel]\n        if len(netStimSpks) > 0:\n            # lastInd = max(spkinds) if len(spkinds)>0 else 0\n            lastInd = sel['spkind'].max() if len(sel['spkind']) > 0 else 0\n            spktsNew = netStimSpks\n            spkindsNew = [lastInd+1+i for i in range(len(netStimSpks))]\n            ns = pd.DataFrame(list(zip(spktsNew, spkindsNew)), columns=['spkt', 'spkind'])\n            ns['spkgidColor'] = popColors['netStims']\n            sel = pd.concat([sel, ns])\n            numNetStims += 1\n        else:\n            pass\n            #print netStimLabel+' produced no spikes'\n    if len(cellGids)>0 and numNetStims:\n        ylabelText = ylabelText + ' and NetStims (at the end)'\n    elif numNetStims:\n        ylabelText = ylabelText + 'NetStims'\n\n    if numCellSpks+numNetStims == 0:\n        print('No spikes available to plot raster')\n        return None\n\n    # Time Range\n    if timeRange == [0,sim.cfg.duration]:\n        pass\n    elif timeRange is None:\n        timeRange = [0,sim.cfg.duration]\n    else:\n        sel = sel.query('spkt >= @timeRange[0] and spkt <= @timeRange[1]')\n\n\n    # Limit to maxSpikes\n    if (len(sel)>maxSpikes):\n        print(('  Showing only the first %i out of %i spikes' % (maxSpikes, len(sel)))) # Limit num of spikes\n        if numNetStims: # sort first if have netStims\n            sel = sel.sort_values(by='spkt')\n        sel = sel.iloc[:maxSpikes]\n        timeRange[1] =  sel['spkt'].max()\n\n    # Calculate spike histogram\n    if spikeHist:\n        histo = np.histogram(sel['spkt'].tolist(), bins = np.arange(timeRange[0], timeRange[1], spikeHistBin))\n        histoT = histo[1][:-1]+spikeHistBin/2\n        histoCount = histo[0]\n\n    # Plot spikes\n    # set font size\n    plt.rcParams.update({'font.size': fontSize})\n\n    fig,ax1 = plt.subplots(figsize=figSize)\n    fontsiz = fontSize\n \n    if spikeHist == 'subplot':\n        gs = gridspec.GridSpec(2, 1,height_ratios=[2,1])\n        ax1=plt.subplot(gs[0])\n    sel['spkt'] = sel['spkt'].apply(pd.to_numeric)\n    sel.plot.scatter(ax=ax1, x='spkt', y='spkind', lw=lw, s=markerSize, marker=marker, c=sel['spkgidColor'].tolist()) # Create raster\n    ax1.set_xlim(timeRange)\n\n    # Plot stats\n    gidPops = df['pop'].tolist()\n    popNumCells = [float(gidPops.count(pop)) for pop in popLabels] if numCellSpks else [0] * len(popLabels)\n    totalSpikes = len(sel)\n    totalConnections = sum([len(conns) for conns in df['conns']])\n    numCells = len(cells)\n    firingRate = float(totalSpikes)/(numCells+numNetStims)/(timeRange[1]-timeRange[0])*1e3 if totalSpikes>0 else 0 # Calculate firing rate\n    connsPerCell = totalConnections/float(numCells) if numCells>0 else 0 # Calculate the number of connections per cell\n    \n    if popRates:\n        avgRates = {}\n        tsecs = (timeRange[1]-timeRange[0])/1e3\n        for i,(pop, popNum) in enumerate(zip(popLabels, popNumCells)):\n            if numCells > 0 and pop != 'NetStims':\n                if numCellSpks == 0:\n                    avgRates[pop] = 0\n                else:\n                    avgRates[pop] = len([spkid for spkid in sel['spkind'].iloc[:numCellSpks-1] if df['pop'].iloc[int(spkid)]==pop])/popNum/tsecs\n        if numNetStims:\n            popNumCells[-1] = numNetStims\n            avgRates['NetStims'] = len([spkid for spkid in sel['spkind'].iloc[numCellSpks:]])/numNetStims/tsecs\n\n    # Plot synchrony lines\n    if syncLines:\n        for spkt in sel['spkt'].tolist():\n            ax1.plot((spkt, spkt), (0, len(cells)+numNetStims), 'r-', linewidth=0.1)\n        plt.title('cells=%i   syns/cell=%0.1f   rate=%0.1f Hz   sync=%0.2f' % (numCells,connsPerCell,firingRate,syncMeasure()), fontsize=fontsiz)\n    else:\n        plt.title('cells=%i   syns/cell=%0.1f   rate=%0.1f Hz' % (numCells,connsPerCell,firingRate), fontsize=fontsiz)\n    # Axis\n    ax1.set_xlabel('Time (ms)', fontsize=fontsiz)\n    ax1.set_ylabel(ylabelText, fontsize=fontsiz)\n    ax1.set_xlim(timeRange)\n    ax1.set_ylim(-1, len(cells)+numNetStims+1)\n\n    # Add legend\n    if popRates:\n        popLabelRates = [popLabel + ' (%.3g Hz)'%(avgRates[popLabel]) for popLabel in popLabels if popLabel in avgRates]\n\n    if labels == 'legend':\n        for ipop,popLabel in enumerate(popLabels):\n            label = popLabelRates[ipop] if popRates else popLabel\n            plt.plot(0,0,color=popColors[popLabel],label=label)\n        plt.legend(fontsize=fontsiz, bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0.)\n        maxLabelLen = max([len(l) for l in popLabels])\n        rightOffset = 0.85 if popRates else 0.9\n        plt.subplots_adjust(right=(rightOffset-0.012*maxLabelLen))\n\n    elif labels == 'overlay':\n        ax = plt.gca()\n        tx = 1.01\n        margin = 1.0/numCells/2\n        minSpacing = float(fontsiz) * 1.1 / float((0.8*figSize[1]*dpi))\n        tys = [(float(popLen)/numCells)*(1-2*margin) for popLen in popNumCells]\n        tysOffset = list(scipy.cumsum(tys))[:-1]\n        tysOffset = [tysOffset[0]] +[tysOffset[i] + max(tysOffset[i+1]-tysOffset[i], minSpacing) for i in range(len(tysOffset)-1)]\n        tysOffset.insert(0, 0)\n        labels = popLabelRates if popRates else popLabels\n        for ipop,(ty, tyOffset, popLabel) in enumerate(zip(tys, tysOffset, popLabels)):\n            label = popLabelRates[ipop] if popRates else popLabel\n            if orderInverse:\n                finalty = 1.0 - (tyOffset + ty/2.0 + 0.01)\n            else:\n                finalty = tyOffset + ty/2.0 - 0.01\n            plt.text(tx, finalty, label, transform=ax.transAxes, fontsize=fontsiz, color=popColors[popLabel])\n        maxLabelLen = min(6, max([len(l) for l in labels]))\n        plt.subplots_adjust(right=(0.95-0.011*maxLabelLen))\n\n\n    # Plot spike hist\n    if spikeHist == 'overlay':\n        ax2 = ax1.twinx()\n        ax2.plot (histoT, histoCount, linewidth=0.5)\n        ax2.set_ylabel('Spike count', fontsize=fontsiz) # add yaxis label in opposite side\n        ax2.set_xlim(timeRange)\n    elif spikeHist == 'subplot':\n        ax2=plt.subplot(gs[1])\n        ax2.plot (histoT, histoCount, linewidth=1.0)\n        ax2.set_xlabel('Time (ms)', fontsize=fontsiz)\n        ax2.set_ylabel('Spike count', fontsize=fontsiz)\n        ax2.set_xlim(timeRange)\n\n    if orderInverse: plt.gca().invert_yaxis()\n\n    # save figure data\n    if saveData:\n        figData = {'spkTimes': sel['spkt'].tolist(), 'spkInds': sel['spkind'].tolist(), 'spkColors': sel['spkgidColor'].tolist(), 'cellGids': cellGids, 'sortedGids': df.index.tolist(), 'numNetStims': numNetStims,\n        'include': include, 'timeRange': timeRange, 'maxSpikes': maxSpikes, 'orderBy': orderBy, 'orderInverse': orderInverse, 'spikeHist': spikeHist,\n        'syncLines': syncLines}\n        _saveFigData(figData, saveData, 'raster')\n\n    # save figure\n    if saveFig:\n        if isinstance(saveFig, basestring):\n            filename = saveFig\n        else:\n            filename = sim.cfg.filename+'_'+'raster.png'\n        plt.savefig(filename, dpi=dpi)\n\n    # show fig\n    if showFig: _showFigure()\n\n    return fig, {'include': include, 'spkts': spkts, 'spkinds': sel['spkind'].tolist(), 'timeRange': timeRange}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plotSpikeHist (include = ['allCells', 'eachPop'], timeRange = None, binSize = 5, overlay=True, graphType='line', yaxis = 'rate',\n    popColors = [], norm = False, dpi = 100, figSize = (10,8), smooth=None, filtFreq = False, filtOrder=3, axis = 'on', saveData = None,\n    saveFig = None, showFig = True, **kwargs):\n    '''\n    Plot spike histogram\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of data series to include.\n            Note: one line per item, not grouped (default: ['allCells', 'eachPop'])\n        - timeRange ([start:stop]): Time range of spikes shown; if None shows all (default: None)\n        - binSize (int): Size in ms of each bin (default: 5)\n        - overlay (True|False): Whether to overlay the data lines or plot in separate subplots (default: True)\n        - graphType ('line'|'bar'): Type of graph to use (line graph or bar plot) (default: 'line')\n        - yaxis ('rate'|'count'): Units of y axis (firing rate in Hz, or spike count) (default: 'rate')\n        - popColors (dict): Dictionary with color (value) used for each population (key) (default: None)\n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure;\n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n        - Returns figure handle\n    '''\n\n    from .. import sim\n\n    print('Plotting spike histogram...')\n\n    # Replace 'eachPop' with list of pops\n    if 'eachPop' in include:\n        include.remove('eachPop')\n        for pop in sim.net.allPops: include.append(pop)\n\n    # Y-axis label\n    if yaxis == 'rate':\n        if norm:\n            yaxisLabel = 'Normalized firing rate'\n        else:\n            yaxisLabel = 'Avg cell firing rate (Hz)'\n    elif yaxis == 'count':\n        if norm:\n            yaxisLabel = 'Normalized spike count'\n        else:\n            yaxisLabel = 'Spike count'\n    else:\n        print('Invalid yaxis value %s', (yaxis))\n        return\n\n\n    # time range\n    if timeRange is None:\n        timeRange = [0,sim.cfg.duration]\n\n    histData = []\n\n    # create fig\n    fig,ax1 = plt.subplots(figsize=figSize)\n    fontsiz = 12\n\n    # Plot separate line for each entry in include\n    for iplot,subset in enumerate(include):\n        cells, cellGids, netStimLabels = getCellsInclude([subset])\n        numNetStims = 0\n\n        # Select cells to include\n        if len(cellGids) > 0:\n            try:\n                spkinds,spkts = list(zip(*[(spkgid,spkt) for spkgid,spkt in zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]))\n            except:\n                spkinds,spkts = [],[]\n        else:\n            spkinds,spkts = [],[]\n\n        # Add NetStim spikes\n        spkts, spkinds = list(spkts), list(spkinds)\n        numNetStims = 0\n        if 'stims' in sim.allSimData:\n            for netStimLabel in netStimLabels:\n                netStimSpks = [spk for cell,stims in sim.allSimData['stims'].items() \\\n                for stimLabel,stimSpks in stims.items() for spk in stimSpks if stimLabel == netStimLabel]\n                if len(netStimSpks) > 0:\n                    lastInd = max(spkinds) if len(spkinds)>0 else 0\n                    spktsNew = netStimSpks\n                    spkindsNew = [lastInd+1+i for i in range(len(netStimSpks))]\n                    spkts.extend(spktsNew)\n                    spkinds.extend(spkindsNew)\n                    numNetStims += 1\n\n        histo = np.histogram(spkts, bins = np.arange(timeRange[0], timeRange[1], binSize))\n        histoT = histo[1][:-1]+binSize/2\n        histoCount = histo[0]\n\n        if yaxis=='rate':\n            histoCount = histoCount * (1000.0 / binSize) / (len(cellGids)+numNetStims) # convert to firing rate\n\n        if filtFreq:\n            from scipy import signal\n            fs = 1000.0/binSize\n            nyquist = fs/2.0\n            if isinstance(filtFreq, list): # bandpass\n                Wn = [filtFreq[0]/nyquist, filtFreq[1]/nyquist]\n                b, a = signal.butter(filtOrder, Wn, btype='bandpass')\n            elif isinstance(filtFreq, Number): # lowpass\n                Wn = filtFreq/nyquist\n                b, a = signal.butter(filtOrder, Wn)\n            histoCount = signal.filtfilt(b, a, histoCount)\n\n        if norm:\n            histoCount /= max(histoCount)\n\n        if smooth:\n            histoCount = _smooth1d(histoCount, smooth)[:len(histoT)]\n\n        histData.append(histoCount)\n\n        color = popColors[subset] if subset in popColors else colorList[iplot%len(colorList)]\n\n        if not overlay:\n            plt.subplot(len(include),1,iplot+1)  # if subplot, create new subplot\n            plt.title (str(subset), fontsize=fontsiz)\n            color = 'blue'\n\n        if graphType == 'line':\n            plt.plot (histoT, histoCount, linewidth=1.0, color = color)\n        elif graphType == 'bar':\n            #plt.bar(histoT, histoCount, width = binSize, color = color, fill=False)\n            plt.plot (histoT, histoCount, linewidth=1.0, color = color, ls='steps')\n\n        if iplot == 0:\n            plt.xlabel('Time (ms)', fontsize=fontsiz)\n            plt.ylabel(yaxisLabel, fontsize=fontsiz) # add yaxis in opposite side\n        plt.xlim(timeRange)\n\n    if len(include) < 5:  # if apply tight_layout with many subplots it inverts the y-axis\n        try:\n            plt.tight_layout()\n        except:\n            pass\n\n    # Add legend\n    if overlay:\n        for i,subset in enumerate(include):\n            color = popColors[subset] if subset in popColors else colorList[i%len(colorList)]\n            plt.plot(0,0,color=color,label=str(subset))\n        plt.legend(fontsize=fontsiz, bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0.)\n        maxLabelLen = min(10,max([len(str(l)) for l in include]))\n        plt.subplots_adjust(right=(0.9-0.012*maxLabelLen))\n\n    # Set axis or scaleber\n    if axis == 'off':\n        ax = plt.gca()\n        scalebarLoc = kwargs.get('scalebarLoc', 7)\n        round_to_n = lambda x, n, m: int(np.round(round(x, -int(np.floor(np.log10(abs(x)))) + (n - 1)) / m)) * m\n        sizex = round_to_n((timeRange[1]-timeRange[0])/10.0, 1, 50)\n        add_scalebar(ax, hidex=False, hidey=True, matchx=False, matchy=True, sizex=sizex, sizey=None,\n                    unitsx='ms', unitsy='Hz', scalex=1, scaley=1, loc=scalebarLoc, pad=2, borderpad=0.5, sep=4, prop=None, barcolor=\"black\", barwidth=3)\n        plt.axis(axis)\n\n    # save figure data\n    if saveData:\n        figData = {'histData': histData, 'histT': histoT, 'include': include, 'timeRange': timeRange, 'binSize': binSize,\n         'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n\n        _saveFigData(figData, saveData, 'spikeHist')\n\n    # save figure\n    if saveFig:\n        if isinstance(saveFig, basestring):\n            filename = saveFig\n        else:\n            filename = sim.cfg.filename+'_'+'spikeHist.png'\n        plt.savefig(filename, dpi=dpi)\n\n    # show fig\n    if showFig: _showFigure()\n\n    return fig, {'include': include, 'histData': histData, 'histoT': histoT, 'timeRange': timeRange}", "response": "Plot a spike histogram for each item in the current population."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plotSpikeStats (include = ['allCells', 'eachPop'], statDataIn = {}, timeRange = None, graphType='boxplot', stats = ['rate', 'isicv'], bins = 50,\n                 popColors = [], histlogy = False, histlogx = False, histmin = 0.0, density = False, includeRate0=False, legendLabels = None, normfit = False,\n                  histShading=True, xlim = None, dpi = 100, figSize = (6,8), fontSize=12, saveData = None, saveFig = None, showFig = True, **kwargs): \n    ''' \n    Plot spike histogram\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of data series to include. \n            Note: one line per item, not grouped (default: ['allCells', 'eachPop'])\n        - timeRange ([start:stop]): Time range of spikes shown; if None shows all (default: None)\n        - graphType ('boxplot', 'histogram'): Type of graph to use (default: 'boxplot')\n        - stats (['rate', |'isicv'| 'sync'| 'pairsync']): Measure to plot stats on (default: ['rate', 'isicv'])\n        - bins (int or list of edges): Number of bins (if integer) of edges (if list) for histogram (default: 50)\n        - popColors (dict): Dictionary with color (value) used for each population (key) (default: None)\n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure;\n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figure handle and statData\n    '''\n\n    from .. import sim\n    print('Plotting spike stats...')\n\n    # Set plot style\n    colors = []\n    params = {\n        'axes.labelsize': fontSize,\n        'font.size': fontSize,\n        'legend.fontsize': fontSize,\n        'xtick.labelsize': fontSize,\n        'ytick.labelsize': fontSize,\n        'text.usetex': False,\n        }\n    plt.rcParams.update(params)\n\n    xlabels = {'rate': 'Rate (Hz)', 'isicv': 'Irregularity (ISI CV)', 'sync':  'Synchrony', ' pairsync': 'Pairwise synchrony'}\n\n    # Replace 'eachPop' with list of pops\n    if 'eachPop' in include: \n        include.remove('eachPop')\n        for pop in sim.net.allPops: include.append(pop)\n\n    # time range\n    if timeRange is None:\n        timeRange = [0,sim.cfg.duration]\n\n    for stat in stats:\n        # create fig\n        fig,ax1 = plt.subplots(figsize=figSize)\n        fontsiz = fontSize\n        xlabel = xlabels[stat]\n\n        statData = []\n        gidsData = []\n        ynormsData = []\n\n        # Calculate data for each entry in include\n        for iplot,subset in enumerate(include):\n\n            if stat in statDataIn:\n                statData = statDataIn[stat]['statData']\n                gidsData = statDataIn[stat].get('gidsData', [])\n                ynormsData = statDataIn[stat].get('ynormsData', [])\n\n            else:\n                cells, cellGids, netStimLabels = getCellsInclude([subset])\n                numNetStims = 0\n\n                # Select cells to include\n                if len(cellGids) > 0:\n                    try:\n                        spkinds,spkts = list(zip(*[(spkgid,spkt) for spkgid,spkt in \n                            zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]))\n                    except:\n                        spkinds,spkts = [],[]\n                else: \n                    spkinds,spkts = [],[]\n\n                # Add NetStim spikes\n                spkts, spkinds = list(spkts), list(spkinds)\n                numNetStims = 0\n                if 'stims' in sim.allSimData:\n                    for netStimLabel in netStimLabels:\n                        netStimSpks = [spk for cell,stims in sim.allSimData['stims'].items() \\\n                        for stimLabel,stimSpks in stims.items() \n                            for spk in stimSpks if stimLabel == netStimLabel]\n                        if len(netStimSpks) > 0:\n                            lastInd = max(spkinds) if len(spkinds)>0 else 0\n                            spktsNew = netStimSpks \n                            spkindsNew = [lastInd+1+i for i in range(len(netStimSpks))]\n                            spkts.extend(spktsNew)\n                            spkinds.extend(spkindsNew)\n                            numNetStims += 1\n                try:\n                    spkts,spkinds = list(zip(*[(spkt, spkind) for spkt, spkind in zip(spkts, spkinds) \n                        if timeRange[0] <= spkt <= timeRange[1]]))\n                except:\n                    pass\n\n                # if scatter get gids and ynorm\n                if graphType == 'scatter':    \n                    if includeRate0:\n                        gids = cellGids\n                    else:\n                        gids = set(spkinds)\n                    ynorms = [sim.net.allCells[int(gid)]['tags']['ynorm'] for gid in gids]\n\n                    gidsData.insert(0, gids)\n                    ynormsData.insert(0, ynorms)\n\n                # rate stats\n                if stat == 'rate':\n                    toRate = 1e3/(timeRange[1]-timeRange[0])\n                    if includeRate0:\n                        rates = [spkinds.count(gid)*toRate for gid in cellGids] \\\n                            if len(spkinds)>0 else [0]*len(cellGids) #cellGids] #set(spkinds)] \n                    else:\n                        rates = [spkinds.count(gid)*toRate for gid in set(spkinds)] \\\n                            if len(spkinds)>0 else [0] #cellGids] #set(spkinds)] \n                    statData.append(rates)\n\n\n                # Inter-spike interval (ISI) coefficient of variation (CV) stats\n                elif stat == 'isicv':\n                    import numpy as np\n                    spkmat = [[spkt for spkind,spkt in zip(spkinds,spkts) if spkind==gid] \n                        for gid in set(spkinds)]\n                    isimat = [[t - s for s, t in zip(spks, spks[1:])] for spks in spkmat if len(spks)>10]\n                    isicv = [np.std(x) / np.mean(x) if len(x)>0 else 0 for x in isimat] # if len(x)>0] \n                    statData.append(isicv) \n\n\n                # synchrony\n                elif stat in ['sync', 'pairsync']:\n                    try: \n                        import pyspike  \n                    except:\n                        print(\"Error: plotSpikeStats() requires the PySpike python package \\\n                            to calculate synchrony (try: pip install pyspike)\")\n                        return 0\n                    \n                    spkmat = [pyspike.SpikeTrain([spkt for spkind,spkt in zip(spkinds,spkts) \n                        if spkind==gid], timeRange) for gid in set(spkinds)]\n                    if stat == 'sync':\n                        # (SPIKE-Sync measure)' # see http://www.scholarpedia.org/article/Measures_of_spike_train_synchrony\n                        syncMat = [pyspike.spike_sync(spkmat)]\n                        #graphType = 'bar'\n                    elif stat == 'pairsync':\n                        # (SPIKE-Sync measure)' # see http://www.scholarpedia.org/article/Measures_of_spike_train_synchrony\n                        syncMat = np.mean(pyspike.spike_sync_matrix(spkmat), 0)\n                        \n\n                    statData.append(syncMat)\n\n            colors.insert(0, popColors[subset] if subset in popColors \n                else colorList[iplot%len(colorList)])  # colors in inverse order\n\n        # if 'allCells' included make it black\n        if include[0] == 'allCells':\n            #if graphType == 'boxplot':\n            colors.insert(len(include), (0.5,0.5,0.5))  # \n            del colors[0]\n\n        # boxplot\n        if graphType == 'boxplot':\n            meanpointprops = dict(marker=(5,1,0), markeredgecolor='black', markerfacecolor='white')\n            labels = legendLabels if legendLabels else include\n            bp=plt.boxplot(statData, labels=labels[::-1], notch=False, sym='k+', meanprops=meanpointprops, \n                        whis=1.5, widths=0.6, vert=False, showmeans=True, patch_artist=True)\n            plt.xlabel(xlabel, fontsize=fontsiz)\n            plt.ylabel('Population', fontsize=fontsiz) \n\n            icolor=0\n            borderColor = 'k'\n            for i in range(0, len(bp['boxes'])):\n                icolor = i\n                bp['boxes'][i].set_facecolor(colors[icolor])\n                bp['boxes'][i].set_linewidth(2)\n                # we have two whiskers!\n                bp['whiskers'][i*2].set_color(borderColor)\n                bp['whiskers'][i*2 + 1].set_color(borderColor)\n                bp['whiskers'][i*2].set_linewidth(2)\n                bp['whiskers'][i*2 + 1].set_linewidth(2)\n                bp['medians'][i].set_color(borderColor)\n                bp['medians'][i].set_linewidth(3)\n                # for f in bp['fliers']:\n                #    f.set_color(colors[icolor])\n                #    print f\n                # and 4 caps to remove\n                for c in bp['caps']:\n                    c.set_color(borderColor)\n                    c.set_linewidth(2)\n\n            ax = plt.gca()\n            ax.spines['top'].set_visible(False)\n            ax.spines['right'].set_visible(False)\n            ax.spines['bottom'].set_visible(False)\n            ax.get_xaxis().tick_bottom()\n            ax.get_yaxis().tick_left()\n            ax.tick_params(axis='x', length=0)\n            ax.tick_params(axis='y', direction='out')\n            ax.grid(axis='x', color=\"0.9\", linestyle='-', linewidth=1)\n            ax.set_axisbelow(True)\n            if xlim: ax.set_xlim(xlim)\n\n        # histogram\n        elif graphType == 'histogram':\n            import numpy as np\n\n            nmax = 0\n            pdfmax = 0\n            binmax = 0\n            for i,data in enumerate(statData):  # fix \n                if histlogx:\n                    histbins = np.logspace(np.log10(histmin), np.log10(max(data)), bins)\n                else:\n                    histbins = bins\n\n                if histmin: # min value \n                    data = np.array(data)\n                    data = data[data>histmin]\n                \n                # if histlogy:\n                #     data = [np.log10(x) for x in data]\n\n                if density:\n                    weights = np.ones_like(data)/float(len(data)) \n                else: \n                    weights = np.ones_like(data)\n\n                n, binedges,_ = plt.hist(data,  bins=histbins, histtype='step', color=colors[i], linewidth=2, weights=weights)#, normed=1)#, normed=density)# weights=weights)\n                if histShading:\n                    plt.hist(data, bins=histbins, alpha=0.05, color=colors[i], linewidth=0, weights=weights) \n                label = legendLabels[-i-1] if legendLabels else str(include[-i-1])\n                if histShading:\n                    plt.hist([-10], bins=histbins, fc=((colors[i][0], colors[i][1], colors[i][2],0.05)), edgecolor=colors[i], linewidth=2, label=label)\n                else:\n                    plt.hist([-10], bins=histbins, fc=((1,1,1),0), edgecolor=colors[i], linewidth=2, label=label)\n                nmax = max(nmax, max(n))\n                binmax = max(binmax, binedges[-1])\n                if histlogx: \n                    plt.xscale('log')\n\n                if normfit:\n                    def lognorm(meaninput, stdinput, binedges, n, popLabel, color):\n                        from scipy import stats \n                        M = float(meaninput) # Geometric mean == median\n                        s = float(stdinput) # Geometric standard deviation\n                        mu = np.log10(M) # Mean of log(X)\n                        sigma = np.log10(s) # Standard deviation of log(X)                        \n                        shape = sigma # Scipy's shape parameter\n                        scale = np.power(10, mu) # Scipy's scale parameter\n                        x = [(binedges[i]+binedges[i+1])/2.0 for i in range(len(binedges)-1)]  #np.linspace(histmin, 30, num=400) # values for x-axis\n                        pdf = stats.lognorm.pdf(x, shape, loc=0, scale=scale) # probability distribution\n                        R, p = scipy.stats.pearsonr(n, pdf)\n                        print('    Pop %s rate: mean=%f, std=%f, lognorm mu=%f, lognorm sigma=%f, R=%.2f (p-value=%.2f)' % (popLabel, M, s, mu, sigma, R, p))\n                        plt.semilogx(x, pdf, color=color, ls='dashed')\n                        return pdf\n\n\n                    fitmean = np.mean(data)\n                    fitstd = np.std(data)\n                    pdf=lognorm(fitmean, fitstd, binedges, n, label, colors[i])\n                    pdfmax = max(pdfmax, max(pdf))\n                    nmax = max(nmax, pdfmax)\n\n                    # check normality of distribution\n                    #W, p = scipy.stats.shapiro(data)\n                    #print 'Pop %s rate: mean = %f, std = %f, normality (Shapiro-Wilk test) = %f, p-value = %f' % (include[i], mu, sigma, W, p)\n\n\n            plt.xlabel(xlabel, fontsize=fontsiz)\n            plt.ylabel('Probability of occurrence' if density else 'Frequency', fontsize=fontsiz)\n            xmax = binmax\n            plt.xlim(histmin, xmax)\n            plt.ylim(0, 1.1*nmax if density else np.ceil(1.1*nmax)) #min(n[n>=0]), max(n[n>=0]))\n            plt.legend(fontsize=fontsiz)\n\n            # plt.show() # REMOVE!\n\n            # if xlim: ax.set_xlim(xlim)\n\n            #from IPython import embed; embed()\n\n        # scatter\n        elif graphType == 'scatter':\n            from scipy import stats\n            for i,(ynorms,data) in enumerate(zip(ynormsData, statData)):\n                mean, binedges, _ = stats.binned_statistic(ynorms, data, 'mean', bins=bins)\n                median, binedges, _ = stats.binned_statistic(ynorms, data, 'median', bins=bins)\n                #p25 = lambda x: np.percentile(x, 25)\n                #p75 = lambda x: np.percentile(x, 75)\n                \n                std, binedges, _ = stats.binned_statistic(ynorms, data, 'std', bins=bins)\n                #per25, binedges, _ = stats.binned_statistic(ynorms, data, p25, bins=bins)\n                #per75, binedges, _ = stats.binned_statistic(ynorms, data, p75, bins=bins)\n                \n                label = legendLabels[-i-1] if legendLabels else str(include[-i-1])\n                if kwargs.get('differentColor', None):\n                    threshold = kwargs['differentColor'][0]\n                    newColor = kwargs['differentColor'][1] #\n                    plt.scatter(ynorms[:threshold], data[:threshold], color=[6/255.0,8/255.0,(64+30)/255.0], label=label, s=2) #, [0/255.0,215/255.0,255/255.0], [88/255.0,204/255.0,20/255.0]\n                    plt.scatter(ynorms[threshold:], data[threshold:], color=newColor, alpha= 0.2, s=2) #[88/255.0,204/255.0,20/255.0]\n                else:\n                    plt.scatter(ynorms, data, color=[0/255.0,215/255.0,255/255.0], label=label, s=2) #[88/255.0,204/255.0,20/255.0]\n                binstep = binedges[1]-binedges[0]\n                bincenters = [b+binstep/2 for b in binedges[:-1]] \n                plt.errorbar(bincenters, mean, yerr=std, color=[6/255.0,70/255.0,163/255.0], fmt = 'o-',capthick=1, capsize=5) #[44/255.0,53/255.0,127/255.0]\n                #plt.errorbar(bincenters, mean, yerr=[mean-per25,per75-mean], fmt='go-',capthick=1, capsize=5)\n            ylims=plt.ylim()\n            plt.ylim(0,ylims[1])\n            plt.xlabel('normalized y location (um)', fontsize=fontsiz)\n            #plt.xlabel('avg rate (Hz)', fontsize=fontsiz)\n            plt.ylabel(xlabel, fontsize=fontsiz)\n            plt.legend(fontsize=fontsiz)\n\n            #from IPython import embed; embed()\n\n        # elif graphType == 'bar':\n        #     print range(1, len(statData)+1), statData\n        #     plt.bar(range(1, len(statData)+1), statData, tick_label=include[::-1], \n        #       orientation='horizontal', colors=colors)\n\n        try:\n            plt.tight_layout()\n        except:\n            pass\n\n        # save figure data\n        if saveData:\n            figData = {'include': include, 'statData': statData, 'timeRange': timeRange, 'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n\n            _saveFigData(figData, saveData, 'spikeStats_'+stat)\n\n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig+'_'+'spikeStat_'+graphType+'_'+stat+'.png'\n            else:\n                filename = sim.cfg.filename+'_'+'spikeStat_'+graphType+'_'+stat+'.png'\n            plt.savefig(filename, dpi=dpi)\n\n        # show fig \n        if showFig: _showFigure()\n\n    return fig, {'include': include, 'statData': statData, 'gidsData':gidsData, 'ynormsData':ynormsData}", "response": "Generate spike stats for a single item in the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plotRatePSD(include=['allCells', 'eachPop'], timeRange=None, binSize=5, maxFreq=100, NFFT=256, noverlap=128, smooth=0, overlay=True,\n    ylim = None, popColors = {}, fontSize=12, figSize=(10,8), saveData=None, saveFig=None, showFig=True): \n    ''' \n    Plot firing rate power spectral density (PSD)\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of data series to include. \n            Note: one line per item, not grouped (default: ['allCells', 'eachPop'])\n        - timeRange ([start:stop]): Time range of spikes shown; if None shows all (default: None)\n        - binSize (int): Size in ms of spike bins (default: 5)\n        - maxFreq (float): Maximum frequency to show in plot (default: 100)\n        - NFFT (float): The number of data points used in each block for the FFT (power of 2) (default: 256)\n        - smooth (int): Window size for smoothing; no smoothing if 0 (default: 0)\n        - overlay (True|False): Whether to overlay the data lines or plot in separate subplots (default: True)\n        - graphType ('line'|'bar'): Type of graph to use (line graph or bar plot) (default: 'line')\n        - yaxis ('rate'|'count'): Units of y axis (firing rate in Hz, or spike count) (default: 'rate')\n        - popColors (dict): Dictionary with color (value) used for each population (key) (default: None)\n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure;\n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figure handle\n    '''\n\n    from .. import sim\n\n    print('Plotting firing rate power spectral density (PSD) ...')\n    \n    # Replace 'eachPop' with list of pops\n    if 'eachPop' in include: \n        include.remove('eachPop')\n        for pop in sim.net.allPops: include.append(pop)\n\n    # time range\n    if timeRange is None:\n        timeRange = [0,sim.cfg.duration]\n\n    histData = []\n\n    # create fig\n    fig,ax1 = plt.subplots(figsize=figSize)\n    fontsiz = fontSize\n\n    # set font size\n    plt.rcParams.update({'font.size': fontSize})\n        \n    allPower, allSignal, allFreqs = [], [], []\n\n    # Plot separate line for each entry in include\n    for iplot,subset in enumerate(include):\n        cells, cellGids, netStimLabels = getCellsInclude([subset])   \n        numNetStims = 0\n\n        # Select cells to include\n        if len(cellGids) > 0:\n            try:\n                spkinds,spkts = list(zip(*[(spkgid,spkt) for spkgid,spkt in zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]))\n            except:\n                spkinds,spkts = [],[]\n        else: \n            spkinds,spkts = [],[]\n\n\n        # Add NetStim spikes\n        spkts, spkinds = list(spkts), list(spkinds)\n        numNetStims = 0\n        if 'stims' in sim.allSimData:\n            for netStimLabel in netStimLabels:\n                netStimSpks = [spk for cell,stims in sim.allSimData['stims'].items() \\\n                    for stimLabel,stimSpks in stims.items() for spk in stimSpks if stimLabel == netStimLabel]\n                if len(netStimSpks) > 0:\n                    lastInd = max(spkinds) if len(spkinds)>0 else 0\n                    spktsNew = netStimSpks \n                    spkindsNew = [lastInd+1+i for i in range(len(netStimSpks))]\n                    spkts.extend(spktsNew)\n                    spkinds.extend(spkindsNew)\n                    numNetStims += 1\n\n        histo = np.histogram(spkts, bins = np.arange(timeRange[0], timeRange[1], binSize))\n        histoT = histo[1][:-1]+binSize/2\n        histoCount = histo[0] \n        histoCount = histoCount * (1000.0 / binSize) / (len(cellGids)+numNetStims) # convert to rates\n\n        histData.append(histoCount)\n\n        color = popColors[subset] if isinstance(subset, (str, tuple)) and subset in popColors else colorList[iplot%len(colorList)] \n\n        if not overlay: \n            plt.subplot(len(include),1,iplot+1)  # if subplot, create new subplot\n            title (str(subset), fontsize=fontsiz)\n            color = 'blue'\n        \n        Fs = 1000.0/binSize # ACTUALLY DEPENDS ON BIN WINDOW!!! RATE NOT SPIKE!\n        power = mlab.psd(histoCount, Fs=Fs, NFFT=NFFT, detrend=mlab.detrend_none, window=mlab.window_hanning, \n            noverlap=noverlap, pad_to=None, sides='default', scale_by_freq=None)\n\n        if smooth:\n            signal = _smooth1d(10*np.log10(power[0]), smooth)\n        else:\n            signal = 10*np.log10(power[0])\n        freqs = power[1]\n\n        allFreqs.append(freqs)\n        allPower.append(power)\n        allSignal.append(signal)\n\n        plt.plot(freqs[freqs<maxFreq], signal[freqs<maxFreq], linewidth=1.5, color=color)\n\n        plt.xlabel('Frequency (Hz)', fontsize=fontsiz)\n        plt.ylabel('Power Spectral Density (dB/Hz)', fontsize=fontsiz) # add yaxis in opposite side\n        plt.xlim([0, maxFreq])\n        if ylim: plt.ylim(ylim)\n\n    # if len(include) < 5:  # if apply tight_layout with many subplots it inverts the y-axis\n    #     try:\n    #         plt.tight_layout()\n    #     except:\n    #         pass\n\n    # Add legend\n    if overlay:\n        for i,subset in enumerate(include):\n            color = popColors[subset] if isinstance(subset, basestring) and subset in popColors else colorList[i%len(colorList)] \n            plt.plot(0,0,color=color,label=str(subset))\n        plt.legend(fontsize=fontsiz, loc=1)#, bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0.)\n        maxLabelLen = min(10,max([len(str(l)) for l in include]))\n        #plt.subplots_adjust(right=(0.9-0.012*maxLabelLen))\n\n\n    # save figure data\n    if saveData:\n        figData = {'histData': histData, 'histT': histoT, 'include': include, 'timeRange': timeRange, 'binSize': binSize,\n         'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n    \n        _saveFigData(figData, saveData, 'spikeHist')\n \n    # save figure\n    if saveFig: \n        if isinstance(saveFig, basestring):\n            filename = saveFig\n        else:\n            filename = sim.cfg.filename+'_'+'spikePSD.png'\n        plt.savefig(filename)\n\n    # show fig \n    if showFig: _showFigure()\n\n    return fig, {'allSignal':allSignal, 'allPower':allPower, 'allFreqs':allFreqs}", "response": "Generate a single firing rate power spectral density plot for a single item."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ckchol(M):\n    from numpy import linalg, matrix, eye, size\n\n    try: # First, try the Cholesky decomposition\n        output=linalg.cholesky(M)\n    except: # If not, just return garbage\n        print('WARNING: Cholesky failed, so returning (invalid) identity matrix!')    \n        output=matrix(eye(size(M,0)))\n    \n    return output", "response": "This function computes Cholesky decomposition of the matrix M and returns the identity matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the soma position ; Used to calculate LFP calc", "response": "def getSomaPos(self):\n        ''' Get soma position;\n        Used to calculate seg coords for LFP calc (one per population cell; assumes same morphology)'''\n        n3dsoma = 0\n        r3dsoma = np.zeros(3)\n        for sec in [sec for secName, sec in self.secs.items() if 'soma' in secName]:\n            sec['hObj'].push()\n            n3d = int(h.n3d())  # get number of n3d points in each section\n            r3d = np.zeros((3, n3d))  # to hold locations of 3D morphology for the current section\n            n3dsoma += n3d\n\n            for i in range(n3d):\n                r3dsoma[0] += h.x3d(i)\n                r3dsoma[1] += h.y3d(i)\n                r3dsoma[2] += h.z3d(i)\n\n            h.pop_section() \n        \n        r3dsoma /= n3dsoma\n\n        return r3dsoma"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates absolute seg coords by translating the relative seg coords", "response": "def calcAbsSegCoords(self):\n        ''' Calculate absolute seg coords by translating the relative seg coords -- used for LFP calc'''\n        from .. import sim\n\n        p3dsoma = self.getSomaPos()\n        pop = self.tags['pop']\n        morphSegCoords = sim.net.pops[pop]._morphSegCoords\n\n        # rotated coordinates around z axis first then shift relative to the soma\n        self._segCoords = {}\n        p3dsoma = p3dsoma[np.newaxis].T  # trasnpose 1d array to enable matrix calculation\n        self._segCoords['p0'] = p3dsoma + morphSegCoords['p0']\n        self._segCoords['p1'] = p3dsoma + morphSegCoords['p1']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setImembPtr(self): \n        jseg = 0\n        for sec in list(self.secs.values()):\n            hSec = sec['hObj']\n            for iseg, seg in enumerate(hSec):\n                self.imembPtr.pset(jseg, seg._ref_i_membrane_)  # notice the underscore at the end (in nA)\n                jseg += 1", "response": "Set PtrVector to point to the i_membrane_"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngathers membrane currents from PtrVector into imVec", "response": "def getImemb(self):\n        \"\"\"Gather membrane currents from PtrVector into imVec (does not need a loop!)\"\"\"\n        self.imembPtr.gather(self.imembVec)\n        return self.imembVec.as_numpy()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls after h. define_shape to update cell coords", "response": "def updateShape(self):\n        \"\"\"Call after h.define_shape() to update cell coords\"\"\"\n        x = self.tags['x']\n        y = -self.tags['y'] # Neuron y-axis positive = upwards, so assume pia=0 and cortical depth = neg\n        z = self.tags['z']\n                \n        for sec in list(self.secs.values()):\n            if 'geom' in sec and 'pt3d' not in sec['geom']:  # only cells that didn't have pt3d before\n                sec['geom']['pt3d'] = []\n                sec['hObj'].push()\n                n3d = int(h.n3d())  # get number of n3d points in each section\n                for i in range(n3d):\n                    # by default L is added in x-axis; shift to y-axis; z increases 100um for each cell so set to 0\n                    pt3d = [h.y3d(i), h.x3d(i), 0, h.diam3d(i)] \n                    sec['geom']['pt3d'].append(pt3d) \n                    h.pt3dchange(i, x+pt3d[0], y+pt3d[1], z+pt3d[2], pt3d[3], sec=sec['hObj'])\n                h.pop_section()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef saveWeights(sim):\n    ''' Save the weights for each plastic synapse '''\n    with open(sim.weightsfilename,'w') as fid:\n        for weightdata in sim.allWeights:\n            fid.write('%0.0f' % weightdata[0]) # Time\n            for i in range(1,len(weightdata)): fid.write('\\t%0.8f' % weightdata[i])\n            fid.write('\\n')\n    print(('Saved weights as %s' % sim.weightsfilename))", "response": "Save the weights for each plastic synapse"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validateFunction(strFunc, netParamsVars):\n    ''' returns True if \"strFunc\" can be evaluated'''\n    from math import exp, log, sqrt, sin, cos, tan, asin, acos, atan, sinh, cosh, tanh, pi, e\n    rand = h.Random()\n    stringFuncRandMethods = ['binomial', 'discunif', 'erlang', 'geometric', 'hypergeo', \n        'lognormal', 'negexp', 'normal', 'poisson', 'uniform', 'weibull']\n    \n    for randmeth in stringFuncRandMethods: strFunc = strFunc.replace(randmeth, 'rand.'+randmeth)\n    \n    variables = {\n        \"pre_x\"  : 1, \"pre_y\"  : 1, \"pre_z\"  : 1,\n        \"post_x\" : 1, \"post_y\" : 1, \"post_z\" : 1, \n        \"dist_x\" : 1, \"dist_y\" : 1, \"dist_z\" : 1,\n        \"pre_xnorm\"  : 1, \"pre_ynorm\"   : 1, \"pre_znorm\"  : 1,\n        \"post_xnorm\" : 1, \"post_ynorm\"  : 1, \"post_znorm\" : 1,\n        \"dist_xnorm\" : 1, \"dist_ynorm\"  : 1, \"dist_znorm\" : 1,\n        \"dist_3D\"    : 1, \"dist_3D_border\" : 1, \"dist_2D\" : 1,\n        \"dist_norm3D\": 1, \"dist_norm2D\" : 1, \"rand\": rand,\n        \"exp\": exp, \"log\":log, \"sqrt\": sqrt,\n        \"sin\":sin, \"cos\":cos, \"tan\":tan, \"asin\":asin, \n        \"acos\":acos, \"atan\":atan, \"sinh\":sinh, \"cosh\":cosh, \n        \"tanh\":tanh, \"pi\":pi,\"e\": e\n    }\n    \n    # add netParams variables\n    for k, v in netParamsVars.items():\n        if isinstance(v, Number):\n            variables[k] = v\n\n    try: \n        eval(strFunc, variables)\n        return True\n    except:\n        return False", "response": "returns True if strFunc can be evaluated"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plotConn (includePre = ['all'], includePost = ['all'], feature = 'strength', orderBy = 'gid', figSize = (10,10), groupBy = 'pop', groupByIntervalPre = None, groupByIntervalPost = None, graphType = 'matrix', synOrConn = 'syn', synMech = None, connsFile = None, tagsFile = None, clim = None, fontSize = 12, saveData = None, saveFig = None, showFig = True): \n    ''' \n    Plot network connectivity\n        - includePre (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Cells to show (default: ['all'])\n        - includePost (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Cells to show (default: ['all'])\n        - feature ('weight'|'delay'|'numConns'|'probability'|'strength'|'convergence'|'divergence'): Feature to show in connectivity matrix; \n            the only features applicable to groupBy='cell' are 'weight', 'delay' and 'numConns';  'strength' = weight * probability (default: 'strength')\n        - groupBy ('pop'|'cell'|'y'|: Show matrix for individual cells, populations, or by other numeric tag such as 'y' (default: 'pop')\n        - groupByInterval (int or float): Interval of groupBy feature to group cells by in conn matrix, e.g. 100 to group by cortical depth in steps of 100 um   (default: None)\n        - orderBy ('gid'|'y'|'ynorm'|...): Unique numeric cell property to order x and y axes by, e.g. 'gid', 'ynorm', 'y' (requires groupBy='cells') (default: 'gid')\n        - graphType ('matrix','bar','pie'): Type of graph to represent data (default: 'matrix')\n        - synOrConn ('syn'|'conn'): Use synapses or connections; note 1 connection can have multiple synapses (default: 'syn')\n        - figSize ((width, height)): Size of figure (default: (10,10))\n        - synMech (['AMPA', 'GABAA',...]): Show results only for these syn mechs (default: None)\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure; \n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure; \n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figure handles\n    '''\n    \n    from .. import sim\n\n    print('Plotting connectivity matrix...')\n\n    if connsFile and tagsFile:\n        connMatrix, pre, post = _plotConnCalculateFromFile(includePre, includePost, feature, orderBy, groupBy, groupByIntervalPre, groupByIntervalPost, synOrConn, synMech, connsFile, tagsFile)\n    else:\n        connMatrix, pre, post = _plotConnCalculateFromSim(includePre, includePost, feature, orderBy, groupBy, groupByIntervalPre, groupByIntervalPost, synOrConn, synMech)\n\n\n    if connMatrix is None:\n        print(\"Error calculating connMatrix in plotConn()\")\n        return None\n\n    # set font size\n    plt.rcParams.update({'font.size': fontSize})\n\n    # matrix plot\n    if graphType == 'matrix':\n        # Create plot\n        fig = plt.figure(figsize=figSize)\n        fig.subplots_adjust(right=0.98) # Less space on right\n        fig.subplots_adjust(top=0.96) # Less space on top\n        fig.subplots_adjust(bottom=0.02) # Less space on bottom\n        h = plt.axes()\n\n        plt.imshow(connMatrix, interpolation='nearest', cmap='viridis', vmin=np.nanmin(connMatrix), vmax=np.nanmax(connMatrix))  #_bicolormap(gap=0)\n\n        # Plot grid lines\n            \n        if groupBy == 'cell':\n            cellsPre, cellsPost = pre, post\n\n            # Make pretty\n            stepy = max(1, int(len(cellsPre)/10.0))\n            basey = 100 if stepy>100 else 10\n            stepy = max(1, int(basey * np.floor(float(stepy)/basey)))\n            stepx = max(1, int(len(cellsPost)/10.0))\n            basex = 100 if stepx>100 else 10\n            stepx = max(1, int(basex * np.floor(float(stepx)/basex)))\n\n            h.set_xticks(np.arange(0,len(cellsPost),stepx))\n            h.set_yticks(np.arange(0,len(cellsPre),stepy))\n            h.set_xticklabels(np.arange(0,len(cellsPost),stepx))\n            h.set_yticklabels(np.arange(0,len(cellsPost),stepy))\n            h.xaxis.set_ticks_position('top')\n            plt.xlim(-0.5,len(cellsPost)-0.5)\n            plt.ylim(len(cellsPre)-0.5,-0.5)\n\n        elif groupBy == 'pop':\n            popsPre, popsPost = pre, post\n\n            for ipop, pop in enumerate(popsPre):\n                plt.plot(np.array([0,len(popsPre)])-0.5,np.array([ipop,ipop])-0.5,'-',c=(0.7,0.7,0.7))\n            for ipop, pop in enumerate(popsPost):\n                plt.plot(np.array([ipop,ipop])-0.5,np.array([0,len(popsPost)])-0.5,'-',c=(0.7,0.7,0.7))\n\n            # Make pretty\n            h.set_xticks(list(range(len(popsPost))))\n            h.set_yticks(list(range(len(popsPre))))\n            h.set_xticklabels(popsPost)\n            h.set_yticklabels(popsPre)\n            h.xaxis.set_ticks_position('top')\n            plt.xlim(-0.5,len(popsPost)-0.5)\n            plt.ylim(len(popsPre)-0.5,-0.5)\n\n        else:\n            groupsPre, groupsPost = pre, post\n\n            for igroup, group in enumerate(groupsPre):\n                plt.plot(np.array([0,len(groupsPre)])-0.5,np.array([igroup,igroup])-0.5,'-',c=(0.7,0.7,0.7))\n            for igroup, group in enumerate(groupsPost):\n                plt.plot(np.array([igroup,igroup])-0.5,np.array([0,len(groupsPost)])-0.5,'-',c=(0.7,0.7,0.7))\n\n            # Make pretty\n            h.set_xticks([i-0.5 for i in range(len(groupsPost))])\n            h.set_yticks([i-0.5 for i in range(len(groupsPre))])\n            h.set_xticklabels([int(x) if x>1 else x for x in groupsPost])\n            h.set_yticklabels([int(x) if x>1 else x for x in groupsPre])\n            h.xaxis.set_ticks_position('top')\n            plt.xlim(-0.5,len(groupsPost)-0.5)\n            plt.ylim(len(groupsPre)-0.5,-0.5)\n\n        if not clim: clim = [np.nanmin(connMatrix), np.nanmax(connMatrix)]\n        plt.clim(clim[0], clim[1])\n        plt.colorbar(label=feature, shrink=0.8) #.set_label(label='Fitness',size=20,weight='bold')\n        plt.xlabel('post')\n        h.xaxis.set_label_coords(0.5, 1.06)\n        plt.ylabel('pre')\n        plt.title ('Connection '+feature+' matrix', y=1.08)\n\n    # stacked bar graph\n    elif graphType == 'bar':\n        if groupBy == 'pop':\n            popsPre, popsPost = pre, post\n\n            from netpyne.support import stackedBarGraph \n            SBG = stackedBarGraph.StackedBarGrapher()\n    \n            fig = plt.figure(figsize=figSize)\n            ax = fig.add_subplot(111)\n            SBG.stackedBarPlot(ax, connMatrix.transpose(), colorList, xLabels=popsPost, gap = 0.1, scale=False, xlabel='postsynaptic', ylabel = feature)\n            plt.title ('Connection '+feature+' stacked bar graph')\n            plt.legend(popsPre)\n            plt.tight_layout()\n\n        elif groupBy == 'cell':\n            print('Error: plotConn graphType=\"bar\" with groupBy=\"cell\" not implemented')\n\n    elif graphType == 'pie':\n        print('Error: plotConn graphType=\"pie\" not yet implemented')\n\n\n    #save figure data\n    if saveData:\n        figData = {'connMatrix': connMatrix, 'feature': feature, 'groupBy': groupBy,\n         'includePre': includePre, 'includePost': includePost, 'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n    \n        _saveFigData(figData, saveData, 'conn')\n \n    # save figure\n    if saveFig: \n        if isinstance(saveFig, basestring):\n            filename = saveFig\n        else:\n            filename = sim.cfg.filename+'_'+'conn_'+feature+'.png'\n        plt.savefig(filename)\n\n    # show fig \n    if showFig: _showFigure()\n\n    return fig, {'connMatrix': connMatrix, 'feature': feature, 'groupBy': groupBy, 'includePre': includePre, 'includePost': includePost}", "response": "Plot connectivity matrix for a single cell or cell."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the 2D representation of the network cell locations and connections.", "response": "def plot2Dnet (include = ['allCells'], figSize = (12,12), view = 'xy', showConns = True, popColors = None, fontSize = 12,\n                tagsFile = None, saveData = None, saveFig = None, showFig = True): \n    ''' \n    Plot 2D representation of network cell positions and connections\n        - include (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Cells to show (default: ['all'])\n        - showConns (True|False): Whether to show connections or not (default: True)\n        - figSize ((width, height)): Size of figure (default: (12,12))\n        - view ('xy', 'xz'): Perspective view: front ('xy') or top-down ('xz')\n        - popColors (dict): Dictionary with color (value) used for each population (key) (default: None)\n        - saveData (None|'fileName'): File name where to save the final data used to generate the figure (default: None)\n        - saveFig (None|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)(default: None)\n        - showFig (True|False): Whether to show the figure or not;\n            if set to True uses filename from simConfig (default: None)\n\n        - Returns figure handles\n    '''\n    from .. import sim\n\n    print('Plotting 2D representation of network cell locations and connections...')\n\n    fig = plt.figure(figsize=figSize)\n\n    # front view\n    if view == 'xy':\n        ycoord = 'y'\n    elif view == 'xz':\n        ycoord = 'z'\n\n    if tagsFile:\n        print('Loading tags file...')\n        import json\n        with open(tagsFile, 'r') as fileObj: tagsTmp = json.load(fileObj)['tags']\n        tagsFormat = tagsTmp.pop('format', [])\n        tags = {int(k): v for k,v in tagsTmp.items()} # find method to load json with int keys?\n        del tagsTmp\n\n        # set indices of fields to read compact format (no keys)\n        missing = []\n        popIndex = tagsFormat.index('pop') if 'pop' in tagsFormat else missing.append('pop')\n        xIndex = tagsFormat.index('x') if 'x' in tagsFormat else missing.append('x')\n        yIndex = tagsFormat.index('y') if 'y' in tagsFormat else missing.append('y')\n        zIndex = tagsFormat.index('z') if 'z' in tagsFormat else missing.append('z')\n        if len(missing) > 0:\n            print(\"Missing:\")\n            print(missing)\n            return None, None, None \n\n        # find pre and post cells\n        if tags:\n            cellGids = getCellsIncludeTags(include, tags, tagsFormat)\n            popLabels = list(set([tags[gid][popIndex] for gid in cellGids]))\n            \n            # pop and cell colors\n            popColorsTmp = {popLabel: colorList[ipop%len(colorList)] for ipop,popLabel in enumerate(popLabels)} # dict with color for each pop\n            if popColors: popColorsTmp.update(popColors)\n            popColors = popColorsTmp\n            cellColors = [popColors[tags[gid][popIndex]] for gid in cellGids]\n            \n            # cell locations\n            posX = [tags[gid][xIndex] for gid in cellGids]  # get all x positions\n            if ycoord == 'y':\n                posY = [tags[gid][yIndex] for gid in cellGids]  # get all y positions\n            elif ycoord == 'z':\n                posY = [tags[gid][zIndex] for gid in cellGids]  # get all y positions\n        else:\n            print('Error loading tags from file') \n            return None\n\n    else:\n        cells, cellGids, _ = getCellsInclude(include)           \n        selectedPops = [cell['tags']['pop'] for cell in cells]\n        popLabels = [pop for pop in sim.net.allPops if pop in selectedPops] # preserves original ordering\n        \n        # pop and cell colors\n        popColorsTmp = {popLabel: colorList[ipop%len(colorList)] for ipop,popLabel in enumerate(popLabels)} # dict with color for each pop\n        if popColors: popColorsTmp.update(popColors)\n        popColors = popColorsTmp\n        cellColors = [popColors[cell['tags']['pop']] for cell in cells]\n\n        # cell locations\n        posX = [cell['tags']['x'] for cell in cells]  # get all x positions\n        posY = [cell['tags'][ycoord] for cell in cells]  # get all y positions\n    \n\n    plt.scatter(posX, posY, s=60, color = cellColors) # plot cell soma positions\n    posXpre, posYpre = [], []\n    posXpost, posYpost = [], []\n    if showConns and not tagsFile:\n        for postCell in cells:\n            for con in postCell['conns']:  # plot connections between cells\n                if not isinstance(con['preGid'], basestring) and con['preGid'] in cellGids:\n                    posXpre,posYpre = next(((cell['tags']['x'],cell['tags'][ycoord]) for cell in cells if cell['gid']==con['preGid']), None)  \n                    posXpost,posYpost = postCell['tags']['x'], postCell['tags'][ycoord] \n                    color='red'\n                    if con['synMech'] in ['inh', 'GABA', 'GABAA', 'GABAB']:\n                        color = 'blue'\n                    width = 0.1 #50*con['weight']\n                    plt.plot([posXpre, posXpost], [posYpre, posYpost], color=color, linewidth=width) # plot line from pre to post\n    \n    plt.xlabel('x (um)')\n    plt.ylabel(ycoord+' (um)') \n    plt.xlim([min(posX)-0.05*max(posX),1.05*max(posX)]) \n    plt.ylim([min(posY)-0.05*max(posY),1.05*max(posY)])\n    fontsiz = fontSize\n\n    for popLabel in popLabels:\n        plt.plot(0,0,color=popColors[popLabel],label=popLabel)\n    plt.legend(fontsize=fontsiz, bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n    ax = plt.gca()\n    ax.invert_yaxis()\n\n    # save figure data\n    if saveData:\n        figData = {'posX': posX, 'posY': posY, 'posX': cellColors, 'posXpre': posXpre, 'posXpost': posXpost, 'posYpre': posYpre, 'posYpost': posYpost,\n         'include': include, 'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n    \n        _saveFigData(figData, saveData, '2Dnet')\n \n    # save figure\n    if saveFig: \n        if isinstance(saveFig, basestring):\n            filename = saveFig\n        else:\n            filename = sim.cfg.filename+'_'+'2Dnet.png'\n        plt.savefig(filename)\n\n    # show fig \n    if showFig: _showFigure()\n\n    return fig, {'include': include, 'posX': posX, 'posY': posY, 'posXpre': posXpre, 'posXpost': posXpost, 'posYpre': posYpre, 'posYpost': posYpost}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plotShape (includePost = ['all'], includePre = ['all'], showSyns = False, showElectrodes = False, synStyle = '.', synSiz=3, dist=0.6, cvar=None, cvals=None, \n    iv=False, ivprops=None, includeAxon=True, bkgColor = None, fontSize = 12, figSize = (10,8), saveData = None, dpi = 300, saveFig = None, showFig = True): \n    ''' \n    Plot 3D cell shape using NEURON Interview PlotShape\n        - includePre: (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of presynaptic cells to consider \n        when plotting connections (default: ['all'])\n        - includePost: (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): List of cells to show shape of (default: ['all'])\n        - showSyns (True|False): Show synaptic connections in 3D view (default: False)\n        - showElectrodes (True|False): Show LFP electrodes in 3D view (default: False)\n        - synStyle: Style of marker to show synapses (default: '.') \n        - dist: 3D distance (like zoom) (default: 0.6)\n        - synSize: Size of marker to show synapses (default: 3)\n        - cvar: ('numSyns'|'weightNorm') Variable to represent in shape plot (default: None)\n        - cvals: List of values to represent in shape plot; must be same as num segments (default: None)\n        - iv: Use NEURON Interviews (instead of matplotlib) to show shape plot (default: None)\n        - ivprops: Dict of properties to plot using Interviews (default: None)\n        - includeAxon: Include axon in shape plot (default: True)\n        - bkgColor (list/tuple with 4 floats): RGBA list/tuple with bakcground color eg. (0.5, 0.2, 0.1, 1.0) (default: None) \n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure; \n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figure handles\n    '''\n\n    from .. import sim\n    from neuron import h\n\n    print('Plotting 3D cell shape ...')\n\n    cellsPreGids = [c.gid for c in sim.getCellsList(includePre)] if includePre else []\n    cellsPost = sim.getCellsList(includePost)\n\n    if not hasattr(sim.net, 'compartCells'): sim.net.compartCells = [c for c in cellsPost if type(c) is sim.CompartCell]\n    sim.net.defineCellShapes()  # in case some cells had stylized morphologies without 3d pts\n\n    if not iv: # plot using Python instead of interviews\n        from mpl_toolkits.mplot3d import Axes3D\n        from netpyne.support import morphology as morph # code adapted from https://github.com/ahwillia/PyNeuron-Toolbox\n        \n        # create secList from include\n        \n        secs = None\n\n        # Set cvals and secs\n        if not cvals and cvar:\n            cvals = []\n            secs = []\n            # weighNorm\n            if cvar == 'weightNorm':\n                for cellPost in cellsPost:\n                    cellSecs = list(cellPost.secs.values()) if includeAxon else [s for s in list(cellPost.secs.values()) if 'axon' not in s['hObj'].hname()] \n                    for sec in cellSecs:\n                        if 'weightNorm' in sec:\n                            secs.append(sec['hObj'])\n                            cvals.extend(sec['weightNorm'])\n\n                cvals = np.array(cvals)\n                cvals = cvals/min(cvals)\n\n            # numSyns\n            elif cvar == 'numSyns':\n                for cellPost in cellsPost:\n                    cellSecs = cellPost.secs if includeAxon else {k:s for k,s in cellPost.secs.items() if 'axon' not in s['hObj'].hname()}\n                    for secLabel,sec in cellSecs.items():\n                        nseg=sec['hObj'].nseg\n                        nsyns = [0] * nseg\n                        secs.append(sec['hObj'])\n                        conns = [conn for conn in cellPost.conns if conn['sec']==secLabel and conn['preGid'] in cellsPreGids]\n                        for conn in conns: nsyns[int(round(conn['loc']*nseg))-1] += 1\n                        cvals.extend(nsyns)\n\n                cvals = np.array(cvals)\n\n        if not secs: secs = [s['hObj'] for cellPost in cellsPost for s in list(cellPost.secs.values())]\n        if not includeAxon:         \n            secs = [sec for sec in secs if 'axon' not in sec.hname()]\n\n        # Plot shapeplot\n        cbLabels = {'numSyns': 'number of synapses per segment', 'weightNorm': 'weight scaling'}\n        plt.rcParams.update({'font.size': fontSize})\n        fig=plt.figure(figsize=figSize)\n        shapeax = plt.subplot(111, projection='3d')\n        shapeax.elev=90 # 90 \n        shapeax.azim=-90 # -90\n        shapeax.dist=dist*shapeax.dist\n        plt.axis('equal')\n        cmap = plt.cm.viridis #plt.cm.jet  #plt.cm.rainbow #plt.cm.jet #YlOrBr_r\n        morph.shapeplot(h,shapeax, sections=secs, cvals=cvals, cmap=cmap)\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n        if cvals is not None and len(cvals)>0: \n            sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=np.min(cvals), vmax=np.max(cvals)))\n            sm._A = []  # fake up the array of the scalar mappable\n            cb = plt.colorbar(sm, fraction=0.15, shrink=0.5, pad=0.05, aspect=20)    \n            if cvar: cb.set_label(cbLabels[cvar], rotation=90, fontsize=fontSize)\n\n        if bkgColor:\n            shapeax.w_xaxis.set_pane_color(bkgColor)\n            shapeax.w_yaxis.set_pane_color(bkgColor)\n            shapeax.w_zaxis.set_pane_color(bkgColor)\n        #shapeax.grid(False)\n\n        # Synapses\n        if showSyns:\n            synColor='red'\n            for cellPost in cellsPost:\n                for sec in list(cellPost.secs.values()):\n                    for synMech in sec['synMechs']:\n                        morph.mark_locations(h, sec['hObj'], synMech['loc'], markspec=synStyle, color=synColor, markersize=synSiz)\n        \n        # Electrodes\n        if showElectrodes:\n            ax = plt.gca()\n            colorOffset = 0\n            if 'avg' in showElectrodes:\n                showElectrodes.remove('avg')\n                colorOffset = 1\n            coords = sim.net.recXElectrode.pos.T[np.array(showElectrodes).astype(int),:]\n            ax.scatter(coords[:,0],coords[:,1],coords[:,2], s=150, c=colorList[colorOffset:len(coords)+colorOffset],\n                marker='v', depthshade=False, edgecolors='k', linewidth=2)\n            for i in range(coords.shape[0]):\n                ax.text(coords[i,0],coords[i,1],coords[i,2], '  '+str(showElectrodes[i]), fontweight='bold' )\n            cb.set_label('Segment total transfer resistance to electrodes (kiloohm)', rotation=90, fontsize=fontSize)\n\n        #plt.title(str(includePre)+' -> '+str(includePost) + ' ' + str(cvar))\n        shapeax.set_xticklabels([])\n        shapeax.set_yticklabels([])\n        shapeax.set_zticklabels([])\n        #shapeax.set_ylabel('y location (um)')\n\n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_shape.png'\n            plt.savefig(filename, dpi=dpi)\n\n        # show fig \n        if showFig: _showFigure()\n\n    else:  # Plot using Interviews\n        # colors: 0 white, 1 black, 2 red, 3 blue, 4 green, 5 orange, 6 brown, 7 violet, 8 yellow, 9 gray\n        from neuron import gui\n        fig = h.Shape()\n        secList = h.SectionList()\n        if not ivprops:\n            ivprops = {'colorSecs': 1, 'colorSyns':2 ,'style': 'O', 'siz':5}\n        \n        for cell in [c for c in cellsPost]: \n            for sec in list(cell.secs.values()):\n                if 'axon' in sec['hObj'].hname() and not includeAxon: continue\n                sec['hObj'].push()\n                secList.append()\n                h.pop_section()\n                if showSyns:\n                    for synMech in sec['synMechs']:\n                        if synMech['hObj']:\n                            # find pre pop using conn[preGid]\n                            # create dict with color for each pre pop; check if exists; increase color counter\n                            # colorsPre[prePop] = colorCounter\n\n                            # find synMech using conn['loc'], conn['sec'] and conn['synMech']\n                            fig.point_mark(synMech['hObj'], ivprops['colorSyns'], ivprops['style'], ivprops['siz']) \n\n        fig.observe(secList)\n        fig.color_list(secList, ivprops['colorSecs'])\n        fig.flush()\n        fig.show(0) # show real diam\n            # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_'+'shape.ps'\n            fig.printfile(filename)\n\n\n    return fig, {}", "response": "Plot the shape of a single cell using NEURON Interview"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters data from freqmin to freqmax using a bandpass filter.", "response": "def bandpass(data, freqmin, freqmax, df, corners=4, zerophase=True):\n    \"\"\"\n    Butterworth-Bandpass Filter.\n\n    Filter data from ``freqmin`` to ``freqmax`` using ``corners``\n    corners.\n    The filter uses :func:`scipy.signal.iirfilter` (for design)\n    and :func:`scipy.signal.sosfilt` (for applying the filter).\n\n    :type data: numpy.ndarray\n    :param data: Data to filter.\n    :param freqmin: Pass band low corner frequency.\n    :param freqmax: Pass band high corner frequency.\n    :param df: Sampling rate in Hz.\n    :param corners: Filter corners / order.\n    :param zerophase: If True, apply filter once forwards and once backwards.\n        This results in twice the filter order but zero phase shift in\n        the resulting filtered trace.\n    :return: Filtered data.\n    \"\"\"\n    fe = 0.5 * df\n    low = freqmin / fe\n    high = freqmax / fe\n    # raise for some bad scenarios\n    if high - 1.0 > -1e-6:\n        msg = (\"Selected high corner frequency ({}) of bandpass is at or \"\n               \"above Nyquist ({}). Applying a high-pass instead.\").format(\n            freqmax, fe)\n        warnings.warn(msg)\n        return highpass(data, freq=freqmin, df=df, corners=corners,\n                        zerophase=zerophase)\n    if low > 1:\n        msg = \"Selected low corner frequency is above Nyquist.\"\n        raise ValueError(msg)\n    z, p, k = iirfilter(corners, [low, high], btype='band',\n                        ftype='butter', output='zpk')\n    sos = zpk2sos(z, p, k)\n    if zerophase:\n        firstpass = sosfilt(sos, data)\n        return sosfilt(sos, firstpass[::-1])[::-1]\n    else:\n        return sosfilt(sos, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter data by bandstop filter.", "response": "def bandstop(data, freqmin, freqmax, df, corners=4, zerophase=False):\n    \"\"\"\n    Butterworth-Bandstop Filter.\n\n    Filter data removing data between frequencies ``freqmin`` and ``freqmax``\n    using ``corners`` corners.\n    The filter uses :func:`scipy.signal.iirfilter` (for design)\n    and :func:`scipy.signal.sosfilt` (for applying the filter).\n\n    :type data: numpy.ndarray\n    :param data: Data to filter.\n    :param freqmin: Stop band low corner frequency.\n    :param freqmax: Stop band high corner frequency.\n    :param df: Sampling rate in Hz.\n    :param corners: Filter corners / order.\n    :param zerophase: If True, apply filter once forwards and once backwards.\n        This results in twice the number of corners but zero phase shift in\n        the resulting filtered trace.\n    :return: Filtered data.\n    \"\"\"\n    fe = 0.5 * df\n    low = freqmin / fe\n    high = freqmax / fe\n    # raise for some bad scenarios\n    if high > 1:\n        high = 1.0\n        msg = \"Selected high corner frequency is above Nyquist. \" + \\\n              \"Setting Nyquist as high corner.\"\n        warnings.warn(msg)\n    if low > 1:\n        msg = \"Selected low corner frequency is above Nyquist.\"\n        raise ValueError(msg)\n    z, p, k = iirfilter(corners, [low, high],\n                        btype='bandstop', ftype='butter', output='zpk')\n    sos = zpk2sos(z, p, k)\n    if zerophase:\n        firstpass = sosfilt(sos, data)\n        return sosfilt(sos, firstpass[::-1])[::-1]\n    else:\n        return sosfilt(sos, data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lowpass(data, freq, df, corners=4, zerophase=False):\n    fe = 0.5 * df\n    f = freq / fe\n    # raise for some bad scenarios\n    if f > 1:\n        f = 1.0\n        msg = \"Selected corner frequency is above Nyquist. \" + \\\n              \"Setting Nyquist as high corner.\"\n        warnings.warn(msg)\n    z, p, k = iirfilter(corners, f, btype='lowpass', ftype='butter',\n                        output='zpk')\n    sos = zpk2sos(z, p, k)\n    if zerophase:\n        firstpass = sosfilt(sos, data)\n        return sosfilt(sos, firstpass[::-1])[::-1]\n    else:\n        return sosfilt(sos, data)", "response": "Butterworth-Lowpass Filter.\n\n    Filter data removing data over certain frequency ``freq`` using ``corners``\n    corners.\n    The filter uses :func:`scipy.signal.iirfilter` (for design)\n    and :func:`scipy.signal.sosfilt` (for applying the filter).\n\n    :type data: numpy.ndarray\n    :param data: Data to filter.\n    :param freq: Filter corner frequency.\n    :param df: Sampling rate in Hz.\n    :param corners: Filter corners / order.\n    :param zerophase: If True, apply filter once forwards and once backwards.\n        This results in twice the number of corners but zero phase shift in\n        the resulting filtered trace.\n    :return: Filtered data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remez_fir(data, freqmin, freqmax, df):\n    # Remez filter description\n    # ========================\n    #\n    # So, let's go over the inputs that you'll have to worry about.\n    # First is numtaps. This parameter will basically determine how good your\n    # filter is and how much processor power it takes up. If you go for some\n    # obscene number of taps (in the thousands) there's other things to worry\n    # about, but with sane numbers (probably below 30-50 in your case) that is\n    # pretty much what it affects (more taps is better, but more expensive\n    #         processing wise). There are other ways to do filters as well\n    # which require less CPU power if you really need it, but I doubt that you\n    # will. Filtering signals basically breaks down to convolution, and apple\n    # has DSP libraries to do lightning fast convolution I'm sure, so don't\n    # worry about this too much. Numtaps is basically equivalent to the number\n    # of terms in the convolution, so a power of 2 is a good idea, 32 is\n    # probably fine.\n    #\n    # bands has literally your list of bands, so you'll break it up into your\n    # low band, your pass band, and your high band. Something like [0, 99, 100,\n    # 999, 1000, 22049] should work, if you want to pass frequencies between\n    # 100-999 Hz (assuming you are sampling at 44.1 kHz).\n    #\n    # desired will just be [0, 1, 0] as you want to drop the high and low\n    # bands, and keep the middle one without modifying the amplitude.\n    #\n    # Also, specify Hz = 44100 (or whatever).\n    #\n    # That should be all you need; run the function and it will spit out a list\n    # of coefficients [c0, ... c(N-1)] where N is your tap count. When you run\n    # this filter, your output signal y[t] will be computed from the input x[t]\n    # like this (t-N means N samples before the current one):\n    #\n    # y[t] = c0*x[t] + c1*x[t-1] + ... + c(N-1)*x[t-(N-1)]\n    #\n    # After playing around with remez for a bit, it looks like numtaps should\n    # be above 100 for a solid filter. See what works out for you. Eventually,\n    # take those coefficients and then move them over and do the convolution\n    # in C or whatever. Also, note the gaps between the bands in the call to\n    # remez. You have to leave some space for the transition in frequency\n    # response to occur, otherwise the call to remez will complain.\n    #\n    # Source:\n    # http://episteme.arstechnica.com/\n    #         eve/forums/a/tpc/f/6330927813/m/175006289731\n    #\n    # take 10% of freqmin and freqmax as \"\"\"corners\"\"\"\n    flt = freqmin - 0.1 * freqmin\n    fut = freqmax + 0.1 * freqmax\n    # bandpass between freqmin and freqmax\n    filt = remez(50, np.array([0, flt, freqmin, freqmax, fut, df / 2 - 1]),\n                 np.array([0, 1, 0]), Hz=df)\n    return convolve(filt, data)", "response": "Filter the data using Remez Fir."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lowpass_fir(data, freq, df, winlen=2048):\n    # Source: Travis Oliphant\n    # https://mail.scipy.org/pipermail/scipy-user/2004-February/002628.html\n    #\n    # There is not currently an FIR-filter design program in SciPy. One\n    # should be constructed as it is not hard to implement (of course making\n    # it generic with all the options you might want would take some time).\n    #\n    # What kind of window are you currently using?\n    #\n    # For your purposes this is what I would do:\n    #\n    # winlen = 2**11 #2**10 = 1024; 2**11 = 2048; 2**12 = 4096\n    # give frequency bins in Hz and sample spacing\n    w = np.fft.fftfreq(winlen, 1 / float(df))\n    # cutoff is low-pass filter\n    myfilter = np.where((abs(w) < freq), 1., 0.)\n    # ideal filter\n    h = np.fft.ifft(myfilter)\n    beta = 11.7\n    # beta implies Kaiser\n    myh = np.fft.fftshift(h) * get_window(beta, winlen)\n    return convolve(abs(myh), data)[winlen / 2:-winlen / 2]", "response": "Low - pass FIR filter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef integer_decimation(data, decimation_factor):\n    if not isinstance(decimation_factor, int):\n        msg = \"Decimation_factor must be an integer!\"\n        raise TypeError(msg)\n\n    # reshape and only use every decimation_factor-th sample\n    data = np.array(data[::decimation_factor])\n    return data", "response": "Downsampling by applying a simple integer decimation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lowpass_cheby_2(data, freq, df, maxorder=12, ba=False,\n                    freq_passband=False):\n    \"\"\"\n    Cheby2-Lowpass Filter\n\n    Filter data by passing data only below a certain frequency.\n    The main purpose of this cheby2 filter is downsampling.\n    #318 shows some plots of this filter design itself.\n\n    This method will iteratively design a filter, whose pass\n    band frequency is determined dynamically, such that the\n    values above the stop band frequency are lower than -96dB.\n\n    :type data: numpy.ndarray\n    :param data: Data to filter.\n    :param freq: The frequency above which signals are attenuated\n        with 95 dB\n    :param df: Sampling rate in Hz.\n    :param maxorder: Maximal order of the designed cheby2 filter\n    :param ba: If True return only the filter coefficients (b, a) instead\n        of filtering\n    :param freq_passband: If True return additionally to the filtered data,\n        the iteratively determined pass band frequency\n    :return: Filtered data.\n    \"\"\"\n    nyquist = df * 0.5\n    # rp - maximum ripple of passband, rs - attenuation of stopband\n    rp, rs, order = 1, 96, 1e99\n    ws = freq / nyquist  # stop band frequency\n    wp = ws  # pass band frequency\n    # raise for some bad scenarios\n    if ws > 1:\n        ws = 1.0\n        msg = \"Selected corner frequency is above Nyquist. \" + \\\n              \"Setting Nyquist as high corner.\"\n        warnings.warn(msg)\n    while True:\n        if order <= maxorder:\n            break\n        wp = wp * 0.99\n        order, wn = cheb2ord(wp, ws, rp, rs, analog=0)\n    if ba:\n        return cheby2(order, rs, wn, btype='low', analog=0, output='ba')\n    z, p, k = cheby2(order, rs, wn, btype='low', analog=0, output='zpk')\n    sos = zpk2sos(z, p, k)\n    if freq_passband:\n        return sosfilt(sos, data), wp * nyquist\n    return sosfilt(sos, data)", "response": "This method is used to filter data by passing data only below a certain frequency."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _distributeCells(numCellsPop):\n    ''' distribute cells across compute nodes using round-robin'''\n    from .. import sim\n        \n    hostCells = {}\n    for i in range(sim.nhosts):\n        hostCells[i] = []\n        \n    for i in range(numCellsPop):\n        hostCells[sim.nextHost].append(i)\n        \n        sim.nextHost+=1\n        if sim.nextHost>=sim.nhosts:\n            sim.nextHost=0\n    \n    if sim.cfg.verbose: \n        print((\"Distributed population of %i cells on %s hosts: %s, next: %s\"%(numCellsPop,sim.nhosts,hostCells,sim.nextHost)))\n    return hostCells", "response": "distribute cells across compute nodes using round - robin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreplacing reconstructed axon with a stub", "response": "def fix_axon_peri(hobj):\n    \"\"\"Replace reconstructed axon with a stub\n    :param hobj: hoc object\n    \"\"\"\n    for i,sec in enumerate(hobj.axon):\n        hobj.axon[i] = None\n\n    for i,sec in enumerate(hobj.all):\n        if 'axon' in sec.name():\n            hobj.all[i] = None\n\n    hobj.all = [sec for sec in hobj.all if sec is not None]\n\n    hobj.axon = None\n\n    #h.execute('create axon[2]', hobj)\n    hobj.axon = [h.Section(name='axon[0]'), h.Section(name='axon[1]')]\n    hobj.axonal = []\n\n    for sec in hobj.axon:\n        sec.L = 30\n        sec.diam = 1\n        hobj.axonal.append(sec)\n        hobj.all.append(sec)  # need to remove this comment\n\n    hobj.axon[0].connect(hobj.soma[0], 0.5, 0)\n    hobj.axon[1].connect(hobj.axon[0], 1, 0)\n\n    h.define_shape()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fix_axon_peri_v2(hobj):\n    for i,sec in enumerate(hobj.axon):\n        if i < 2:\n            sec.L = 30\n            sec.diam = 1\n        else:\n            sec.L = 1e-6\n            sec.diam = 1\n\n    h.define_shape()", "response": "Replace reconstructed axon with a stub\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_sec_nseg(secs, dL):\n\n    for secName in secs:\n        secs[secName]['geom']['nseg'] = 1 + 2 * int(secs[secName]['geom']['L'] / (2*dL))", "response": "Set nseg of sections based on dL param"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef createConns(self):\n        # SONATA method - works but same results as NeuroMLlite\n        '''\n        from sonata.io import File, Edge\n        data = File(data_files=[self.subs('$NETWORK_DIR/excvirt_cortex_edges.h5')],\n                data_type_files=[self.subs('$NETWORK_DIR/excvirt_cortex_edge_types.csv')])\n        '''\n\n        # NeuroMLlite Method\n        self.edges_info = {}\n        self.conn_info = {}\n\n        synMechSubs = {'level_of_detail': 'mod', \n                        'erev': 'e'}\n        \n        if 'edges' in self.network_config['networks']:\n            for e in self.network_config['networks']['edges']:\n                edges_file = self.subs(e['edges_file'])\n                edge_types_file = self.subs(e['edge_types_file'])\n\n                print(\"\\nLoading edges from %s and %s\"%(edges_file,edge_types_file))\n\n                h5file=tables.open_file(edges_file,mode='r')\n\n                print(\"Opened HDF5 file: %s\"%(h5file.filename))\n                self.parse_group(h5file.root.edges)\n                h5file.close()\n                self.edges_info[self.current_edge] = load_csv_props(edge_types_file)\n                self.current_edge = None\n\n        for conn in self.conn_info:\n            \n            pre_node = self.conn_info[conn]['pre_node']\n            post_node = self.conn_info[conn]['post_node']\n            \n            print('   Adding projection %s: %s -> %s '%(conn, pre_node, post_node))\n\n            # add all synMechs in this projection to netParams.synMechParams\n            for type in self.edges_info[conn]:\n                syn_label = self.edges_info[conn][type]['dynamics_params'].split('.')[0]\n                if syn_label not in sim.net.params.synMechParams:\n                    dynamics_params_file = self.subs(self.network_config['components']['synaptic_models_dir']) +'/'+self.edges_info[conn][type]['dynamics_params']        \n                    syn_dyn_params = load_json(dynamics_params_file)\n                    synMechParams = dict(syn_dyn_params)\n                    for k in synMechParams:  # replace keys\n                        if k in synMechSubs:\n                            synMechParams[synMechSubs[k]] = synMechParams.pop(k) \n                    synMechParams['mod'] = self.edges_info[conn][type]['model_template']\n                    sim.net.params.synMechParams[syn_label] = synMechParams\n                    print('   Added synMech %s '%(syn_label))\n\n            # add individual connections in this projection\n            for i in range(len(self.conn_info[conn]['pre_id'])):\n                pre_id = self.conn_info[conn]['pre_id'][i]\n                post_id = self.conn_info[conn]['post_id'][i]\n                pre_gid = self.cell_info[pre_node]['gid_from_id'][pre_id] \n                post_gid = self.cell_info[post_node]['gid_from_id'][post_id]\n\n\n                if post_gid in sim.net.gid2lid:\n\n                    type = self.conn_info[conn]['edge_type_id'][i]\n\n                    print('   Conn: type %s pop %s (id %s) -> pop %s (id %s) MAPPED TO: cell gid %s -> cell gid %s'%(type,pre_node,pre_id,post_node,post_id, pre_gid,post_gid))\n                    #print(self.edges_info[conn][type])\n                    \n                    connParams = {}\n                    postCell = sim.net.cells[sim.net.gid2lid[post_gid]]\n\n                    # preGid\n                    connParams['preGid'] = pre_gid\n\n                    # synMech\n                    connParams['synMech'] = self.edges_info[conn][type]['dynamics_params'].split('.')[0]                \n                    \n                    # weight\n                    sign = syn_dyn_params['sign'] if 'sign' in syn_dyn_params else 1\n                    try:\n                        weight = self.conn_info[conn]['syn_weight'][i] \n                    except:\n                        weight = self.edges_info[conn][type]['syn_weight'] if 'syn_weight' in self.edges_info[conn][type] else 1.0\n                    connParams['weight'] = sign*weight\n                    \n                    # delay\n                    connParams['delay'] = self.edges_info[conn][type]['delay'] if 'delay' in self.edges_info[conn][type] else 0\n                    \n                    # sec \n                    sec_id = self.conn_info[conn]['sec_id'][i] \n                    connParams['sec'] = postCell.secLists['SONATA_sec_id'][sec_id]\n\n                    # loc\n                    connParams['loc'] = self.conn_info[conn]['sec_x'][i] \n\n                    # add connection\n                    postCell.addConn(connParams)", "response": "Create connection tables and edges info."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport a single cell from a HOC template or python file into a framework format", "response": "def importCell (fileName, cellName, cellArgs = None, cellInstance = False):\n    h.initnrn()\n    varList = mechVarList()  # list of properties for all density mechanisms and point processes\n    origGlob = getGlobals(list(varList['mechs'].keys())+list(varList['pointps'].keys()))\n    origGlob['v_init'] = -65  # add by hand since won't be set unless load h.load_file('stdrun')\n\n    if cellArgs is None: cellArgs = [] # Define as empty list if not otherwise defined\n\n    ''' Import cell from HOC template or python file into framework format (dict of sections, with geom, topol, mechs, syns)'''\n    if fileName.endswith('.hoc') or fileName.endswith('.tem'):\n        h.load_file(fileName)\n        if not cellInstance:\n            if isinstance(cellArgs, dict):\n                cell = getattr(h, cellName)(**cellArgs)  # create cell using template, passing dict with args\n            else:\n                cell = getattr(h, cellName)(*cellArgs) # create cell using template, passing list with args\n        else:\n            try:\n                cell = getattr(h, cellName)\n            except:\n                cell = None\n    elif fileName.endswith('.py'):\n        filePath,fileNameOnly = os.path.split(fileName)  # split path from filename\n        if filePath not in sys.path:  # add to path if not there (need to import module)\n            sys.path.insert(0, filePath)\n            removeFilePath = True\n        else:\n            removeFilePath = False\n        moduleName = fileNameOnly.split('.py')[0]  # remove .py to obtain module name\n        tempModule = importlib.import_module(moduleName)\n        modulePointer = tempModule\n        if isinstance(cellArgs, dict):\n            cell = getattr(modulePointer, cellName)(**cellArgs) # create cell using template, passing dict with args\n        else:\n            cell = getattr(modulePointer, cellName)(*cellArgs)  # create cell using template, passing list with args\n        if removeFilePath: sys.path.remove(filePath)\n    else:\n        print(\"File name should be either .hoc or .py file\")\n        return\n\n    secDic, secListDic, synMechs, globs = getCellParams(cell, varList, origGlob)\n    \n    if fileName.endswith('.py'):\n        _delete_module(moduleName)\n        _delete_module('tempModule')\n        del modulePointer\n    elif fileName.endswith('.hoc'):\n        for sec in h.allsec():\n            try:\n                if h.cas()!=sec: sec.push()\n                h.delete_section()\n                h.pop_section()\n            except:\n                pass\n    h.initnrn()\n\n    setGlobals(origGlob)  # restore original globals\n\n    return secDic, secListDic, synMechs, globs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports cells from a. hoc or. py file into a dict of sections topol mechs and syns", "response": "def importCellsFromNet (netParams, fileName, labelList, condsList, cellNamesList, importSynMechs):\n    h.initnrn()\n    \n    ''' Import cell from HOC template or python file into framework format (dict of sections, with geom, topol, mechs, syns)'''\n    if fileName.endswith('.hoc') or fileName.endswith('.tem'):\n        print('Importing from .hoc network not yet supported')\n        return\n        # h.load_file(fileName)\n        # for cellName in cellNames:\n        #     cell = getattr(h, cellName) # create cell using template, passing dict with args\n        #     secDic, secListDic, synMechs = getCellParams(cell)\n\n    elif fileName.endswith('.py'):\n        origDir = os.getcwd()\n        filePath,fileNameOnly = os.path.split(fileName)  # split path from filename\n        if filePath not in sys.path:  # add to path if not there (need to import module)\n            sys.path.insert(0, filePath)\n            removeFilePath = True\n        else:\n            removeFilePath = False\n        moduleName = fileNameOnly.split('.py')[0]  # remove .py to obtain module name\n        os.chdir(filePath)\n        print('\\nRunning network in %s to import cells into NetPyNE ...\\n'%(fileName))\n        from neuron import load_mechanisms\n        load_mechanisms(filePath)\n        tempModule = importlib.import_module(moduleName)\n        modulePointer = tempModule\n        if removeFilePath: sys.path.remove(filePath)\n    else:\n        print(\"File name should be either .hoc or .py file\")\n        return\n\n    for label, conds, cellName in zip(labelList, condsList, cellNamesList):\n        print('\\nImporting %s from %s ...'%(cellName, fileName))\n        exec('cell = tempModule' + '.' + cellName)\n        #cell = getattr(modulePointer, cellName) # get cell object\n        varList = mechVarList()\n        origGlob = getGlobals(list(varList['mechs'].keys())+list(varList['pointps'].keys()))\n        secs, secLists, synMechs = getCellParams(cell, varList, origGlob)\n        cellRule = {'conds': conds, 'secs': secs, 'secLists': secLists}\n        netParams.addCellParams(label, cellRule)\n        if importSynMechs:\n            for synMech in synMechs: netParams.addSynMechParams(synMech.pop('label'), synMech)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getCSD (lfps,sampr,minf=0.05,maxf=300,norm=True,vaknin=False,spacing=1.0):\n  datband = getbandpass(lfps,sampr,minf,maxf)\n  if datband.shape[0] > datband.shape[1]: # take CSD along smaller dimension\n    ax = 1\n  else:\n    ax = 0\n  # can change default to run Vaknin on bandpass filtered LFPs before calculating CSD, that\n  # way would have same number of channels in CSD and LFP (but not critical, and would take more RAM);\n  if vaknin: datband = Vaknin(datband)\n  if norm: removemean(datband,ax=ax)\n  # NB: when drawing CSD make sure that negative values (depolarizing intracellular current) drawn in red,\n  # and positive values (hyperpolarizing intracellular current) drawn in blue\n  CSD = -numpy.diff(datband,n=2,axis=ax) / spacing**2 # now each column (or row) is an electrode -- CSD along electrodes\n  return CSD", "response": "calculate current source density of a set of local field potentials"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an exponentially decaying synapse", "response": "def createSynapses(self):\n        \"\"\"Add an exponentially decaying synapse \"\"\"\n        synsoma = h.ExpSyn(self.soma(0.5))\n        synsoma.tau = 2\n        synsoma.e = 0\n        syndend = h.ExpSyn(self.dend(0.5))\n        syndend.tau = 2\n        syndend.e = 0\n        self.synlist.append(synsoma) # synlist is defined in Cell\n        self.synlist.append(syndend)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates netcon to record spikes", "response": "def createNetcon(self, thresh=10):\n        \"\"\" created netcon to record spikes \"\"\"\n        nc = h.NetCon(self.soma(0.5)._ref_v, None, sec = self.soma)\n        nc.threshold = thresh\n        return nc"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the sections of the cell.", "response": "def createSections(self):\n        \"\"\"Create the sections of the cell.\"\"\"\n        self.soma = h.Section(name='soma', cell=self)\n        self.dend = h.Section(name='dend', cell=self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef defineGeometry(self):\n        self.soma.L = 18.8\n        self.soma.diam = 18.8\n        self.soma.Ra = 123.0\n\n        self.dend.L = 200.0\n        self.dend.diam = 1.0\n        self.dend.Ra = 100.0", "response": "Set the 3D geometry of the cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef defineBiophysics(self):\n        # Insert active Hodgkin-Huxley current in the soma\n        self.soma.insert('hh')\n        self.soma.gnabar_hh = 0.12  # Sodium conductance in S/cm2\n        self.soma.gkbar_hh = 0.036  # Potassium conductance in S/cm2\n        self.soma.gl_hh = 0.003    # Leak conductance in S/cm2\n        self.soma.el_hh = -70       # Reversal potential in mV\n        \n        self.dend.insert('pas')\n        self.dend.g_pas = 0.001  # Passive conductance in S/cm2\n        self.dend.e_pas = -65    # Leak reversal potential mV\n        self.dend.nseg = 1000", "response": "Assign the membrane properties across the cell."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mat2dict(obj): \n    '''\n    A recursive function which constructs from matobjects nested dictionaries\n    Enforce lists for conns, synMechs and stims even if 1 element (matlab converts to dict otherwise)\n    '''\n    import scipy.io as spio\n    import numpy as np\n\n    if isinstance(obj, dict):\n        out = {}\n        for key in obj:\n            if isinstance(obj[key], spio.matlab.mio5_params.mat_struct):\n                if key in ['conns', 'stims', 'synMechs']:\n                    out[key] = [_mat2dict(obj[key])]  # convert to 1-element list\n                else:\n                    out[key] = _mat2dict(obj[key])\n            elif isinstance(obj[key], np.ndarray):\n                out[key] = _mat2dict(obj[key])\n            else:\n                out[key] = obj[key]\n\n    elif isinstance(obj, spio.matlab.mio5_params.mat_struct):\n        out = {}\n        for key in obj._fieldnames:\n            val = obj.__dict__[key]\n            if isinstance(val, spio.matlab.mio5_params.mat_struct):\n                if key in ['conns', 'stims', 'synMechs']:\n                    out[key] = [_mat2dict(val)]  # convert to 1-element list\n                else:\n                    out[key] = _mat2dict(val)\n            elif isinstance(val, np.ndarray):\n                out[key] = _mat2dict(val)\n            else:\n                out[key] = val\n\n    elif isinstance(obj, np.ndarray):\n        out = []\n        for item in obj:\n            if isinstance(item, spio.matlab.mio5_params.mat_struct) or isinstance(item, np.ndarray):\n                out.append(_mat2dict(item))\n            else:\n                out.append(item)\n\n    else:\n        out = obj\n\n    return out", "response": "A recursive function which constructs from matobjects nested dictionaries\n    Enforce lists for conns synMechs and stims even if 1 element"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload an NCBI SWC file and instantiate inside cell.", "response": "def load(filename, fileformat=None, cell=None, use_axon=True, xshift=0, yshift=0, zshift=0):\n    \"\"\"\n    Load an SWC from filename and instantiate inside cell. Code kindly provided\n    by @ramcdougal.\n\n    Args:\n        filename = .swc file containing morphology\n        cell = Cell() object. (Default: None, creates new object)\n        filename = the filename of the SWC file\n        use_axon = include the axon? Default: True (yes)\n        xshift, yshift, zshift = use to position the cell\n\n    Returns:\n        Cell() object with populated soma, axon, dend, & apic fields\n\n    Minimal example:\n        # pull the morphology for the demo from NeuroMorpho.Org\n        from PyNeuronToolbox import neuromorphoorg\n        with open('c91662.swc', 'w') as f:\n            f.write(neuromorphoorg.morphology('c91662'))\n        cell = load_swc(filename)\n\n    \"\"\"\n\n    if cell is None:\n        cell = Cell(name=string.join(filename.split('.')[:-1]))\n\n    if fileformat is None:\n        fileformat = filename.split('.')[-1]\n\n    name_form = {1: 'soma[%d]', 2: 'axon[%d]', 3: 'dend[%d]', 4: 'apic[%d]'}\n\n    # load the data. Use Import3d_SWC_read for swc, Import3d_Neurolucida3 for\n    # Neurolucida V3, Import3d_MorphML for MorphML (level 1 of NeuroML), or\n    # Import3d_Eutectic_read for Eutectic.\n    if fileformat == 'swc':\n        morph = h.Import3d_SWC_read()\n    elif fileformat == 'asc':\n        morph = h.Import3d_Neurolucida3()\n    else:\n        raise Exception('file format `%s` not recognized'%(fileformat))\n    morph.input(filename)\n\n    # easiest to instantiate by passing the loaded morphology to the Import3d_GUI\n    # tool; with a second argument of 0, it won't display the GUI, but it will allow\n    # use of the GUI's features\n    i3d = h.Import3d_GUI(morph, 0)\n\n    # get a list of the swc section objects\n    swc_secs = i3d.swc.sections\n    swc_secs = [swc_secs.object(i) for i in range(int(swc_secs.count()))]\n\n    # initialize the lists of sections\n    sec_list = {1: cell.soma, 2: cell.axon, 3: cell.dend, 4: cell.apic}\n\n    # name and create the sections\n    real_secs = {}\n    for swc_sec in swc_secs:\n        cell_part = int(swc_sec.type)\n\n        # skip everything else if it's an axon and we're not supposed to\n        # use it... or if is_subsidiary\n        if (not(use_axon) and cell_part == 2) or swc_sec.is_subsidiary:\n            continue\n        \n        # figure out the name of the new section\n        if cell_part not in name_form:\n            raise Exception('unsupported point type')\n        name = name_form[cell_part] % len(sec_list[cell_part])\n\n        # create the section\n        sec = h.Section(name=name)\n        \n        # connect to parent, if any\n        if swc_sec.parentsec is not None:\n            sec.connect(real_secs[swc_sec.parentsec.hname()](swc_sec.parentx))\n\n        # define shape\n        if swc_sec.first == 1:\n            h.pt3dstyle(1, swc_sec.raw.getval(0, 0), swc_sec.raw.getval(1, 0),\n                        swc_sec.raw.getval(2, 0), sec=sec)\n\n        j = swc_sec.first\n        xx, yy, zz = [swc_sec.raw.getrow(i).c(j) for i in range(3)]\n        dd = swc_sec.d.c(j)\n        if swc_sec.iscontour_:\n            # never happens in SWC files, but can happen in other formats supported\n            # by NEURON's Import3D GUI\n            raise Exception('Unsupported section style: contour')\n\n        if dd.size() == 1:\n            # single point soma; treat as sphere\n            x, y, z, d = [dim.x[0] for dim in [xx, yy, zz, dd]]\n            for xprime in [x - d / 2., x, x + d / 2.]:\n                h.pt3dadd(xprime + xshift, y + yshift, z + zshift, d, sec=sec)\n        else:\n            for x, y, z, d in zip(xx, yy, zz, dd):\n                h.pt3dadd(x + xshift, y + yshift, z + zshift, d, sec=sec)\n\n        # store the section in the appropriate list in the cell and lookup table               \n        sec_list[cell_part].append(sec)    \n        real_secs[swc_sec.hname()] = sec\n\n    cell.all = cell.soma + cell.apic + cell.dend + cell.axon\n    return cell"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a sequence of cartesian coordinates into a sequence of line segments defined by spherical coordinates.", "response": "def sequential_spherical(xyz):\n    \"\"\"\n    Converts sequence of cartesian coordinates into a sequence of\n    line segments defined by spherical coordinates.\n    \n    Args:\n        xyz = 2d numpy array, each row specifies a point in\n              cartesian coordinates (x,y,z) tracing out a\n              path in 3D space.\n    \n    Returns:\n        r = lengths of each line segment (1D array)\n        theta = angles of line segments in XY plane (1D array)\n        phi = angles of line segments down from Z axis (1D array)\n    \"\"\"\n    d_xyz = np.diff(xyz,axis=0)\n    \n    r = np.linalg.norm(d_xyz,axis=1)\n    theta = np.arctan2(d_xyz[:,1], d_xyz[:,0])\n    hyp = d_xyz[:,0]**2 + d_xyz[:,1]**2\n    phi = np.arctan2(np.sqrt(hyp), d_xyz[:,2])\n    \n    return (r,theta,phi)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef spherical_to_cartesian(r,theta,phi):\n    x = r * np.sin(phi) * np.cos(theta)\n    y = r * np.sin(phi) * np.sin(theta)\n    z = r * np.cos(phi)\n    return (x,y,z)", "response": "Simple conversion of spherical to cartesian coordinates\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_coord(targ_length,xyz,rcum,theta,phi):\n    #   [1] Find spherical coordinates for the line segment containing\n    #           the endpoint.\n    #   [2] Find endpoint in spherical coords and convert to cartesian\n    i = np.nonzero(rcum <= targ_length)[0][-1]\n    if i == len(theta):\n        return xyz[-1,:]\n    else:\n        r_lcl = targ_length-rcum[i] # remaining length along line segment\n        (dx,dy,dz) = spherical_to_cartesian(r_lcl,theta[i],phi[i])\n        return xyz[i,:] + [dx,dy,dz]", "response": "Find the end of a segment path along a line segment and return the coordinates of the end of the segment path along the line segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interpolate_jagged(xyz,nseg):\n    \n    # Spherical coordinates specifying the angles of all line\n    # segments that make up the section path\n    (r,theta,phi) = sequential_spherical(xyz)\n    \n    # cumulative length of section path at each coordinate\n    rcum = np.append(0,np.cumsum(r))\n\n    # breakpoints for segment paths along section path\n    breakpoints = np.linspace(0,rcum[-1],nseg+1)\n    np.delete(breakpoints,0)\n    \n    # Find segment paths\n    seg_paths = []\n    for a in range(nseg):\n        path = []\n        \n        # find (x,y,z) starting coordinate of path\n        if a == 0:\n            start_coord = xyz[0,:]\n        else:\n            start_coord = end_coord # start at end of last path\n        path.append(start_coord)\n\n        # find all coordinates between the start and end points\n        start_length = breakpoints[a]\n        end_length = breakpoints[a+1]\n        mid_boolean = (rcum > start_length) & (rcum < end_length)\n        mid_indices = np.nonzero(mid_boolean)[0]\n        for mi in mid_indices:\n            path.append(xyz[mi,:])\n\n        # find (x,y,z) ending coordinate of path\n        end_coord = find_coord(end_length,xyz,rcum,theta,phi)\n        path.append(end_coord)\n\n        # Append path to list of segment paths\n        seg_paths.append(np.array(path))\n    \n    # Return all segment paths\n    return seg_paths", "response": "Interpolates along a jagged path in 3D\nCTYPE"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots a 3D shapeplot of the neuron with the given color limits and color limits.", "response": "def shapeplot(h,ax,sections=None,order='pre',cvals=None,\\\n              clim=None,cmap=cm.YlOrBr_r, legend=True,  **kwargs):  # meanLineWidth=1.0, maxLineWidth=10.0,\n    \"\"\"\n    Plots a 3D shapeplot\n\n    Args:\n        h = hocObject to interface with neuron\n        ax = matplotlib axis for plotting\n        sections = list of h.Section() objects to be plotted\n        order = { None= use h.allsec() to get sections\n                  'pre'= pre-order traversal of morphology }\n        cvals = list/array with values mapped to color by cmap; useful\n                for displaying voltage, calcium or some other state\n                variable across the shapeplot.\n        **kwargs passes on to matplotlib (e.g. color='r' for red lines)\n\n    Returns:\n        lines = list of line objects making up shapeplot\n    \"\"\"\n    \n    # Default is to plot all sections. \n    if sections is None:\n        if order == 'pre':\n            sections = allsec_preorder(h) # Get sections in \"pre-order\"\n        else:\n            sections = list(h.allsec())\n    \n    # Determine color limits\n    if cvals is not None and clim is None: \n        clim = [np.nanmin(cvals), np.nanmax(cvals)]\n\n    # Plot each segement as a line\n    lines = []\n    i = 0\n\n    allDiams = []\n    for sec in sections:\n        allDiams.append(get_section_diams(h,sec))\n    #maxDiams = max([max(d) for d in allDiams])\n    #meanDiams = np.mean([np.mean(d) for d in allDiams])\n\n    for isec,sec in enumerate(sections):\n        xyz = get_section_path(h,sec)\n        seg_paths = interpolate_jagged(xyz,sec.nseg)\n        diams = allDiams[isec]  # represent diams as linewidths\n        linewidths = diams # linewidth is in points so can use actual diams to plot\n        # linewidths = [min(d/meanDiams*meanLineWidth, maxLineWidth) for d in diams]  # use if want to scale size \n\n        for (j,path) in enumerate(seg_paths):\n            line, = plt.plot(path[:,0], path[:,1], path[:,2], '-k', **kwargs)\n            try:\n                line.set_linewidth(linewidths[j])\n            except:\n                pass\n            if cvals is not None:\n                if isinstance(cvals[i], numbers.Number):\n                    # map number to colormap\n                    try:\n                        col = cmap(int((cvals[i]-clim[0])*255/(clim[1]-clim[0])))\n                    except:\n                        col = cmap(0)\n                else:\n                    # use input directly. E.g. if user specified color with a string.\n                    col = cvals[i]\n                line.set_color(col)\n            lines.append(line)\n            i += 1\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn animate function which updates color of shapeplot", "response": "def shapeplot_animate(v,lines,nframes=None,tscale='linear',\\\n                      clim=[-80,50],cmap=cm.YlOrBr_r):\n    \"\"\" Returns animate function which updates color of shapeplot \"\"\"\n    if nframes is None:\n        nframes = v.shape[0]\n    if tscale == 'linear':\n        def animate(i):\n            i_t = int((i/nframes)*v.shape[0])\n            for i_seg in range(v.shape[1]):\n                lines[i_seg].set_color(cmap(int((v[i_t,i_seg]-clim[0])*255/(clim[1]-clim[0]))))\n            return []\n    elif tscale == 'log':\n        def animate(i):\n            i_t = int(np.round((v.shape[0] ** (1.0/(nframes-1))) ** i - 1))\n            for i_seg in range(v.shape[1]):\n                lines[i_seg].set_color(cmap(int((v[i_t,i_seg]-clim[0])*255/(clim[1]-clim[0]))))\n            return []\n    else:\n        raise ValueError(\"Unrecognized option '%s' for tscale\" % tscale)\n\n    return animate"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mark_locations(h,section,locs,markspec='or',**kwargs):\n\n    # get list of cartesian coordinates specifying section path\n    xyz = get_section_path(h,section)\n    (r,theta,phi) = sequential_spherical(xyz)\n    rcum = np.append(0,np.cumsum(r))\n\n    # convert locs into lengths from the beginning of the path\n    if type(locs) is float or type(locs) is np.float64:\n        locs = np.array([locs])\n    if type(locs) is list:\n        locs = np.array(locs)\n    lengths = locs*rcum[-1]\n\n    # find cartesian coordinates for markers\n    xyz_marks = []\n    for targ_length in lengths:\n        xyz_marks.append(find_coord(targ_length,xyz,rcum,theta,phi))\n    xyz_marks = np.array(xyz_marks)\n\n    # plot markers\n    line, = plt.plot(xyz_marks[:,0], xyz_marks[:,1], \\\n                     xyz_marks[:,2], markspec, **kwargs)\n    return line", "response": "Mark locations on a section."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef root_sections(h):\n    roots = []\n    for section in h.allsec():\n        sref = h.SectionRef(sec=section)\n        # has_parent returns a float... cast to bool\n        if sref.has_parent() < 0.9:\n            roots.append(section)\n    return roots", "response": "Returns a list of all sections that have no parent."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef leaf_sections(h):\n    leaves = []\n    for section in h.allsec():\n        sref = h.SectionRef(sec=section)\n        # nchild returns a float... cast to bool\n        if sref.nchild() < 0.9:\n            leaves.append(section)\n    return leaves", "response": "Returns a list of all sections that have no children."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the index of all sections without a parent.", "response": "def root_indices(sec_list):\n    \"\"\"\n    Returns the index of all sections without a parent.\n    \"\"\"\n    roots = []\n    for i,section in enumerate(sec_list):\n        sref = h.SectionRef(sec=section)\n        # has_parent returns a float... cast to bool\n        if sref.has_parent() < 0.9:\n            roots.append(i)\n    return roots"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning all sections in order from the root neurons in the pre - order order.", "response": "def allsec_preorder(h):\n    \"\"\"\n    Alternative to using h.allsec(). This returns all sections in order from\n    the root. Traverses the topology each neuron in \"pre-order\"\n    \"\"\"\n    #Iterate over all sections, find roots\n    roots = root_sections(h)\n\n    # Build list of all sections\n    sec_list = []\n    for r in roots:\n        add_pre(h,sec_list,r)\n    return sec_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_pre(h,sec_list,section,order_list=None,branch_order=None):\n\n    sec_list.append(section)\n    sref = h.SectionRef(sec=section)\n\n    if branch_order is not None:\n        order_list.append(branch_order)\n        if len(sref.child) > 1:\n            branch_order += 1\n    \n    for next_node in sref.child:\n        add_pre(h,sec_list,next_node,order_list,branch_order)", "response": "A helper function that traverses a section and adds it in pre - order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the distance between two segments.", "response": "def dist_between(h,seg1,seg2):\n    \"\"\"\n    Calculates the distance between two segments. I stole this function from\n    a post by Michael Hines on the NEURON forum\n    (www.neuron.yale.edu/phpbb/viewtopic.php?f=2&t=2114)\n    \"\"\"\n    h.distance(0, seg1.x, sec=seg1.sec)\n    return h.distance(seg2.x, sec=seg2.sec)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of branch orders for each section in the archive", "response": "def all_branch_orders(h):\n    \"\"\"\n    Produces a list branch orders for each section (following pre-order tree\n    traversal)\n    \"\"\"\n    #Iterate over all sections, find roots\n    roots = []\n    for section in h.allsec():\n        sref = h.SectionRef(sec=section)\n        # has_parent returns a float... cast to bool\n        if sref.has_parent() < 0.9:\n            roots.append(section)\n\n    # Build list of all sections\n    order_list = []\n    for r in roots:\n        add_pre(h,[],r,order_list,0)\n    return order_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef branch_order(h,section, path=[]):\n    path.append(section)\n    sref = h.SectionRef(sec=section)\n    # has_parent returns a float... cast to bool\n    if sref.has_parent() < 0.9:\n        return 0 # section is a root\n    else:\n        nchild = len(list(h.SectionRef(sec=sref.parent).child))\n        if nchild <= 1.1:\n            return branch_order(h,sref.parent,path)\n        else:\n            return 1+branch_order(h,sref.parent,path)", "response": "Returns the branch order of a section"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction to instantiate Cell objects based on the characteristics of this population", "response": "def createCells(self):\n        '''Function to instantiate Cell objects based on the characteristics of this population'''\n        # add individual cells\n        if 'cellsList' in self.tags:\n            cells = self.createCellsList()\n\n        # create cells based on fixed number of cells\n        elif 'numCells' in self.tags:\n            cells = self.createCellsFixedNum()\n\n        # create cells based on density (optional ynorm-dep)\n        elif 'density' in self.tags:\n            cells = self.createCellsDensity()\n\n        # create cells based on density (optional ynorm-dep)\n        elif 'gridSpacing' in self.tags:\n            cells = self.createCellsGrid()\n\n        # not enough tags to create cells\n        else:\n            self.tags['numCells'] = 1\n            print('Warninig: number or density of cells not specified for population %s; defaulting to numCells = 1' % (self.tags['pop']))\n            cells = self.createCellsFixedNum()\n\n        return cells"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a population of cells based on fixed number of cells.", "response": "def createCellsFixedNum (self):\n        ''' Create population cells based on fixed number of cells'''\n        from .. import sim\n\n        cells = []\n        self.rand.Random123(self.tags['numCells'], sim.net.lastGid, sim.cfg.seeds['loc'])\n        self.rand.uniform(0, 1)\n        vec = h.Vector(self.tags['numCells']*3)\n        vec.setrand(self.rand)\n        randLocs = np.array(vec).reshape(self.tags['numCells'], 3)  # create random x,y,z locations\n\n        if sim.net.params.shape == 'cylinder':\n            # Use the x,z random vales \n            rho = randLocs[:,0] # use x rand value as the radius rho in the interval [0, 1)\n            phi = 2 * pi * randLocs[:,2] # use z rand value as the angle phi in the interval [0, 2*pi) \n            x = (1 + sqrt(rho) * cos(phi))/2.0\n            z = (1 + sqrt(rho) * sin(phi))/2.0\n            randLocs[:,0] = x\n            randLocs[:,2] = z\n    \n        elif sim.net.params.shape == 'ellipsoid':\n            # Use the x,y,z random vales \n            rho = np.power(randLocs[:,0], 1.0/3.0) # use x rand value as the radius rho in the interval [0, 1); cuberoot\n            phi = 2 * pi * randLocs[:,1] # use y rand value as the angle phi in the interval [0, 2*pi) \n            costheta = (2 * randLocs[:,2]) - 1 # use z rand value as cos(theta) in the interval [-1, 1); ensures uniform dist \n            theta = arccos(costheta)  # obtain theta from cos(theta)\n            x = (1 + rho * cos(phi) * sin(theta))/2.0\n            y = (1 + rho * sin(phi) * sin(theta))/2.0\n            z = (1 + rho * cos(theta))/2.0 \n            randLocs[:,0] = x\n            randLocs[:,1] = y\n            randLocs[:,2] = z\n        \n        for icoord, coord in enumerate(['x', 'y', 'z']):\n            if coord+'Range' in self.tags:  # if user provided absolute range, convert to normalized\n                self.tags[coord+'normRange'] = [float(point) / getattr(sim.net.params, 'size'+coord.upper()) for point in self.tags[coord+'Range']]\n            # constrain to range set by user\n            if coord+'normRange' in self.tags:  # if normalized range, rescale random locations\n                minv = self.tags[coord+'normRange'][0] \n                maxv = self.tags[coord+'normRange'][1] \n                randLocs[:,icoord] = randLocs[:,icoord] * (maxv-minv) + minv\n\n        for i in self._distributeCells(int(sim.net.params.scale * self.tags['numCells']))[sim.rank]:\n            gid = sim.net.lastGid+i\n            self.cellGids.append(gid)  # add gid list of cells belonging to this population - not needed?\n            cellTags = {k: v for (k, v) in self.tags.items() if k in sim.net.params.popTagsCopiedToCells}  # copy all pop tags to cell tags, except those that are pop-specific\n            cellTags['pop'] = self.tags['pop']\n            cellTags['xnorm'] = randLocs[i,0] # set x location (um)\n            cellTags['ynorm'] = randLocs[i,1] # set y location (um)\n            cellTags['znorm'] = randLocs[i,2] # set z location (um)\n            cellTags['x'] = sim.net.params.sizeX * randLocs[i,0] # set x location (um)\n            cellTags['y'] = sim.net.params.sizeY * randLocs[i,1] # set y location (um)\n            cellTags['z'] = sim.net.params.sizeZ * randLocs[i,2] # set z location (um)            \n            if 'spkTimes' in self.tags:  # if VecStim, copy spike times to params\n                if isinstance(self.tags['spkTimes'][0], list):\n                    try:\n                        cellTags['params']['spkTimes'] = self.tags['spkTimes'][i] # 2D list\n                    except:\n                        pass\n                else:\n                    cellTags['params']['spkTimes'] = self.tags['spkTimes'] # 1D list (same for all)\n            cells.append(self.cellModelClass(gid, cellTags)) # instantiate Cell object\n\n            if sim.cfg.verbose: print(('Cell %d/%d (gid=%d) of pop %s, on node %d, '%(i, sim.net.params.scale * self.tags['numCells']-1, gid, self.tags['pop'], sim.rank)))\n        sim.net.lastGid = sim.net.lastGid + self.tags['numCells'] \n        return cells"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef createCellsDensity (self):\n        ''' Create population cells based on density'''\n        from .. import sim\n\n        cells = []\n        shape = sim.net.params.shape\n        sizeX = sim.net.params.sizeX\n        sizeY = sim.net.params.sizeY\n        sizeZ = sim.net.params.sizeZ\n        \n        # calculate volume\n        if shape == 'cuboid':\n            volume = sizeY/1e3 * sizeX/1e3 * sizeZ/1e3  \n        elif shape == 'cylinder':\n            volume = sizeY/1e3 * sizeX/1e3/2 * sizeZ/1e3/2 * pi\n        elif shape == 'ellipsoid':\n            volume = sizeY/1e3/2.0 * sizeX/1e3/2.0 * sizeZ/1e3/2.0 * pi * 4.0 / 3.0\n\n        for coord in ['x', 'y', 'z']:\n            if coord+'Range' in self.tags:  # if user provided absolute range, convert to normalized\n                self.tags[coord+'normRange'] = [point / getattr(sim.net.params, 'size'+coord.upper()) for point in self.tags[coord+'Range']]\n            if coord+'normRange' in self.tags:  # if normalized range, rescale volume\n                minv = self.tags[coord+'normRange'][0] \n                maxv = self.tags[coord+'normRange'][1] \n                volume = volume * (maxv-minv)\n\n        funcLocs = None  # start with no locations as a function of density function\n        if isinstance(self.tags['density'], basestring): # check if density is given as a function \n            if shape == 'cuboid':  # only available for cuboids\n                strFunc = self.tags['density']  # string containing function\n                strVars = [var for var in ['xnorm', 'ynorm', 'znorm'] if var in strFunc]  # get list of variables used \n                if not len(strVars) == 1:\n                    print('Error: density function (%s) for population %s does not include \"xnorm\", \"ynorm\" or \"znorm\"'%(strFunc,self.tags['pop']))\n                    return\n                coordFunc = strVars[0] \n                lambdaStr = 'lambda ' + coordFunc +': ' + strFunc # convert to lambda function \n                densityFunc = eval(lambdaStr)\n                minRange = self.tags[coordFunc+'Range'][0]\n                maxRange = self.tags[coordFunc+'Range'][1]\n\n                interval = 0.001  # interval of location values to evaluate func in order to find the max cell density\n                maxDensity = max(list(map(densityFunc, (np.arange(minRange, maxRange, interval)))))  # max cell density \n                maxCells = volume * maxDensity  # max number of cells based on max value of density func \n                \n                self.rand.Random123(int(maxDensity), sim.net.lastGid, sim.cfg.seeds['loc'])\n                locsAll = minRange + ((maxRange-minRange)) * np.array([self.rand.uniform(0, 1) for i in range(int(maxCells))])  # random location values \n                locsProb = np.array(list(map(densityFunc, locsAll))) / maxDensity  # calculate normalized density for each location value (used to prune)\n                allrands = np.array([self.rand.uniform(0, 1) for i in range(len(locsProb))])  # create an array of random numbers for checking each location pos \n                \n                makethiscell = locsProb>allrands  # perform test to see whether or not this cell should be included (pruning based on density func)\n                funcLocs = [locsAll[i] for i in range(len(locsAll)) if i in np.array(makethiscell.nonzero()[0],dtype='int')] # keep only subset of yfuncLocs based on density func\n                self.tags['numCells'] = len(funcLocs)  # final number of cells after pruning of location values based on density func\n                if sim.cfg.verbose: print('Volume=%.2f, maxDensity=%.2f, maxCells=%.0f, numCells=%.0f'%(volume, maxDensity, maxCells, self.tags['numCells']))\n            else:\n                print('Error: Density functions are only implemented for cuboid shaped networks')\n                exit(0)\n        else:  # NO ynorm-dep\n            self.tags['numCells'] = int(self.tags['density'] * volume)  # = density (cells/mm^3) * volume (mm^3)\n\n        # calculate locations of cells \n        self.rand.Random123(self.tags['numCells'], sim.net.lastGid, sim.cfg.seeds['loc'])\n        self.rand.uniform(0, 1)\n        vec = h.Vector(self.tags['numCells']*3)\n        vec.setrand(self.rand)\n        randLocs = np.array(vec).reshape(self.tags['numCells'], 3)  # create random x,y,z locations\n\n        if sim.net.params.shape == 'cylinder':\n            # Use the x,z random vales \n            rho = randLocs[:,0] # use x rand value as the radius rho in the interval [0, 1)\n            phi = 2 * pi * randLocs[:,2] # use z rand value as the angle phi in the interval [0, 2*pi) \n            x = (1 + sqrt(rho) * cos(phi))/2.0\n            z = (1 + sqrt(rho) * sin(phi))/2.0\n            randLocs[:,0] = x\n            randLocs[:,2] = z\n    \n        elif sim.net.params.shape == 'ellipsoid':\n            # Use the x,y,z random vales \n            rho = np.power(randLocs[:,0], 1.0/3.0) # use x rand value as the radius rho in the interval [0, 1); cuberoot\n            phi = 2 * pi * randLocs[:,1] # use y rand value as the angle phi in the interval [0, 2*pi) \n            costheta = (2 * randLocs[:,2]) - 1 # use z rand value as cos(theta) in the interval [-1, 1); ensures uniform dist \n            theta = arccos(costheta)  # obtain theta from cos(theta)\n            x = (1 + rho * cos(phi) * sin(theta))/2.0\n            y = (1 + rho * sin(phi) * sin(theta))/2.0\n            z = (1 + rho * cos(theta))/2.0 \n            randLocs[:,0] = x\n            randLocs[:,1] = y\n            randLocs[:,2] = z\n\n        for icoord, coord in enumerate(['x', 'y', 'z']):\n            if coord+'normRange' in self.tags:  # if normalized range, rescale random locations\n                minv = self.tags[coord+'normRange'][0] \n                maxv = self.tags[coord+'normRange'][1] \n                randLocs[:,icoord] = randLocs[:,icoord] * (maxv-minv) + minv\n            if funcLocs and coordFunc == coord+'norm':  # if locations for this coordinate calculated using density function\n                randLocs[:,icoord] = funcLocs\n\n        if sim.cfg.verbose and not funcLocs: print('Volume=%.4f, density=%.2f, numCells=%.0f'%(volume, self.tags['density'], self.tags['numCells']))\n\n        for i in self._distributeCells(self.tags['numCells'])[sim.rank]:\n            gid = sim.net.lastGid+i\n            self.cellGids.append(gid)  # add gid list of cells belonging to this population - not needed?\n            cellTags = {k: v for (k, v) in self.tags.items() if k in sim.net.params.popTagsCopiedToCells}  # copy all pop tags to cell tags, except those that are pop-specific\n            cellTags['pop'] = self.tags['pop']\n            cellTags['xnorm'] = randLocs[i,0]  # calculate x location (um)\n            cellTags['ynorm'] = randLocs[i,1]  # calculate y location (um)\n            cellTags['znorm'] = randLocs[i,2]  # calculate z location (um)\n            cellTags['x'] = sizeX * randLocs[i,0]  # calculate x location (um)\n            cellTags['y'] = sizeY * randLocs[i,1]  # calculate y location (um)\n            cellTags['z'] = sizeZ * randLocs[i,2]  # calculate z location (um)\n            cells.append(self.cellModelClass(gid, cellTags)) # instantiate Cell object\n            if sim.cfg.verbose: \n                print(('Cell %d/%d (gid=%d) of pop %s, pos=(%2.f, %2.f, %2.f), on node %d, '%(i, self.tags['numCells']-1, gid, self.tags['pop'],cellTags['x'], cellTags['y'], cellTags['z'], sim.rank)))\n        sim.net.lastGid = sim.net.lastGid + self.tags['numCells'] \n        return cells", "response": "Create cells based on density function"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the cells list based on list of individual cells", "response": "def createCellsList (self):\n        ''' Create population cells based on list of individual cells'''\n        from .. import sim\n        \n        cells = []\n        self.tags['numCells'] = len(self.tags['cellsList'])\n        for i in self._distributeCells(len(self.tags['cellsList']))[sim.rank]:\n            #if 'cellModel' in self.tags['cellsList'][i]:\n            #    self.cellModelClass = getattr(f, self.tags['cellsList'][i]['cellModel'])  # select cell class to instantiate cells based on the cellModel tags\n            gid = sim.net.lastGid+i\n            self.cellGids.append(gid)  # add gid list of cells belonging to this population - not needed?\n            cellTags = {k: v for (k, v) in self.tags.items() if k in sim.net.params.popTagsCopiedToCells}  # copy all pop tags to cell tags, except those that are pop-specific\n            cellTags['pop'] = self.tags['pop']\n            cellTags.update(self.tags['cellsList'][i])  # add tags specific to this cells\n            for coord in ['x','y','z']:\n                if coord in cellTags:  # if absolute coord exists\n                    cellTags[coord+'norm'] = cellTags[coord]/getattr(sim.net.params, 'size'+coord.upper())  # calculate norm coord\n                elif coord+'norm' in cellTags:  # elif norm coord exists\n                    cellTags[coord] = cellTags[coord+'norm']*getattr(sim.net.params, 'size'+coord.upper())  # calculate norm coord\n                else:\n                    cellTags[coord+'norm'] = cellTags[coord] = 0\n            if 'cellModel' in self.tags.keys() and self.tags['cellModel'] == 'Vecstim':  # if VecStim, copy spike times to params\n                cellTags['params']['spkTimes'] = self.tags['cellsList'][i]['spkTimes']\n            cells.append(self.cellModelClass(gid, cellTags)) # instantiate Cell object\n            if sim.cfg.verbose: print(('Cell %d/%d (gid=%d) of pop %d, on node %d, '%(i, self.tags['numCells']-1, gid, i, sim.rank)))\n        sim.net.lastGid = sim.net.lastGid + len(self.tags['cellsList'])\n        return cells"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef createCellsGrid (self):\n        ''' Create population cells based on fixed number of cells'''\n        from .. import sim\n\n        cells = []\n        \n        rangeLocs = [[0, getattr(sim.net.params, 'size'+coord)] for coord in ['X','Y','Z']]\n        for icoord, coord in enumerate(['x', 'y', 'z']):\n            # constrain to range set by user\n            if coord+'normRange' in self.tags:  # if normalized range, convert to normalized\n                self.tags[coord+'Range'] = [float(point) * getattr(sim.net.params, 'size'+coord.upper()) for point in self.tags[coord+'Range']]                \n            if coord+'Range' in self.tags:  # if user provided absolute range, calculate range\n                self.tags[coord+'normRange'] = [float(point) / getattr(sim.net.params, 'size'+coord.upper()) for point in self.tags[coord+'Range']]\n                rangeLocs[icoord] = [self.tags[coord+'Range'][0], self.tags[coord+'Range'][1]] \n              \n        gridSpacing = self.tags['gridSpacing']\n        gridLocs = []\n        for x in np.arange(rangeLocs[0][0], rangeLocs[0][1]+1, gridSpacing):\n            for y in np.arange(rangeLocs[1][0], rangeLocs[1][1]+1, gridSpacing):\n                for z in np.arange(rangeLocs[2][0], rangeLocs[2][1]+1, gridSpacing):\n                    gridLocs.append((x, y, z))\n\n        numCells = len(gridLocs)\n\n        for i in self._distributeCells(numCells)[sim.rank]:\n            gid = sim.net.lastGid+i\n            self.cellGids.append(gid)  # add gid list of cells belonging to this population - not needed?\n            cellTags = {k: v for (k, v) in self.tags.items() if k in sim.net.params.popTagsCopiedToCells}  # copy all pop tags to cell tags, except those that are pop-specific\n            cellTags['pop'] = self.tags['pop']\n            cellTags['xnorm'] = gridLocs[i][0] / sim.net.params.sizeX # set x location (um)\n            cellTags['ynorm'] = gridLocs[i][1] / sim.net.params.sizeY # set y location (um)\n            cellTags['znorm'] = gridLocs[i][2] / sim.net.params.sizeZ # set z location (um)\n            cellTags['x'] = gridLocs[i][0]   # set x location (um)\n            cellTags['y'] = gridLocs[i][1] # set y location (um)\n            cellTags['z'] = gridLocs[i][2] # set z location (um)\n            cells.append(self.cellModelClass(gid, cellTags)) # instantiate Cell object\n            if sim.cfg.verbose: print(('Cell %d/%d (gid=%d) of pop %s, on node %d, '%(i, numCells, gid, self.tags['pop'], sim.rank)))\n        sim.net.lastGid = sim.net.lastGid + numCells\n        return cells", "response": "Create cells grid based on fixed number of cells"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setCellClass (self):\n        ''' Set cell class (CompartCell, PointCell, etc)'''\n        from .. import sim\n        \n        # Check whether it's a NeuroML2 based cell\n        if 'originalFormat' in self.tags:\n            if self.tags['originalFormat'] == 'NeuroML2':\n                self.cellModelClass = sim.NML2Cell\n            if self.tags['originalFormat'] == 'NeuroML2_SpikeSource':\n                self.cellModelClass = sim.NML2SpikeSource\n\n        else:\n            # set cell class: CompartCell for compartmental cells of PointCell for point neurons (NetStims, IntFire1,...)\n            try: # check if cellModel corresponds to an existing point process mechanism; if so, use PointCell\n\n                    tmp = getattr(h, self.tags['cellModel'])\n                    self.cellModelClass = sim.PointCell\n                    excludeTags = ['pop', 'cellModel', 'cellType', 'numCells', 'density', 'cellsList',\n                                'xRange', 'yRange', 'zRange', 'xnormRange', 'ynormRange', 'znormRange', 'vref', 'spkTimes']\n                    params = {k: v for k,v in self.tags.items() if k not in excludeTags}\n                    self.tags['params'] = params\n                    for k in self.tags['params']: self.tags.pop(k)\n                    sim.net.params.popTagsCopiedToCells.append('params')\n            except:\n                if getattr(self.tags, 'cellModel', None) in ['NetStim', 'VecStim', 'IntFire1', 'IntFire2', 'IntFire4']:\n                    print('Warning: could not find %s point process mechanism required for population %s' % (self.tags['cellModel'], self.tags['pop']))\n                self.cellModelClass = sim.CompartCell", "response": "Set the class of the cell in the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates relative segment coordinates from 3d point coordinates used for LFP calculation", "response": "def calcRelativeSegCoords(self):   \n        \"\"\"Calculate segment coordinates from 3d point coordinates\n        Used for LFP calc (one per population cell; assumes same morphology)\"\"\"\n\n        from .. import sim\n\n        localPopGids = list(set(sim.net.gid2lid.keys()).intersection(set(self.cellGids)))\n        if localPopGids: \n            cell = sim.net.cells[sim.net.gid2lid[localPopGids[0]]]\n        else:\n            return -1\n\n        ix = 0  # segment index\n\n        p3dsoma = cell.getSomaPos()\n        nseg = sum([sec['hObj'].nseg for sec in list(cell.secs.values())])\n        \n        p0 = np.zeros((3, nseg))  # hold the coordinates of segment starting points\n        p1 = np.zeros((3, nseg))  # hold the coordinates of segment end points\n        d0 = np.zeros(nseg) \n        d1 = np.zeros(nseg) \n\n        for sec in list(cell.secs.values()):\n            hSec = sec['hObj']\n            hSec.push()\n            n3d = int(h.n3d())  # get number of n3d points in each section\n            p3d = np.zeros((3, n3d))  # to hold locations of 3D morphology for the current section\n            l3d = np.zeros(n3d)  # to hold locations of 3D morphology for the current section\n            diam3d = np.zeros(n3d)  # to diameters\n\n            for i in range(n3d):\n                p3d[0, i] = h.x3d(i) - p3dsoma[0]\n                p3d[1, i] = h.y3d(i) - p3dsoma[1]  # shift coordinates such to place soma at the origin.\n                p3d[2, i] = h.z3d(i) - p3dsoma[2]\n                diam3d[i] = h.diam3d(i)\n                l3d[i] = h.arc3d(i)\n\n            l3d /= hSec.L                  # normalize\n            nseg = hSec.nseg\n            \n            l0 = np.zeros(nseg)     # keep range of segment starting point \n            l1 = np.zeros(nseg)     # keep range of segment ending point \n            \n            for iseg, seg in enumerate(hSec):\n                l0[iseg] = seg.x - 0.5*1/nseg   # x (normalized distance along the section) for the beginning of the segment\n                l1[iseg] = seg.x + 0.5*1/nseg   # x for the end of the segment\n\n            p0[0, ix:ix+nseg] = np.interp(l0, l3d, p3d[0, :])\n            p0[1, ix:ix+nseg] = np.interp(l0, l3d, p3d[1, :])\n            p0[2, ix:ix+nseg] = np.interp(l0, l3d, p3d[2, :])\n            d0[ix:ix+nseg] = np.interp(l0, l3d, diam3d[:])\n\n            p1[0, ix:ix+nseg] = np.interp(l1, l3d, p3d[0, :])\n            p1[1, ix:ix+nseg] = np.interp(l1, l3d, p3d[1, :])\n            p1[2, ix:ix+nseg] = np.interp(l1, l3d, p3d[2, :])\n            d1[ix:ix+nseg] = np.interp(l1, l3d, diam3d[:])\n            ix += nseg\n            h.pop_section() \n\n        self._morphSegCoords = {}\n\n        self._morphSegCoords['p0'] = p0\n        self._morphSegCoords['p1'] = p1\n\n        self._morphSegCoords['d0'] = d0\n        self._morphSegCoords['d1'] = d1\n\n        return self._morphSegCoords"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a single LFP plot for the given set of electrodes and plots.", "response": "def plotLFP (electrodes = ['avg', 'all'], plots = ['timeSeries', 'PSD', 'spectrogram', 'locations'], timeRange=None, NFFT=256, noverlap=128, \n    nperseg=256, minFreq=1, maxFreq=100, stepFreq=1, smooth=0, separation=1.0, includeAxon=True, logx=False, logy=False, norm=False, dpi=200, overlay=False, filtFreq = False, filtOrder=3, detrend=False, specType='morlet', fontSize=14, colors = None, maxPlots=8, figSize = (8,8), saveData = None, saveFig = None, showFig = True): \n    ''' \n    Plot LFP\n        - electrodes (list): List of electrodes to include; 'avg'=avg of all electrodes; 'all'=each electrode separately (default: ['avg', 'all'])\n        - plots (list): list of plot types to show (default: ['timeSeries', 'PSD', 'timeFreq', 'locations']) \n        - timeRange ([start:stop]): Time range of spikes shown; if None shows all (default: None)\n        - NFFT (int, power of 2): Number of data points used in each block for the PSD and time-freq FFT (default: 256)\n        - noverlap (int, <nperseg): Number of points of overlap between segments for PSD and time-freq (default: 128)\n        - minFreq (float)\n        - maxFreq (float): Maximum frequency shown in plot for PSD and time-freq (default: 100 Hz)\n        - stepFreq (float)\n        - nperseg (int): Length of each segment for time-freq (default: 256)\n        - smooth (int): Window size for smoothing LFP; no smoothing if 0 (default: 0)\n        - separation (float): Separation factor between time-resolved LFP plots; multiplied by max LFP value (default: 1.0)\n        - includeAxon (boolean): Whether to show the axon in the location plot (default: True)\n        - logx (boolean)\n        - logy (boolean)\n        - norm (boolean)\n        - filtFreq (float)\n        - filtOrder (int)\n        - detrend (false)\n        - specType ('morlet'|'fft')\n        - overlay (boolean)\n        - dpi (int) \n        - figSize ((width, height)): Size of figure (default: (10,8))\n        - saveData (None|True|'fileName'): File name where to save the final data used to generate the figure; \n            if set to True uses filename from simConfig (default: None)\n        - saveFig (None|True|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)\n        - showFig (True|False): Whether to show the figure or not (default: True)\n\n        - Returns figure handles\n    \n    '''\n\n    from .. import sim\n    from ..support.scalebar import add_scalebar\n\n    print('Plotting LFP ...')\n\n    if not colors: colors = colorList\n    \n    # set font size\n    plt.rcParams.update({'font.size': fontSize})\n    \n    # time range\n    if timeRange is None:\n        timeRange = [0,sim.cfg.duration]\n\n    lfp = np.array(sim.allSimData['LFP'])[int(timeRange[0]/sim.cfg.recordStep):int(timeRange[1]/sim.cfg.recordStep),:]\n\n    if filtFreq:\n        from scipy import signal\n        fs = 1000.0/sim.cfg.recordStep\n        nyquist = fs/2.0    \n        if isinstance(filtFreq, list): # bandpass\n            Wn = [filtFreq[0]/nyquist, filtFreq[1]/nyquist]\n            b, a = signal.butter(filtOrder, Wn, btype='bandpass')\n        elif isinstance(filtFreq, Number): # lowpass\n            Wn = filtFreq/nyquist\n            b, a = signal.butter(filtOrder, Wn)\n        for i in range(lfp.shape[1]):\n            lfp[:,i] = signal.filtfilt(b, a, lfp[:,i])\n\n    if detrend:\n        from scipy import signal\n        for i in range(lfp.shape[1]):\n            lfp[:,i] = signal.detrend(lfp[:,i])\n\n    if norm:\n        for i in range(lfp.shape[1]):\n            offset = min(lfp[:,i])\n            if offset <= 0:\n                lfp[:,i] += abs(offset)\n            lfp[:,i] /= max(lfp[:,i])\n\n    # electrode selection\n    if 'all' in electrodes:\n        electrodes.remove('all')\n        electrodes.extend(list(range(int(sim.net.recXElectrode.nsites))))\n\n    # plotting\n    figs = []\n    #maxPlots = 8.0\n    \n    data = {'lfp': lfp}  # returned data\n\n\n    # time series -----------------------------------------\n    if 'timeSeries' in plots:\n        ydisp = np.absolute(lfp).max() * separation\n        offset = 1.0*ydisp\n        t = np.arange(timeRange[0], timeRange[1], sim.cfg.recordStep)\n\n        if figSize:\n            figs.append(plt.figure(figsize=figSize))\n\n        for i,elec in enumerate(electrodes):\n            if elec == 'avg':\n                lfpPlot = np.mean(lfp, axis=1)\n                color = 'k'\n                lw=1.0\n            elif isinstance(elec, Number) and elec <= sim.net.recXElectrode.nsites:\n                lfpPlot = lfp[:, elec]\n                color = colors[i%len(colors)]\n                lw=1.0\n            plt.plot(t, -lfpPlot+(i*ydisp), color=color, linewidth=lw)\n            if len(electrodes) > 1:\n                plt.text(timeRange[0]-0.07*(timeRange[1]-timeRange[0]), (i*ydisp), elec, color=color, ha='center', va='top', fontsize=fontSize, fontweight='bold')\n\n        ax = plt.gca()\n\n        data['lfpPlot'] = lfpPlot\n        data['ydisp'] =  ydisp\n        data['t'] = t\n\n        # format plot\n        if len(electrodes) > 1:\n            plt.text(timeRange[0]-0.14*(timeRange[1]-timeRange[0]), (len(electrodes)*ydisp)/2.0, 'LFP electrode', color='k', ha='left', va='bottom', fontSize=fontSize, rotation=90)\n            plt.ylim(-offset, (len(electrodes))*ydisp)\n        else:       \n            plt.suptitle('LFP Signal', fontSize=fontSize, fontweight='bold')\n        ax.invert_yaxis()\n        plt.xlabel('time (ms)', fontsize=fontSize)\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        plt.subplots_adjust(bottom=0.1, top=1.0, right=1.0)\n\n        # calculate scalebar size and add scalebar\n        round_to_n = lambda x, n, m: int(np.ceil(round(x, -int(np.floor(np.log10(abs(x)))) + (n - 1)) / m)) * m \n        scaley = 1000.0  # values in mV but want to convert to uV\n        m = 10.0\n        sizey = 100/scaley\n        while sizey > 0.25*ydisp:\n            try:\n                sizey = round_to_n(0.2*ydisp*scaley, 1, m) / scaley\n            except:\n                sizey /= 10.0\n            m /= 10.0\n        labely = '%.3g $\\mu$V'%(sizey*scaley)#)[1:]\n        if len(electrodes) > 1:\n            add_scalebar(ax,hidey=True, matchy=False, hidex=False, matchx=False, sizex=0, sizey=-sizey, labely=labely, unitsy='$\\mu$V', scaley=scaley, \n                loc=3, pad=0.5, borderpad=0.5, sep=3, prop=None, barcolor=\"black\", barwidth=2)\n        else:\n            add_scalebar(ax, hidey=True, matchy=False, hidex=True, matchx=True, sizex=None, sizey=-sizey, labely=labely, unitsy='$\\mu$V', scaley=scaley, \n                unitsx='ms', loc=3, pad=0.5, borderpad=0.5, sep=3, prop=None, barcolor=\"black\", barwidth=2)\n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_'+'lfp.png'\n            plt.savefig(filename, dpi=dpi)\n\n    # PSD ----------------------------------\n    if 'PSD' in plots:\n        if overlay:\n            figs.append(plt.figure(figsize=figSize))\n        else:\n            numCols = 1# np.round(len(electrodes) / maxPlots) + 1\n            figs.append(plt.figure(figsize=(figSize[0]*numCols, figSize[1])))\n            #import seaborn as sb\n\n        allFreqs = []\n        allSignal = []\n        data['allFreqs'] = allFreqs\n        data['allSignal'] = allSignal\n\n        for i,elec in enumerate(electrodes):\n            if not overlay:\n                plt.subplot(np.ceil(len(electrodes)/numCols), numCols,i+1)\n            if elec == 'avg':\n                lfpPlot = np.mean(lfp, axis=1)\n                color = 'k'\n                lw=1.5\n            elif isinstance(elec, Number) and elec <= sim.net.recXElectrode.nsites:\n                lfpPlot = lfp[:, elec]\n                color = colors[i%len(colors)]\n                lw=1.5\n            \n            Fs = int(1000.0/sim.cfg.recordStep)\n            power = mlab.psd(lfpPlot, Fs=Fs, NFFT=NFFT, detrend=mlab.detrend_none, window=mlab.window_hanning, \n                noverlap=noverlap, pad_to=None, sides='default', scale_by_freq=None)\n\n            if smooth:\n                signal = _smooth1d(10*np.log10(power[0]), smooth)\n            else:\n                signal = 10*np.log10(power[0])\n            freqs = power[1]\n\n            allFreqs.append(freqs)\n            allSignal.append(signal)\n\n            plt.plot(freqs[freqs<maxFreq], signal[freqs<maxFreq], linewidth=lw, color=color, label='Electrode %s'%(str(elec)))\n            plt.xlim([0, maxFreq])\n            if len(electrodes) > 1 and not overlay:\n                plt.title('Electrode %s'%(str(elec)), fontsize=fontSize)\n            plt.ylabel('dB/Hz', fontsize=fontSize)\n            \n            # ALTERNATIVE PSD CALCULATION USING WELCH\n            # from http://joelyancey.com/lfp-python-practice/\n            # from scipy import signal as spsig\n            # Fs = int(1000.0/sim.cfg.recordStep)\n            # maxFreq=100\n            # f, psd = spsig.welch(lfpPlot, Fs, nperseg=100)\n            # plt.semilogy(f,psd,'k')\n            # sb.despine()\n            # plt.xlim((0,maxFreq))\n            # plt.yticks(size=fontsiz)\n            # plt.xticks(size=fontsiz)\n            # plt.ylabel('$uV^{2}/Hz$',size=fontsiz)\n\n        # format plot\n        plt.xlabel('Frequency (Hz)', fontsize=fontSize)\n        if overlay:\n            plt.legend(fontsize=fontSize)\n        plt.tight_layout()\n        plt.suptitle('LFP Power Spectral Density', fontsize=fontSize, fontweight='bold') # add yaxis in opposite side\n        plt.subplots_adjust(bottom=0.08, top=0.92)\n\n        if logx:\n            pass\n        #from IPython import embed; embed()\n\n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_'+'lfp_psd.png'\n            plt.savefig(filename, dpi=dpi)\n\n    # Spectrogram ------------------------------\n    if 'spectrogram' in plots:\n        import matplotlib.cm as cm\n        numCols = 1 #np.round(len(electrodes) / maxPlots) + 1\n        figs.append(plt.figure(figsize=(figSize[0]*numCols, figSize[1])))\n        #t = np.arange(timeRange[0], timeRange[1], sim.cfg.recordStep)\n        \n\n        if specType == 'morlet':\n            from ..support.morlet import MorletSpec, index2ms\n\n            spec = []\n            \n            for i,elec in enumerate(electrodes):\n                if elec == 'avg':\n                    lfpPlot = np.mean(lfp, axis=1)\n                elif isinstance(elec, Number) and elec <= sim.net.recXElectrode.nsites:\n                    lfpPlot = lfp[:, elec]\n                fs = int(1000.0 / sim.cfg.recordStep)\n                t_spec = np.linspace(0, index2ms(len(lfpPlot), fs), len(lfpPlot))\n                spec.append(MorletSpec(lfpPlot, fs, freqmin=minFreq, freqmax=maxFreq, freqstep=stepFreq))\n                \n            f = np.array(range(minFreq, maxFreq+1, stepFreq))  # only used as output for user\n\n            vmin = np.array([s.TFR for s in spec]).min()\n            vmax = np.array([s.TFR for s in spec]).max()\n            for i,elec in enumerate(electrodes):\n                plt.subplot(np.ceil(len(electrodes) / numCols), numCols, i + 1)\n                T = timeRange\n                F = spec[i].f\n                if norm:\n                    spec[i].TFR = spec[i].TFR / vmax\n                    S = spec[i].TFR\n                    vc = [0, 1]\n                else:\n                    S = spec[i].TFR\n                    vc = [vmin, vmax]\n\n                \n                plt.imshow(S, extent=(np.amin(T), np.amax(T), np.amin(F), np.amax(F)), origin='lower', interpolation='None', aspect='auto', vmin=vc[0], vmax=vc[1], cmap=plt.get_cmap('viridis'))\n                plt.colorbar(label='Power')\n                plt.ylabel('Hz')\n                plt.tight_layout()                \n                if len(electrodes) > 1:\n                    plt.title('Electrode %s' % (str(elec)), fontsize=fontSize - 2)\n\n\n        elif specType == 'fft':\n\n            from scipy import signal as spsig\n            spec = []\n            \n            for i,elec in enumerate(electrodes):\n                if elec == 'avg':\n                    lfpPlot = np.mean(lfp, axis=1)\n                elif isinstance(elec, Number) and elec <= sim.net.recXElectrode.nsites:\n                    lfpPlot = lfp[:, elec]\n                # creates spectrogram over a range of data \n                # from: http://joelyancey.com/lfp-python-practice/\n                fs = int(1000.0/sim.cfg.recordStep)\n                f, t_spec, x_spec = spsig.spectrogram(lfpPlot, fs=fs, window='hanning',\n                detrend=mlab.detrend_none, nperseg=nperseg, noverlap=noverlap, nfft=NFFT,  mode='psd')\n                x_mesh, y_mesh = np.meshgrid(t_spec*1000.0, f[f<maxFreq])\n                spec.append(10*np.log10(x_spec[f<maxFreq]))\n\n            vmin = np.array(spec).min()\n            vmax = np.array(spec).max()\n            for i,elec in enumerate(electrodes):\n                plt.subplot(np.ceil(len(electrodes)/numCols), numCols, i+1)\n                plt.pcolormesh(x_mesh, y_mesh, spec[i], cmap=cm.viridis, vmin=vmin, vmax=vmax)\n                plt.colorbar(label='dB/Hz', ticks=[np.ceil(vmin), np.floor(vmax)])\n                if logy:\n                    plt.yscale('log')\n                    plt.ylabel('Log-frequency (Hz)')\n                    if isinstance(logy, list):\n                        yticks = tuple(logy)\n                        plt.yticks(yticks, yticks)\n                else:\n                    plt.ylabel('(Hz)')\n                if len(electrodes) > 1:\n                    plt.title('Electrode %s'%(str(elec)), fontsize=fontSize-2)\n\n        plt.xlabel('time (ms)', fontsize=fontSize)\n        plt.tight_layout()\n        plt.suptitle('LFP spectrogram', size=fontSize, fontweight='bold')\n        plt.subplots_adjust(bottom=0.08, top=0.90)\n        \n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_'+'lfp_timefreq.png'\n            plt.savefig(filename, dpi=dpi)\n\n    # locations ------------------------------\n    if 'locations' in plots:\n        cvals = [] # used to store total transfer resistance\n\n        for cell in sim.net.compartCells:\n            trSegs = list(np.sum(sim.net.recXElectrode.getTransferResistance(cell.gid)*1e3, axis=0)) # convert from Mohm to kilohm\n            if not includeAxon:\n                i = 0\n                for secName, sec in cell.secs.items():\n                    nseg = sec['hObj'].nseg #.geom.nseg\n                    if 'axon' in secName:\n                        for j in range(i,i+nseg): del trSegs[j] \n                    i+=nseg\n            cvals.extend(trSegs)  \n            \n        includePost = [c.gid for c in sim.net.compartCells]\n        fig = sim.analysis.plotShape(includePost=includePost, showElectrodes=electrodes, cvals=cvals, includeAxon=includeAxon, dpi=dpi,\n        fontSize=fontSize, saveFig=saveFig, showFig=showFig, figSize=figSize)[0]\n        figs.append(fig)\n\n\n    outputData = {'LFP': lfp, 'electrodes': electrodes, 'timeRange': timeRange, 'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n\n    if 'PSD' in plots:\n        outputData.update({'allFreqs': allFreqs, 'allSignal': allSignal})\n    \n    if 'spectrogram' in plots:\n        outputData.update({'spec': spec, 't': t_spec*1000.0, 'freqs': f[f<=maxFreq]})\n\n    #save figure data\n    if saveData:\n        figData = outputData\n    \n        _saveFigData(figData, saveData, 'lfp')\n\n\n    # show fig \n    if showFig: _showFigure()\n\n    return figs, outputData"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsequences of commands to create network", "response": "def create (netParams=None, simConfig=None, output=False):\n    ''' Sequence of commands to create network '''\n    from .. import sim\n    import __main__ as top\n    if not netParams: netParams = top.netParams\n    if not simConfig: simConfig = top.simConfig\n\n    sim.initialize(netParams, simConfig)  # create network object and set cfg and net params\n    pops = sim.net.createPops()                  # instantiate network populations\n    cells = sim.net.createCells()                 # instantiate network cells based on defined populations\n    conns = sim.net.connectCells()                # create connections between cells based on params\n    stims = sim.net.addStims()                    # add external stimulation to cells (IClamps etc)\n    rxd = sim.net.addRxD()                    # add reaction-diffusion (RxD)\n    simData = sim.setupRecording()             # setup variables to record for each cell (spikes, V traces, etc)\n\n    if output: return (pops, cells, conns, rxd, stims, simData)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsequence of commands to simulate network", "response": "def intervalSimulate (interval):\n    ''' Sequence of commands to simulate network '''\n    from .. import sim\n    sim.runSimWithIntervalFunc(interval, sim.intervalSave)                      # run parallel Neuron simulation  \n    #this gather is justa merging of files\n    sim.fileGather()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef createSimulate (netParams=None, simConfig=None, output=False):\n    ''' Sequence of commands create, simulate and analyse network '''\n    from .. import sim\n    (pops, cells, conns, stims, rxd, simData) = sim.create(netParams, simConfig, output=True)\n    sim.simulate() \n\n    if output: return (pops, cells, conns, stims, simData)", "response": "Sequence of commands create simulate and analyse network"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsequences of commands create simulate and analyse network", "response": "def intervalCreateSimulateAnalyze (netParams=None, simConfig=None, output=False, interval=None):\n    ''' Sequence of commands create, simulate and analyse network '''\n    import os\n    from .. import sim\n    (pops, cells, conns, stims, rxd, simData) = sim.create(netParams, simConfig, output=True)\n    try:\n        if sim.rank==0:\n            if os.path.exists('temp'):\n                for f in os.listdir('temp'):\n                    os.unlink('temp/{}'.format(f))\n            else:\n                os.mkdir('temp')\n        sim.intervalSimulate(interval)\n    except Exception as e:\n        print(e)\n        return\n    sim.pc.barrier()\n    sim.analyze()\n    if output: return (pops, cells, conns, stims, simData)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load (filename, simConfig=None, output=False, instantiate=True, createNEURONObj=True):\n    ''' Sequence of commands load, simulate and analyse network '''\n    from .. import sim\n    sim.initialize()  # create network object and set cfg and net params\n    sim.cfg.createNEURONObj = createNEURONObj\n    sim.loadAll(filename, instantiate=instantiate, createNEURONObj=createNEURONObj)\n    if simConfig: sim.setSimCfg(simConfig)  # set after to replace potentially loaded cfg\n    if len(sim.net.cells) == 0 and instantiate:\n        pops = sim.net.createPops()                  # instantiate network populations\n        cells = sim.net.createCells()                 # instantiate network cells based on defined populations\n        conns = sim.net.connectCells()                # create connections between cells based on params\n        stims = sim.net.addStims()                    # add external stimulation to cells (IClamps etc)\n        rxd = sim.net.addRxD()                    # add reaction-diffusion (RxD)\n\n    simData = sim.setupRecording()              # setup variables to record for each cell (spikes, V traces, etc)\n\n    if output: \n        try:\n            return (pops, cells, conns, stims, rxd, simData)\n        except:\n            pass", "response": "Load a file into a new network"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef createExportNeuroML2 (netParams=None, simConfig=None, reference=None, connections=True, stimulations=True, output=False, format='xml'):\n    ''' Sequence of commands to create and export network to NeuroML2 '''\n    from .. import sim\n    import __main__ as top\n    if not netParams: netParams = top.netParams\n    if not simConfig: simConfig = top.simConfig\n\n    sim.initialize(netParams, simConfig)  # create network object and set cfg and net params\n    pops = sim.net.createPops()                  # instantiate network populations\n    cells = sim.net.createCells()                 # instantiate network cells based on defined populations\n    conns = sim.net.connectCells()                # create connections between cells based on params\n    stims = sim.net.addStims()                    # add external stimulation to cells (IClamps etc)\n    rxd = sim.net.addRxD()                    # add reaction-diffusion (RxD)\n    simData = sim.setupRecording()              # setup variables to record for each cell (spikes, V traces, etc)\n    sim.exportNeuroML2(reference,connections,stimulations,format)     # export cells and connectivity to NeuroML 2 format\n\n    if output: return (pops, cells, conns, stims, rxd, simData)", "response": "Create and export a NeuroML2 network"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef invertDictMapping(d):\n    inv_map = {}\n    for k, v in d.items():\n        inv_map[v] = inv_map.get(v, [])\n        inv_map[v].append(k)\n    return inv_map", "response": "Invert mapping of dictionary to list of keys"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning spike ids and times ; with allCells = True just need to identify slice of time so can omit cellGids", "response": "def getSpktSpkid(cellGids=[], timeRange=None, allCells=False):\n    '''return spike ids and times; with allCells=True just need to identify slice of time so can omit cellGids'''\n    from .. import sim\n    import pandas as pd\n    \n    try: # Pandas 0.24 and later\n        from pandas import _lib as pandaslib\n    except: # Pandas 0.23 and earlier\n        from pandas import lib as pandaslib\n    df = pd.DataFrame(pandaslib.to_object_array([sim.allSimData['spkt'], sim.allSimData['spkid']]).transpose(), columns=['spkt', 'spkid'])\n    #df = pd.DataFrame(pd.lib.to_object_array([sim.allSimData['spkt'], sim.allSimData['spkid']]).transpose(), columns=['spkt', 'spkid'])\n    \n    if timeRange:\n        min, max = [int(df['spkt'].searchsorted(timeRange[i])) for i in range(2)] # binary search faster than query\n    else: # timeRange None or empty list means all times\n        min, max = 0, len(df)\n    if len(cellGids)==0 or allCells: # get all by either using flag or giving empty list -- can get rid of the flag\n        sel = df[min:max]\n    else:\n        sel = df[min:max].query('spkid in @cellGids')\n    return sel, sel['spkt'].tolist(), sel['spkid'].tolist()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calcTransferResistance(self, gid, seg_coords):\n        sigma = 0.3  # mS/mm \n\n        # Value used in NEURON extracellular recording example (\"extracellular_stim_and_rec\")\n        # rho = 35.4  # ohm cm, squid axon cytoplasm = 2.8249e-2 S/cm = 0.028 S/cm = 0.0028 S/mm = 2.8 mS/mm \n                    # rho_um = 35.4 * 0.01 = 35.4 / 1e6 * 1e4 = 0.354 Mohm um ~= 3 uS / um = 3000 uS / mm = 3 mS /mm\n                    # equivalent sigma value (~3) is 10x larger than Allen (0.3) \n                    # if use same sigma value, results are consistent\n\n        r05 = (seg_coords['p0'] + seg_coords['p1'])/2\n        dl = seg_coords['p1'] - seg_coords['p0']\n        \n        nseg = r05.shape[1]\n        \n        tr = np.zeros((self.nsites,nseg))\n        # tr_NEURON = np.zeros((self.nsites,nseg))  # used to compare with NEURON extracellular example\n\n        for j in range(self.nsites):   # calculate mapping for each site on the electrode\n            rel = np.expand_dims(self.pos[:, j], axis=1)   # coordinates of a j-th site on the electrode\n            rel_05 = rel - r05  # distance between electrode and segment centers\n            r2 = np.einsum('ij,ij->j', rel_05, rel_05)    # compute dot product column-wise, the resulting array has as many columns as original\n            \n            rlldl = np.einsum('ij,ij->j', rel_05, dl)    # compute dot product column-wise, the resulting array has as many columns as original\n            dlmag = np.linalg.norm(dl, axis=0)  # length of each segment\n            rll = abs(rlldl/dlmag)   # component of r parallel to the segment axis it must be always positive\n            rT2 = r2 - rll**2  # square of perpendicular component\n            up = rll + dlmag/2\n            low = rll - dlmag/2\n            num = up + np.sqrt(up**2 + rT2)\n            den = low + np.sqrt(low**2 + rT2)\n            tr[j, :] = np.log(num/den)/dlmag  # units of (1/um) use with imemb_ (total seg current)\n\n            # Consistent with NEURON extracellular recording example\n            # r = np.sqrt(rel_05[0,:]**2 + rel_05[1,:]**2 + rel_05[2,:]**2)\n            # tr_NEURON[j, :] = (rho / 4 / math.pi)*(1/r)*0.01\n\n        tr *= 1/(4*math.pi*sigma)  # units: 1/um / (mS/mm) = mm/um / mS = 1e3 * kOhm = MOhm\n        self.transferResistances[gid] = tr", "response": "Calculates the transfer resistance for each entry in the segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the nTE of a set of cells from which to obtain spike train 1.", "response": "def nTE(cells1 = [], cells2 = [], spks1 = None, spks2 = None, timeRange = None, binSize = 20, numShuffle = 30):\n    ''' \n    Calculate normalized transfer entropy\n        - cells1 (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Subset of cells from which to obtain spike train 1 (default: [])\n        - cells2 (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Subset of cells from which to obtain spike train 1 (default: [])\n        - spks1 (list): Spike train 1; list of spike times; if omitted then obtains spikes from cells1 (default: None)\n        - spks2 (list): Spike train 2; list of spike times; if omitted then obtains spikes from cells2 (default: None)\n        - timeRange ([min, max]): Range of time to calculate nTE in ms (default: [0,cfg.duration])\n        - binSize (int): Bin size used to convert spike times into histogram \n        - numShuffle (int): Number of times to shuffle spike train 1 to calculate TEshuffled; note: nTE = (TE - TEShuffled)/H(X2F|X2P)\n\n        - Returns nTE (float): normalized transfer entropy \n    '''\n\n    from neuron import h\n    import netpyne\n    from .. import sim\n    import os\n            \n    root = os.path.dirname(netpyne.__file__)\n    \n    if 'nte' not in dir(h): \n        try: \n            print(' Warning: support/nte.mod not compiled; attempting to compile from %s via \"nrnivmodl support\"'%(root))\n            os.system('cd ' + root + '; nrnivmodl support')\n            from neuron import load_mechanisms\n            load_mechanisms(root)\n            print(' Compilation of support folder mod files successful')\n        except:\n            print(' Error compiling support folder mod files')\n            return\n\n    h.load_file(root+'/support/nte.hoc') # nTE code (also requires support/net.mod)\n    \n    if not spks1:  # if doesnt contain a list of spk times, obtain from cells specified\n        cells, cellGids, netStimPops = getCellsInclude(cells1)\n        numNetStims = 0\n\n        # Select cells to include\n        if len(cellGids) > 0:\n            try:\n                spkts = [spkt for spkgid,spkt in zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]\n            except:\n                spkts = []\n        else: \n            spkts = []\n\n        # Add NetStim spikes\n        spkts = list(spkts)\n        numNetStims = 0\n        for netStimPop in netStimPops:\n            if 'stims' in sim.allSimData:\n                cellStims = [cellStim for cell,cellStim in sim.allSimData['stims'].items() if netStimPop in cellStim]\n                if len(cellStims) > 0:\n                    spktsNew = [spkt for cellStim in cellStims for spkt in cellStim[netStimPop] ]\n                    spkts.extend(spktsNew)\n                    numNetStims += len(cellStims)\n\n        spks1 = list(spkts)\n\n    if not spks2:  # if doesnt contain a list of spk times, obtain from cells specified\n        cells, cellGids, netStimPops = getCellsInclude(cells2)\n        numNetStims = 0\n\n        # Select cells to include\n        if len(cellGids) > 0:\n            try:\n                spkts = [spkt for spkgid,spkt in zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]\n            except:\n                spkts = []\n        else: \n            spkts = []\n\n        # Add NetStim spikes\n        spkts = list(spkts)\n        numNetStims = 0\n        for netStimPop in netStimPops:\n            if 'stims' in sim.allSimData:\n                cellStims = [cellStim for cell,cellStim in sim.allSimData['stims'].items() if netStimPop in cellStim]\n                if len(cellStims) > 0:\n                    spktsNew = [spkt for cellStim in cellStims for spkt in cellStim[netStimPop] ]\n                    spkts.extend(spktsNew)\n                    numNetStims += len(cellStims)\n\n        spks2 = list(spkts)\n\n    # time range\n    if getattr(sim, 'cfg', None):\n        timeRange = [0,sim.cfg.duration]\n    else:\n        timeRange = [0, max(spks1+spks2)]\n\n    inputVec = h.Vector()\n    outputVec = h.Vector()\n    histo1 = np.histogram(spks1, bins = np.arange(timeRange[0], timeRange[1], binSize))\n    histoCount1 = histo1[0] \n    histo2 = np.histogram(spks2, bins = np.arange(timeRange[0], timeRange[1], binSize))\n    histoCount2 = histo2[0] \n\n    inputVec.from_python(histoCount1)\n    outputVec.from_python(histoCount2)\n    out = h.normte(inputVec, outputVec, numShuffle)\n    TE, H, nTE, _, _ = out.to_python()\n    return nTE"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a single Granger Causality V1 and V2 grid.", "response": "def granger(cells1 = [], cells2 = [], spks1 = None, spks2 = None, label1 = 'spkTrain1', label2 = 'spkTrain2', timeRange = None, binSize=5, plotFig = True, \n    saveData = None, saveFig = None, showFig = True):\n    ''' \n    Calculate and optionally plot Granger Causality \n        - cells1 (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Subset of cells from which to obtain spike train 1 (default: [])\n        - cells2 (['all',|'allCells','allNetStims',|,120,|,'E1'|,('L2', 56)|,('L5',[4,5,6])]): Subset of cells from which to obtain spike train 2 (default: [])\n        - spks1 (list): Spike train 1; list of spike times; if omitted then obtains spikes from cells1 (default: None)\n        - spks2 (list): Spike train 2; list of spike times; if omitted then obtains spikes from cells2 (default: None)\n        - label1 (string): Label for spike train 1 to use in plot\n        - label2 (string): Label for spike train 2 to use in plot\n        - timeRange ([min, max]): Range of time to calculate nTE in ms (default: [0,cfg.duration])\n        - binSize (int): Bin size used to convert spike times into histogram \n        - plotFig (True|False): Whether to plot a figure showing Granger Causality Fx2y and Fy2x\n        - saveData (None|'fileName'): File name where to save the final data used to generate the figure (default: None)\n        - saveFig (None|'fileName'): File name where to save the figure;\n            if set to True uses filename from simConfig (default: None)(default: None)\n        - showFig (True|False): Whether to show the figure or not;\n            if set to True uses filename from simConfig (default: None)\n\n        - Returns \n            F: list of freqs\n            Fx2y: causality measure from x to y \n            Fy2x: causality from y to x \n            Fxy: instantaneous causality between x and y \n            fig: Figure handle \n    '''\n    \n    from .. import sim\n    import numpy as np\n    from netpyne.support.bsmart import pwcausalr\n\n    if not spks1:  # if doesnt contain a list of spk times, obtain from cells specified\n        cells, cellGids, netStimPops = getCellsInclude(cells1)\n        numNetStims = 0\n\n        # Select cells to include\n        if len(cellGids) > 0:\n            try:\n                spkts = [spkt for spkgid,spkt in zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]\n            except:\n                spkts = []\n        else: \n            spkts = []\n\n        # Add NetStim spikes\n        spkts = list(spkts)\n        numNetStims = 0\n        for netStimPop in netStimPops:\n            if 'stims' in sim.allSimData:\n                cellStims = [cellStim for cell,cellStim in sim.allSimData['stims'].items() if netStimPop in cellStim]\n                if len(cellStims) > 0:\n                    spktsNew = [spkt for cellStim in cellStims for spkt in cellStim[netStimPop] ]\n                    spkts.extend(spktsNew)\n                    numNetStims += len(cellStims)\n\n        spks1 = list(spkts)\n\n    if not spks2:  # if doesnt contain a list of spk times, obtain from cells specified\n        cells, cellGids, netStimPops = getCellsInclude(cells2)\n        numNetStims = 0\n\n        # Select cells to include\n        if len(cellGids) > 0:\n            try:\n                spkts = [spkt for spkgid,spkt in zip(sim.allSimData['spkid'],sim.allSimData['spkt']) if spkgid in cellGids]\n            except:\n                spkts = []\n        else: \n            spkts = []\n\n        # Add NetStim spikes\n        spkts = list(spkts)\n        numNetStims = 0\n        for netStimPop in netStimPops:\n            if 'stims' in sim.allSimData:\n                cellStims = [cellStim for cell,cellStim in sim.allSimData['stims'].items() if netStimPop in cellStim]\n                if len(cellStims) > 0:\n                    spktsNew = [spkt for cellStim in cellStims for spkt in cellStim[netStimPop] ]\n                    spkts.extend(spktsNew)\n                    numNetStims += len(cellStims)\n\n        spks2 = list(spkts)\n\n\n    # time range\n    if timeRange is None:\n        if getattr(sim, 'cfg', None):\n            timeRange = [0,sim.cfg.duration]\n        else:\n            timeRange = [0, max(spks1+spks2)]\n\n    histo1 = np.histogram(spks1, bins = np.arange(timeRange[0], timeRange[1], binSize))\n    histoCount1 = histo1[0] \n\n    histo2 = np.histogram(spks2, bins = np.arange(timeRange[0], timeRange[1], binSize))\n    histoCount2 = histo2[0] \n\n    fs = int(1000/binSize)\n    F,pp,cohe,Fx2y,Fy2x,Fxy = pwcausalr(np.array([histoCount1, histoCount2]), 1, len(histoCount1), 10, fs, int(fs/2))\n\n\n    # plot granger\n    fig = -1\n    if plotFig:\n        fig = plt.figure()\n        plt.plot(F, Fy2x[0], label = label2 + ' -> ' + label1)\n        plt.plot(F, Fx2y[0], 'r', label = label1 + ' -> ' + label2)\n        plt.xlabel('Frequency (Hz)')\n        plt.ylabel('Granger Causality')\n        plt.legend()\n        \n        # save figure data\n        if saveData:\n            figData = {'cells1': cells1, 'cells2': cells2, 'spks1': cells1, 'spks2': cells2, 'binSize': binSize, 'Fy2x': Fy2x[0], 'Fx2y': Fx2y[0], \n            'saveData': saveData, 'saveFig': saveFig, 'showFig': showFig}\n        \n            _saveFigData(figData, saveData, '2Dnet')\n     \n        # save figure\n        if saveFig: \n            if isinstance(saveFig, basestring):\n                filename = saveFig\n            else:\n                filename = sim.cfg.filename+'_'+'2Dnet.png'\n            plt.savefig(filename)\n\n        # show fig \n        if showFig: _showFigure()\n\n    return fig, {'F': F, 'Fx2y': Fx2y[0], 'Fy2x': Fy2x[0], 'Fxy': Fxy[0]}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef importConnFromExcel (fileName, sheetName):\n    ''' Import connectivity rules from Excel sheet'''\n    import openpyxl as xl\n\n    # set columns\n    colPreTags = 0 # 'A'\n    colPostTags = 1 # 'B'\n    colConnFunc = 2 # 'C'\n    colSyn = 3 # 'D'\n    colProb = 5 # 'F'\n    colWeight = 6 # 'G'\n    colAnnot = 8 # 'I' \n\n    outFileName = fileName[:-5]+'_'+sheetName+'.py' # set output file name\n\n    connText = \"\"\"## Generated using importConnFromExcel() function in params/utils.py \\n\\nnetParams['connParams'] = [] \\n\\n\"\"\"\n    \n    # open excel file and sheet\n    wb = xl.load_workbook(fileName)\n    sheet = wb.get_sheet_by_name(sheetName)\n    numRows = sheet.get_highest_row()\n\n    with open(outFileName, 'w') as f:\n        f.write(connText)  # write starting text\n        for row in range(1,numRows+1):\n            if sheet.cell(row=row, column=colProb).value:  # if not empty row\n                print('Creating conn rule for row ' + str(row))\n                # read row values\n                pre = sheet.cell(row=row, column=colPreTags).value\n                post = sheet.cell(row=row, column=colPostTags).value\n                func = sheet.cell(row=row, column=colConnFunc).value\n                syn = sheet.cell(row=row, column=colSyn).value\n                prob = sheet.cell(row=row, column=colProb).value\n                weight = sheet.cell(row=row, column=colWeight).value\n\n                # write preTags\n                line = \"netParams['connParams'].append({'preConds': {\"\n                for i,cond in enumerate(pre.split(';')):  # split into different conditions\n                    if i>0: line = line + \", \"\n                    cond2 = cond.split('=')  # split into key and value\n                    line = line + \"'\" + cond2[0].replace(' ','') + \"': \" + cond2[1].replace(' ','')   # generate line\n                line = line + \"}\" # end of preTags      \n\n                # write postTags\n                line = line + \",\\n'postConds': {\"\n                for i,cond in enumerate(post.split(';')):  # split into different conditions\n                    if i>0: line = line + \", \"\n                    cond2 = cond.split('=')  # split into key and value\n                    line = line + \"'\" + cond2[0].replace(' ','') + \"': \" + cond2[1].replace(' ','')   # generate line\n                line = line + \"}\" # end of postTags         \n                line = line + \",\\n'connFunc': '\" + func + \"'\"  # write connFunc\n                line = line + \",\\n'synMech': '\" + syn + \"'\"  # write synReceptor\n                line = line + \",\\n'probability': \" + str(prob)  # write prob\n                line = line + \",\\n'weight': \" + str(weight)  # write prob\n                line = line + \"})\"  # add closing brackets\n                line = line + '\\n\\n' # new line after each conn rule\n                f.write(line)", "response": "Imports connectivity rules from Excel sheet"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, searched_resource, uri_parameters=None, request_body_dict=None, query_parameters_dict=None,\n               additional_headers=None):\n        \"\"\"\n        This method is used to create a resource using the POST HTTP Method\n        :param searched_resource: A valid display name in the RAML file matching the resource\n        :param uri_parameters: A dictionary with the URI Parameters expected by the resource\n        :param request_body_dict: A dictionary containing the body parameter in the format\n               {'baseObject': {nested parameters}}. You can use extract_resource_body_schema to create it\n        :param query_parameters_dict: A dictionary containing optional or mandatory query parameters\n        :param additional_headers: a dictionary of additional Headers to send in your request, e.g. if-match used\n               with the dfw calls\n        :return: This method returns a dictionary containing the received header and body data\n        NOTE: The _resource_url and _request_body are constructed and passed by the decorator function\n        \"\"\"\n        return self._request(searched_resource, 'post', uri_parameters, request_body_dict, query_parameters_dict,\n                             additional_headers)", "response": "This method is used to create a resource in the RAML file matching the searched_resource."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, searched_resource, uri_parameters=None, request_body_dict=None, query_parameters_dict=None,\n               additional_headers=None):\n        \"\"\"\n        This method is used to delete a resource using the DELETE HTTP Method\n        :param searched_resource: A valid display name in the RAML file matching the resource\n        :param uri_parameters: A dictionary with the URI Parameters expected by the resource\n        :param request_body_dict: A dictionary containing the body parameter in the format\n               {'baseObject': {nested parameters}}. You can use extract_resource_body_schema to create it\n        :param query_parameters_dict: A dictionary containing optional or mandatory query parameters\n        :param additional_headers: a dictionary of additional Headers to send in your request, e.g. if-match used\n               with the dfw calls\n        :return: This method returns a dictionary containing the received header and body data\n        NOTE: The _resource_url and _request_body are constructed and passed by the decorator function\n        \"\"\"\n        return self._request(searched_resource, 'delete', uri_parameters, request_body_dict, query_parameters_dict,\n                             additional_headers)", "response": "This method is used to delete a resource in the RAML file matching the searched_resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_request(self, method, url, data=None, headers=None, params=None):\n\n        response_content = None\n        if data:\n            if headers:\n                headers.update({'Content-Type': 'application/xml'})\n            else:\n                headers = {'Content-Type': 'application/xml'}\n\n            if self._debug:\n                print md.parseString(data).toprettyxml()\n\n        response = self._session.request(method, url, headers=headers, params=params, data=data)\n\n        if 'content-type' in response.headers:\n            if response.headers['content-type'].find('application/xml') != -1:\n                response_content = xmloperations.xml_to_dict(et.fromstring(response.content))\n            elif response.headers['content-type'].find('application/json') != -1:\n                response_content = json.loads(response.content)\n            else:\n                response_content = response.content\n\n        response_odict = OrderedDict([('status', response.status_code), ('body', response_content),\n                                      ('location', None), ('objectId', None), ('Etag', None)])\n\n        if 'location' in response.headers:\n            response_odict['location'] = response.headers['location']\n            response_odict['objectId'] = response.headers['location'].split('/')[-1]\n\n        if 'Etag' in response.headers:\n            response_odict['Etag'] = response.headers['Etag']\n\n        if response.status_code not in [200, 201, 202, 204]:\n            if self.fail_mode == 'exit':\n                sys.exit('receive bad status code {}\\n{}'.format(response.status_code, response_content))\n            elif self.fail_mode == 'raise':\n                raise NsxError(response.status_code, response_content)\n            elif self.fail_mode == 'continue':\n                pass\n\n        return response_odict", "response": "Handle HTTP requests and responses for the specified API endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(data, **kwds):\n    if _usedefaultyamlloader:\n        return yaml.safe_dump(data, **kwds)\n    else:\n        return odyldo.safe_dump(data, **kwds)", "response": "dump the data as YAML"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting the http request to the service to the export service", "response": "def execute(self):\n        \"\"\"\n        Execute the http request to the export service\n        :return ads-classic formatted export string\n        \"\"\"\n        url = os.path.join(self.HTTP_ENDPOINT, self.format)\n        self.response = ExportResponse.load_http_response(\n            self.session.post(url, data=self.json_payload)\n        )\n        return self.response.result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_field(self, field):\n        if not hasattr(self, \"id\") or self.id is None:\n            raise APIResponseError(\"Cannot query an article without an id\")\n        sq = next(SearchQuery(q=\"id:{}\".format(self.id), fl=field))\n        # If the requested field is not present in the returning Solr doc,\n        # return None instead of hitting _get_field again.\n        if field not in sq._raw:\n            # These fields will never be in the result solr document;\n            # pass through to __getattribute__ to allow the relevant\n            # secondary service queries\n            if field in [\"reference\", \"citation\", \"metrics\", \"bibtex\"]:\n                pass\n            else:\n                return None\n        value = sq.__getattribute__(field)\n        self._raw[field] = value\n        return value", "response": "Queries the api for a single field for the article by id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a BiBTeX entry for the current article.", "response": "def bibtex(self):\n        \"\"\"Return a BiBTeX entry for the current article.\"\"\"\n        warnings.warn(\"bibtex should be queried with ads.ExportQuery(); You will \"\n                      \"hit API ratelimits very quickly otherwise.\", UserWarning)\n        return ExportQuery(bibcodes=self.bibcode, format=\"bibtex\").execute()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of articles", "response": "def articles(self):\n        \"\"\"\n        articles getter\n        \"\"\"\n        if self._articles is None:\n            self._articles = []\n            for doc in self.docs:\n                # ensure all fields in the \"fl\" are in the doc to address\n                # issue #38\n                for k in set(self.fl).difference(doc.keys()):\n                    doc[k] = None\n                self._articles.append(Article(**doc))\n        return self._articles"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef progress(self):\n        if self.response is None:\n            return \"Query has not been executed\"\n        return \"{}/{}\".format(len(self.articles), self.response.numFound)", "response": "Returns a string representation of the search progress"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends the HTTP request to the server and returns the result.", "response": "def execute(self):\n        \"\"\"\n        Sends the http request implied by the self.query\n        In addition, set up the request such that we can call next()\n        to provide the next page of results\n        \"\"\"\n        self.response = SolrResponse.load_http_response(\n            self.session.get(self.HTTP_ENDPOINT, params=self.query)\n        )\n\n        # ADS will apply a ceiling to 'rows' and re-write the query\n        # This code checks if that happened by comparing the reponse\n        # \"rows\" with what we sent in our query\n        # references https://github.com/andycasey/ads/issues/45\n        recv_rows = int(self.response.responseHeader.get(\"params\", {}).get(\"rows\"))\n        if recv_rows != self.query.get(\"rows\"):\n            self._query['rows'] = recv_rows\n            warnings.warn(\"Response rows did not match input rows. \"\n                          \"Setting this query's rows to {}\".format(self.query['rows']))\n\n        self._articles.extend(self.response.articles)\n        if self._query.get('start') is not None:\n            self._query['start'] += self._query['rows']\n        elif self._query.get('cursorMark') is not None:\n            self._query['cursorMark'] = self.response.json.get(\"nextCursorMark\")\n\n        self._highlights.update(self.response.json.get(\"highlighting\", {}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_pdf(article, debug=False):\n\n    print('Retrieving {0}'.format(article))\n\n    identifier = [_ for _ in article.identifier if 'arXiv' in _]\n    if identifier:\n        url = 'http://arXiv.org/pdf/{0}.{1}'.format(identifier[0][9:13],\n            ''.join(_ for _ in identifier[0][14:] if _.isdigit()))\n    else:\n        # No arXiv version. Ask ADS to redirect us to the journal article.\n        params = {\n            'bibcode': article.bibcode,\n            'link_type': 'ARTICLE',\n            'db_key': 'AST'\n        }\n        url = requests.get('http://adsabs.harvard.edu/cgi-bin/nph-data_query', \n            params=params).url\n\n    q = requests.get(url)\n    if not q.ok:\n        print('Error retrieving {0}: {1} for {2}'.format(\n            article, q.status_code, url))\n        if debug: q.raise_for_status()\n        else: return None\n\n    # Check if the journal has given back forbidden HTML.\n    if q.content.endswith('</html>'):\n        print('Error retrieving {0}: 200 (access denied?) for {1}'.format(\n            article, url))\n        return None\n    return q.content", "response": "Download an article PDF from arXiv and return the binary content."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollate the first page from each of the PDFs provided into a single PDF.", "response": "def summarise_pdfs(pdfs):\n    \"\"\"\n    Collate the first page from each of the PDFs provided into a single PDF.\n\n    :param pdfs:\n        The contents of several PDF files.\n\n    :type pdfs:\n        list of str\n\n    :returns:\n        The contents of single PDF, which can be written directly to disk.\n    \"\"\"\n\n    # Ignore None.\n    print('Summarising {0} articles ({1} had errors)'.format(\n        len(pdfs), pdfs.count(None)))\n    pdfs = [_ for _ in pdfs if _ is not None]\n\n    summary = PdfFileWriter()\n    for pdf in pdfs:\n        summary.addPage(PdfFileReader(StringIO(pdf)).getPage(0))\n    return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self):\n        self.response = MetricsResponse.load_http_response(\n            self.session.post(self.HTTP_ENDPOINT, data=self.json_payload)\n        )\n        return self.response.metrics", "response": "Execute the http request to the metrics service"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints all of the instantiated Singletons", "response": "def get_info(cls):\n        \"\"\"\n        Print all of the instantiated Singletons\n        \"\"\"\n        return '\\n'.join(\n            [str(cls._instances[key]) for key in cls._instances]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_http_response(cls, http_response):\n        if not http_response.ok:\n            raise APIResponseError(http_response.text)\n        c = cls(http_response)\n        c.response = http_response\n\n        RateLimits.getRateLimits(cls.__name__).set(c.response.headers)\n\n        return c", "response": "This method should return an instantiated class and set its response\n        to the requests. Response object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef token(self):\n        if self._token is None:\n            for v in map(os.environ.get, TOKEN_ENVIRON_VARS):\n                if v is not None:\n                    self._token = v\n                    return self._token\n            for f in TOKEN_FILES:\n                try:\n                    with open(f) as fp:\n                        self._token = fp.read().strip()\n                        return self._token\n                except IOError:\n                    pass\n            if ads.config.token is not None:\n                self._token = ads.config.token\n                return self._token\n            warnings.warn(\"No token found\", RuntimeWarning)\n        return self._token", "response": "set the instance attribute token"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef session(self):\n        if self._session is None:\n            self._session = requests.session()\n            self._session.headers.update(\n                {\n                    \"Authorization\": \"Bearer {}\".format(self.token),\n                    \"User-Agent\": \"ads-api-client/{}\".format(__version__),\n                    \"Content-Type\": \"application/json\",\n                }\n            )\n        return self._session", "response": "http session interface transparent proxy to requests. session\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ml(line, cell=None):\n  parser = google.datalab.utils.commands.CommandParser(\n      prog='%ml',\n      description=textwrap.dedent(\"\"\"\\\n          Execute MLWorkbench operations\n\n          Use \"%ml <command> -h\" for help on a specific command.\n      \"\"\"))\n\n  dataset_parser = parser.subcommand(\n      'dataset',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Create or explore datasets.')\n  dataset_sub_commands = dataset_parser.add_subparsers(dest='command')\n  dataset_create_parser = dataset_sub_commands.add_parser(\n      'create', help='Create datasets', formatter_class=argparse.RawTextHelpFormatter,\n      epilog=textwrap.dedent(\"\"\"\\\n          Example usage:\n\n          %%ml dataset\n          name: mydata\n          format: csv\n          train: path/to/train.csv\n          eval: path/to/eval.csv\n          schema:\n            - name: news_label\n              type: STRING\n            - name: text\n              type: STRING\"\"\"))\n\n  dataset_create_parser.add_argument('--name', required=True,\n                                     help='the name of the dataset to define. ')\n  dataset_create_parser.add_argument('--format', required=True,\n                                     choices=['csv', 'bigquery', 'transformed'],\n                                     help='The format of the data.')\n  dataset_create_parser.add_argument('--train', required=True,\n                                     help='The path of the training file pattern if format ' +\n                                          'is csv or transformed, or table name if format ' +\n                                          'is bigquery.')\n  dataset_create_parser.add_argument('--eval', required=True,\n                                     help='The path of the eval file pattern if format ' +\n                                          'is csv or transformed, or table name if format ' +\n                                          'is bigquery.')\n  dataset_create_parser.add_cell_argument('schema',\n                                          help='yaml representation of CSV schema, or path to ' +\n                                          'schema file. Only needed if format is csv.')\n  dataset_create_parser.set_defaults(func=_dataset_create)\n\n  dataset_explore_parser = dataset_sub_commands.add_parser(\n      'explore', help='Explore training data.')\n  dataset_explore_parser.add_argument('--name', required=True,\n                                      help='The name of the dataset to explore.')\n\n  dataset_explore_parser.add_argument('--overview', action='store_true', default=False,\n                                      help='Plot overview of sampled data. Set \"sample_size\" ' +\n                                           'to change the default sample size.')\n  dataset_explore_parser.add_argument('--facets', action='store_true', default=False,\n                                      help='Plot facets view of sampled data. Set ' +\n                                           '\"sample_size\" to change the default sample size.')\n  dataset_explore_parser.add_argument('--sample_size', type=int, default=1000,\n                                      help='sample size for overview or facets view. Only ' +\n                                           'used if either --overview or --facets is set.')\n  dataset_explore_parser.set_defaults(func=_dataset_explore)\n\n  analyze_parser = parser.subcommand(\n      'analyze',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Analyze training data and generate stats, such as min/max/mean '\n           'for numeric values, vocabulary for text columns.',\n      epilog=textwrap.dedent(\"\"\"\\\n          Example usage:\n\n          %%ml analyze [--cloud]\n          output: path/to/dir\n          data: $mydataset\n          features:\n            serialId:\n              transform: key\n            num1:\n              transform: scale\n              value: 1\n            num2:\n              transform: identity\n            text1:\n              transform: bag_of_words\n\n          Also supports in-notebook variables, such as:\n          %%ml analyze --output path/to/dir\n          training_data: $my_csv_dataset\n          features: $features_def\"\"\"))\n\n  analyze_parser.add_argument('--output', required=True,\n                              help='path of output directory.')\n  analyze_parser.add_argument('--cloud', action='store_true', default=False,\n                              help='whether to run analysis in cloud or local.')\n  analyze_parser.add_argument('--package', required=False,\n                              help='A local or GCS tarball path to use as the source. '\n                                   'If not set, the default source package will be used.')\n  analyze_parser.add_cell_argument(\n    'data',\n    required=True,\n    help=\"\"\"Training data. A dataset defined by \"%%ml dataset\".\"\"\")\n  analyze_parser.add_cell_argument(\n      'features',\n      required=True,\n      help=textwrap.dedent(\"\"\"\\\n          features config indicating how to transform data into features. The\n          list of supported transforms:\n              \"transform: identity\"\n                   does nothing (for numerical columns).\n              \"transform: scale\n               value: x\"\n                   scale a numerical column to [-a, a]. If value is missing, x\n                   defaults to 1.\n              \"transform: one_hot\"\n                   treats the string column as categorical and makes one-hot\n                   encoding of it.\n              \"transform: embedding\n               embedding_dim: d\"\n                   treats the string column as categorical and makes embeddings of\n                   it with specified dimension size.\n              \"transform: bag_of_words\"\n                   treats the string column as text and make bag of words\n                   transform of it.\n              \"transform: tfidf\"\n                   treats the string column as text and make TFIDF transform of it.\n              \"transform: image_to_vec\n               checkpoint: gs://b/o\"\n                   from image gs url to embeddings. \"checkpoint\" is a inception v3\n                   checkpoint. If absent, a default checkpoint is used.\n              \"transform: target\"\n                   denotes the column is the target. If the schema type of this\n                   column is string, a one_hot encoding is automatically applied.\n                   If numerical, an identity transform is automatically applied.\n              \"transform: key\"\n                   column contains metadata-like information and will be output\n                   as-is in prediction.\"\"\"))\n  analyze_parser.set_defaults(func=_analyze)\n\n  transform_parser = parser.subcommand(\n      'transform',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Transform the data into tf.example which is more efficient in training.',\n      epilog=textwrap.dedent(\"\"\"\\\n          Example usage:\n\n          %%ml transform [--cloud] [--shuffle]\n          analysis: path/to/analysis_output_folder\n          output: path/to/dir\n          batch_size: 100\n          data: $mydataset\n          cloud:\n            num_workers: 3\n            worker_machine_type: n1-standard-1\n            project_id: my_project_id\"\"\"))\n  transform_parser.add_argument('--analysis', required=True,\n                                help='path of analysis output directory.')\n  transform_parser.add_argument('--output', required=True,\n                                help='path of output directory.')\n  transform_parser.add_argument('--cloud', action='store_true', default=False,\n                                help='whether to run transform in cloud or local.')\n  transform_parser.add_argument('--shuffle', action='store_true', default=False,\n                                help='whether to shuffle the training data in output.')\n  transform_parser.add_argument('--batch_size', type=int, default=100,\n                                help='number of instances in a batch to process once. '\n                                     'Larger batch is more efficient but may consume more memory.')\n  transform_parser.add_argument('--package', required=False,\n                                help='A local or GCS tarball path to use as the source. '\n                                     'If not set, the default source package will be used.')\n  transform_parser.add_cell_argument(\n      'data',\n      required=True,\n      help=\"\"\"Training data. A dataset defined by \"%%ml dataset\".\"\"\")\n  transform_parser.add_cell_argument(\n      'cloud_config',\n      help=textwrap.dedent(\"\"\"\\\n          A dictionary of cloud config. All of them are optional.\n              num_workers: Dataflow number of workers. If not set, DataFlow\n                  service will determine the number.\n              worker_machine_type: a machine name from\n                  https://cloud.google.com/compute/docs/machine-types\n                  If not given, the service uses the default machine type.\n              project_id: id of the project to use for DataFlow service. If not set,\n                  Datalab's default project (set by %%datalab project set) is used.\n              job_name: Unique name for a Dataflow job to use. If not set, a\n                  random name will be used.\"\"\"))\n  transform_parser.set_defaults(func=_transform)\n\n  train_parser = parser.subcommand(\n      'train',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Train a model.',\n      epilog=textwrap.dedent(\"\"\"\\\n          Example usage:\n\n          %%ml train [--cloud]\n          analysis: path/to/analysis_output\n          output: path/to/dir\n          data: $mydataset\n          model_args:\n            model: linear_regression\n          cloud_config:\n            region: us-central1\"\"\"))\n  train_parser.add_argument('--analysis', required=True,\n                            help='path of analysis output directory.')\n  train_parser.add_argument('--output', required=True,\n                            help='path of trained model directory.')\n  train_parser.add_argument('--cloud', action='store_true', default=False,\n                            help='whether to run training in cloud or local.')\n  train_parser.add_argument('--notb', action='store_true', default=False,\n                            help='If set, tensorboard is not automatically started.')\n  train_parser.add_argument('--package', required=False,\n                            help='A local or GCS tarball path to use as the source. '\n                                 'If not set, the default source package will be used.')\n  train_parser.add_cell_argument(\n      'data',\n      required=True,\n      help=\"\"\"Training data. A dataset defined by \"%%ml dataset\".\"\"\")\n\n  package_model_help = subprocess.Popen(\n      ['python', '-m', 'trainer.task', '--datalab-help'],\n      cwd=DEFAULT_PACKAGE_PATH,\n      stdout=subprocess.PIPE).communicate()[0]\n  package_model_help = ('model_args: a dictionary of model specific args, including:\\n\\n' +\n                        package_model_help.decode())\n  train_parser.add_cell_argument('model_args', help=package_model_help)\n\n  train_parser.add_cell_argument(\n      'cloud_config',\n      help=textwrap.dedent(\"\"\"\\\n          A dictionary of cloud training config, including:\n              job_id: the name of the job. If not provided, a default job name is created.\n              region: see {url}\n              runtime_version: see \"region\". Must be a string like '1.2'.\n              scale_tier: see \"region\".\"\"\".format(\n          url='https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training')))\n  train_parser.set_defaults(func=_train)\n\n  predict_parser = parser.subcommand(\n      'predict',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Predict with local or deployed models. (Good for small datasets).',\n      epilog=textwrap.dedent(\"\"\"\\\n          Example usage:\n\n          %%ml predict\n          headers: key,num\n          model: path/to/model\n          data:\n            - key1,value1\n            - key2,value2\n\n          Or, in another cell, define a list of dict:\n\n          my_data = [{'key': 1, 'num': 1.2}, {'key': 2, 'num': 2.8}]\n\n          Then:\n\n          %%ml predict\n          headers: key,num\n          model: path/to/model\n          data: $my_data\"\"\"))\n  predict_parser.add_argument('--model', required=True,\n                              help='The model path.')\n  predict_parser.add_argument('--no_show_image', action='store_true', default=False,\n                              help='If not set, add a column of images in output.')\n  predict_parser.add_cell_argument(\n      'data',\n      required=True,\n      help=textwrap.dedent(\"\"\"\\\n          Prediction data can be\n              1) CSV lines in the input cell in yaml format or\n              2) a local variable which is one of\n                a) list of dict\n                b) list of strings of csv lines\n                c) a Pandas DataFrame\"\"\"))\n  predict_parser.set_defaults(func=_predict)\n\n  batch_predict_parser = parser.subcommand(\n      'batch_predict',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Batch prediction with local or deployed models. (Good for large datasets)',\n      epilog=textwrap.dedent(\"\"\"\\\n\n      Example usage:\n\n      %%ml batch_predict [--cloud]\n      model: path/to/model\n      output: path/to/output\n      format: csv\n      data:\n        csv: path/to/file_pattern\"\"\"))\n  batch_predict_parser.add_argument('--model', required=True,\n                                    help='The model path if not --cloud, or the id in '\n                                         'the form of model.version if --cloud.')\n  batch_predict_parser.add_argument('--output', required=True,\n                                    help='The path of output directory with prediction results. '\n                                         'If --cloud, it has to be GCS path.')\n  batch_predict_parser.add_argument('--format',\n                                    help='csv or json. For cloud run, '\n                                         'the only supported format is json.')\n  batch_predict_parser.add_argument('--batch_size', type=int, default=100,\n                                    help='number of instances in a batch to process once. '\n                                         'Larger batch is more efficient but may consume '\n                                         'more memory. Only used in local run.')\n  batch_predict_parser.add_argument('--cloud', action='store_true', default=False,\n                                    help='whether to run prediction in cloud or local.')\n  batch_predict_parser.add_cell_argument(\n      'data',\n      required=True,\n      help='Data to predict with. Only csv is supported.')\n  batch_predict_parser.add_cell_argument(\n      'cloud_config',\n      help=textwrap.dedent(\"\"\"\\\n          A dictionary of cloud batch prediction config.\n              job_id: the name of the job. If not provided, a default job name is created.\n              region: see {url}\n              max_worker_count: see reference in \"region\".\"\"\".format(\n                  url='https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/prediction')))  # noqa\n  batch_predict_parser.set_defaults(func=_batch_predict)\n\n  explain_parser = parser.subcommand(\n      'explain',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Explain a prediction with LIME tool.')\n  explain_parser.add_argument('--type', default='all', choices=['text', 'image', 'tabular', 'all'],\n                              help='the type of column to explain.')\n  explain_parser.add_argument('--algorithm', choices=['lime', 'ig'], default='lime',\n                              help='\"lime\" is the open sourced project for prediction explainer.' +\n                                   '\"ig\" means integrated gradients and currently only applies ' +\n                                   'to image.')\n  explain_parser.add_argument('--model', required=True,\n                              help='path of the model directory used for prediction.')\n  explain_parser.add_argument('--labels', required=True,\n                              help='comma separated labels to explain.')\n  explain_parser.add_argument('--column_name',\n                              help='the name of the column to explain. Optional if text type ' +\n                                   'and there is only one text column, or image type and ' +\n                                   'there is only one image column.')\n  explain_parser.add_cell_argument('data', required=True,\n                                   help='Prediction Data. Can be a csv line, or a dict.')\n  explain_parser.add_cell_argument('training_data',\n                                   help='A csv or bigquery dataset defined by %%ml dataset. ' +\n                                        'Used by tabular explainer only to determine the ' +\n                                        'distribution of numeric and categorical values. ' +\n                                        'Suggest using original training dataset.')\n\n  # options specific for lime\n  explain_parser.add_argument('--num_features', type=int,\n                              help='number of features to analyze. In text, it is number of ' +\n                                   'words. In image, it is number of areas. For lime only.')\n  explain_parser.add_argument('--num_samples', type=int,\n                              help='size of the neighborhood to learn the linear model. ' +\n                                   'For lime only.')\n  explain_parser.add_argument('--hide_color', type=int, default=0,\n                              help='the color to use for perturbed area. If -1, average of ' +\n                                   'each channel is used for each channel. For image only.')\n  explain_parser.add_argument('--include_negative', action='store_true', default=False,\n                              help='whether to show only positive areas. For lime image only.')\n  explain_parser.add_argument('--overview', action='store_true', default=False,\n                              help='whether to show overview instead of details view.' +\n                                   'For lime text and tabular only.')\n  explain_parser.add_argument('--batch_size', type=int, default=100,\n                              help='size of batches passed to prediction. For lime only.')\n\n  # options specific for integrated gradients\n  explain_parser.add_argument('--num_gradients', type=int, default=50,\n                              help='the number of scaled images to get gradients from. Larger ' +\n                                   'number usually produces better results but slower.')\n  explain_parser.add_argument('--percent_show', type=int, default=10,\n                              help='the percentage of top impactful pixels to show.')\n\n  explain_parser.set_defaults(func=_explain)\n\n  tensorboard_parser = parser.subcommand(\n      'tensorboard',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Start/stop/list TensorBoard instances.')\n  tensorboard_sub_commands = tensorboard_parser.add_subparsers(dest='command')\n\n  tensorboard_start_parser = tensorboard_sub_commands.add_parser(\n      'start', help='Start a tensorboard instance.')\n  tensorboard_start_parser.add_argument('--logdir', required=True,\n                                        help='The local or GCS logdir path.')\n  tensorboard_start_parser.set_defaults(func=_tensorboard_start)\n\n  tensorboard_stop_parser = tensorboard_sub_commands.add_parser(\n      'stop', help='Stop a tensorboard instance.')\n  tensorboard_stop_parser.add_argument('--pid', required=True, type=int,\n                                       help='The pid of the tensorboard instance.')\n  tensorboard_stop_parser.set_defaults(func=_tensorboard_stop)\n\n  tensorboard_list_parser = tensorboard_sub_commands.add_parser(\n      'list', help='List tensorboard instances.')\n  tensorboard_list_parser.set_defaults(func=_tensorboard_list)\n\n  evaluate_parser = parser.subcommand(\n      'evaluate',\n      formatter_class=argparse.RawTextHelpFormatter,\n      help='Analyze model evaluation results, such as confusion matrix, ROC, RMSE.')\n  evaluate_sub_commands = evaluate_parser.add_subparsers(dest='command')\n\n  def _add_data_params_for_evaluate(parser):\n    parser.add_argument('--csv', help='csv file path patterns.')\n    parser.add_argument('--headers',\n                        help='csv file headers. Required if csv is specified and ' +\n                             'predict_results_schema.json does not exist in the same directory.')\n    parser.add_argument('--bigquery',\n                        help='can be bigquery table, query as a string, or ' +\n                             'a pre-defined query (%%bq query --name).')\n\n  evaluate_cm_parser = evaluate_sub_commands.add_parser(\n      'confusion_matrix', help='Get confusion matrix from evaluation results.')\n  _add_data_params_for_evaluate(evaluate_cm_parser)\n  evaluate_cm_parser.add_argument('--plot', action='store_true', default=False,\n                                  help='Whether to plot confusion matrix as graph.')\n  evaluate_cm_parser.add_argument('--size', type=int, default=10,\n                                  help='The size of the confusion matrix.')\n  evaluate_cm_parser.set_defaults(func=_evaluate_cm)\n\n  evaluate_accuracy_parser = evaluate_sub_commands.add_parser(\n      'accuracy', help='Get accuracy results from classification evaluation results.')\n  _add_data_params_for_evaluate(evaluate_accuracy_parser)\n  evaluate_accuracy_parser.set_defaults(func=_evaluate_accuracy)\n\n  evaluate_pr_parser = evaluate_sub_commands.add_parser(\n      'precision_recall', help='Get precision recall metrics from evaluation results.')\n  _add_data_params_for_evaluate(evaluate_pr_parser)\n  evaluate_pr_parser.add_argument('--plot', action='store_true', default=False,\n                                  help='Whether to plot precision recall as graph.')\n  evaluate_pr_parser.add_argument('--num_thresholds', type=int, default=20,\n                                  help='Number of thresholds which determines how many ' +\n                                       'points in the graph.')\n  evaluate_pr_parser.add_argument('--target_class', required=True,\n                                  help='The target class to determine correctness of ' +\n                                       'a prediction.')\n  evaluate_pr_parser.add_argument('--probability_column',\n                                  help='The name of the column holding the probability ' +\n                                       'value of the target class. If absent, the value ' +\n                                       'of target class is used.')\n  evaluate_pr_parser.set_defaults(func=_evaluate_pr)\n\n  evaluate_roc_parser = evaluate_sub_commands.add_parser(\n      'roc', help='Get ROC metrics from evaluation results.')\n  _add_data_params_for_evaluate(evaluate_roc_parser)\n  evaluate_roc_parser.add_argument('--plot', action='store_true', default=False,\n                                   help='Whether to plot ROC as graph.')\n  evaluate_roc_parser.add_argument('--num_thresholds', type=int, default=20,\n                                   help='Number of thresholds which determines how many ' +\n                                        'points in the graph.')\n  evaluate_roc_parser.add_argument('--target_class', required=True,\n                                   help='The target class to determine correctness of ' +\n                                        'a prediction.')\n  evaluate_roc_parser.add_argument('--probability_column',\n                                   help='The name of the column holding the probability ' +\n                                        'value of the target class. If absent, the value ' +\n                                        'of target class is used.')\n  evaluate_roc_parser.set_defaults(func=_evaluate_roc)\n\n  evaluate_regression_parser = evaluate_sub_commands.add_parser(\n      'regression', help='Get regression metrics from evaluation results.')\n  _add_data_params_for_evaluate(evaluate_regression_parser)\n  evaluate_regression_parser.set_defaults(func=_evaluate_regression)\n\n  model_parser = parser.subcommand(\n      'model',\n      help='Models and versions management such as deployment, deletion, listing.')\n  model_sub_commands = model_parser.add_subparsers(dest='command')\n  model_list_parser = model_sub_commands.add_parser(\n      'list', help='List models and versions.')\n  model_list_parser.add_argument('--name',\n                                 help='If absent, list all models of specified or current ' +\n                                      'project. If provided, list all versions of the ' +\n                                      'model.')\n  model_list_parser.add_argument('--project',\n                                 help='The project to list model(s) or version(s). If absent, ' +\n                                      'use Datalab\\'s default project.')\n  model_list_parser.set_defaults(func=_model_list)\n\n  model_delete_parser = model_sub_commands.add_parser(\n      'delete', help='Delete models or versions.')\n  model_delete_parser.add_argument('--name', required=True,\n                                   help='If no \".\" in the name, try deleting the specified ' +\n                                        'model. If \"model.version\" is provided, try deleting ' +\n                                        'the specified version.')\n  model_delete_parser.add_argument('--project',\n                                   help='The project to delete model or version. If absent, ' +\n                                        'use Datalab\\'s default project.')\n  model_delete_parser.set_defaults(func=_model_delete)\n\n  model_deploy_parser = model_sub_commands.add_parser(\n      'deploy', help='Deploy a model version.')\n  model_deploy_parser.add_argument('--name', required=True,\n                                   help='Must be model.version to indicate the model ' +\n                                        'and version name to deploy.')\n  model_deploy_parser.add_argument('--path', required=True,\n                                   help='The GCS path of the model to be deployed.')\n  model_deploy_parser.add_argument('--runtime_version',\n                                   help='The TensorFlow version to use for this model. ' +\n                                        'For example, \"1.2.1\". If absent, the current ' +\n                                        'TensorFlow version installed in Datalab will be used.')\n  model_deploy_parser.add_argument('--project',\n                                   help='The project to deploy a model version. If absent, ' +\n                                        'use Datalab\\'s default project.')\n  model_deploy_parser.set_defaults(func=_model_deploy)\n\n  return google.datalab.utils.commands.handle_magic_line(line, cell, parser)", "response": "Implements the datalab cell magic for MLWorkbench operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_csv(input_csv_pattern, headers=None, schema_file=None):\n\n    if headers is not None:\n      names = headers\n    elif schema_file is not None:\n      with _util.open_local_or_gcs(schema_file, mode='r') as f:\n        schema = json.load(f)\n      names = [x['name'] for x in schema]\n    else:\n      raise ValueError('Either headers or schema_file is needed')\n\n    metrics = Metrics(input_csv_pattern=input_csv_pattern, headers=names)\n    return metrics", "response": "Create a Metrics instance from a CSV file pattern."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_bigquery(sql):\n\n    if isinstance(sql, bq.Query):\n      sql = sql._expanded_sql()\n\n    parts = sql.split('.')\n    if len(parts) == 1 or len(parts) > 3 or any(' ' in x for x in parts):\n      sql = '(' + sql + ')'  # query, not a table name\n    else:\n      sql = '`' + sql + '`'  # table name\n\n    metrics = Metrics(bigquery=sql)\n    return metrics", "response": "Create a Metrics instance from a BigQuery table or query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_data_from_csv_files(self):\n\n    all_df = []\n    for file_name in self._input_csv_files:\n      with _util.open_local_or_gcs(file_name, mode='r') as f:\n        all_df.append(pd.read_csv(f, names=self._headers))\n    df = pd.concat(all_df, ignore_index=True)\n    return df", "response": "Get data from input csv files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_data_from_bigquery(self, queries):\n\n    all_df = []\n    for query in queries:\n      all_df.append(query.execute().result().to_dataframe())\n    df = pd.concat(all_df, ignore_index=True)\n    return df", "response": "Get data from bigquery table or query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a DataFrame with two columns class and accuracy.", "response": "def accuracy(self):\n    \"\"\"Get accuracy numbers for each target and overall.\n\n    Returns:\n      A DataFrame with two columns: 'class' and 'accuracy'. It also contains the overall\n      accuracy with class being '_all'.\n\n    Raises:\n      Exception if the CSV headers do not include 'target' or 'predicted', or BigQuery\n      does not return 'target' or 'predicted' column.\n    \"\"\"\n\n    if self._input_csv_files:\n      df = self._get_data_from_csv_files()\n      if 'target' not in df or 'predicted' not in df:\n        raise ValueError('Cannot find \"target\" or \"predicted\" column')\n\n      labels = sorted(set(df['target']) | set(df['predicted']))\n      accuracy_results = []\n\n      for label in labels:\n        correct_count = len(df[(df['target'] == df['predicted']) & (df['target'] == label)])\n        total_count = len(df[(df['target'] == label)])\n        accuracy_results.append({\n            'target': label,\n            'accuracy': float(correct_count) / total_count if total_count > 0 else 0,\n            'count': total_count\n        })\n\n      total_correct_count = len(df[(df['target'] == df['predicted'])])\n      if len(df) > 0:\n        total_accuracy = float(total_correct_count) / len(df)\n        accuracy_results.append({'target': '_all', 'accuracy': total_accuracy, 'count': len(df)})\n      return pd.DataFrame(accuracy_results)\n    elif self._bigquery:\n      query = bq.Query(\"\"\"\nSELECT\n  target,\n  SUM(CASE WHEN target=predicted THEN 1 ELSE 0 END)/COUNT(*) as accuracy,\n  COUNT(*) as count\nFROM\n  %s\nGROUP BY\n  target\"\"\" % self._bigquery)\n      query_all = bq.Query(\"\"\"\nSELECT\n  \"_all\" as target,\n  SUM(CASE WHEN target=predicted THEN 1 ELSE 0 END)/COUNT(*) as accuracy,\n  COUNT(*) as count\nFROM\n  %s\"\"\" % self._bigquery)\n\n      df = self._get_data_from_bigquery([query, query_all])\n      return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets true positive rate values from evaluation results.", "response": "def roc(self, num_thresholds, target_class, probability_column=None):\n    \"\"\"Get true positive rate, false positive rate values from evaluation results.\n\n    Args:\n      num_thresholds: an integer. Number of thresholds.\n      target_class: a string indciating the target class, i.e. \"daisy\" in flower classification.\n      probability_column: the name of the probability column. If None, defaults to\n          value of target_class.\n\n    Returns:\n      A DataFrame with columns: 'tpr', 'fpr', 'threshold' with number of rows\n      equal to num_thresholds.\n\n    Raises:\n      Exception if the CSV headers do not include 'target' or probability_column, or BigQuery\n      does not return 'target' or probability_column column.\n    \"\"\"\n\n    if not probability_column:\n      probability_column = target_class\n\n    thresholds = np.linspace(0, 1, num_thresholds + 1)\n    if self._input_csv_files:\n      df = self._get_data_from_csv_files()\n      if 'target' not in df or probability_column not in df:\n        raise ValueError('Cannot find \"target\" or \"%s\" column' % probability_column)\n\n      total_positive = sum(1 for x in df['target'] if x == target_class)\n      total_negative = len(df) - total_positive\n      true_positives, false_positives = [], []\n      for threshold in thresholds:\n        true_positive_count = len(df[(df[probability_column] > threshold) &\n                                  (df['target'] == target_class)])\n        false_positive_count = len(df[(df[probability_column] > threshold) &\n                                   (df['target'] != target_class)])\n        true_positives.append(true_positive_count)\n        false_positives.append(false_positive_count)\n\n      data = []\n      for tp, fp, t in zip(true_positives, false_positives, thresholds):\n        tpr = (float)(tp) / total_positive if total_positive > 0. else 0.\n        fpr = (float)(fp) / total_negative if total_negative > 0. else 0.\n        data.append({'tpr': tpr, 'fpr': fpr, 'threshold': t})\n      return pd.DataFrame(data)\n\n    elif self._bigquery:\n      true_positive_query = bq.Query(\"\"\"\n        SELECT\n          COUNT(*) as true_positive\n        FROM\n          %s\n        CROSS JOIN\n          (SELECT * FROM UNNEST ([%s]) as t)\n        WHERE\n          %s > t AND target = '%s'\n        GROUP BY t\n        ORDER BY t\n      \"\"\" % (self._bigquery, ','.join(map(str, thresholds)), probability_column, target_class))\n\n      false_positive_query = bq.Query(\"\"\"\n        SELECT\n          COUNT(*) as false_positive\n        FROM\n          %s\n        CROSS JOIN\n          (SELECT * FROM UNNEST ([%s]) as t)\n        WHERE\n          %s > t AND target != '%s'\n        GROUP BY t\n        ORDER BY t\n      \"\"\" % (self._bigquery, ','.join(map(str, thresholds)), probability_column, target_class))\n\n      total_positive_query = bq.Query(\"\"\"\n        SELECT\n          COUNT(*) as total_positive\n        FROM\n          %s\n        WHERE\n          target = '%s'\n      \"\"\" % (self._bigquery, target_class))\n\n      total_negative_query = bq.Query(\"\"\"\n        SELECT\n          COUNT(*) as total_negative\n        FROM\n          %s\n        WHERE\n          target != '%s'\n      \"\"\" % (self._bigquery, target_class))\n\n      true_positives = true_positive_query.execute().result()\n      false_positives = false_positive_query.execute().result()\n      total_positive = total_positive_query.execute().result()[0]['total_positive']\n      total_negative = total_negative_query.execute().result()[0]['total_negative']\n      data = []\n      for tp, fp, t in zip(true_positives, false_positives, thresholds):\n        tpr = (float)(tp['true_positive']) / total_positive if total_positive > 0. else 0.\n        fpr = (float)(fp['false_positive']) / total_negative if total_negative > 0. else 0.\n        data.append({'tpr': tpr, 'fpr': fpr, 'threshold': t})\n      data.append({'tpr': 0., 'fpr': 0., 'threshold': 1.0})\n      return pd.DataFrame(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef precision_recall(self, num_thresholds, target_class, probability_column=None):\n\n    if not probability_column:\n      probability_column = target_class\n\n    # threshold = 1.0 is excluded.\n    thresholds = np.linspace(0, 1, num_thresholds + 1)[0:-1]\n    if self._input_csv_files:\n      df = self._get_data_from_csv_files()\n      if 'target' not in df or probability_column not in df:\n        raise ValueError('Cannot find \"target\" or \"%s\" column' % probability_column)\n\n      total_target = sum(1 for x in df['target'] if x == target_class)\n      total_predicted = []\n      correct_predicted = []\n      for threshold in thresholds:\n        predicted_count = sum(1 for x in df[probability_column] if x > threshold)\n        total_predicted.append(predicted_count)\n        correct_count = len(df[(df[probability_column] > threshold) &\n                            (df['target'] == target_class)])\n        correct_predicted.append(correct_count)\n\n      data = []\n      for p, c, t in zip(total_predicted, correct_predicted, thresholds):\n        precision = (float)(c) / p if p > 0. else 0.\n        recall = (float)(c) / total_target if total_target > 0. else 0.\n        data.append({'precision': precision, 'recall': recall, 'threshold': t})\n      return pd.DataFrame(data)\n\n    elif self._bigquery:\n      total_predicted_query = bq.Query(\"\"\"\n        SELECT\n          COUNT(*) as total_predicted\n        FROM\n          %s\n        CROSS JOIN\n          (SELECT * FROM UNNEST ([%s]) as t)\n        WHERE\n          %s > t\n        GROUP BY t\n        ORDER BY t\n      \"\"\" % (self._bigquery, ','.join(map(str, thresholds)), probability_column))\n\n      correct_predicted_query = bq.Query(\"\"\"\n        SELECT\n          COUNT(*) as correct_predicted\n        FROM\n          %s\n        CROSS JOIN\n          (SELECT * FROM UNNEST ([%s]) as t)\n        WHERE\n          %s > t AND target='%s'\n        GROUP BY t\n        ORDER BY t\n      \"\"\" % (self._bigquery, ','.join(map(str, thresholds)), probability_column, target_class))\n\n      total_target_query = bq.Query(\"\"\"\n        SELECT\n          COUNT(*) as total_target\n        FROM\n          %s\n        WHERE\n          target='%s'\n      \"\"\" % (self._bigquery, target_class))\n\n      total_predicted = total_predicted_query.execute().result()\n      correct_predicted = correct_predicted_query.execute().result()\n      total_target = total_target_query.execute().result()[0]['total_target']\n      data = []\n      for p, c, t in zip(total_predicted, correct_predicted, thresholds):\n        precision = ((float)(c['correct_predicted']) / p['total_predicted']\n                     if p['total_predicted'] > 0. else 0.)\n        recall = (float)(c['correct_predicted']) / total_target if total_target > 0. else 0.\n        data.append({'precision': precision, 'recall': recall, 'threshold': t})\n      return pd.DataFrame(data)", "response": "Returns precision and recall values from evaluation results."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the RMSE for regression model evaluation results.", "response": "def rmse(self):\n    \"\"\"Get RMSE for regression model evaluation results.\n\n    Returns:\n      the RMSE float number.\n\n    Raises:\n      Exception if the CSV headers do not include 'target' or 'predicted', or BigQuery\n      does not return 'target' or 'predicted' column, or if target or predicted is not\n      number.\n    \"\"\"\n\n    if self._input_csv_files:\n      df = self._get_data_from_csv_files()\n      if 'target' not in df or 'predicted' not in df:\n        raise ValueError('Cannot find \"target\" or \"predicted\" column')\n\n      df = df[['target', 'predicted']].apply(pd.to_numeric)\n      # if df is empty or contains non-numeric, scikit learn will raise error.\n      mse = mean_squared_error(df['target'], df['predicted'])\n      return math.sqrt(mse)\n    elif self._bigquery:\n      query = bq.Query(\"\"\"\n        SELECT\n          SQRT(SUM(ABS(predicted-target) * ABS(predicted-target)) / COUNT(*)) as rmse\n        FROM\n          %s\"\"\" % self._bigquery)\n      df = self._get_data_from_bigquery([query])\n      if df.empty:\n        return None\n      return df['rmse'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget MAE for regression model evaluation results.", "response": "def mae(self):\n    \"\"\"Get MAE (Mean Absolute Error) for regression model evaluation results.\n\n    Returns:\n      the MAE float number.\n\n    Raises:\n      Exception if the CSV headers do not include 'target' or 'predicted', or BigQuery\n      does not return 'target' or 'predicted' column, or if target or predicted is not\n      number.\n    \"\"\"\n\n    if self._input_csv_files:\n      df = self._get_data_from_csv_files()\n      if 'target' not in df or 'predicted' not in df:\n        raise ValueError('Cannot find \"target\" or \"predicted\" column')\n\n      df = df[['target', 'predicted']].apply(pd.to_numeric)\n      mae = mean_absolute_error(df['target'], df['predicted'])\n      return mae\n    elif self._bigquery:\n      query = bq.Query(\"\"\"\n        SELECT\n          SUM(ABS(predicted-target)) / COUNT(*) as mae\n        FROM\n          %s\"\"\" % self._bigquery)\n      df = self._get_data_from_bigquery([query])\n      if df.empty:\n        return None\n      return df['mae'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget nearest percentile from regression model evaluation results.", "response": "def percentile_nearest(self, percentile):\n    \"\"\"Get nearest percentile from regression model evaluation results.\n\n    Args:\n      percentile: a 0~100 float number.\n\n    Returns:\n      the percentile float number.\n\n    Raises:\n      Exception if the CSV headers do not include 'target' or 'predicted', or BigQuery\n      does not return 'target' or 'predicted' column, or if target or predicted is not\n      number.\n    \"\"\"\n\n    if self._input_csv_files:\n      df = self._get_data_from_csv_files()\n      if 'target' not in df or 'predicted' not in df:\n        raise ValueError('Cannot find \"target\" or \"predicted\" column')\n\n      df = df[['target', 'predicted']].apply(pd.to_numeric)\n      abs_errors = np.array((df['target'] - df['predicted']).apply(abs))\n      return np.percentile(abs_errors, percentile, interpolation='nearest')\n    elif self._bigquery:\n      query = bq.Query(\"\"\"\n        SELECT\n          PERCENTILE_DISC(ABS(predicted-target), %f) OVER() AS percentile\n        FROM\n          %s\n        LIMIT 1\"\"\" % (float(percentile) / 100, self._bigquery))\n      df = self._get_data_from_bigquery([query])\n      if df.empty:\n        return None\n      return df['percentile'][0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the expanded BigQuery SQL string of this UDF", "response": "def _expanded_sql(self):\n    \"\"\"Get the expanded BigQuery SQL string of this UDF\n\n    Returns\n      The expanded SQL string of this UDF\n    \"\"\"\n    if not self._sql:\n      self._sql = UDF._build_udf(self._name, self._code, self._return_type, self._params,\n                                 self._language, self._imports)\n    return self._sql"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the UDF part of a BigQuery query.", "response": "def _build_udf(name, code, return_type, params, language, imports):\n    \"\"\"Creates the UDF part of a BigQuery query using its pieces\n\n    Args:\n      name: the name of the javascript function\n      code: function body implementing the logic.\n      return_type: BigQuery data type of the function return. See supported data types in\n        the BigQuery docs\n      params: dictionary of parameter names and types\n      language: see list of supported languages in the BigQuery docs\n      imports: a list of GCS paths containing further support code.\n      \"\"\"\n\n    params = ','.join(['%s %s' % named_param for named_param in params])\n    imports = ','.join(['library=\"%s\"' % i for i in imports])\n\n    if language.lower() == 'sql':\n        udf = 'CREATE TEMPORARY FUNCTION {name} ({params})\\n' + \\\n              'RETURNS {return_type}\\n' + \\\n              'AS (\\n' + \\\n              '{code}\\n' + \\\n              ');'\n    else:\n        udf = 'CREATE TEMPORARY FUNCTION {name} ({params})\\n' +\\\n              'RETURNS {return_type}\\n' + \\\n              'LANGUAGE {language}\\n' + \\\n              'AS \"\"\"\\n' +\\\n              '{code}\\n' +\\\n              '\"\"\"\\n' +\\\n              'OPTIONS (\\n' +\\\n              '{imports}\\n' +\\\n              ');'\n    return udf.format(name=name, params=params, return_type=return_type,\n                      language=language, code=code, imports=imports)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a GCS URL into the bucket and object names.", "response": "def parse_name(name):\n  \"\"\" Parse a gs:// URL into the bucket and object names.\n\n  Args:\n    name: a GCS URL of the form gs://bucket or gs://bucket/object\n  Returns:\n    The bucket name (with no gs:// prefix), and the object name if present. If the name\n    could not be parsed returns None for both.\n  \"\"\"\n  bucket = None\n  obj = None\n  m = re.match(_STORAGE_NAME, name)\n  if m:\n    # We want to return the last two groups as first group is the optional 'gs://'\n    bucket = m.group(1)\n    obj = m.group(2)\n    if obj is not None:\n      obj = obj[1:]  # Strip '/'\n  else:\n    m = re.match('(' + _OBJECT_NAME + ')', name)\n    if m:\n      obj = m.group(1)\n  return bucket, obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef metadata(self):\n    if self._info is None:\n      try:\n        self._info = self._api.buckets_get(self._name)\n      except Exception as e:\n        raise e\n\n    return BucketMetadata(self._info) if self._info else None", "response": "Retrieves metadata about the bucket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve a Storage Object for the specified key in this bucket.", "response": "def object(self, key):\n    \"\"\"Retrieves a Storage Object for the specified key in this bucket.\n\n    The object need not exist.\n\n    Args:\n      key: the key of the object within the bucket.\n    Returns:\n      An Object instance representing the specified key.\n    \"\"\"\n    return _object.Object(self._name, key, context=self._context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting an iterator for the objects within this bucket.", "response": "def objects(self, prefix=None, delimiter=None):\n    \"\"\"Get an iterator for the objects within this bucket.\n\n    Args:\n      prefix: an optional prefix to match objects.\n      delimiter: an optional string to simulate directory-like semantics. The returned objects\n           will be those whose names do not contain the delimiter after the prefix. For\n           the remaining objects, the names will be returned truncated after the delimiter\n           with duplicates removed (i.e. as pseudo-directories).\n    Returns:\n      An iterable list of objects within this bucket.\n    \"\"\"\n    return _object.Objects(self._name, prefix, delimiter, context=self._context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self):\n    if self.exists():\n      try:\n        self._api.buckets_delete(self._name)\n      except Exception as e:\n        raise e", "response": "Deletes the bucket.\n\n    Raises:\n      Exception if there was an error deleting the bucket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains(self, name):\n    try:\n      self._api.buckets_get(name)\n    except google.datalab.utils.RequestException as e:\n      if e.status == 404:\n        return False\n      raise e\n    except Exception as e:\n      raise e\n    return True", "response": "Checks if the specified bucket exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a GCS URL into the bucket and item names.", "response": "def parse_name(name):\n  \"\"\" Parse a gs:// URL into the bucket and item names.\n\n  Args:\n    name: a GCS URL of the form gs://bucket or gs://bucket/item\n  Returns:\n    The bucket name (with no gs:// prefix), and the item name if present. If the name\n    could not be parsed returns None for both.\n  \"\"\"\n  bucket = None\n  item = None\n  m = re.match(_STORAGE_NAME, name)\n  if m:\n    # We want to return the last two groups as first group is the optional 'gs://'\n    bucket = m.group(1)\n    item = m.group(2)\n    if item is not None:\n      item = item[1:]  # Strip '/'\n  else:\n    m = re.match('(' + _OBJECT_NAME + ')', name)\n    if m:\n      item = m.group(1)\n  return bucket, item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef item(self, key):\n    return _item.Item(self._name, key, context=self._context)", "response": "Retrieves an Item object for the specified key in this bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an iterator for the items within this bucket.", "response": "def items(self, prefix=None, delimiter=None):\n    \"\"\"Get an iterator for the items within this bucket.\n\n    Args:\n      prefix: an optional prefix to match items.\n      delimiter: an optional string to simulate directory-like semantics. The returned items\n           will be those whose names do not contain the delimiter after the prefix. For\n           the remaining items, the names will be returned truncated after the delimiter\n           with duplicates removed (i.e. as pseudo-directories).\n    Returns:\n      An iterable list of items within this bucket.\n    \"\"\"\n    return _item.Items(self._name, prefix, delimiter, context=self._context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, project_id=None):\n    if not self.exists():\n      if project_id is None:\n        project_id = self._api.project_id\n      try:\n        self._info = self._api.buckets_insert(self._name, project_id=project_id)\n      except Exception as e:\n        raise e\n    return self", "response": "Creates the bucket.\n\n    Args:\n      project_id: the project in which to create the bucket.\n    Returns:\n      The bucket.\n    Raises:\n      Exception if there was an error creating the bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new bucket.", "response": "def create(self, name):\n    \"\"\"Creates a new bucket.\n\n    Args:\n      name: a unique name for the new bucket.\n    Returns:\n      The newly created bucket.\n    Raises:\n      Exception if there was an error creating the bucket.\n    \"\"\"\n    return Bucket(name, context=self._context).create(self._project_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef train(train_dataset,\n          eval_dataset,\n          analysis_dir,\n          output_dir,\n          features,\n          layer_sizes,\n          max_steps=5000,\n          num_epochs=None,\n          train_batch_size=100,\n          eval_batch_size=16,\n          min_eval_frequency=100,\n          learning_rate=0.01,\n          epsilon=0.0005,\n          job_name=None,\n          cloud=None,\n          ):\n  \"\"\"Blocking version of train_async. See documentation for train_async.\"\"\"\n  job = train_async(\n      train_dataset=train_dataset,\n      eval_dataset=eval_dataset,\n      analysis_dir=analysis_dir,\n      output_dir=output_dir,\n      features=features,\n      layer_sizes=layer_sizes,\n      max_steps=max_steps,\n      num_epochs=num_epochs,\n      train_batch_size=train_batch_size,\n      eval_batch_size=eval_batch_size,\n      min_eval_frequency=min_eval_frequency,\n      learning_rate=learning_rate,\n      epsilon=epsilon,\n      job_name=job_name,\n      cloud=cloud,\n  )\n  job.wait()\n  print('Training: ' + str(job.state))", "response": "Blocking version of train_async. See documentation for train_async."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrain the model locally or in the cloud.", "response": "def train_async(train_dataset,\n                eval_dataset,\n                analysis_dir,\n                output_dir,\n                features,\n                layer_sizes,\n                max_steps=5000,\n                num_epochs=None,\n                train_batch_size=100,\n                eval_batch_size=16,\n                min_eval_frequency=100,\n                learning_rate=0.01,\n                epsilon=0.0005,\n                job_name=None,\n                cloud=None,\n                ):\n  \"\"\"Train model locally or in the cloud.\n\n  Local Training:\n\n  Args:\n    train_dataset: CsvDataSet\n    eval_dataset: CsvDataSet\n    analysis_dir:  The output directory from local_analysis\n    output_dir:  Output directory of training.\n    features: file path or features object. Example:\n        {\n          \"col_A\": {\"transform\": \"scale\", \"default\": 0.0},\n          \"col_B\": {\"transform\": \"scale\",\"value\": 4},\n          # Note col_C is missing, so default transform used.\n          \"col_D\": {\"transform\": \"hash_one_hot\", \"hash_bucket_size\": 4},\n          \"col_target\": {\"transform\": \"target\"},\n          \"col_key\": {\"transform\": \"key\"}\n        }\n        The keys correspond to the columns in the input files as defined by the\n        schema file during preprocessing. Some notes\n        1) The \"key\" and \"target\" transforms are required.\n        2) Default values are optional. These are used if the input data has\n           missing values during training and prediction. If not supplied for a\n           column, the default value for a numerical column is that column's\n           mean vlaue, and for a categorical column the empty string is used.\n        3) For numerical colums, the following transforms are supported:\n           i) {\"transform\": \"identity\"}: does nothing to the number. (default)\n           ii) {\"transform\": \"scale\"}: scales the colum values to -1, 1.\n           iii) {\"transform\": \"scale\", \"value\": a}: scales the colum values\n              to -a, a.\n\n           For categorical colums, the following transforms are supported:\n          i) {\"transform\": \"one_hot\"}: A one-hot vector using the full\n              vocabulary is used. (default)\n          ii) {\"transform\": \"embedding\", \"embedding_dim\": d}: Each label is\n              embedded into an d-dimensional space.\n    max_steps: Int. Number of training steps to perform.\n    num_epochs: Maximum number of training data epochs on which to train.\n        The training job will run for max_steps or num_epochs, whichever occurs\n        first.\n    train_batch_size: number of rows to train on in one step.\n    eval_batch_size: number of rows to eval in one step. One pass of the eval\n        dataset is done. If eval_batch_size does not perfectly divide the numer\n        of eval instances, the last fractional batch is not used.\n    min_eval_frequency: Minimum number of training steps between evaluations.\n    layer_sizes: List. Represents the layers in the connected DNN.\n        If the model type is DNN, this must be set. Example [10, 3, 2], this\n        will create three DNN layers where the first layer will have 10 nodes,\n        the middle layer will have 3 nodes, and the laster layer will have 2\n        nodes.\n    learning_rate: tf.train.AdamOptimizer's learning rate,\n    epsilon: tf.train.AdamOptimizer's epsilon value.\n\n  Cloud Training:\n\n  All local training arguments are valid for cloud training. Cloud training\n  contains two additional args:\n\n  Args:\n    cloud: A CloudTrainingConfig object.\n    job_name: Training job name. A default will be picked if None.\n\n  Returns:\n    Datalab job\n  \"\"\"\n  return core_train(\n      train_dataset=train_dataset,\n      eval_dataset=eval_dataset,\n      analysis_dir=analysis_dir,\n      output_dir=output_dir,\n      features=features,\n      model_type='dnn_regression',\n      max_steps=max_steps,\n      num_epochs=num_epochs,\n      train_batch_size=train_batch_size,\n      eval_batch_size=eval_batch_size,\n      min_eval_frequency=min_eval_frequency,\n      top_n=None,\n      layer_sizes=layer_sizes,\n      learning_rate=learning_rate,\n      epsilon=epsilon,\n      job_name=job_name,\n      job_name_prefix='mltoolbox_regression_dnn',\n      cloud=cloud,\n  )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of resource descriptors that match the filters.", "response": "def list(self, pattern='*'):\n    \"\"\"Returns a list of resource descriptors that match the filters.\n\n    Args:\n      pattern: An optional pattern to further filter the descriptors. This can\n          include Unix shell-style wildcards. E.g. ``\"aws*\"``, ``\"*cluster*\"``.\n\n    Returns:\n      A list of ResourceDescriptor objects that match the filters.\n    \"\"\"\n    if self._descriptors is None:\n      self._descriptors = self._client.list_resource_descriptors(\n          filter_string=self._filter_string)\n    return [resource for resource in self._descriptors\n            if fnmatch.fnmatch(resource.type, pattern)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_dataframe(self, pattern='*', max_rows=None):\n    data = []\n    for i, resource in enumerate(self.list(pattern)):\n      if max_rows is not None and i >= max_rows:\n        break\n      labels = ', '. join([l.key for l in resource.labels])\n      data.append([resource.type, resource.display_name, labels])\n\n    return pandas.DataFrame(data, columns=self._DISPLAY_HEADERS)", "response": "Creates a pandas dataframe from the descriptors that match the filters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extension(line, cell=None):\n  parser = _commands.CommandParser(prog='%extension', description=\"\"\"\nLoad an extension into Datalab. Currently only mathjax is supported.\n\"\"\")\n  subparser = parser.subcommand('mathjax', 'Enabled MathJaX support in Datalab.')\n  subparser.set_defaults(ext='mathjax')\n  parser.set_defaults(func=_extension)\n  return _utils.handle_magic_line(line, cell, parser)", "response": "Load an extension into Datalab."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gcs(line, cell=None):\n  parser = google.datalab.utils.commands.CommandParser(prog='%gcs', description=\"\"\"\nExecute various Google Cloud Storage related operations. Use \"%gcs <command> -h\"\nfor help on a specific command.\n\"\"\")\n\n  # TODO(gram): consider adding a move command too. I did try this already using the\n  # objects.patch API to change the object name but that fails with an error:\n  #\n  # Value 'newname' in content does not agree with value 'oldname'. This can happen when a value\n  # set through a parameter is inconsistent with a value set in the request.\n  #\n  # This is despite 'name' being identified as writable in the storage API docs.\n  # The alternative would be to use a copy/delete.\n  copy_parser = parser.subcommand('copy', 'Copy one or more Google Cloud Storage objects to a '\n                                          'different location.')\n  copy_parser.add_argument('-s', '--source', help='The name of the object(s) to copy', nargs='+')\n  copy_parser.add_argument('-d', '--destination', required=True,\n                           help='The copy destination. For multiple source objects this must be a '\n                                'bucket.')\n  copy_parser.set_defaults(func=_gcs_copy)\n\n  create_parser = parser.subcommand('create', 'Create one or more Google Cloud Storage buckets.')\n  create_parser.add_argument('-p', '--project', help='The project associated with the objects')\n  create_parser.add_argument('-b', '--bucket', help='The name of the bucket(s) to create',\n                             nargs='+')\n  create_parser.set_defaults(func=_gcs_create)\n\n  delete_parser = parser.subcommand('delete', 'Delete one or more Google Cloud Storage buckets or '\n                                              'objects.')\n  delete_parser.add_argument('-b', '--bucket', nargs='*',\n                             help='The name of the bucket(s) to remove')\n  delete_parser.add_argument('-o', '--object', nargs='*',\n                             help='The name of the object(s) to remove')\n  delete_parser.set_defaults(func=_gcs_delete)\n\n  list_parser = parser.subcommand('list', 'List buckets in a project, or contents of a bucket.')\n  list_parser.add_argument('-p', '--project', help='The project associated with the objects')\n  list_parser.add_argument('-o', '--objects',\n                           help='List objects under the given Google Cloud Storage path',\n                           nargs='?')\n  list_parser.set_defaults(func=_gcs_list)\n\n  read_parser = parser.subcommand('read', 'Read the contents of a Google Cloud Storage object into '\n                                          'a Python variable.')\n  read_parser.add_argument('-o', '--object', help='The name of the object to read',\n                           required=True)\n  read_parser.add_argument('-v', '--variable', required=True,\n                           help='The name of the Python variable to set')\n  read_parser.set_defaults(func=_gcs_read)\n\n  view_parser = parser.subcommand('view', 'View the contents of a Google Cloud Storage object.')\n  view_parser.add_argument('-n', '--head', type=int, default=20,\n                           help='The number of initial lines to view')\n  view_parser.add_argument('-t', '--tail', type=int, default=20,\n                           help='The number of lines from end to view')\n  view_parser.add_argument('-o', '--object', help='The name of the object to view',\n                           required=True)\n  view_parser.set_defaults(func=_gcs_view)\n\n  write_parser = parser.subcommand('write', 'Write the value of a Python variable to a Google '\n                                            'Cloud Storage object.')\n  write_parser.add_argument('-v', '--variable', help='The name of the source Python variable',\n                            required=True)\n  write_parser.add_argument('-o', '--object', required=True,\n                            help='The name of the destination Google Cloud Storage object to write')\n  write_parser.add_argument('-c', '--content_type', help='MIME type', default='text/plain')\n  write_parser.set_defaults(func=_gcs_write)\n\n  return google.datalab.utils.commands.handle_magic_line(line, cell, parser)", "response": "Implements the gcs cell magic for ipython notebooks."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands a list of object names into a list of objects.", "response": "def _expand_list(names):\n  \"\"\" Do a wildchar name expansion of object names in a list and return expanded list.\n\n    The objects are expected to exist as this is used for copy sources or delete targets.\n    Currently we support wildchars in the key name only.\n  \"\"\"\n\n  if names is None:\n    names = []\n  elif isinstance(names, basestring):\n    names = [names]\n\n  results = []  # The expanded list.\n  objects = {}  # Cached contents of buckets; used for matching.\n  for name in names:\n    bucket, key = google.datalab.storage._bucket.parse_name(name)\n    results_len = len(results)  # If we fail to add any we add name and let caller deal with it.\n    if bucket:\n      if not key:\n        # Just a bucket; add it.\n        results.append('gs://%s' % bucket)\n      elif google.datalab.storage.Object(bucket, key).exists():\n        results.append('gs://%s/%s' % (bucket, key))\n      else:\n        # Expand possible key values.\n        if bucket not in objects and key[:1] == '*':\n          # We need the full list; cache a copy for efficiency.\n          objects[bucket] = [obj.metadata.name\n                             for obj in list(google.datalab.storage.Bucket(bucket).objects())]\n        # If we have a cached copy use it\n        if bucket in objects:\n          candidates = objects[bucket]\n        # else we have no cached copy but can use prefix matching which is more efficient than\n        # getting the full contents.\n        else:\n          # Get the non-wildchar prefix.\n          match = re.search('\\?|\\*|\\[', key)\n          prefix = key\n          if match:\n            prefix = key[0:match.start()]\n\n          candidates = [obj.metadata.name\n                        for obj in google.datalab.storage.Bucket(bucket).objects(prefix=prefix)]\n\n        for obj in candidates:\n          if fnmatch.fnmatch(obj, key):\n            results.append('gs://%s/%s' % (bucket, obj))\n\n    # If we added no matches, add the original name and let caller deal with it.\n    if len(results) == results_len:\n      results.append(name)\n\n  return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate one or more buckets.", "response": "def _gcs_create(args, _):\n  \"\"\" Create one or more buckets. \"\"\"\n  errs = []\n  for name in args['bucket']:\n    try:\n      bucket, key = google.datalab.storage._bucket.parse_name(name)\n      if bucket and not key:\n        google.datalab.storage.Bucket(bucket).create(_make_context(args['project']))\n      else:\n        raise Exception(\"Invalid bucket name %s\" % name)\n    except Exception as e:\n      errs.append(\"Couldn't create %s: %s\" %\n                  (name, _extract_gcs_api_response_error(str(e))))\n  if errs:\n    raise Exception('\\n'.join(errs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _gcs_delete(args, _):\n  objects = _expand_list(args['bucket'])\n  objects.extend(_expand_list(args['object']))\n  errs = []\n  for obj in objects:\n    try:\n      bucket, key = google.datalab.storage._bucket.parse_name(obj)\n      if bucket and key:\n        gcs_object = google.datalab.storage.Object(bucket, key)\n        if gcs_object.exists():\n          google.datalab.storage.Object(bucket, key).delete()\n        else:\n          errs.append(\"%s does not exist\" % obj)\n      elif bucket:\n        gcs_bucket = google.datalab.storage.Bucket(bucket)\n        if gcs_bucket.exists():\n          gcs_bucket.delete()\n        else:\n          errs.append(\"%s does not exist\" % obj)\n      else:\n        raise Exception(\"Can't delete object with invalid name %s\" % obj)\n    except Exception as e:\n      errs.append(\"Couldn't delete %s: %s\" %\n                  (obj, _extract_gcs_api_response_error(str(e))))\n  if errs:\n    raise Exception('\\n'.join(errs))", "response": "Internal function to delete one or more objects or buckets."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all Google Cloud Storage buckets that match a pattern.", "response": "def _gcs_list_buckets(project, pattern):\n  \"\"\" List all Google Cloud Storage buckets that match a pattern. \"\"\"\n  data = [{'Bucket': 'gs://' + bucket.name, 'Created': bucket.metadata.created_on}\n          for bucket in google.datalab.storage.Buckets(_make_context(project))\n          if fnmatch.fnmatch(bucket.name, pattern)]\n  return google.datalab.utils.commands.render_dictionary(data, ['Bucket', 'Created'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets names of all Google Cloud Storage keys in a specified bucket that match a pattern.", "response": "def _gcs_get_keys(bucket, pattern):\n  \"\"\" Get names of all Google Cloud Storage keys in a specified bucket that match a pattern. \"\"\"\n  return [obj for obj in list(bucket.objects()) if fnmatch.fnmatch(obj.metadata.name, pattern)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget names of all Google Cloud Storage keys that match a pattern.", "response": "def _gcs_get_key_names(bucket, pattern):\n  \"\"\" Get names of all Google Cloud Storage keys in a specified bucket that match a pattern. \"\"\"\n  return [obj.metadata.name for obj in _gcs_get_keys(bucket, pattern)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist all Google Cloud Storage keys in a specified bucket that match a pattern.", "response": "def _gcs_list_keys(bucket, pattern):\n  \"\"\" List all Google Cloud Storage keys in a specified bucket that match a pattern. \"\"\"\n  data = [{'Name': obj.metadata.name,\n           'Type': obj.metadata.content_type,\n           'Size': obj.metadata.size,\n           'Updated': obj.metadata.updated_on}\n          for obj in _gcs_get_keys(bucket, pattern)]\n  return google.datalab.utils.commands.render_dictionary(data, ['Name', 'Type', 'Size', 'Updated'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the contents of a bucket or a key.", "response": "def _gcs_list(args, _):\n  \"\"\" List the buckets or the contents of a bucket.\n\n  This command is a bit different in that we allow wildchars in the bucket name and will list\n  the buckets that match.\n  \"\"\"\n  target = args['objects']\n  project = args['project']\n  if target is None:\n    return _gcs_list_buckets(project, '*')  # List all buckets.\n\n  bucket_name, key = google.datalab.storage._bucket.parse_name(target)\n  if bucket_name is None:\n    raise Exception('Cannot list %s; not a valid bucket name' % target)\n\n  # If a target was specified, list keys inside it\n  if target:\n    if not re.search('\\?|\\*|\\[', target):\n      # If no wild characters are present in the key string, append a '/*' suffix to show all keys\n      key = key.strip('/') + '/*' if key else '*'\n\n    if project:\n      # Only list if the bucket is in the project\n      for bucket in google.datalab.storage.Buckets(_make_context(project)):\n        if bucket.name == bucket_name:\n          break\n      else:\n        raise Exception('%s does not exist in project %s' % (target, project))\n    else:\n      bucket = google.datalab.storage.Bucket(bucket_name)\n\n    if bucket.exists():\n      return _gcs_list_keys(bucket, key)\n    else:\n      raise Exception('Bucket %s does not exist' % target)\n\n  else:\n    # Treat the bucket name as a pattern and show matches. We don't use bucket_name as that\n    # can strip off wildchars and so we need to strip off gs:// here.\n    return _gcs_list_buckets(project, target.strip('/')[5:])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse command line arguments.", "response": "def parse_arguments(argv):\n  \"\"\"Parse command line arguments.\n  Args:\n    argv: list of command line arguments including program name.\n  Returns:\n    The parsed arguments as returned by argparse.ArgumentParser.\n  \"\"\"\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.RawDescriptionHelpFormatter,\n      description=textwrap.dedent(\"\"\"\\\n          Runs preprocessing on raw data for TensorFlow training.\n\n          This script applies some transformations to raw data to improve\n          training performance. Some data transformations can be expensive\n          such as the tf-idf text column transformation. During training, the\n          same raw data row might be used multiply times to train a model. This\n          means the same transformations are applied to the same data row\n          multiple times. This can be very inefficient, so this script applies\n          partial transformations to the raw data and writes an intermediate \n          preprocessed datasource to disk for training. \n\n          Running this transformation step is required for two usage paths:\n            1) If the img_url_to_vec transform is used. This is because\n               preprocessing as image is expensive and TensorFlow cannot easily\n               read raw image files during training.\n            2) If the raw data is in BigQuery. TensorFlow cannot read from a \n               BigQuery source.\n\n          Running this transformation step is recommended if a text transform is\n          used (like tf-idf or bag-of-words), and the text value for each row\n          is very long.\n\n          Running this transformation step may not have an interesting training\n          performance impact if the transforms are all simple like scaling\n          numerical values.\"\"\"))\n\n  source_group = parser.add_mutually_exclusive_group(required=True)\n\n  source_group.add_argument(\n      '--csv',\n      metavar='FILE',\n      required=False,\n      action='append',\n      help='CSV data to transform.')\n\n  source_group.add_argument(\n      '--bigquery',\n      metavar='PROJECT_ID.DATASET.TABLE_NAME',\n      type=str,\n      required=False,\n      help=('Must be in the form `project.dataset.table_name`. BigQuery '\n            'data to transform'))\n\n  parser.add_argument(\n      '--analysis',\n      metavar='ANALYSIS_OUTPUT_DIR',\n      required=True,\n      help='The output folder of analyze')\n\n  parser.add_argument(\n      '--prefix',\n      metavar='OUTPUT_FILENAME_PREFIX',\n      required=True,\n      type=str)\n\n  parser.add_argument(\n      '--output',\n      metavar='DIR',\n      default=None,\n      required=True,\n      help=('Google Cloud Storage or Local directory in which '\n            'to place outputs.'))\n\n  parser.add_argument(\n      '--shuffle',\n      action='store_true',\n      default=False,\n      help='If used, data source is shuffled. This is recommended for training data.')\n\n  parser.add_argument(\n      '--batch-size',\n      metavar='N',\n      type=int,\n      default=100,\n      help='Larger values increase performance and peak memory usage.')\n\n  cloud_group = parser.add_argument_group(\n      title='Cloud Parameters',\n      description='These parameters are only used if --cloud is used.')\n\n  cloud_group.add_argument(\n      '--cloud',\n      action='store_true',\n      help='Run preprocessing on the cloud.')\n\n  cloud_group.add_argument(\n      '--job-name',\n      type=str,\n      help='Unique dataflow job name.')\n\n  cloud_group.add_argument(\n      '--project-id',\n      help='The project to which the job will be submitted.')\n\n  cloud_group.add_argument(\n      '--num-workers',\n      metavar='N',\n      type=int,\n      default=0,\n      help='Set to 0 to use the default size determined by the Dataflow service.')\n\n  cloud_group.add_argument(\n      '--worker-machine-type',\n      metavar='NAME',\n      type=str,\n      help='A machine name from https://cloud.google.com/compute/docs/machine-types. '\n           ' If not given, the service uses the default machine type.')\n\n  cloud_group.add_argument(\n      '--async',\n      action='store_true',\n      help='If used, this script returns before the dataflow job is completed.')\n\n  args = parser.parse_args(args=argv[1:])\n\n  if args.cloud and not args.project_id:\n    raise ValueError('--project-id is needed for --cloud')\n\n  if args.async and not args.cloud:\n    raise ValueError('--async should only be used with --cloud')\n\n  if not args.job_name:\n    args.job_name = ('dataflow-job-{}'.format(\n        datetime.datetime.now().strftime('%Y%m%d%H%M%S')))\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef image_transform_columns(features):\n  import six\n  from trainer import feature_transforms\n\n  img_cols = []\n  for name, transform in six.iteritems(features):\n    if transform['transform'] == feature_transforms.IMAGE_TRANSFORM:\n      img_cols.append(name)\n\n  return img_cols", "response": "Returns a list of columns that prepare_image_transforms() should be run on."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_image_transforms(element, image_columns):\n  import base64\n  import cStringIO\n  from PIL import Image\n  from tensorflow.python.lib.io import file_io as tf_file_io\n  from apache_beam.metrics import Metrics\n\n  img_error_count = Metrics.counter('main', 'ImgErrorCount')\n  img_missing_count = Metrics.counter('main', 'ImgMissingCount')\n\n  for name in image_columns:\n    uri = element[name]\n    if not uri:\n      img_missing_count.inc()\n      continue\n    try:\n      with tf_file_io.FileIO(uri, 'r') as f:\n        img = Image.open(f).convert('RGB')\n\n    # A variety of different calling libraries throw different exceptions here.\n    # They all correspond to an unreadable file so we treat them equivalently.\n    # pylint: disable broad-except\n    except Exception as e:\n      logging.exception('Error processing image %s: %s', uri, str(e))\n      img_error_count.inc()\n      return\n\n    # Convert to desired format and output.\n    output = cStringIO.StringIO()\n    img.save(output, 'jpeg')\n    element[name] = base64.urlsafe_b64encode(output.getvalue())\n\n  return element", "response": "Prepare an image file path with its jpeg bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a csv line into a dict.", "response": "def decode_csv(csv_string, column_names):\n  \"\"\"Parse a csv line into a dict.\n\n  Args:\n    csv_string: a csv string. May contain missing values \"a,,c\"\n    column_names: list of column names\n\n  Returns:\n    Dict of {column_name, value_from_csv}. If there are missing values, \n    value_from_csv will be ''.\n  \"\"\"\n  import csv\n  r = next(csv.reader([csv_string]))\n  if len(r) != len(column_names):\n    raise ValueError('csv line %s does not have %d columns' % (csv_string, len(column_names)))\n  return {k: v for k, v in zip(column_names, r)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode_csv(data_dict, column_names):\n  import csv\n  import six\n  values = [str(data_dict[x]) for x in column_names]\n  str_buff = six.StringIO()\n  writer = csv.writer(str_buff, lineterminator='')\n  writer.writerow(values)\n  return str_buff.getvalue()", "response": "Builds a csv string version of data_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a serialized tf. example version of transformed_json_data.", "response": "def serialize_example(transformed_json_data, info_dict):\n  \"\"\"Makes a serialized tf.example.\n\n  Args:\n    transformed_json_data: dict of transformed data.\n    info_dict: output of feature_transforms.get_transfrormed_feature_info()\n\n  Returns:\n    The serialized tf.example version of transformed_json_data.\n  \"\"\"\n  import six\n  import tensorflow as tf\n\n  def _make_int64_list(x):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=x))\n  def _make_bytes_list(x):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=x))\n  def _make_float_list(x):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=x))\n\n  if sorted(six.iterkeys(transformed_json_data)) != sorted(six.iterkeys(info_dict)):\n    raise ValueError('Keys do not match %s, %s' % (list(six.iterkeys(transformed_json_data)),\n                     list(six.iterkeys(info_dict))))\n\n  ex_dict = {}\n  for name, info in six.iteritems(info_dict):\n    if info['dtype'] == tf.int64:\n      ex_dict[name] = _make_int64_list(transformed_json_data[name])\n    elif info['dtype'] == tf.float32:\n      ex_dict[name] = _make_float_list(transformed_json_data[name])\n    elif info['dtype'] == tf.string:\n      ex_dict[name] = _make_bytes_list(transformed_json_data[name])      \n    else:\n      raise ValueError('Unsupported data type %s' % info['dtype'])\n\n  ex = tf.train.Example(features=tf.train.Features(feature=ex_dict))\n  return ex.SerializeToString()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preprocess(pipeline, args):\n  from tensorflow.python.lib.io import file_io\n  from trainer import feature_transforms\n\n  schema = json.loads(file_io.read_file_to_string(\n      os.path.join(args.analysis, feature_transforms.SCHEMA_FILE)).decode())\n  features = json.loads(file_io.read_file_to_string(\n      os.path.join(args.analysis, feature_transforms.FEATURES_FILE)).decode())\n  stats = json.loads(file_io.read_file_to_string(\n      os.path.join(args.analysis, feature_transforms.STATS_FILE)).decode())\n\n  column_names = [col['name'] for col in schema]\n\n  if args.csv:\n    all_files = []\n    for i, file_pattern in enumerate(args.csv):\n      all_files.append(pipeline | ('ReadCSVFile%d' % i) >> beam.io.ReadFromText(file_pattern))\n    raw_data = (\n        all_files\n        | 'MergeCSVFiles' >> beam.Flatten()\n        | 'ParseCSVData' >> beam.Map(decode_csv, column_names))\n  else:\n    columns = ', '.join(column_names)\n    query = 'SELECT {columns} FROM `{table}`'.format(columns=columns,\n                                                     table=args.bigquery)\n    raw_data = (\n        pipeline\n        | 'ReadBiqQueryData'\n        >> beam.io.Read(beam.io.BigQuerySource(query=query,\n                                               use_standard_sql=True)))\n\n  # Note that prepare_image_transforms does not make embeddings, it justs reads\n  # the image files and converts them to byte stings. TransformFeaturesDoFn()\n  # will make the image embeddings.\n  image_columns = image_transform_columns(features)\n  \n  clean_csv_data = (\n      raw_data\n      | 'PreprocessTransferredLearningTransformations'\n      >> beam.Map(prepare_image_transforms, image_columns)\n      | 'BuildCSVString'\n      >> beam.Map(encode_csv, column_names))\n\n  if args.shuffle:\n    clean_csv_data = clean_csv_data | 'ShuffleData' >> shuffle()\n\n  transform_dofn = TransformFeaturesDoFn(args.analysis, features, schema, stats)\n  (transformed_data, errors) = (\n       clean_csv_data\n       | 'Batch Input' \n       >> beam.ParDo(EmitAsBatchDoFn(args.batch_size)) \n       | 'Run TF Graph on Batches' \n       >> beam.ParDo(transform_dofn).with_outputs('errors', main='main'))\n\n  _ = (transformed_data\n        | 'SerializeExamples' >> beam.Map(serialize_example, feature_transforms.get_transformed_feature_info(features, schema))\n        | 'WriteExamples'\n        >> beam.io.WriteToTFRecord(\n            os.path.join(args.output, args.prefix),\n            file_name_suffix='.tfrecord.gz'))\n  _ = (errors\n       | 'WriteErrors'\n       >> beam.io.WriteToText(\n           os.path.join(args.output, 'errors_' + args.prefix),\n           file_name_suffix='.txt'))", "response": "This function is called by the training pipeline to preprocess the input data into a dict format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(argv=None):\n  args = parse_arguments(sys.argv if argv is None else argv)\n  temp_dir = os.path.join(args.output, 'tmp')\n\n  if args.cloud:\n    pipeline_name = 'DataflowRunner'\n  else:\n    pipeline_name = 'DirectRunner'\n    # Suppress TF warnings.\n    os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n\n  options = {\n      'job_name': args.job_name,\n      'temp_location': temp_dir,\n      'project': args.project_id,\n      'setup_file':\n          os.path.abspath(os.path.join(\n              os.path.dirname(__file__),\n              'setup.py')),\n  }\n  if args.num_workers:\n    options['num_workers'] = args.num_workers\n  if args.worker_machine_type:\n    options['worker_machine_type'] = args.worker_machine_type\n\n  pipeline_options = beam.pipeline.PipelineOptions(flags=[], **options)\n\n  p = beam.Pipeline(pipeline_name, options=pipeline_options)\n  preprocess(pipeline=p, args=args)\n  pipeline_result = p.run()\n\n  if not args.async:\n    pipeline_result.wait_until_finish()\n  if args.async and args.cloud:\n    print('View job at https://console.developers.google.com/dataflow/job/%s?project=%s' %\n        (pipeline_result.job_id(), args.project_id))", "response": "Run Preprocessing as a Dataflow."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_bundle(self, element=None):\n    import tensorflow as tf\n    from trainer import feature_transforms\n\n    g = tf.Graph()\n    session = tf.Session(graph=g)\n\n    # Build the transformation graph\n    with g.as_default():\n      transformed_features, _, placeholders = (\n          feature_transforms.build_csv_serving_tensors_for_transform_step(\n              analysis_path=self._analysis_output_dir, \n              features=self._features, \n              schema=self._schema,\n              stats=self._stats,\n              keep_target=True))\n      session.run(tf.tables_initializer())\n    \n    self._session = session\n    self._transformed_features = transformed_features\n    self._input_placeholder_tensor = placeholders['csv_example']", "response": "Build the transfromation graph once."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the transformation graph on the input data.", "response": "def process(self, element):\n    \"\"\"Run the transformation graph on batched input data\n\n    Args:\n      element: list of csv strings, representing one batch input to the TF graph.\n\n    Returns:\n      dict containing the transformed data. Results are un-batched. Sparse\n      tensors are converted to lists.\n    \"\"\"\n    import apache_beam as beam\n    import six\n    import tensorflow as tf\n\n    # This function is invoked by a separate sub-process so setting the logging level\n    # does not affect Datalab's kernel process.\n    tf.logging.set_verbosity(tf.logging.ERROR)\n    try:\n      clean_element = []\n      for line in element:\n        clean_element.append(line.rstrip())\n\n      # batch_result is list of numpy arrays with batch_size many rows.\n      batch_result = self._session.run(\n          fetches=self._transformed_features,\n          feed_dict={self._input_placeholder_tensor: clean_element})\n\n      # ex batch_result. \n      # Dense tensor: {'col1': array([[batch_1], [batch_2]])}\n      # Sparse tensor: {'col1': tf.SparseTensorValue(\n      #   indices=array([[batch_1, 0], [batch_1, 1], ...,\n      #                  [batch_2, 0], [batch_2, 1], ...]],\n      #   values=array[value, value, value, ...])}\n\n      # Unbatch the results.\n      for i in range(len(clean_element)):\n        transformed_features = {}\n        for name, value in six.iteritems(batch_result):\n          if isinstance(value, tf.SparseTensorValue):\n            batch_i_indices = value.indices[:, 0] == i\n            batch_i_values = value.values[batch_i_indices]\n            transformed_features[name] = batch_i_values.tolist()\n          else:\n            transformed_features[name] = value[i].tolist()\n\n        yield transformed_features\n\n    except Exception as e:  # pylint: disable=broad-except\n      yield beam.pvalue.TaggedOutput('errors', (str(e), element))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a row from a BigQuery response into an equivalent object.", "response": "def parse_row(schema, data):\n    \"\"\"Parses a row from query results into an equivalent object.\n\n    Args:\n      schema: the array of fields defining the schema of the data.\n      data: the JSON row from a query result.\n    Returns:\n      The parsed row object.\n    \"\"\"\n    def parse_value(data_type, value):\n      \"\"\"Parses a value returned from a BigQuery response.\n\n      Args:\n        data_type: the type of the value as specified by the schema.\n        value: the raw value to return (before casting to data_type).\n\n      Returns:\n        The value cast to the data_type.\n      \"\"\"\n      if value is not None:\n        if value == 'null':\n          value = None\n        elif data_type == 'INTEGER':\n          value = int(value)\n        elif data_type == 'FLOAT':\n          value = float(value)\n        elif data_type == 'TIMESTAMP':\n          value = datetime.datetime.utcfromtimestamp(float(value))\n        elif data_type == 'BOOLEAN':\n          value = value == 'true'\n        elif (type(value) != str):\n          # TODO(gram): Handle nested JSON records\n          value = str(value)\n      return value\n\n    row = {}\n    if data is None:\n      return row\n\n    for i, (field, schema_field) in enumerate(zip(data['f'], schema)):\n      val = field['v']\n      name = schema_field['name']\n      data_type = schema_field['type']\n      repeated = True if 'mode' in schema_field and schema_field['mode'] == 'REPEATED' else False\n\n      if repeated and val is None:\n        row[name] = []\n      elif data_type == 'RECORD':\n        sub_schema = schema_field['fields']\n        if repeated:\n          row[name] = [Parser.parse_row(sub_schema, v['v']) for v in val]\n        else:\n          row[name] = Parser.parse_row(sub_schema, val)\n      elif repeated:\n        row[name] = [parse_value(data_type, v['v']) for v in val]\n      else:\n        row[name] = parse_value(data_type, val)\n\n    return row"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tf_load_model(sess, model_dir):\n\n  meta_graph_pb = tf.saved_model.loader.load(\n      sess=sess,\n      tags=[tf.saved_model.tag_constants.SERVING],\n      export_dir=model_dir)\n\n  signature = meta_graph_pb.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n  input_alias_map = {friendly_name: tensor_info_proto.name\n                     for (friendly_name, tensor_info_proto) in signature.inputs.items()}\n  output_alias_map = {friendly_name: tensor_info_proto.name\n                      for (friendly_name, tensor_info_proto) in signature.outputs.items()}\n\n  return input_alias_map, output_alias_map", "response": "Load a tf model from model_dir and return input and output alias maps."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading images given image columns.", "response": "def _download_images(data, img_cols):\n  \"\"\"Download images given image columns.\"\"\"\n\n  images = collections.defaultdict(list)\n  for d in data:\n    for img_col in img_cols:\n      if d.get(img_col, None):\n        if isinstance(d[img_col], Image.Image):\n          # If it is already an Image, just copy and continue.\n          images[img_col].append(d[img_col])\n        else:\n          # Otherwise it is image url. Load the image.\n          with file_io.FileIO(d[img_col], 'rb') as fi:\n            im = Image.open(fi)\n          images[img_col].append(im)\n      else:\n        images[img_col].append('')\n\n  return images"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates CSV lines from list - of - dict data.", "response": "def _get_predicton_csv_lines(data, headers, images):\n  \"\"\"Create CSV lines from list-of-dict data.\"\"\"\n\n  if images:\n    data = copy.deepcopy(data)\n    for img_col in images:\n      for d, im in zip(data, images[img_col]):\n        if im == '':\n          continue\n\n        im = im.copy()\n        im.thumbnail((299, 299), Image.ANTIALIAS)\n        buf = BytesIO()\n        im.save(buf, \"JPEG\")\n        content = base64.urlsafe_b64encode(buf.getvalue()).decode('ascii')\n        d[img_col] = content\n\n  csv_lines = []\n  for d in data:\n    buf = six.StringIO()\n    writer = csv.DictWriter(buf, fieldnames=headers, lineterminator='')\n    writer.writerow(d)\n    csv_lines.append(buf.getvalue())\n\n  return csv_lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate display data by converting image urls to base64 strings.", "response": "def _get_display_data_with_images(data, images):\n  \"\"\"Create display data by converting image urls to base64 strings.\"\"\"\n\n  if not images:\n    return data\n\n  display_data = copy.deepcopy(data)\n  for img_col in images:\n    for d, im in zip(display_data, images[img_col]):\n      if im == '':\n        d[img_col + '_image'] = ''\n      else:\n        im = im.copy()\n        im.thumbnail((128, 128), Image.ANTIALIAS)\n        buf = BytesIO()\n        im.save(buf, \"PNG\")\n        content = base64.b64encode(buf.getvalue()).decode('ascii')\n        d[img_col + '_image'] = content\n\n  return display_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_model_schema_and_features(model_dir):\n  schema_file = os.path.join(model_dir, 'assets.extra', 'schema.json')\n  schema = json.loads(file_io.read_file_to_string(schema_file))\n  features_file = os.path.join(model_dir, 'assets.extra', 'features.json')\n  features_config = json.loads(file_io.read_file_to_string(features_file))\n  return schema, features_config", "response": "Get a local model s schema and features config."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_prediction_results(model_dir_or_id, data, headers, img_cols=None,\n                           cloud=False, with_source=True, show_image=True):\n  \"\"\" Predict with a specified model.\n\n  It predicts with the model, join source data with prediction results, and formats\n  the results so they can be displayed nicely in Datalab.\n\n  Args:\n    model_dir_or_id: The model directory if cloud is False, or model.version if cloud is True.\n    data: Can be a list of dictionaries, a list of csv lines, or a Pandas DataFrame. If it is not\n        a list of csv lines, data will be converted to csv lines first, using the orders specified\n        by headers and then send to model. For images, it can be image gs urls or in-memory PIL\n        images. Images will be converted to base64 encoded strings before prediction.\n    headers: the column names of data. It specifies the order of the columns when\n        serializing to csv lines for prediction.\n    img_cols: The image url columns. If specified, the img_urls will be converted to\n        base64 encoded image bytes.\n    with_source: Whether return a joined prediction source and prediction results, or prediction\n        results only.\n    show_image: When displaying prediction source, whether to add a column of image bytes for\n        each image url column.\n\n  Returns:\n    A dataframe of joined prediction source and prediction results, or prediction results only.\n  \"\"\"\n\n  if img_cols is None:\n    img_cols = []\n\n  if isinstance(data, pd.DataFrame):\n    data = list(data.T.to_dict().values())\n  elif isinstance(data[0], six.string_types):\n    data = list(csv.DictReader(data, fieldnames=headers))\n\n  images = _download_images(data, img_cols)\n  predict_data = _get_predicton_csv_lines(data, headers, images)\n\n  if cloud:\n    parts = model_dir_or_id.split('.')\n    if len(parts) != 2:\n      raise ValueError('Invalid model name for cloud prediction. Use \"model.version\".')\n\n    predict_results = ml.ModelVersions(parts[0]).predict(parts[1], predict_data)\n  else:\n    tf_logging_level = logging.getLogger(\"tensorflow\").level\n    logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n    try:\n      predict_results = _tf_predict(model_dir_or_id, predict_data)\n    finally:\n      logging.getLogger(\"tensorflow\").setLevel(tf_logging_level)\n\n  df_r = pd.DataFrame(predict_results)\n  if not with_source:\n    return df_r\n\n  display_data = data\n  if show_image:\n    display_data = _get_display_data_with_images(data, images)\n\n  df_s = pd.DataFrame(display_data)\n  df = pd.concat([df_r, df_s], axis=1)\n  # Remove duplicate columns. All 'key' columns are duplicate here.\n  df = df.loc[:, ~df.columns.duplicated()]\n\n  return df", "response": "Predicts with a specified model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive ML Workbench prediction results get probs for each class.", "response": "def get_probs_for_labels(labels, prediction_results):\n  \"\"\" Given ML Workbench prediction results, get probs of each label for each instance.\n\n  The prediction results are like:\n  [\n    {'predicted': 'daisy', 'probability': 0.8, 'predicted_2': 'rose', 'probability_2': 0.1},\n    {'predicted': 'sunflower', 'probability': 0.9, 'predicted_2': 'daisy', 'probability_2': 0.01},\n    ...\n  ]\n\n  Each instance is ordered by prob. But in some cases probs are needed for fixed\n  order of labels. For example, given labels = ['daisy', 'rose', 'sunflower'], the\n  results of above is expected to be:\n  [\n    [0.8, 0.1, 0.0],\n    [0.01, 0.0, 0.9],\n    ...\n  ]\n  Note that the sum of each instance may not be always 1. If model's top_n is set to\n  none-zero, and is less than number of labels, then prediction results may not contain\n  probs for all labels.\n\n  Args:\n    labels: a list of labels specifying the order of the labels.\n    prediction_results: a pandas DataFrame containing prediction results, usually returned\n        by get_prediction_results() call.\n\n  Returns:\n    A list of list of probs for each class.\n  \"\"\"\n\n  probs = []\n  if 'probability' in prediction_results:\n    # 'probability' exists so top-n is set to none zero, and results are like\n    # \"predicted, predicted_2,...,probability,probability_2,...\n    for i, r in prediction_results.iterrows():\n      probs_one = [0.0] * len(labels)\n      for k, v in six.iteritems(r):\n        if v in labels and k.startswith('predicted'):\n          if k == 'predict':\n            prob_name = 'probability'\n          else:\n            prob_name = 'probability' + k[9:]\n          probs_one[labels.index(v)] = r[prob_name]\n      probs.append(probs_one)\n    return probs\n  else:\n    # 'probability' does not exist, so top-n is set to zero. Results are like\n    # \"predicted, class_name1, class_name2,...\n    for i, r in prediction_results.iterrows():\n      probs_one = [0.0] * len(labels)\n      for k, v in six.iteritems(r):\n        if k in labels:\n          probs_one[labels.index(k)] = v\n      probs.append(probs_one)\n    return probs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef local_batch_predict(model_dir, csv_file_pattern, output_dir, output_format, batch_size=100):\n\n  file_io.recursive_create_dir(output_dir)\n  csv_files = file_io.get_matching_files(csv_file_pattern)\n  if len(csv_files) == 0:\n    raise ValueError('No files found given ' + csv_file_pattern)\n\n  with tf.Graph().as_default(), tf.Session() as sess:\n    input_alias_map, output_alias_map = _tf_load_model(sess, model_dir)\n    csv_tensor_name = list(input_alias_map.values())[0]\n    output_schema = _get_output_schema(sess, output_alias_map)\n    for csv_file in csv_files:\n      output_file = os.path.join(\n          output_dir,\n          'predict_results_' +\n          os.path.splitext(os.path.basename(csv_file))[0] + '.' + output_format)\n      with file_io.FileIO(output_file, 'w') as f:\n        prediction_source = _batch_csv_reader(csv_file, batch_size)\n        for batch in prediction_source:\n          batch = [l.rstrip() for l in batch if l]\n          predict_results = sess.run(fetches=output_alias_map, feed_dict={csv_tensor_name: batch})\n          formatted_results = _format_results(output_format, output_schema, predict_results)\n          f.write('\\n'.join(formatted_results) + '\\n')\n\n  file_io.write_string_to_file(os.path.join(output_dir, 'predict_results_schema.json'),\n                               json.dumps(output_schema, indent=2))", "response": "Batch Predict with a specified model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrefresh the state of the job.", "response": "def _refresh_state(self):\n    \"\"\" Refresh the job info. \"\"\"\n    self._info = self._api.projects().jobs().get(name=self._name).execute()\n    self._fatal_error = self._info.get('errorMessage', None)\n    state = str(self._info.get('state'))\n    self._is_complete = (state == 'SUCCEEDED' or state == 'FAILED')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubmit a training job.", "response": "def submit_training(job_request, job_id=None):\n    \"\"\"Submit a training job.\n\n    Args:\n      job_request: the arguments of the training job in a dict. For example,\n          {\n            'package_uris':  'gs://my-bucket/iris/trainer-0.1.tar.gz',\n            'python_module': 'trainer.task',\n            'scale_tier': 'BASIC',\n            'region': 'us-central1',\n            'args': {\n              'train_data_paths': ['gs://mubucket/data/features_train'],\n              'eval_data_paths': ['gs://mubucket/data/features_eval'],\n              'metadata_path': 'gs://mubucket/data/metadata.yaml',\n              'output_path': 'gs://mubucket/data/mymodel/',\n            }\n          }\n          If 'args' is present in job_request and is a dict, it will be expanded to\n          --key value or --key list_item_0 --key list_item_1, ...\n      job_id: id for the training job. If None, an id based on timestamp will be generated.\n    Returns:\n      A Job object representing the cloud training job.\n    \"\"\"\n    new_job_request = dict(job_request)\n    # convert job_args from dict to list as service required.\n    if 'args' in job_request and isinstance(job_request['args'], dict):\n      job_args = job_request['args']\n      args = []\n      for k, v in six.iteritems(job_args):\n        if isinstance(v, list):\n          for item in v:\n            args.append('--' + str(k))\n            args.append(str(item))\n        else:\n          args.append('--' + str(k))\n          args.append(str(v))\n      new_job_request['args'] = args\n\n    if job_id is None:\n      job_id = datetime.datetime.now().strftime('%y%m%d_%H%M%S')\n      if 'python_module' in new_job_request:\n        job_id = new_job_request['python_module'].replace('.', '_') + \\\n            '_' + job_id\n\n    job = {\n        'job_id': job_id,\n        'training_input': new_job_request,\n    }\n    context = datalab.Context.default()\n    cloudml = discovery.build('ml', 'v1', credentials=context.credentials)\n    request = cloudml.projects().jobs().create(body=job,\n                                               parent='projects/' + context.project_id)\n    request.headers['user-agent'] = 'GoogleCloudDataLab/1.0'\n    request.execute()\n    return Job(job_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef submit_batch_prediction(job_request, job_id=None):\n\n    if job_id is None:\n      job_id = 'prediction_' + datetime.datetime.now().strftime('%y%m%d_%H%M%S')\n\n    job = {\n        'job_id': job_id,\n        'prediction_input': job_request,\n    }\n    context = datalab.Context.default()\n    cloudml = discovery.build('ml', 'v1', credentials=context.credentials)\n    request = cloudml.projects().jobs().create(body=job,\n                                               parent='projects/' + context.project_id)\n    request.headers['user-agent'] = 'GoogleCloudDataLab/1.0'\n    request.execute()\n    return Job(job_id)", "response": "Submit a batch prediction job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing command line arguments and return a Namespace object.", "response": "def parse_arguments(argv):\n  \"\"\"Parse command line arguments.\n\n  Args:\n    argv: list of command line arguments, including program name.\n\n  Returns:\n    An argparse Namespace object.\n\n  Raises:\n    ValueError: for bad parameters\n  \"\"\"\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.RawDescriptionHelpFormatter,\n      description=textwrap.dedent(\"\"\"\\\n          Runs analysis on structured data and produces auxiliary files for\n          training. The output files can also be used by the Transform step\n          to materialize TF.Examples files, which for some problems can speed up\n          training.\n\n          Description of input files\n          --------------------------\n\n          1) If using csv files, the --schema parameter must be the file path to\n             a schema file. The format of this file must be a valid BigQuery\n             schema file, which is a JSON file containing a list of dicts.\n             Consider the example schema file below:\n\n             [\n                {\"name\": \"column_name_1\", \"type\": \"integer\"},\n                {\"name\": \"column_name_2\", \"type\": \"float\"},\n                {\"name\": \"column_name_3\", \"type\": \"string\"},\n                {\"name\": \"column_name_4\", \"type\": \"string\"},\n             ]\n\n             Note that the column names in the csv file much match the order\n             in the schema list. Also, we only support three BigQuery types (\n             integer, float, and string).\n\n             If instead of csv files, --bigquery is used, the schema file\n             is not needed as this program will extract it from\n             the table directly.\n\n          2) --features is a file path to a file describing the\n             transformations. Below is an example features file:\n\n             {\n                \"column_name_1\": {\"transform\": \"scale\"},\n                \"column_name_3\": {\"transform\": \"target\"},\n                \"column_name_2\": {\"transform\": \"one_hot\"},\n                \"new_feature_name\": {\"transform\": \"multi_hot\", \"source_column\": \"column_name_4\"},\n             }\n\n             The format of the dict is `name`: `transform-dict` where the\n             `name` is the name of the transformed feature. The `source_column`\n             value lists what column in the input data is the source for this\n             transformation. If `source_column` is missing, it is assumed the\n             `name` is a source column and the transformed feature will have\n             the same name as the input column.\n\n             A list of supported `transform-dict`s for xgboost is below:\n\n             {\"transform\": \"identity\"}: does nothing (for numerical columns).\n             {\"transform\": \"scale\", \"value\": x}: scale a numerical column to\n                [-a, a]. If value is missing, x defaults to 1.\n             {\"transform\": \"one_hot\"}: makes a one-hot encoding of a string\n                column.\n             {\"transform\": \"multi_hot\", \"separator\": ' '}: makes a multi-hot\n                encoding of a string column.\n             {\"transform\": \"image_to_vec\", \"checkpoint\": \"gs://b/o\"}: From image\n                gs url to embeddings. \"checkpoint\" is a inception v3 checkpoint.\n                If absent, a default checkpoint is used.\n             {\"transform\": \"target\"}: denotes what column is the target. If the\n                schema type of this column is string, a one_hot encoding is\n                automatically applied. If type is numerical, a identity transform\n                is automatically applied.\n  \"\"\"))\n  parser.add_argument('--cloud',\n                      action='store_true',\n                      help='Analysis will use cloud services.')\n  parser.add_argument('--output',\n                      metavar='DIR',\n                      type=str,\n                      required=True,\n                      help='GCS or local folder')\n\n  input_group = parser.add_argument_group(\n      title='Data Source Parameters',\n      description='schema is only needed if using --csv')\n\n  # CSV input\n  input_group.add_argument('--csv',\n                           metavar='FILE',\n                           type=str,\n                           required=False,\n                           action='append',\n                           help='Input CSV absolute file paths. May contain a '\n                                'file pattern.')\n  input_group.add_argument('--schema',\n                           metavar='FILE',\n                           type=str,\n                           required=False,\n                           help='Schema file path. Only required if using csv files')\n\n  # Bigquery input\n  input_group.add_argument('--bigquery',\n                           metavar='PROJECT_ID.DATASET.TABLE_NAME',\n                           type=str,\n                           required=False,\n                           help=('Must be in the form project.dataset.table_name'))\n\n  parser.add_argument('--features',\n                      metavar='FILE',\n                      type=str,\n                      required=True,\n                      help='Features file path')\n\n  args = parser.parse_args(args=argv[1:])\n\n  if args.cloud:\n    if not args.output.startswith('gs://'):\n      raise ValueError('--output must point to a location on GCS')\n    if (args.csv and\n       not all(x.startswith('gs://') for x in args.csv)):\n      raise ValueError('--csv must point to a location on GCS')\n    if args.schema and not args.schema.startswith('gs://'):\n      raise ValueError('--schema must point to a location on GCS')\n\n  if not args.cloud and args.bigquery:\n    raise ValueError('--bigquery must be used with --cloud')\n\n  if not ((args.bigquery and args.csv is None and\n           args.schema is None) or\n          (args.bigquery is None and args.csv and\n           args.schema)):\n    raise ValueError('either --csv and --schema must both'\n                     ' be set or just --bigquery is set')\n\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_cloud_analysis(output_dir, csv_file_pattern, bigquery_table, schema,\n                       features):\n  \"\"\"Use BigQuery to analyze input date.\n\n  Only one of csv_file_pattern or bigquery_table should be non-None.\n\n  Args:\n    output_dir: output folder\n    csv_file_pattern: list of csv file paths, may contain wildcards\n    bigquery_table: project_id.dataset_name.table_name\n    schema: schema list\n    features: features config\n  \"\"\"\n\n  def _execute_sql(sql, table):\n    \"\"\"Runs a BigQuery job and dowloads the results into local memeory.\n\n    Args:\n      sql: a SQL string\n      table: bq.ExternalDataSource or bq.Table\n\n    Returns:\n      A Pandas dataframe.\n    \"\"\"\n    import google.datalab.bigquery as bq\n    if isinstance(table, bq.ExternalDataSource):\n      query = bq.Query(sql, data_sources={'csv_table': table})\n    else:\n      query = bq.Query(sql)\n    return query.execute().result().to_dataframe()\n\n  feature_analysis.expand_defaults(schema, features)  # features are updated.\n  inverted_features = feature_analysis.invert_features(features)\n  feature_analysis.check_schema_transforms_match(schema, inverted_features)\n\n  import google.datalab.bigquery as bq\n  if bigquery_table:\n    table_name = '`%s`' % bigquery_table\n    table = None\n  else:\n    table_name = 'csv_table'\n    table = bq.ExternalDataSource(\n        source=csv_file_pattern,\n        schema=bq.Schema(schema))\n\n  # Make a copy of inverted_features and update the target transform to be\n  # identity or one hot depending on the schema.\n  inverted_features_target = copy.deepcopy(inverted_features)\n  for name, transforms in six.iteritems(inverted_features_target):\n    transform_set = {x['transform'] for x in transforms}\n    if transform_set == set([constant.TARGET_TRANSFORM]):\n      target_schema = next(col['type'].lower() for col in schema if col['name'] == name)\n      if target_schema in constant.NUMERIC_SCHEMA:\n        inverted_features_target[name] = [{'transform': constant.IDENTITY_TRANSFORM}]\n      else:\n        inverted_features_target[name] = [{'transform': constant.ONE_HOT_TRANSFORM}]\n\n  numerical_vocab_stats = {}\n  for col_name, transform_set in six.iteritems(inverted_features_target):\n    sys.stdout.write('Analyzing column %s...\\n' % col_name)\n    sys.stdout.flush()\n    # All transforms in transform_set require the same analysis. So look\n    # at the first transform.\n    transform = next(iter(transform_set))\n    if (transform['transform'] in constant.CATEGORICAL_TRANSFORMS or\n       transform['transform'] in constant.TEXT_TRANSFORMS):\n      if transform['transform'] in constant.TEXT_TRANSFORMS:\n        # Split strings on space, then extract labels and how many rows each\n        # token is in. This is done by making two temp tables:\n        #   SplitTable: each text row is made into an array of strings. The\n        #       array may contain repeat tokens\n        #   TokenTable: SplitTable with repeated tokens removed per row.\n        # Then to flatten the arrays, TokenTable has to be joined with itself.\n        # See the sections 'Flattening Arrays' and 'Filtering Arrays' at\n        # https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays\n        separator = transform.get('separator', ' ')\n        sql = ('WITH SplitTable AS '\n               '         (SELECT SPLIT({name}, \\'{separator}\\') as token_array FROM {table}), '\n               '     TokenTable AS '\n               '         (SELECT ARRAY(SELECT DISTINCT x '\n               '                       FROM UNNEST(token_array) AS x) AS unique_tokens_per_row '\n               '          FROM SplitTable) '\n               'SELECT token, COUNT(token) as token_count '\n               'FROM TokenTable '\n               'CROSS JOIN UNNEST(TokenTable.unique_tokens_per_row) as token '\n               'WHERE LENGTH(token) > 0 '\n               'GROUP BY token '\n               'ORDER BY token_count DESC, token ASC').format(separator=separator,\n                                                              name=col_name,\n                                                              table=table_name)\n      else:\n        # Extract label and frequency\n        sql = ('SELECT {name} as token, count(*) as count '\n               'FROM {table} '\n               'WHERE {name} IS NOT NULL '\n               'GROUP BY {name} '\n               'ORDER BY count DESC, token ASC').format(name=col_name,\n                                                        table=table_name)\n\n      df = _execute_sql(sql, table)\n\n      # Save the vocab\n      csv_string = df.to_csv(index=False, header=False)\n      file_io.write_string_to_file(\n          os.path.join(output_dir, constant.VOCAB_ANALYSIS_FILE % col_name),\n          csv_string)\n      numerical_vocab_stats[col_name] = {'vocab_size': len(df)}\n\n      # free memeory\n      del csv_string\n      del df\n    elif transform['transform'] in constant.NUMERIC_TRANSFORMS:\n      # get min/max/average\n      sql = ('SELECT max({name}) as max_value, min({name}) as min_value, '\n             'avg({name}) as avg_value from {table}').format(name=col_name,\n                                                             table=table_name)\n      df = _execute_sql(sql, table)\n      numerical_vocab_stats[col_name] = {'min': df.iloc[0]['min_value'],\n                                         'max': df.iloc[0]['max_value'],\n                                         'mean': df.iloc[0]['avg_value']}\n    sys.stdout.write('column %s analyzed.\\n' % col_name)\n    sys.stdout.flush()\n\n  # get num examples\n  sql = 'SELECT count(*) as num_examples from {table}'.format(table=table_name)\n  df = _execute_sql(sql, table)\n  num_examples = df.iloc[0]['num_examples']\n\n  # Write the stats file.\n  stats = {'column_stats': numerical_vocab_stats, 'num_examples': num_examples}\n  file_io.write_string_to_file(\n      os.path.join(output_dir, constant.STATS_FILE),\n      json.dumps(stats, indent=2, separators=(',', ': ')))\n\n  feature_analysis.save_schema_features(schema, features, output_dir)", "response": "Use BigQuery to analyze input date."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope='InceptionV3'):\n  \"\"\"Inception model from http://arxiv.org/abs/1512.00567.\n\n  \"Rethinking the Inception Architecture for Computer Vision\"\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse 'scope' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if 'depth_multiplier' is less than or equal to zero.\n  \"\"\"\n  if depth_multiplier <= 0:\n    raise ValueError('depth_multiplier is not greater than zero.')\n\n  def depth(d):\n      return max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding='SAME'):\n        aux_logits = end_points['Mixed_6e']\n        with tf.variable_scope('AuxLogits'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding='VALID',\n              scope='AvgPool_1a_5x5')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope='Conv2d_1b_1x1')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding='VALID', scope='Conv2d_2a_{}x{}'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope='Conv2d_2b_1x1')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name='SpatialSqueeze')\n          end_points['AuxLogits'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope('Logits'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding='VALID',\n                              scope='AvgPool_1a_{}x{}'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n        end_points['PreLogits'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope='Conv2d_1c_1x1')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n        # 1000\n      end_points['Logits'] = logits\n      end_points['Predictions'] = prediction_fn(logits, scope='Predictions')\n  return logits, end_points", "response": "This function constructs the inception model from the Inception Architecture for Computer Vision."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out", "response": "Define kernel size which is automatically reduced for small input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inception_v3_arg_scope(weight_decay=0.00004,\n                           stddev=0.1,\n                           batch_norm_var_collection='moving_vars'):\n  \"\"\"Defines the default InceptionV3 arg scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    batch_norm_var_collection: The name of the collection for the batch norm\n      variables.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  \"\"\"\n  batch_norm_params = {\n      # Decay for the moving averages.\n      'decay': 0.9997,\n      # epsilon to prevent 0s in variance.\n      'epsilon': 0.001,\n      # collection containing update_ops.\n      'updates_collections': tf.GraphKeys.UPDATE_OPS,\n      # collection containing the moving mean and moving variance.\n      'variables_collections': {\n          'beta': None,\n          'gamma': None,\n          'moving_mean': [batch_norm_var_collection],\n          'moving_variance': [batch_norm_var_collection],\n      }\n  }\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d],\n                        weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n                        activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as sc:\n      return sc", "response": "Defines the default InceptionV3 arg scope."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict(model_dir, image_files, resize, show_image):\n\n    from . import _predictor\n\n    images = _util.load_images(image_files, resize=resize)\n    labels_and_scores = _predictor.predict(model_dir, images)\n    results = zip(image_files, images, labels_and_scores)\n    ret = _util.process_prediction_results(results, show_image)\n    return ret", "response": "Predict using an image in a local or GCS directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbatches predict running locally.", "response": "def batch_predict(dataset, model_dir, output_csv, output_bq_table):\n    \"\"\"Batch predict running locally.\"\"\"\n\n    import apache_beam as beam\n    from google.datalab.utils import LambdaJob\n    from . import _predictor\n\n    if output_csv is None and output_bq_table is None:\n      raise ValueError('output_csv and output_bq_table cannot both be None.')\n\n    job_id = ('batch-predict-image-classification-' +\n              datetime.datetime.now().strftime('%y%m%d-%H%M%S'))\n\n    # Project is needed for bigquery data source, even in local run.\n    options = {\n        'project': _util.default_project(),\n    }\n    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n    p = beam.Pipeline('DirectRunner', options=opts)\n    _predictor.configure_pipeline(p, dataset, model_dir, output_csv, output_bq_table)\n    job = LambdaJob(lambda: p.run().wait_until_finish(), job_id)\n    return job"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the result for a job. This will block until the job is complete.", "response": "def result(self):\n    \"\"\" Get the result for a job. This will block if the job is incomplete.\n\n    Returns:\n      The result for the Job.\n\n    Raises:\n      An exception if the Job resulted in an exception.\n\n    \"\"\"\n    self.wait()\n    if self._fatal_error:\n      raise self._fatal_error\n    return self._result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _refresh_state(self):\n    if self._is_complete:\n      return\n\n    if not self._future:\n      raise Exception('Please implement this in the derived class')\n\n    if self._future.done():\n      self._is_complete = True\n      self._end_time = datetime.datetime.utcnow()\n      try:\n        self._result = self._future.result()\n      except Exception as e:\n        message = str(e)\n        self._fatal_error = JobError(location=traceback.format_exc(), message=message,\n                                     reason=str(type(e)))", "response": "Refreshes the state of the job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait(self, timeout=None):\n    if self._future:\n      try:\n        # Future.exception() will return rather than raise any exception so we use it.\n        self._future.exception(timeout)\n      except concurrent.futures.TimeoutError:\n        self._timeout()\n      self._refresh_state()\n    else:\n      # fall back to polling\n      while not self.is_complete:\n        if timeout is not None:\n          if timeout <= 0:\n            self._timeout()\n          timeout -= Job._POLL_INTERVAL_SECONDS\n        time.sleep(Job._POLL_INTERVAL_SECONDS)\n    return self", "response": "Wait for the job to complete or timeout in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef state(self):\n    state = 'in progress'\n    if self.is_complete:\n      if self.failed:\n        state = 'failed with error: %s' % str(self._fatal_error)\n      elif self._errors:\n        state = 'completed with some non-fatal errors'\n      else:\n        state = 'completed'\n    return state", "response": "Describe the state of a Job."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait_any(jobs, timeout=None):\n    return Job._wait(jobs, timeout, concurrent.futures.FIRST_COMPLETED)", "response": "Wait until at least one of the specified jobs have completed or timeout expires."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait_all(jobs, timeout=None):\n    return Job._wait(jobs, timeout, concurrent.futures.ALL_COMPLETED)", "response": "Wait for all of the specified jobs to complete or timeout expires."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef evaluate(self, num_eval_batches=None):\n\n    num_eval_batches = num_eval_batches or self.num_eval_batches\n    with tf.Graph().as_default() as graph:\n      self.tensors = self.model.build_eval_graph(self.eval_data_paths,\n                                                 self.batch_size)\n      self.summary = tf.summary.merge_all()\n      self.saver = tf.train.Saver()\n\n    self.summary_writer = tf.summary.FileWriter(self.output_path)\n    self.sv = tf.train.Supervisor(\n        graph=graph,\n        logdir=self.output_path,\n        summary_op=None,\n        global_step=None,\n        saver=self.saver)\n\n    last_checkpoint = tf.train.latest_checkpoint(self.checkpoint_path)\n    with self.sv.managed_session(master='', start_standard_services=False) as session:\n      self.sv.saver.restore(session, last_checkpoint)\n\n      if not self.batch_of_examples:\n        self.sv.start_queue_runners(session)\n        for i in range(num_eval_batches):\n          self.batch_of_examples.append(session.run(self.tensors.examples))\n\n      for i in range(num_eval_batches):\n        session.run(self.tensors.metric_updates,\n                    {self.tensors.examples: self.batch_of_examples[i]})\n\n      metric_values = session.run(self.tensors.metric_values)\n      global_step = tf.train.global_step(session, self.tensors.global_step)\n      summary = session.run(self.summary)\n      self.summary_writer.add_summary(summary, global_step)\n      self.summary_writer.flush()\n      return metric_values", "response": "Run one round of evaluation return loss and accuracy."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_training(self):\n    self.train_path = os.path.join(self.output_path, 'train')\n    self.model_path = os.path.join(self.output_path, 'model')\n    self.is_master = self.task.type != 'worker'\n    log_interval = 15\n    self.eval_interval = 30\n    if self.is_master and self.task.index > 0:\n      raise Exception('Only one replica of master expected')\n\n    if self.cluster:\n      logging.info('Starting %s/%d', self.task.type, self.task.index)\n      server = start_server(self.cluster, self.task)\n      target = server.target\n      device_fn = tf.train.replica_device_setter(\n          ps_device='/job:ps',\n          worker_device='/job:%s/task:%d' % (self.task.type, self.task.index),\n          cluster=self.cluster)\n      # We use a device_filter to limit the communication between this job\n      # and the parameter servers, i.e., there is no need to directly\n      # communicate with the other workers; attempting to do so can result\n      # in reliability problems.\n      device_filters = [\n          '/job:ps', '/job:%s/task:%d' % (self.task.type, self.task.index)\n      ]\n      config = tf.ConfigProto(device_filters=device_filters)\n    else:\n      target = ''\n      device_fn = ''\n      config = None\n\n    with tf.Graph().as_default() as graph:\n      with tf.device(device_fn):\n        # Build the training graph.\n        self.tensors = self.model.build_train_graph(self.train_data_paths,\n                                                    self.batch_size)\n\n        # Add the variable initializer Op.\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints.\n        self.saver = tf.train.Saver()\n\n        # Build the summary operation based on the TF collection of Summaries.\n        self.summary_op = tf.summary.merge_all()\n\n    # Create a \"supervisor\", which oversees the training process.\n    self.sv = tf.train.Supervisor(\n        graph,\n        is_chief=self.is_master,\n        logdir=self.train_path,\n        init_op=init_op,\n        saver=self.saver,\n        # Write summary_ops by hand.\n        summary_op=None,\n        global_step=self.tensors.global_step,\n        # No saving; we do it manually in order to easily evaluate immediately\n        # afterwards.\n        save_model_secs=0)\n\n    should_retry = True\n    to_run = [self.tensors.global_step, self.tensors.train]\n\n    while should_retry:\n      try:\n        should_retry = False\n        with self.sv.managed_session(target, config=config) as session:\n          self.start_time = start_time = time.time()\n          self.last_save = self.last_log = 0\n          self.global_step = self.last_global_step = 0\n          self.local_step = self.last_local_step = 0\n          self.last_global_time = self.last_local_time = start_time\n\n          # Loop until the supervisor shuts down or max_steps have\n          # completed.\n          max_steps = self.max_steps\n          while not self.sv.should_stop() and self.global_step < max_steps:\n            try:\n              # Run one step of the model.\n              self.global_step = session.run(to_run)[0]\n              self.local_step += 1\n\n              self.now = time.time()\n              is_time_to_eval = (self.now - self.last_save) > self.eval_interval\n              is_time_to_log = (self.now - self.last_log) > log_interval\n              should_eval = self.is_master and is_time_to_eval\n              should_log = is_time_to_log or should_eval\n\n              if should_log:\n                self.log(session)\n\n              if should_eval:\n                self.eval(session)\n            except tf.errors.AbortedError:\n              should_retry = True\n\n          if self.is_master:\n            # Take the final checkpoint and compute the final accuracy.\n            # self.saver.save(session, self.sv.save_path, self.tensors.global_step)\n            self.eval(session)\n\n      except tf.errors.AbortedError:\n        print('Hitting an AbortedError. Trying it again.')\n        should_retry = True\n\n    # Export the model for inference.\n    if self.is_master:\n      self.model.export(tf.train.latest_checkpoint(self.train_path), self.model_path)\n\n    # Ask for all the services to stop.\n    self.sv.stop()", "response": "Runs a Master training."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the evaluation loop.", "response": "def eval(self, session):\n    \"\"\"Runs evaluation loop.\"\"\"\n    eval_start = time.time()\n    self.saver.save(session, self.sv.save_path, self.tensors.global_step)\n    logging.info(\n        'Eval, step %d:\\n- on train set %s\\n-- on eval set %s',\n        self.global_step,\n        self.model.format_metric_values(self.train_evaluator.evaluate()),\n        self.model.format_metric_values(self.evaluator.evaluate()))\n    now = time.time()\n\n    # Make sure eval doesn't consume too much of total time.\n    eval_time = now - eval_start\n    train_eval_rate = self.eval_interval / eval_time\n    if train_eval_rate < self.min_train_eval_rate and self.last_save > 0:\n      logging.info('Adjusting eval interval from %.2fs to %.2fs',\n                   self.eval_interval, self.min_train_eval_rate * eval_time)\n      self.eval_interval = self.min_train_eval_rate * eval_time\n\n    self.last_save = now\n    self.last_log = now"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfeatures slice view browser expects data in the format of: {\"metricValues\": {\"count\": 12, \"accuracy\": 1.0}, \"feature\": \"species:Iris-setosa\"} {\"metricValues\": {\"count\": 11, \"accuracy\": 0.72}, \"feature\": \"species:Iris-versicolor\"} ... This function converts a DataFrame to such format.", "response": "def _get_lantern_format(self, df):\n    \"\"\" Feature slice view browser expects data in the format of:\n          {\"metricValues\": {\"count\": 12, \"accuracy\": 1.0}, \"feature\": \"species:Iris-setosa\"}\n          {\"metricValues\": {\"count\": 11, \"accuracy\": 0.72}, \"feature\": \"species:Iris-versicolor\"}\n          ...\n        This function converts a DataFrame to such format.\n    \"\"\"\n\n    if ('count' not in df) or ('feature' not in df):\n      raise Exception('No \"count\" or \"feature\" found in data.')\n    if len(df.columns) < 3:\n      raise Exception('Need at least one metrics column.')\n    if len(df) == 0:\n      raise Exception('Data is empty')\n\n    data = []\n    for _, row in df.iterrows():\n      metric_values = dict(row)\n      feature = metric_values.pop('feature')\n      data.append({'feature': feature, 'metricValues': metric_values})\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot(self, data):\n    import IPython\n\n    if ((sys.version_info.major > 2 and isinstance(data, str)) or\n       (sys.version_info.major <= 2 and isinstance(data, basestring))):\n      data = bq.Query(data)\n\n    if isinstance(data, bq.Query):\n      df = data.execute().result().to_dataframe()\n      data = self._get_lantern_format(df)\n    elif isinstance(data, pd.core.frame.DataFrame):\n      data = self._get_lantern_format(data)\n    else:\n      raise Exception('data needs to be a sql query, or a pandas DataFrame.')\n\n    HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/gcpdatalab/extern/lantern-browser.html\" >\n        <lantern-browser id=\"{html_id}\"></lantern-browser>\n        <script>\n        var browser = document.querySelector('#{html_id}');\n        browser.metrics = {metrics};\n        browser.data = {data};\n        browser.sourceType = 'colab';\n        browser.weightedExamplesColumn = 'count';\n        browser.calibrationPlotUriFn = function(s) {{ return '/' + s; }}\n        </script>\"\"\"\n    # Serialize the data and list of metrics names to JSON string.\n    metrics_str = str(map(str, data[0]['metricValues'].keys()))\n    data_str = str([{str(k): json.dumps(v) for k, v in elem.iteritems()} for elem in data])\n    html_id = 'l' + datalab.utils.commands.Html.next_id()\n    html = HTML_TEMPLATE.format(html_id=html_id, metrics=metrics_str, data=data_str)\n    IPython.display.display(IPython.display.HTML(html))", "response": "Plots a featire slice view on given data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_arguments(argv):\n  parser = argparse.ArgumentParser(\n      description='Runs Preprocessing on structured data.')\n  parser.add_argument('--output-dir',\n                      type=str,\n                      required=True,\n                      help='Google Cloud Storage which to place outputs.')\n\n  parser.add_argument('--schema-file',\n                      type=str,\n                      required=False,\n                      help=('BigQuery json schema file'))\n  parser.add_argument('--input-file-pattern',\n                      type=str,\n                      required=False,\n                      help='Input CSV file names. May contain a file pattern')\n\n  # If using bigquery table\n  # TODO(brandondutra): maybe also support an sql input, so the table can be\n  # ad-hoc.\n  parser.add_argument('--bigquery-table',\n                      type=str,\n                      required=False,\n                      help=('project:dataset.table_name'))\n\n  args = parser.parse_args(args=argv[1:])\n\n  if not args.output_dir.startswith('gs://'):\n    raise ValueError('--output-dir must point to a location on GCS')\n\n  if args.bigquery_table:\n    if args.schema_file or args.input_file_pattern:\n      raise ValueError('If using --bigquery-table, then --schema-file and '\n                       '--input-file-pattern, '\n                       'are not needed.')\n  else:\n    if not args.schema_file or not args.input_file_pattern:\n      raise ValueError('If not using --bigquery-table, then --schema-file and '\n                       '--input-file-pattern '\n                       'are required.')\n\n    if not args.input_file_pattern.startswith('gs://'):\n      raise ValueError('--input-file-pattern must point to files on GCS')\n\n  return args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_table_name(bigquery_table):\n\n  id_name = bigquery_table.split(':')\n  if len(id_name) != 2:\n    raise ValueError('Bigquery table name should be in the form '\n                     'project_id:dataset.table_name. Got %s' % bigquery_table)\n  return id_name[1]", "response": "Giving a string a : b. c returns b. c."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_numerical_analysis(table, schema_list, args):\n  import google.datalab.bigquery as bq\n\n  # Get list of numerical columns.\n  numerical_columns = []\n  for col_schema in schema_list:\n    col_type = col_schema['type'].lower()\n    if col_type == 'integer' or col_type == 'float':\n      numerical_columns.append(col_schema['name'])\n\n  # Run the numerical analysis\n  if numerical_columns:\n    sys.stdout.write('Running numerical analysis...')\n    max_min = [\n        ('max({name}) as max_{name}, '\n         'min({name}) as min_{name}, '\n         'avg({name}) as avg_{name} ').format(name=name)\n        for name in numerical_columns]\n    if args.bigquery_table:\n      sql = 'SELECT  %s from `%s`' % (', '.join(max_min), parse_table_name(args.bigquery_table))\n      numerical_results = bq.Query(sql).execute().result().to_dataframe()\n    else:\n      sql = 'SELECT  %s from csv_table' % ', '.join(max_min)\n      query = bq.Query(sql, data_sources={'csv_table': table})\n      numerical_results = query.execute().result().to_dataframe()\n\n    # Convert the numerical results to a json file.\n    results_dict = {}\n    for name in numerical_columns:\n      results_dict[name] = {'max': numerical_results.iloc[0]['max_%s' % name],\n                            'min': numerical_results.iloc[0]['min_%s' % name],\n                            'mean': numerical_results.iloc[0]['avg_%s' % name]}\n\n    file_io.write_string_to_file(\n        os.path.join(args.output_dir, NUMERICAL_ANALYSIS_FILE),\n        json.dumps(results_dict, indent=2, separators=(',', ': ')))\n\n    sys.stdout.write('done.\\n')", "response": "Run the numerical analysis on the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the categorical analysis on the given table.", "response": "def run_categorical_analysis(table, schema_list, args):\n  \"\"\"Find vocab values for the categorical columns and writes a csv file.\n\n  The vocab files are in the from\n  label1\n  label2\n  label3\n  ...\n\n  Args:\n    table: Reference to FederatedTable (if bigquery_table is false) or a\n        regular Table (otherwise)\n    schema_list: Bigquery schema json object\n    args: the command line args\n  \"\"\"\n  import google.datalab.bigquery as bq\n\n  # Get list of categorical columns.\n  categorical_columns = []\n  for col_schema in schema_list:\n    col_type = col_schema['type'].lower()\n    if col_type == 'string':\n      categorical_columns.append(col_schema['name'])\n\n  if categorical_columns:\n    sys.stdout.write('Running categorical analysis...')\n    for name in categorical_columns:\n      if args.bigquery_table:\n        table_name = parse_table_name(args.bigquery_table)\n      else:\n        table_name = 'table_name'\n\n      sql = \"\"\"\n            SELECT\n              {name}\n            FROM\n              {table}\n            WHERE\n              {name} IS NOT NULL\n            GROUP BY\n              {name}\n            ORDER BY\n              {name}\n      \"\"\".format(name=name, table=table_name)\n      out_file = os.path.join(args.output_dir,\n                              CATEGORICAL_ANALYSIS_FILE % name)\n\n      # extract_async seems to have a bug and sometimes hangs. So get the\n      # results direclty.\n      if args.bigquery_table:\n        df = bq.Query(sql).execute().result().to_dataframe()\n      else:\n        query = bq.Query(sql, data_sources={'table_name': table})\n        df = query.execute().result().to_dataframe()\n\n      # Write the results to a file.\n      string_buff = six.StringIO()\n      df.to_csv(string_buff, index=False, header=False)\n      file_io.write_string_to_file(out_file, string_buff.getvalue())\n\n    sys.stdout.write('done.\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_analysis(args):\n  import google.datalab.bigquery as bq\n  if args.bigquery_table:\n    table = bq.Table(args.bigquery_table)\n    schema_list = table.schema._bq_schema\n  else:\n    schema_list = json.loads(\n        file_io.read_file_to_string(args.schema_file).decode())\n    table = bq.ExternalDataSource(\n        source=args.input_file_pattern,\n        schema=bq.Schema(schema_list))\n\n  # Check the schema is supported.\n  for col_schema in schema_list:\n    col_type = col_schema['type'].lower()\n    if col_type != 'string' and col_type != 'integer' and col_type != 'float':\n      raise ValueError('Schema contains an unsupported type %s.' % col_type)\n\n  run_numerical_analysis(table, schema_list, args)\n  run_categorical_analysis(table, schema_list, args)\n\n  # Save a copy of the schema to the output location.\n  file_io.write_string_to_file(\n      os.path.join(args.output_dir, SCHEMA_FILE),\n      json.dumps(schema_list, indent=2, separators=(',', ': ')))", "response": "Builds an analysis file for training."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the project_id for the context.", "response": "def set_project_id(self, project_id):\n    \"\"\" Set the project_id for the context. \"\"\"\n    self._project_id = project_id\n    if self == Context._global_context:\n      du.save_project_id(self._project_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a default Context object creates it if necessary.", "response": "def default():\n    \"\"\"Retrieves a default Context object, creating it if necessary.\n\n      The default Context is a global shared instance used every time the default context is\n      retrieved.\n\n      Attempting to use a Context with no project_id will raise an exception, so on first use\n      set_project_id must be called.\n\n    Returns:\n      An initialized and shared instance of a Context object.\n    \"\"\"\n    credentials = du.get_credentials()\n    project = du.get_default_project_id()\n    if Context._global_context is None:\n      config = Context._get_default_config()\n      Context._global_context = Context(project, credentials, config)\n    else:\n      # Always update everything in case the access token is revoked or expired, config changed,\n      # or project changed.\n      Context._global_context.set_credentials(credentials)\n      Context._global_context.set_project_id(project)\n    return Context._global_context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a ConfusionMatrix from a CSV file.", "response": "def from_csv(input_csv, headers=None, schema_file=None):\n    \"\"\"Create a ConfusionMatrix from a csv file.\n\n    Args:\n      input_csv: Path to a Csv file (with no header). Can be local or GCS path.\n      headers: Csv headers. If present, it must include 'target' and 'predicted'.\n      schema_file: Path to a JSON file containing BigQuery schema. Used if \"headers\" is None.\n          If present, it must include 'target' and 'predicted' columns.\n    Returns:\n      A ConfusionMatrix that can be plotted.\n    Raises:\n      ValueError if both headers and schema_file are None, or it does not include 'target'\n          or 'predicted' columns.\n    \"\"\"\n\n    if headers is not None:\n      names = headers\n    elif schema_file is not None:\n      with _util.open_local_or_gcs(schema_file, mode='r') as f:\n        schema = json.load(f)\n      names = [x['name'] for x in schema]\n    else:\n      raise ValueError('Either headers or schema_file is needed')\n\n    all_files = _util.glob_files(input_csv)\n    all_df = []\n    for file_name in all_files:\n      with _util.open_local_or_gcs(file_name, mode='r') as f:\n        all_df.append(pd.read_csv(f, names=names))\n    df = pd.concat(all_df, ignore_index=True)\n\n    if 'target' not in df or 'predicted' not in df:\n      raise ValueError('Cannot find \"target\" or \"predicted\" column')\n\n    labels = sorted(set(df['target']) | set(df['predicted']))\n    cm = confusion_matrix(df['target'], df['predicted'], labels=labels)\n    return ConfusionMatrix(cm, labels)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a ConfusionMatrix from a BigQuery table or a query.", "response": "def from_bigquery(sql):\n    \"\"\"Create a ConfusionMatrix from a BigQuery table or query.\n\n    Args:\n      sql: Can be one of:\n          A SQL query string.\n          A Bigquery table string.\n          A Query object defined with '%%bq query --name [query_name]'.\n      The query results or table must include \"target\", \"predicted\" columns.\n    Returns:\n      A ConfusionMatrix that can be plotted.\n    Raises:\n      ValueError if query results or table does not include 'target' or 'predicted' columns.\n    \"\"\"\n    if isinstance(sql, bq.Query):\n      sql = sql._expanded_sql()\n\n    parts = sql.split('.')\n    if len(parts) == 1 or len(parts) > 3 or any(' ' in x for x in parts):\n      sql = '(' + sql + ')'  # query, not a table name\n    else:\n      sql = '`' + sql + '`'  # table name\n\n    query = bq.Query(\n        'SELECT target, predicted, count(*) as count FROM %s group by target, predicted' % sql)\n    df = query.execute().result().to_dataframe()\n    labels = sorted(set(df['target']) | set(df['predicted']))\n    labels_count = len(labels)\n    df['target'] = [labels.index(x) for x in df['target']]\n    df['predicted'] = [labels.index(x) for x in df['predicted']]\n    cm = [[0] * labels_count for i in range(labels_count)]\n    for index, row in df.iterrows():\n      cm[row['target']][row['predicted']] = row['count']\n    return ConfusionMatrix(cm, labels)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the confusion matrix to a dataframe.", "response": "def to_dataframe(self):\n    \"\"\"Convert the confusion matrix to a dataframe.\n\n    Returns:\n      A DataFrame with \"target\", \"predicted\", \"count\" columns.\n    \"\"\"\n\n    data = []\n    for target_index, target_row in enumerate(self._cm):\n      for predicted_index, count in enumerate(target_row):\n        data.append((self._labels[target_index], self._labels[predicted_index], count))\n\n    return pd.DataFrame(data, columns=['target', 'predicted', 'count'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(self, figsize=None, rotation=45):\n\n    fig, ax = plt.subplots(figsize=figsize)\n\n    plt.imshow(self._cm, interpolation='nearest', cmap=plt.cm.Blues, aspect='auto')\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(self._labels))\n    plt.xticks(tick_marks, self._labels, rotation=rotation)\n    plt.yticks(tick_marks, self._labels)\n    if isinstance(self._cm, list):\n      # If cm is created from BigQuery then it is a list.\n      thresh = max(max(self._cm)) / 2.\n      for i, j in itertools.product(range(len(self._labels)), range(len(self._labels))):\n        plt.text(j, i, self._cm[i][j], horizontalalignment=\"center\",\n                 color=\"white\" if self._cm[i][j] > thresh else \"black\")\n    else:\n      # If cm is created from csv then it is a sklearn's confusion_matrix.\n      thresh = self._cm.max() / 2.\n      for i, j in itertools.product(range(len(self._labels)), range(len(self._labels))):\n        plt.text(j, i, self._cm[i, j], horizontalalignment=\"center\",\n                 color=\"white\" if self._cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')", "response": "Plots the confusion matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_environment_details(zone, environment):\n    default_context = google.datalab.Context.default()\n    url = (Api._ENDPOINT + (Api._ENVIRONMENTS_PATH_FORMAT % (default_context.project_id, zone,\n                                                             environment)))\n\n    return google.datalab.utils.Http.request(url, credentials=default_context.credentials)", "response": "Issues a request to Composer to get the environment details."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef buckets_delete(self, bucket):\n    url = Api._ENDPOINT + (Api._BUCKET_PATH % bucket)\n    google.datalab.utils.Http.request(url, method='DELETE', credentials=self._credentials,\n                                      raw_response=True)", "response": "Issues a request to delete a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nissuing a request to retrieve information about a bucket.", "response": "def buckets_get(self, bucket, projection='noAcl'):\n    \"\"\"Issues a request to retrieve information about a bucket.\n\n    Args:\n      bucket: the name of the bucket.\n      projection: the projection of the bucket information to retrieve.\n    Returns:\n      A parsed bucket information dictionary.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    args = {'projection': projection}\n    url = Api._ENDPOINT + (Api._BUCKET_PATH % bucket)\n    return google.datalab.utils.Http.request(url, credentials=self._credentials, args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nissuing a request to retrieve the list of buckets.", "response": "def buckets_list(self, projection='noAcl', max_results=0, page_token=None, project_id=None):\n    \"\"\"Issues a request to retrieve the list of buckets.\n\n    Args:\n      projection: the projection of the bucket information to retrieve.\n      max_results: an optional maximum number of objects to retrieve.\n      page_token: an optional token to continue the retrieval.\n      project_id: the project whose buckets should be listed.\n    Returns:\n      A parsed list of bucket information dictionaries.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    if max_results == 0:\n      max_results = Api._MAX_RESULTS\n\n    args = {'project': project_id if project_id else self._project_id, 'maxResults': max_results}\n    if projection is not None:\n      args['projection'] = projection\n    if page_token is not None:\n      args['pageToken'] = page_token\n\n    url = Api._ENDPOINT + (Api._BUCKET_PATH % '')\n    return google.datalab.utils.Http.request(url, args=args, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef object_download(self, bucket, key, start_offset=0, byte_count=None):\n    args = {'alt': 'media'}\n    headers = {}\n    if start_offset > 0 or byte_count is not None:\n      header = 'bytes=%d-' % start_offset\n      if byte_count is not None:\n        header += '%d' % byte_count\n      headers['Range'] = header\n    url = Api._DOWNLOAD_ENDPOINT + (Api._OBJECT_PATH % (bucket, Api._escape_key(key)))\n    return google.datalab.utils.Http.request(url, args=args, headers=headers,\n                                             credentials=self._credentials, raw_response=True)", "response": "Reads the contents of an object as text."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads text content to the object.", "response": "def object_upload(self, bucket, key, content, content_type):\n    \"\"\"Writes text content to the object.\n\n    Args:\n      bucket: the name of the bucket containing the object.\n      key: the key of the object to be written.\n      content: the text content to be written.\n      content_type: the type of text content.\n    Raises:\n      Exception if the object could not be written to.\n    \"\"\"\n    args = {'uploadType': 'media', 'name': key}\n    headers = {'Content-Type': content_type}\n\n    url = Api._UPLOAD_ENDPOINT + (Api._OBJECT_PATH % (bucket, ''))\n    return google.datalab.utils.Http.request(url, args=args, data=content, headers=headers,\n                                             credentials=self._credentials, raw_response=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nissue a request to retrieve information about an object in a bucket.", "response": "def objects_list(self, bucket, prefix=None, delimiter=None, projection='noAcl', versions=False,\n                   max_results=0, page_token=None):\n    \"\"\"Issues a request to retrieve information about an object.\n\n    Args:\n      bucket: the name of the bucket.\n      prefix: an optional key prefix.\n      delimiter: an optional key delimiter.\n      projection: the projection of the objects to retrieve.\n      versions: whether to list each version of a file as a distinct object.\n      max_results: an optional maximum number of objects to retrieve.\n      page_token: an optional token to continue the retrieval.\n    Returns:\n      A parsed list of object information dictionaries.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    if max_results == 0:\n      max_results = Api._MAX_RESULTS\n\n    args = {'maxResults': max_results}\n    if prefix is not None:\n      args['prefix'] = prefix\n    if delimiter is not None:\n      args['delimiter'] = delimiter\n    if projection is not None:\n      args['projection'] = projection\n    if versions:\n      args['versions'] = 'true'\n    if page_token is not None:\n      args['pageToken'] = page_token\n\n    url = Api._ENDPOINT + (Api._OBJECT_PATH % (bucket, ''))\n    return google.datalab.utils.Http.request(url, args=args, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef objects_patch(self, bucket, key, info):\n    url = Api._ENDPOINT + (Api._OBJECT_PATH % (bucket, Api._escape_key(key)))\n    return google.datalab.utils.Http.request(url, method='PATCH', data=info,\n                                             credentials=self._credentials)", "response": "Updates the metadata associated with an object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef verify_permitted_to_read(gs_path):\n    # TODO(qimingj): Storage APIs need to be modified to allow absence of project\n    #                or credential on Objects. When that happens we can move the function\n    #                to Objects class.\n    from . import _bucket\n    bucket, prefix = _bucket.parse_name(gs_path)\n    credentials = None\n    if google.datalab.Context._is_signed_in():\n      credentials = google.datalab.Context.default().credentials\n    args = {\n        'maxResults': Api._MAX_RESULTS,\n        'projection': 'noAcl'\n    }\n    if prefix is not None:\n      args['prefix'] = prefix\n    url = Api._ENDPOINT + (Api._OBJECT_PATH % (bucket, ''))\n    try:\n      google.datalab.utils.Http.request(url, args=args, credentials=credentials)\n    except google.datalab.utils.RequestException as e:\n      if e.status == 401:\n        raise Exception('Not permitted to read from specified path. '\n                        'Please sign in and make sure you have read access.')\n      raise e", "response": "Checks if the user is permitted to read from the given GCS path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preprocess_async(train_dataset, output_dir, eval_dataset=None, checkpoint=None, cloud=None):\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    if cloud is None:\n      return _local.Local.preprocess(train_dataset, output_dir, eval_dataset, checkpoint)\n\n    if not isinstance(cloud, dict):\n      cloud = {}\n    return _cloud.Cloud.preprocess(train_dataset, output_dir, eval_dataset, checkpoint, cloud)", "response": "Preprocess data. Produce output that can be used by training efficiently."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nblocks version of preprocess_async.", "response": "def preprocess(train_dataset, output_dir, eval_dataset=None, checkpoint=None, cloud=None):\n  \"\"\"Blocking version of preprocess_async(). The only difference is that it blocks the caller\n     until the job finishes, and it does not have a return value.\n  \"\"\"\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    job = preprocess_async(train_dataset, output_dir, eval_dataset, checkpoint, cloud)\n    job.wait()\n    print(job.state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntraining model. The output can be used for batch prediction or online deployment. Args: input_dir: A directory path containing preprocessed results. Can be local or GCS path. batch_size: size of batch used for training. max_steps: number of steps to train. output_dir: The output directory to use. Can be local or GCS path. checkpoint: the Inception checkpoint to use. If None, a default checkpoint is used. cloud: a google.datalab.ml.CloudTrainingConfig object to let it run in cloud. If None, it runs locally. Returns: A google.datalab.utils.Job object that can be used to query state from or wait.", "response": "def train_async(input_dir, batch_size, max_steps, output_dir, checkpoint=None, cloud=None):\n  \"\"\"Train model. The output can be used for batch prediction or online deployment.\n\n  Args:\n    input_dir: A directory path containing preprocessed results. Can be local or GCS path.\n    batch_size: size of batch used for training.\n    max_steps: number of steps to train.\n    output_dir: The output directory to use. Can be local or GCS path.\n    checkpoint: the Inception checkpoint to use. If None, a default checkpoint is used.\n    cloud: a google.datalab.ml.CloudTrainingConfig object to let it run in cloud.\n        If None, it runs locally.\n\n  Returns:\n    A google.datalab.utils.Job object that can be used to query state from or wait.\n  \"\"\"\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    if cloud is None:\n      return _local.Local.train(input_dir, batch_size, max_steps, output_dir, checkpoint)\n\n    return _cloud.Cloud.train(input_dir, batch_size, max_steps, output_dir, checkpoint, cloud)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train(input_dir, batch_size, max_steps, output_dir, checkpoint=None, cloud=None):\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    job = train_async(input_dir, batch_size, max_steps, output_dir, checkpoint, cloud)\n    job.wait()\n    print(job.state)", "response": "Blocking version of train_async."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npredicts using an image file in a local or GCS directory.", "response": "def predict(model, image_files, resize=False, show_image=True, cloud=None):\n  \"\"\"Predict using an model in a local or GCS directory (offline), or a deployed model (online).\n\n  Args:\n    model: if cloud is None, a local or GCS directory of a trained model. Otherwise, it specifies\n        a deployed model identified by model.version, such as \"imagemodel.v1\".\n    image_files: The paths to the image files to predict labels. Can be local or GCS paths.\n    resize: Whether to resize the image to a reasonable size (300x300) before prediction.\n    show_image: Whether to show images in the results.\n    cloud: if None, predicts with offline model locally. Otherwise, predict with a deployed online\n        model.\n\n  Returns:\n    A pandas DataFrame including the prediction results.\n  \"\"\"\n\n  print('Predicting...')\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    if cloud is None:\n      results = _local.Local.predict(model, image_files, resize, show_image)\n    else:\n      results = _cloud.Cloud.predict(model, image_files, resize, show_image)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbatching prediction with an offline model.", "response": "def batch_predict_async(dataset, model_dir, output_csv=None, output_bq_table=None, cloud=None):\n  \"\"\"Batch prediction with an offline model.\n\n  Args:\n    dataset: CsvDataSet or BigQueryDataSet for batch prediction input. Can contain either\n        one column 'image_url', or two columns with another being 'label'.\n    model_dir: The directory of a trained inception model. Can be local or GCS paths.\n    output_csv: The output csv file for prediction results. If specified,\n        it will also output a csv schema file with the name output_csv + '.schema.json'.\n    output_bq_table: if specified, the output BigQuery table for prediction results.\n        output_csv and output_bq_table can both be set.\n    cloud: a DataFlow pipeline option dictionary such as {'num_workers': 3}. If anything but\n        not None, it will run in cloud. Otherwise, it runs locally.\n        If specified, it must include 'temp_location' with value being a GCS path, because cloud\n        run requires a staging GCS directory.\n\n  Raises:\n    ValueError if both output_csv and output_bq_table are None, or if cloud is not None\n        but it does not include 'temp_location'.\n\n  Returns:\n    A google.datalab.utils.Job object that can be used to query state from or wait.\n  \"\"\"\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    if cloud is None:\n      return _local.Local.batch_predict(dataset, model_dir, output_csv, output_bq_table)\n\n    if not isinstance(cloud, dict):\n      cloud = {}\n    return _cloud.Cloud.batch_predict(dataset, model_dir, output_csv, output_bq_table, cloud)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batch_predict(dataset, model_dir, output_csv=None, output_bq_table=None, cloud=None):\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    job = batch_predict_async(dataset, model_dir, output_csv, output_bq_table, cloud)\n    job.wait()\n    print(job.state)", "response": "Blocking version of batch_predict_async."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the state of a job.", "response": "def _refresh_state(self):\n    \"\"\" Get the state of a job. If the job is complete this does nothing;\n        otherwise it gets a refreshed copy of the job resource.\n    \"\"\"\n    # TODO(gram): should we put a choke on refreshes? E.g. if the last call was less than\n    # a second ago should we return the cached value?\n    if self._is_complete:\n      return\n\n    try:\n      response = self._api.jobs_get(self._job_id)\n    except Exception as e:\n      raise e\n\n    if 'status' in response:\n      status = response['status']\n      if 'state' in status and status['state'] == 'DONE':\n        self._end_time = datetime.datetime.utcnow()\n        self._is_complete = True\n        self._process_job_status(status)\n\n    if 'statistics' in response:\n      statistics = response['statistics']\n      start_time = statistics.get('creationTime', None)\n      end_time = statistics.get('endTime', None)\n      if start_time and end_time and end_time >= start_time:\n        self._start_time = datetime.datetime.fromtimestamp(float(start_time) / 1000.0)\n        self._end_time = datetime.datetime.fromtimestamp(float(end_time) / 1000.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef monitoring(line, cell=None):\n  parser = datalab.utils.commands.CommandParser(prog='monitoring', description=(\n      'Execute various Monitoring-related operations. Use \"%monitoring '\n      '<command> -h\" for help on a specific command.'))\n\n  list_parser = parser.subcommand(\n      'list', 'List the metrics or resource types in a monitored project.')\n\n  list_metric_parser = list_parser.subcommand(\n      'metrics',\n      'List the metrics that are available through the Monitoring API.')\n  list_metric_parser.add_argument(\n      '-t', '--type',\n      help='The type of metric(s) to list; can include wildchars.')\n  list_metric_parser.add_argument(\n      '-p', '--project', help='The project on which to execute the request.')\n  list_metric_parser.set_defaults(func=_list_metric_descriptors)\n\n  list_resource_parser = list_parser.subcommand(\n      'resource_types',\n      ('List the monitored resource types that are available through the '\n       'Monitoring API.'))\n  list_resource_parser.add_argument(\n      '-p', '--project', help='The project on which to execute the request.')\n  list_resource_parser.add_argument(\n      '-t', '--type',\n      help='The resource type(s) to list; can include wildchars.')\n  list_resource_parser.set_defaults(func=_list_resource_descriptors)\n\n  list_group_parser = list_parser.subcommand(\n      'groups',\n      ('List the Stackdriver groups in this project.'))\n  list_group_parser.add_argument(\n      '-p', '--project', help='The project on which to execute the request.')\n  list_group_parser.add_argument(\n      '-n', '--name',\n      help='The name of the group(s) to list; can include wildchars.')\n  list_group_parser.set_defaults(func=_list_groups)\n\n  return datalab.utils.commands.handle_magic_line(line, cell, parser)", "response": "Implements the monitoring cell magic for ipython notebooks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the metric descriptors in the project.", "response": "def _list_metric_descriptors(args, _):\n  \"\"\"Lists the metric descriptors in the project.\"\"\"\n  project_id = args['project']\n  pattern = args['type'] or '*'\n  descriptors = gcm.MetricDescriptors(project_id=project_id)\n  dataframe = descriptors.as_dataframe(pattern=pattern)\n  return _render_dataframe(dataframe)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _list_resource_descriptors(args, _):\n  project_id = args['project']\n  pattern = args['type'] or '*'\n  descriptors = gcm.ResourceDescriptors(project_id=project_id)\n  dataframe = descriptors.as_dataframe(pattern=pattern)\n  return _render_dataframe(dataframe)", "response": "Lists the resource descriptors in the project."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _list_groups(args, _):\n  project_id = args['project']\n  pattern = args['name'] or '*'\n  groups = gcm.Groups(project_id=project_id)\n  dataframe = groups.as_dataframe(pattern=pattern)\n  return _render_dataframe(dataframe)", "response": "Lists the groups in the project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate html representation of a job.", "response": "def html_job_status(job_name, job_type, refresh_interval, html_on_running, html_on_success):\n  \"\"\"create html representation of status of a job (long running operation).\n\n  Args:\n    job_name: the full name of the job.\n    job_type: type of job. Can be 'local' or 'cloud'.\n    refresh_interval: how often should the client refresh status.\n    html_on_running: additional html that the job view needs to include on job running.\n    html_on_success: additional html that the job view needs to include on job success.\n  \"\"\"\n  _HTML_TEMPLATE = \"\"\"\n    <div class=\"jobstatus\" id=\"%s\">\n    </div>\n    <script>\n      require(['datalab/job', 'datalab/element!%s', 'base/js/events',\n          'datalab/style!/nbextensions/datalab/job.css'],\n        function(job, dom, events) {\n          job.render(dom, events, '%s', '%s', %s, '%s', '%s');\n        }\n      );\n    </script>\"\"\"\n  div_id = _html.Html.next_id()\n  return IPython.core.display.HTML(_HTML_TEMPLATE % (div_id, div_id, job_name, job_type,\n                                   refresh_interval, html_on_running, html_on_success))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_job_status(line):\n  try:\n    args = line.strip().split()\n    job_name = args[0]\n\n    job = None\n    if job_name in _local_jobs:\n      job = _local_jobs[job_name]\n    else:\n      raise Exception('invalid job %s' % job_name)\n\n    if job is not None:\n      error = '' if job.fatal_error is None else str(job.fatal_error)\n      data = {'exists': True, 'done': job.is_complete, 'error': error}\n    else:\n      data = {'exists': False}\n\n  except Exception as e:\n    google.datalab.utils.print_exception_with_last_stack(e)\n    data = {'done': True, 'error': str(e)}\n\n  return IPython.core.display.JSON(data)", "response": "magic used as an endpoint for client to get the status of a job."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete this object from its bucket.", "response": "def delete(self, wait_for_deletion=True):\n    \"\"\"Deletes this object from its bucket.\n\n    Args:\n      wait_for_deletion: If True, we poll until this object no longer appears in\n          objects.list operations for this bucket before returning.\n\n    Raises:\n      Exception if there was an error deleting the object.\n    \"\"\"\n    if self.exists():\n      try:\n        self._api.objects_delete(self._bucket, self._key)\n      except Exception as e:\n        raise e\n      if wait_for_deletion:\n        for _ in range(_MAX_POLL_ATTEMPTS):\n          objects = Objects(self._bucket, prefix=self.key, delimiter='/',\n                            context=self._context)\n          if any(o.key == self.key for o in objects):\n            time.sleep(_POLLING_SLEEP)\n            continue\n          break\n        else:\n          logging.error('Failed to see object deletion after %d attempts.',\n                        _MAX_POLL_ATTEMPTS)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves metadata about the object.", "response": "def metadata(self):\n    \"\"\"Retrieves metadata about the object.\n\n    Returns:\n      An ObjectMetadata instance with information about this object.\n    Raises:\n      Exception if there was an error requesting the object's metadata.\n    \"\"\"\n    if self._info is None:\n      try:\n        self._info = self._api.objects_get(self._bucket, self._key)\n      except Exception as e:\n        raise e\n    return ObjectMetadata(self._info) if self._info else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the content of this object as text.", "response": "def read_stream(self, start_offset=0, byte_count=None):\n    \"\"\"Reads the content of this object as text.\n\n    Args:\n      start_offset: the start offset of bytes to read.\n      byte_count: the number of bytes to read. If None, it reads to the end.\n    Returns:\n      The text content within the object.\n    Raises:\n      Exception if there was an error requesting the object's content.\n    \"\"\"\n    try:\n      return self._api.object_download(self._bucket, self._key,\n                                       start_offset=start_offset, byte_count=byte_count)\n    except Exception as e:\n      raise e"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the content of this object as text and returns a list of lines up to some max_lines.", "response": "def read_lines(self, max_lines=None):\n    \"\"\"Reads the content of this object as text, and return a list of lines up to some max.\n\n    Args:\n      max_lines: max number of lines to return. If None, return all lines.\n    Returns:\n      The text content of the object as a list of lines.\n    Raises:\n      Exception if there was an error requesting the object's content.\n    \"\"\"\n    if max_lines is None:\n      return self.read_stream().split('\\n')\n\n    max_to_read = self.metadata.size\n    bytes_to_read = min(100 * max_lines, self.metadata.size)\n    while True:\n      content = self.read_stream(byte_count=bytes_to_read)\n\n      lines = content.split('\\n')\n      if len(lines) > max_lines or bytes_to_read >= max_to_read:\n        break\n      # try 10 times more bytes or max\n      bytes_to_read = min(bytes_to_read * 10, max_to_read)\n\n    # remove the partial line at last\n    del lines[-1]\n    return lines[0:max_lines]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsample rows from GCS or local file and save results to target file.", "response": "def sample_to(self, count, skip_header_rows, strategy, target):\n    \"\"\"Sample rows from GCS or local file and save results to target file.\n\n    Args:\n      count: number of rows to sample. If strategy is \"BIGQUERY\", it is used as approximate number.\n      skip_header_rows: whether to skip first row when reading from source.\n      strategy: can be \"LOCAL\" or \"BIGQUERY\". If local, the sampling happens in local memory,\n          and number of resulting rows matches count. If BigQuery, sampling is done\n          with BigQuery in cloud, and the number of resulting rows will be approximated to\n          count.\n      target: The target file path, can be GCS or local path.\n    Raises:\n      Exception if strategy is \"BIGQUERY\" but source is not a GCS path.\n    \"\"\"\n    # TODO(qimingj) Add unit test\n    # Read data from source into DataFrame.\n\n    if sys.version_info.major > 2:\n      xrange = range  # for python 3 compatibility\n\n    if strategy == 'BIGQUERY':\n      import datalab.bigquery as bq\n      if not self.path.startswith('gs://'):\n        raise Exception('Cannot use BIGQUERY if data is not in GCS')\n      federated_table = self._create_federated_table(skip_header_rows)\n      row_count = self._get_gcs_csv_row_count(federated_table)\n      query = bq.Query('SELECT * from data', data_sources={'data': federated_table})\n      sampling = bq.Sampling.random(count * 100 / float(row_count))\n      sample = query.sample(sampling=sampling)\n      df = sample.to_dataframe()\n    elif strategy == 'LOCAL':\n      local_file = self.path\n      if self.path.startswith('gs://'):\n        local_file = tempfile.mktemp()\n        datalab.utils.gcs_copy_file(self.path, local_file)\n      with open(local_file) as f:\n        row_count = sum(1 for line in f)\n      start_row = 1 if skip_header_rows is True else 0\n      skip_count = row_count - count - 1 if skip_header_rows is True else row_count - count\n      skip = sorted(random.sample(xrange(start_row, row_count), skip_count))\n      header_row = 0 if skip_header_rows is True else None\n      df = pd.read_csv(local_file, skiprows=skip, header=header_row, delimiter=self._delimiter)\n      if self.path.startswith('gs://'):\n        os.remove(local_file)\n    else:\n      raise Exception('strategy must be BIGQUERY or LOCAL')\n    # Write to target.\n    if target.startswith('gs://'):\n      with tempfile.NamedTemporaryFile() as f:\n        df.to_csv(f, header=False, index=False)\n        f.flush()\n        datalab.utils.gcs_copy_file(f.name, target)\n    else:\n      with open(target, 'w') as f:\n        df.to_csv(f, header=False, index=False, sep=str(self._delimiter))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ProtoFromTfRecordFiles(self,\n                             files,\n                             max_entries=10000,\n                             features=None,\n                             is_sequence=False,\n                             iterator_options=None):\n    \"\"\"Creates a feature statistics proto from a set of TFRecord files.\n\n    Args:\n      files: A list of dicts describing files for each dataset for the proto.\n        Each\n          entry contains a 'path' field with the path to the TFRecord file on\n            disk\n          and a 'name' field to identify the dataset in the proto.\n      max_entries: The maximum number of examples to load from each dataset\n          in order to create the proto. Defaults to 10000.\n      features: A list of strings that is a whitelist of feature names to create\n          feature statistics for. If set to None then all features in the\n            dataset\n          are analyzed. Defaults to None.\n      is_sequence: True if the input data from 'tables' are tf.SequenceExamples,\n          False if tf.Examples. Defaults to false.\n      iterator_options: Options to pass to the iterator that reads the examples.\n          Defaults to None.\n\n    Returns:\n      The feature statistics proto for the provided files.\n    \"\"\"\n    datasets = []\n    for entry in files:\n      entries, size = self._GetTfRecordEntries(entry['path'], max_entries,\n                                               is_sequence, iterator_options)\n      datasets.append({'entries': entries, 'size': size, 'name': entry['name']})\n    return self.GetDatasetsProto(datasets, features)", "response": "Returns a feature statistics proto from a set of TFRecord files."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a single example into a tree of entries.", "response": "def _ParseExample(self, example_features, example_feature_lists, entries,\n                    index):\n    \"\"\"Parses data from an example, populating a dictionary of feature values.\n\n    Args:\n      example_features: A map of strings to tf.Features from the example.\n      example_feature_lists: A map of strings to tf.FeatureLists from the\n        example.\n      entries: A dictionary of all features parsed thus far and arrays of their\n          values. This is mutated by the function.\n      index: The index of the example to parse from a list of examples.\n    Raises:\n      TypeError: Raises an exception when a feature has inconsistent types\n      across\n          examples.\n    \"\"\"\n    features_seen = set()\n\n    for feature_list, is_feature in zip(\n        [example_features, example_feature_lists], [True, False]):\n      sequence_length = None\n      for feature_name in feature_list:\n        # If this feature has not been seen in previous examples, then\n        # initialize its entry into the entries dictionary.\n        if feature_name not in entries:\n          entries[feature_name] = {\n              'vals': [],\n              'counts': [],\n              'feat_lens': [],\n              'missing': index\n          }\n\n        feature_entry = entries[feature_name]\n        feature = feature_list[feature_name]\n\n        value_type = None\n        value_list = []\n        if is_feature:\n          # If parsing a tf.Feature, extract the type and values simply.\n          if feature.HasField('float_list'):\n            value_list = feature.float_list.value\n            value_type = self.fs_proto.FLOAT\n          elif feature.HasField('bytes_list'):\n            value_list = feature.bytes_list.value\n            value_type = self.fs_proto.STRING\n          elif feature.HasField('int64_list'):\n            value_list = feature.int64_list.value\n            value_type = self.fs_proto.INT\n        else:\n          # If parsing a tf.FeatureList, get the type and values by iterating\n          # over all Features in the FeatureList.\n          sequence_length = len(feature.feature)\n          if sequence_length != 0 and feature.feature[0].HasField('float_list'):\n            for feat in feature.feature:\n              for value in feat.float_list.value:\n                value_list.append(value)\n            value_type = self.fs_proto.FLOAT\n          elif sequence_length != 0 and feature.feature[0].HasField(\n              'bytes_list'):\n            for feat in feature.feature:\n              for value in feat.bytes_list.value:\n                value_list.append(value)\n            value_type = self.fs_proto.STRING\n          elif sequence_length != 0 and feature.feature[0].HasField(\n              'int64_list'):\n            for feat in feature.feature:\n              for value in feat.int64_list.value:\n                value_list.append(value)\n            value_type = self.fs_proto.INT\n        if value_type is not None:\n          if 'type' not in feature_entry:\n            feature_entry['type'] = value_type\n          elif feature_entry['type'] != value_type:\n            raise TypeError('type mismatch for feature ' + feature_name)\n        feature_entry['counts'].append(len(value_list))\n        feature_entry['vals'].extend(value_list)\n        if sequence_length is not None:\n          feature_entry['feat_lens'].append(sequence_length)\n        if value_list:\n          features_seen.add(feature_name)\n\n    # For all previously-seen features not found in this example, update the\n    # feature's missing value.\n    for f in entries:\n      fv = entries[f]\n      if f not in features_seen:\n        fv['missing'] += 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract examples into a dictionary of feature values.", "response": "def _GetEntries(self,\n                  paths,\n                  max_entries,\n                  iterator_from_file,\n                  is_sequence=False):\n    \"\"\"Extracts examples into a dictionary of feature values.\n\n    Args:\n      paths: A list of the paths to the files to parse.\n      max_entries: The maximum number of examples to load.\n      iterator_from_file: A method that takes a file path string and returns an\n          iterator to the examples in that file.\n      is_sequence: True if the input data from 'iterator_from_file' are\n           tf.SequenceExamples, False if tf.Examples. Defaults to false.\n\n    Returns:\n      A tuple with two elements:\n          - A dictionary of all features parsed thus far and arrays of their\n            values.\n          - The number of examples parsed.\n    \"\"\"\n    entries = {}\n    index = 0\n    for filepath in paths:\n      reader = iterator_from_file(filepath)\n      for record in reader:\n        if is_sequence:\n          sequence_example = tf.train.SequenceExample.FromString(record)\n          self._ParseExample(sequence_example.context.feature,\n                             sequence_example.feature_lists.feature_list,\n                             entries, index)\n        else:\n          self._ParseExample(\n              tf.train.Example.FromString(record).features.feature, [], entries,\n              index)\n        index += 1\n        if index == max_entries:\n          return entries, index\n    return entries, index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract TFRecord examples into a dictionary of feature values.", "response": "def _GetTfRecordEntries(self, path, max_entries, is_sequence,\n                          iterator_options):\n    \"\"\"Extracts TFRecord examples into a dictionary of feature values.\n\n    Args:\n      path: The path to the TFRecord file(s).\n      max_entries: The maximum number of examples to load.\n      is_sequence: True if the input data from 'path' are tf.SequenceExamples,\n           False if tf.Examples. Defaults to false.\n      iterator_options: Options to pass to the iterator that reads the examples.\n          Defaults to None.\n\n    Returns:\n      A tuple with two elements:\n          - A dictionary of all features parsed thus far and arrays of their\n            values.\n          - The number of examples parsed.\n    \"\"\"\n    return self._GetEntries([path], max_entries,\n                            partial(\n                                tf.python_io.tf_record_iterator,\n                                options=iterator_options), is_sequence)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nissue a request to create a new bucket.", "response": "def buckets_insert(self, bucket, project_id=None):\n    \"\"\"Issues a request to create a new bucket.\n\n    Args:\n      bucket: the name of the bucket.\n      project_id: the project to use when inserting the bucket.\n    Returns:\n      A parsed bucket information dictionary.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    args = {'project': project_id if project_id else self._project_id}\n    data = {'name': bucket}\n\n    url = Api._ENDPOINT + (Api._BUCKET_PATH % '')\n    return datalab.utils.Http.request(url, args=args, data=data, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies an object from source to target.", "response": "def objects_copy(self, source_bucket, source_key, target_bucket, target_key):\n    \"\"\"Updates the metadata associated with an object.\n\n    Args:\n      source_bucket: the name of the bucket containing the source object.\n      source_key: the key of the source object being copied.\n      target_bucket: the name of the bucket that will contain the copied object.\n      target_key: the key of the copied object.\n    Returns:\n      A parsed object information dictionary.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._OBJECT_COPY_PATH % (source_bucket, Api._escape_key(source_key),\n                                                    target_bucket, Api._escape_key(target_key)))\n    return datalab.utils.Http.request(url, method='POST', credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef objects_delete(self, bucket, key):\n    url = Api._ENDPOINT + (Api._OBJECT_PATH % (bucket, Api._escape_key(key)))\n    datalab.utils.Http.request(url, method='DELETE', credentials=self._credentials,\n                               raw_response=True)", "response": "Deletes the specified object within the specified bucket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nissuing a request to retrieve information about an object within a bucket.", "response": "def objects_get(self, bucket, key, projection='noAcl'):\n    \"\"\"Issues a request to retrieve information about an object.\n\n    Args:\n      bucket: the name of the bucket.\n      key: the key of the object within the bucket.\n      projection: the projection of the object to retrieve.\n    Returns:\n      A parsed object information dictionary.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    args = {}\n    if projection is not None:\n      args['projection'] = projection\n\n    url = Api._ENDPOINT + (Api._OBJECT_PATH % (bucket, Api._escape_key(key)))\n    return datalab.utils.Http.request(url, args=args, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify_permitted_to_read(gs_path):\n    # TODO(qimingj): Storage APIs need to be modified to allow absence of project\n    #                or credential on Items. When that happens we can move the function\n    #                to Items class.\n    from . import _bucket\n    bucket, prefix = _bucket.parse_name(gs_path)\n    credentials = None\n    if datalab.context.Context.is_signed_in():\n      credentials = datalab.context._utils.get_credentials()\n    args = {\n        'maxResults': Api._MAX_RESULTS,\n        'projection': 'noAcl'\n    }\n    if prefix is not None:\n      args['prefix'] = prefix\n    url = Api._ENDPOINT + (Api._OBJECT_PATH % (bucket, ''))\n    try:\n      datalab.utils.Http.request(url, args=args, credentials=credentials)\n    except datalab.utils.RequestException as e:\n      if e.status == 401:\n        raise Exception('Not permitted to read from specified path. '\n                        'Please sign in and make sure you have read access.')\n      raise e", "response": "Checks if the user is permitted to read from the given GCS path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for the job to complete or a timeout to happen.", "response": "def wait(self, timeout=None):\n    \"\"\" Wait for the job to complete, or a timeout to happen.\n\n      This is more efficient than the version in the base Job class, in that we can\n      use a call that blocks for the poll duration rather than a sleep. That means we\n      shouldn't block unnecessarily long and can also poll less.\n\n    Args:\n      timeout: how long to wait (in seconds) before giving up; default None which means no timeout.\n\n    Returns:\n      The QueryJob\n    \"\"\"\n    poll = 30\n    while not self._is_complete:\n      try:\n        query_result = self._api.jobs_query_results(self._job_id,\n                                                    project_id=self._context.project_id,\n                                                    page_size=0,\n                                                    timeout=poll * 1000)\n      except Exception as e:\n        raise e\n      if query_result['jobComplete']:\n        if 'totalBytesProcessed' in query_result:\n          self._bytes_processed = int(query_result['totalBytesProcessed'])\n        self._cache_hit = query_result.get('cacheHit', None)\n        if 'totalRows' in query_result:\n          self._total_rows = int(query_result['totalRows'])\n        break\n\n      if timeout is not None:\n        timeout -= poll\n        if timeout <= 0:\n          break\n\n    self._refresh_state()\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the table used for the query.", "response": "def results(self):\n    \"\"\" Get the table used for the results of the query. If the query is incomplete, this blocks.\n\n    Raises:\n      Exception if we timed out waiting for results or the query failed.\n    \"\"\"\n    self.wait()\n    if self.failed:\n      raise Exception('Query failed: %s' % str(self.errors))\n    return self._table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of MetricDescriptor objects that match the filters.", "response": "def list(self, pattern='*'):\n    \"\"\"Returns a list of metric descriptors that match the filters.\n\n    Args:\n      pattern: An optional pattern to further filter the descriptors. This can\n          include Unix shell-style wildcards. E.g. ``\"compute*\"``,\n          ``\"*cpu/load_??m\"``.\n\n    Returns:\n      A list of MetricDescriptor objects that match the filters.\n    \"\"\"\n    if self._descriptors is None:\n      self._descriptors = self._client.list_metric_descriptors(\n          filter_string=self._filter_string, type_prefix=self._type_prefix)\n    return [metric for metric in self._descriptors\n            if fnmatch.fnmatch(metric.type, pattern)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_dataframe(self, pattern='*', max_rows=None):\n    data = []\n    for i, metric in enumerate(self.list(pattern)):\n      if max_rows is not None and i >= max_rows:\n        break\n      labels = ', '. join([l.key for l in metric.labels])\n      data.append([\n          metric.type, metric.display_name, metric.metric_kind,\n          metric.value_type, metric.unit, labels])\n\n    return pandas.DataFrame(data, columns=self._DISPLAY_HEADERS)", "response": "Creates a pandas dataframe from the descriptors that match the filters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default query for the module.", "response": "def get_default_query_from_module(module):\n  \"\"\" Given a %%sql module return the default (last) query for the module.\n\n  Args:\n    module: the %%sql module.\n\n  Returns:\n    The default query associated with this module.\n  \"\"\"\n  if isinstance(module, types.ModuleType):\n    return module.__dict__.get(_SQL_MODULE_LAST, None)\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _from_dataframe(dataframe, default_type='STRING'):\n\n    type_mapping = {\n      'i': 'INTEGER',\n      'b': 'BOOLEAN',\n      'f': 'FLOAT',\n      'O': 'STRING',\n      'S': 'STRING',\n      'U': 'STRING',\n      'M': 'TIMESTAMP'\n    }\n\n    fields = []\n    for column_name, dtype in dataframe.dtypes.iteritems():\n      fields.append({'name': column_name,\n                     'type': type_mapping.get(dtype.kind, default_type)})\n\n    return fields", "response": "Infer a BigQuery table schema from a Pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninferring a BigQuery Table schema from a dictionary.", "response": "def _from_dict_record(data):\n    \"\"\"\n    Infer a BigQuery table schema from a dictionary. If the dictionary has entries that\n    are in turn OrderedDicts these will be turned into RECORD types. Ideally this will\n    be an OrderedDict but it is not required.\n\n    Args:\n      data: The dict to infer a schema from.\n    Returns:\n      A list of dictionaries containing field 'name' and 'type' entries, suitable for use in a\n          BigQuery Tables resource schema.\n    \"\"\"\n    return [Schema._get_field_entry(name, value) for name, value in list(data.items())]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _from_list_record(data):\n    return [Schema._get_field_entry('Column%d' % (i + 1), value) for i, value in enumerate(data)]", "response": "Infer a BigQuery table schema from a list of values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _from_record(data):\n    if isinstance(data, dict):\n      return Schema._from_dict_record(data)\n    elif isinstance(data, list):\n      return Schema._from_list_record(data)\n    else:\n      raise Exception('Cannot create a schema from record %s' % str(data))", "response": "Infer a BigQuery table schema from a list of fields or a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninfer a table or view schema from its JSON representation a list of records a dictionary representing a record or a list of homogeneous records.", "response": "def from_data(source):\n    \"\"\"Infers a table/view schema from its JSON representation, a list of records, or a Pandas\n       dataframe.\n\n    Args:\n      source: the Pandas Dataframe, a dictionary representing a record, a list of heterogeneous\n          data (record) or homogeneous data (list of records) from which to infer the schema, or\n          a definition of the schema as a list of dictionaries with 'name' and 'type' entries\n          and possibly 'mode' and 'description' entries. Only used if no data argument was provided.\n          'mode' can be 'NULLABLE', 'REQUIRED' or 'REPEATED'. For the allowed types, see:\n          https://cloud.google.com/bigquery/preparing-data-for-bigquery#datatypes\n\n          Note that there is potential ambiguity when passing a list of lists or a list of\n          dicts between whether that should be treated as a list of records or a single record\n          that is a list. The heuristic used is to check the length of the entries in the\n          list; if they are equal then a list of records is assumed. To avoid this ambiguity\n          you can instead use the Schema.from_record method which assumes a single record,\n          in either list of values or dictionary of key-values form.\n\n    Returns:\n      A Schema for the data.\n    \"\"\"\n    if isinstance(source, pandas.DataFrame):\n      bq_schema = Schema._from_dataframe(source)\n    elif isinstance(source, list):\n      if len(source) == 0:\n        bq_schema = source\n      elif all(isinstance(d, dict) for d in source):\n        if all('name' in d and 'type' in d for d in source):\n          # It looks like a bq_schema; use it as-is.\n          bq_schema = source\n        elif all(len(d) == len(source[0]) for d in source):\n          bq_schema = Schema._from_dict_record(source[0])\n        else:\n          raise Exception(('Cannot create a schema from heterogeneous list %s; perhaps you meant ' +\n                          'to use Schema.from_record?') % str(source))\n      elif isinstance(source[0], list) and \\\n              all([isinstance(l, list) and len(l) == len(source[0]) for l in source]):\n        # A list of lists all of the same length; treat first entry as a list record.\n        bq_schema = Schema._from_record(source[0])\n      else:\n        # A heterogeneous list; treat as a record.\n        raise Exception(('Cannot create a schema from heterogeneous list %s; perhaps you meant ' +\n                        'to use Schema.from_record?') % str(source))\n    elif isinstance(source, dict):\n      raise Exception(('Cannot create a schema from dict %s; perhaps you meant to use ' +\n                      'Schema.from_record?') % str(source))\n    else:\n      raise Exception('Cannot create a schema from %s' % str(source))\n    return Schema(bq_schema)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the index of a field in the flattened list given its fully - qualified name.", "response": "def find(self, name):\n    \"\"\" Get the index of a field in the flattened list given its (fully-qualified) name.\n\n    Args:\n      name: the fully-qualified name of the field.\n    Returns:\n      The index of the field, if found; else -1.\n    \"\"\"\n    for i in range(0, len(self)):\n      if self[i].name == name:\n        return i\n    return -1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the list of arguments that are referenced in the command line.", "response": "def create_args(line, namespace):\n    \"\"\" Expand any meta-variable references in the argument list. \"\"\"\n    args = []\n    # Using shlex.split handles quotes args and escape characters.\n    for arg in shlex.split(line):\n      if not arg:\n         continue\n      if arg[0] == '$':\n        var_name = arg[1:]\n        if var_name in namespace:\n          args.append((namespace[var_name]))\n        else:\n          raise Exception('Undefined variable referenced in command line: %s' % arg)\n      else:\n        args.append(arg)\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, line, namespace=None):\n    try:\n      if namespace is None:\n        ipy = IPython.get_ipython()\n        namespace = ipy.user_ns\n      args = CommandParser.create_args(line, namespace)\n      return self.parse_args(args)\n    except Exception as e:\n      print(str(e))\n      return None", "response": "Parses a line into a dictionary of arguments expanding meta - variables from a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a parser for a sub - command.", "response": "def subcommand(self, name, help):\n    \"\"\"Creates a parser for a sub-command. \"\"\"\n    if self._subcommands is None:\n      self._subcommands = self.add_subparsers(help='commands')\n    return self._subcommands.add_parser(name, description=help, help=help)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary list formatted as a HTML table.", "response": "def render_dictionary(data, headers=None):\n  \"\"\" Return a dictionary list formatted as a HTML table.\n\n  Args:\n    data: the dictionary list\n    headers: the keys in the dictionary to use as table columns, in order.\n  \"\"\"\n  return IPython.core.display.HTML(_html.HtmlBuilder.render_table(data, headers))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render_text(text, preformatted=False):\n  return IPython.core.display.HTML(_html.HtmlBuilder.render_text(text, preformatted))", "response": "Return text formatted as a HTML\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a field list spec into a real list of field names.", "response": "def get_field_list(fields, schema):\n  \"\"\" Convert a field list spec into a real list of field names.\n\n      For tables, we return only the top-level non-RECORD fields as Google charts\n      can't handle nested data.\n  \"\"\"\n  # If the fields weren't supplied get them from the schema.\n  if isinstance(fields, list):\n    return fields\n  if isinstance(fields, basestring) and fields != '*':\n    return fields.split(',')\n  if not schema:\n    return []\n  return [f['name'] for f in schema._bq_schema if f['type'] != 'RECORD']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets column metadata for Google Charts based on field list and schema.", "response": "def _get_cols(fields, schema):\n  \"\"\" Get column metadata for Google Charts based on field list and schema. \"\"\"\n  typemap = {\n    'STRING': 'string',\n    'INT64': 'number',\n    'INTEGER': 'number',\n    'FLOAT': 'number',\n    'FLOAT64': 'number',\n    'BOOL': 'boolean',\n    'BOOLEAN': 'boolean',\n    'DATE': 'date',\n    'TIME': 'timeofday',\n    'DATETIME': 'datetime',\n    'TIMESTAMP': 'datetime'\n  }\n  cols = []\n  for col in fields:\n    if schema:\n      f = schema[col]\n      t = 'string' if f.mode == 'REPEATED' else typemap.get(f.data_type, 'string')\n      cols.append({'id': f.name, 'label': f.name, 'type': t})\n    else:\n      # This will only happen if we had no rows to infer a schema from, so the type\n      # is not really important, except that GCharts will choke if we pass such a schema\n      # to a chart if it is string x string so we default to number.\n      cols.append({'id': col, 'label': col, 'type': 'number'})\n  return cols"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_data_from_empty_list(source, fields='*', first_row=0, count=-1, schema=None):\n  fields = get_field_list(fields, schema)\n  return {'cols': _get_cols(fields, schema), 'rows': []}, 0", "response": "Helper function for _get_data that handles empty lists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling a line from a magic command line.", "response": "def handle_magic_line(line, cell, parser, namespace=None):\n  \"\"\" Helper function for handling magic command lines given a parser with handlers set. \"\"\"\n  args = parser.parse(line, namespace)\n  if args:\n    try:\n      return args.func(vars(args), cell)\n    except Exception as e:\n      sys.stderr.write(str(e))\n      sys.stderr.write('\\n')\n      sys.stderr.flush()\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpands a variable in the user supplied dictionary.", "response": "def expand_var(v, env):\n  \"\"\" If v is a variable reference (for example: '$myvar'), replace it using the supplied\n      env dictionary.\n\n  Args:\n    v: the variable to replace if needed.\n    env: user supplied dictionary.\n\n  Raises:\n    Exception if v is a variable reference but it is not found in env.\n  \"\"\"\n  if len(v) == 0:\n    return v\n  # Using len() and v[0] instead of startswith makes this Unicode-safe.\n  if v[0] == '$':\n    v = v[1:]\n    if len(v) and v[0] != '$':\n      if v in env:\n        v = env[v]\n      else:\n        raise Exception('Cannot expand variable $%s' % v)\n  return v"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing variable references in config using the supplied env dictionary.", "response": "def replace_vars(config, env):\n  \"\"\" Replace variable references in config using the supplied env dictionary.\n\n  Args:\n    config: the config to parse. Can be a tuple, list or dict.\n    env: user supplied dictionary.\n\n  Raises:\n    Exception if any variable references are not found in env.\n  \"\"\"\n  if isinstance(config, dict):\n    for k, v in list(config.items()):\n      if isinstance(v, dict) or isinstance(v, list) or isinstance(v, tuple):\n        replace_vars(v, env)\n      elif isinstance(v, basestring):\n        config[k] = expand_var(v, env)\n  elif isinstance(config, list):\n    for i, v in enumerate(config):\n      if isinstance(v, dict) or isinstance(v, list) or isinstance(v, tuple):\n        replace_vars(v, env)\n      elif isinstance(v, basestring):\n        config[i] = expand_var(v, env)\n  elif isinstance(config, tuple):\n    # TODO(gram): figure out how to handle these if the tuple elements are scalar\n    for v in config:\n      if isinstance(v, dict) or isinstance(v, list) or isinstance(v, tuple):\n        replace_vars(v, env)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_config(config, env, as_dict=True):\n\n  if config is None:\n    return None\n  stripped = config.strip()\n  if len(stripped) == 0:\n    config = {}\n  elif stripped[0] == '{':\n    config = json.loads(config)\n  else:\n    config = yaml.load(config)\n  if as_dict:\n    config = dict(config)\n\n  # Now we need to walk the config dictionary recursively replacing any '$name' vars.\n  replace_vars(config, env)\n  return config", "response": "Parse a config from a magic cell body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate a config dictionary to make sure it includes all required keys and does not include any unexpected keys.", "response": "def validate_config(config, required_keys, optional_keys=None):\n  \"\"\" Validate a config dictionary to make sure it includes all required keys\n      and does not include any unexpected keys.\n\n  Args:\n    config: the config to validate.\n    required_keys: the names of the keys that the config must have.\n    optional_keys: the names of the keys that the config can have.\n\n  Raises:\n    Exception if the config is not a dict or invalid.\n  \"\"\"\n  if optional_keys is None:\n    optional_keys = []\n  if not isinstance(config, dict):\n    raise Exception('config is not dict type')\n  invalid_keys = set(config) - set(required_keys + optional_keys)\n  if len(invalid_keys) > 0:\n    raise Exception('Invalid config with unexpected keys \"%s\"' % ', '.join(e for e in invalid_keys))\n  missing_keys = set(required_keys) - set(config)\n  if len(missing_keys) > 0:\n    raise Exception('Invalid config with missing keys \"%s\"' % ', '.join(missing_keys))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_config_must_have(config, required_keys):\n  missing_keys = set(required_keys) - set(config)\n  if len(missing_keys) > 0:\n    raise Exception('Invalid config with missing keys \"%s\"' % ', '.join(missing_keys))", "response": "Validate a config dictionary to make sure it has all of the specified keys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate a config dictionary to make sure it has one and only one key in one_of_keys.", "response": "def validate_config_has_one_of(config, one_of_keys):\n  \"\"\" Validate a config dictionary to make sure it has one and only one\n      key in one_of_keys.\n\n  Args:\n    config: the config to validate.\n    one_of_keys: the list of possible keys that config can have one and only one.\n\n  Raises:\n    Exception if the config does not have any of them, or multiple of them.\n  \"\"\"\n  intersection = set(config).intersection(one_of_keys)\n  if len(intersection) > 1:\n    raise Exception('Only one of the values in \"%s\" is needed' % ', '.join(intersection))\n  if len(intersection) == 0:\n    raise Exception('One of the values in \"%s\" is needed' % ', '.join(one_of_keys))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_config_value(value, possible_values):\n  if value not in possible_values:\n    raise Exception('Invalid config value \"%s\". Possible values are '\n                    '%s' % (value, ', '.join(e for e in possible_values)))", "response": "Validate a config value to make sure it is one of the possible values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether a given path is a valid GCS path.", "response": "def validate_gcs_path(path, require_object):\n  \"\"\" Check whether a given path is a valid GCS path.\n\n  Args:\n    path: the config to check.\n    require_object: if True, the path has to be an object path but not bucket path.\n\n  Raises:\n    Exception if the path is invalid\n  \"\"\"\n  bucket, key = datalab.storage._bucket.parse_name(path)\n  if bucket is None:\n    raise Exception('Invalid GCS path \"%s\"' % path)\n  if require_object and key is None:\n    raise Exception('It appears the GCS path \"%s\" is a bucket path but not an object path' % path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_control_options(controls, variable_defaults=None):\n  controls_html = ''\n  control_defaults = {}\n  control_ids = []\n  div_id = _html.Html.next_id()\n  if variable_defaults is None:\n    variable_defaults = {}\n  for varname, control in list(controls.items()):\n    label = control.get('label', varname)\n    control_id = div_id + '__' + varname\n    control_ids.append(control_id)\n    value = control.get('value', variable_defaults.get(varname, None))\n    # The user should usually specify the type but we will default to 'textbox' for strings\n    # and 'set' for lists.\n    if isinstance(value, basestring):\n      type = 'textbox'\n    elif isinstance(value, list):\n      type = 'set'\n    else:\n      type = None\n    type = control.get('type', type)\n\n    if type == 'picker':\n      choices = control.get('choices', value)\n      if not isinstance(choices, list) or len(choices) == 0:\n        raise Exception('picker control must specify a nonempty set of choices')\n      if value is None:\n        value = choices[0]\n      choices_html = ''\n      for i, choice in enumerate(choices):\n        choices_html += \"<option value=\\\"%s\\\" %s>%s</option>\" % \\\n                        (choice, (\"selected=\\\"selected\\\"\" if choice == value else ''), choice)\n      control_html = \"{label}<select disabled id=\\\"{id}\\\">{choices}</select>\" \\\n          .format(label=label, id=control_id, choices=choices_html)\n    elif type == 'set':  # Multi-picker; implemented as checkboxes.\n      # TODO(gram): consider using \"name\" property of the control to group checkboxes. That\n      # way we can save the code of constructing and parsing control Ids with sequential\n      #  numbers in it. Multiple checkboxes can share the same name.\n      choices = control.get('choices', value)\n      if not isinstance(choices, list) or len(choices) == 0:\n        raise Exception('set control must specify a nonempty set of choices')\n      if value is None:\n        value = choices\n      choices_html = ''\n      control_ids[-1] = '%s:%d' % (control_id, len(choices))  # replace ID to include count.\n      for i, choice in enumerate(choices):\n        checked = choice in value\n        choice_id = '%s:%d' % (control_id, i)\n        # TODO(gram): we may want a 'Submit/Refresh button as we may not want to rerun\n        # query on each checkbox change.\n        choices_html += \"\"\"\n          <div>\n            <label>\n              <input type=\"checkbox\" id=\"{id}\" value=\"{choice}\" {checked} disabled>\n              {choice}\n            </label>\n          </div>\n        \"\"\".format(id=choice_id, choice=choice, checked=\"checked\" if checked else '')\n      control_html = \"{label}<div>{choices}</div>\".format(label=label, choices=choices_html)\n    elif type == 'checkbox':\n      control_html = \"\"\"\n            <label>\n              <input type=\"checkbox\" id=\"{id}\" {checked} disabled>\n              {label}\n            </label>\n        \"\"\".format(label=label, id=control_id, checked=\"checked\" if value else '')\n    elif type == 'slider':\n      min_ = control.get('min', None)\n      max_ = control.get('max', None)\n      if min_ is None or max_ is None:\n        raise Exception('slider control must specify a min and max value')\n      if max_ <= min_:\n        raise Exception('slider control must specify a min value less than max value')\n      step = control.get('step', 1 if isinstance(min_, int) and isinstance(max_, int)\n                         else (float(max_ - min_) / 10.0))\n      if value is None:\n        value = min_\n      control_html = \"\"\"\n        {label}\n        <input type=\"text\" class=\"gchart-slider_value\" id=\"{id}_value\" value=\"{value}\" disabled/>\n        <input type=\"range\" class=\"gchart-slider\" id=\"{id}\" min=\"{min}\" max=\"{max}\" step=\"{step}\"\n            value=\"{value}\" disabled/>\n      \"\"\".format(label=label, id=control_id, value=value, min=min_, max=max_, step=step)\n    elif type == 'textbox':\n      if value is None:\n        value = ''\n      control_html = \"{label}<input type=\\\"text\\\" value=\\\"{value}\\\" id=\\\"{id}\\\" disabled/>\" \\\n          .format(label=label, value=value, id=control_id)\n    else:\n      raise Exception(\n          'Unknown control type %s (expected picker, slider, checkbox, textbox or set)' % type)\n\n    control_defaults[varname] = value\n    controls_html += \"<div class=\\\"gchart-control\\\">{control}</div>\\n\" \\\n        .format(control=control_html)\n\n  controls_html = \"<div class=\\\"gchart-controls\\\">{controls}</div>\".format(controls=controls_html)\n  return controls_html, control_defaults, control_ids", "response": "Parse a set of control options."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a profile of data in a dataframe.", "response": "def profile_df(df):\n  \"\"\" Generate a profile of data in a dataframe.\n\n  Args:\n    df: the Pandas dataframe.\n  \"\"\"\n  # The bootstrap CSS messes up the Datalab display so we tweak it to not have an effect.\n  # TODO(gram): strip it out rather than this kludge.\n  return IPython.core.display.HTML(\n      pandas_profiling.ProfileReport(df).html.replace('bootstrap', 'nonexistent'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _assert_gcs_files(files):\n\n  if sys.version_info.major > 2:\n    string_type = (str, bytes)  # for python 3 compatibility\n  else:\n    string_type = basestring  # noqa\n\n  if isinstance(files, string_type):\n    files = [files]\n\n  for f in files:\n    if f is not None and not f.startswith('gs://'):\n      raise ValueError('File %s is not a gcs path' % f)", "response": "Checks that a file path is a gcs path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrepackages this package from local installed location and copy it to GCS.", "response": "def _package_to_staging(staging_package_url):\n    \"\"\"Repackage this package from local installed location and copy it to GCS.\n\n    Args:\n      staging_package_url: GCS path.\n    \"\"\"\n    import google.datalab.ml as ml\n\n    # Find the package root. __file__ is under [package_root]/mltoolbox/_structured_data/this_file\n    package_root = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), '../../'))\n    setup_path = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), 'master_setup.py'))\n    tar_gz_path = os.path.join(staging_package_url, 'staging', 'trainer.tar.gz')\n\n    print('Building package and uploading to %s' % tar_gz_path)\n    ml.package_and_copy(package_root, setup_path, tar_gz_path)\n\n    return tar_gz_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analyze(output_dir, dataset, cloud=False, project_id=None):\n  job = analyze_async(\n      output_dir=output_dir,\n      dataset=dataset,\n      cloud=cloud,\n      project_id=project_id)\n  job.wait()\n  print('Analyze: ' + str(job.state))", "response": "Blocking version of analyze_async. See documentation of analyze_async."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nanalyze data locally or in the cloud with BigQuery.", "response": "def analyze_async(output_dir, dataset, cloud=False, project_id=None):\n  \"\"\"Analyze data locally or in the cloud with BigQuery.\n\n  Produce analysis used by training. This can take a while, even for small\n  datasets. For small datasets, it may be faster to use local_analysis.\n\n  Args:\n    output_dir: The output directory to use.\n    dataset: only CsvDataSet is supported currently.\n    cloud: If False, runs analysis locally with Pandas. If Ture, runs analysis\n        in the cloud with BigQuery.\n    project_id: Uses BigQuery with this project id. Default is datalab's\n        default project id.\n\n  Returns:\n    A google.datalab.utils.Job object that can be used to query state from or wait.\n  \"\"\"\n  import google.datalab.utils as du\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fn = lambda: _analyze(output_dir, dataset, cloud, project_id)  # noqa\n    return du.LambdaJob(fn, job_id=None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntrains the model locally or in the cloud.", "response": "def train_async(train_dataset,\n                eval_dataset,\n                analysis_dir,\n                output_dir,\n                features,\n                model_type,\n                max_steps=5000,\n                num_epochs=None,\n                train_batch_size=100,\n                eval_batch_size=16,\n                min_eval_frequency=100,\n                top_n=None,\n                layer_sizes=None,\n                learning_rate=0.01,\n                epsilon=0.0005,\n                job_name=None,  # cloud param\n                job_name_prefix='',  # cloud param\n                cloud=None,  # cloud param\n                ):\n  # NOTE: if you make a chane go this doc string, you MUST COPY it 4 TIMES in\n  # mltoolbox.{classification|regression}.{dnn|linear}, but you must remove\n  # the model_type parameter, and maybe change the layer_sizes and top_n\n  # parameters!\n  # Datalab does some tricky things and messing with train.__doc__ will\n  # not work!\n  \"\"\"Train model locally or in the cloud.\n\n  Local Training:\n\n  Args:\n    train_dataset: CsvDataSet\n    eval_dataset: CsvDataSet\n    analysis_dir:  The output directory from local_analysis\n    output_dir:  Output directory of training.\n    features: file path or features object. Example:\n        {\n          \"col_A\": {\"transform\": \"scale\", \"default\": 0.0},\n          \"col_B\": {\"transform\": \"scale\",\"value\": 4},\n          # Note col_C is missing, so default transform used.\n          \"col_D\": {\"transform\": \"hash_one_hot\", \"hash_bucket_size\": 4},\n          \"col_target\": {\"transform\": \"target\"},\n          \"col_key\": {\"transform\": \"key\"}\n        }\n        The keys correspond to the columns in the input files as defined by the\n        schema file during preprocessing. Some notes\n        1) The \"key\" and \"target\" transforms are required.\n        2) Default values are optional. These are used if the input data has\n           missing values during training and prediction. If not supplied for a\n           column, the default value for a numerical column is that column's\n           mean vlaue, and for a categorical column the empty string is used.\n        3) For numerical colums, the following transforms are supported:\n           i) {\"transform\": \"identity\"}: does nothing to the number. (default)\n           ii) {\"transform\": \"scale\"}: scales the colum values to -1, 1.\n           iii) {\"transform\": \"scale\", \"value\": a}: scales the colum values\n              to -a, a.\n\n           For categorical colums, the following transforms are supported:\n          i) {\"transform\": \"one_hot\"}: A one-hot vector using the full\n              vocabulary is used. (default)\n          ii) {\"transform\": \"embedding\", \"embedding_dim\": d}: Each label is\n              embedded into an d-dimensional space.\n    model_type: One of 'linear_classification', 'linear_regression',\n        'dnn_classification', 'dnn_regression'.\n    max_steps: Int. Number of training steps to perform.\n    num_epochs: Maximum number of training data epochs on which to train.\n        The training job will run for max_steps or num_epochs, whichever occurs\n        first.\n    train_batch_size: number of rows to train on in one step.\n    eval_batch_size: number of rows to eval in one step. One pass of the eval\n        dataset is done. If eval_batch_size does not perfectly divide the numer\n        of eval instances, the last fractional batch is not used.\n    min_eval_frequency: Minimum number of training steps between evaluations.\n    top_n: Int. For classification problems, the output graph will contain the\n        labels and scores for the top n classes with a default of n=1. Use\n        None for regression problems.\n    layer_sizes: List. Represents the layers in the connected DNN.\n        If the model type is DNN, this must be set. Example [10, 3, 2], this\n        will create three DNN layers where the first layer will have 10 nodes,\n        the middle layer will have 3 nodes, and the laster layer will have 2\n        nodes.\n    learning_rate: tf.train.AdamOptimizer's learning rate,\n    epsilon: tf.train.AdamOptimizer's epsilon value.\n\n  Cloud Training:\n\n  Args:\n    All local training arguments are valid for cloud training. Cloud training\n    contains two additional args:\n\n    cloud: A CloudTrainingConfig object.\n    job_name: Training job name. A default will be picked if None.\n    job_name_prefix: If job_name is None, the job will be named\n        '<job_name_prefix>_<timestamp>'.\n\n  Returns:\n    A google.datalab.utils.Job object that can be used to query state from or wait.\n  \"\"\"\n  import google.datalab.utils as du\n\n  if model_type not in ['linear_classification', 'linear_regression', 'dnn_classification',\n                        'dnn_regression']:\n    raise ValueError('Unknown model_type %s' % model_type)\n\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    if cloud:\n      return cloud_train(\n          train_dataset=train_dataset,\n          eval_dataset=eval_dataset,\n          analysis_dir=analysis_dir,\n          output_dir=output_dir,\n          features=features,\n          model_type=model_type,\n          max_steps=max_steps,\n          num_epochs=num_epochs,\n          train_batch_size=train_batch_size,\n          eval_batch_size=eval_batch_size,\n          min_eval_frequency=min_eval_frequency,\n          top_n=top_n,\n          layer_sizes=layer_sizes,\n          learning_rate=learning_rate,\n          epsilon=epsilon,\n          job_name=job_name,\n          job_name_prefix=job_name_prefix,\n          config=cloud,\n      )\n    else:\n      def fn():\n        return local_train(\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            analysis_dir=analysis_dir,\n            output_dir=output_dir,\n            features=features,\n            model_type=model_type,\n            max_steps=max_steps,\n            num_epochs=num_epochs,\n            train_batch_size=train_batch_size,\n            eval_batch_size=eval_batch_size,\n            min_eval_frequency=min_eval_frequency,\n            top_n=top_n,\n            layer_sizes=layer_sizes,\n            learning_rate=learning_rate,\n            epsilon=epsilon)\n      return du.LambdaJob(fn, job_id=None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cloud_train(train_dataset,\n                eval_dataset,\n                analysis_dir,\n                output_dir,\n                features,\n                model_type,\n                max_steps,\n                num_epochs,\n                train_batch_size,\n                eval_batch_size,\n                min_eval_frequency,\n                top_n,\n                layer_sizes,\n                learning_rate,\n                epsilon,\n                job_name,\n                job_name_prefix,\n                config):\n  \"\"\"Train model using CloudML.\n\n  See local_train() for a description of the args.\n  Args:\n    config: A CloudTrainingConfig object.\n    job_name: Training job name. A default will be picked if None.\n  \"\"\"\n  import google.datalab.ml as ml\n\n  if len(train_dataset.input_files) != 1 or len(eval_dataset.input_files) != 1:\n    raise ValueError('CsvDataSets must be built with a file pattern, not list '\n                     'of files.')\n\n  if file_io.file_exists(output_dir):\n    raise ValueError('output_dir already exist. Use a new output path.')\n\n  if isinstance(features, dict):\n    # Make a features file.\n    if not file_io.file_exists(output_dir):\n      file_io.recursive_create_dir(output_dir)\n    features_file = os.path.join(output_dir, 'features_file.json')\n    file_io.write_string_to_file(\n        features_file,\n        json.dumps(features))\n  else:\n    features_file = features\n\n  if not isinstance(config, ml.CloudTrainingConfig):\n    raise ValueError('cloud should be an instance of '\n                     'google.datalab.ml.CloudTrainingConfig for cloud training.')\n\n  _assert_gcs_files([output_dir, train_dataset.input_files[0], eval_dataset.input_files[0],\n                     features_file, analysis_dir])\n\n  args = ['--train-data-paths=%s' % train_dataset.input_files[0],\n          '--eval-data-paths=%s' % eval_dataset.input_files[0],\n          '--preprocess-output-dir=%s' % analysis_dir,\n          '--transforms-file=%s' % features_file,\n          '--model-type=%s' % model_type,\n          '--max-steps=%s' % str(max_steps),\n          '--train-batch-size=%s' % str(train_batch_size),\n          '--eval-batch-size=%s' % str(eval_batch_size),\n          '--min-eval-frequency=%s' % str(min_eval_frequency),\n          '--learning-rate=%s' % str(learning_rate),\n          '--epsilon=%s' % str(epsilon)]\n  if num_epochs:\n    args.append('--num-epochs=%s' % str(num_epochs))\n  if top_n:\n    args.append('--top-n=%s' % str(top_n))\n  if layer_sizes:\n    for i in range(len(layer_sizes)):\n      args.append('--layer-size%s=%s' % (i + 1, str(layer_sizes[i])))\n\n  job_request = {\n    'package_uris': [_package_to_staging(output_dir), _TF_GS_URL, _PROTOBUF_GS_URL],\n    'python_module': 'mltoolbox._structured_data.trainer.task',\n    'job_dir': output_dir,\n    'args': args\n  }\n  job_request.update(dict(config._asdict()))\n\n  if not job_name:\n    job_name = job_name_prefix or 'structured_data_train'\n    job_name += '_' + datetime.datetime.now().strftime('%y%m%d_%H%M%S')\n  job = ml.Job.submit_training(job_request, job_name)\n  print('Job request send. View status of job at')\n  print('https://console.developers.google.com/ml/jobs?project=%s' %\n        _default_project())\n\n  return job", "response": "Train a CloudML model using CloudML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun prediction locally or on the cloud.", "response": "def predict(data, training_dir=None, model_name=None, model_version=None, cloud=False):\n  \"\"\"Runs prediction locally or on the cloud.\n\n  Args:\n    data: List of csv strings or a Pandas DataFrame that match the model schema.\n    training_dir: local path to the trained output folder.\n    model_name: deployed model name\n    model_version: depoyed model version\n    cloud: bool. If False, does local prediction and data and training_dir\n        must be set. If True, does cloud prediction and data, model_name,\n        and model_version must be set.\n\n\n  For cloud prediction, the model must be created. This can be done by running\n  two gcloud commands::\n    1) gcloud beta ml models create NAME\n    2) gcloud beta ml versions create VERSION --model NAME --origin gs://BUCKET/training_dir/model\n  or these datalab commands:\n    1) import google.datalab as datalab\n      model = datalab.ml.ModelVersions(MODEL_NAME)\n      model.deploy(version_name=VERSION, path='gs://BUCKET/training_dir/model')\n  Note that the model must be on GCS.\n\n  Returns:\n    Pandas DataFrame.\n  \"\"\"\n  if cloud:\n    if not model_version or not model_name:\n      raise ValueError('model_version or model_name is not set')\n    if training_dir:\n      raise ValueError('training_dir not needed when cloud is True')\n    with warnings.catch_warnings():\n      warnings.simplefilter(\"ignore\")\n      return cloud_predict(model_name, model_version, data)\n  else:\n    if not training_dir:\n      raise ValueError('training_dir is not set')\n    if model_version or model_name:\n      raise ValueError('model_name and model_version not needed when cloud is '\n                       'False.')\n    with warnings.catch_warnings():\n      warnings.simplefilter(\"ignore\")\n      return local_predict(training_dir, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef local_predict(training_dir, data):\n  # from . import predict as predict_module\n  from .prediction import predict as predict_module\n\n  # Save the instances to a file, call local batch prediction, and return it\n  tmp_dir = tempfile.mkdtemp()\n  _, input_file_path = tempfile.mkstemp(dir=tmp_dir, suffix='.csv',\n                                        prefix='input')\n\n  try:\n    if isinstance(data, pd.DataFrame):\n      data.to_csv(input_file_path, header=False, index=False)\n    else:\n      with open(input_file_path, 'w') as f:\n        for line in data:\n          f.write(line + '\\n')\n\n    model_dir = os.path.join(training_dir, 'model')\n    if not file_io.file_exists(model_dir):\n      raise ValueError('training_dir should contain the folder model')\n\n    cmd = ['predict.py',\n           '--predict-data=%s' % input_file_path,\n           '--trained-model-dir=%s' % model_dir,\n           '--output-dir=%s' % tmp_dir,\n           '--output-format=csv',\n           '--batch-size=16',\n           '--mode=prediction',\n           '--no-shard-files']\n\n    # runner_results = predict_module.predict.main(cmd)\n    runner_results = predict_module.main(cmd)\n    runner_results.wait_until_finish()\n\n    # Read the header file.\n    schema_file = os.path.join(tmp_dir, 'csv_schema.json')\n    with open(schema_file, 'r') as f:\n      schema = json.loads(f.read())\n\n    # Print any errors to the screen.\n    errors_file = glob.glob(os.path.join(tmp_dir, 'errors*'))\n    if errors_file and os.path.getsize(errors_file[0]) > 0:\n      print('Warning: there are errors. See below:')\n      with open(errors_file[0], 'r') as f:\n        text = f.read()\n        print(text)\n\n    # Read the predictions data.\n    prediction_file = glob.glob(os.path.join(tmp_dir, 'predictions*'))\n    if not prediction_file:\n      raise FileNotFoundError('Prediction results not found')\n    predictions = pd.read_csv(prediction_file[0],\n                              header=None,\n                              names=[col['name'] for col in schema])\n    return predictions\n  finally:\n    shutil.rmtree(tmp_dir)", "response": "Runs local prediction on the prediction graph and saves the results in a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nblocking versoin of batch_predict.", "response": "def batch_predict(training_dir, prediction_input_file, output_dir,\n                  mode, batch_size=16, shard_files=True, output_format='csv',\n                  cloud=False):\n  \"\"\"Blocking versoin of batch_predict.\n\n  See documentation of batch_prediction_async.\n  \"\"\"\n  job = batch_predict_async(\n      training_dir=training_dir,\n      prediction_input_file=prediction_input_file,\n      output_dir=output_dir,\n      mode=mode,\n      batch_size=batch_size,\n      shard_files=shard_files,\n      output_format=output_format,\n      cloud=cloud)\n  job.wait()\n  print('Batch predict: ' + str(job.state))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbatch prediction for the current node.", "response": "def batch_predict_async(training_dir, prediction_input_file, output_dir,\n                        mode, batch_size=16, shard_files=True, output_format='csv', cloud=False):\n  \"\"\"Local and cloud batch prediction.\n\n  Args:\n    training_dir: The output folder of training.\n    prediction_input_file: csv file pattern to a file. File must be on GCS if\n        running cloud prediction\n    output_dir: output location to save the results. Must be a GSC path if\n        running cloud prediction.\n    mode: 'evaluation' or 'prediction'. If 'evaluation', the input data must\n        contain a target column. If 'prediction', the input data must not\n        contain a target column.\n    batch_size: Int. How many instances to run in memory at once. Larger values\n        mean better performace but more memeory consumed.\n    shard_files: If False, the output files are not shardded.\n    output_format: csv or json. Json file are json-newlined.\n    cloud: If ture, does cloud batch prediction. If False, runs batch prediction\n        locally.\n\n  Returns:\n    A google.datalab.utils.Job object that can be used to query state from or wait.\n  \"\"\"\n  import google.datalab.utils as du\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    if cloud:\n      runner_results = cloud_batch_predict(training_dir, prediction_input_file, output_dir, mode,\n                                           batch_size, shard_files, output_format)\n      job = du.DataflowJob(runner_results)\n    else:\n      runner_results = local_batch_predict(training_dir, prediction_input_file, output_dir, mode,\n                                           batch_size, shard_files, output_format)\n      job = du.LambdaJob(lambda: runner_results.wait_until_finish(), job_id=None)\n\n  return job"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing local predict and local prediction.", "response": "def local_batch_predict(training_dir, prediction_input_file, output_dir, mode, batch_size,\n                        shard_files, output_format):\n  \"\"\"See batch_predict\"\"\"\n  # from . import predict as predict_module\n  from .prediction import predict as predict_module\n\n  if mode == 'evaluation':\n    model_dir = os.path.join(training_dir, 'evaluation_model')\n  elif mode == 'prediction':\n    model_dir = os.path.join(training_dir, 'model')\n  else:\n    raise ValueError('mode must be evaluation or prediction')\n\n  if not file_io.file_exists(model_dir):\n    raise ValueError('Model folder %s does not exist' % model_dir)\n\n  cmd = ['predict.py',\n         '--predict-data=%s' % prediction_input_file,\n         '--trained-model-dir=%s' % model_dir,\n         '--output-dir=%s' % output_dir,\n         '--output-format=%s' % output_format,\n         '--batch-size=%s' % str(batch_size),\n         '--shard-files' if shard_files else '--no-shard-files',\n         '--has-target' if mode == 'evaluation' else '--no-has-target'\n         ]\n\n  # return predict_module.predict.main(cmd)\n  return predict_module.main(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbatches predict on the model.", "response": "def cloud_batch_predict(training_dir, prediction_input_file, output_dir, mode, batch_size,\n                        shard_files, output_format):\n  \"\"\"See batch_predict\"\"\"\n  # from . import predict as predict_module\n  from .prediction import predict as predict_module\n\n  if mode == 'evaluation':\n    model_dir = os.path.join(training_dir, 'evaluation_model')\n  elif mode == 'prediction':\n    model_dir = os.path.join(training_dir, 'model')\n  else:\n    raise ValueError('mode must be evaluation or prediction')\n\n  if not file_io.file_exists(model_dir):\n    raise ValueError('Model folder %s does not exist' % model_dir)\n\n  _assert_gcs_files([training_dir, prediction_input_file, output_dir])\n\n  cmd = ['predict.py',\n         '--cloud',\n         '--project-id=%s' % _default_project(),\n         '--predict-data=%s' % prediction_input_file,\n         '--trained-model-dir=%s' % model_dir,\n         '--output-dir=%s' % output_dir,\n         '--output-format=%s' % output_format,\n         '--batch-size=%s' % str(batch_size),\n         '--shard-files' if shard_files else '--no-shard-files',\n         '--extra-package=%s' % _TF_GS_URL,\n         '--extra-package=%s' % _PROTOBUF_GS_URL,\n         '--extra-package=%s' % _package_to_staging(output_dir)\n         ]\n\n  return predict_module.main(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_arguments(argv):\n  parser = argparse.ArgumentParser(\n      description='Runs Prediction inside a beam or Dataflow job.')\n  # cloud options\n  parser.add_argument('--project-id',\n                      help='The project to which the job will be submitted.')\n  parser.add_argument('--cloud',\n                      action='store_true',\n                      help='Run preprocessing on the cloud.')\n  parser.add_argument('--job-name',\n                      default=('mltoolbox-batch-prediction-' +\n                               datetime.datetime.now().strftime('%Y%m%d%H%M%S')),\n                      help='Dataflow job name. Must be unique over all jobs.')\n  parser.add_argument('--extra-package',\n                      default=[],\n                      action='append',\n                      help=('If using --cloud, also installs these packages on '\n                            'each dataflow worker'))\n\n  # I/O args\n  parser.add_argument('--predict-data',\n                      required=True,\n                      help='Data to run prediction on')\n  parser.add_argument('--trained-model-dir',\n                      required=True,\n                      help='Usually train_output_path/model.')\n  parser.add_argument('--output-dir',\n                      required=True,\n                      help=('Location to save output.'))\n\n  # Other args\n  parser.add_argument('--batch-size',\n                      required=False,\n                      default=1000,\n                      type=int,\n                      help=('Batch size. Larger values consumes more memrory '\n                            'but takes less time to finish.'))\n  parser.add_argument('--shard-files',\n                      dest='shard_files',\n                      action='store_true',\n                      help='Shard files')\n  parser.add_argument('--no-shard-files',\n                      dest='shard_files',\n                      action='store_false',\n                      help='Don\\'t shard files')\n  parser.set_defaults(shard_files=True)\n\n  parser.add_argument('--output-format',\n                      choices=['csv', 'json'],\n                      default='csv',\n                      help=\"\"\"\n      The output results.\n        raw_json: produces a newline file where each line is json. No\n            post processing is performed and the output matches what the trained\n            model produces.\n        csv: produces a csv file without a header row and a header csv file.\n            For classification problems, the vector of probabalities for each\n            target class is split into individual csv columns.\"\"\")\n\n  args, _ = parser.parse_known_args(args=argv[1:])\n\n  if args.cloud:\n    if not args.project_id:\n      raise ValueError('--project-id needed with --cloud')\n    if not args.trained_model_dir.startswith('gs://'):\n      raise ValueError('--trained-model-dir needs to be a GCS path,')\n    if not args.output_dir.startswith('gs://'):\n      raise ValueError('--output-dir needs to be a GCS path.')\n    if not args.predict_data.startswith('gs://'):\n      raise ValueError('--predict-data needs to be a GCS path.')\n\n  return args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_prediction_pipeline(pipeline, args):\n\n  # DF bug: DF does not work with unicode strings\n  predicted_values, errors = (\n      pipeline |\n      'Read CSV Files' >>\n      beam.io.ReadFromText(str(args.predict_data),\n                           strip_trailing_newlines=True) |\n      'Batch Input' >>\n      beam.ParDo(EmitAsBatchDoFn(args.batch_size)) |\n      'Run TF Graph on Batches' >>\n      beam.ParDo(RunGraphDoFn(args.trained_model_dir)).with_outputs('errors', main='main'))\n\n  ((predicted_values, errors) |\n   'Format and Save' >>\n   FormatAndSave(args))", "response": "Builds the prediction pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process(self, element):\n    import collections\n    import apache_beam as beam\n\n    num_in_batch = 0\n    try:\n      assert self._session is not None\n\n      feed_dict = collections.defaultdict(list)\n      for line in element:\n\n        # Remove trailing newline.\n        if line.endswith('\\n'):\n          line = line[:-1]\n\n        feed_dict[self._input_alias_map.values()[0]].append(line)\n        num_in_batch += 1\n\n      # batch_result is list of numpy arrays with batch_size many rows.\n      batch_result = self._session.run(fetches=self._tensor_names,\n                                       feed_dict=feed_dict)\n\n      # ex batch_result for batch_size > 1:\n      # (array([value1, value2, ..., value_batch_size]),\n      #  array([[a1, b1, c1]], ..., [a_batch_size, b_batch_size, c_batch_size]]),\n      #  ...)\n      # ex batch_result for batch_size == 1:\n      # (value,\n      #  array([a1, b1, c1]),\n      #  ...)\n\n      # Convert the results into a dict and unbatch the results.\n      if num_in_batch > 1:\n        for result in zip(*batch_result):\n          predictions = {}\n          for name, value in zip(self._aliases, result):\n            predictions[name] = (value.tolist() if getattr(value, 'tolist', None) else value)\n          yield predictions\n      else:\n        predictions = {}\n        for i in range(len(self._aliases)):\n          value = batch_result[i]\n          value = (value.tolist() if getattr(value, 'tolist', None)\n                   else value)\n          predictions[self._aliases[i]] = value\n        yield predictions\n\n    except Exception as e:  # pylint: disable=broad-except\n      yield beam.pvalue.TaggedOutput('errors', (str(e), element))", "response": "Run batch prediciton on a TF graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(self, tf_graph_predictions):\n    row = []\n    for col in self._header:\n      row.append(str(tf_graph_predictions[col]))\n\n    return ','.join(row)", "response": "Encodes the graph json prediction into csv."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a pandas dataframe from the query metadata.", "response": "def as_dataframe(self, max_rows=None):\n      \"\"\"Creates a pandas dataframe from the query metadata.\n\n      Args:\n        max_rows: The maximum number of timeseries metadata to return. If None,\n            return all.\n\n      Returns:\n        A pandas dataframe containing the resource type, resource labels and\n        metric labels. Each row in this dataframe corresponds to the metadata\n        from one time series.\n      \"\"\"\n      max_rows = len(self._timeseries_list) if max_rows is None else max_rows\n      headers = [{\n          'resource': ts.resource._asdict(), 'metric': ts.metric._asdict()}\n          for ts in self._timeseries_list[:max_rows]]\n\n      if not headers:\n        return pandas.DataFrame()\n\n      dataframe = pandas.io.json.json_normalize(headers)\n\n      # Add a 2 level column header.\n      dataframe.columns = pandas.MultiIndex.from_tuples(\n          [(col, '') if col == 'resource.type' else col.rsplit('.', 1)\n           for col in dataframe.columns])\n\n      # Re-order the columns.\n      resource_keys = google.cloud.monitoring._dataframe._sorted_resource_labels(\n          dataframe['resource.labels'].columns)\n      sorted_columns = [('resource.type', '')]\n      sorted_columns += [('resource.labels', key) for key in resource_keys]\n      sorted_columns += sorted(col for col in dataframe.columns\n                               if col[0] == 'metric.labels')\n      dataframe = dataframe[sorted_columns]\n\n      # Sort the data, and clean up index values, and NaNs.\n      dataframe = dataframe.sort_values(sorted_columns)\n      dataframe = dataframe.reset_index(drop=True).fillna('')\n      return dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting preprocessed training and eval files.", "response": "def get_train_eval_files(input_dir):\n  \"\"\"Get preprocessed training and eval files.\"\"\"\n  data_dir = _get_latest_data_dir(input_dir)\n  train_pattern = os.path.join(data_dir, 'train*.tfrecord.gz')\n  eval_pattern = os.path.join(data_dir, 'eval*.tfrecord.gz')\n  train_files = file_io.get_matching_files(train_pattern)\n  eval_files = file_io.get_matching_files(eval_pattern)\n  return train_files, eval_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_labels(input_dir):\n  data_dir = _get_latest_data_dir(input_dir)\n  labels_file = os.path.join(data_dir, 'labels')\n  with file_io.FileIO(labels_file, 'r') as f:\n    labels = f.read().rstrip().split('\\n')\n  return labels", "response": "Get a list of labels from preprocessed output dir."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread examples from a list of input files and returns a list of examples.", "response": "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\n  \"\"\"Creates readers and queues for reading example protos.\"\"\"\n  files = []\n  for e in input_files:\n    for path in e.split(','):\n      files.extend(file_io.get_matching_files(path))\n  thread_count = multiprocessing.cpu_count()\n\n  # The minimum number of instances in a queue from which examples are drawn\n  # randomly. The larger this number, the more randomness at the expense of\n  # higher memory requirements.\n  min_after_dequeue = 1000\n\n  # When batching data, the queue's capacity will be larger than the batch_size\n  # by some factor. The recommended formula is (num_threads + a small safety\n  # margin). For now, we use a single thread for reading, so this can be small.\n  queue_size_multiplier = thread_count + 3\n\n  # Convert num_epochs == 0 -> num_epochs is None, if necessary\n  num_epochs = num_epochs or None\n\n  # Build a queue of the filenames to be read.\n  filename_queue = tf.train.string_input_producer(files, num_epochs, shuffle)\n\n  options = tf.python_io.TFRecordOptions(\n      compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n  example_id, encoded_example = tf.TFRecordReader(options=options).read_up_to(\n      filename_queue, batch_size)\n\n  if shuffle:\n    capacity = min_after_dequeue + queue_size_multiplier * batch_size\n    return tf.train.shuffle_batch(\n        [example_id, encoded_example],\n        batch_size,\n        capacity,\n        min_after_dequeue,\n        enqueue_many=True,\n        num_threads=thread_count)\n  else:\n    capacity = queue_size_multiplier * batch_size\n    return tf.train.batch(\n        [example_id, encoded_example],\n        batch_size,\n        capacity=capacity,\n        enqueue_many=True,\n        num_threads=thread_count)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef override_if_not_in_args(flag, argument, args):\n  if flag not in args:\n    args.extend([flag, argument])", "response": "Checks if flags is in args adds the flag to args."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate aggregated mean loss.", "response": "def loss(loss_value):\n  \"\"\"Calculates aggregated mean loss.\"\"\"\n  total_loss = tf.Variable(0.0, False)\n  loss_count = tf.Variable(0, False)\n  total_loss_update = tf.assign_add(total_loss, loss_value)\n  loss_count_update = tf.assign_add(loss_count, 1)\n  loss_op = total_loss / tf.cast(loss_count, tf.float32)\n  return [total_loss_update, loss_count_update], loss_op"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate we have a good dataset.", "response": "def check_dataset(dataset, mode):\n  \"\"\"Validate we have a good dataset.\"\"\"\n\n  names = [x['name'] for x in dataset.schema]\n  types = [x['type'] for x in dataset.schema]\n  if mode == 'train':\n    if (set(['image_url', 'label']) != set(names) or any(t != 'STRING' for t in types)):\n      raise ValueError('Invalid dataset. Expect only \"image_url,label\" STRING columns.')\n  else:\n    if (set(['image_url']) != set(names) and set(['image_url', 'label']) != set(names)) or \\\n            any(t != 'STRING' for t in types):\n      raise ValueError('Invalid dataset. Expect only \"image_url\" or \"image_url,label\" ' +\n                       'STRING columns.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets pcollection from dataset.", "response": "def get_sources_from_dataset(p, dataset, mode):\n  \"\"\"get pcollection from dataset.\"\"\"\n\n  import apache_beam as beam\n  import csv\n  from google.datalab.ml import CsvDataSet, BigQueryDataSet\n\n  check_dataset(dataset, mode)\n  if type(dataset) is CsvDataSet:\n    source_list = []\n    for ii, input_path in enumerate(dataset.files):\n      source_list.append(p | 'Read from Csv %d (%s)' % (ii, mode) >>\n                         beam.io.ReadFromText(input_path, strip_trailing_newlines=True))\n    return (source_list |\n            'Flatten Sources (%s)' % mode >>\n            beam.Flatten() |\n            'Create Dict from Csv (%s)' % mode >>\n            beam.Map(lambda line: csv.DictReader([line], fieldnames=['image_url',\n                                                                     'label']).next()))\n  elif type(dataset) is BigQueryDataSet:\n    bq_source = (beam.io.BigQuerySource(table=dataset.table) if dataset.table is not None else\n                 beam.io.BigQuerySource(query=dataset.query))\n    return p | 'Read source from BigQuery (%s)' % mode >> beam.io.Read(bq_source)\n  else:\n    raise ValueError('Invalid DataSet. Expect CsvDataSet or BigQueryDataSet')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_and_resize(image_str_tensor):\n\n  # These constants are set by Inception v3's expectations.\n  height = 299\n  width = 299\n  channels = 3\n\n  image = tf.image.decode_jpeg(image_str_tensor, channels=channels)\n  # Note resize expects a batch_size, but tf_map supresses that index,\n  # thus we have to expand then squeeze.  Resize returns float32 in the\n  # range [0, uint8_max]\n  image = tf.expand_dims(image, 0)\n  image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\n  image = tf.squeeze(image, squeeze_dims=[0])\n  image = tf.cast(image, dtype=tf.uint8)\n  return image", "response": "Decodes jpeg string resizes it and returns a uint8 tensor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resize_image(image_str_tensor):\n\n  image = decode_and_resize(image_str_tensor)\n  image = tf.image.encode_jpeg(image, quality=100)\n  return image", "response": "Decodes jpeg string resizes it and re - encode it to jpeg."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_images(image_files, resize=True):\n\n  images = []\n  for image_file in image_files:\n    with file_io.FileIO(image_file, 'r') as ff:\n      images.append(ff.read())\n  if resize is False:\n    return images\n\n  # To resize, run a tf session so we can reuse 'decode_and_resize()'\n  # which is used in prediction graph. This makes sure we don't lose\n  # any quality in prediction, while decreasing the size of the images\n  # submitted to the model over network.\n  image_str_tensor = tf.placeholder(tf.string, shape=[None])\n  image = tf.map_fn(resize_image, image_str_tensor, back_prop=False)\n  feed_dict = collections.defaultdict(list)\n  feed_dict[image_str_tensor.name] = images\n  with tf.Session() as sess:\n    images_resized = sess.run(image, feed_dict=feed_dict)\n  return images_resized", "response": "Load images from files and optionally resize it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating DataFrames out of prediction results and display images in IPython if requested.", "response": "def process_prediction_results(results, show_image):\n  \"\"\"Create DataFrames out of prediction results, and display images in IPython if requested.\"\"\"\n\n  import pandas as pd\n\n  if (is_in_IPython() and show_image is True):\n    import IPython\n    for image_url, image, label_and_score in results:\n      IPython.display.display_html('<p style=\"font-size:28px\">%s(%.5f)</p>' % label_and_score,\n                                   raw=True)\n      IPython.display.display(IPython.display.Image(data=image))\n  result_dict = [{'image_url': url, 'label': r[0], 'score': r[1]} for url, _, r in results]\n  return pd.DataFrame(result_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrepackages it from local installed location and copy it to GCS.", "response": "def repackage_to_staging(output_path):\n  \"\"\"Repackage it from local installed location and copy it to GCS.\"\"\"\n\n  import google.datalab.ml as ml\n\n  # Find the package root. __file__ is under [package_root]/mltoolbox/image/classification.\n  package_root = os.path.join(os.path.dirname(__file__), '../../../')\n  # We deploy setup.py in the same dir for repackaging purpose.\n  setup_py = os.path.join(os.path.dirname(__file__), 'setup.py')\n  staging_package_url = os.path.join(output_path, 'staging', 'image_classification.tar.gz')\n  ml.package_and_copy(package_root, setup_py, staging_package_url)\n  return staging_package_url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the airflow python spec for the given pipeline object.", "response": "def generate_airflow_spec(name, pipeline_spec):\n    \"\"\" Gets the airflow python spec for the Pipeline object.\n    \"\"\"\n    task_definitions = ''\n    up_steam_statements = ''\n    parameters = pipeline_spec.get('parameters')\n    for (task_id, task_details) in sorted(pipeline_spec['tasks'].items()):\n      task_def = PipelineGenerator._get_operator_definition(task_id, task_details, parameters)\n      task_definitions = task_definitions + task_def\n      dependency_def = PipelineGenerator._get_dependency_definition(\n        task_id, task_details.get('up_stream', []))\n      up_steam_statements = up_steam_statements + dependency_def\n\n    schedule_config = pipeline_spec.get('schedule', {})\n\n    default_args = PipelineGenerator._get_default_args(schedule_config,\n                                                       pipeline_spec.get('emails', {}))\n    dag_definition = PipelineGenerator._get_dag_definition(\n      name, schedule_config.get('interval', '@once'), schedule_config.get('catchup', False))\n    return PipelineGenerator._imports + default_args + dag_definition + task_definitions + \\\n        up_steam_statements"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_dependency_definition(task_id, dependencies):\n    set_upstream_statements = ''\n    for dependency in dependencies:\n      set_upstream_statements = set_upstream_statements + \\\n          '{0}.set_upstream({1})'.format(task_id, dependency) + '\\n'\n    return set_upstream_statements", "response": "Internal helper returns the Airflow equivalent python sytax for specifying the dependencies of the task."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef size(self):\n    if self._size is None:\n      self._size = 0\n      for csv_file in self.files:\n        self._size += sum(1 if line else 0 for line in _util.open_local_or_gcs(csv_file, 'r'))\n\n    return self._size", "response": "The size of the schema."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(self, n):\n\n    row_total_count = 0\n    row_counts = []\n    for file in self.files:\n      with _util.open_local_or_gcs(file, 'r') as f:\n        num_lines = sum(1 for line in f)\n        row_total_count += num_lines\n        row_counts.append(num_lines)\n\n    names = None\n    dtype = None\n    if self._schema:\n      _MAPPINGS = {\n        'FLOAT': np.float64,\n        'INTEGER': np.int64,\n        'TIMESTAMP': np.datetime64,\n        'BOOLEAN': np.bool,\n      }\n      names = [x['name'] for x in self._schema]\n      dtype = {x['name']: _MAPPINGS.get(x['type'], object) for x in self._schema}\n\n    skip_count = row_total_count - n\n    # Get all skipped indexes. These will be distributed into each file.\n    # Note that random.sample will raise Exception if skip_count is greater than rows count.\n    skip_all = sorted(random.sample(range(0, row_total_count), skip_count))\n    dfs = []\n    for file, row_count in zip(self.files, row_counts):\n      skip = [x for x in skip_all if x < row_count]\n      skip_all = [x - row_count for x in skip_all if x >= row_count]\n      with _util.open_local_or_gcs(file, 'r') as f:\n        dfs.append(pd.read_csv(f, skiprows=skip, names=names, dtype=dtype, header=None))\n    return pd.concat(dfs, axis=0, ignore_index=True)", "response": "Samples data into a Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsample data into a Pandas DataFrame.", "response": "def sample(self, n):\n    \"\"\"Samples data into a Pandas DataFrame. Note that it calls BigQuery so it will\n       incur cost.\n\n    Args:\n      n: number of sampled counts. Note that the number of counts returned is approximated.\n    Returns:\n      A dataframe containing sampled data.\n    Raises:\n      Exception if n is larger than number of rows.\n    \"\"\"\n    total = bq.Query('select count(*) from %s' %\n                     self._get_source()).execute().result()[0].values()[0]\n    if n > total:\n      raise ValueError('sample larger than population')\n    sampling = bq.Sampling.random(percent=n * 100.0 / float(total))\n    if self._query is not None:\n      source = self._query\n    else:\n      source = 'SELECT * FROM `%s`' % self._table\n    sample = bq.Query(source).execute(sampling=sampling).result()\n    df = sample.to_dataframe()\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef size(self):\n    import tensorflow as tf\n\n    if self._size is None:\n      self._size = 0\n      options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n      for tfexample_file in self.files:\n        self._size += sum(1 for x\n                          in tf.python_io.tf_record_iterator(tfexample_file, options=options))\n\n    return self._size", "response": "The number of instances in the data source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of groups that match the filters.", "response": "def list(self, pattern='*'):\n    \"\"\"Returns a list of groups that match the filters.\n\n    Args:\n      pattern: An optional pattern to filter the groups based on their display\n          name. This can include Unix shell-style wildcards. E.g.\n          ``\"Production*\"``.\n\n    Returns:\n      A list of Group objects that match the filters.\n    \"\"\"\n    if self._group_dict is None:\n      self._group_dict = collections.OrderedDict(\n          (group.id, group) for group in self._client.list_groups())\n\n    return [group for group in self._group_dict.values()\n            if fnmatch.fnmatch(group.display_name, pattern)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_dataframe(self, pattern='*', max_rows=None):\n    data = []\n    for i, group in enumerate(self.list(pattern)):\n      if max_rows is not None and i >= max_rows:\n        break\n      parent = self._group_dict.get(group.parent_id)\n      parent_display_name = '' if parent is None else parent.display_name\n      data.append([\n          group.id, group.display_name, group.parent_id,\n          parent_display_name, group.is_cluster, group.filter])\n\n    return pandas.DataFrame(data, columns=self._DISPLAY_HEADERS)", "response": "Creates a pandas dataframe from the groups that match the filters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_recursive_dependencies(sql, values, code, resolved_vars, resolving_vars=None):\n\n    # Get the set of $var references in this SQL.\n    dependencies = SqlStatement._get_dependencies(sql)\n    for dependency in dependencies:\n      # Now we check each dependency. If it is in complete - i.e., we have an expansion\n      # for it already - we just continue.\n      if dependency in resolved_vars:\n        continue\n      # Look it up in our resolution namespace dictionary.\n      dep = datalab.utils.get_item(values, dependency)\n      # If it is a SQL module, get the main/last query from the module, so users can refer\n      # to $module. Useful especially if final query in module has no DEFINE QUERY <name> part.\n      if isinstance(dep, types.ModuleType):\n        dep = _utils.get_default_query_from_module(dep)\n      # If we can't resolve the $name, give up.\n      if dep is None:\n        raise Exception(\"Unsatisfied dependency $%s\" % dependency)\n      # If it is a SqlStatement, it may have its own $ references in turn; check to make\n      # sure we don't have circular references, and if not, recursively expand it and add\n      # it to the set of complete dependencies.\n      if isinstance(dep, SqlStatement):\n        if resolving_vars is None:\n          resolving_vars = []\n        elif dependency in resolving_vars:\n          # Circular dependency\n          raise Exception(\"Circular dependency in $%s\" % dependency)\n        resolving_vars.append(dependency)\n        SqlStatement._find_recursive_dependencies(dep._sql, values, code, resolved_vars,\n                                                  resolving_vars)\n        resolving_vars.pop()\n        resolved_vars[dependency] = SqlStatement(dep._sql)\n      else:\n        resolved_vars[dependency] = dep", "response": "Recursive helper method for expanding variables including transitive dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string that can be used to expand variables in a query within an environment.", "response": "def format(sql, args=None):\n    \"\"\" Resolve variable references in a query within an environment.\n\n    This computes and resolves the transitive dependencies in the query and raises an\n    exception if that fails due to either undefined or circular references.\n\n    Args:\n      sql: query to format.\n      args: a dictionary of values to use in variable expansion.\n\n    Returns:\n      The resolved SQL text with variables expanded.\n\n    Raises:\n      Exception on failure.\n    \"\"\"\n    resolved_vars = {}\n    code = []\n    SqlStatement._find_recursive_dependencies(sql, args, code=code,\n                                              resolved_vars=resolved_vars)\n\n    # Rebuild the SQL string, substituting just '$' for escaped $ occurrences,\n    # variable references substituted with their values, or literal text copied\n    # over as-is.\n    parts = []\n    for (escape, placeholder, _, literal) in SqlStatement._get_tokens(sql):\n      if escape:\n        parts.append('$')\n      elif placeholder:\n        variable = placeholder[1:]\n        try:\n          value = resolved_vars[variable]\n        except KeyError as e:\n          raise Exception('Invalid sql. Unable to substitute $%s.' % e.args[0])\n\n        if isinstance(value, types.ModuleType):\n          value = _utils.get_default_query_from_module(value)\n\n        if isinstance(value, SqlStatement):\n          sql = value.format(value._sql, resolved_vars)\n          value = '(%s)' % sql\n        elif '_repr_sql_' in dir(value):\n          # pylint: disable=protected-access\n          value = value._repr_sql_()\n        elif isinstance(value, basestring):\n          value = SqlStatement._escape_string(value)\n        elif isinstance(value, list) or isinstance(value, tuple):\n          if isinstance(value, tuple):\n            value = list(value)\n          expansion = '('\n          for v in value:\n            if len(expansion) > 1:\n              expansion += ', '\n            if isinstance(v, basestring):\n              expansion += SqlStatement._escape_string(v)\n            else:\n              expansion += str(v)\n          expansion += ')'\n          value = expansion\n        else:\n          value = str(value)\n        parts.append(value)\n      elif literal:\n        parts.append(literal)\n\n    expanded = ''.join(parts)\n    return expanded"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of variables referenced in this SQL.", "response": "def _get_dependencies(sql):\n    \"\"\" Return the list of variables referenced in this SQL. \"\"\"\n    dependencies = []\n    for (_, placeholder, dollar, _) in SqlStatement._get_tokens(sql):\n      if placeholder:\n        variable = placeholder[1:]\n        if variable not in dependencies:\n          dependencies.append(variable)\n      elif dollar:\n        raise Exception('Invalid sql; $ with no following $ or identifier: %s.' % sql)\n    return dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and subsequently auto - imports a python module.", "response": "def pymodule(line, cell=None):\n  \"\"\"Creates and subsequently auto-imports a python module.\n  \"\"\"\n  parser = _commands.CommandParser.create('pymodule')\n  parser.add_argument('-n', '--name',\n                      help='the name of the python module to create and import')\n  parser.set_defaults(func=_pymodule_cell)\n  return _utils.handle_magic_line(line, cell, parser)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the project_id for the context.", "response": "def set_project_id(self, project_id):\n    \"\"\" Set the project_id for the context. \"\"\"\n    self._project_id = project_id\n    if self == Context._global_context:\n      try:\n        from google.datalab import Context as new_context\n        new_context.default().set_project_id(project_id)\n      except ImportError:\n        # If the new library is not loaded, then we have nothing to do.\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef default():\n    credentials = _utils.get_credentials()\n    if Context._global_context is None:\n      project = _project.Projects.get_default_id(credentials)\n      Context._global_context = Context(project, credentials)\n    else:\n      # Always update the credentials in case the access token is revoked or expired\n      Context._global_context.set_credentials(credentials)\n    return Context._global_context", "response": "Retrieves a default Context object creates it if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompares two datetimes safely.", "response": "def compare_datetimes(d1, d2):\n  \"\"\" Compares two datetimes safely, whether they are timezone-naive or timezone-aware.\n\n  If either datetime is naive it is converted to an aware datetime assuming UTC.\n\n  Args:\n    d1: first datetime.\n    d2: second datetime.\n\n  Returns:\n    -1 if d1 < d2, 0 if they are the same, or +1 is d1 > d2.\n  \"\"\"\n  if d1.tzinfo is None or d1.tzinfo.utcoffset(d1) is None:\n    d1 = d1.replace(tzinfo=pytz.UTC)\n  if d2.tzinfo is None or d2.tzinfo.utcoffset(d2) is None:\n    d2 = d2.replace(tzinfo=pytz.UTC)\n  if d1 < d2:\n    return -1\n  elif d1 > d2:\n    return 1\n  return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an unused port on the VM.", "response": "def pick_unused_port():\n  \"\"\" get an unused port on the VM.\n\n  Returns:\n    An unused port.\n  \"\"\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.bind(('localhost', 0))\n  addr, port = s.getsockname()\n  s.close()\n  return port"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_http_running_on(port):\n  try:\n    conn = httplib.HTTPConnection('127.0.0.1:' + str(port))\n    conn.connect()\n    conn.close()\n    return True\n  except Exception:\n    return False", "response": "Checks if an http server runs on a given port."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave project id to config file.", "response": "def save_project_id(project_id):\n  \"\"\" Save project id to config file.\n\n  Args:\n    project_id: the project_id to save.\n  \"\"\"\n  # Try gcloud first. If gcloud fails (probably because it does not exist), then\n  # write to a config file.\n  try:\n    subprocess.call(['gcloud', 'config', 'set', 'project', project_id])\n  except:\n    config_file = os.path.join(get_config_dir(), 'config.json')\n    config = {}\n    if os.path.exists(config_file):\n      with open(config_file) as f:\n        config = json.loads(f.read())\n    config['project_id'] = project_id\n    with open(config_file, 'w') as f:\n      f.write(json.dumps(config))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_project_id():\n  # Try getting default project id from gcloud. If it fails try config.json.\n  try:\n    proc = subprocess.Popen(['gcloud', 'config', 'list', '--format', 'value(core.project)'],\n                            stdout=subprocess.PIPE)\n    stdout, _ = proc.communicate()\n    value = stdout.strip()\n    if proc.poll() == 0 and value:\n      if isinstance(value, six.string_types):\n        return value\n      else:\n        # Hope it's a utf-8 string encoded in bytes. Otherwise an exception will\n        # be thrown and config.json will be checked.\n        return value.decode()\n  except:\n    pass\n\n  config_file = os.path.join(get_config_dir(), 'config.json')\n  if os.path.exists(config_file):\n    with open(config_file) as f:\n      config = json.loads(f.read())\n      if 'project_id' in config and config['project_id']:\n        return str(config['project_id'])\n\n  if os.getenv('PROJECT_ID') is not None:\n    return os.getenv('PROJECT_ID')\n  return None", "response": "Try getting default project id from config or environment var."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _construct_context_for_args(args):\n  global_default_context = google.datalab.Context.default()\n  config = {}\n  for key in global_default_context.config:\n    config[key] = global_default_context.config[key]\n\n  billing_tier_arg = args.get('billing', None)\n  if billing_tier_arg:\n    config['bigquery_billing_tier'] = billing_tier_arg\n\n  return google.datalab.Context(\n    project_id=global_default_context.project_id,\n    credentials=global_default_context.credentials,\n    config=config)", "response": "Construct a new Context based on the parsed arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting bytes into a string type.", "response": "def python_portable_string(string, encoding='utf-8'):\n  \"\"\"Converts bytes into a string type.\n\n  Valid string types are retuned without modification. So in Python 2, type str\n  and unicode are not converted.\n\n  In Python 3, type bytes is converted to type str (unicode)\n  \"\"\"\n  if isinstance(string, six.string_types):\n    return string\n\n  if six.PY3:\n    return string.decode(encoding)\n\n  raise ValueError('Unsupported type %s' % str(type(string)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extract_storage_api_response_error(message):\n  try:\n    if len(message) == 3:\n      # Try treat the last part as JSON\n      data = json.loads(message[2])\n      return data['error']['errors'][0]['message']\n  except Exception:\n    pass\n  return message", "response": "Helper function to extract user - friendly error messages from service exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef storage(line, cell=None):\n  parser = datalab.utils.commands.CommandParser(prog='storage', description=\"\"\"\nExecute various storage-related operations. Use \"%storage <command> -h\"\nfor help on a specific command.\n\"\"\")\n\n  # TODO(gram): consider adding a move command too. I did try this already using the\n  # objects.patch API to change the object name but that fails with an error:\n  #\n  # Value 'newname' in content does not agree with value 'oldname'. This can happen when a value\n  # set through a parameter is inconsistent with a value set in the request.\n  #\n  # This is despite 'name' being identified as writable in the storage API docs.\n  # The alternative would be to use a copy/delete.\n  copy_parser = parser.subcommand('copy',\n                                  'Copy one or more GCS objects to a different location.')\n  copy_parser.add_argument('-s', '--source', help='The name of the object(s) to copy', nargs='+')\n  copy_parser.add_argument('-d', '--destination', required=True,\n                           help='The copy destination. For multiple source items this must be a '\n                                'bucket.')\n  copy_parser.set_defaults(func=_storage_copy)\n\n  create_parser = parser.subcommand('create', 'Create one or more GCS buckets.')\n  create_parser.add_argument('-p', '--project', help='The project associated with the objects')\n  create_parser.add_argument('-b', '--bucket', help='The name of the bucket(s) to create',\n                             nargs='+')\n  create_parser.set_defaults(func=_storage_create)\n\n  delete_parser = parser.subcommand('delete', 'Delete one or more GCS buckets or objects.')\n  delete_parser.add_argument('-b', '--bucket', nargs='*',\n                             help='The name of the bucket(s) to remove')\n  delete_parser.add_argument('-o', '--object', nargs='*',\n                             help='The name of the object(s) to remove')\n  delete_parser.set_defaults(func=_storage_delete)\n\n  list_parser = parser.subcommand('list', 'List buckets in a project, or contents of a bucket.')\n  list_parser.add_argument('-p', '--project', help='The project associated with the objects')\n  group = list_parser.add_mutually_exclusive_group()\n  group.add_argument('-o', '--object',\n                     help='The name of the objects(s) to list; can include wildchars',\n                     nargs='?')\n  group.add_argument('-b', '--bucket',\n                     help='The name of the buckets(s) to list; can include wildchars',\n                     nargs='?')\n  list_parser.set_defaults(func=_storage_list)\n\n  read_parser = parser.subcommand('read',\n                                  'Read the contents of a storage object into a Python variable.')\n  read_parser.add_argument('-o', '--object', help='The name of the object to read',\n                           required=True)\n  read_parser.add_argument('-v', '--variable', required=True,\n                           help='The name of the Python variable to set')\n  read_parser.set_defaults(func=_storage_read)\n\n  view_parser = parser.subcommand('view', 'View the contents of a storage object.')\n  view_parser.add_argument('-n', '--head', type=int, default=20,\n                           help='The number of initial lines to view')\n  view_parser.add_argument('-t', '--tail', type=int, default=20,\n                           help='The number of lines from end to view')\n  view_parser.add_argument('-o', '--object', help='The name of the object to view',\n                           required=True)\n  view_parser.set_defaults(func=_storage_view)\n\n  write_parser = parser.subcommand('write',\n                                   'Write the value of a Python variable to a storage object.')\n  write_parser.add_argument('-v', '--variable', help='The name of the source Python variable',\n                            required=True)\n  write_parser.add_argument('-o', '--object', required=True,\n                            help='The name of the destination GCS object to write')\n  write_parser.add_argument('-c', '--content_type', help='MIME type', default='text/plain')\n  write_parser.set_defaults(func=_storage_write)\n\n  return datalab.utils.commands.handle_magic_line(line, cell, parser)", "response": "Implements the storage cell magic for ipython notebooks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpand a list of object names into a list of objects.", "response": "def _expand_list(names):\n  \"\"\" Do a wildchar name expansion of object names in a list and return expanded list.\n\n    The items are expected to exist as this is used for copy sources or delete targets.\n    Currently we support wildchars in the key name only.\n  \"\"\"\n\n  if names is None:\n    names = []\n  elif isinstance(names, basestring):\n    names = [names]\n\n  results = []  # The expanded list.\n  items = {}  # Cached contents of buckets; used for matching.\n  for name in names:\n    bucket, key = datalab.storage._bucket.parse_name(name)\n    results_len = len(results)  # If we fail to add any we add name and let caller deal with it.\n    if bucket:\n      if not key:\n        # Just a bucket; add it.\n        results.append('gs://%s' % bucket)\n      elif datalab.storage.Item(bucket, key).exists():\n        results.append('gs://%s/%s' % (bucket, key))\n      else:\n        # Expand possible key values.\n        if bucket not in items and key[:1] == '*':\n          # We need the full list; cache a copy for efficiency.\n          items[bucket] = [item.metadata.name\n                           for item in list(datalab.storage.Bucket(bucket).items())]\n        # If we have a cached copy use it\n        if bucket in items:\n          candidates = items[bucket]\n        # else we have no cached copy but can use prefix matching which is more efficient than\n        # getting the full contents.\n        else:\n          # Get the non-wildchar prefix.\n          match = re.search('\\?|\\*|\\[', key)\n          prefix = key\n          if match:\n            prefix = key[0:match.start()]\n\n          candidates = [item.metadata.name\n                        for item in datalab.storage.Bucket(bucket).items(prefix=prefix)]\n\n        for item in candidates:\n          if fnmatch.fnmatch(item, key):\n            results.append('gs://%s/%s' % (bucket, item))\n\n    # If we added no matches, add the original name and let caller deal with it.\n    if len(results) == results_len:\n      results.append(name)\n\n  return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates one or more buckets.", "response": "def _storage_create(args, _):\n  \"\"\" Create one or more buckets. \"\"\"\n  buckets = datalab.storage.Buckets(project_id=args['project'])\n  errs = []\n  for name in args['bucket']:\n    try:\n      bucket, key = datalab.storage._bucket.parse_name(name)\n      if bucket and not key:\n        buckets.create(bucket)\n      else:\n        raise Exception(\"Invalid bucket name %s\" % name)\n    except Exception as e:\n      errs.append(\"Couldn't create %s: %s\" %\n                  (name, _extract_storage_api_response_error(str(e))))\n  if errs:\n    raise Exception('\\n'.join(errs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _storage_delete(args, _):\n  items = _expand_list(args['bucket'])\n  items.extend(_expand_list(args['object']))\n  errs = []\n  for item in items:\n    try:\n      bucket, key = datalab.storage._bucket.parse_name(item)\n      if bucket and key:\n        gcs_item = datalab.storage.Item(bucket, key)\n        if gcs_item.exists():\n          datalab.storage.Item(bucket, key).delete()\n        else:\n          errs.append(\"%s does not exist\" % item)\n      elif bucket:\n        gcs_bucket = datalab.storage.Bucket(bucket)\n        if gcs_bucket.exists():\n          gcs_bucket.delete()\n        else:\n          errs.append(\"%s does not exist\" % item)\n      else:\n        raise Exception(\"Can't delete item with invalid name %s\" % item)\n    except Exception as e:\n      errs.append(\"Couldn't delete %s: %s\" %\n                  (item, _extract_storage_api_response_error(str(e))))\n  if errs:\n    raise Exception('\\n'.join(errs))", "response": "Delete one or more buckets or objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _storage_list_buckets(project, pattern):\n  data = [{'Bucket': 'gs://' + bucket.name, 'Created': bucket.metadata.created_on}\n          for bucket in datalab.storage.Buckets(project_id=project)\n          if fnmatch.fnmatch(bucket.name, pattern)]\n  return datalab.utils.commands.render_dictionary(data, ['Bucket', 'Created'])", "response": "List all storage buckets that match a pattern."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _storage_get_keys(bucket, pattern):\n  return [item for item in list(bucket.items()) if fnmatch.fnmatch(item.metadata.name, pattern)]", "response": "Get names of all storage keys in a specified bucket that match a pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets names of all storage keys in a specified bucket that match a pattern.", "response": "def _storage_get_key_names(bucket, pattern):\n  \"\"\" Get names of all storage keys in a specified bucket that match a pattern. \"\"\"\n  return [item.metadata.name for item in _storage_get_keys(bucket, pattern)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _storage_list_keys(bucket, pattern):\n  data = [{'Name': item.metadata.name,\n           'Type': item.metadata.content_type,\n           'Size': item.metadata.size,\n           'Updated': item.metadata.updated_on}\n          for item in _storage_get_keys(bucket, pattern)]\n  return datalab.utils.commands.render_dictionary(data, ['Name', 'Type', 'Size', 'Updated'])", "response": "List all storage keys in a specified bucket that match a pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists the contents of a bucket or a key.", "response": "def _storage_list(args, _):\n  \"\"\" List the buckets or the contents of a bucket.\n\n  This command is a bit different in that we allow wildchars in the bucket name and will list\n  the buckets that match.\n  \"\"\"\n  target = args['object'] if args['object'] else args['bucket']\n  project = args['project']\n  if target is None:\n    return _storage_list_buckets(project, '*')  # List all buckets.\n\n  bucket_name, key = datalab.storage._bucket.parse_name(target)\n  if bucket_name is None:\n    raise Exception('Cannot list %s; not a valid bucket name' % target)\n\n  if key or not re.search('\\?|\\*|\\[', target):\n    # List the contents of the bucket\n    if not key:\n      key = '*'\n    if project:\n      # Only list if the bucket is in the project\n      for bucket in datalab.storage.Buckets(project_id=project):\n        if bucket.name == bucket_name:\n          break\n      else:\n        raise Exception('%s does not exist in project %s' % (target, project))\n    else:\n      bucket = datalab.storage.Bucket(bucket_name)\n\n    if bucket.exists():\n      return _storage_list_keys(bucket, key)\n    else:\n      raise Exception('Bucket %s does not exist' % target)\n\n  else:\n    # Treat the bucket name as a pattern and show matches. We don't use bucket_name as that\n    # can strip off wildchars and so we need to strip off gs:// here.\n    return _storage_list_buckets(project, target[5:])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nissuing a request to insert a query job into the BigQuery table.", "response": "def jobs_insert_query(self, sql, table_name=None, append=False,\n                        overwrite=False, dry_run=False, use_cache=True, batch=True,\n                        allow_large_results=False, table_definitions=None, query_params=None):\n    \"\"\"Issues a request to insert a query job.\n\n    Args:\n      sql: the SQL string representing the query to execute.\n      table_name: None for an anonymous table, or a name parts tuple for a long-lived table.\n      append: if True, append to the table if it is non-empty; else the request will fail if table\n          is non-empty unless overwrite is True.\n      overwrite: if the table already exists, truncate it instead of appending or raising an\n          Exception.\n      dry_run: whether to actually execute the query or just dry run it.\n      use_cache: whether to use past query results or ignore cache. Has no effect if destination is\n          specified.\n      batch: whether to run this as a batch job (lower priority) or as an interactive job (high\n          priority, more expensive).\n      allow_large_results: whether to allow large results (slower with some restrictions but\n          can handle big jobs).\n      table_definitions: a dictionary of ExternalDataSource names and objects for any external\n          tables referenced in the query.\n      query_params: a dictionary containing query parameter types and values, passed to BigQuery.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._JOBS_PATH % (self.project_id, ''))\n\n    data = {\n        'kind': 'bigquery#job',\n        'configuration': {\n            'query': {\n                'query': sql,\n                'useQueryCache': use_cache,\n                'allowLargeResults': allow_large_results,\n                'useLegacySql': False\n            },\n            'dryRun': dry_run,\n            'priority': 'BATCH' if batch else 'INTERACTIVE',\n        },\n    }\n\n    query_config = data['configuration']['query']\n\n    if table_definitions:\n      expanded_definitions = {}\n      for td in table_definitions:\n        expanded_definitions[td] = table_definitions[td]._to_query_json()\n      query_config['tableDefinitions'] = expanded_definitions\n\n    if table_name:\n      query_config['destinationTable'] = {\n          'projectId': table_name.project_id,\n          'datasetId': table_name.dataset_id,\n          'tableId': table_name.table_id\n      }\n      if append:\n        query_config['writeDisposition'] = \"WRITE_APPEND\"\n      elif overwrite:\n        query_config['writeDisposition'] = \"WRITE_TRUNCATE\"\n\n    if self.bigquery_billing_tier:\n        query_config['maximumBillingTier'] = self.bigquery_billing_tier\n\n    if query_params:\n      query_config['queryParameters'] = query_params\n\n    return google.datalab.utils.Http.request(url, data=data, credentials=self.credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nissues a request to the jobs / getQueryResults method.", "response": "def jobs_query_results(self, job_id, project_id, page_size, timeout, start_index=0):\n    \"\"\"Issues a request to the jobs/getQueryResults method.\n\n    Args:\n      job_id: the id of job from a previously executed query.\n      project_id: the project id to use to fetch the results; use None for the default project.\n      page_size: limit to the number of rows to fetch.\n      timeout: duration (in milliseconds) to wait for the query to complete.\n      start_index: the index of the row (0-based) at which to start retrieving the page of result\n          rows.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    if timeout is None:\n      timeout = Api._DEFAULT_TIMEOUT\n    if project_id is None:\n      project_id = self.project_id\n\n    args = {\n        'maxResults': page_size,\n        'timeoutMs': timeout,\n        'startIndex': start_index\n    }\n    url = Api._ENDPOINT + (Api._QUERIES_PATH % (project_id, job_id))\n    return google.datalab.utils.Http.request(url, args=args, credentials=self.credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nissue a request to retrieve a list of tables.", "response": "def tables_list(self, dataset_name, max_results=0, page_token=None):\n    \"\"\"Issues a request to retrieve a list of tables.\n\n    Args:\n      dataset_name: the name of the dataset to enumerate.\n      max_results: an optional maximum number of tables to retrieve.\n      page_token: an optional token to continue the retrieval.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT +\\\n        (Api._TABLES_PATH % (dataset_name.project_id, dataset_name.dataset_id, '', ''))\n\n    args = {}\n    if max_results != 0:\n      args['maxResults'] = max_results\n    if page_token is not None:\n      args['pageToken'] = page_token\n\n    return google.datalab.utils.Http.request(url, args=args, credentials=self.credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscale a column to the given range.", "response": "def _scale(x, min_x_value, max_x_value, output_min, output_max):\n  \"\"\"Scale a column to [output_min, output_max].\n\n  Assumes the columns's range is [min_x_value, max_x_value]. If this is not\n  true at training or prediction time, the output value of this scale could be\n  outside the range [output_min, output_max].\n\n  Raises:\n    ValueError: if min_x_value = max_x_value, as the column is constant.\n  \"\"\"\n\n  if round(min_x_value - max_x_value, 7) == 0:\n    # There is something wrong with the data.\n    # Why round to 7 places? It's the same as unittest's assertAlmostEqual.\n    raise ValueError('In make_scale_tito, min_x_value == max_x_value')\n\n  def _scale(x):\n    min_x_valuef = tf.to_float(min_x_value)\n    max_x_valuef = tf.to_float(max_x_value)\n    output_minf = tf.to_float(output_min)\n    output_maxf = tf.to_float(output_max)\n    return ((((tf.to_float(x) - min_x_valuef) * (output_maxf - output_minf)) /\n            (max_x_valuef - min_x_valuef)) + output_minf)\n\n  return _scale(x)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _string_to_int(x, vocab):\n\n  def _map_to_int(x):\n    \"\"\"Maps string tensor into indexes using vocab.\n\n    Args:\n      x : a Tensor/SparseTensor of string.\n    Returns:\n      a Tensor/SparseTensor of indexes (int) of the same shape as x.\n    \"\"\"\n    table = lookup.index_table_from_tensor(\n        vocab,\n        default_value=len(vocab))\n    return table.lookup(x)\n\n  return _map_to_int(x)", "response": "Maps a string tensor into an integer tensor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _tfidf(x, reduced_term_freq, vocab_size, corpus_size):\n\n  def _map_to_vocab_range(x):\n    \"\"\"Enforces that the vocab_ids in x are positive.\"\"\"\n    return tf.SparseTensor(\n        indices=x.indices,\n        values=tf.mod(x.values, vocab_size),\n        dense_shape=x.dense_shape)\n\n  def _map_to_tfidf(x):\n    \"\"\"Calculates the inverse document frequency of terms in the corpus.\n    Args:\n      x : a SparseTensor of int64 representing string indices in vocab.\n    Returns:\n      The tf*idf values\n    \"\"\"\n    # Add one to the reduced term freqnencies to avoid dividing by zero.\n    idf = tf.log(tf.to_double(corpus_size) / (\n        1.0 + tf.to_double(reduced_term_freq)))\n\n    dense_doc_sizes = tf.to_double(tf.sparse_reduce_sum(tf.SparseTensor(\n        indices=x.indices,\n        values=tf.ones_like(x.values),\n        dense_shape=x.dense_shape), 1))\n\n    # For every term in x, divide the idf by the doc size.\n    # The two gathers both result in shape <sum_doc_sizes>\n    idf_over_doc_size = (tf.gather(idf, x.values) /\n                         tf.gather(dense_doc_sizes, x.indices[:, 0]))\n\n    return tf.SparseTensor(\n        indices=x.indices,\n        values=idf_over_doc_size,\n        dense_shape=x.dense_shape)\n\n  cleaned_input = _map_to_vocab_range(x)\n\n  weights = _map_to_tfidf(cleaned_input)\n  return tf.to_float(weights)", "response": "Returns the tfidf of the terms in x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the bag of words weights", "response": "def _bag_of_words(x):\n  \"\"\"Computes bag of words weights\n\n  Note the return type is a float sparse tensor, not a int sparse tensor. This\n  is so that the output types batch tfidf, and any downstream transformation\n  in tf layers during training can be applied to both.\n  \"\"\"\n  def _bow(x):\n    \"\"\"Comptue BOW weights.\n\n    As tf layer's sum combiner is used, the weights can be just ones. Tokens are\n    not summed together here.\n    \"\"\"\n    return tf.SparseTensor(\n      indices=x.indices,\n      values=tf.to_float(tf.ones_like(x.values)),\n      dense_shape=x.dense_shape)\n\n  return _bow(x)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a tensor - in - tensor - out function that produces embeddings from image bytes.", "response": "def _make_image_to_vec_tito(feature_name, tmp_dir=None, checkpoint=None):\n  \"\"\"Creates a tensor-in-tensor-out function that produces embeddings from image bytes.\n\n  Image to embedding is implemented with Tensorflow's inception v3 model and a pretrained\n  checkpoint. It returns 1x2048 'PreLogits' embeddings for each image.\n\n  Args:\n    feature_name: The name of the feature. Used only to identify the image tensors so\n      we can get gradients for probe in image prediction explaining.\n    tmp_dir: a local directory that is used for downloading the checkpoint. If\n      non, a temp folder will be made and deleted.\n    checkpoint: the inception v3 checkpoint gs or local path. If None, default checkpoint\n      is used.\n\n  Returns: a tensor-in-tensor-out function that takes image string tensor and returns embeddings.\n  \"\"\"\n\n  def _image_to_vec(image_str_tensor):\n\n    def _decode_and_resize(image_tensor):\n      \"\"\"Decodes jpeg string, resizes it and returns a uint8 tensor.\"\"\"\n\n      # These constants are set by Inception v3's expectations.\n      height = 299\n      width = 299\n      channels = 3\n\n      image_tensor = tf.where(tf.equal(image_tensor, ''), IMAGE_DEFAULT_STRING, image_tensor)\n\n      # Fork by whether image_tensor value is a file path, or a base64 encoded string.\n      slash_positions = tf.equal(tf.string_split([image_tensor], delimiter=\"\").values, '/')\n      is_file_path = tf.cast(tf.count_nonzero(slash_positions), tf.bool)\n\n      # The following two functions are required for tf.cond. Note that we can not replace them\n      # with lambda. According to TF docs, if using inline lambda, both branches of condition\n      # will be executed. The workaround is to use a function call.\n      def _read_file():\n        return tf.read_file(image_tensor)\n\n      def _decode_base64():\n        return tf.decode_base64(image_tensor)\n\n      image = tf.cond(is_file_path, lambda: _read_file(), lambda: _decode_base64())\n      image = tf.image.decode_jpeg(image, channels=channels)\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\n      image = tf.squeeze(image, squeeze_dims=[0])\n      image = tf.cast(image, dtype=tf.uint8)\n      return image\n\n    # The CloudML Prediction API always \"feeds\" the Tensorflow graph with\n    # dynamic batch sizes e.g. (?,).  decode_jpeg only processes scalar\n    # strings because it cannot guarantee a batch of images would have\n    # the same output size.  We use tf.map_fn to give decode_jpeg a scalar\n    # string from dynamic batches.\n    image = tf.map_fn(_decode_and_resize, image_str_tensor, back_prop=False, dtype=tf.uint8)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # \"gradients_[feature_name]\" will be used for computing integrated gradients.\n    image = tf.identity(image, name='gradients_' + feature_name)\n    image = tf.subtract(image, 0.5)\n    inception_input = tf.multiply(image, 2.0)\n\n    # Build Inception layers, which expect a tensor of type float from [-1, 1)\n    # and shape [batch_size, height, width, channels].\n    with tf.contrib.slim.arg_scope(inception_v3_arg_scope()):\n      _, end_points = inception_v3(inception_input, is_training=False)\n\n    embeddings = end_points['PreLogits']\n    inception_embeddings = tf.squeeze(embeddings, [1, 2], name='SpatialSqueeze')\n    return inception_embeddings\n\n  def _tito_from_checkpoint(tito_in, checkpoint, exclude):\n    \"\"\" Create an all-constants tito function from an original tito function.\n\n    Given a tensor-in-tensor-out function which contains variables and a checkpoint path,\n    create a new tensor-in-tensor-out function which includes only constants, and can be\n    used in tft.map.\n    \"\"\"\n\n    def _tito_out(tensor_in):\n      checkpoint_dir = tmp_dir\n      if tmp_dir is None:\n        checkpoint_dir = tempfile.mkdtemp()\n\n      g = tf.Graph()\n      with g.as_default():\n        si = tf.placeholder(dtype=tensor_in.dtype, shape=tensor_in.shape, name=tensor_in.op.name)\n        so = tito_in(si)\n        all_vars = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n        saver = tf.train.Saver(all_vars)\n        # Downloading the checkpoint from GCS to local speeds up saver.restore() a lot.\n        checkpoint_tmp = os.path.join(checkpoint_dir, 'checkpoint')\n        with file_io.FileIO(checkpoint, 'r') as f_in, file_io.FileIO(checkpoint_tmp, 'w') as f_out:\n          f_out.write(f_in.read())\n        with tf.Session() as sess:\n          saver.restore(sess, checkpoint_tmp)\n          output_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n                                                                          g.as_graph_def(),\n                                                                          [so.op.name])\n        file_io.delete_file(checkpoint_tmp)\n        if tmp_dir is None:\n          shutil.rmtree(checkpoint_dir)\n\n      tensors_out = tf.import_graph_def(output_graph_def,\n                                        input_map={si.name: tensor_in},\n                                        return_elements=[so.name])\n      return tensors_out[0]\n\n    return _tito_out\n\n  if not checkpoint:\n    checkpoint = INCEPTION_V3_CHECKPOINT\n  return _tito_from_checkpoint(_image_to_vec, checkpoint, INCEPTION_EXCLUDED_VARIABLES)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a preprocessing function.", "response": "def make_preprocessing_fn(output_dir, features, keep_target):\n  \"\"\"Makes a preprocessing function.\n\n  Args:\n    output_dir: folder path that contains the vocab and stats files.\n    features: the features dict\n\n  Returns:\n    a function that takes a dict of input tensors\n  \"\"\"\n  def preprocessing_fn(inputs):\n    \"\"\"Preprocessing function.\n\n    Args:\n      inputs: dictionary of raw input tensors\n\n    Returns:\n      A dictionary of transformed tensors\n    \"\"\"\n    stats = json.loads(\n      file_io.read_file_to_string(\n          os.path.join(output_dir, STATS_FILE)).decode())\n\n    result = {}\n    for name, transform in six.iteritems(features):\n      transform_name = transform['transform']\n      source_column = transform['source_column']\n\n      if transform_name == KEY_TRANSFORM:\n        transform_name = 'identity'\n      elif transform_name == TARGET_TRANSFORM:\n        if not keep_target:\n          continue\n        if file_io.file_exists(os.path.join(output_dir, VOCAB_ANALYSIS_FILE % source_column)):\n          transform_name = 'one_hot'\n        else:\n          transform_name = 'identity'\n\n      if transform_name == 'identity':\n        result[name] = inputs[source_column]\n      elif transform_name == 'scale':\n        result[name] = _scale(\n            inputs[name],\n            min_x_value=stats['column_stats'][source_column]['min'],\n            max_x_value=stats['column_stats'][source_column]['max'],\n            output_min=transform.get('value', 1) * (-1),\n            output_max=transform.get('value', 1))\n      elif transform_name in [ONE_HOT_TRANSFORM, EMBEDDING_TRANSFROM, MULTI_HOT_TRANSFORM,\n                              TFIDF_TRANSFORM, BOW_TRANSFORM]:\n        vocab, ex_count = read_vocab_file(\n            os.path.join(output_dir, VOCAB_ANALYSIS_FILE % source_column))\n\n        if transform_name == TFIDF_TRANSFORM:\n          separator = transform.get('separator', ' ')\n          tokens = tf.string_split(inputs[source_column], separator)\n          ids = _string_to_int(tokens, vocab)\n          weights = _tfidf(\n              x=ids,\n              reduced_term_freq=ex_count + [0],\n              vocab_size=len(vocab) + 1,\n              corpus_size=stats['num_examples'])\n\n          result[name + '_ids'] = ids\n          result[name + '_weights'] = weights\n        elif transform_name == BOW_TRANSFORM:\n          separator = transform.get('separator', ' ')\n          tokens = tf.string_split(inputs[source_column], separator)\n          ids = _string_to_int(tokens, vocab)\n          weights = _bag_of_words(x=ids)\n\n          result[name + '_ids'] = ids\n          result[name + '_weights'] = weights\n        elif transform_name == MULTI_HOT_TRANSFORM:\n          separator = transform.get('separator', ' ')\n          tokens = tf.string_split(inputs[source_column], separator)\n          result[name] = _string_to_int(tokens, vocab)\n        else:\n          # ONE_HOT_TRANSFORM: making a dense vector is done at training\n          # EMBEDDING_TRANSFROM: embedding vectors have to be done at training\n          result[name] = _string_to_int(inputs[source_column], vocab)\n      elif transform_name == IMAGE_TRANSFORM:\n        make_image_to_vec_fn = _make_image_to_vec_tito(\n            name, checkpoint=transform.get('checkpoint', None))\n        result[name] = make_image_to_vec_fn(inputs[source_column])\n      else:\n        raise ValueError('unknown transform %s' % transform_name)\n    return result\n\n  return preprocessing_fn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_transformed_feature_info(features, schema):\n\n  info = collections.defaultdict(dict)\n\n  for name, transform in six.iteritems(features):\n    transform_name = transform['transform']\n    source_column = transform['source_column']\n\n    if transform_name == IDENTITY_TRANSFORM:\n      schema_type = next(col['type'].lower() for col in schema if col['name'] == source_column)\n      if schema_type == FLOAT_SCHEMA:\n        info[name]['dtype'] = tf.float32\n      elif schema_type == INTEGER_SCHEMA:\n        info[name]['dtype'] = tf.int64\n      else:\n        raise ValueError('itentity should only be applied to integer or float'\n                         'columns, but was used on %s' % name)\n      info[name]['size'] = 1\n    elif transform_name == SCALE_TRANSFORM:\n      info[name]['dtype'] = tf.float32\n      info[name]['size'] = 1\n    elif transform_name == ONE_HOT_TRANSFORM:\n      info[name]['dtype'] = tf.int64\n      info[name]['size'] = 1\n    elif transform_name == EMBEDDING_TRANSFROM:\n      info[name]['dtype'] = tf.int64\n      info[name]['size'] = 1\n    elif transform_name == MULTI_HOT_TRANSFORM:\n      info[name]['dtype'] = tf.int64\n      info[name]['size'] = None\n    elif transform_name == BOW_TRANSFORM or transform_name == TFIDF_TRANSFORM:\n      info[name + '_ids']['dtype'] = tf.int64\n      info[name + '_weights']['dtype'] = tf.float32\n      info[name + '_ids']['size'] = None\n      info[name + '_weights']['size'] = None\n    elif transform_name == KEY_TRANSFORM:\n      schema_type = next(col['type'].lower() for col in schema if col['name'] == source_column)\n      if schema_type == FLOAT_SCHEMA:\n        info[name]['dtype'] = tf.float32\n      elif schema_type == INTEGER_SCHEMA:\n        info[name]['dtype'] = tf.int64\n      else:\n        info[name]['dtype'] = tf.string\n      info[name]['size'] = 1\n    elif transform_name == TARGET_TRANSFORM:\n      # If the input is a string, it gets converted to an int (id)\n      schema_type = next(col['type'].lower() for col in schema if col['name'] == source_column)\n      if schema_type in NUMERIC_SCHEMA:\n        info[name]['dtype'] = tf.float32\n      else:\n        info[name]['dtype'] = tf.int64\n      info[name]['size'] = 1\n    elif transform_name == IMAGE_TRANSFORM:\n      info[name]['dtype'] = tf.float32\n      info[name]['size'] = IMAGE_BOTTLENECK_TENSOR_SIZE\n    else:\n      raise ValueError('Unknown transfrom %s' % transform_name)\n\n  return info", "response": "Returns the info about the transformed features."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets csv header and default lists.", "response": "def csv_header_and_defaults(features, schema, stats, keep_target):\n  \"\"\"Gets csv header and default lists.\"\"\"\n\n  target_name = get_target_name(features)\n  if keep_target and not target_name:\n    raise ValueError('Cannot find target transform')\n\n  csv_header = []\n  record_defaults = []\n  for col in schema:\n    if not keep_target and col['name'] == target_name:\n      continue\n\n    # Note that numerical key columns do not have a stats entry, hence the use\n    # of get(col['name'], {})\n    csv_header.append(col['name'])\n    if col['type'].lower() == INTEGER_SCHEMA:\n      dtype = tf.int64\n      default = int(stats['column_stats'].get(col['name'], {}).get('mean', 0))\n    elif col['type'].lower() == FLOAT_SCHEMA:\n      dtype = tf.float32\n      default = float(stats['column_stats'].get(col['name'], {}).get('mean', 0.0))\n    else:\n      dtype = tf.string\n      default = ''\n\n    record_defaults.append(tf.constant([default], dtype=dtype))\n\n  return csv_header, record_defaults"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a serving function starting from a raw csv.", "response": "def build_csv_serving_tensors_for_transform_step(analysis_path,\n                                                 features,\n                                                 schema,\n                                                 stats,\n                                                 keep_target):\n  \"\"\"Builds a serving function starting from raw csv.\n\n  This should only be used by transform.py (the transform step), and the\n\n  For image columns, the image should be a base64 string encoding the image.\n  The output of this function will transform that image to a 2048 long vector\n  using the inception model.\n  \"\"\"\n\n  csv_header, record_defaults = csv_header_and_defaults(features, schema, stats, keep_target)\n\n  placeholder = tf.placeholder(dtype=tf.string, shape=(None,),\n                               name='csv_input_placeholder')\n  tensors = tf.decode_csv(placeholder, record_defaults)\n  raw_features = dict(zip(csv_header, tensors))\n\n  transform_fn = make_preprocessing_fn(analysis_path, features, keep_target)\n  transformed_tensors = transform_fn(raw_features)\n\n  transformed_features = {}\n  # Expand the dims of non-sparse tensors\n  for k, v in six.iteritems(transformed_tensors):\n    if isinstance(v, tf.Tensor) and v.get_shape().ndims == 1:\n      transformed_features[k] = tf.expand_dims(v, -1)\n    else:\n      transformed_features[k] = v\n\n  return input_fn_utils.InputFnOps(\n      transformed_features, None, {\"csv_example\": placeholder})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a serving function for training a single step.", "response": "def build_csv_serving_tensors_for_training_step(analysis_path,\n                                                features,\n                                                schema,\n                                                stats,\n                                                keep_target):\n  \"\"\"Builds a serving function starting from raw csv, used at model export time.\n\n  For image columns, the image should be a base64 string encoding the image.\n  The output of this function will transform that image to a 2048 long vector\n  using the inception model and then a fully connected net is attached to\n  the 2048 long image embedding.\n  \"\"\"\n\n  transformed_features, _, placeholder_dict = build_csv_serving_tensors_for_transform_step(\n      analysis_path=analysis_path,\n      features=features,\n      schema=schema,\n      stats=stats,\n      keep_target=keep_target)\n\n  transformed_features = image_feature_engineering(\n      features=features,\n      feature_tensors_dict=transformed_features)\n\n  return input_fn_utils.InputFnOps(\n      transformed_features, None, placeholder_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild training input_fn suitable for training that reads raw csv training data and applies transforms.", "response": "def build_csv_transforming_training_input_fn(schema,\n                                             features,\n                                             stats,\n                                             analysis_output_dir,\n                                             raw_data_file_pattern,\n                                             training_batch_size,\n                                             num_epochs=None,\n                                             randomize_input=False,\n                                             min_after_dequeue=1,\n                                             reader_num_threads=1,\n                                             allow_smaller_final_batch=True):\n  \"\"\"Creates training input_fn that reads raw csv data and applies transforms.\n\n  Args:\n    schema: schema list\n    features: features dict\n    stats: stats dict\n    analysis_output_dir: output folder from analysis\n    raw_data_file_pattern: file path, or list of files\n    training_batch_size: An int specifying the batch size to use.\n    num_epochs: numer of epochs to read from the files. Use None to read forever.\n    randomize_input: If true, the input rows are read out of order. This\n        randomness is limited by the min_after_dequeue value.\n    min_after_dequeue: Minimum number elements in the reading queue after a\n        dequeue, used to ensure a level of mixing of elements. Only used if\n        randomize_input is True.\n    reader_num_threads: The number of threads enqueuing data.\n    allow_smaller_final_batch: If false, fractional batches at the end of\n        training or evaluation are not used.\n\n  Returns:\n    An input_fn suitable for training that reads raw csv training data and\n    applies transforms.\n\n  \"\"\"\n\n  def raw_training_input_fn():\n    \"\"\"Training input function that reads raw data and applies transforms.\"\"\"\n\n    if isinstance(raw_data_file_pattern, six.string_types):\n      filepath_list = [raw_data_file_pattern]\n    else:\n      filepath_list = raw_data_file_pattern\n\n    files = []\n    for path in filepath_list:\n      files.extend(file_io.get_matching_files(path))\n\n    filename_queue = tf.train.string_input_producer(\n        files, num_epochs=num_epochs, shuffle=randomize_input)\n\n    csv_id, csv_lines = tf.TextLineReader().read_up_to(filename_queue, training_batch_size)\n\n    queue_capacity = (reader_num_threads + 3) * training_batch_size + min_after_dequeue\n    if randomize_input:\n      _, batch_csv_lines = tf.train.shuffle_batch(\n          tensors=[csv_id, csv_lines],\n          batch_size=training_batch_size,\n          capacity=queue_capacity,\n          min_after_dequeue=min_after_dequeue,\n          enqueue_many=True,\n          num_threads=reader_num_threads,\n          allow_smaller_final_batch=allow_smaller_final_batch)\n\n    else:\n      _, batch_csv_lines = tf.train.batch(\n          tensors=[csv_id, csv_lines],\n          batch_size=training_batch_size,\n          capacity=queue_capacity,\n          enqueue_many=True,\n          num_threads=reader_num_threads,\n          allow_smaller_final_batch=allow_smaller_final_batch)\n\n    csv_header, record_defaults = csv_header_and_defaults(features, schema, stats, keep_target=True)\n    parsed_tensors = tf.decode_csv(batch_csv_lines, record_defaults, name='csv_to_tensors')\n    raw_features = dict(zip(csv_header, parsed_tensors))\n\n    transform_fn = make_preprocessing_fn(analysis_output_dir, features, keep_target=True)\n    transformed_tensors = transform_fn(raw_features)\n\n    # Expand the dims of non-sparse tensors. This is needed by tf.learn.\n    transformed_features = {}\n    for k, v in six.iteritems(transformed_tensors):\n      if isinstance(v, tf.Tensor) and v.get_shape().ndims == 1:\n        transformed_features[k] = tf.expand_dims(v, -1)\n      else:\n        transformed_features[k] = v\n\n    # image_feature_engineering does not need to be called as images are not\n    # supported in raw csv for training.\n\n    # Remove the target tensor, and return it directly\n    target_name = get_target_name(features)\n    if not target_name or target_name not in transformed_features:\n      raise ValueError('Cannot find target transform in features')\n\n    transformed_target = transformed_features.pop(target_name)\n\n    return transformed_features, transformed_target\n\n  return raw_training_input_fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds training input_fn that reads transformed data from the raw data file pattern and returns the input_fn suitable for training.", "response": "def build_tfexample_transfored_training_input_fn(schema,\n                                                 features,\n                                                 analysis_output_dir,\n                                                 raw_data_file_pattern,\n                                                 training_batch_size,\n                                                 num_epochs=None,\n                                                 randomize_input=False,\n                                                 min_after_dequeue=1,\n                                                 reader_num_threads=1,\n                                                 allow_smaller_final_batch=True):\n  \"\"\"Creates training input_fn that reads transformed tf.example files.\n\n  Args:\n    schema: schema list\n    features: features dict\n    analysis_output_dir: output folder from analysis\n    raw_data_file_pattern: file path, or list of files\n    training_batch_size: An int specifying the batch size to use.\n    num_epochs: numer of epochs to read from the files. Use None to read forever.\n    randomize_input: If true, the input rows are read out of order. This\n        randomness is limited by the min_after_dequeue value.\n    min_after_dequeue: Minimum number elements in the reading queue after a\n        dequeue, used to ensure a level of mixing of elements. Only used if\n        randomize_input is True.\n    reader_num_threads: The number of threads enqueuing data.\n    allow_smaller_final_batch: If false, fractional batches at the end of\n        training or evaluation are not used.\n\n  Returns:\n    An input_fn suitable for training that reads transformed data in tf record\n      files of tf.example.\n  \"\"\"\n\n  def transformed_training_input_fn():\n    \"\"\"Training input function that reads transformed data.\"\"\"\n\n    if isinstance(raw_data_file_pattern, six.string_types):\n      filepath_list = [raw_data_file_pattern]\n    else:\n      filepath_list = raw_data_file_pattern\n\n    files = []\n    for path in filepath_list:\n      files.extend(file_io.get_matching_files(path))\n\n    filename_queue = tf.train.string_input_producer(\n        files, num_epochs=num_epochs, shuffle=randomize_input)\n\n    options = tf.python_io.TFRecordOptions(\n        compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n    ex_id, ex_str = tf.TFRecordReader(options=options).read_up_to(\n        filename_queue, training_batch_size)\n\n    queue_capacity = (reader_num_threads + 3) * training_batch_size + min_after_dequeue\n    if randomize_input:\n      _, batch_ex_str = tf.train.shuffle_batch(\n          tensors=[ex_id, ex_str],\n          batch_size=training_batch_size,\n          capacity=queue_capacity,\n          min_after_dequeue=min_after_dequeue,\n          enqueue_many=True,\n          num_threads=reader_num_threads,\n          allow_smaller_final_batch=allow_smaller_final_batch)\n\n    else:\n      _, batch_ex_str = tf.train.batch(\n          tensors=[ex_id, ex_str],\n          batch_size=training_batch_size,\n          capacity=queue_capacity,\n          enqueue_many=True,\n          num_threads=reader_num_threads,\n          allow_smaller_final_batch=allow_smaller_final_batch)\n\n    feature_spec = {}\n    feature_info = get_transformed_feature_info(features, schema)\n    for name, info in six.iteritems(feature_info):\n      if info['size'] is None:\n        feature_spec[name] = tf.VarLenFeature(dtype=info['dtype'])\n      else:\n        feature_spec[name] = tf.FixedLenFeature(shape=[info['size']], dtype=info['dtype'])\n\n    parsed_tensors = tf.parse_example(batch_ex_str, feature_spec)\n\n    # Expand the dims of non-sparse tensors. This is needed by tf.learn.\n    transformed_features = {}\n    for k, v in six.iteritems(parsed_tensors):\n      if isinstance(v, tf.Tensor) and v.get_shape().ndims == 1:\n        transformed_features[k] = tf.expand_dims(v, -1)\n      else:\n        # Sparse tensor\n        transformed_features[k] = v\n\n    transformed_features = image_feature_engineering(\n        features=features,\n        feature_tensors_dict=transformed_features)\n\n    # Remove the target tensor, and return it directly\n    target_name = get_target_name(features)\n    if not target_name or target_name not in transformed_features:\n      raise ValueError('Cannot find target transform in features')\n\n    transformed_target = transformed_features.pop(target_name)\n\n    return transformed_features, transformed_target\n\n  return transformed_training_input_fn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a hidden layer on image features.", "response": "def image_feature_engineering(features, feature_tensors_dict):\n  \"\"\"Add a hidden layer on image features.\n\n  Args:\n    features: features dict\n    feature_tensors_dict: dict of feature-name: tensor\n  \"\"\"\n  engineered_features = {}\n  for name, feature_tensor in six.iteritems(feature_tensors_dict):\n    if name in features and features[name]['transform'] == IMAGE_TRANSFORM:\n      with tf.name_scope(name, 'Wx_plus_b'):\n        hidden = tf.contrib.layers.fully_connected(\n            feature_tensor,\n            IMAGE_HIDDEN_TENSOR_SIZE)\n        engineered_features[name] = hidden\n    else:\n      engineered_features[name] = feature_tensor\n  return engineered_features"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a vocab file to memeory.", "response": "def read_vocab_file(file_path):\n  \"\"\"Reads a vocab file to memeory.\n\n  Args:\n    file_path: Each line of the vocab is in the form \"token,example_count\"\n\n  Returns:\n    Two lists, one for the vocab, and one for just the example counts.\n  \"\"\"\n  with file_io.FileIO(file_path, 'r') as f:\n    vocab_pd = pd.read_csv(\n        f,\n        header=None,\n        names=['vocab', 'count'],\n        dtype=str,  # Prevent pd from converting numerical categories.\n        na_filter=False)  # Prevent pd from converting 'NA' to a NaN.\n\n  vocab = vocab_pd['vocab'].tolist()\n  ex_count = vocab_pd['count'].astype(int).tolist()\n\n  return vocab, ex_count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _to_query_json(self):\n    json = {\n      'compression': 'GZIP' if self._compressed else 'NONE',\n      'ignoreUnknownValues': self._ignore_unknown_values,\n      'maxBadRecords': self._max_bad_records,\n      'sourceFormat': self._bq_source_format,\n      'sourceUris': self._source,\n    }\n    if self._source_format == 'csv' and self._csv_options:\n      json['csvOptions'] = {}\n      json['csvOptions'].update(self._csv_options._to_query_json())\n    if self._schema:\n      json['schema'] = {'fields': self._schema._bq_schema}\n    return json", "response": "Return the table as a dictionary to be used as JSON in a query job."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_ipython_extension(shell):\n\n  # Inject our user agent on all requests by monkey-patching a wrapper around httplib2.Http.request.\n\n  def _request(self, uri, method=\"GET\", body=None, headers=None,\n               redirections=_httplib2.DEFAULT_MAX_REDIRECTS, connection_type=None):\n    if headers is None:\n      headers = {}\n    headers['user-agent'] = 'GoogleCloudDataLab/1.0'\n    return _orig_request(self, uri, method=method, body=body, headers=headers,\n                         redirections=redirections, connection_type=connection_type)\n\n  _httplib2.Http.request = _request\n\n  # Similarly for the requests library.\n\n  def _init_session(self):\n    _orig_init(self)\n    self.headers['User-Agent'] = 'GoogleCloudDataLab/1.0'\n\n  _requests.Session.__init__ = _init_session\n\n  # Be more tolerant with magics. If the user specified a cell magic that doesn't\n  # exist and an empty cell body but a line magic with that name exists, run that\n  # instead. Conversely, if the user specified a line magic that doesn't exist but\n  # a cell magic exists with that name, run the cell magic with an empty body.\n\n  def _run_line_magic(self, magic_name, line):\n    fn = self.find_line_magic(magic_name)\n    if fn is None:\n      cm = self.find_cell_magic(magic_name)\n      if cm:\n        return _run_cell_magic(self, magic_name, line, None)\n    return _orig_run_line_magic(self, magic_name, line)\n\n  def _run_cell_magic(self, magic_name, line, cell):\n    if cell is None or len(cell) == 0 or cell.isspace():\n      fn = self.find_line_magic(magic_name)\n      if fn:\n        return _orig_run_line_magic(self, magic_name, line)\n      # IPython will complain if cell is empty string but not if it is None\n      cell = None\n    return _orig_run_cell_magic(self, magic_name, line, cell)\n\n  _shell.InteractiveShell.run_cell_magic = _run_cell_magic\n  _shell.InteractiveShell.run_line_magic = _run_line_magic\n\n  # Define global 'project_id' and 'set_project_id' functions to manage the default project ID. We\n  # do this conditionally in a try/catch # to avoid the call to Context.default() when running tests\n  # which mock IPython.get_ipython().\n\n  def _get_project_id():\n    try:\n      return google.datalab.Context.default().project_id\n    except Exception:\n      return None\n\n  def _set_project_id(project_id):\n    context = google.datalab.Context.default()\n    context.set_project_id(project_id)\n    try:\n      from datalab.context import Context as _old_context\n      _old_context.default().set_project_id(project_id)\n    except ImportError:\n      # If the old library is not loaded, then we don't have to do anything\n      pass\n\n  try:\n    if 'datalab_project_id' not in _IPython.get_ipython().user_ns:\n      _IPython.get_ipython().user_ns['datalab_project_id'] = _get_project_id\n      _IPython.get_ipython().user_ns['set_datalab_project_id'] = _set_project_id\n  except TypeError:\n    pass", "response": "Loads an IPython notebook extension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a set of %%sql arguments or get the default value of the arguments.", "response": "def _get_sql_args(parser, args=None):\n    \"\"\" Parse a set of %%sql arguments or get the default value of the arguments.\n\n    Args:\n      parser: the argument parser to use.\n      args: the argument flags. May be a string or a list. If omitted the empty string is used so\n          we can get the default values for the arguments. These are all used to override the\n          arg parser. Alternatively args may be a dictionary, in which case it overrides the\n          default values from the arg parser.\n    Returns:\n      A dictionary of argument names and values.\n    \"\"\"\n    overrides = None\n    if args is None:\n      tokens = []\n    elif isinstance(args, basestring):\n      command_line = ' '.join(args.split('\\n'))\n      tokens = shlex.split(command_line)\n    elif isinstance(args, dict):\n      overrides = args\n      tokens = []\n    else:\n      tokens = args\n\n    args = {} if parser is None else vars(parser.parse_args(tokens))\n    if overrides:\n      args.update(overrides)\n\n    # Don't return any args that are None as we don't want to expand to 'None'\n    return {arg: value for arg, value in args.items() if value is not None}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a SQLStatement string or module plus command line arguments or a dictionary return a SqlStatement and a final dictionary for variable resolution.", "response": "def get_sql_statement_with_environment(item, args=None):\n    \"\"\" Given a SQLStatement, string or module plus command line args or a dictionary,\n     return a SqlStatement and final dictionary for variable resolution.\n\n    Args:\n      item: a SqlStatement, %%sql module, or string containing a query.\n      args: a string of command line arguments or a dictionary of values.\n\n    Returns:\n      A SqlStatement for the query or module, plus a dictionary of variable values to use.\n    \"\"\"\n    if isinstance(item, basestring):\n      item = _sql_statement.SqlStatement(item)\n    elif not isinstance(item, _sql_statement.SqlStatement):\n      item = SqlModule.get_default_query_from_module(item)\n      if not item:\n        raise Exception('Expected a SQL statement or module but got %s' % str(item))\n\n    env = {}\n    if item.module:\n      env.update(item.module.__dict__)\n      parser = env.get(_utils._SQL_MODULE_ARGPARSE, None)\n      if parser:\n        args = SqlModule._get_sql_args(parser, args=args)\n      else:\n        args = None\n\n    if isinstance(args, dict):\n      env.update(args)\n\n    return item, env"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand a SqlStatement with a set of arguments.", "response": "def expand(sql, args=None):\n    \"\"\" Expand a SqlStatement, query string or SqlModule with a set of arguments.\n\n    Args:\n      sql: a SqlStatement, %%sql module, or string containing a query.\n      args: a string of command line arguments or a dictionary of values. If a string, it is\n          passed to the argument parser for the SqlModule associated with the SqlStatement or\n          SqlModule. If a dictionary, it is used to override any default arguments from the\n          argument parser. If the sql argument is a string then args must be None or a dictionary\n          as in this case there is no associated argument parser.\n    Returns:\n      The expanded SQL, list of referenced scripts, and list of referenced external tables.\n    \"\"\"\n    sql, args = SqlModule.get_sql_statement_with_environment(sql, args)\n    return _sql_statement.SqlStatement.format(sql._sql, args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_dataset_name(name, project_id=None):\n  _project_id = _dataset_id = None\n  if isinstance(name, basestring):\n    # Try to parse as absolute name first.\n    m = re.match(_ABS_DATASET_NAME_PATTERN, name, re.IGNORECASE)\n    if m is not None:\n      _project_id, _dataset_id = m.groups()\n    else:\n      # Next try to match as a relative name implicitly scoped within current project.\n      m = re.match(_REL_DATASET_NAME_PATTERN, name)\n      if m is not None:\n        groups = m.groups()\n        _dataset_id = groups[0]\n  elif isinstance(name, dict):\n    try:\n      _dataset_id = name['dataset_id']\n      _project_id = name['project_id']\n    except KeyError:\n      pass\n  else:\n    # Try treat as an array or tuple\n    if len(name) == 2:\n      # Treat as a tuple or array.\n      _project_id, _dataset_id = name\n    elif len(name) == 1:\n      _dataset_id = name[0]\n  if not _dataset_id:\n    raise Exception('Invalid dataset name: ' + str(name))\n  if not _project_id:\n    _project_id = project_id\n\n  return DatasetName(_project_id, _dataset_id)", "response": "Parses a dataset name into its individual parts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a table name into its individual parts.", "response": "def parse_table_name(name, project_id=None, dataset_id=None):\n  \"\"\"Parses a table name into its individual parts.\n\n  Args:\n    name: the name to parse, or a tuple, dictionary or array containing the parts.\n    project_id: the expected project ID. If the name does not contain a project ID,\n        this will be used; if the name does contain a project ID and it does not match\n        this, an exception will be thrown.\n    dataset_id: the expected dataset ID. If the name does not contain a dataset ID,\n        this will be used; if the name does contain a dataset ID and it does not match\n        this, an exception will be thrown.\n  Returns:\n    A TableName named tuple consisting of the full name and individual name parts.\n  Raises:\n    Exception: raised if the name doesn't match the expected formats, or a project_id and/or\n        dataset_id was provided that does not match that in the name.\n  \"\"\"\n  _project_id = _dataset_id = _table_id = _decorator = None\n  if isinstance(name, basestring):\n    # Try to parse as absolute name first.\n    m = re.match(_ABS_TABLE_NAME_PATTERN, name, re.IGNORECASE)\n    if m is not None:\n      _project_id, _dataset_id, _table_id, _decorator = m.groups()\n    else:\n      # Next try to match as a relative name implicitly scoped within current project.\n      m = re.match(_REL_TABLE_NAME_PATTERN, name)\n      if m is not None:\n        groups = m.groups()\n        _project_id, _dataset_id, _table_id, _decorator =\\\n            project_id, groups[0], groups[1], groups[2]\n      else:\n        # Finally try to match as a table name only.\n        m = re.match(_TABLE_NAME_PATTERN, name)\n        if m is not None:\n          groups = m.groups()\n          _project_id, _dataset_id, _table_id, _decorator =\\\n              project_id, dataset_id, groups[0], groups[1]\n  elif isinstance(name, dict):\n    try:\n      _table_id = name['table_id']\n      _dataset_id = name['dataset_id']\n      _project_id = name['project_id']\n    except KeyError:\n      pass\n  else:\n    # Try treat as an array or tuple\n    if len(name) == 4:\n      _project_id, _dataset_id, _table_id, _decorator = name\n    elif len(name) == 3:\n      _project_id, _dataset_id, _table_id = name\n    elif len(name) == 2:\n      _dataset_id, _table_id = name\n  if not _table_id:\n    raise Exception('Invalid table name: ' + str(name))\n  if not _project_id:\n    _project_id = project_id\n  if not _dataset_id:\n    _dataset_id = dataset_id\n  if not _decorator:\n    _decorator = ''\n\n  return TableName(_project_id, _dataset_id, _table_id, _decorator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a chart using Google Charts.", "response": "def chart(line, cell=None):\n  \"\"\" Generate charts with Google Charts. Use %chart --help for more details. \"\"\"\n  parser = _commands.CommandParser(prog='%chart', description=\"\"\"\nGenerate an inline chart using Google Charts using the data in a Table, Query, dataframe, or list.\nNumerous types of charts are supported. Options for the charts can be specified in the cell body\nusing YAML or JSON.\n\"\"\")\n  for chart_type in ['annotation', 'area', 'bars', 'bubbles', 'calendar', 'candlestick', 'columns',\n                     'combo', 'gauge', 'geo', 'heatmap', 'histogram', 'line', 'map', 'org',\n                     'paged_table', 'pie', 'sankey', 'scatter', 'stepped_area', 'table',\n                     'timeline', 'treemap']:\n    subparser = parser.subcommand(chart_type, 'Generate a %s chart.' % chart_type)\n    subparser.add_argument('-f', '--fields',\n                           help='The field(s) to include in the chart')\n    subparser.add_argument('-d', '--data',\n                           help='The name of the variable referencing the Table or Query to chart',\n                           required=True)\n    subparser.set_defaults(chart=chart_type)\n\n  parser.set_defaults(func=_chart_cell)\n  return _utils.handle_magic_line(line, cell, parser)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a predict_fn that can be used by LIME text explainer.", "response": "def _make_text_predict_fn(self, labels, instance, column_to_explain):\n        \"\"\"Create a predict_fn that can be used by LIME text explainer. \"\"\"\n\n        def _predict_fn(perturbed_text):\n            predict_input = []\n            for x in perturbed_text:\n                instance_copy = dict(instance)\n                instance_copy[column_to_explain] = x\n                predict_input.append(instance_copy)\n\n            df = _local_predict.get_prediction_results(self._model_dir, predict_input,\n                                                       self._headers, with_source=False)\n            probs = _local_predict.get_probs_for_labels(labels, df)\n            return np.asarray(probs)\n\n        return _predict_fn"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a predict_fn that can be used by LIME image explainer.", "response": "def _make_image_predict_fn(self, labels, instance, column_to_explain):\n        \"\"\"Create a predict_fn that can be used by LIME image explainer. \"\"\"\n\n        def _predict_fn(perturbed_image):\n\n            predict_input = []\n            for x in perturbed_image:\n                instance_copy = dict(instance)\n                instance_copy[column_to_explain] = Image.fromarray(x)\n                predict_input.append(instance_copy)\n\n            df = _local_predict.get_prediction_results(\n                self._model_dir, predict_input, self._headers,\n                img_cols=self._image_columns, with_source=False)\n            probs = _local_predict.get_probs_for_labels(labels, df)\n            return np.asarray(probs)\n\n        return _predict_fn"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all categories for each categorical column from training data.", "response": "def _get_unique_categories(self, df):\n        \"\"\"Get all categories for each categorical columns from training data.\"\"\"\n\n        categories = []\n        for col in self._categorical_columns:\n            categocial = pd.Categorical(df[col])\n            col_categories = list(map(str, categocial.categories))\n            col_categories.append('_UNKNOWN')\n            categories.append(col_categories)\n        return categories"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _preprocess_data_for_tabular_explain(self, df, categories):\n\n        df = df.copy()\n\n        # Remove non tabular columns (text, image).\n        for col in list(df.columns):\n            if col not in (self._categorical_columns + self._numeric_columns):\n                del df[col]\n\n        # Convert categorical values into indices.\n        for col_name, col_categories in zip(self._categorical_columns, categories):\n            df[col_name] = df[col_name].apply(\n                lambda x: col_categories.index(str(x)) if str(x) in col_categories\n                else len(col_categories) - 1)\n\n        # Make sure numeric values are really numeric\n        for numeric_col in self._numeric_columns:\n            df[numeric_col] = df[numeric_col].apply(lambda x: float(x))\n\n        return df.as_matrix(self._categorical_columns + self._numeric_columns)", "response": "Preprocess training set in numpy array and categorical names from raw training data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_tabular_predict_fn(self, labels, instance, categories):\n\n        def _predict_fn(np_instance):\n\n            df = pd.DataFrame(\n                np_instance,\n                columns=(self._categorical_columns + self._numeric_columns))\n\n            # Convert categorical indices back to categories.\n            for col_name, col_categories in zip(self._categorical_columns, categories):\n                df[col_name] = df[col_name].apply(lambda x: col_categories[int(x)])\n\n            # Add columns that do not exist in the perturbed data,\n            # such as key, text, and image data.\n            for col_name in self._headers:\n                if col_name not in (self._categorical_columns + self._numeric_columns):\n                    df[col_name] = instance[col_name]\n\n            r = _local_predict.get_prediction_results(\n                self._model_dir, df, self._headers, with_source=False)\n            probs = _local_predict.get_probs_for_labels(labels, r)\n            probs = np.asarray(probs)\n            return probs\n\n        return _predict_fn", "response": "Create a predict_fn that can be used by LIME tabular explainer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef explain_tabular(self, trainset, labels, instance, num_features=5, kernel_width=3):\n        from lime.lime_tabular import LimeTabularExplainer\n\n        if isinstance(instance, six.string_types):\n            instance = next(csv.DictReader([instance], fieldnames=self._headers))\n\n        categories = self._get_unique_categories(trainset)\n        np_trainset = self._preprocess_data_for_tabular_explain(trainset, categories)\n        predict_fn = self._make_tabular_predict_fn(labels, instance, categories)\n        prediction_df = pd.DataFrame([instance])\n        prediction_instance = self._preprocess_data_for_tabular_explain(prediction_df, categories)\n\n        explainer = LimeTabularExplainer(\n            np_trainset,\n            feature_names=(self._categorical_columns + self._numeric_columns),\n            class_names=labels,\n            categorical_features=range(len(categories)),\n            categorical_names={i: v for i, v in enumerate(categories)},\n            kernel_width=kernel_width)\n\n        exp = explainer.explain_instance(\n            prediction_instance[0],\n            predict_fn,\n            num_features=num_features,\n            labels=range(len(labels)))\n        return exp", "response": "Explain categorical and numeric features for a prediction by LIME."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexplain a text field of a prediction. It analyze the prediction by LIME, and returns a report of which words are most impactful in contributing to certain labels. Args: labels: a list of labels to explain. instance: the prediction instance. It needs to conform to model's input. Can be a csv line string, or a dict. column_name: which text column to explain. Can be None if there is only one text column in the model input. num_features: maximum number of words (features) to analyze. Passed to LIME LimeTextExplainer directly. num_samples: size of the neighborhood to learn the linear model. Passed to LIME LimeTextExplainer directly. Returns: A LIME's lime.explanation.Explanation. Throws: ValueError if the given text column is not found in model input or column_name is None but there are multiple text columns in model input.", "response": "def explain_text(self, labels, instance, column_name=None, num_features=10, num_samples=5000):\n        \"\"\"Explain a text field of a prediction.\n\n        It analyze the prediction by LIME, and returns a report of which words are most impactful\n        in contributing to certain labels.\n\n        Args:\n          labels: a list of labels to explain.\n          instance: the prediction instance. It needs to conform to model's input. Can be a csv\n              line string, or a dict.\n          column_name: which text column to explain. Can be None if there is only one text column\n              in the model input.\n          num_features: maximum number of words (features) to analyze. Passed to\n              LIME LimeTextExplainer directly.\n          num_samples: size of the neighborhood to learn the linear model. Passed to\n              LIME LimeTextExplainer directly.\n\n        Returns:\n          A LIME's lime.explanation.Explanation.\n\n        Throws:\n          ValueError if the given text column is not found in model input or column_name is None\n              but there are multiple text columns in model input.\n        \"\"\"\n\n        from lime.lime_text import LimeTextExplainer\n\n        if len(self._text_columns) > 1 and not column_name:\n            raise ValueError('There are multiple text columns in the input of the model. ' +\n                             'Please specify \"column_name\".')\n        elif column_name and column_name not in self._text_columns:\n            raise ValueError('Specified column_name \"%s\" not found in the model input.'\n                             % column_name)\n\n        text_column_name = column_name if column_name else self._text_columns[0]\n        if isinstance(instance, six.string_types):\n            instance = next(csv.DictReader([instance], fieldnames=self._headers))\n\n        predict_fn = self._make_text_predict_fn(labels, instance, text_column_name)\n        explainer = LimeTextExplainer(class_names=labels)\n        exp = explainer.explain_instance(\n            instance[text_column_name], predict_fn, labels=range(len(labels)),\n            num_features=num_features, num_samples=num_samples)\n        return exp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexplaining an image of a prediction. It analyze the prediction by LIME, and returns a report of which words are most impactful in contributing to certain labels. Args: labels: a list of labels to explain. instance: the prediction instance. It needs to conform to model's input. Can be a csv line string, or a dict. column_name: which image column to explain. Can be None if there is only one image column in the model input. num_features: maximum number of areas (features) to analyze. Passed to LIME LimeImageExplainer directly. num_samples: size of the neighborhood to learn the linear model. Passed to LIME LimeImageExplainer directly. batch_size: size of batches passed to predict_fn. Passed to LIME LimeImageExplainer directly. hide_color: the color used to perturb images. Passed to LIME LimeImageExplainer directly. Returns: A LIME's lime.explanation.Explanation. Throws: ValueError if the given image column is not found in model input or column_name is None but there are multiple image columns in model input.", "response": "def explain_image(self, labels, instance, column_name=None, num_features=100000,\n                      num_samples=300, batch_size=200, hide_color=0):\n        \"\"\"Explain an image of a prediction.\n\n        It analyze the prediction by LIME, and returns a report of which words are most impactful\n        in contributing to certain labels.\n\n        Args:\n          labels: a list of labels to explain.\n          instance: the prediction instance. It needs to conform to model's input. Can be a csv\n              line string, or a dict.\n          column_name: which image column to explain. Can be None if there is only one image column\n              in the model input.\n          num_features: maximum number of areas (features) to analyze. Passed to\n              LIME LimeImageExplainer directly.\n          num_samples: size of the neighborhood to learn the linear model. Passed to\n              LIME LimeImageExplainer directly.\n          batch_size: size of batches passed to predict_fn. Passed to\n              LIME LimeImageExplainer directly.\n          hide_color: the color used to perturb images. Passed to\n              LIME LimeImageExplainer directly.\n\n        Returns:\n          A LIME's lime.explanation.Explanation.\n\n        Throws:\n          ValueError if the given image column is not found in model input or column_name is None\n              but there are multiple image columns in model input.\n        \"\"\"\n\n        from lime.lime_image import LimeImageExplainer\n\n        if len(self._image_columns) > 1 and not column_name:\n            raise ValueError('There are multiple image columns in the input of the model. ' +\n                             'Please specify \"column_name\".')\n        elif column_name and column_name not in self._image_columns:\n            raise ValueError('Specified column_name \"%s\" not found in the model input.'\n                             % column_name)\n\n        image_column_name = column_name if column_name else self._image_columns[0]\n        if isinstance(instance, six.string_types):\n            instance = next(csv.DictReader([instance], fieldnames=self._headers))\n\n        predict_fn = self._make_image_predict_fn(labels, instance, image_column_name)\n        explainer = LimeImageExplainer()\n        with file_io.FileIO(instance[image_column_name], 'rb') as fi:\n            im = Image.open(fi)\n        im.thumbnail((299, 299), Image.ANTIALIAS)\n        rgb_im = np.asarray(im.convert('RGB'))\n        exp = explainer.explain_instance(\n            rgb_im, predict_fn, labels=range(len(labels)), top_labels=None,\n            hide_color=hide_color, num_features=num_features,\n            num_samples=num_samples, batch_size=batch_size)\n        return exp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing gradients from prob of label to image. Used by integrated gradients.", "response": "def _image_gradients(self, input_csvlines, label, image_column_name):\n        \"\"\"Compute gradients from prob of label to image. Used by integrated gradients (probe).\"\"\"\n\n        with tf.Graph().as_default() as g, tf.Session() as sess:\n            logging_level = tf.logging.get_verbosity()\n            try:\n                tf.logging.set_verbosity(tf.logging.ERROR)\n                meta_graph_pb = tf.saved_model.loader.load(\n                    sess=sess,\n                    tags=[tf.saved_model.tag_constants.SERVING],\n                    export_dir=self._model_dir)\n            finally:\n                tf.logging.set_verbosity(logging_level)\n\n            signature = meta_graph_pb.signature_def['serving_default']\n            input_alias_map = {name: tensor_info_proto.name\n                               for (name, tensor_info_proto) in signature.inputs.items()}\n            output_alias_map = {name: tensor_info_proto.name\n                                for (name, tensor_info_proto) in signature.outputs.items()}\n\n            csv_tensor_name = list(input_alias_map.values())[0]\n\n            # The image tensor is already built into ML Workbench graph.\n            float_image = g.get_tensor_by_name(\"import/gradients_%s:0\" % image_column_name)\n            if label not in output_alias_map:\n                raise ValueError('The label \"%s\" does not exist in output map.' % label)\n\n            prob = g.get_tensor_by_name(output_alias_map[label])\n            grads = tf.gradients(prob, float_image)[0]\n            grads_values = sess.run(fetches=grads, feed_dict={csv_tensor_name: input_csvlines})\n\n        return grads_values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef probe_image(self, labels, instance, column_name=None, num_scaled_images=50,\n                    top_percent=10):\n        \"\"\" Get pixel importance of the image.\n\n        It performs pixel sensitivity analysis by showing only the most important pixels to a\n        certain label in the image. It uses integrated gradients to measure the\n        importance of each pixel.\n\n        Args:\n            labels: labels to compute gradients from.\n            instance: the prediction instance. It needs to conform to model's input. Can be a csv\n              line string, or a dict.\n            img_column_name: the name of the image column to probe. If there is only one image\n                column it can be None.\n            num_scaled_images: Number of scaled images to get grads from. For example, if 10,\n                the image will be scaled by 0.1, 0.2, ..., 0,9, 1.0 and it will produce\n                10 images for grads computation.\n            top_percent: The percentile of pixels to show only. for example, if 10,\n                only top 10% impactful pixels will be shown and rest of the pixels will be black.\n\n        Returns:\n            A tuple. First is the resized original image (299x299x3). Second is a list of\n                the visualization with same size that highlights the most important pixels, one\n                per each label.\n        \"\"\"\n\n        if len(self._image_columns) > 1 and not column_name:\n            raise ValueError('There are multiple image columns in the input of the model. ' +\n                             'Please specify \"column_name\".')\n        elif column_name and column_name not in self._image_columns:\n            raise ValueError('Specified column_name \"%s\" not found in the model input.' %\n                             column_name)\n\n        image_column_name = column_name if column_name else self._image_columns[0]\n        if isinstance(instance, six.string_types):\n            instance = next(csv.DictReader([instance], fieldnames=self._headers))\n\n        image_path = instance[image_column_name]\n\n        with file_io.FileIO(image_path, 'rb') as fi:\n            im = Image.open(fi)\n\n        resized_image = im.resize((299, 299))\n\n        # Produce a list of scaled images, create instances (csv lines) from these images.\n        step = 1. / num_scaled_images\n        scales = np.arange(0.0, 1.0, step) + step\n        csv_lines = []\n        for s in scales:\n            pixels = (np.asarray(resized_image) * s).astype('uint8')\n            scaled_image = Image.fromarray(pixels)\n            buf = io.BytesIO()\n            scaled_image.save(buf, \"JPEG\")\n            encoded_image = base64.urlsafe_b64encode(buf.getvalue()).decode('ascii')\n            instance_copy = dict(instance)\n            instance_copy[image_column_name] = encoded_image\n\n            buf = six.StringIO()\n            writer = csv.DictWriter(buf, fieldnames=self._headers, lineterminator='')\n            writer.writerow(instance_copy)\n            csv_lines.append(buf.getvalue())\n\n        integrated_gradients_images = []\n        for label in labels:\n          # Send to tf model to get gradients.\n          grads = self._image_gradients(csv_lines, label, image_column_name)\n          integrated_grads = resized_image * np.average(grads, axis=0)\n\n          # Gray scale the grads by removing color dimension.\n          # abs() is for getting the most impactful pixels regardless positive or negative.\n          grayed = np.average(abs(integrated_grads), axis=2)\n          grayed = np.transpose([grayed, grayed, grayed], axes=[1, 2, 0])\n\n          # Only show the most impactful pixels.\n          p = np.percentile(grayed, 100 - top_percent)\n          viz_window = np.where(grayed > p, 1, 0)\n          vis = resized_image * viz_window\n          im_vis = Image.fromarray(np.uint8(vis))\n          integrated_gradients_images.append(im_vis)\n\n        return resized_image, integrated_gradients_images", "response": "This function is used to probe the image of a specific class. It returns the image that is used to compute the gradients of the image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting details of the specified model from CloudML Service.", "response": "def get_model_details(self, model_name):\n    \"\"\"Get details of the specified model from CloudML Service.\n\n    Args:\n      model_name: the name of the model. It can be a model full name\n          (\"projects/[project_id]/models/[model_name]\") or just [model_name].\n      Returns: a dictionary of the model details.\n    \"\"\"\n    full_name = model_name\n    if not model_name.startswith('projects/'):\n      full_name = ('projects/%s/models/%s' % (self._project_id, model_name))\n    return self._api.projects().models().get(name=full_name).execute()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self, model_name):\n    body = {'name': model_name}\n    parent = 'projects/' + self._project_id\n    # Model creation is instant. If anything goes wrong, Exception will be thrown.\n    return self._api.projects().models().create(body=body, parent=parent).execute()", "response": "Create a model.\n\n    Args:\n      model_name: the short name of the model, such as \"iris\".\n    Returns:\n      If successful, returns informaiton of the model, such as\n      {u'regions': [u'us-central1'], u'name': u'projects/myproject/models/mymodel'}\n    Raises:\n      If the model creation failed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a model. Args: model_name: the name of the model. It can be a model full name (\"projects/[project_id]/models/[model_name]\") or just [model_name].", "response": "def delete(self, model_name):\n    \"\"\"Delete a model.\n\n    Args:\n      model_name: the name of the model. It can be a model full name\n          (\"projects/[project_id]/models/[model_name]\") or just [model_name].\n    \"\"\"\n    full_name = model_name\n    if not model_name.startswith('projects/'):\n      full_name = ('projects/%s/models/%s' % (self._project_id, model_name))\n    response = self._api.projects().models().delete(name=full_name).execute()\n    if 'name' not in response:\n      raise Exception('Invalid response from service. \"name\" is not found.')\n    _util.wait_for_long_running_operation(response['name'])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(self, count=10):\n    import IPython\n    data = []\n    # Add range(count) to loop so it will stop either it reaches count, or iteration\n    # on self is exhausted. \"self\" is iterable (see __iter__() method).\n    for _, model in zip(range(count), self.get_iterator()):\n      element = {'name': model['name']}\n      if 'defaultVersion' in model:\n        version_short_name = model['defaultVersion']['name'].split('/')[-1]\n        element['defaultVersion'] = version_short_name\n      data.append(element)\n\n    IPython.display.display(\n        datalab.utils.commands.render_dictionary(data, ['name', 'defaultVersion']))", "response": "List the models under the current project in a table view."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef describe(self, model_name):\n    model_yaml = yaml.safe_dump(self.get_model_details(model_name), default_flow_style=False)\n    print(model_yaml)", "response": "Prints the details of a specified model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_version_details(self, version_name):\n    name = ('%s/versions/%s' % (self._full_model_name, version_name))\n    return self._api.projects().models().versions().get(name=name).execute()", "response": "Get details of a version."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeploys a model version to the cloud.", "response": "def deploy(self, version_name, path, runtime_version=None):\n    \"\"\"Deploy a model version to the cloud.\n\n    Args:\n      version_name: the name of the version in short form, such as \"v1\".\n      path: the Google Cloud Storage path (gs://...) which contains the model files.\n      runtime_version: the ML Engine runtime version as a string, example '1.2'.\n          See https://cloud.google.com/ml-engine/docs/concepts/runtime-version-list\n          for a list of runtimes. If None, the ML Engine service will pick one.\n\n    Raises: Exception if the path is invalid or does not contain expected files.\n            Exception if the service returns invalid response.\n    \"\"\"\n    if not path.startswith('gs://'):\n      raise Exception('Invalid path. Only Google Cloud Storage path (gs://...) is accepted.')\n\n    # If there is no \"export.meta\" or\"saved_model.pb\" under path but there is\n    # path/model/export.meta or path/model/saved_model.pb, then append /model to the path.\n    if not datalab.storage.Object.from_url(os.path.join(path, 'export.meta')).exists() and not \\\n            datalab.storage.Object.from_url(os.path.join(path, 'saved_model.pb')).exists():\n      if datalab.storage.Object.from_url(os.path.join(path, 'model', 'export.meta')).exists() or \\\n              datalab.storage.Object.from_url(os.path.join(path, 'model',\n                                                           'saved_model.pb')).exists():\n        path = os.path.join(path, 'model')\n      else:\n        print('Cannot find export.meta or saved_model.pb, but continue with deployment anyway.')\n\n    body = {'name': self._model_name}\n    parent = 'projects/' + self._project_id\n    try:\n      self._api.projects().models().create(body=body, parent=parent).execute()\n    except:\n      # Trying to create an already existing model gets an error. Ignore it.\n      pass\n    body = {\n      'name': version_name,\n      'deployment_uri': path,\n    }\n\n    if runtime_version:\n      body['runtime_version'] = runtime_version\n\n    response = self._api.projects().models().versions().create(\n      body=body, parent=self._full_model_name).execute()\n    if 'name' not in response:\n      raise Exception('Invalid response from service. \"name\" is not found.')\n    _util.wait_for_long_running_operation(response['name'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a version of model.", "response": "def delete(self, version_name):\n    \"\"\"Delete a version of model.\n\n    Args:\n      version_name: the name of the version in short form, such as \"v1\".\n    \"\"\"\n    name = ('%s/versions/%s' % (self._full_model_name, version_name))\n    response = self._api.projects().models().versions().delete(name=name).execute()\n    if 'name' not in response:\n      raise Exception('Invalid response from service. \"name\" is not found.')\n    _util.wait_for_long_running_operation(response['name'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget prediction results from features instances.", "response": "def predict(self, version_name, data):\n    \"\"\"Get prediction results from features instances.\n\n    Args:\n      version_name: the name of the version used for prediction.\n      data: typically a list of instance to be submitted for prediction. The format of the\n          instance depends on the model. For example, structured data model may require\n          a csv line for each instance.\n          Note that online prediction only works on models that take one placeholder value,\n          such as a string encoding a csv line.\n    Returns:\n      A list of prediction results for given instances. Each element is a dictionary representing\n          output mapping from the graph.\n      An example:\n        [{\"predictions\": 1, \"score\": [0.00078, 0.71406, 0.28515]},\n         {\"predictions\": 1, \"score\": [0.00244, 0.99634, 0.00121]}]\n    \"\"\"\n    full_version_name = ('%s/versions/%s' % (self._full_model_name, version_name))\n    request = self._api.projects().predict(body={'instances': data},\n                                           name=full_version_name)\n    request.headers['user-agent'] = 'GoogleCloudDataLab/1.0'\n    result = request.execute()\n    if 'predictions' not in result:\n      raise Exception('Invalid response from service. Cannot find \"predictions\" in response.')\n\n    return result['predictions']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef describe(self, version_name):\n    version_yaml = yaml.safe_dump(self.get_version_details(version_name),\n                                  default_flow_style=False)\n    print(version_yaml)", "response": "Prints information about a specified version of the current version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list(self):\n    import IPython\n\n    # \"self\" is iterable (see __iter__() method).\n    data = [{'name': version['name'].split()[-1],\n             'deploymentUri': version['deploymentUri'], 'createTime': version['createTime']}\n            for version in self.get_iterator()]\n    IPython.display.display(\n        datalab.utils.commands.render_dictionary(data, ['name', 'deploymentUri', 'createTime']))", "response": "List versions under the current model in a table view."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef result(self):\n    self.wait()\n    if self.failed:\n      raise Exception('Query failed: %s' % str(self.errors))\n    return self._table", "response": "Get the table used for the query."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_preprocessing_fn(output_dir, features, keep_target):\n  def preprocessing_fn(inputs):\n    \"\"\"Preprocessing function.\n\n    Args:\n      inputs: dictionary of raw input tensors\n\n    Returns:\n      A dictionary of transformed tensors\n    \"\"\"\n    stats = json.loads(\n      file_io.read_file_to_string(\n          os.path.join(output_dir, STATS_FILE)).decode())\n\n    result = {}\n    for name, transform in six.iteritems(features):\n      transform_name = transform['transform']\n      source_column = transform['source_column']\n\n      if transform_name == TARGET_TRANSFORM:\n        if not keep_target:\n          continue\n        if file_io.file_exists(os.path.join(output_dir, VOCAB_ANALYSIS_FILE % source_column)):\n          transform_name = 'one_hot'\n        else:\n          transform_name = 'identity'\n\n      if transform_name == 'identity':\n        result[name] = inputs[source_column]\n      elif transform_name == 'scale':\n        result[name] = _scale(\n            inputs[name],\n            min_x_value=stats['column_stats'][source_column]['min'],\n            max_x_value=stats['column_stats'][source_column]['max'],\n            output_min=transform.get('value', 1) * (-1),\n            output_max=transform.get('value', 1))\n      elif transform_name in [ONE_HOT_TRANSFORM, MULTI_HOT_TRANSFORM]:\n        vocab, ex_count = read_vocab_file(\n            os.path.join(output_dir, VOCAB_ANALYSIS_FILE % source_column))\n        if transform_name == MULTI_HOT_TRANSFORM:\n          separator = transform.get('separator', ' ')\n          tokens = tf.string_split(inputs[source_column], separator)\n          result[name] = _string_to_int(tokens, vocab)\n        else:\n          result[name] = _string_to_int(inputs[source_column], vocab)\n      elif transform_name == IMAGE_TRANSFORM:\n        make_image_to_vec_fn = _make_image_to_vec_tito(\n            name, checkpoint=transform.get('checkpoint', None))\n        result[name] = make_image_to_vec_fn(inputs[source_column])\n      else:\n        raise ValueError('unknown transform %s' % transform_name)\n    return result\n\n  return preprocessing_fn", "response": "Makes a preprocessing function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the indices of the transformed features.", "response": "def get_transformed_feature_indices(features, stats):\n  \"\"\"Returns information about the transformed features.\n\n  Returns:\n    List in the from\n    [(transformed_feature_name, {size: int, index_start: int})]\n  \"\"\"\n\n  feature_indices = []\n  index_start = 1\n  for name, transform in sorted(six.iteritems(features)):\n    transform_name = transform['transform']\n    source_column = transform['source_column']\n    info = {}\n    if transform_name in [IDENTITY_TRANSFORM, SCALE_TRANSFORM]:\n      info['size'] = 1\n    elif transform_name in [ONE_HOT_TRANSFORM, MULTI_HOT_TRANSFORM]:\n      info['size'] = stats['column_stats'][source_column]['vocab_size']\n    elif transform_name == IMAGE_TRANSFORM:\n      info['size'] = IMAGE_BOTTLENECK_TENSOR_SIZE\n    elif transform_name == TARGET_TRANSFORM:\n      info['size'] = 0\n    else:\n      raise ValueError('xgboost does not support transform \"%s\"' % transform)\n\n    info['index_start'] = index_start\n    index_start += info['size']\n    feature_indices.append((name, info))\n\n  return feature_indices"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a feature_map from transformed features.", "response": "def create_feature_map(features, feature_indices, output_dir):\n  \"\"\"Returns feature_map about the transformed features.\n\n  feature_map includes information such as:\n    1, cat1=0\n    2, cat1=1\n    3, numeric1\n    ...\n  Returns:\n    List in the from\n    [(index, feature_description)]\n  \"\"\"\n  feature_map = []\n  for name, info in feature_indices:\n    transform_name = features[name]['transform']\n    source_column = features[name]['source_column']\n    if transform_name in [IDENTITY_TRANSFORM, SCALE_TRANSFORM]:\n      feature_map.append((info['index_start'], name))\n    elif transform_name in [ONE_HOT_TRANSFORM, MULTI_HOT_TRANSFORM]:\n      vocab, _ = read_vocab_file(\n          os.path.join(output_dir, VOCAB_ANALYSIS_FILE % source_column))\n      for i, word in enumerate(vocab):\n        if transform_name == ONE_HOT_TRANSFORM:\n          feature_map.append((info['index_start'] + i, '%s=%s' % (source_column, word)))\n        elif transform_name == MULTI_HOT_TRANSFORM:\n          feature_map.append((info['index_start'] + i, '%s has \"%s\"' % (source_column, word)))\n    elif transform_name == IMAGE_TRANSFORM:\n      for i in range(info['size']):\n        feature_map.append((info['index_start'] + i, '%s image feature %d' % (source_column, i)))\n\n  return feature_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, query):\n    if isinstance(query, _query.Query):\n      query = query.sql\n    try:\n      response = self._table._api.tables_insert(self._table.name, query=query)\n    except Exception as e:\n      raise e\n    if 'selfLink' in response:\n      return self\n    raise Exception(\"View %s could not be created as it already exists\" % str(self))", "response": "Creates a new view with the specified query."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(self, fields=None, count=5, sampling=None, use_cache=True, dialect=None,\n             billing_tier=None):\n    \"\"\"Retrieves a sampling of data from the view.\n\n    Args:\n      fields: an optional list of field names to retrieve.\n      count: an optional count of rows to retrieve which is used if a specific\n          sampling is not specified.\n      sampling: an optional sampling strategy to apply to the view.\n      use_cache: whether to use cached results or not.\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A QueryResultsTable object containing the resulting data.\n    Raises:\n      Exception if the sample query could not be executed or the query response was malformed.\n    \"\"\"\n    return self._table.sample(fields=fields, count=count, sampling=sampling, use_cache=use_cache,\n                              dialect=dialect, billing_tier=billing_tier)", "response": "Retrieves a sample of data from the table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef results(self, use_cache=True, dialect=None, billing_tier=None):\n    return self._materialization.results(use_cache=use_cache, dialect=dialect,\n                                         billing_tier=billing_tier)", "response": "Materialize the view synchronously and return a QueryResultsTable containing the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_async(self, table_name=None, table_mode='create', use_cache=True, priority='high',\n                    allow_large_results=False, dialect=None, billing_tier=None):\n    \"\"\"Materialize the View asynchronously.\n\n    Args:\n      table_name: the result table name; if None, then a temporary table will be used.\n      table_mode: one of 'create', 'overwrite' or 'append'. If 'create' (the default), the request\n          will fail if the table exists.\n      use_cache: whether to use past query results or ignore cache. Has no effect if destination is\n          specified (default True).\n      priority:one of 'low' or 'high' (default). Note that 'high' is more expensive, but is\n          better suited to exploratory analysis.\n      allow_large_results: whether to allow large results; i.e. compressed data over 100MB. This is\n          slower and requires a table_name to be specified) (default False).\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A QueryJob for the materialization\n    Raises:\n      Exception (KeyError) if View could not be materialized.\n    \"\"\"\n    return self._materialization.execute_async(table_name=table_name, table_mode=table_mode,\n                                               use_cache=use_cache, priority=priority,\n                                               allow_large_results=allow_large_results,\n                                               dialect=dialect, billing_tier=billing_tier)", "response": "Materialize the View asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_notebook_item(name):\n  env = notebook_environment()\n  return google.datalab.utils.get_item(env, name)", "response": "Get an item from the IPython environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_field_list(fields, schema):\n  # If the fields weren't supplied get them from the schema.\n  if schema:\n    all_fields = [f['name'] for f in schema._bq_schema if f['type'] != 'RECORD']\n\n  if isinstance(fields, list):\n    if schema:\n      # validate fields exist\n      for f in fields:\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n    return fields\n  if isinstance(fields, basestring) and fields != '*':\n    if schema:\n      # validate fields exist\n      for f in fields.split(','):\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n      return fields.split(',')\n  if not schema:\n    return []\n  return all_fields", "response": "Convert a field list spec into a real list of field names."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_data_from_list_of_lists(source, fields='*', first_row=0, count=-1, schema=None):\n  if schema is None:\n    schema = google.datalab.bigquery.Schema.from_data(source)\n  fields = get_field_list(fields, schema)\n  gen = source[first_row:first_row + count] if count >= 0 else source\n  cols = [schema.find(name) for name in fields]\n  rows = [{'c': [{'v': row[i]} for i in cols]} for row in gen]\n  return {'cols': _get_cols(fields, schema), 'rows': rows}, len(source)", "response": "Helper function for _get_data that handles lists of lists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_data_from_dataframe(source, fields='*', first_row=0, count=-1, schema=None):\n  if schema is None:\n    schema = google.datalab.bigquery.Schema.from_data(source)\n  fields = get_field_list(fields, schema)\n  rows = []\n  if count < 0:\n    count = len(source.index)\n  df_slice = source.reset_index(drop=True)[first_row:first_row + count]\n  for index, data_frame_row in df_slice.iterrows():\n    row = data_frame_row.to_dict()\n    for key in list(row.keys()):\n      val = row[key]\n      if isinstance(val, pandas.Timestamp):\n        row[key] = val.to_pydatetime()\n\n    rows.append({'c': [{'v': row[c]} if c in row else {} for c in fields]})\n  cols = _get_cols(fields, schema)\n  return {'cols': cols, 'rows': rows}, len(source)", "response": "Helper function for _get_data that handles Pandas DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_magic_line(line, cell, parser, namespace=None):\n  try:\n    args, cell = parser.parse(line, cell, namespace)\n    if args:\n      return args['func'](args, cell)\n  except Exception as e:\n    # e.args[0] is 'exit_0' if --help is provided in line.\n    # In this case don't write anything to stderr.\n    if e.args and e.args[0] == 'exit_0':\n      return\n    sys.stderr.write('\\n' + str(e))\n    sys.stderr.flush()", "response": "Handles a line from a magic command line."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a config from a magic cell body for selected config keys.", "response": "def parse_config_for_selected_keys(content, keys):\n  \"\"\" Parse a config from a magic cell body for selected config keys.\n\n  For example, if 'content' is:\n    config_item1: value1\n    config_item2: value2\n    config_item3: value3\n  and 'keys' are: [config_item1, config_item3]\n\n  The results will be a tuple of\n  1. The parsed config items (dict): {config_item1: value1, config_item3: value3}\n  2. The remaining content (string): config_item2: value2\n\n  Args:\n    content: the input content. A string. It has to be a yaml or JSON string.\n    keys: a list of keys to retrieve from content. Note that it only checks top level keys\n        in the dict.\n\n  Returns:\n    A tuple. First is the parsed config including only selected keys. Second is\n      the remaining content.\n\n  Raises:\n    Exception if the content is not a valid yaml or JSON string.\n  \"\"\"\n\n  config_items = {key: None for key in keys}\n  if not content:\n    return config_items, content\n\n  stripped = content.strip()\n  if len(stripped) == 0:\n    return {}, None\n  elif stripped[0] == '{':\n    config = json.loads(content)\n  else:\n    config = yaml.load(content)\n\n  if not isinstance(config, dict):\n    raise ValueError('Invalid config.')\n\n  for key in keys:\n    config_items[key] = config.pop(key, None)\n\n  if not config:\n    return config_items, None\n\n  if stripped[0] == '{':\n    content_out = json.dumps(config, indent=4)\n  else:\n    content_out = yaml.dump(config, default_flow_style=False)\n\n  return config_items, content_out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chart_html(driver_name, chart_type, source, chart_options=None, fields='*', refresh_interval=0,\n               refresh_data=None, control_defaults=None, control_ids=None, schema=None):\n  \"\"\" Return HTML for a chart.\n\n  Args:\n    driver_name: the name of the chart driver. Currently we support 'plotly' or 'gcharts'.\n    chart_type: string specifying type of chart.\n    source: the data source for the chart. Can be actual data (e.g. list) or the name of\n        a data source (e.g. the name of a query module).\n    chart_options: a dictionary of options for the chart. Can contain a 'controls' entry\n        specifying controls. Other entries are passed as JSON to Google Charts.\n    fields: the fields to chart. Can be '*' for all fields (only sensible if the columns are\n        ordered; e.g. a Query or list of lists, but not a list of dictionaries); otherwise a\n        string containing a comma-separated list of field names.\n    refresh_interval: a time in seconds after which the chart data will be refreshed. 0 if the\n        chart should not be refreshed (i.e. the data is static).\n    refresh_data: if the source is a list or other raw data, this is a YAML string containing\n        metadata needed to support calls to refresh (get_chart_data).\n    control_defaults: the default variable values for controls that are shared across charts\n        including this one.\n    control_ids: the DIV IDs for controls that are shared across charts including this one.\n    schema: an optional schema for the data; if not supplied one will be inferred.\n\n  Returns:\n    A string containing the HTML for the chart.\n\n  \"\"\"\n  div_id = _html.Html.next_id()\n  controls_html = ''\n  if control_defaults is None:\n    control_defaults = {}\n  if control_ids is None:\n    control_ids = []\n  if chart_options is not None and 'variables' in chart_options:\n    controls = chart_options['variables']\n    del chart_options['variables']  # Just to make sure GCharts doesn't see them.\n    controls_html, defaults, ids = parse_control_options(controls)\n    # We augment what we are passed so that in principle we can have controls that are\n    # shared by charts as well as controls that are specific to a chart.\n    control_defaults.update(defaults)\n    control_ids.extend(ids),\n\n  _HTML_TEMPLATE = \"\"\"\n    <div class=\"bqgc-container\">\n      {controls}\n      <div class=\"bqgc {extra_class}\" id=\"{id}\">\n      </div>\n    </div>\n    <script src=\"/static/components/requirejs/require.js\"></script>\n    <script>\n      require.config({{\n        paths: {{\n          base: '/static/base',\n          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n        }},\n        map: {{\n          '*': {{\n            datalab: 'nbextensions/gcpdatalab'\n          }}\n        }},\n        shim: {{\n          plotly: {{\n            deps: ['d3', 'jquery'],\n            exports: 'plotly'\n          }}\n        }}\n      }});\n\n      require(['datalab/charting',\n               'datalab/element!{id}',\n               'base/js/events',\n               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n              ],\n        function(charts, dom, events) {{\n          charts.render(\n              '{driver}',\n              dom,\n              events,\n              '{chart_type}',\n              {control_ids},\n              {data},\n              {options},\n              {refresh_data},\n              {refresh_interval},\n              {total_rows});\n          }}\n        );\n    </script>\n  \"\"\"\n  count = 25 if chart_type == 'paged_table' else -1\n  data, total_count = get_data(source, fields, control_defaults, 0, count, schema)\n  if refresh_data is None:\n    if isinstance(source, basestring):\n      source_index = get_data_source_index(source)\n      refresh_data = {'source_index': source_index, 'name': source_index}\n    else:\n      refresh_data = {'name': 'raw data'}\n  refresh_data['fields'] = fields\n\n  # TODO(gram): check if we need to augment env with user_ns\n  return _HTML_TEMPLATE \\\n      .format(driver=driver_name,\n              controls=controls_html,\n              id=div_id,\n              chart_type=chart_type,\n              extra_class=\" bqgc-controlled\" if len(controls_html) else '',\n              data=json.dumps(data, cls=google.datalab.utils.JSONEncoder),\n              options=json.dumps(chart_options, cls=google.datalab.utils.JSONEncoder),\n              refresh_data=json.dumps(refresh_data, cls=google.datalab.utils.JSONEncoder),\n              refresh_interval=refresh_interval,\n              control_ids=str(control_ids),\n              total_rows=total_count)", "response": "Return the HTML for a chart."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprovide a simple default sampling strategy which limits the result set by a count.", "response": "def default(fields=None, count=5):\n    \"\"\"Provides a simple default sampling strategy which limits the result set by a count.\n\n    Args:\n      fields: an optional list of field names to retrieve.\n      count: optional number of rows to limit the sampled results to.\n    Returns:\n      A sampling function that can be applied to get a random sampling.\n    \"\"\"\n    projection = Sampling._create_projection(fields)\n    return lambda sql: 'SELECT %s FROM (%s) LIMIT %d' % (projection, sql, count)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sorted(field_name, ascending=True, fields=None, count=5):\n    if field_name is None:\n      raise Exception('Sort field must be specified')\n    direction = '' if ascending else ' DESC'\n    projection = Sampling._create_projection(fields)\n    return lambda sql: 'SELECT %s FROM (%s) ORDER BY %s%s LIMIT %d' % (projection, sql, field_name,\n                                                                       direction, count)", "response": "Returns a sampling strategy that picks from an ordered set of rows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a sampling function that can be applied to get a hash - based sampling of a percentage of data.", "response": "def hashed(field_name, percent, fields=None, count=0):\n    \"\"\"Provides a sampling strategy based on hashing and selecting a percentage of data.\n\n    Args:\n      field_name: the name of the field to hash.\n      percent: the percentage of the resulting hashes to select.\n      fields: an optional list of field names to retrieve.\n      count: optional maximum count of rows to pick.\n    Returns:\n      A sampling function that can be applied to get a hash-based sampling.\n    \"\"\"\n    if field_name is None:\n      raise Exception('Hash field must be specified')\n\n    def _hashed_sampling(sql):\n      projection = Sampling._create_projection(fields)\n      sql = 'SELECT %s FROM (%s) WHERE MOD(ABS(FARM_FINGERPRINT(CAST(%s AS STRING))), 100) < %d' % \\\n            (projection, sql, field_name, percent)\n      if count != 0:\n        sql = '%s LIMIT %d' % (sql, count)\n      return sql\n    return _hashed_sampling"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random(percent, fields=None, count=0):\n    def _random_sampling(sql):\n      projection = Sampling._create_projection(fields)\n      sql = 'SELECT %s FROM (%s) WHERE rand() < %f' % (projection, sql, (float(percent) / 100.0))\n      if count != 0:\n        sql = '%s LIMIT %d' % (sql, count)\n      return sql\n    return _random_sampling", "response": "Provides a sampling strategy that picks a few random rows from the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct a sampling function according to the provided sampling technique", "response": "def _auto(method, fields, count, percent, key_field, ascending):\n    \"\"\"Construct a sampling function according to the provided sampling technique, provided all\n    its needed fields are passed as arguments\n\n    Args:\n      method: one of the supported sampling methods: {limit,random,hashed,sorted}\n      fields: an optional list of field names to retrieve.\n      count: maximum number of rows to limit the sampled results to.\n      percent: the percentage of the resulting hashes to select if using hashed sampling\n      key_field: the name of the field to sort the rows by or use for hashing\n      ascending: whether to sort in ascending direction or not.\n    Returns:\n      A sampling function using the provided arguments\n    Raises:\n      Exception if an unsupported mathod name is passed\n    \"\"\"\n    if method == 'limit':\n      return Sampling.default(fields=fields, count=count)\n    elif method == 'random':\n      return Sampling.random(fields=fields, percent=percent, count=count)\n    elif method == 'hashed':\n      return Sampling.hashed(fields=fields, field_name=key_field, percent=percent, count=count)\n    elif method == 'sorted':\n      return Sampling.sorted(fields=fields, field_name=key_field, ascending=ascending, count=count)\n    else:\n      raise Exception('Unsupported sampling method: %s' % method)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _to_query_json(self):\n    return {\n      'quote': self._quote,\n      'fieldDelimiter': self._delimiter,\n      'encoding': self._encoding.upper(),\n      'skipLeadingRows': self._skip_leading_rows,\n      'allowQuotedNewlines': self._allow_quoted_newlines,\n      'allowJaggedRows': self._allow_jagged_rows\n    }", "response": "Return the options as a dictionary to be used as JSON in a query job."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nissue a request to load data from GCS to a BigQuery table.", "response": "def jobs_insert_load(self, source, table_name, append=False, overwrite=False, create=False,\n                       source_format='CSV', field_delimiter=',', allow_jagged_rows=False,\n                       allow_quoted_newlines=False, encoding='UTF-8', ignore_unknown_values=False,\n                       max_bad_records=0, quote='\"', skip_leading_rows=0):\n    \"\"\" Issues a request to load data from GCS to a BQ table\n\n    Args:\n      source: the URL of the source bucket(s). Can include wildcards, and can be a single\n          string argument or a list.\n      table_name: a tuple representing the full name of the destination table.\n      append: if True append onto existing table contents.\n      overwrite: if True overwrite existing table contents.\n      create: if True, create the table if it doesn't exist\n      source_format: the format of the data; default 'CSV'. Other options are DATASTORE_BACKUP\n          or NEWLINE_DELIMITED_JSON.\n      field_delimiter: The separator for fields in a CSV file. BigQuery converts the string to\n          ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the data\n          as raw binary (default ',').\n      allow_jagged_rows: If True, accept rows in CSV files that are missing trailing optional\n          columns; the missing values are treated as nulls (default False).\n      allow_quoted_newlines: If True, allow quoted data sections in CSV files that contain newline\n          characters (default False).\n      encoding: The character encoding of the data, either 'UTF-8' (the default) or 'ISO-8859-1'.\n      ignore_unknown_values: If True, accept rows that contain values that do not match the schema;\n          the unknown values are ignored (default False).\n      max_bad_records: The maximum number of bad records that are allowed (and ignored) before\n          returning an 'invalid' error in the Job result (default 0).\n      quote: The value used to quote data sections in a CSV file; default '\"'. If your data does\n          not contain quoted sections, set the property value to an empty string. If your data\n          contains quoted newline characters, you must also enable allow_quoted_newlines.\n      skip_leading_rows: A number of rows at the top of a CSV file to skip (default 0).\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._JOBS_PATH % (table_name.project_id, ''))\n    if isinstance(source, basestring):\n      source = [source]\n    write_disposition = 'WRITE_EMPTY'\n    if overwrite:\n      write_disposition = 'WRITE_TRUNCATE'\n    if append:\n      write_disposition = 'WRITE_APPEND'\n    data = {\n        'kind': 'bigquery#job',\n        'configuration': {\n            'load': {\n                'sourceUris': source,\n                'destinationTable': {\n                    'projectId': table_name.project_id,\n                    'datasetId': table_name.dataset_id,\n                    'tableId': table_name.table_id\n                },\n                'createDisposition': 'CREATE_IF_NEEDED' if create else 'CREATE_NEVER',\n                'writeDisposition': write_disposition,\n                'sourceFormat': source_format,\n                'ignoreUnknownValues': ignore_unknown_values,\n                'maxBadRecords': max_bad_records,\n            }\n        }\n    }\n    if source_format == 'CSV':\n      load_config = data['configuration']['load']\n      load_config.update({\n          'fieldDelimiter': field_delimiter,\n          'allowJaggedRows': allow_jagged_rows,\n          'allowQuotedNewlines': allow_quoted_newlines,\n          'quote': quote,\n          'encoding': encoding,\n          'skipLeadingRows': skip_leading_rows\n      })\n\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef jobs_insert_query(self, sql, code=None, imports=None, table_name=None, append=False,\n                        overwrite=False, dry_run=False, use_cache=True, batch=True,\n                        allow_large_results=False, table_definitions=None, dialect=None,\n                        billing_tier=None):\n    \"\"\"Issues a request to insert a query job.\n\n    Args:\n      sql: the SQL string representing the query to execute.\n      code: code for Javascript UDFs, if any.\n      imports: a list of GCS URLs containing additional Javascript UDF support code, if any.\n      table_name: None for an anonymous table, or a name parts tuple for a long-lived table.\n      append: if True, append to the table if it is non-empty; else the request will fail if table\n          is non-empty unless overwrite is True.\n      overwrite: if the table already exists, truncate it instead of appending or raising an\n          Exception.\n      dry_run: whether to actually execute the query or just dry run it.\n      use_cache: whether to use past query results or ignore cache. Has no effect if destination is\n          specified.\n      batch: whether to run this as a batch job (lower priority) or as an interactive job (high\n          priority, more expensive).\n      allow_large_results: whether to allow large results (slower with some restrictions but\n          can handle big jobs).\n      table_definitions: a list of JSON external table definitions for any external tables\n          referenced in the query.\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._JOBS_PATH % (self._project_id, ''))\n\n    if dialect is None:\n        dialect = datalab.bigquery.Dialect.default().bq_dialect\n\n    data = {\n        'kind': 'bigquery#job',\n        'configuration': {\n            'query': {\n                'query': sql,\n                'useQueryCache': use_cache,\n                'allowLargeResults': allow_large_results,\n                'useLegacySql': dialect == 'legacy'\n            },\n            'dryRun': dry_run,\n            'priority': 'BATCH' if batch else 'INTERACTIVE',\n        },\n    }\n\n    query_config = data['configuration']['query']\n\n    resources = []\n    if code:\n      resources.extend([{'inlineCode': fragment} for fragment in code])\n\n    if imports:\n      resources.extend([{'resourceUri': uri} for uri in imports])\n\n    query_config['userDefinedFunctionResources'] = resources\n\n    if table_definitions:\n      query_config['tableDefinitions'] = table_definitions\n\n    if table_name:\n      query_config['destinationTable'] = {\n          'projectId': table_name.project_id,\n          'datasetId': table_name.dataset_id,\n          'tableId': table_name.table_id\n      }\n      if append:\n        query_config['writeDisposition'] = \"WRITE_APPEND\"\n      elif overwrite:\n        query_config['writeDisposition'] = \"WRITE_TRUNCATE\"\n\n    if billing_tier:\n        query_config['maximumBillingTier'] = billing_tier\n\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)", "response": "Issues a request to insert a query into a job list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nissue a request to retrieve information about a job.", "response": "def jobs_get(self, job_id, project_id=None):\n    \"\"\"Issues a request to retrieve information about a job.\n\n    Args:\n      job_id: the id of the job\n      project_id: the project id to use to fetch the results; use None for the default project.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    if project_id is None:\n      project_id = self._project_id\n    url = Api._ENDPOINT + (Api._JOBS_PATH % (project_id, job_id))\n    return datalab.utils.Http.request(url, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nissue a request to create a dataset.", "response": "def datasets_insert(self, dataset_name, friendly_name=None, description=None):\n    \"\"\"Issues a request to create a dataset.\n\n    Args:\n      dataset_name: the name of the dataset to create.\n      friendly_name: (optional) the friendly name for the dataset\n      description: (optional) a description for the dataset\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._DATASETS_PATH % (dataset_name.project_id, ''))\n    data = {\n        'kind': 'bigquery#dataset',\n        'datasetReference': {\n            'projectId': dataset_name.project_id,\n            'datasetId': dataset_name.dataset_id\n        },\n    }\n    if friendly_name:\n      data['friendlyName'] = friendly_name\n    if description:\n      data['description'] = description\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef datasets_delete(self, dataset_name, delete_contents):\n    url = Api._ENDPOINT + (Api._DATASETS_PATH % dataset_name)\n    args = {}\n    if delete_contents:\n      args['deleteContents'] = True\n    return datalab.utils.Http.request(url, method='DELETE', args=args,\n                                      credentials=self._credentials, raw_response=True)", "response": "Issues a request to delete a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef datasets_update(self, dataset_name, dataset_info):\n    url = Api._ENDPOINT + (Api._DATASETS_PATH % dataset_name)\n    return datalab.utils.Http.request(url, method='PUT', data=dataset_info,\n                                      credentials=self._credentials)", "response": "Updates the Dataset info."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datasets_get(self, dataset_name):\n    url = Api._ENDPOINT + (Api._DATASETS_PATH % dataset_name)\n    return datalab.utils.Http.request(url, credentials=self._credentials)", "response": "Issues a request to retrieve information about a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nissue a request to list the datasets in the project.", "response": "def datasets_list(self, project_id=None, max_results=0, page_token=None):\n    \"\"\"Issues a request to list the datasets in the project.\n\n    Args:\n      project_id: the project id to use to fetch the results; use None for the default project.\n      max_results: an optional maximum number of tables to retrieve.\n      page_token: an optional token to continue the retrieval.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    if project_id is None:\n      project_id = self._project_id\n    url = Api._ENDPOINT + (Api._DATASETS_PATH % (project_id, ''))\n\n    args = {}\n    if max_results != 0:\n      args['maxResults'] = max_results\n    if page_token is not None:\n      args['pageToken'] = page_token\n\n    return datalab.utils.Http.request(url, args=args, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nissuing a request to retrieve information about a table.", "response": "def tables_get(self, table_name):\n    \"\"\"Issues a request to retrieve information about a table.\n\n    Args:\n      table_name: a tuple representing the full name of the table.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._TABLES_PATH % table_name)\n    return datalab.utils.Http.request(url, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nissues a request to create a table or view in the specified dataset with the specified id.", "response": "def tables_insert(self, table_name, schema=None, query=None, friendly_name=None,\n                    description=None):\n    \"\"\"Issues a request to create a table or view in the specified dataset with the specified id.\n       A schema must be provided to create a Table, or a query must be provided to create a View.\n\n    Args:\n      table_name: the name of the table as a tuple of components.\n      schema: the schema, if this is a Table creation.\n      query: the query, if this is a View creation.\n      friendly_name: an optional friendly name.\n      description: an optional description.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + \\\n        (Api._TABLES_PATH % (table_name.project_id, table_name.dataset_id, '', ''))\n\n    data = {\n        'kind': 'bigquery#table',\n        'tableReference': {\n            'projectId': table_name.project_id,\n            'datasetId': table_name.dataset_id,\n            'tableId': table_name.table_id\n        }\n    }\n    if schema:\n      data['schema'] = {'fields': schema}\n    if query:\n      data['view'] = {'query': query}\n    if friendly_name:\n      data['friendlyName'] = friendly_name\n    if description:\n      data['description'] = description\n\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tabledata_insert_all(self, table_name, rows):\n    url = Api._ENDPOINT + (Api._TABLES_PATH % table_name) + \"/insertAll\"\n\n    data = {\n        'kind': 'bigquery#tableDataInsertAllRequest',\n        'rows': rows\n    }\n\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)", "response": "Issues a request to insert data into a table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the contents of a table. Args: table_name: the name of the table as a tuple of components. start_index: the index of the row at which to start retrieval. max_results: an optional maximum number of rows to retrieve. page_token: an optional token to continue the retrieval. Returns: A parsed result object. Raises: Exception if there is an error performing the operation.", "response": "def tabledata_list(self, table_name, start_index=None, max_results=None, page_token=None):\n    \"\"\" Retrieves the contents of a table.\n\n    Args:\n      table_name: the name of the table as a tuple of components.\n      start_index: the index of the row at which to start retrieval.\n      max_results: an optional maximum number of rows to retrieve.\n      page_token: an optional token to continue the retrieval.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._TABLEDATA_PATH % table_name)\n    args = {}\n    if start_index:\n      args['startIndex'] = start_index\n    if max_results:\n      args['maxResults'] = max_results\n    if page_token is not None:\n      args['pageToken'] = page_token\n    return datalab.utils.Http.request(url, args=args, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef table_delete(self, table_name):\n    url = Api._ENDPOINT + (Api._TABLES_PATH % table_name)\n    return datalab.utils.Http.request(url, method='DELETE', credentials=self._credentials,\n                                      raw_response=True)", "response": "Issues a request to delete a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports the table to GCS.", "response": "def table_extract(self, table_name, destination, format='CSV', compress=True,\n                    field_delimiter=',', print_header=True):\n    \"\"\"Exports the table to GCS.\n\n    Args:\n      table_name: the name of the table as a tuple of components.\n      destination: the destination URI(s). Can be a single URI or a list.\n      format: the format to use for the exported data; one of CSV, NEWLINE_DELIMITED_JSON or AVRO.\n          Defaults to CSV.\n      compress: whether to compress the data on export. Compression is not supported for\n          AVRO format. Defaults to False.\n      field_delimiter: for CSV exports, the field delimiter to use. Defaults to ','\n      print_header: for CSV exports, whether to include an initial header line. Default true.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._JOBS_PATH % (table_name.project_id, ''))\n    if isinstance(destination, basestring):\n      destination = [destination]\n    data = {\n        # 'projectId': table_name.project_id, # Code sample shows this but it is not in job\n        # reference spec. Filed as b/19235843\n        'kind': 'bigquery#job',\n        'configuration': {\n            'extract': {\n                'sourceTable': {\n                    'projectId': table_name.project_id,\n                    'datasetId': table_name.dataset_id,\n                    'tableId': table_name.table_id,\n                },\n                'compression': 'GZIP' if compress else 'NONE',\n                'fieldDelimiter': field_delimiter,\n                'printHeader': print_header,\n                'destinationUris': destination,\n                'destinationFormat': format,\n            }\n        }\n    }\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef table_update(self, table_name, table_info):\n    url = Api._ENDPOINT + (Api._TABLES_PATH % table_name)\n    return datalab.utils.Http.request(url, method='PUT', data=table_info,\n                                      credentials=self._credentials)", "response": "Updates the info of a table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrefreshing the state of the job.", "response": "def _refresh_state(self):\n    \"\"\" Refresh the job info. \"\"\"\n\n    # DataFlow's DataflowPipelineResult does not refresh state, so we have to do it ourselves\n    # as a workaround.\n    self._runner_results._job = (\n        self._runner_results._runner.dataflow_client.get_job(self._runner_results.job_id()))\n    self._is_complete = self._runner_results.state in ['STOPPED', 'DONE', 'FAILED', 'CANCELLED']\n    self._fator_error = getattr(self._runner_results._runner, 'last_error_msg', None)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_archive(archive_path, dest):\n  # Make the dest folder if it does not exist\n  if not os.path.isdir(dest):\n    os.makedirs(dest)\n\n  try:\n    tmpfolder = None\n\n    if (not tf.gfile.Exists(archive_path)) or tf.gfile.IsDirectory(archive_path):\n      raise ValueError('archive path %s is not a file' % archive_path)\n\n    if archive_path.startswith('gs://'):\n      # Copy the file to a local temp folder\n      tmpfolder = tempfile.mkdtemp()\n      cmd_args = ['gsutil', 'cp', archive_path, tmpfolder]\n      _shell_process.run_and_monitor(cmd_args, os.getpid())\n      archive_path = os.path.join(tmpfolder, os.path.name(archive_path))\n\n    if archive_path.lower().endswith('.tar.gz'):\n      flags = '-xzf'\n    elif archive_path.lower().endswith('.tar'):\n      flags = '-xf'\n    else:\n      raise ValueError('Only tar.gz or tar.Z files are supported.')\n\n    cmd_args = ['tar', flags, archive_path, '-C', dest]\n    _shell_process.run_and_monitor(cmd_args, os.getpid())\n  finally:\n    if tmpfolder:\n      shutil.rmtree(tmpfolder)", "response": "Extract a local or GCS archive file to a folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef preprocess(train_dataset, output_dir, eval_dataset, checkpoint, pipeline_option):\n\n    import apache_beam as beam\n    import google.datalab.utils\n    from . import _preprocess\n\n    if checkpoint is None:\n      checkpoint = _util._DEFAULT_CHECKPOINT_GSURL\n\n    job_name = ('preprocess-image-classification-' +\n                datetime.datetime.now().strftime('%y%m%d-%H%M%S'))\n\n    staging_package_url = _util.repackage_to_staging(output_dir)\n    tmpdir = tempfile.mkdtemp()\n    # suppress DataFlow warnings about wheel package as extra package.\n    original_level = logging.getLogger().getEffectiveLevel()\n    logging.getLogger().setLevel(logging.ERROR)\n    try:\n      # Workaround for DataFlow 2.0, which doesn't work well with extra packages in GCS.\n      # Remove when the issue is fixed and new version of DataFlow is included in Datalab.\n      extra_packages = [staging_package_url, _TF_GS_URL, _PROTOBUF_GS_URL]\n      local_packages = [os.path.join(tmpdir, os.path.basename(p))\n                        for p in extra_packages]\n      for source, dest in zip(extra_packages, local_packages):\n        file_io.copy(source, dest, overwrite=True)\n\n      options = {\n          'staging_location': os.path.join(output_dir, 'tmp', 'staging'),\n          'temp_location': os.path.join(output_dir, 'tmp'),\n          'job_name': job_name,\n          'project': _util.default_project(),\n          'extra_packages': local_packages,\n          'teardown_policy': 'TEARDOWN_ALWAYS',\n          'no_save_main_session': True\n      }\n      if pipeline_option is not None:\n        options.update(pipeline_option)\n\n      opts = beam.pipeline.PipelineOptions(flags=[], **options)\n      p = beam.Pipeline('DataflowRunner', options=opts)\n      _preprocess.configure_pipeline(p, train_dataset, eval_dataset,\n                                     checkpoint, output_dir, job_name)\n      job_results = p.run()\n    finally:\n      shutil.rmtree(tmpdir)\n      logging.getLogger().setLevel(original_level)\n\n    if (_util.is_in_IPython()):\n      import IPython\n      dataflow_url = 'https://console.developers.google.com/dataflow?project=%s' % \\\n                     _util.default_project()\n      html = 'Job \"%s\" submitted.' % job_name\n      html += '<p>Click <a href=\"%s\" target=\"_blank\">here</a> to track preprocessing job. <br/>' \\\n          % dataflow_url\n      IPython.display.display_html(html, raw=True)\n    return google.datalab.utils.DataflowJob(job_results)", "response": "Preprocess data in Cloud with DataFlow."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train(input_dir, batch_size, max_steps, output_dir, checkpoint, cloud_train_config):\n\n    import google.datalab.ml as ml\n    if checkpoint is None:\n      checkpoint = _util._DEFAULT_CHECKPOINT_GSURL\n    staging_package_url = _util.repackage_to_staging(output_dir)\n    job_args = {\n      'input_dir': input_dir,\n      'max_steps': max_steps,\n      'batch_size': batch_size,\n      'checkpoint': checkpoint\n    }\n    job_request = {\n      'package_uris': [staging_package_url, _TF_GS_URL, _PROTOBUF_GS_URL],\n      'python_module': 'mltoolbox.image.classification.task',\n      'job_dir': output_dir,\n      'args': job_args\n    }\n    job_request.update(dict(cloud_train_config._asdict()))\n    job_id = 'image_classification_train_' + datetime.datetime.now().strftime('%y%m%d_%H%M%S')\n    job = ml.Job.submit_training(job_request, job_id)\n    if (_util.is_in_IPython()):\n      import IPython\n      log_url_query_strings = {\n        'project': _util.default_project(),\n        'resource': 'ml.googleapis.com/job_id/' + job.info['jobId']\n      }\n      log_url = 'https://console.developers.google.com/logs/viewer?' + \\\n          urllib.urlencode(log_url_query_strings)\n      html = 'Job \"%s\" submitted.' % job.info['jobId']\n      html += '<p>Click <a href=\"%s\" target=\"_blank\">here</a> to view cloud log. <br/>' % log_url\n      IPython.display.display_html(html, raw=True)\n    return job", "response": "Train model in the cloud with CloudML trainer service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict(model_id, image_files, resize, show_image):\n\n    import google.datalab.ml as ml\n\n    images = _util.load_images(image_files, resize=resize)\n\n    parts = model_id.split('.')\n    if len(parts) != 2:\n      raise ValueError('Invalid model name for cloud prediction. Use \"model.version\".')\n    if len(images) == 0:\n      raise ValueError('images is empty.')\n\n    data = []\n    for ii, image in enumerate(images):\n      image_encoded = base64.b64encode(image)\n      data.append({\n        'key': str(ii),\n        'image_bytes': {'b64': image_encoded}\n      })\n\n    predictions = ml.ModelVersions(parts[0]).predict(parts[1], data)\n    if len(predictions) == 0:\n      raise Exception('Prediction results are empty.')\n    # Although prediction results contains a labels list in each instance, they are all the same\n    # so taking the first one.\n    labels = predictions[0]['labels']\n    labels_and_scores = [(x['prediction'], x['scores'][labels.index(x['prediction'])])\n                         for x in predictions]\n    results = zip(image_files, images, labels_and_scores)\n    ret = _util.process_prediction_results(results, show_image)\n    return ret", "response": "Predict using a deployed model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Query object that will return the specified fields from the given Table.", "response": "def from_table(table, fields=None):\n    \"\"\" Return a Query for the given Table object\n\n    Args:\n      table: the Table object to construct a Query out of\n      fields: the fields to return. If None, all fields will be returned. This can be a string\n          which will be injected into the Query after SELECT, or a list of field names.\n\n    Returns:\n      A Query object that will return the specified fields from the records in the Table.\n    \"\"\"\n    if fields is None:\n      fields = '*'\n    elif isinstance(fields, list):\n      fields = ','.join(fields)\n    return Query('SELECT %s FROM %s' % (fields, table._repr_sql_()))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _expanded_sql(self, sampling=None):\n\n    # use lists to preserve the order of subqueries, bigquery will not like listing subqueries\n    # out of order if they depend on each other. for example. the following will be rejected:\n    # WITH q2 as (SELECT * FROM q1),\n    #      q1 as (SELECT * FROM mytable),\n    # SELECT * FROM q2\n    # so when we're getting the dependencies, use recursion into a list to maintain the order\n    udfs = []\n    subqueries = []\n    expanded_sql = ''\n\n    def _recurse_subqueries(query):\n      \"\"\"Recursively scan subqueries and add their pieces to global scope udfs and subqueries\n      \"\"\"\n      if query._subqueries:\n        for subquery in query._subqueries:\n          _recurse_subqueries(subquery[1])\n        subqueries.extend([s for s in query._subqueries if s not in subqueries])\n      if query._udfs:\n        # query._udfs is a list of (name, UDF) tuples; we just want the UDF.\n        udfs.extend([u[1] for u in query._udfs if u[1] not in udfs])\n\n    _recurse_subqueries(self)\n\n    if udfs:\n      expanded_sql += '\\n'.join([udf._expanded_sql() for udf in udfs])\n      expanded_sql += '\\n'\n\n    def _indent_query(subquery):\n      return '  ' + subquery._sql.replace('\\n', '\\n  ')\n\n    if subqueries:\n      expanded_sql += 'WITH ' + \\\n                      '\\n),\\n'.join(['%s AS (\\n%s' % (sq[0], _indent_query(sq[1]))\n                                     for sq in subqueries])\n      expanded_sql += '\\n)\\n\\n'\n\n    expanded_sql += sampling(self._sql) if sampling else self._sql\n\n    return expanded_sql", "response": "Returns the expanded SQL of this object including all subqueries UDFs and external datasources."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrying run a query and return some useful statistics.", "response": "def dry_run(self, context=None, query_params=None):\n    \"\"\"Dry run a query, to check the validity of the query and return some useful statistics.\n\n    Args:\n      context: an optional Context object providing project_id and credentials. If a specific\n          project id or credentials are unspecified, the default ones configured at the global\n          level are used.\n      query_params: a dictionary containing query parameter types and values, passed to BigQuery.\n\n    Returns:\n      A dict with 'cacheHit' and 'totalBytesProcessed' fields.\n    Raises:\n      An exception if the query was malformed.\n    \"\"\"\n\n    context = context or google.datalab.Context.default()\n    api = _api.Api(context)\n    try:\n      query_result = api.jobs_insert_query(self.sql, dry_run=True,\n                                           table_definitions=self.data_sources,\n                                           query_params=query_params)\n    except Exception as e:\n      raise e\n    return query_result['statistics']['query']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a query asynchronously.", "response": "def execute_async(self, output_options=None, sampling=None, context=None, query_params=None):\n    \"\"\" Initiate the query and return a QueryJob.\n\n    Args:\n      output_options: a QueryOutput object describing how to execute the query\n      sampling: sampling function to use. No sampling is done if None. See bigquery.Sampling\n      context: an optional Context object providing project_id and credentials. If a specific\n          project id or credentials are unspecified, the default ones configured at the global\n          level are used.\n      query_params: a dictionary containing query parameter types and values, passed to BigQuery.\n    Returns:\n      A Job object that can wait on creating a table or exporting to a file\n      If the output is a table, the Job object additionally has run statistics\n      and query results\n    Raises:\n      Exception if query could not be executed.\n    \"\"\"\n\n    # Default behavior is to execute to a table\n    if output_options is None:\n      output_options = QueryOutput.table()\n\n    # First, execute the query into a table, using a temporary one if no name is specified\n    batch = output_options.priority == 'low'\n    append = output_options.table_mode == 'append'\n    overwrite = output_options.table_mode == 'overwrite'\n    table_name = output_options.table_name\n    context = context or google.datalab.Context.default()\n    api = _api.Api(context)\n    if table_name is not None:\n      table_name = _utils.parse_table_name(table_name, api.project_id)\n\n    sql = self._expanded_sql(sampling)\n\n    try:\n      query_result = api.jobs_insert_query(sql, table_name=table_name,\n                                           append=append, overwrite=overwrite, batch=batch,\n                                           use_cache=output_options.use_cache,\n                                           allow_large_results=output_options.allow_large_results,\n                                           table_definitions=self.data_sources,\n                                           query_params=query_params)\n    except Exception as e:\n      raise e\n    if 'jobReference' not in query_result:\n      raise Exception('Unexpected response from server')\n\n    job_id = query_result['jobReference']['jobId']\n    if not table_name:\n      try:\n        destination = query_result['configuration']['query']['destinationTable']\n        table_name = (destination['projectId'], destination['datasetId'], destination['tableId'])\n      except KeyError:\n        # The query was in error\n        raise Exception(_utils.format_query_errors(query_result['status']['errors']))\n\n    execute_job = _query_job.QueryJob(job_id, table_name, sql, context=context)\n\n    # If all we need is to execute the query to a table, we're done\n    if output_options.type == 'table':\n      return execute_job\n    # Otherwise, build an async Job that waits on the query execution then carries out\n    # the specific export operation\n    else:\n      export_args = export_kwargs = None\n      if output_options.type == 'file':\n        if output_options.file_path.startswith('gs://'):\n          export_func = execute_job.result().extract\n          export_args = [output_options.file_path]\n          export_kwargs = {\n            'format': output_options.file_format,\n            'csv_delimiter': output_options.csv_delimiter,\n            'csv_header': output_options.csv_header,\n            'compress': output_options.compress_file\n          }\n        else:\n          export_func = execute_job.result().to_file\n          export_args = [output_options.file_path]\n          export_kwargs = {\n            'format': output_options.file_format,\n            'csv_delimiter': output_options.csv_delimiter,\n            'csv_header': output_options.csv_header\n          }\n      elif output_options.type == 'dataframe':\n        export_func = execute_job.result().to_dataframe\n        export_args = []\n        export_kwargs = {\n          'start_row': output_options.dataframe_start_row,\n          'max_rows': output_options.dataframe_max_rows\n        }\n\n      # Perform the export operation with the specified parameters\n      export_func = google.datalab.utils.async_function(export_func)\n      return export_func(*export_args, **export_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitiate the query and return a QueryJob.", "response": "def execute(self, output_options=None, sampling=None, context=None, query_params=None):\n    \"\"\" Initiate the query and return a QueryJob.\n\n    Args:\n      output_options: a QueryOutput object describing how to execute the query\n      sampling: sampling function to use. No sampling is done if None. See bigquery.Sampling\n      context: an optional Context object providing project_id and credentials. If a specific\n          project id or credentials are unspecified, the default ones configured at the global\n          level are used.\n    Returns:\n      A Job object that can be used to get the query results, or export to a file or dataframe\n    Raises:\n      Exception if query could not be executed.\n    \"\"\"\n    return self.execute_async(output_options, sampling=sampling, context=context,\n                              query_params=query_params).wait()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_query_parameters(config_parameters, date_time=datetime.datetime.now()):\n    merged_parameters = Query.merge_parameters(config_parameters, date_time=date_time,\n                                               macros=False, types_and_values=True)\n    # We're exposing a simpler schema format than the one actually required by BigQuery to make\n    # magics easier. We need to convert between the two formats\n    parsed_params = []\n    for key, value in merged_parameters.items():\n      parsed_params.append({\n        'name': key,\n        'parameterType': {\n          'type': value['type']\n        },\n        'parameterValue': {\n          'value': value['value']\n        }\n      })\n    return parsed_params", "response": "Returns a list of query parameters that can be used to create a BigQuery service."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves a format modifier with the corresponding value.", "response": "def resolve_parameters(value, parameters, date_time=datetime.datetime.now(), macros=False):\n    \"\"\" Resolve a format modifier with the corresponding value.\n\n    Args:\n      value: The string (path, table, or any other artifact in a cell_body) which may have format\n          modifiers. E.g. a table name could be <project-id>.<dataset-id>.logs_%(_ds)s\n      parameters: The user-specified list of parameters in the cell-body.\n      date_time: The timestamp at which the parameters need to be evaluated. E.g. when the table\n          is <project-id>.<dataset-id>.logs_%(_ds)s, the '_ds' evaluates to the current date-time.\n      macros: When true, the format modifers in the value are replaced with the corresponding\n          airflow macro equivalents (like '{{ ds }}'. When false, the actual values are used (like\n          '2015-12-12'.\n\n    Returns:\n      The resolved value, i.e. the value with the format modifiers replaced with the corresponding\n          parameter-values. E.g. if value is <project-id>.<dataset-id>.logs_%(_ds)s, the returned\n          value is something like <project-id>.<dataset-id>.logs_2017-12-21\n    \"\"\"\n    merged_parameters = Query.merge_parameters(parameters, date_time=date_time, macros=macros,\n                                               types_and_values=False)\n    return Query._resolve_parameters(value, merged_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _resolve_parameters(operator_param_value, merged_parameters):\n    if isinstance(operator_param_value, list):\n      return [Query._resolve_parameters(item, merged_parameters)\n              for item in operator_param_value]\n    if isinstance(operator_param_value, dict):\n      return {Query._resolve_parameters(k, merged_parameters): Query._resolve_parameters(\n        v, merged_parameters) for k, v in operator_param_value.items()}\n    if isinstance(operator_param_value, six.string_types) and merged_parameters:\n      return operator_param_value % merged_parameters\n    return operator_param_value", "response": "Resolves a format modifier with the corresponding value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _airflow_macro_formats(date_time, macros, types_and_values):\n    day = date_time.date()\n    airflow_macros = {\n      # the datetime formatted as YYYY-MM-DD\n      '_ds': {'type': 'STRING', 'value': day.isoformat(), 'macro': '{{ ds }}'},\n      # the full ISO-formatted timestamp YYYY-MM-DDTHH:MM:SS.mmmmmm\n      '_ts': {'type': 'STRING', 'value': date_time.isoformat(), 'macro': '{{ ts }}'},\n      # the datetime formatted as YYYYMMDD (i.e. YYYY-MM-DD with 'no dashes')\n      '_ds_nodash': {'type': 'STRING', 'value': day.strftime('%Y%m%d'),\n                     'macro': '{{ ds_nodash }}'},\n      # the timestamp formatted as YYYYMMDDTHHMMSSmmmmmm (i.e full ISO-formatted timestamp\n      # YYYY-MM-DDTHH:MM:SS.mmmmmm with no dashes or colons).\n      '_ts_nodash': {'type': 'STRING', 'value': date_time.strftime('%Y%m%d%H%M%S%f'),\n                     'macro': '{{ ts_nodash }}'},\n      '_ts_year': {'type': 'STRING', 'value': day.strftime('%Y'),\n                   'macro': \"\"\"{{ '{:04d}'.format(execution_date.year) }}\"\"\"},\n      '_ts_month': {'type': 'STRING', 'value': day.strftime('%m'),\n                    'macro': \"\"\"{{ '{:02d}'.format(execution_date.month) }}\"\"\"},\n      '_ts_day': {'type': 'STRING', 'value': day.strftime('%d'),\n                  'macro': \"\"\"{{ '{:02d}'.format(execution_date.day) }}\"\"\"},\n      '_ts_hour': {'type': 'STRING', 'value': date_time.strftime('%H'),\n                   'macro': \"\"\"{{ '{:02d}'.format(execution_date.hour) }}\"\"\"},\n      '_ts_minute': {'type': 'STRING', 'value': date_time.strftime('%M'),\n                     'macro': \"\"\"{{ '{:02d}'.format(execution_date.minute) }}\"\"\"},\n      '_ts_second': {'type': 'STRING', 'value': date_time.strftime('%S'),\n                     'macro': \"\"\"{{ '{:02d}'.format(execution_date.second) }}\"\"\"},\n    }\n\n    if macros:\n      return {key: value['macro'] for key, value in airflow_macros.items()}\n\n    if types_and_values:\n      return {\n        key: {\n          'type': item['type'],\n          'value': item['value']\n        } for key, item in airflow_macros.items()\n      }\n\n    # By default only return values\n    return {key: value['value'] for key, value in airflow_macros.items()}", "response": "Return a mapping from airflow macro names to values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_parameters(parameters, date_time, macros, types_and_values):\n    merged_parameters = Query._airflow_macro_formats(date_time=date_time, macros=macros,\n                                                     types_and_values=types_and_values)\n    if parameters:\n      if types_and_values:\n        parameters = {\n          item['name']: {'value': item['value'], 'type': item['type']}\n          for item in parameters\n        }\n      else:  # macros = True, or the default (i.e. just values)\n        parameters = {item['name']: item['value'] for item in parameters}\n\n      merged_parameters.update(parameters)\n    return merged_parameters", "response": "Merge the given parameters into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a process and monitor it.", "response": "def run_and_monitor(args, pid_to_wait, std_out_filter_fn=None, cwd=None):\n  \"\"\" Start a process, and have it depend on another specified process.\n\n  Args:\n    args: the args of the process to start and monitor.\n    pid_to_wait: the process to wait on. If the process ends, also kill the started process.\n    std_out_filter_fn: a filter function which takes a string content from the stdout of the\n        started process, and returns True if the string should be redirected to console stdout.\n    cwd: the current working directory for the process to start.\n  \"\"\"\n\n  monitor_process = None\n  try:\n    p = subprocess.Popen(args,\n                         cwd=cwd,\n                         env=os.environ,\n                         stdout=subprocess.PIPE,\n                         stderr=subprocess.STDOUT)\n\n    pids_to_kill = [p.pid]\n    script = ('import %s;%s._wait_and_kill(%s, %s)' %\n              (__name__, __name__, str(pid_to_wait), str(pids_to_kill)))\n    monitor_process = subprocess.Popen(['python', '-c', script], env=os.environ)\n    while p.poll() is None:\n      line = p.stdout.readline()\n\n      if not six.PY2:\n        line = line.decode()\n\n      if std_out_filter_fn is None or std_out_filter_fn(line):\n        sys.stdout.write(line)\n        # Cannot do sys.stdout.flush(). It appears that too many flush() calls will hang browser.\n  finally:\n    if monitor_process:\n      monitor_process.kill()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expires_on(self):\n    timestamp = self._info.get('expirationTime', None)\n    if timestamp is None:\n      return None\n    return _parser.Parser.parse_timestamp(timestamp)", "response": "The timestamp for when the table will expire."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_info(self):\n    if self._info is None:\n      try:\n        self._info = self._api.tables_get(self._name_parts)\n      except Exception as e:\n        raise e", "response": "Loads metadata about this table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the table exists.", "response": "def exists(self):\n    \"\"\"Checks if the table exists.\n\n    Returns:\n      True if the table exists; False otherwise.\n    Raises:\n      Exception if there was an error requesting information about the table.\n    \"\"\"\n    try:\n      info = self._api.tables_get(self._name_parts)\n    except google.datalab.utils.RequestException as e:\n      if e.status == 404:\n        return False\n      raise e\n    except Exception as e:\n      raise e\n    self._info = info\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self):\n    try:\n      self._api.table_delete(self._name_parts)\n    except google.datalab.utils.RequestException:\n      # TODO(gram): May want to check the error reasons here and if it is not\n      # because the file didn't exist, return an error.\n      pass\n    except Exception as e:\n      raise e\n    return not self.exists()", "response": "Delete the table.\n\n    Returns:\n      True if the Table no longer exists; False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new table with the specified schema.", "response": "def create(self, schema, overwrite=False):\n    \"\"\" Create the table with the specified schema.\n\n    Args:\n      schema: the schema to use to create the table. Should be a list of dictionaries, each\n          containing at least a pair of entries, 'name' and 'type'.\n          See https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n      overwrite: if True, delete the table first if it exists. If False and the table exists,\n          creation will fail and raise an Exception.\n    Returns:\n      The Table instance.\n    Raises:\n      Exception if the table couldn't be created or already exists and truncate was False.\n    \"\"\"\n    if overwrite and self.exists():\n      self.delete()\n    if not isinstance(schema, _schema.Schema):\n      # Convert to a Schema object\n      schema = _schema.Schema(schema)\n    try:\n      response = self._api.tables_insert(self._name_parts, schema=schema._bq_schema)\n    except Exception as e:\n      raise e\n    if 'selfLink' in response:\n      self._schema = schema\n      return self\n    raise Exception(\"Table %s could not be created as it already exists\" % self._full_name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a job to export the table to GCS.", "response": "def extract_async(self, destination, format='csv', csv_delimiter=None, csv_header=True,\n                    compress=False):\n    \"\"\"Starts a job to export the table to GCS.\n\n    Args:\n      destination: the destination URI(s). Can be a single URI or a list.\n      format: the format to use for the exported data; one of 'csv', 'json', or 'avro'\n          (default 'csv').\n      csv_delimiter: for CSV exports, the field delimiter to use. Defaults to ','\n      csv_header: for CSV exports, whether to include an initial header line. Default true.\n      compress: whether to compress the data on export. Compression is not supported for\n          AVRO format. Defaults to False.\n    Returns:\n      A Job object for the export Job if it was started successfully; else None.\n    \"\"\"\n    format = format.upper()\n    if format == 'JSON':\n      format = 'NEWLINE_DELIMITED_JSON'\n    if format == 'CSV' and csv_delimiter is None:\n      csv_delimiter = ','\n    try:\n      response = self._api.table_extract(self._name_parts, destination, format, compress,\n                                         csv_delimiter, csv_header)\n      return self._init_job_from_response(response)\n    except Exception as e:\n      raise google.datalab.JobError(location=traceback.format_exc(), message=str(e),\n                                    reason=str(type(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract(self, destination, format='csv', csv_delimiter=None, csv_header=True, compress=False):\n    job = self.extract_async(destination, format=format, csv_delimiter=csv_delimiter,\n                             csv_header=csv_header, compress=compress)\n    if job is not None:\n      job.wait()\n    return job", "response": "Exports the table to GCS ; blocks until the export is complete."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart importing a table from GCS and returns a Future that completes the import.", "response": "def load_async(self, source, mode='create', source_format='csv', csv_options=None,\n                 ignore_unknown_values=False, max_bad_records=0):\n    \"\"\" Starts importing a table from GCS and return a Future.\n\n    Args:\n      source: the URL of the source objects(s). Can include a wildcard '*' at the end of the item\n         name. Can be a single source or a list.\n      mode: one of 'create', 'append', or 'overwrite'. 'append' or 'overwrite' will fail if the\n          table does not already exist, while 'create' will fail if it does. The default is\n          'create'. If 'create' the schema will be inferred if necessary.\n      source_format: the format of the data, 'csv' or 'json'; default 'csv'.\n      csv_options: if source format is 'csv', additional options as a CSVOptions object.\n      ignore_unknown_values: If True, accept rows that contain values that do not match the schema;\n          the unknown values are ignored (default False).\n      max_bad_records: the maximum number of bad records that are allowed (and ignored) before\n          returning an 'invalid' error in the Job result (default 0).\n\n    Returns:\n      A Job object for the import if it was started successfully or None if not.\n    Raises:\n      Exception if the load job failed to be started or invalid arguments were supplied.\n    \"\"\"\n    if source_format == 'csv':\n      source_format = 'CSV'\n    elif source_format == 'json':\n      source_format = 'NEWLINE_DELIMITED_JSON'\n    else:\n      raise Exception(\"Invalid source format %s\" % source_format)\n\n    if not(mode == 'create' or mode == 'append' or mode == 'overwrite'):\n      raise Exception(\"Invalid mode %s\" % mode)\n\n    if csv_options is None:\n      csv_options = _csv_options.CSVOptions()\n\n    try:\n      response = self._api.jobs_insert_load(source, self._name_parts,\n                                            append=(mode == 'append'),\n                                            overwrite=(mode == 'overwrite'),\n                                            create=(mode == 'create'),\n                                            source_format=source_format,\n                                            field_delimiter=csv_options.delimiter,\n                                            allow_jagged_rows=csv_options.allow_jagged_rows,\n                                            allow_quoted_newlines=csv_options.allow_quoted_newlines,\n                                            encoding=csv_options.encoding.upper(),\n                                            ignore_unknown_values=ignore_unknown_values,\n                                            max_bad_records=max_bad_records,\n                                            quote=csv_options.quote,\n                                            skip_leading_rows=csv_options.skip_leading_rows)\n    except Exception as e:\n      raise e\n    return self._init_job_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, source, mode='create', source_format='csv', csv_options=None,\n           ignore_unknown_values=False, max_bad_records=0):\n    \"\"\" Load the table from GCS.\n\n    Args:\n      source: the URL of the source objects(s). Can include a wildcard '*' at the end of the item\n         name. Can be a single source or a list.\n      mode: one of 'create', 'append', or 'overwrite'. 'append' or 'overwrite' will fail if the\n          table does not already exist, while 'create' will fail if it does. The default is\n          'create'. If 'create' the schema will be inferred if necessary.\n      source_format: the format of the data, 'csv' or 'json'; default 'csv'.\n      csv_options: if source format is 'csv', additional options as a CSVOptions object.\n      ignore_unknown_values: if True, accept rows that contain values that do not match the schema;\n          the unknown values are ignored (default False).\n      max_bad_records: the maximum number of bad records that are allowed (and ignored) before\n          returning an 'invalid' error in the Job result (default 0).\n\n    Returns:\n      A Job object for the completed load Job if it was started successfully; else None.\n    \"\"\"\n    job = self.load_async(source,\n                          mode=mode,\n                          source_format=source_format,\n                          csv_options=csv_options,\n                          ignore_unknown_values=ignore_unknown_values,\n                          max_bad_records=max_bad_records)\n    if job is not None:\n      job.wait()\n    return job", "response": "Load the item\n         table from GCS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_row_fetcher(self, start_row=0, max_rows=None, page_size=_DEFAULT_PAGE_SIZE):\n    if not start_row:\n      start_row = 0\n    elif start_row < 0:  # We are measuring from the table end\n      if self.length >= 0:\n        start_row += self.length\n      else:\n        raise Exception('Cannot use negative indices for table of unknown length')\n\n    schema = self.schema._bq_schema\n    name_parts = self._name_parts\n\n    def _retrieve_rows(page_token, count):\n\n      page_rows = []\n      if max_rows and count >= max_rows:\n        page_token = None\n      else:\n        if max_rows and page_size > (max_rows - count):\n          max_results = max_rows - count\n        else:\n          max_results = page_size\n\n        try:\n          if page_token:\n            response = self._api.tabledata_list(name_parts, page_token=page_token,\n                                                max_results=max_results)\n          else:\n            response = self._api.tabledata_list(name_parts, start_index=start_row,\n                                                max_results=max_results)\n        except Exception as e:\n          raise e\n        page_token = response['pageToken'] if 'pageToken' in response else None\n        if 'rows' in response:\n          page_rows = response['rows']\n\n      rows = []\n      for row_dict in page_rows:\n        rows.append(_parser.Parser.parse_row(schema, row_dict))\n\n      return rows, page_token\n\n    return _retrieve_rows", "response": "Get a function that can retrieve a page of rows."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports the table to a Pandas dataframe.", "response": "def to_dataframe(self, start_row=0, max_rows=None):\n    \"\"\" Exports the table to a Pandas dataframe.\n\n    Args:\n      start_row: the row of the table at which to start the export (default 0)\n      max_rows: an upper limit on the number of rows to export (default None)\n    Returns:\n      A Pandas dataframe containing the table data.\n    \"\"\"\n    fetcher = self._get_row_fetcher(start_row=start_row,\n                                    max_rows=max_rows,\n                                    page_size=self._MAX_PAGE_SIZE)\n    count = 0\n    page_token = None\n\n    # Collect results of page fetcher in separate dataframe objects, then\n    # concatenate them to reduce the amount of copying\n    df_list = []\n    df = None\n\n    while True:\n      page_rows, page_token = fetcher(page_token, count)\n      if len(page_rows):\n        count += len(page_rows)\n        df_list.append(pandas.DataFrame.from_records(page_rows))\n      if not page_token:\n        break\n    if df_list:\n      df = pandas.concat(df_list, ignore_index=True, copy=False)\n\n    # Need to reorder the dataframe to preserve column ordering\n    ordered_fields = [field.name for field in self.schema]\n    return df[ordered_fields] if df is not None else pandas.DataFrame()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_file(self, destination, format='csv', csv_delimiter=',', csv_header=True):\n    f = codecs.open(destination, 'w', 'utf-8')\n    fieldnames = []\n    for column in self.schema:\n      fieldnames.append(column.name)\n    if sys.version_info[0] == 2:\n      csv_delimiter = csv_delimiter.encode('unicode_escape')\n    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=csv_delimiter)\n    if csv_header:\n      writer.writeheader()\n    for row in self:\n      writer.writerow(row)\n    f.close()", "response": "Save the results to a local file in CSV format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the schema of the table.", "response": "def schema(self):\n    \"\"\"Retrieves the schema of the table.\n\n    Returns:\n      A Schema object containing a list of schema fields and associated metadata.\n    Raises\n      Exception if the request could not be executed or the response was malformed.\n    \"\"\"\n    if not self._schema:\n      try:\n        self._load_info()\n        self._schema = _schema.Schema(self._info['schema']['fields'])\n      except KeyError:\n        raise Exception('Unexpected table response: missing schema')\n    return self._schema"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new Table which is a snapshot of this table at the specified time.", "response": "def snapshot(self, at):\n    \"\"\" Return a new Table which is a snapshot of this table at the specified time.\n\n    Args:\n      at: the time of the snapshot. This can be a Python datetime (absolute) or timedelta\n          (relative to current time). The result must be after the table was created and no more\n          than seven days in the past. Passing None will get a reference the oldest snapshot.\n\n          Note that using a datetime will get a snapshot at an absolute point in time, while\n          a timedelta will provide a varying snapshot; any queries issued against such a Table\n          will be done against a snapshot that has an age relative to the execution time of the\n          query.\n\n    Returns:\n      A new Table object referencing the snapshot.\n\n    Raises:\n      An exception if this Table is already decorated, or if the time specified is invalid.\n    \"\"\"\n    if self._name_parts.decorator != '':\n      raise Exception(\"Cannot use snapshot() on an already decorated table\")\n\n    value = Table._convert_decorator_time(at)\n    return Table(\"%s@%s\" % (self._full_name, str(value)), context=self._context)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef window(self, begin, end=None):\n    if self._name_parts.decorator != '':\n      raise Exception(\"Cannot use window() on an already decorated table\")\n\n    start = Table._convert_decorator_time(begin)\n    if end is None:\n      if isinstance(begin, datetime.timedelta):\n        end = datetime.timedelta(0)\n      else:\n        end = datetime.datetime.utcnow()\n    stop = Table._convert_decorator_time(end)\n\n    # Both values must have the same sign\n    if (start > 0 >= stop) or (stop > 0 >= start):\n      raise Exception(\"window: Between arguments must both be absolute or relative: %s, %s\" %\n                      (str(begin), str(end)))\n\n    # start must be less than stop\n    if start > stop:\n      raise Exception(\"window: Between arguments: begin must be before end: %s, %s\" %\n                      (str(begin), str(end)))\n\n    return Table(\"%s@%s-%s\" % (self._full_name, str(start), str(stop)), context=self._context)", "response": "Returns a new Table limited to the rows added to this Table during the specified time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _refresh_state(self):\n\n    # DataFlow's DataflowPipelineResult does not refresh state, so we have to do it ourselves\n    # as a workaround.\n    # TODO(Change this to use runner_results.state once it refreshes itself)\n    dataflow_internal_job = (\n        self._runner_results._runner.dataflow_client.get_job(self._runner_results.job_id()))\n    self._is_complete = str(dataflow_internal_job.currentState) in ['JOB_STATE_STOPPED',\n                                                                    'JOB_STATE_DONE',\n                                                                    'JOB_STATE_FAILED',\n                                                                    'JOB_STATE_CANCELLED']\n    self._fatal_error = getattr(self._runner_results._runner, 'last_error_msg', None)\n    # Sometimes Dataflow does not populate runner.last_error_msg even if the job fails.\n    if self._fatal_error is None and self._runner_results.state == 'FAILED':\n      self._fatal_error = 'FAILED'", "response": "Refresh the state of the job."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes an instance of data in libsvm format.", "response": "def serialize_example(transformed_json_data, features, feature_indices, target_name):\n  \"\"\"Makes an instance of data in libsvm format.\n\n  Args:\n    transformed_json_data: dict of transformed data.\n    features: features config.\n    feature_indices: output of feature_transforms.get_transformed_feature_indices()\n\n  Returns:\n    The text line representation of an instance in libsvm format.\n  \"\"\"\n  import six\n  import tensorflow as tf\n  from trainer import feature_transforms\n\n  line = str(transformed_json_data[target_name][0])\n  for name, info in feature_indices:\n    if features[name]['transform'] in [feature_transforms.IDENTITY_TRANSFORM,\n                                       feature_transforms.SCALE_TRANSFORM]:\n      line += ' %d:%s' % (info['index_start'], str(transformed_json_data[name][0]))\n    elif features[name]['transform'] in [feature_transforms.ONE_HOT_TRANSFORM,\n                                         feature_transforms.MULTI_HOT_TRANSFORM]:\n      for i in range(info['size']):\n        if i in transformed_json_data[name]:\n          line += ' %d:1' % (info['index_start'] + i)\n    elif features[name]['transform'] in [feature_transforms.IMAGE_TRANSFORM]:\n      for i in range(info['size']):\n        line += ' %d:%s' % (info['index_start'] + i, str(transformed_json_data[name][i]))\n\n  return line"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, delete_contents=False):\n    if not self.exists():\n      raise Exception('Cannot delete non-existent dataset %s' % self._full_name)\n    try:\n      self._api.datasets_delete(self._name_parts, delete_contents=delete_contents)\n    except Exception as e:\n      raise e\n    self._info = None\n    return None", "response": "Issues a request to delete the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the Dataset with the specified friendly name and description.", "response": "def create(self, friendly_name=None, description=None):\n    \"\"\"Creates the Dataset with the specified friendly name and description.\n\n    Args:\n      friendly_name: (optional) the friendly name for the dataset if it is being created.\n      description: (optional) a description for the dataset if it is being created.\n    Returns:\n      The Dataset.\n    Raises:\n      Exception if the Dataset could not be created.\n    \"\"\"\n    if not self.exists():\n      try:\n        response = self._api.datasets_insert(self._name_parts,\n                                             friendly_name=friendly_name,\n                                             description=description)\n      except Exception as e:\n        raise e\n      if 'selfLink' not in response:\n        raise Exception(\"Could not create dataset %s\" % self._full_name)\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing command line arguments.", "response": "def parse_arguments(argv):\n  \"\"\"Parse command line arguments.\n\n  Args:\n    argv: list of command line arguments, includeing programe name.\n\n  Returns:\n    An argparse Namespace object.\n  \"\"\"\n  parser = argparse.ArgumentParser(\n      description='Runs Preprocessing on structured CSV data.')\n  parser.add_argument('--input-file-pattern',\n                      type=str,\n                      required=True,\n                      help='Input CSV file names. May contain a file pattern')\n  parser.add_argument('--output-dir',\n                      type=str,\n                      required=True,\n                      help='Google Cloud Storage which to place outputs.')\n  parser.add_argument('--schema-file',\n                      type=str,\n                      required=True,\n                      help=('BigQuery json schema file'))\n\n  args = parser.parse_args(args=argv[1:])\n\n  # Make sure the output folder exists if local folder.\n  file_io.recursive_create_dir(args.output_dir)\n\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_numerical_categorical_analysis(args, schema_list):\n  header = [column['name'] for column in schema_list]\n  input_files = file_io.get_matching_files(args.input_file_pattern)\n\n  # Check the schema is valid\n  for col_schema in schema_list:\n    col_type = col_schema['type'].lower()\n    if col_type != 'string' and col_type != 'integer' and col_type != 'float':\n      raise ValueError('Schema contains an unsupported type %s.' % col_type)\n\n  # initialize the results\n  def _init_numerical_results():\n    return {'min': float('inf'),\n            'max': float('-inf'),\n            'count': 0,\n            'sum': 0.0}\n  numerical_results = collections.defaultdict(_init_numerical_results)\n  categorical_results = collections.defaultdict(set)\n\n  # for each file, update the numerical stats from that file, and update the set\n  # of unique labels.\n  for input_file in input_files:\n    with file_io.FileIO(input_file, 'r') as f:\n      for line in f:\n        parsed_line = dict(zip(header, line.strip().split(',')))\n\n        for col_schema in schema_list:\n          col_name = col_schema['name']\n          col_type = col_schema['type']\n          if col_type.lower() == 'string':\n            categorical_results[col_name].update([parsed_line[col_name]])\n          else:\n            # numerical column.\n\n            # if empty, skip\n            if not parsed_line[col_name].strip():\n              continue\n\n            numerical_results[col_name]['min'] = (\n              min(numerical_results[col_name]['min'],\n                  float(parsed_line[col_name])))\n            numerical_results[col_name]['max'] = (\n              max(numerical_results[col_name]['max'],\n                  float(parsed_line[col_name])))\n            numerical_results[col_name]['count'] += 1\n            numerical_results[col_name]['sum'] += float(parsed_line[col_name])\n\n  # Update numerical_results to just have min/min/mean\n  for col_schema in schema_list:\n    if col_schema['type'].lower() != 'string':\n      col_name = col_schema['name']\n      mean = numerical_results[col_name]['sum'] / numerical_results[col_name]['count']\n      del numerical_results[col_name]['sum']\n      del numerical_results[col_name]['count']\n      numerical_results[col_name]['mean'] = mean\n\n  # Write the numerical_results to a json file.\n  file_io.write_string_to_file(\n      os.path.join(args.output_dir, NUMERICAL_ANALYSIS_FILE),\n      json.dumps(numerical_results, indent=2, separators=(',', ': ')))\n\n  # Write the vocab files. Each label is on its own line.\n  for name, unique_labels in six.iteritems(categorical_results):\n    labels = '\\n'.join(list(unique_labels))\n    file_io.write_string_to_file(\n        os.path.join(args.output_dir, CATEGORICAL_ANALYSIS_FILE % name),\n        labels)", "response": "Makes the numerical and categorical analysis on the input files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding an analysis files for training.", "response": "def run_analysis(args):\n  \"\"\"Builds an analysis files for training.\"\"\"\n\n  # Read the schema and input feature types\n  schema_list = json.loads(\n      file_io.read_file_to_string(args.schema_file))\n\n  run_numerical_categorical_analysis(args, schema_list)\n\n  # Also save a copy of the schema in the output folder.\n  file_io.copy(args.schema_file,\n               os.path.join(args.output_dir, SCHEMA_FILE),\n               overwrite=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _repr_html_(self):\n    parts = []\n    if self._class:\n      parts.append('<div id=\"hh_%s\" class=\"%s\">%s</div>' % (self._id, self._class, self._markup))\n    else:\n      parts.append('<div id=\"hh_%s\">%s</div>' % (self._id, self._markup))\n\n    if len(self._script) != 0:\n      parts.append('<script>')\n      parts.append('require([')\n      parts.append(','.join(['\"%s\"' % d[0] for d in self._dependencies]))\n      parts.append('], function(')\n      parts.append(','.join([d[1] for d in self._dependencies]))\n      parts.append(') {')\n      parts.append(self._script)\n      parts.append('});')\n      parts.append('</script>')\n\n    return ''.join(parts)", "response": "Generates the HTML representation of the HTML element."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _render_objects(self, items, attributes=None, datatype='object'):\n    if not items:\n      return\n\n    if datatype == 'chartdata':\n      if not attributes:\n        attributes = [items['cols'][i]['label'] for i in range(0, len(items['cols']))]\n      items = items['rows']\n      indices = {attributes[i]: i for i in range(0, len(attributes))}\n\n    num_segments = len(self._segments)\n    self._segments.append('<table>')\n\n    first = True\n    for o in items:\n      if first:\n        first = False\n        if datatype == 'dict' and not attributes:\n          attributes = list(o.keys())\n\n        if attributes is not None:\n          self._segments.append('<tr>')\n          for attr in attributes:\n            self._segments.append('<th>%s</th>' % attr)\n          self._segments.append('</tr>')\n\n      self._segments.append('<tr>')\n      if attributes is None:\n        self._segments.append('<td>%s</td>' % HtmlBuilder._format(o))\n      else:\n        for attr in attributes:\n          if datatype == 'dict':\n            self._segments.append('<td>%s</td>' % HtmlBuilder._format(o.get(attr, None), nbsp=True))\n          elif datatype == 'chartdata':\n            self._segments.append('<td>%s</td>' % HtmlBuilder._format(o['c'][indices[attr]]['v'],\n                                                                      nbsp=True))\n          else:\n            self._segments.append('<td>%s</td>' % HtmlBuilder._format(o.__getattribute__(attr),\n                                                                      nbsp=True))\n\n      self._segments.append('</tr>')\n\n    self._segments.append('</table>')\n    if first:\n      # The table was empty; drop it from the segments.\n      self._segments = self._segments[:num_segments]", "response": "Renders an HTML table with the specified list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender an HTML formatted text block with the specified text.", "response": "def _render_text(self, text, preformatted=False):\n    \"\"\"Renders an HTML formatted text block with the specified text.\n\n    Args:\n      text: the text to render\n      preformatted: whether the text should be rendered as preformatted\n    \"\"\"\n    tag = 'pre' if preformatted else 'div'\n    self._segments.append('<%s>%s</%s>' % (tag, HtmlBuilder._format(text), tag))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _render_list(self, items, empty='<pre>&lt;empty&gt;</pre>'):\n    if not items or len(items) == 0:\n      self._segments.append(empty)\n      return\n    self._segments.append('<ul>')\n    for o in items:\n      self._segments.append('<li>')\n      self._segments.append(str(o))\n      self._segments.append('</li>')\n    self._segments.append('</ul>')", "response": "Renders an HTML list with the specified list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render_text(text, preformatted=False):\n    builder = HtmlBuilder()\n    builder._render_text(text, preformatted=preformatted)\n    return builder._to_html()", "response": "Renders an HTML formatted text block with the specified text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary list formatted as a HTML table.", "response": "def render_table(data, headers=None):\n    \"\"\" Return a dictionary list formatted as a HTML table.\n\n    Args:\n      data: a list of dictionaries, one per row.\n      headers: the keys in the dictionary to use as table columns, in order.\n    \"\"\"\n    builder = HtmlBuilder()\n    builder._render_objects(data, headers, datatype='dict')\n    return builder._to_html()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary list formatted as an HTML table.", "response": "def render_chart_data(data):\n    \"\"\" Return a dictionary list formatted as a HTML table.\n\n    Args:\n      data: data in the form consumed by Google Charts.\n    \"\"\"\n    builder = HtmlBuilder()\n    builder._render_objects(data, datatype='chartdata')\n    return builder._to_html()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(self, fields=None, count=5, sampling=None, use_cache=True, dialect=None,\n             billing_tier=None):\n    \"\"\"Retrieves a sampling of data from the table.\n\n    Args:\n      fields: an optional list of field names to retrieve.\n      count: an optional count of rows to retrieve which is used if a specific\n          sampling is not specified.\n      sampling: an optional sampling strategy to apply to the table.\n      use_cache: whether to use cached results or not.\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A QueryResultsTable object containing the resulting data.\n    Raises:\n      Exception if the sample query could not be executed or query response was malformed.\n    \"\"\"\n    # Do import here to avoid top-level circular dependencies.\n    from . import _query\n    sql = self._repr_sql_()\n    return _query.Query.sampling_query(sql, context=self._context, count=count, fields=fields,\n                                       sampling=sampling).results(use_cache=use_cache,\n                                                                  dialect=dialect,\n                                                                  billing_tier=billing_tier)", "response": "Retrieves a sampling of data from the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _encode_dict_as_row(record, column_name_map):\n    for k in list(record.keys()):\n      v = record[k]\n      # If the column is a date, convert to ISO string.\n      if isinstance(v, pandas.Timestamp) or isinstance(v, datetime.datetime):\n        v = record[k] = record[k].isoformat()\n\n      # If k has invalid characters clean it up\n      if k not in column_name_map:\n        column_name_map[k] = ''.join(c for c in k if c in Table._VALID_COLUMN_NAME_CHARACTERS)\n      new_k = column_name_map[k]\n      if k != new_k:\n        record[new_k] = v\n        del record[k]\n    return record", "response": "Encode a dictionary representing a table row in a form suitable for streaming to BQ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_data(self, data, include_index=False, index_name=None):\n    # TODO(gram): we could create the Table here is it doesn't exist using a schema derived\n    # from the data. IIRC we decided not to but doing so seems less unwieldy that having to\n    # create it first and then validate the schema against it itself.\n\n    # There are BigQuery limits on the streaming API:\n    #\n    # max_rows_per_post = 500\n    # max_bytes_per_row = 20000\n    # max_rows_per_second = 10000\n    # max_bytes_per_post = 1000000\n    # max_bytes_per_second = 10000000\n    #\n    # It is non-trivial to enforce these here, and the max bytes per row is not something we\n    # can really control. As an approximation we enforce the 500 row limit\n    # with a 0.05 sec POST interval (to enforce the 10,000 rows per sec limit).\n    max_rows_per_post = 500\n    post_interval = 0.05\n\n    # TODO(gram): add different exception types for each failure case.\n    if not self.exists():\n      raise Exception('Table %s does not exist.' % self._full_name)\n\n    data_schema = _schema.Schema.from_data(data)\n    if isinstance(data, list):\n      if include_index:\n        if not index_name:\n          index_name = 'Index'\n        data_schema._add_field(index_name, 'INTEGER')\n\n    table_schema = self.schema\n\n    # Do some validation of the two schema to make sure they are compatible.\n    for data_field in data_schema:\n      name = data_field.name\n      table_field = table_schema[name]\n      if table_field is None:\n        raise Exception('Table does not contain field %s' % name)\n      data_type = data_field.data_type\n      table_type = table_field.data_type\n      if table_type != data_type:\n        raise Exception('Field %s in data has type %s but in table has type %s' %\n                        (name, data_type, table_type))\n\n    total_rows = len(data)\n    total_pushed = 0\n\n    job_id = uuid.uuid4().hex\n    rows = []\n    column_name_map = {}\n\n    is_dataframe = isinstance(data, pandas.DataFrame)\n    if is_dataframe:\n      # reset_index creates a new dataframe so we don't affect the original. reset_index(drop=True)\n      # drops the original index and uses an integer range.\n      gen = data.reset_index(drop=not include_index).iterrows()\n    else:\n      gen = enumerate(data)\n\n    for index, row in gen:\n      if is_dataframe:\n        row = row.to_dict()\n      elif include_index:\n        row[index_name] = index\n\n      rows.append({\n        'json': self._encode_dict_as_row(row, column_name_map),\n        'insertId': job_id + str(index)\n      })\n\n      total_pushed += 1\n\n      if (total_pushed == total_rows) or (len(rows) == max_rows_per_post):\n        try:\n          response = self._api.tabledata_insert_all(self._name_parts, rows)\n        except Exception as e:\n          raise e\n        if 'insertErrors' in response:\n          raise Exception('insertAll failed: %s' % response['insertErrors'])\n\n        time.sleep(post_interval)  # Streaming API is rate-limited\n        rows = []\n\n    # Block until data is ready\n    while True:\n      self._info = self._api.tables_get(self._name_parts)\n      if 'streamingBuffer' not in self._info or \\\n              'estimatedRows' not in self._info['streamingBuffer'] or \\\n              int(self._info['streamingBuffer']['estimatedRows']) > 0:\n        break\n      time.sleep(2)\n\n    return self", "response": "Insert the contents of a Pandas DataFrame or a list of dictionaries into the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an iterator to iterate through a set of table rows.", "response": "def range(self, start_row=0, max_rows=None):\n    \"\"\" Get an iterator to iterate through a set of table rows.\n\n    Args:\n      start_row: the row of the table at which to start the iteration (default 0)\n      max_rows: an upper limit on the number of rows to iterate through (default None)\n\n    Returns:\n      A row iterator.\n    \"\"\"\n    fetcher = self._get_row_fetcher(start_row=start_row, max_rows=max_rows)\n    return iter(datalab.utils.Iterator(fetcher))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport the table to a Pandas dataframe.", "response": "def to_dataframe(self, start_row=0, max_rows=None):\n    \"\"\" Exports the table to a Pandas dataframe.\n\n    Args:\n      start_row: the row of the table at which to start the export (default 0)\n      max_rows: an upper limit on the number of rows to export (default None)\n    Returns:\n      A Pandas dataframe containing the table data.\n    \"\"\"\n    fetcher = self._get_row_fetcher(start_row=start_row, max_rows=max_rows)\n    count = 0\n    page_token = None\n    df = None\n    while True:\n      page_rows, page_token = fetcher(page_token, count)\n      if len(page_rows):\n        count += len(page_rows)\n        if df is None:\n          df = pandas.DataFrame.from_records(page_rows)\n        else:\n          df = df.append(page_rows, ignore_index=True)\n      if not page_token:\n        break\n\n    # Need to reorder the dataframe to preserve column ordering\n    ordered_fields = [field.name for field in self.schema]\n    return df[ordered_fields] if df is not None else pandas.DataFrame()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_file_async(self, destination, format='csv', csv_delimiter=',', csv_header=True):\n    self.to_file(destination, format=format, csv_delimiter=csv_delimiter, csv_header=csv_header)", "response": "Save the results to a local file in CSV format and return a Job for completion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_query(self, fields=None):\n    # Do import here to avoid top-level circular dependencies.\n    from . import _query\n    if fields is None:\n      fields = '*'\n    elif isinstance(fields, list):\n      fields = ','.join(fields)\n    return _query.Query('SELECT %s FROM %s' % (fields, self._repr_sql_()), context=self._context)", "response": "Return a Query object that will return the specified fields from the records in this Table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_to(self, new_key, bucket=None):\n    if bucket is None:\n      bucket = self._bucket\n    try:\n      new_info = self._api.objects_copy(self._bucket, self._key, bucket, new_key)\n    except Exception as e:\n      raise e\n    return Item(bucket, new_key, new_info, context=self._context)", "response": "Copies this item to the specified new key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exists(self):\n    try:\n      return self.metadata is not None\n    except datalab.utils.RequestException:\n      return False\n    except Exception as e:\n      raise e", "response": "Checks if the item exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self):\n    if self.exists():\n      try:\n        self._api.objects_delete(self._bucket, self._key)\n      except Exception as e:\n        raise e", "response": "Deletes this item from its bucket."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_to(self, content, content_type):\n    try:\n      self._api.object_upload(self._bucket, self._key, content, content_type)\n    except Exception as e:\n      raise e", "response": "Writes text content to this item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the specified item exists in the object tree.", "response": "def contains(self, key):\n    \"\"\"Checks if the specified item exists.\n\n    Args:\n      key: the key of the item to lookup.\n    Returns:\n      True if the item exists; False otherwise.\n    Raises:\n      Exception if there was an error requesting information about the item.\n    \"\"\"\n    try:\n      self._api.objects_get(self._bucket, key)\n    except datalab.utils.RequestException as e:\n      if e.status == 404:\n        return False\n      raise e\n    except Exception as e:\n      raise e\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the credentials to use.", "response": "def get_credentials():\n  \"\"\" Get the credentials to use. We try application credentials first, followed by\n      user credentials. The path to the application credentials can be overridden\n      by pointing the GOOGLE_APPLICATION_CREDENTIALS environment variable to some file;\n      the path to the user credentials can be overridden by pointing the CLOUDSDK_CONFIG\n      environment variable to some directory (after which we will look for the file\n      $CLOUDSDK_CONFIG/gcloud/credentials). Unless you have specific reasons for\n      overriding these the defaults should suffice.\n  \"\"\"\n  try:\n    credentials, _ = google.auth.default()\n    credentials = google.auth.credentials.with_scopes_if_required(credentials, CREDENTIAL_SCOPES)\n    return credentials\n  except Exception as e:\n\n    # Try load user creds from file\n    cred_file = get_config_dir() + '/credentials'\n    if os.path.exists(cred_file):\n      with open(cred_file) as f:\n        creds = json.loads(f.read())\n      # Use the first gcloud one we find\n      for entry in creds['data']:\n        if entry['key']['type'] == 'google-cloud-sdk':\n          creds = oauth2client.client.OAuth2Credentials.from_json(json.dumps(entry['credential']))\n          return _convert_oauth2client_creds(creds)\n\n    if type(e) == google.auth.exceptions.DefaultCredentialsError:\n      # If we are in Datalab container, change the message to be about signing in.\n      if _in_datalab_docker():\n        raise Exception('No application credentials found. Perhaps you should sign in.')\n\n    raise e"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimplement the datalab cell magic for ipython notebooks.", "response": "def datalab(line, cell=None):\n  \"\"\"Implements the datalab cell magic for ipython notebooks.\n\n  Args:\n    line: the contents of the datalab line.\n  Returns:\n    The results of executing the cell.\n  \"\"\"\n  parser = google.datalab.utils.commands.CommandParser(\n      prog='%datalab',\n      description=\"\"\"\nExecute operations that apply to multiple Datalab APIs.\n\nUse \"%datalab <command> -h\" for help on a specific command.\n\"\"\")\n\n  config_parser = parser.subcommand(\n      'config', help='List or set API-specific configurations.')\n  config_sub_commands = config_parser.add_subparsers(dest='command')\n\n  # %%datalab config list\n  config_list_parser = config_sub_commands.add_parser(\n      'list', help='List configurations')\n  config_list_parser.set_defaults(func=_config_list_fn)\n\n  # %%datalab config set -n <NAME> -v <VALUE>\n  config_set_parser = config_sub_commands.add_parser(\n      'set', help='Set configurations')\n  config_set_parser.add_argument(\n      '-n', '--name',\n      help='The name of the configuration value', required=True)\n  config_set_parser.add_argument(\n      '-v', '--value', help='The value to set', required=True)\n  config_set_parser.set_defaults(func=_config_set_fn)\n\n  project_parser = parser.subcommand(\n      'project', help='Get or set the default project ID')\n  project_sub_commands = project_parser.add_subparsers(dest='command')\n\n  # %%datalab project get\n  project_get_parser = project_sub_commands.add_parser(\n      'get', help='Get the default project ID')\n  project_get_parser.set_defaults(func=_project_get_fn)\n\n  # %%datalab project set -p <PROJECT_ID>\n  project_set_parser = project_sub_commands.add_parser(\n      'set', help='Set the default project ID')\n  project_set_parser.add_argument(\n      '-p', '--project', help='The default project ID', required=True)\n  project_set_parser.set_defaults(func=_project_set_fn)\n\n  return google.datalab.utils.commands.handle_magic_line(line, cell, parser)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request(url, args=None, data=None, headers=None, method=None,\n              credentials=None, raw_response=False, stats=None):\n    \"\"\"Issues HTTP requests.\n\n    Args:\n      url: the URL to request.\n      args: optional query string arguments.\n      data: optional data to be sent within the request.\n      headers: optional headers to include in the request.\n      method: optional HTTP method to use. If unspecified this is inferred\n          (GET or POST) based on the existence of request data.\n      credentials: optional set of credentials to authorize the request.\n      raw_response: whether the raw response content should be returned as-is.\n      stats: an optional dictionary that, if provided, will be populated with some\n          useful info about the request, like 'duration' in seconds and 'data_size' in\n          bytes. These may be useful optimizing the access to rate-limited APIs.\n    Returns:\n      The parsed response object.\n    Raises:\n      Exception when the HTTP request fails or the response cannot be processed.\n    \"\"\"\n    if headers is None:\n      headers = {}\n\n    headers['user-agent'] = 'GoogleCloudDataLab/1.0'\n    # Add querystring to the URL if there are any arguments.\n    if args is not None:\n      qs = urllib.parse.urlencode(args)\n      url = url + '?' + qs\n\n    # Setup method to POST if unspecified, and appropriate request headers\n    # if there is data to be sent within the request.\n    if data is not None:\n      if method is None:\n        method = 'POST'\n\n      if data != '':\n        # If there is a content type specified, use it (and the data) as-is.\n        # Otherwise, assume JSON, and serialize the data object.\n        if 'Content-Type' not in headers:\n          data = json.dumps(data)\n          headers['Content-Type'] = 'application/json'\n      headers['Content-Length'] = str(len(data))\n    else:\n      if method == 'POST':\n        headers['Content-Length'] = '0'\n\n    # If the method is still unset, i.e. it was unspecified, and there\n    # was no data to be POSTed, then default to GET request.\n    if method is None:\n      method = 'GET'\n\n    http = Http.http\n\n    # Authorize with credentials if given\n    if credentials is not None:\n      # Make a copy of the shared http instance before we modify it.\n      http = copy.copy(http)\n      http = google_auth_httplib2.AuthorizedHttp(credentials)\n    if stats is not None:\n      stats['duration'] = datetime.datetime.utcnow()\n\n    response = None\n    try:\n      log.debug('request: method[%(method)s], url[%(url)s], body[%(data)s]' % locals())\n      response, content = http.request(url,\n                                       method=method,\n                                       body=data,\n                                       headers=headers)\n      if 200 <= response.status < 300:\n        if raw_response:\n          return content\n        if type(content) == str:\n          return json.loads(content)\n        else:\n          return json.loads(str(content, encoding='UTF-8'))\n      else:\n        raise RequestException(response.status, content)\n    except ValueError:\n      raise Exception('Failed to process HTTP response.')\n    except httplib2.HttpLib2Error:\n      raise Exception('Failed to send HTTP request.')\n    finally:\n      if stats is not None:\n        stats['data_size'] = len(data)\n        stats['status'] = response.status\n        stats['duration'] = (datetime.datetime.utcnow() - stats['duration']).total_seconds()", "response": "Issues a HTTP request to the specified URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_cell(args, cell_body):\n  name = args.get('name')\n  if name is None:\n    raise Exception(\"Pipeline name was not specified.\")\n  pipeline_spec = google.datalab.utils.commands.parse_config(\n    cell_body, google.datalab.utils.commands.notebook_environment())\n  airflow_spec = google.datalab.contrib.pipeline._pipeline.PipelineGenerator.generate_airflow_spec(\n      name, pipeline_spec)\n\n  debug = args.get('debug')\n  if debug is True:\n    return airflow_spec", "response": "Implements the pipeline cell create magic used to create the airflow spec."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate and initialize a pipeline subcommand handler.", "response": "def _add_command(parser, subparser_fn, handler, cell_required=False,\n                 cell_prohibited=False):\n  \"\"\" Create and initialize a pipeline subcommand handler. \"\"\"\n  sub_parser = subparser_fn(parser)\n  sub_parser.set_defaults(func=lambda args, cell: _dispatch_handler(\n      args, cell, sub_parser, handler, cell_required=cell_required,\n      cell_prohibited=cell_prohibited))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_pipeline_parser():\n  parser = google.datalab.utils.commands.CommandParser(\n      prog='%pipeline', description=\"\"\"\nExecute various pipeline-related operations. Use \"%pipeline <command> -h\"\nfor help on a specific command.\n  \"\"\")\n\n  # %%pipeline create\n  _add_command(parser, _create_create_subparser, _create_cell)\n\n  return parser", "response": "Create the parser for the %pipeline magics."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimplements the pipeline cell magic for ipython notebooks.", "response": "def pipeline(line, cell=None):\n  \"\"\"Implements the pipeline cell magic for ipython notebooks.\n\n  The supported syntax is:\n\n    %%pipeline <command> [<args>]\n    <cell>\n\n  or:\n\n    %pipeline <command> [<args>]\n\n  Use %pipeline --help for a list of commands, or %pipeline <command> --help for\n  help on a specific command.\n  \"\"\"\n  return google.datalab.utils.commands.handle_magic_line(line, cell, _pipeline_parser)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dispatch_handler(args, cell, parser, handler, cell_required=False,\n                      cell_prohibited=False):\n  \"\"\" Makes sure cell magics include cell and line magics don't, before\n    dispatching to handler.\n\n  Args:\n    args: the parsed arguments from the magic line.\n    cell: the contents of the cell, if any.\n    parser: the argument parser for <cmd>; used for error message.\n    handler: the handler to call if the cell present/absent check passes.\n    cell_required: True for cell magics, False for line magics that can't be\n      cell magics.\n    cell_prohibited: True for line magics, False for cell magics that can't be\n      line magics.\n  Returns:\n    The result of calling the handler.\n  Raises:\n    Exception if the invocation is not valid.\n  \"\"\"\n  if cell_prohibited:\n    if cell and len(cell.strip()):\n      parser.print_help()\n      raise Exception(\n          'Additional data is not supported with the %s command.' % parser.prog)\n    return handler(args)\n\n  if cell_required and not cell:\n    parser.print_help()\n    raise Exception('The %s command requires additional data' % parser.prog)\n\n  return handler(args, cell)", "response": "Dispatches the handler to the arguments cell and line magics."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ProtoFromTfRecordFiles(files,\n                           max_entries=10000,\n                           features=None,\n                           is_sequence=False,\n                           iterator_options=None):\n  \"\"\"Creates a feature statistics proto from a set of TFRecord files.\n\n  Args:\n    files: A list of dicts describing files for each dataset for the proto.\n      Each\n        entry contains a 'path' field with the path to the TFRecord file on\n          disk\n        and a 'name' field to identify the dataset in the proto.\n    max_entries: The maximum number of examples to load from each dataset\n        in order to create the proto. Defaults to 10000.\n    features: A list of strings that is a whitelist of feature names to create\n        feature statistics for. If set to None then all features in the\n          dataset\n        are analyzed. Defaults to None.\n    is_sequence: True if the input data from 'tables' are tf.SequenceExamples,\n        False if tf.Examples. Defaults to false.\n    iterator_options: Options to pass to the iterator that reads the examples.\n        Defaults to None.\n\n  Returns:\n    The feature statistics proto for the provided files.\n  \"\"\"\n  warnings.warn(\n      'Use GenericFeatureStatisticsGenerator class method instead.',\n      DeprecationWarning)\n  return FeatureStatisticsGenerator().ProtoFromTfRecordFiles(\n      files, max_entries, features, is_sequence, iterator_options)", "response": "Creates a feature statistics proto from a set of TFRecord files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the transform and schema are compatible.", "response": "def check_schema_transforms_match(schema, inverted_features):\n  \"\"\"Checks that the transform and schema do not conflict.\n\n  Args:\n    schema: schema list\n    inverted_features: inverted_features dict\n\n  Raises:\n    ValueError if transform cannot be applied given schema type.\n  \"\"\"\n  num_target_transforms = 0\n\n  for col_schema in schema:\n    col_name = col_schema['name']\n    col_type = col_schema['type'].lower()\n\n    # Check each transform and schema are compatible\n    if col_name in inverted_features:\n      for transform in inverted_features[col_name]:\n        transform_name = transform['transform']\n        if transform_name == constant.TARGET_TRANSFORM:\n          num_target_transforms += 1\n          continue\n\n        elif col_type in constant.NUMERIC_SCHEMA:\n          if transform_name not in constant.NUMERIC_TRANSFORMS:\n            raise ValueError(\n                'Transform %s not supported by schema %s' % (transform_name, col_type))\n        elif col_type == constant.STRING_SCHEMA:\n          if (transform_name not in constant.CATEGORICAL_TRANSFORMS + constant.TEXT_TRANSFORMS and\n             transform_name != constant.IMAGE_TRANSFORM):\n            raise ValueError(\n                'Transform %s not supported by schema %s' % (transform_name, col_type))\n        else:\n          raise ValueError('Unsupported schema type %s' % col_type)\n\n    # Check each transform is compatible for the same source column.\n    # inverted_features[col_name] should belong to exactly 1 of the 5 groups.\n    if col_name in inverted_features:\n      transform_set = {x['transform'] for x in inverted_features[col_name]}\n      if 1 != sum([transform_set.issubset(set(constant.NUMERIC_TRANSFORMS)),\n                   transform_set.issubset(set(constant.CATEGORICAL_TRANSFORMS)),\n                   transform_set.issubset(set(constant.TEXT_TRANSFORMS)),\n                   transform_set.issubset(set([constant.IMAGE_TRANSFORM])),\n                   transform_set.issubset(set([constant.TARGET_TRANSFORM]))]):\n        message = \"\"\"\n          The source column of a feature can only be used in multiple\n          features within the same family of transforms. The familes are\n\n          1) text transformations: %s\n          2) categorical transformations: %s\n          3) numerical transformations: %s\n          4) image transformations: %s\n          5) target transform: %s\n\n          Any column can also be a key column.\n\n          But column %s is used by transforms %s.\n          \"\"\" % (str(constant.TEXT_TRANSFORMS),\n                 str(constant.CATEGORICAL_TRANSFORMS),\n                 str(constant.NUMERIC_TRANSFORMS),\n                 constant.IMAGE_TRANSFORM,\n                 constant.TARGET_TRANSFORM,\n                 col_name,\n                 str(transform_set))\n        raise ValueError(message)\n\n  if num_target_transforms != 1:\n    raise ValueError('Must have exactly one target transform')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expand_defaults(schema, features):\n\n  schema_names = [x['name'] for x in schema]\n\n  # Add missing source columns\n  for name, transform in six.iteritems(features):\n    if 'source_column' not in transform:\n      transform['source_column'] = name\n\n  # Check source columns are in the schema and collect which are used.\n  used_schema_columns = []\n  for name, transform in six.iteritems(features):\n    if transform['source_column'] not in schema_names:\n      raise ValueError('source column %s is not in the schema for transform %s'\n                       % (transform['source_column'], name))\n    used_schema_columns.append(transform['source_column'])\n\n  # Update default transformation based on schema.\n  for col_schema in schema:\n    schema_name = col_schema['name']\n    schema_type = col_schema['type'].lower()\n\n    if schema_type not in constant.NUMERIC_SCHEMA + [constant.STRING_SCHEMA]:\n      raise ValueError(('Only the following schema types are supported: %s'\n                        % ' '.join(constant.NUMERIC_SCHEMA + [constant.STRING_SCHEMA])))\n\n    if schema_name not in used_schema_columns:\n      # add the default transform to the features\n      if schema_type in constant.NUMERIC_SCHEMA:\n        features[schema_name] = {\n            'transform': constant.DEFAULT_NUMERIC_TRANSFORM,\n            'source_column': schema_name}\n      elif schema_type == constant.STRING_SCHEMA:\n        features[schema_name] = {\n            'transform': constant.DEFAULT_CATEGORICAL_TRANSFORM,\n            'source_column': schema_name}\n      else:\n        raise NotImplementedError('Unknown type %s' % schema_type)", "response": "Add to features any default transformations that are applied on the specified schema."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninvert the features in the order they appear in the source column.", "response": "def invert_features(features):\n  \"\"\"Make a dict in the form source column : set of transforms.\n\n  Note that the key transform is removed.\n  \"\"\"\n  inverted_features = collections.defaultdict(list)\n  for transform in six.itervalues(features):\n    source_column = transform['source_column']\n    inverted_features[source_column].append(transform)\n\n  return dict(inverted_features)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_local_analysis(output_dir, csv_file_pattern, schema, features):\n  sys.stdout.write('Expanding any file patterns...\\n')\n  sys.stdout.flush()\n  header = [column['name'] for column in schema]\n  input_files = []\n  for file_pattern in csv_file_pattern:\n    input_files.extend(file_io.get_matching_files(file_pattern))\n  sys.stdout.write('file list computed.\\n')\n  sys.stdout.flush()\n\n  expand_defaults(schema, features)  # features are updated.\n  inverted_features = invert_features(features)\n  check_schema_transforms_match(schema, inverted_features)\n\n  # Make a copy of inverted_features and update the target transform to be\n  # identity or one hot depending on the schema.\n  inverted_features_target = copy.deepcopy(inverted_features)\n  for name, transforms in six.iteritems(inverted_features_target):\n    transform_set = {x['transform'] for x in transforms}\n    if transform_set == set([constant.TARGET_TRANSFORM]):\n      target_schema = next(col['type'].lower() for col in schema if col['name'] == name)\n      if target_schema in constant.NUMERIC_SCHEMA:\n        inverted_features_target[name] = [{'transform': constant.IDENTITY_TRANSFORM}]\n      else:\n        inverted_features_target[name] = [{'transform': constant.ONE_HOT_TRANSFORM}]\n\n  # initialize the results\n  def _init_numerical_results():\n    return {'min': float('inf'),\n            'max': float('-inf'),\n            'count': 0,\n            'sum': 0.0}\n  numerical_results = collections.defaultdict(_init_numerical_results)\n  vocabs = collections.defaultdict(lambda: collections.defaultdict(int))\n\n  num_examples = 0\n  # for each file, update the numerical stats from that file, and update the set\n  # of unique labels.\n  for input_file in input_files:\n    sys.stdout.write('Analyzing file %s...\\n' % input_file)\n    sys.stdout.flush()\n    with file_io.FileIO(input_file, 'r') as f:\n      for line in csv.reader(f):\n        if len(header) != len(line):\n          raise ValueError('Schema has %d columns but a csv line only has %d columns.' %\n                           (len(header), len(line)))\n        parsed_line = dict(zip(header, line))\n        num_examples += 1\n\n        for col_name, transform_set in six.iteritems(inverted_features_target):\n          # All transforms in transform_set require the same analysis. So look\n          # at the first transform.\n          transform = next(iter(transform_set))\n          if transform['transform'] in constant.TEXT_TRANSFORMS:\n            separator = transform.get('separator', ' ')\n            split_strings = parsed_line[col_name].split(separator)\n\n            # If a label is in the row N times, increase it's vocab count by 1.\n            # This is needed for TFIDF, but it's also an interesting stat.\n            for one_label in set(split_strings):\n              # Filter out empty strings\n              if one_label:\n                vocabs[col_name][one_label] += 1\n          elif transform['transform'] in constant.CATEGORICAL_TRANSFORMS:\n            if parsed_line[col_name]:\n              vocabs[col_name][parsed_line[col_name]] += 1\n          elif transform['transform'] in constant.NUMERIC_TRANSFORMS:\n            if not parsed_line[col_name].strip():\n              continue\n\n            numerical_results[col_name]['min'] = (\n              min(numerical_results[col_name]['min'],\n                  float(parsed_line[col_name])))\n            numerical_results[col_name]['max'] = (\n              max(numerical_results[col_name]['max'],\n                  float(parsed_line[col_name])))\n            numerical_results[col_name]['count'] += 1\n            numerical_results[col_name]['sum'] += float(parsed_line[col_name])\n\n    sys.stdout.write('file %s analyzed.\\n' % input_file)\n    sys.stdout.flush()\n\n  # Write the vocab files. Each label is on its own line.\n  vocab_sizes = {}\n  for name, label_count in six.iteritems(vocabs):\n    # df is now:\n    # label1,count\n    # label2,count\n    # ...\n    # where label1 is the most frequent label, and label2 is the 2nd most, etc.\n    df = pd.DataFrame([{'label': label, 'count': count}\n                       for label, count in sorted(six.iteritems(label_count),\n                                                  key=lambda x: x[1],\n                                                  reverse=True)],\n                      columns=['label', 'count'])\n    csv_string = df.to_csv(index=False, header=False)\n\n    file_io.write_string_to_file(\n        os.path.join(output_dir, constant.VOCAB_ANALYSIS_FILE % name),\n        csv_string)\n\n    vocab_sizes[name] = {'vocab_size': len(label_count)}\n\n  # Update numerical_results to just have min/min/mean\n  for col_name in numerical_results:\n    if float(numerical_results[col_name]['count']) == 0:\n      raise ValueError('Column %s has a zero count' % col_name)\n    mean = (numerical_results[col_name]['sum'] /\n            float(numerical_results[col_name]['count']))\n    del numerical_results[col_name]['sum']\n    del numerical_results[col_name]['count']\n    numerical_results[col_name]['mean'] = mean\n\n  # Write the stats file.\n  numerical_results.update(vocab_sizes)\n  stats = {'column_stats': numerical_results, 'num_examples': num_examples}\n  file_io.write_string_to_file(\n      os.path.join(output_dir, constant.STATS_FILE),\n      json.dumps(stats, indent=2, separators=(',', ': ')))\n\n  save_schema_features(schema, features, output_dir)", "response": "Use pandas to analyze csv files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_query_argument(args, cell, env):\n  sql_arg = args.get('query', None)\n  if sql_arg is None:\n    # Assume we have inline SQL in the cell\n    if not isinstance(cell, basestring):\n      raise Exception('Expected a --query argument or inline SQL')\n    return datalab.bigquery.Query(cell, values=env)\n\n  item = datalab.utils.commands.get_notebook_item(sql_arg)\n  if isinstance(item, datalab.bigquery.Query):  # Queries are already expanded.\n    return item\n\n  # Create an expanded BQ Query.\n  config = datalab.utils.commands.parse_config(cell, env)\n  item, env = datalab.data.SqlModule.get_sql_statement_with_environment(item, config)\n  if cell:\n    env.update(config)  # config is both a fallback and an override.\n  return datalab.bigquery.Query(item, values=env)", "response": "Get a query argument to a cell magic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sample_cell(args, cell_body):\n\n  env = datalab.utils.commands.notebook_environment()\n  query = None\n  table = None\n  view = None\n\n  if args['query']:\n    query = _get_query_argument(args, cell_body, env)\n  elif args['table']:\n    table = _get_table(args['table'])\n  elif args['view']:\n    view = datalab.utils.commands.get_notebook_item(args['view'])\n    if not isinstance(view, datalab.bigquery.View):\n      raise Exception('%s is not a view' % args['view'])\n  else:\n    query = datalab.bigquery.Query(cell_body, values=env)\n\n  count = args['count']\n  method = args['method']\n  if method == 'random':\n    sampling = datalab.bigquery.Sampling.random(percent=args['percent'], count=count)\n  elif method == 'hashed':\n    sampling = datalab.bigquery.Sampling.hashed(field_name=args['field'], percent=args['percent'],\n                                                count=count)\n  elif method == 'sorted':\n    ascending = args['order'] == 'ascending'\n    sampling = datalab.bigquery.Sampling.sorted(args['field'],\n                                                ascending=ascending,\n                                                count=count)\n  elif method == 'limit':\n    sampling = datalab.bigquery.Sampling.default(count=count)\n  else:\n    sampling = datalab.bigquery.Sampling.default(count=count)\n\n  if query:\n    results = query.sample(sampling=sampling, dialect=args['dialect'], billing_tier=args['billing'])\n  elif view:\n    results = view.sample(sampling=sampling)\n  else:\n    results = table.sample(sampling=sampling)\n  if args['verbose']:\n    print(results.sql)\n  if args['profile']:\n    return datalab.utils.commands.profile_df(results.to_dataframe())\n  else:\n    return results", "response": "Implements the bigquery sample cell magic for ipython notebooks."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimplement the BigQuery cell magic used to create datasets and tables.", "response": "def _create_cell(args, cell_body):\n  \"\"\"Implements the BigQuery cell magic used to create datasets and tables.\n\n   The supported syntax is:\n\n     %%bigquery create dataset -n|--name <name> [-f|--friendly <friendlyname>]\n     [<description>]\n\n   or:\n\n     %%bigquery create table -n|--name <tablename> [--overwrite]\n     [<YAML or JSON cell_body defining schema to use for tables>]\n\n  Args:\n    args: the argument following '%bigquery create <command>'.\n  \"\"\"\n  if args['command'] == 'dataset':\n    try:\n      datalab.bigquery.Dataset(args['name']).create(friendly_name=args['friendly'],\n                                                    description=cell_body)\n    except Exception as e:\n      print('Failed to create dataset %s: %s' % (args['name'], e))\n  else:\n    if cell_body is None:\n      print('Failed to create %s: no schema specified' % args['name'])\n    else:\n      try:\n        record = datalab.utils.commands.parse_config(cell_body,\n                                                     datalab.utils.commands.notebook_environment(),\n                                                     as_dict=False)\n        schema = datalab.bigquery.Schema(record)\n        datalab.bigquery.Table(args['name']).create(schema=schema, overwrite=args['overwrite'])\n      except Exception as e:\n        print('Failed to create table %s: %s' % (args['name'], e))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimplementing the BigQuery cell delete magic used to delete datasets and tables.", "response": "def _delete_cell(args, _):\n  \"\"\"Implements the BigQuery cell magic used to delete datasets and tables.\n\n   The supported syntax is:\n\n     %%bigquery delete dataset -n|--name <name>\n\n   or:\n\n     %%bigquery delete table -n|--name <name>\n\n  Args:\n    args: the argument following '%bigquery delete <command>'.\n  \"\"\"\n  # TODO(gram): add support for wildchars and multiple arguments at some point. The latter is\n  # easy, the former a bit more tricky if non-default projects are involved.\n  if args['command'] == 'dataset':\n    try:\n      datalab.bigquery.Dataset(args['name']).delete()\n    except Exception as e:\n      print('Failed to delete dataset %s: %s' % (args['name'], e))\n  else:\n    try:\n      datalab.bigquery.Table(args['name']).delete()\n    except Exception as e:\n      print('Failed to delete table %s: %s' % (args['name'], e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements the BigQuery cell magic used to dry run BQ queries.", "response": "def _dryrun_cell(args, cell_body):\n  \"\"\"Implements the BigQuery cell magic used to dry run BQ queries.\n\n   The supported syntax is:\n   %%bigquery dryrun [-q|--sql <query identifier>]\n   [<YAML or JSON cell_body or inline SQL>]\n\n  Args:\n    args: the argument following '%bigquery dryrun'.\n    cell_body: optional contents of the cell interpreted as YAML or JSON.\n  Returns:\n    The response wrapped in a DryRunStats object\n  \"\"\"\n  query = _get_query_argument(args, cell_body, datalab.utils.commands.notebook_environment())\n\n  if args['verbose']:\n    print(query.sql)\n  result = query.execute_dry_run(dialect=args['dialect'], billing_tier=args['billing'])\n  return datalab.bigquery._query_stats.QueryStats(total_bytes=result['totalBytesProcessed'],\n                                                  is_cached=result['cacheHit'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimplementing the bigquery_udf cell magic for ipython notebooks.", "response": "def _udf_cell(args, js):\n  \"\"\"Implements the bigquery_udf cell magic for ipython notebooks.\n\n  The supported syntax is:\n  %%bigquery udf --module <var>\n  <js function>\n\n  Args:\n    args: the optional arguments following '%%bigquery udf'.\n    js: the UDF declaration (inputs and outputs) and implementation in javascript.\n  Returns:\n    The results of executing the UDF converted to a dataframe if no variable\n    was specified. None otherwise.\n  \"\"\"\n  variable_name = args['module']\n  if not variable_name:\n    raise Exception('Declaration must be of the form %%bigquery udf --module <variable name>')\n\n  # Parse out the input and output specification\n  spec_pattern = r'\\{\\{([^}]+)\\}\\}'\n  spec_part_pattern = r'[a-z_][a-z0-9_]*'\n\n  specs = re.findall(spec_pattern, js)\n  if len(specs) < 2:\n    raise Exception('The JavaScript must declare the input row and output emitter parameters '\n                    'using valid jsdoc format comments.\\n'\n                    'The input row param declaration must be typed as {{field:type, field2:type}} '\n                    'and the output emitter param declaration must be typed as '\n                    'function({{field:type, field2:type}}.')\n\n  inputs = []\n  input_spec_parts = re.findall(spec_part_pattern, specs[0], flags=re.IGNORECASE)\n  if len(input_spec_parts) % 2 != 0:\n    raise Exception('Invalid input row param declaration. The jsdoc type expression must '\n                    'define an object with field and type pairs.')\n  for n, t in zip(input_spec_parts[0::2], input_spec_parts[1::2]):\n    inputs.append((n, t))\n\n  outputs = []\n  output_spec_parts = re.findall(spec_part_pattern, specs[1], flags=re.IGNORECASE)\n  if len(output_spec_parts) % 2 != 0:\n    raise Exception('Invalid output emitter param declaration. The jsdoc type expression must '\n                    'define a function accepting an an object with field and type pairs.')\n  for n, t in zip(output_spec_parts[0::2], output_spec_parts[1::2]):\n    outputs.append((n, t))\n\n  # Look for imports. We use a non-standard @import keyword; we could alternatively use @requires.\n  # Object names can contain any characters except \\r and \\n.\n  import_pattern = r'@import[\\s]+(gs://[a-z\\d][a-z\\d_\\.\\-]*[a-z\\d]/[^\\n\\r]+)'\n  imports = re.findall(import_pattern, js)\n\n  # Split the cell if necessary. We look for a 'function(' with no name and a header comment\n  # block with @param and assume this is the primary function, up to a closing '}' at the start\n  # of the line. The remaining cell content is used as support code.\n  split_pattern = r'(.*)(/\\*.*?@param.*?@param.*?\\*/\\w*\\n\\w*function\\w*\\(.*?^}\\n?)(.*)'\n  parts = re.match(split_pattern, js, re.MULTILINE | re.DOTALL)\n  support_code = ''\n  if parts:\n    support_code = (parts.group(1) + parts.group(3)).strip()\n    if len(support_code):\n      js = parts.group(2)\n\n  # Finally build the UDF object\n  udf = datalab.bigquery.UDF(inputs, outputs, variable_name, js, support_code, imports)\n  datalab.utils.commands.notebook_environment()[variable_name] = udf"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _execute_cell(args, cell_body):\n  query = _get_query_argument(args, cell_body, datalab.utils.commands.notebook_environment())\n  if args['verbose']:\n    print(query.sql)\n  return query.execute(args['target'], table_mode=args['mode'], use_cache=not args['nocache'],\n                       allow_large_results=args['large'], dialect=args['dialect'],\n                       billing_tier=args['billing']).results", "response": "Implements the BigQuery cell magic used to execute BigQuery queries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pipeline_cell(args, cell_body):\n  if args['action'] == 'deploy':\n    raise Exception('Deploying a pipeline is not yet supported')\n\n  env = {}\n  for key, value in datalab.utils.commands.notebook_environment().items():\n    if isinstance(value, datalab.bigquery._udf.UDF):\n      env[key] = value\n\n  query = _get_query_argument(args, cell_body, env)\n  if args['verbose']:\n    print(query.sql)\n  if args['action'] == 'dryrun':\n    print(query.sql)\n    result = query.execute_dry_run()\n    return datalab.bigquery._query_stats.QueryStats(total_bytes=result['totalBytesProcessed'],\n                                                    is_cached=result['cacheHit'])\n  if args['action'] == 'run':\n    return query.execute(args['target'], table_mode=args['mode'], use_cache=not args['nocache'],\n                         allow_large_results=args['large'], dialect=args['dialect'],\n                         billing_tier=args['billing']).results", "response": "Implements the BigQuery cell magic used to validate execute or deploy BQ pipelines."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimplement the BigQuery table magic used to display tables.", "response": "def _table_line(args):\n  \"\"\"Implements the BigQuery table magic used to display tables.\n\n   The supported syntax is:\n   %bigquery table -t|--table <name> <other args>\n\n  Args:\n    args: the arguments following '%bigquery table'.\n  Returns:\n    The HTML rendering for the table.\n  \"\"\"\n  # TODO(gram): It would be good to turn _table_viewer into a class that has a registered\n  # renderer. That would allow this to return a table viewer object which is easier to test.\n  name = args['table']\n  table = _get_table(name)\n  if table and table.exists():\n    fields = args['cols'].split(',') if args['cols'] else None\n    html = _table_viewer(table, rows_per_page=args['rows'], fields=fields)\n    return IPython.core.display.HTML(html)\n  else:\n    raise Exception('Table %s does not exist; cannot display' % name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a variable or table name get the Schema if it exists.", "response": "def _get_schema(name):\n  \"\"\" Given a variable or table name, get the Schema if it exists. \"\"\"\n  item = datalab.utils.commands.get_notebook_item(name)\n  if not item:\n    item = _get_table(name)\n\n  if isinstance(item, datalab.bigquery.Schema):\n    return item\n  if hasattr(item, 'schema') and isinstance(item.schema, datalab.bigquery._schema.Schema):\n    return item.schema\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _schema_line(args):\n  # TODO(gram): surely we could just return the schema itself?\n  name = args['table'] if args['table'] else args['view']\n  if name is None:\n    raise Exception('No table or view specified; cannot show schema')\n\n  schema = _get_schema(name)\n  if schema:\n    html = _repr_html_table_schema(schema)\n    return IPython.core.display.HTML(html)\n  else:\n    raise Exception('%s is not a schema and does not appear to have a schema member' % name)", "response": "Implements the BigQuery schema magic used to display table or view schemas.\n Returns the HTML rendering for the schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _render_table(data, fields=None):\n  return IPython.core.display.HTML(datalab.utils.commands.HtmlBuilder.render_table(data, fields))", "response": "Helper to render a list of dictionaries as an HTML display object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _datasets_line(args):\n  filter_ = args['filter'] if args['filter'] else '*'\n  return _render_list([str(dataset) for dataset in datalab.bigquery.Datasets(args['project'])\n                       if fnmatch.fnmatch(str(dataset), filter_)])", "response": "Implements the BigQuery datasets magic used to display datasets in a project."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimplements the BigQuery tables magic used to display tables in a dataset.", "response": "def _tables_line(args):\n  \"\"\"Implements the BigQuery tables magic used to display tables in a dataset.\n\n   The supported syntax is:\n\n       %bigquery tables -p|--project <project_id>  -d|--dataset <dataset_id>\n\n  Args:\n    args: the arguments following '%bigquery tables'.\n  Returns:\n    The HTML rendering for the list of tables.\n  \"\"\"\n  filter_ = args['filter'] if args['filter'] else '*'\n  if args['dataset']:\n    if args['project'] is None:\n      datasets = [datalab.bigquery.Dataset(args['dataset'])]\n    else:\n      datasets = [datalab.bigquery.Dataset((args['project'], args['dataset']))]\n  else:\n    datasets = datalab.bigquery.Datasets(args['project'])\n\n  tables = []\n  for dataset in datasets:\n    tables.extend([str(table) for table in dataset if fnmatch.fnmatch(str(table), filter_)])\n\n  return _render_list(tables)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_line(args):\n  name = args['source']\n  source = datalab.utils.commands.get_notebook_item(name)\n  if not source:\n    source = _get_table(name)\n\n  if not source:\n    raise Exception('No source named %s found' % name)\n  elif isinstance(source, datalab.bigquery.Table) and not source.exists():\n    raise Exception('Table %s does not exist' % name)\n  else:\n\n    job = source.extract(args['destination'],\n                         format='CSV' if args['format'] == 'csv' else 'NEWLINE_DELIMITED_JSON',\n                         compress=args['compress'],\n                         csv_delimiter=args['delimiter'],\n                         csv_header=args['header'])\n    if job.failed:\n      raise Exception('Extract failed: %s' % str(job.fatal_error))\n    elif job.errors:\n      raise Exception('Extract completed with errors: %s' % str(job.errors))", "response": "Implements the BigQuery extract magic used to extract data to GCS."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_cell(args, schema):\n  name = args['destination']\n  table = _get_table(name)\n  if not table:\n    table = datalab.bigquery.Table(name)\n\n  if table.exists():\n    if args['mode'] == 'create':\n      raise Exception('%s already exists; use --append or --overwrite' % name)\n  elif schema:\n    table.create(json.loads(schema))\n  elif not args['infer']:\n    raise Exception(\n        'Table does not exist, no schema specified in cell and no --infer flag; cannot load')\n\n  # TODO(gram): we should probably try do the schema infer ourselves as BQ doesn't really seem\n  # to be able to do it. Alternatively we can drop the --infer argument and force the user\n  # to use a pre-existing table or supply a JSON schema.\n  csv_options = datalab.bigquery.CSVOptions(delimiter=args['delimiter'],\n                                            skip_leading_rows=args['skip'],\n                                            allow_jagged_rows=not args['strict'],\n                                            quote=args['quote'])\n  job = table.load(args['source'],\n                   mode=args['mode'],\n                   source_format=('CSV' if args['format'] == 'csv' else 'NEWLINE_DELIMITED_JSON'),\n                   csv_options=csv_options,\n                   ignore_unknown_values=not args['strict'])\n  if job.failed:\n    raise Exception('Load failed: %s' % str(job.fatal_error))\n  elif job.errors:\n    raise Exception('Load completed with errors: %s' % str(job.errors))", "response": "Implements the BigQuery load magic used to load data from GCS to a table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the parser for the %bigquery magics.", "response": "def _create_bigquery_parser():\n  \"\"\" Create the parser for the %bigquery magics.\n\n  Note that because we use the func default handler dispatch mechanism of argparse,\n  our handlers can take only one argument which is the parsed args. So we must create closures\n  for the handlers that bind the cell contents and thus must recreate this parser for each\n  cell upon execution.\n  \"\"\"\n  parser = datalab.utils.commands.CommandParser(prog='bigquery', description=\"\"\"\nExecute various BigQuery-related operations. Use \"%bigquery <command> -h\"\nfor help on a specific command.\n  \"\"\")\n\n  # This is a bit kludgy because we want to handle some line magics and some cell magics\n  # with the bigquery command.\n\n  # %%bigquery sample\n  _add_command(parser, _create_sample_subparser, _sample_cell)\n\n  # %%bigquery create\n  _add_command(parser, _create_create_subparser, _create_cell)\n\n  # %%bigquery delete\n  _add_command(parser, _create_delete_subparser, _delete_cell)\n\n  # %%bigquery dryrun\n  _add_command(parser, _create_dry_run_subparser, _dryrun_cell)\n\n  # %%bigquery udf\n  _add_command(parser, _create_udf_subparser, _udf_cell, cell_required=True)\n\n  # %%bigquery execute\n  _add_command(parser, _create_execute_subparser, _execute_cell)\n\n  # %%bigquery pipeline\n  _add_command(parser, _create_pipeline_subparser, _pipeline_cell)\n\n  # %bigquery table\n  _add_command(parser, _create_table_subparser, _table_line, cell_prohibited=True)\n\n  # %bigquery schema\n  _add_command(parser, _create_schema_subparser, _schema_line, cell_prohibited=True)\n\n  # %bigquery datasets\n  _add_command(parser, _create_datasets_subparser, _datasets_line, cell_prohibited=True)\n\n  # %bigquery tables\n  _add_command(parser, _create_tables_subparser, _tables_line, cell_prohibited=True)\n\n  # % bigquery extract\n  _add_command(parser, _create_extract_subparser, _extract_line, cell_prohibited=True)\n\n  # %bigquery load\n  # TODO(gram): need some additional help, esp. around the option of specifying schema in\n  # cell body and how schema infer may fail.\n  _add_command(parser, _create_load_subparser, _load_cell)\n  return parser"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimplementing the bigquery cell magic for ipython notebooks.", "response": "def bigquery(line, cell=None):\n  \"\"\"Implements the bigquery cell magic for ipython notebooks.\n\n  The supported syntax is:\n\n    %%bigquery <command> [<args>]\n    <cell>\n\n  or:\n\n    %bigquery <command> [<args>]\n\n  Use %bigquery --help for a list of commands, or %bigquery <command> --help for help\n  on a specific command.\n  \"\"\"\n  namespace = {}\n  if line.find('$') >= 0:\n    # We likely have variables to expand; get the appropriate context.\n    namespace = datalab.utils.commands.notebook_environment()\n\n  return datalab.utils.commands.handle_magic_line(line, cell, _bigquery_parser, namespace=namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a QueryOutput object that represents a table.", "response": "def table(name=None, mode='create', use_cache=True, priority='interactive',\n            allow_large_results=False):\n    \"\"\" Construct a query output object where the result is a table\n\n    Args:\n      name: the result table name as a string or TableName; if None (the default), then a\n          temporary table will be used.\n      table_mode: one of 'create', 'overwrite' or 'append'. If 'create' (the default), the request\n          will fail if the table exists.\n      use_cache: whether to use past query results or ignore cache. Has no effect if destination is\n          specified (default True).\n      priority:one of 'batch' or 'interactive' (default). 'interactive' jobs should be scheduled\n          to run quickly but are subject to rate limits; 'batch' jobs could be delayed by as much\n          as three hours but are not rate-limited.\n      allow_large_results: whether to allow large results; i.e. compressed data over 100MB. This is\n          slower and requires a name to be specified) (default False).\n    \"\"\"\n    output = QueryOutput()\n    output._output_type = 'table'\n    output._table_name = name\n    output._table_mode = mode\n    output._use_cache = use_cache\n    output._priority = priority\n    output._allow_large_results = allow_large_results\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef file(path, format='csv', csv_delimiter=',', csv_header=True, compress=False,\n           use_cache=True):\n    \"\"\" Construct a query output object where the result is either a local file or a GCS path\n\n    Note that there are two jobs that may need to be run sequentially, one to run the query,\n    and the second to extract the resulting table. These are wrapped by a single outer Job.\n\n    If the query has already been executed and you would prefer to get a Job just for the\n    extract, you can can call extract[_async] on the QueryResultsTable returned by the query\n\n    Args:\n      path: the destination path. Can either be a local or GCS URI (starting with gs://)\n      format: the format to use for the exported data; one of 'csv', 'json', or 'avro'\n          (default 'csv').\n      csv_delimiter: for CSV exports, the field delimiter to use (default ',').\n      csv_header: for CSV exports, whether to include an initial header line (default True).\n      compress: whether to compress the data on export. Compression is not supported for\n          AVRO format (default False). Applies only to GCS URIs.\n      use_cache: whether to use cached results or not (default True).\n    \"\"\"\n    output = QueryOutput()\n    output._output_type = 'file'\n    output._file_path = path\n    output._file_format = format\n    output._csv_delimiter = csv_delimiter\n    output._csv_header = csv_header\n    output._compress_file = compress\n    return output", "response": "Construct a QueryOutput object that represents a local file or GCS URI."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct a query output object where the result is a dataframe.", "response": "def dataframe(start_row=0, max_rows=None, use_cache=True):\n    \"\"\" Construct a query output object where the result is a dataframe\n\n    Args:\n      start_row: the row of the table at which to start the export (default 0).\n      max_rows: an upper limit on the number of rows to export (default None).\n      use_cache: whether to use cached results or not (default True).\n    \"\"\"\n    output = QueryOutput()\n    output._output_type = 'dataframe'\n    output._dataframe_start_row = start_row\n    output._dataframe_max_rows = max_rows\n    output._use_cache = use_cache\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting running TensorBoard instances.", "response": "def list():\n    \"\"\"List running TensorBoard instances.\"\"\"\n\n    running_list = []\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--logdir')\n    parser.add_argument('--port')\n    for p in psutil.process_iter():\n      if p.name() != 'tensorboard' or p.status() == psutil.STATUS_ZOMBIE:\n        continue\n      cmd_args = p.cmdline()\n      del cmd_args[0:2]  # remove 'python' and 'tensorboard'\n      args = parser.parse_args(cmd_args)\n      running_list.append({'pid': p.pid, 'logdir': args.logdir, 'port': args.port})\n    return pd.DataFrame(running_list)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a TensorBoard instance.", "response": "def start(logdir):\n    \"\"\"Start a TensorBoard instance.\n\n    Args:\n      logdir: the logdir to run TensorBoard on.\n    Raises:\n      Exception if the instance cannot be started.\n    \"\"\"\n    if logdir.startswith('gs://'):\n      # Check user does have access. TensorBoard will start successfully regardless\n      # the user has read permissions or not so we check permissions here to\n      # give user alerts if needed.\n      datalab.storage._api.Api.verify_permitted_to_read(logdir)\n\n    port = datalab.utils.pick_unused_port()\n    args = ['tensorboard', '--logdir=' + logdir, '--port=' + str(port)]\n    p = subprocess.Popen(args)\n    retry = 10\n    while (retry > 0):\n      if datalab.utils.is_http_running_on(port):\n        basepath = os.environ.get('DATALAB_ENDPOINT_URL', '')\n        url = '%s/_proxy/%d/' % (basepath.rstrip('/'), port)\n        html = '<p>TensorBoard was started successfully with pid %d. ' % p.pid\n        html += 'Click <a href=\"%s\" target=\"_blank\">here</a> to access it.</p>' % url\n        IPython.display.display_html(html, raw=True)\n        return p.pid\n      time.sleep(1)\n      retry -= 1\n\n    raise Exception('Cannot start TensorBoard.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop(pid):\n    if psutil.pid_exists(pid):\n      try:\n        p = psutil.Process(pid)\n        p.kill()\n      except Exception:\n        pass", "response": "Shut down a specific process."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the core by building a wrapper around the Inception graph.", "response": "def build_graph(self):\n    \"\"\"Forms the core by building a wrapper around the inception graph.\n\n      Here we add the necessary input & output tensors, to decode jpegs,\n      serialize embeddings, restore from checkpoint etc.\n\n      To use other Inception models modify this file. Note that to use other\n      models beside Inception, you should make sure input_shape matches\n      their input. Resizing or other modifications may be necessary as well.\n      See tensorflow/contrib/slim/python/slim/nets/inception_v3.py for\n      details about InceptionV3.\n\n    Returns:\n      input_jpeg: A tensor containing raw image bytes as the input layer.\n      embedding: The embeddings tensor, that will be materialized later.\n    \"\"\"\n\n    import tensorflow as tf\n    input_jpeg = tf.placeholder(tf.string, shape=None)\n    image = tf.image.decode_jpeg(input_jpeg, channels=self.CHANNELS)\n\n    # Note resize expects a batch_size, but we are feeding a single image.\n    # So we have to expand then squeeze.  Resize returns float32 in the\n    # range [0, uint8_max]\n    image = tf.expand_dims(image, 0)\n\n    # convert_image_dtype also scales [0, uint8_max] -> [0 ,1).\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = tf.image.resize_bilinear(\n        image, [self.HEIGHT, self.WIDTH], align_corners=False)\n\n    # Then rescale range to [-1, 1) for Inception.\n    image = tf.subtract(image, 0.5)\n    inception_input = tf.multiply(image, 2.0)\n\n    # Build Inception layers, which expect a tensor of type float from [-1, 1)\n    # and shape [batch_size, height, width, channels].\n    with tf.contrib.slim.arg_scope(_inceptionlib.inception_v3_arg_scope()):\n      _, end_points = _inceptionlib.inception_v3(inception_input, is_training=False)\n\n    embedding = end_points['PreLogits']\n    return input_jpeg, embedding"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restore_from_checkpoint(self, checkpoint_path):\n    import tensorflow as tf\n    # Get all variables to restore. Exclude Logits and AuxLogits because they\n    # depend on the input data and we do not need to intialize them from\n    # checkpoint.\n    all_vars = tf.contrib.slim.get_variables_to_restore(\n        exclude=['InceptionV3/AuxLogits', 'InceptionV3/Logits', 'global_step'])\n\n    saver = tf.train.Saver(all_vars)\n    saver.restore(self.tf_session, checkpoint_path)", "response": "To restore inception model variables from the checkpoint file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the embeddings for a given JPEG image.", "response": "def calculate_embedding(self, batch_image_bytes):\n    \"\"\"Get the embeddings for a given JPEG image.\n\n    Args:\n      batch_image_bytes: As if returned from [ff.read() for ff in file_list].\n\n    Returns:\n      The Inception embeddings (bottleneck layer output)\n    \"\"\"\n    return self.tf_session.run(\n        self.embedding, feed_dict={self.input_jpeg: batch_image_bytes})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loss(logits, labels):\n  labels = tf.to_int64(labels)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=logits, labels=labels, name='xentropy')\n  return tf.reduce_mean(cross_entropy, name='xentropy_mean')", "response": "Calculates the loss from the logits and labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef training(loss_op):\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  with tf.name_scope('train'):\n    optimizer = tf.train.AdamOptimizer(epsilon=0.001)\n    train_op = optimizer.minimize(loss_op, global_step)\n    return train_op, global_step", "response": "Calculates the loss from the logits and labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_final_training_ops(self,\n                             embeddings,\n                             all_labels_count,\n                             bottleneck_tensor_size,\n                             hidden_layer_size=BOTTLENECK_TENSOR_SIZE / 4,\n                             dropout_keep_prob=None):\n    \"\"\"Adds a new softmax and fully-connected layer for training.\n\n     The set up for the softmax and fully-connected layers is based on:\n     https://tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\n\n     This function can be customized to add arbitrary layers for\n     application-specific requirements.\n    Args:\n      embeddings: The embedding (bottleneck) tensor.\n      all_labels_count: The number of all labels including the default label.\n      bottleneck_tensor_size: The number of embeddings.\n      hidden_layer_size: The size of the hidden_layer. Roughtly, 1/4 of the\n                         bottleneck tensor size.\n      dropout_keep_prob: the percentage of activation values that are retained.\n    Returns:\n      softmax: The softmax or tensor. It stores the final scores.\n      logits: The logits tensor.\n    \"\"\"\n    with tf.name_scope('input'):\n      bottleneck_input = tf.placeholder_with_default(\n          embeddings,\n          shape=[None, bottleneck_tensor_size],\n          name='ReshapeSqueezed')\n      bottleneck_with_no_gradient = tf.stop_gradient(bottleneck_input)\n\n      with tf.name_scope('Wx_plus_b'):\n        hidden = layers.fully_connected(bottleneck_with_no_gradient,\n                                        hidden_layer_size)\n        # We need a dropout when the size of the dataset is rather small.\n        if dropout_keep_prob:\n          hidden = tf.nn.dropout(hidden, dropout_keep_prob)\n        logits = layers.fully_connected(\n            hidden, all_labels_count, activation_fn=None)\n\n    softmax = tf.nn.softmax(logits, name='softmax')\n    return softmax, logits", "response": "Adds a softmax and fully - connected layer for training."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_inception_graph(self):\n    image_str_tensor = tf.placeholder(tf.string, shape=[None])\n\n    # The CloudML Prediction API always \"feeds\" the Tensorflow graph with\n    # dynamic batch sizes e.g. (?,).  decode_jpeg only processes scalar\n    # strings because it cannot guarantee a batch of images would have\n    # the same output size.  We use tf.map_fn to give decode_jpeg a scalar\n    # string from dynamic batches.\n    image = tf.map_fn(\n        _util.decode_and_resize, image_str_tensor, back_prop=False, dtype=tf.uint8)\n    # convert_image_dtype, also scales [0, uint8_max] -> [0 ,1).\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Then shift images to [-1, 1) for Inception.\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n\n    # Build Inception layers, which expect A tensor of type float from [-1, 1)\n    # and shape [batch_size, height, width, channels].\n    with slim.arg_scope(_inceptionlib.inception_v3_arg_scope()):\n      _, end_points = _inceptionlib.inception_v3(image, is_training=False)\n\n    inception_embeddings = end_points['PreLogits']\n    inception_embeddings = tf.squeeze(\n        inception_embeddings, [1, 2], name='SpatialSqueeze')\n    return image_str_tensor, inception_embeddings", "response": "Builds an inception graph and adds the necessary input & output tensors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_graph(self, data_paths, batch_size, graph_mod):\n    tensors = GraphReferences()\n    is_training = graph_mod == GraphMod.TRAIN\n    if data_paths:\n      _, tensors.examples = _util.read_examples(\n          data_paths,\n          batch_size,\n          shuffle=is_training,\n          num_epochs=None if is_training else 2)\n    else:\n      tensors.examples = tf.placeholder(tf.string, name='input', shape=(None,))\n\n    if graph_mod == GraphMod.PREDICT:\n      inception_input, inception_embeddings = self.build_inception_graph()\n      # Build the Inception graph. We later add final training layers\n      # to this graph. This is currently used only for prediction.\n      # For training, we use pre-processed data, so it is not needed.\n      embeddings = inception_embeddings\n      tensors.input_jpeg = inception_input\n    else:\n      # For training and evaluation we assume data is preprocessed, so the\n      # inputs are tf-examples.\n      # Generate placeholders for examples.\n      with tf.name_scope('inputs'):\n        feature_map = {\n            'image_uri':\n                tf.FixedLenFeature(\n                    shape=[], dtype=tf.string, default_value=['']),\n            # Some images may have no labels. For those, we assume a default\n            # label. So the number of labels is label_count+1 for the default\n            # label.\n            'label':\n                tf.FixedLenFeature(\n                    shape=[1], dtype=tf.int64,\n                    default_value=[len(self.labels)]),\n            'embedding':\n                tf.FixedLenFeature(\n                    shape=[BOTTLENECK_TENSOR_SIZE], dtype=tf.float32)\n        }\n        parsed = tf.parse_example(tensors.examples, features=feature_map)\n        labels = tf.squeeze(parsed['label'])\n        uris = tf.squeeze(parsed['image_uri'])\n        embeddings = parsed['embedding']\n\n    # We assume a default label, so the total number of labels is equal to\n    # label_count+1.\n    all_labels_count = len(self.labels) + 1\n    with tf.name_scope('final_ops'):\n      softmax, logits = self.add_final_training_ops(\n          embeddings,\n          all_labels_count,\n          BOTTLENECK_TENSOR_SIZE,\n          dropout_keep_prob=self.dropout if is_training else None)\n\n    # Prediction is the index of the label with the highest score. We are\n    # interested only in the top score.\n    prediction = tf.argmax(softmax, 1)\n    tensors.predictions = [prediction, softmax, embeddings]\n\n    if graph_mod == GraphMod.PREDICT:\n      return tensors\n\n    with tf.name_scope('evaluate'):\n      loss_value = loss(logits, labels)\n\n    # Add to the Graph the Ops that calculate and apply gradients.\n    if is_training:\n      tensors.train, tensors.global_step = training(loss_value)\n    else:\n      tensors.global_step = tf.Variable(0, name='global_step', trainable=False)\n      tensors.uris = uris\n\n    # Add means across all batches.\n    loss_updates, loss_op = _util.loss(loss_value)\n    accuracy_updates, accuracy_op = _util.accuracy(logits, labels)\n\n    if not is_training:\n      tf.summary.scalar('accuracy', accuracy_op)\n      tf.summary.scalar('loss', loss_op)\n\n    tensors.metric_updates = loss_updates + accuracy_updates\n    tensors.metric_values = [loss_op, accuracy_op]\n    return tensors", "response": "Builds a generic graph for training or evaluation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild prediction graph and registers appropriate endpoints.", "response": "def build_prediction_graph(self):\n    \"\"\"Builds prediction graph and registers appropriate endpoints.\"\"\"\n\n    tensors = self.build_graph(None, 1, GraphMod.PREDICT)\n\n    keys_placeholder = tf.placeholder(tf.string, shape=[None])\n    inputs = {\n        'key': keys_placeholder,\n        'image_bytes': tensors.input_jpeg\n    }\n\n    # To extract the id, we need to add the identity function.\n    keys = tf.identity(keys_placeholder)\n    labels = self.labels + ['UNKNOWN']\n    labels_tensor = tf.constant(labels)\n    labels_table = tf.contrib.lookup.index_to_string_table_from_tensor(mapping=labels_tensor)\n    predicted_label = labels_table.lookup(tensors.predictions[0])\n\n    # Need to duplicate the labels by num_of_instances so the output is one batch\n    # (all output members share the same outer dimension).\n    # The labels are needed for client to match class scores list.\n    labels_tensor = tf.expand_dims(tf.constant(labels), 0)\n    num_instance = tf.shape(keys)\n    labels_tensors_n = tf.tile(labels_tensor, tf.concat(axis=0, values=[num_instance, [1]]))\n\n    outputs = {\n        'key': keys,\n        'prediction': predicted_label,\n        'labels': labels_tensors_n,\n        'scores': tensors.predictions[1],\n    }\n    return inputs, outputs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export(self, last_checkpoint, output_dir):\n    logging.info('Exporting prediction graph to %s', output_dir)\n    with tf.Session(graph=tf.Graph()) as sess:\n      # Build and save prediction meta graph and trained variable values.\n      inputs, outputs = self.build_prediction_graph()\n      signature_def_map = {\n        'serving_default': signature_def_utils.predict_signature_def(inputs, outputs)\n      }\n      init_op = tf.global_variables_initializer()\n      sess.run(init_op)\n      self.restore_from_checkpoint(sess, self.inception_checkpoint_file,\n                                   last_checkpoint)\n      init_op_serving = control_flow_ops.group(\n          variables.local_variables_initializer(),\n          tf.tables_initializer())\n\n      builder = saved_model_builder.SavedModelBuilder(output_dir)\n      builder.add_meta_graph_and_variables(\n          sess, [tag_constants.SERVING],\n          signature_def_map=signature_def_map,\n          legacy_init_op=init_op_serving)\n      builder.save(False)", "response": "Builds a prediction graph and xports the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats the metric values for logging purposes.", "response": "def format_metric_values(self, metric_values):\n    \"\"\"Formats metric values - used for logging purpose.\"\"\"\n\n    # Early in training, metric_values may actually be None.\n    loss_str = 'N/A'\n    accuracy_str = 'N/A'\n    try:\n      loss_str = 'loss: %.3f' % metric_values[0]\n      accuracy_str = 'accuracy: %.3f' % metric_values[1]\n    except (TypeError, IndexError):\n      pass\n\n    return '%s, %s' % (loss_str, accuracy_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef package_and_copy(package_root_dir, setup_py, output_tar_path):\n  if not output_tar_path.startswith('gs://'):\n    raise ValueError('output_tar_path needs to be a GCS path.')\n  if not os.path.isfile(setup_py):\n    raise ValueError('Supplied file \"%s\" does not exist.' % setup_py)\n\n  dest_setup_py = os.path.join(package_root_dir, 'setup.py')\n  if dest_setup_py != setup_py:\n    # setuptools requires a \"setup.py\" in the current dir, so copy setup.py there.\n    # Also check if there is an existing setup.py. If so, back it up.\n    if os.path.isfile(dest_setup_py):\n      os.rename(dest_setup_py, dest_setup_py + '._bak_')\n    shutil.copyfile(setup_py, dest_setup_py)\n\n  tempdir = tempfile.mkdtemp()\n  previous_cwd = os.getcwd()\n  os.chdir(package_root_dir)\n  try:\n    # Repackage.\n    sdist = ['python', dest_setup_py, 'sdist', '--format=gztar', '-d', tempdir]\n    subprocess.check_call(sdist)\n\n    # Copy to GCS.\n    source = os.path.join(tempdir, '*.tar.gz')\n    gscopy = ['gsutil', 'cp', source, output_tar_path]\n    subprocess.check_call(gscopy)\n    return\n  finally:\n    os.chdir(previous_cwd)\n    if dest_setup_py != setup_py:\n      os.remove(dest_setup_py)\n    if os.path.isfile(dest_setup_py + '._bak_'):\n      os.rename(dest_setup_py + '._bak_', dest_setup_py)\n    shutil.rmtree(tempdir)", "response": "Repackage an existing CloudML package and copy it to a staging dir."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_file_to_string(path):\n  bytes_string = tf.gfile.Open(path, 'r').read()\n  return dlutils.python_portable_string(bytes_string)", "response": "Read a file into a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a SQL module with one or more queries.", "response": "def sql(line, cell=None):\n  \"\"\" Create a SQL module with one or more queries. Use %sql --help for more details.\n\n  The supported syntax is:\n\n  %%sql [--module <modulename>]\n  [<optional Python code for default argument values>]\n  [<optional named queries>]\n  [<optional unnamed query>]\n\n  At least one query should be present. Named queries should start with:\n\n    DEFINE QUERY <name>\n\n  on a line by itself.\n\n  Args:\n  args: the optional arguments following '%%sql'.\n  cell: the contents of the cell; Python code for arguments followed by SQL queries.\n   \"\"\"\n  if cell is None:\n    _sql_parser.print_help()\n  else:\n    return handle_magic_line(line, cell, _sql_parser)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _date(val, offset=None):\n  if val is None:\n    return val\n  if val == '' or val == 'now':\n    when = datetime.datetime.utcnow()\n  elif val == 'today':\n    dt = datetime.datetime.utcnow()\n    when = datetime.datetime(dt.year, dt.month, dt.day)\n  elif val == 'yesterday':\n    dt = datetime.datetime.utcnow() - datetime.timedelta(1)\n    when = datetime.datetime(dt.year, dt.month, dt.day)\n  else:\n    when = datetime.datetime.strptime(val, \"%Y%m%d\")\n  if offset is not None:\n    for part in offset.split(','):\n      unit = part[-1]\n      quantity = int(part[:-1])\n      # We can use timedelta for days and under, but not for years and months\n      if unit == 'y':\n        when = datetime.datetime(year=when.year + quantity, month=when.month, day=when.day,\n                                 hour=when.hour, minute=when.minute)\n      elif unit == 'm':\n        new_year = when.year\n        new_month = when.month + quantity\n        if new_month < 1:\n          new_month = -new_month\n          new_year += 1 + (new_month // 12)\n          new_month = 12 - new_month % 12\n        elif new_month > 12:\n          new_year += (new_month - 1) // 12\n          new_month = 1 + (new_month - 1) % 12\n        when = datetime.datetime(year=new_year, month=new_month, day=when.day,\n                                 hour=when.hour, minute=when.minute)\n      elif unit == 'd':\n        when += datetime.timedelta(days=quantity)\n      elif unit == 'h':\n        when += datetime.timedelta(hours=quantity)\n      elif unit == 'M':\n        when += datetime.timedelta(minutes=quantity)\n\n  return when", "response": "A function to parse a date string into a sequence of deltas."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_table_formatter(f, offset=None):\n  format = f\n  delta = offset\n  return lambda v: _resolve_table(v, format, delta)", "response": "A closure -izer for table arguments that include a format and possibly an offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _arguments(code, module):\n  arg_parser = CommandParser.create('')\n  try:\n    # Define our special argument 'types' and add them to the environment.\n    builtins = {'source': _table, 'datestring': _datestring}\n    env = {}\n    env.update(builtins)\n\n    # Execute the cell which should be one or more calls to arg().\n    exec(code, env)\n\n    # Iterate through the module dictionary. For any newly defined objects,\n    # add args to the parser.\n    for key in env:\n\n      # Skip internal/private stuff.\n      if key in builtins or key[0] == '_':\n        continue\n      # If we want to support importing query modules into other query modules, uncomment next 4\n      # Skip imports but add them to the module\n      # if isinstance(env[key], types.ModuleType):\n      #   module.__dict__[key] = env[key]\n      #   continue\n\n      val = env[key]\n      key = '--%s' % key\n\n      if isinstance(val, bool):\n        if val:\n          arg_parser.add_argument(key, default=val, action='store_true')\n        else:\n          arg_parser.add_argument(key, default=val, action='store_false')\n      elif isinstance(val, basestring) or isinstance(val, int) or isinstance(val, float) \\\n              or isinstance(val, int):\n        arg_parser.add_argument(key, default=val)\n      elif isinstance(val, list):\n        arg_parser.add_argument(key, default=val, nargs='+')\n      elif isinstance(val, tuple):\n        arg_parser.add_argument(key, default=list(val), nargs='+')\n\n      # Is this one of our pseudo-types for dates/tables?\n      elif isinstance(val, dict) and 'type' in val:\n        if val['type'] == 'datestring':\n          arg_parser.add_argument(key, default='',\n                                  type=_make_string_formatter(val['format'],\n                                                              offset=val['offset']))\n        elif val['type'] == 'table':\n          if val['format'] is not None:\n            arg_parser.add_argument(key, default='',\n                                    type=_make_table_formatter(val['format'],\n                                                               offset=val['offset']))\n          else:\n            arg_parser.add_argument(key, default=val['name'], type=_make_table)\n        else:\n          raise Exception('Cannot generate argument for %s of type %s' % (key, type(val)))\n      else:\n        raise Exception('Cannot generate argument for %s of type %s' % (key, type(val)))\n\n  except Exception as e:\n    print(\"%%sql arguments: %s from code '%s'\" % (str(e), str(code)))\n  return arg_parser", "response": "Define pipeline arguments.\n\n  Args:\n    code: the Python code to execute that defines the arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _split_cell(cell, module):\n  lines = cell.split('\\n')\n  code = None\n  last_def = -1\n  name = None\n  define_wild_re = re.compile('^DEFINE\\s+.*$', re.IGNORECASE)\n  define_re = re.compile('^DEFINE\\s+QUERY\\s+([A-Z]\\w*)\\s*?(.*)$', re.IGNORECASE)\n  select_re = re.compile('^SELECT\\s*.*$', re.IGNORECASE)\n  standard_sql_re = re.compile('^(CREATE|WITH|INSERT|DELETE|UPDATE)\\s*.*$', re.IGNORECASE)\n  # TODO(gram): a potential issue with this code is if we have leading Python code followed\n  # by a SQL-style comment before we see SELECT/DEFINE. When switching to the tokenizer see\n  # if we can address this.\n  for i, line in enumerate(lines):\n    define_match = define_re.match(line)\n    select_match = select_re.match(line)\n    standard_sql_match = standard_sql_re.match(line)\n\n    if i:\n      prior_content = ''.join(lines[:i]).strip()\n      if select_match:\n        # Avoid matching if previous token was '(' or if Standard SQL is found\n        # TODO: handle the possibility of comments immediately preceding SELECT\n        select_match = len(prior_content) == 0 or \\\n            (prior_content[-1] != '(' and not standard_sql_re.match(prior_content))\n      if standard_sql_match:\n        standard_sql_match = len(prior_content) == 0 or not standard_sql_re.match(prior_content)\n\n    if define_match or select_match or standard_sql_match:\n      # If this is the first query, get the preceding Python code.\n      if code is None:\n        code = ('\\n'.join(lines[:i])).strip()\n        if len(code):\n          code += '\\n'\n      elif last_def >= 0:\n\n        # This is not the first query, so gather the previous query text.\n        query = '\\n'.join([line for line in lines[last_def:i] if len(line)]).strip()\n        if select_match and name != datalab.data._utils._SQL_MODULE_MAIN and len(query) == 0:\n          # Avoid DEFINE query name\\nSELECT ... being seen as an empty DEFINE followed by SELECT\n          continue\n\n        # Save the query\n        statement = datalab.data.SqlStatement(query, module)\n        module.__dict__[name] = statement\n        # And set the 'last' query to be this too\n        module.__dict__[datalab.data._utils._SQL_MODULE_LAST] = statement\n\n      # Get the query name and strip off our syntactic sugar if appropriate.\n      if define_match:\n        name = define_match.group(1)\n        lines[i] = define_match.group(2)\n      else:\n        name = datalab.data._utils._SQL_MODULE_MAIN\n\n      # Save the starting line index of the new query\n      last_def = i\n    else:\n      define_wild_match = define_wild_re.match(line)\n      if define_wild_match:\n        raise Exception('Expected \"DEFINE QUERY <name>\"')\n\n  if last_def >= 0:\n    # We were in a query so save this tail query.\n    query = '\\n'.join([line for line in lines[last_def:] if len(line)]).strip()\n    statement = datalab.data.SqlStatement(query, module)\n    module.__dict__[name] = statement\n    module.__dict__[datalab.data._utils._SQL_MODULE_LAST] = statement\n\n  if code is None:\n    code = ''\n  module.__dict__[datalab.data._utils._SQL_MODULE_ARGPARSE] = _arguments(code, module)\n  return module.__dict__.get(datalab.data._utils._SQL_MODULE_LAST, None)", "response": "Splits a hybrid %%sql cell into the Python code and the queries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimplement the SQL cell magic for ipython notebooks.", "response": "def sql_cell(args, cell):\n  \"\"\"Implements the SQL cell magic for ipython notebooks.\n\n  The supported syntax is:\n\n      %%sql [--module <modulename>]\n      [<optional Python code for default argument values>]\n      [<optional named queries>]\n      [<optional unnamed query>]\n\n  At least one query should be present. Named queries should start with:\n\n      DEFINE QUERY <name>\n\n  on a line by itself.\n\n  Args:\n    args: the optional arguments following '%%sql'.\n    cell: the contents of the cell; Python code for arguments followed by SQL queries.\n  \"\"\"\n  name = args['module'] if args['module'] else '_sql_cell'\n  module = imp.new_module(name)\n  query = _split_cell(cell, module)\n  ipy = IPython.get_ipython()\n  if not args['module']:\n      # Execute now\n      if query:\n        return datalab.bigquery.Query(query, values=ipy.user_ns) \\\n          .execute(dialect=args['dialect'], billing_tier=args['billing']).results\n  else:\n    # Add it as a module\n    sys.modules[name] = module\n    exec('import %s' % name, ipy.user_ns)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a function to read the input features from the given data paths.", "response": "def get_reader_input_fn(train_config, preprocess_output_dir, model_type,\n                        data_paths, batch_size, shuffle, num_epochs=None):\n  \"\"\"Builds input layer for training.\"\"\"\n\n  def get_input_features():\n    \"\"\"Read the input features from the given data paths.\"\"\"\n    _, examples = util.read_examples(\n        input_files=data_paths,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_epochs=num_epochs)\n    features = util.parse_example_tensor(examples=examples,\n                                         train_config=train_config,\n                                         keep_target=True)\n\n    target_name = train_config['target_column']\n    target = features.pop(target_name)\n    features, target = util.preprocess_input(\n        features=features,\n        target=target,\n        train_config=train_config,\n        preprocess_output_dir=preprocess_output_dir,\n        model_type=model_type)\n\n    return features, target\n\n  # Return a function to input the feaures into the model from a data path.\n  return get_input_features"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_experiment_fn(args):\n\n  def get_experiment(output_dir):\n    # Merge schema, input features, and transforms.\n    train_config = util.merge_metadata(args.preprocess_output_dir,\n                                       args.transforms_file)\n\n    # Get the model to train.\n    estimator = util.get_estimator(output_dir, train_config, args)\n\n    # Save a copy of the scehma and input to the model folder.\n    schema_file = os.path.join(args.preprocess_output_dir, util.SCHEMA_FILE)\n\n    # Make list of files to save with the trained model.\n    additional_assets = {'features.json': args.transforms_file,\n                         util.SCHEMA_FILE: schema_file}\n    if util.is_classification_model(args.model_type):\n      target_name = train_config['target_column']\n      vocab_file_name = util.CATEGORICAL_ANALYSIS % target_name\n      vocab_file_path = os.path.join(\n          args.preprocess_output_dir, vocab_file_name)\n      assert file_io.file_exists(vocab_file_path)\n      additional_assets[vocab_file_name] = vocab_file_path\n\n    export_strategy_target = util.make_export_strategy(\n        train_config=train_config,\n        args=args,\n        keep_target=True,\n        assets_extra=additional_assets)\n    export_strategy_notarget = util.make_export_strategy(\n        train_config=train_config,\n        args=args,\n        keep_target=False,\n        assets_extra=additional_assets)\n\n    input_reader_for_train = get_reader_input_fn(\n        train_config=train_config,\n        preprocess_output_dir=args.preprocess_output_dir,\n        model_type=args.model_type,\n        data_paths=args.train_data_paths,\n        batch_size=args.train_batch_size,\n        shuffle=True,\n        num_epochs=args.num_epochs)\n\n    input_reader_for_eval = get_reader_input_fn(\n        train_config=train_config,\n        preprocess_output_dir=args.preprocess_output_dir,\n        model_type=args.model_type,\n        data_paths=args.eval_data_paths,\n        batch_size=args.eval_batch_size,\n        shuffle=False,\n        num_epochs=1)\n\n    return tf.contrib.learn.Experiment(\n        estimator=estimator,\n        train_input_fn=input_reader_for_train,\n        eval_input_fn=input_reader_for_eval,\n        train_steps=args.max_steps,\n        export_strategies=[export_strategy_target, export_strategy_notarget],\n        min_eval_frequency=args.min_eval_frequency,\n        eval_steps=None,\n    )\n\n  # Return a function to create an Experiment.\n  return get_experiment", "response": "Builds the experiment function for learn_runner. run."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_arguments(argv):\n  parser = argparse.ArgumentParser(\n      description=('Train a regression or classification model. Note that if '\n                   'using a DNN model, --layer-size1=NUM, --layer-size2=NUM, '\n                   'should be used. '))\n\n  # I/O file parameters\n  parser.add_argument('--train-data-paths', type=str, action='append',\n                      required=True)\n  parser.add_argument('--eval-data-paths', type=str, action='append',\n                      required=True)\n  parser.add_argument('--job-dir', type=str, required=True)\n  parser.add_argument('--preprocess-output-dir',\n                      type=str,\n                      required=True,\n                      help=('Output folder of preprocessing. Should contain the'\n                            ' schema file, and numerical stats and vocab files.'\n                            ' Path must be on GCS if running'\n                            ' cloud training.'))\n  parser.add_argument('--transforms-file',\n                      type=str,\n                      required=True,\n                      help=('File describing the the transforms to apply on '\n                            'each column'))\n\n  # HP parameters\n  parser.add_argument('--learning-rate', type=float, default=0.01,\n                      help='tf.train.AdamOptimizer learning rate')\n  parser.add_argument('--epsilon', type=float, default=0.0005,\n                      help='tf.train.AdamOptimizer epsilon')\n  # --layer_size See below\n\n  # Model problems\n  parser.add_argument('--model-type',\n                      choices=['linear_classification', 'linear_regression',\n                               'dnn_classification', 'dnn_regression'],\n                      required=True)\n  parser.add_argument('--top-n',\n                      type=int,\n                      default=1,\n                      help=('For classification problems, the output graph '\n                            'will contain the labels and scores for the top '\n                            'n classes.'))\n  # Training input parameters\n  parser.add_argument('--max-steps', type=int, default=5000,\n                      help='Maximum number of training steps to perform.')\n  parser.add_argument('--num-epochs',\n                      type=int,\n                      help=('Maximum number of training data epochs on which '\n                            'to train. If both --max-steps and --num-epochs '\n                            'are specified, the training job will run for '\n                            '--max-steps or --num-epochs, whichever occurs '\n                            'first. If unspecified will run for --max-steps.'))\n  parser.add_argument('--train-batch-size', type=int, default=1000)\n  parser.add_argument('--eval-batch-size', type=int, default=1000)\n  parser.add_argument('--min-eval-frequency', type=int, default=100,\n                      help=('Minimum number of training steps between '\n                            'evaluations'))\n\n  # other parameters\n  parser.add_argument('--save-checkpoints-secs', type=int, default=600,\n                      help=('How often the model should be checkpointed/saved '\n                            'in seconds'))\n\n  args, remaining_args = parser.parse_known_args(args=argv[1:])\n\n  # All HP parambeters must be unique, so we need to support an unknown number\n  # of --layer_size1=10 --layer_size2=10 ...\n  # Look at remaining_args for layer_size\\d+ to get the layer info.\n\n  # Get number of layers\n  pattern = re.compile('layer-size(\\d+)')\n  num_layers = 0\n  for other_arg in remaining_args:\n    match = re.search(pattern, other_arg)\n    if match:\n      num_layers = max(num_layers, int(match.group(1)))\n\n  # Build a new parser so we catch unknown args and missing layer_sizes.\n  parser = argparse.ArgumentParser()\n  for i in range(num_layers):\n    parser.add_argument('--layer-size%s' % str(i + 1), type=int, required=True)\n\n  layer_args = vars(parser.parse_args(args=remaining_args))\n  layer_sizes = []\n  for i in range(num_layers):\n    key = 'layer_size%s' % str(i + 1)\n    layer_sizes.append(layer_args[key])\n\n  assert len(layer_sizes) == num_layers\n  args.layer_sizes = layer_sizes\n\n  return args", "response": "Parse the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(argv=None):\n  args = parse_arguments(sys.argv if argv is None else argv)\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  learn_runner.run(\n      experiment_fn=get_experiment_fn(args),\n      output_dir=args.job_dir)", "response": "Run a Tensorflow model on the Iris dataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sd(line, cell=None):\n  parser = google.datalab.utils.commands.CommandParser(prog='%sd', description=(\n      'Execute various Stackdriver related operations. Use \"%sd '\n      '<stackdriver_product> -h\" for help on a specific Stackdriver product.'))\n\n  # %%sd monitoring\n  _create_monitoring_subparser(parser)\n  return google.datalab.utils.commands.handle_magic_line(line, cell, parser)", "response": "Implements the stackdriver cell magic for ipython notebooks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the metric descriptors in the project.", "response": "def _monitoring_metrics_list(args, _):\n  \"\"\"Lists the metric descriptors in the project.\"\"\"\n  project_id = args['project']\n  pattern = args['type'] or '*'\n  descriptors = gcm.MetricDescriptors(context=_make_context(project_id))\n  dataframe = descriptors.as_dataframe(pattern=pattern)\n  return _render_dataframe(dataframe)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _monitoring_resource_types_list(args, _):\n  project_id = args['project']\n  pattern = args['type'] or '*'\n  descriptors = gcm.ResourceDescriptors(context=_make_context(project_id))\n  dataframe = descriptors.as_dataframe(pattern=pattern)\n  return _render_dataframe(dataframe)", "response": "Lists the resource types in the project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist the groups in the project.", "response": "def _monitoring_groups_list(args, _):\n  \"\"\"Lists the groups in the project.\"\"\"\n  project_id = args['project']\n  pattern = args['name'] or '*'\n  groups = gcm.Groups(context=_make_context(project_id))\n  dataframe = groups.as_dataframe(pattern=pattern)\n  return _render_dataframe(dataframe)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the command line arguments.", "response": "def parse_arguments(argv):\n  \"\"\"Parse the command line arguments.\"\"\"\n  parser = DatalabParser(\n      epilog=('Note that if using a DNN model, --hidden-layer-size1=NUM, '\n              '--hidden-layer-size2=NUM, ..., is also required. '),\n      datalab_epilog=(\"\"\"\n  Note that if using a DNN model,\n  hidden-layer-size1: NUM\n  hidden-layer-size2: NUM\n  ...\n  is also required. \"\"\"))\n\n  # HP parameters\n  parser.add_argument(\n      '--epsilon', type=float, default=0.0005, metavar='R',\n      help='tf.train.AdamOptimizer epsilon. Only used in dnn models.')\n  parser.add_argument(\n      '--l1-regularization', type=float, default=0.0, metavar='R',\n      help='L1 term for linear models.')\n  parser.add_argument(\n      '--l2-regularization', type=float, default=0.0, metavar='R',\n      help='L2 term for linear models.')\n\n  # Model parameters\n  parser.add_argument(\n    '--model', required=True,\n    choices=['linear_classification', 'linear_regression', 'dnn_classification', 'dnn_regression'])\n  parser.add_argument(\n      '--top-n', type=int, default=0, metavar='N',\n      help=('For classification problems, the output graph will contain the '\n            'labels and scores for the top n classes, and results will be in the form of '\n            '\"predicted, predicted_2, ..., probability, probability_2, ...\". '\n            'If --top-n=0, then all labels and scores are returned in the form of '\n            '\"predicted, class_name1, class_name2,...\".'))\n\n  # HP parameters\n  parser.add_argument(\n      '--learning-rate', type=float, default=0.01, metavar='R',\n      help='optimizer learning rate.')\n\n  # Training input parameters\n  parser.add_argument(\n      '--max-steps', type=int, metavar='N',\n      help='Maximum number of training steps to perform. If unspecified, will '\n           'honor \"max-epochs\".')\n  parser.add_argument(\n      '--max-epochs', type=int, default=1000, metavar='N',\n      help='Maximum number of training data epochs on which to train. If '\n           'both \"max-steps\" and \"max-epochs\" are specified, the training '\n           'job will run for \"max-steps\" or \"num-epochs\", whichever occurs '\n           'first. If early stopping is enabled, training may also stop '\n           'earlier.')\n  parser.add_argument(\n      '--train-batch-size', type=int, default=64, metavar='N',\n      help='How many training examples are used per step. If num-epochs is '\n           'used, the last batch may not be full.')\n  parser.add_argument(\n      '--eval-batch-size', type=int, default=64, metavar='N',\n      help='Batch size during evaluation. Larger values increase performance '\n           'but also increase peak memory usgae on the master node. One pass '\n           'over the full eval set is performed per evaluation run.')\n  parser.add_argument(\n      '--min-eval-frequency', type=int, default=1000, metavar='N',\n      help='Minimum number of training steps between evaluations. Evaluation '\n           'does not occur if no new checkpoint is available, hence, this is '\n           'the minimum. If 0, the evaluation will only happen after training. ')\n  parser.add_argument(\n      '--early-stopping-num_evals', type=int, default=3,\n      help='Automatic training stop after results of specified number of evals '\n           'in a row show the model performance does not improve. Set to 0 to '\n           'disable early stopping.')\n  parser.add_argument(\n      '--logging-level', choices=['error', 'warning', 'info'],\n      help='The TF logging level. If absent, use info for cloud training '\n           'and warning for local training.')\n\n  args, remaining_args = parser.parse_known_args(args=argv[1:])\n\n  # All HP parambeters must be unique, so we need to support an unknown number\n  # of --hidden-layer-size1=10 --lhidden-layer-size2=10 ...\n  # Look at remaining_args for hidden-layer-size\\d+ to get the layer info.\n\n  # Get number of layers\n  pattern = re.compile('hidden-layer-size(\\d+)')\n  num_layers = 0\n  for other_arg in remaining_args:\n    match = re.search(pattern, other_arg)\n    if match:\n      if int(match.group(1)) <= 0:\n        raise ValueError('layer size must be a positive integer. Was given %s' % other_arg)\n      num_layers = max(num_layers, int(match.group(1)))\n\n  # Build a new parser so we catch unknown args and missing layer_sizes.\n  parser = argparse.ArgumentParser()\n  for i in range(num_layers):\n    parser.add_argument('--hidden-layer-size%s' % str(i + 1), type=int, required=True)\n\n  layer_args = vars(parser.parse_args(args=remaining_args))\n  hidden_layer_sizes = []\n  for i in range(num_layers):\n    key = 'hidden_layer_size%s' % str(i + 1)\n    hidden_layer_sizes.append(layer_args[key])\n\n  assert len(hidden_layer_sizes) == num_layers\n  args.hidden_layer_sizes = hidden_layer_sizes\n\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy the contents of src_dir into the folder dest_dir. Args: src_dir: gsc or local path. dest_dir: gcs or local path.", "response": "def recursive_copy(src_dir, dest_dir):\n  \"\"\"Copy the contents of src_dir into the folder dest_dir.\n  Args:\n    src_dir: gsc or local path.\n    dest_dir: gcs or local path.\n  \"\"\"\n\n  file_io.recursive_create_dir(dest_dir)\n  for file_name in file_io.list_directory(src_dir):\n    old_path = os.path.join(src_dir, file_name)\n    new_path = os.path.join(dest_dir, file_name)\n\n    if file_io.is_directory(old_path):\n      recursive_copy(old_path, new_path)\n    else:\n      file_io.copy(old_path, new_path, overwrite=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_prediction_output_tensors(args, features, input_ops, model_fn_ops,\n                                   keep_target):\n  \"\"\"Makes the final prediction output layer.\"\"\"\n  target_name = feature_transforms.get_target_name(features)\n  key_names = get_key_names(features)\n\n  outputs = {}\n  outputs.update({key_name: tf.squeeze(input_ops.features[key_name])\n                  for key_name in key_names})\n\n  if is_classification_model(args.model):\n\n    # build maps from ints to the origional categorical strings.\n    class_names = read_vocab(args, target_name)\n    table = tf.contrib.lookup.index_to_string_table_from_tensor(\n        mapping=class_names,\n        default_value='UNKNOWN')\n\n    # Get the label of the input target.\n    if keep_target:\n      input_target_label = table.lookup(input_ops.features[target_name])\n      outputs[PG_TARGET] = tf.squeeze(input_target_label)\n\n    # TODO(brandondutra): get the score of the target label too.\n    probabilities = model_fn_ops.predictions['probabilities']\n\n    # if top_n == 0, this means use all the classes. We will use class names as\n    # probabilities labels.\n    if args.top_n == 0:\n      predicted_index = tf.argmax(probabilities, axis=1)\n      predicted = table.lookup(predicted_index)\n      outputs.update({PG_CLASSIFICATION_FIRST_LABEL: predicted})\n      probabilities_list = tf.unstack(probabilities, axis=1)\n      for class_name, p in zip(class_names, probabilities_list):\n        outputs[class_name] = p\n    else:\n      top_n = args.top_n\n\n      # get top k labels and their scores.\n      (top_k_values, top_k_indices) = tf.nn.top_k(probabilities, k=top_n)\n      top_k_labels = table.lookup(tf.to_int64(top_k_indices))\n\n      # Write the top_k values using 2*top_n columns.\n      num_digits = int(math.ceil(math.log(top_n, 10)))\n      if num_digits == 0:\n        num_digits = 1\n      for i in range(0, top_n):\n        # Pad i based on the size of k. So if k = 100, i = 23 -> i = '023'. This\n        # makes sorting the columns easy.\n        padded_i = str(i + 1).zfill(num_digits)\n\n        if i == 0:\n          label_alias = PG_CLASSIFICATION_FIRST_LABEL\n        else:\n          label_alias = PG_CLASSIFICATION_LABEL_TEMPLATE % padded_i\n\n        label_tensor_name = (tf.squeeze(\n            tf.slice(top_k_labels, [0, i], [tf.shape(top_k_labels)[0], 1])))\n\n        if i == 0:\n          score_alias = PG_CLASSIFICATION_FIRST_SCORE\n        else:\n          score_alias = PG_CLASSIFICATION_SCORE_TEMPLATE % padded_i\n\n        score_tensor_name = (tf.squeeze(\n            tf.slice(top_k_values,\n                     [0, i],\n                     [tf.shape(top_k_values)[0], 1])))\n\n        outputs.update({label_alias: label_tensor_name,\n                        score_alias: score_tensor_name})\n\n  else:\n    if keep_target:\n      outputs[PG_TARGET] = tf.squeeze(input_ops.features[target_name])\n\n    scores = model_fn_ops.predictions['scores']\n    outputs[PG_REGRESSION_PREDICTED_TARGET] = tf.squeeze(scores)\n\n  return outputs", "response": "Makes the final prediction output layer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_export_strategy(\n        args,\n        keep_target,\n        assets_extra,\n        features,\n        schema,\n        stats):\n  \"\"\"Makes prediction graph that takes json input.\n\n  Args:\n    args: command line args\n    keep_target: If ture, target column is returned in prediction graph. Target\n        column must also exist in input data\n    assets_extra: other fiels to copy to the output folder\n    job_dir: root job folder\n    features: features dict\n    schema: schema list\n    stats: stats dict\n  \"\"\"\n  target_name = feature_transforms.get_target_name(features)\n  csv_header = [col['name'] for col in schema]\n  if not keep_target:\n    csv_header.remove(target_name)\n\n  def export_fn(estimator, export_dir_base, checkpoint_path=None, eval_result=None):\n    with ops.Graph().as_default() as g:\n      contrib_variables.create_global_step(g)\n\n      input_ops = feature_transforms.build_csv_serving_tensors_for_training_step(\n          args.analysis, features, schema, stats, keep_target)\n      model_fn_ops = estimator._call_model_fn(input_ops.features,\n                                              None,\n                                              model_fn_lib.ModeKeys.INFER)\n      output_fetch_tensors = make_prediction_output_tensors(\n          args=args,\n          features=features,\n          input_ops=input_ops,\n          model_fn_ops=model_fn_ops,\n          keep_target=keep_target)\n\n      # Don't use signature_def_utils.predict_signature_def as that renames\n      # tensor names if there is only 1 input/output tensor!\n      signature_inputs = {key: tf.saved_model.utils.build_tensor_info(tensor)\n                          for key, tensor in six.iteritems(input_ops.default_inputs)}\n      signature_outputs = {key: tf.saved_model.utils.build_tensor_info(tensor)\n                           for key, tensor in six.iteritems(output_fetch_tensors)}\n      signature_def_map = {\n          'serving_default':\n              signature_def_utils.build_signature_def(\n                  signature_inputs,\n                  signature_outputs,\n                  tf.saved_model.signature_constants.PREDICT_METHOD_NAME)}\n\n      if not checkpoint_path:\n        # Locate the latest checkpoint\n        checkpoint_path = saver.latest_checkpoint(estimator._model_dir)\n      if not checkpoint_path:\n        raise ValueError(\"Couldn't find trained model at %s.\"\n                         % estimator._model_dir)\n\n      export_dir = saved_model_export_utils.get_timestamped_export_dir(\n          export_dir_base)\n\n      if (model_fn_ops.scaffold is not None and\n         model_fn_ops.scaffold.saver is not None):\n        saver_for_restore = model_fn_ops.scaffold.saver\n      else:\n        saver_for_restore = saver.Saver(sharded=True)\n\n      with tf_session.Session('') as session:\n        saver_for_restore.restore(session, checkpoint_path)\n        init_op = control_flow_ops.group(\n            variables.local_variables_initializer(),\n            resources.initialize_resources(resources.shared_resources()),\n            tf.tables_initializer())\n\n        # Perform the export\n        builder = saved_model_builder.SavedModelBuilder(export_dir)\n        builder.add_meta_graph_and_variables(\n            session, [tag_constants.SERVING],\n            signature_def_map=signature_def_map,\n            assets_collection=ops.get_collection(\n                ops.GraphKeys.ASSET_FILEPATHS),\n            legacy_init_op=init_op)\n        builder.save(False)\n\n      # Add the extra assets\n      if assets_extra:\n        assets_extra_path = os.path.join(compat.as_bytes(export_dir),\n                                         compat.as_bytes('assets.extra'))\n        for dest_relative, source in assets_extra.items():\n          dest_absolute = os.path.join(compat.as_bytes(assets_extra_path),\n                                       compat.as_bytes(dest_relative))\n          dest_path = os.path.dirname(dest_absolute)\n          file_io.recursive_create_dir(dest_path)\n          file_io.copy(source, dest_absolute)\n\n    # only keep the last 3 models\n    saved_model_export_utils.garbage_collect_exports(\n        export_dir_base,\n        exports_to_keep=3)\n\n    # save the last model to the model folder.\n    # export_dir_base = A/B/intermediate_models/\n    if keep_target:\n      final_dir = os.path.join(args.job_dir, 'evaluation_model')\n    else:\n      final_dir = os.path.join(args.job_dir, 'model')\n    if file_io.is_directory(final_dir):\n      file_io.delete_recursively(final_dir)\n    file_io.recursive_create_dir(final_dir)\n    recursive_copy(export_dir, final_dir)\n\n    return export_dir\n\n  if keep_target:\n    intermediate_dir = 'intermediate_evaluation_models'\n  else:\n    intermediate_dir = 'intermediate_prediction_models'\n\n  return export_strategy.ExportStrategy(intermediate_dir, export_fn)", "response": "Makes prediction graph that takes json input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_vocab(args, column_name):\n  vocab_path = os.path.join(args.analysis,\n                            feature_transforms.VOCAB_ANALYSIS_FILE % column_name)\n\n  if not file_io.file_exists(vocab_path):\n    return []\n\n  vocab, _ = feature_transforms.read_vocab_file(vocab_path)\n  return vocab", "response": "Reads a vocab file if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_experiment_fn(args):\n\n  def get_experiment(output_dir):\n    # Read schema, input features, and transforms.\n    schema_path_with_target = os.path.join(args.analysis,\n                                           feature_transforms.SCHEMA_FILE)\n    features_path = os.path.join(args.analysis,\n                                 feature_transforms.FEATURES_FILE)\n    stats_path = os.path.join(args.analysis,\n                              feature_transforms.STATS_FILE)\n\n    schema = read_json_file(schema_path_with_target)\n    features = read_json_file(features_path)\n    stats = read_json_file(stats_path)\n\n    target_column_name = feature_transforms.get_target_name(features)\n    if not target_column_name:\n      raise ValueError('target missing from features file.')\n\n    # Make a copy of the schema file without the target column.\n    schema_without_target = [col for col in schema if col['name'] != target_column_name]\n    schema_path_without_target = os.path.join(args.job_dir, 'schema_without_target.json')\n    file_io.recursive_create_dir(args.job_dir)\n    file_io.write_string_to_file(schema_path_without_target,\n                                 json.dumps(schema_without_target, indent=2))\n\n    # Make list of files to save with the trained model.\n    additional_assets_with_target = {\n        feature_transforms.FEATURES_FILE: features_path,\n        feature_transforms.SCHEMA_FILE: schema_path_with_target}\n    additional_assets_without_target = {\n        feature_transforms.FEATURES_FILE: features_path,\n        feature_transforms.SCHEMA_FILE: schema_path_without_target}\n\n    # Get the model to train.\n    target_vocab = read_vocab(args, target_column_name)\n    estimator = get_estimator(args, output_dir, features, stats, len(target_vocab))\n\n    export_strategy_csv_notarget = make_export_strategy(\n        args=args,\n        keep_target=False,\n        assets_extra=additional_assets_without_target,\n        features=features,\n        schema=schema,\n        stats=stats)\n    export_strategy_csv_target = make_export_strategy(\n        args=args,\n        keep_target=True,\n        assets_extra=additional_assets_with_target,\n        features=features,\n        schema=schema,\n        stats=stats)\n\n    # Build readers for training.\n    if args.transform:\n      if any(v['transform'] == feature_transforms.IMAGE_TRANSFORM\n             for k, v in six.iteritems(features)):\n        raise ValueError('\"image_to_vec\" transform requires transformation step. ' +\n                         'Cannot train from raw data.')\n\n      input_reader_for_train = feature_transforms.build_csv_transforming_training_input_fn(\n          schema=schema,\n          features=features,\n          stats=stats,\n          analysis_output_dir=args.analysis,\n          raw_data_file_pattern=args.train,\n          training_batch_size=args.train_batch_size,\n          num_epochs=args.max_epochs,\n          randomize_input=True,\n          min_after_dequeue=10,\n          reader_num_threads=multiprocessing.cpu_count())\n      input_reader_for_eval = feature_transforms.build_csv_transforming_training_input_fn(\n          schema=schema,\n          features=features,\n          stats=stats,\n          analysis_output_dir=args.analysis,\n          raw_data_file_pattern=args.eval,\n          training_batch_size=args.eval_batch_size,\n          num_epochs=1,\n          randomize_input=False,\n          reader_num_threads=multiprocessing.cpu_count())\n    else:\n      input_reader_for_train = feature_transforms.build_tfexample_transfored_training_input_fn(\n          schema=schema,\n          features=features,\n          analysis_output_dir=args.analysis,\n          raw_data_file_pattern=args.train,\n          training_batch_size=args.train_batch_size,\n          num_epochs=args.max_epochs,\n          randomize_input=True,\n          min_after_dequeue=10,\n          reader_num_threads=multiprocessing.cpu_count())\n      input_reader_for_eval = feature_transforms.build_tfexample_transfored_training_input_fn(\n          schema=schema,\n          features=features,\n          analysis_output_dir=args.analysis,\n          raw_data_file_pattern=args.eval,\n          training_batch_size=args.eval_batch_size,\n          num_epochs=1,\n          randomize_input=False,\n          reader_num_threads=multiprocessing.cpu_count())\n\n    if args.early_stopping_num_evals == 0:\n      train_monitors = None\n    else:\n      if is_classification_model(args.model):\n        early_stop_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n            input_fn=input_reader_for_eval,\n            every_n_steps=args.min_eval_frequency,\n            early_stopping_rounds=(args.early_stopping_num_evals * args.min_eval_frequency),\n            early_stopping_metric='accuracy',\n            early_stopping_metric_minimize=False)\n      else:\n        early_stop_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n            input_fn=input_reader_for_eval,\n            every_n_steps=args.min_eval_frequency,\n            early_stopping_rounds=(args.early_stopping_num_evals * args.min_eval_frequency))\n      train_monitors = [early_stop_monitor]\n\n    return tf.contrib.learn.Experiment(\n        estimator=estimator,\n        train_input_fn=input_reader_for_train,\n        eval_input_fn=input_reader_for_eval,\n        train_steps=args.max_steps,\n        train_monitors=train_monitors,\n        export_strategies=[export_strategy_csv_notarget, export_strategy_csv_target],\n        min_eval_frequency=args.min_eval_frequency,\n        eval_steps=None)\n\n  # Return a function to create an Experiment.\n  return get_experiment", "response": "Builds the experiment function for learn_runner. run.\n"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_datalab_help_action(self):\n    datalab_help = self.datalab_help\n    epilog = self.datalab_epilog\n\n    class _CustomAction(argparse.Action):\n\n      def __init__(self, option_strings, dest, help=None):\n        super(_CustomAction, self).__init__(\n            option_strings=option_strings, dest=dest, nargs=0, help=help)\n\n      def __call__(self, parser, args, values, option_string=None):\n        print('\\n\\n'.join(datalab_help))\n        if epilog:\n          print(epilog)\n\n        # We have printed all help string datalab needs. If we don't quit, it will complain about\n        # missing required arguments later.\n        quit()\n    return _CustomAction", "response": "Return an argparse. Action that prints the datalab - help string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef browse(self, max_lines=None, headers=None):\n    if self.path.startswith('gs://'):\n      lines = CsvFile._read_gcs_lines(self.path, max_lines)\n    else:\n      lines = CsvFile._read_local_lines(self.path, max_lines)\n    if len(lines) == 0:\n      return pd.DataFrame(columns=headers)\n    columns_size = len(next(csv.reader([lines[0]], delimiter=self._delimiter)))\n    if headers is None:\n      headers = ['col' + newstr(e) for e in range(columns_size)]\n    if len(headers) != columns_size:\n      raise Exception('Number of columns in CSV do not match number of headers')\n    buf = StringIO()\n    for line in lines:\n      buf.write(line)\n      buf.write('\\n')\n    buf.seek(0)\n    df = pd.read_csv(buf, names=headers, delimiter=self._delimiter)\n    for key, col in df.iteritems():\n      if self._is_probably_categorical(col):\n        df[key] = df[key].astype('category')\n    return df", "response": "Try reading specified number of lines from the CSV file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets an item from a dictionary handling nested lookups with dotted notation.", "response": "def get_item(env, name, default=None):\n  \"\"\" Get an item from a dictionary, handling nested lookups with dotted notation.\n\n  Args:\n    env: the environment (dictionary) to use to look up the name.\n    name: the name to look up, in dotted notation.\n    default: the value to return if the name if not found.\n\n  Returns:\n    The result of looking up the name, if found; else the default.\n  \"\"\"\n  # TODO: handle attributes\n  for key in name.split('.'):\n    if isinstance(env, dict) and key in env:\n      env = env[key]\n    elif isinstance(env, types.ModuleType) and key in env.__dict__:\n      env = env.__dict__[key]\n    else:\n      return default\n  return env"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef configure_pipeline(p, dataset, model_dir, output_csv, output_bq_table):\n\n  data = _util.get_sources_from_dataset(p, dataset, 'predict')\n  if len(dataset.schema) == 2:\n    output_schema = [\n        {'name': 'image_url', 'type': 'STRING'},\n        {'name': 'target', 'type': 'STRING'},\n        {'name': 'predicted', 'type': 'STRING'},\n        {'name': 'target_prob', 'type': 'FLOAT'},\n        {'name': 'predicted_prob', 'type': 'FLOAT'},\n    ]\n  else:\n    output_schema = [\n        {'name': 'image_url', 'type': 'STRING'},\n        {'name': 'predicted', 'type': 'STRING'},\n        {'name': 'predicted_prob', 'type': 'FLOAT'},\n    ]\n  results = (data |\n             'Load Images' >> beam.ParDo(LoadImagesDoFn()) |\n             'Batch Inputs' >> beam.ParDo(EmitAsBatchDoFn(20)) |\n             'Batch Predict' >> beam.ParDo(PredictBatchDoFn(model_dir)) |\n             'Unbatch' >> beam.ParDo(UnbatchDoFn()) |\n             'Process Results' >> beam.ParDo(ProcessResultsDoFn()))\n\n  if output_csv is not None:\n    schema_file = output_csv + '.schema.json'\n    results_save = (results |\n                    'Prepare For Output' >> beam.ParDo(MakeCsvLineDoFn()) |\n                    'Write Csv Results' >> beam.io.textio.WriteToText(output_csv,\n                                                                      shard_name_template=''))\n    (results_save |\n     'Sample One' >> beam.transforms.combiners.Sample.FixedSizeGlobally(1) |\n     'Serialize Schema' >> beam.Map(lambda path: json.dumps(output_schema)) |\n     'Write Schema' >> beam.io.textio.WriteToText(schema_file, shard_name_template=''))\n\n  if output_bq_table is not None:\n    # BigQuery sink takes schema in the form of 'field1:type1,field2:type2...'\n    bq_schema_string = ','.join(x['name'] + ':' + x['type'] for x in output_schema)\n    sink = beam.io.BigQuerySink(output_bq_table, schema=bq_schema_string,\n                                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n    results | 'Write BQ Results' >> beam.io.Write(sink)", "response": "Configures a dataflow pipeline for batch prediction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a sampling Query for the SQL object.", "response": "def sampling_query(sql, context, fields=None, count=5, sampling=None, udfs=None,\n                     data_sources=None):\n    \"\"\"Returns a sampling Query for the SQL object.\n\n    Args:\n      sql: the SQL statement (string) or Query object to sample.\n      context: a Context object providing project_id and credentials.\n      fields: an optional list of field names to retrieve.\n      count: an optional count of rows to retrieve which is used if a specific\n          sampling is not specified.\n      sampling: an optional sampling strategy to apply to the table.\n      udfs: array of UDFs referenced in the SQL.\n      data_sources: dictionary of federated (external) tables referenced in the SQL.\n    Returns:\n      A Query object for sampling the table.\n    \"\"\"\n    return Query(_sampling.Sampling.sampling_query(sql, fields, count, sampling), context=context,\n                 udfs=udfs, data_sources=data_sources)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef results(self, use_cache=True, dialect=None, billing_tier=None):\n    if not use_cache or (self._results is None):\n      self.execute(use_cache=use_cache, dialect=dialect, billing_tier=billing_tier)\n    return self._results.results", "response": "Retrieves the table of results for the query."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract(self, storage_uris, format='csv', csv_delimiter=',', csv_header=True,\n              compress=False, use_cache=True, dialect=None, billing_tier=None):\n    \"\"\"Exports the query results to GCS.\n\n    Args:\n      storage_uris: the destination URI(s). Can be a single URI or a list.\n      format: the format to use for the exported data; one of 'csv', 'json', or 'avro'\n          (default 'csv').\n      csv_delimiter: for csv exports, the field delimiter to use (default ',').\n      csv_header: for csv exports, whether to include an initial header line (default True).\n      compress: whether to compress the data on export. Compression is not supported for\n          AVRO format (default False).\n      use_cache: whether to use cached results or not (default True).\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A Job object for the export Job if it was completed successfully; else None.\n    Raises:\n      An Exception if the query or extract failed.\n    \"\"\"\n    return self.results(use_cache=use_cache, dialect=dialect,\n                        billing_tier=billing_tier).extract(storage_uris, format=format,\n                                                           csv_delimiter=csv_delimiter,\n                                                           csv_header=csv_header,\n                                                           compress=compress)", "response": "Exports the query results to GCS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dataframe(self, start_row=0, max_rows=None, use_cache=True, dialect=None,\n                   billing_tier=None):\n    \"\"\" Exports the query results to a Pandas dataframe.\n\n    Args:\n      start_row: the row of the table at which to start the export (default 0).\n      max_rows: an upper limit on the number of rows to export (default None).\n      use_cache: whether to use cached results or not (default True).\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A Pandas dataframe containing the table data.\n    \"\"\"\n    return self.results(use_cache=use_cache, dialect=dialect, billing_tier=billing_tier) \\\n        .to_dataframe(start_row=start_row, max_rows=max_rows)", "response": "Exports the query results to a Pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_file(self, path, format='csv', csv_delimiter=',', csv_header=True, use_cache=True,\n              dialect=None, billing_tier=None):\n    \"\"\"Save the results to a local file in CSV format.\n\n    Args:\n      path: path on the local filesystem for the saved results.\n      format: the format to use for the exported data; currently only 'csv' is supported.\n      csv_delimiter: for CSV exports, the field delimiter to use. Defaults to ','\n      csv_header: for CSV exports, whether to include an initial header line. Default true.\n      use_cache: whether to use cached results or not.\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      The path to the local file.\n    Raises:\n      An Exception if the operation failed.\n    \"\"\"\n    self.results(use_cache=use_cache, dialect=dialect, billing_tier=billing_tier) \\\n        .to_file(path, format=format, csv_delimiter=csv_delimiter, csv_header=csv_header)\n    return path", "response": "Save the results to a local file in CSV format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a sampling of rows for the query.", "response": "def sample(self, count=5, fields=None, sampling=None, use_cache=True, dialect=None,\n             billing_tier=None):\n    \"\"\"Retrieves a sampling of rows for the query.\n\n    Args:\n      count: an optional count of rows to retrieve which is used if a specific\n          sampling is not specified (default 5).\n      fields: the list of fields to sample (default None implies all).\n      sampling: an optional sampling strategy to apply to the table.\n      use_cache: whether to use cached results or not (default True).\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A QueryResultsTable containing a sampling of the result set.\n    Raises:\n      Exception if the query could not be executed or query response was malformed.\n    \"\"\"\n    return Query.sampling_query(self._sql, self._context, count=count, fields=fields,\n                                sampling=sampling, udfs=self._udfs,\n                                data_sources=self._data_sources).results(use_cache=use_cache,\n                                                                         dialect=dialect,\n                                                                         billing_tier=billing_tier)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndry run a query and return some useful statistics.", "response": "def execute_dry_run(self, dialect=None, billing_tier=None):\n    \"\"\"Dry run a query, to check the validity of the query and return some useful statistics.\n\n    Args:\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A dict with 'cacheHit' and 'totalBytesProcessed' fields.\n    Raises:\n      An exception if the query was malformed.\n    \"\"\"\n    try:\n      query_result = self._api.jobs_insert_query(self._sql, self._code, self._imports,\n                                                 dry_run=True,\n                                                 table_definitions=self._external_tables,\n                                                 dialect=dialect, billing_tier=billing_tier)\n    except Exception as e:\n      raise e\n    return query_result['statistics']['query']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a query asynchronously.", "response": "def execute_async(self, table_name=None, table_mode='create', use_cache=True,\n                    priority='interactive', allow_large_results=False, dialect=None,\n                    billing_tier=None):\n    \"\"\" Initiate the query and return a QueryJob.\n\n    Args:\n      table_name: the result table name as a string or TableName; if None (the default), then a\n          temporary table will be used.\n      table_mode: one of 'create', 'overwrite' or 'append'. If 'create' (the default), the request\n          will fail if the table exists.\n      use_cache: whether to use past query results or ignore cache. Has no effect if destination is\n          specified (default True).\n      priority:one of 'batch' or 'interactive' (default). 'interactive' jobs should be scheduled\n          to run quickly but are subject to rate limits; 'batch' jobs could be delayed by as much\n          as three hours but are not rate-limited.\n      allow_large_results: whether to allow large results; i.e. compressed data over 100MB. This is\n          slower and requires a table_name to be specified) (default False).\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      A QueryJob.\n    Raises:\n      Exception if query could not be executed.\n    \"\"\"\n    batch = priority == 'low'\n    append = table_mode == 'append'\n    overwrite = table_mode == 'overwrite'\n    if table_name is not None:\n      table_name = _utils.parse_table_name(table_name, self._api.project_id)\n\n    try:\n      query_result = self._api.jobs_insert_query(self._sql, self._code, self._imports,\n                                                 table_name=table_name,\n                                                 append=append,\n                                                 overwrite=overwrite,\n                                                 use_cache=use_cache,\n                                                 batch=batch,\n                                                 allow_large_results=allow_large_results,\n                                                 table_definitions=self._external_tables,\n                                                 dialect=dialect,\n                                                 billing_tier=billing_tier)\n    except Exception as e:\n      raise e\n    if 'jobReference' not in query_result:\n      raise Exception('Unexpected response from server')\n\n    job_id = query_result['jobReference']['jobId']\n    if not table_name:\n      try:\n        destination = query_result['configuration']['query']['destinationTable']\n        table_name = (destination['projectId'], destination['datasetId'], destination['tableId'])\n      except KeyError:\n        # The query was in error\n        raise Exception(_utils.format_query_errors(query_result['status']['errors']))\n    return _query_job.QueryJob(job_id, table_name, self._sql, context=self._context)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, table_name=None, table_mode='create', use_cache=True, priority='interactive',\n              allow_large_results=False, dialect=None, billing_tier=None):\n    \"\"\" Initiate the query, blocking until complete and then return the results.\n\n    Args:\n      table_name: the result table name as a string or TableName; if None (the default), then a\n          temporary table will be used.\n      table_mode: one of 'create', 'overwrite' or 'append'. If 'create' (the default), the request\n          will fail if the table exists.\n      use_cache: whether to use past query results or ignore cache. Has no effect if destination is\n          specified (default True).\n      priority:one of 'batch' or 'interactive' (default). 'interactive' jobs should be scheduled\n          to run quickly but are subject to rate limits; 'batch' jobs could be delayed by as much\n          as three hours but are not rate-limited.\n      allow_large_results: whether to allow large results; i.e. compressed data over 100MB. This is\n          slower and requires a table_name to be specified) (default False).\n      dialect : {'legacy', 'standard'}, default 'legacy'\n          'legacy' : Use BigQuery's legacy SQL dialect.\n          'standard' : Use BigQuery's standard SQL (beta), which is\n          compliant with the SQL 2011 standard.\n      billing_tier: Limits the billing tier for this job. Queries that have resource\n          usage beyond this tier will fail (without incurring a charge). If unspecified, this\n          will be set to your project default. This can also be used to override your\n          project-wide default billing tier on a per-query basis.\n    Returns:\n      The QueryResultsTable for the query.\n    Raises:\n      Exception if query could not be executed.\n    \"\"\"\n    job = self.execute_async(table_name=table_name, table_mode=table_mode, use_cache=use_cache,\n                             priority=priority, allow_large_results=allow_large_results,\n                             dialect=dialect, billing_tier=billing_tier)\n    self._results = job.wait()\n    return self._results", "response": "Executes a query and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a View from this Query.", "response": "def to_view(self, view_name):\n    \"\"\" Create a View from this Query.\n\n    Args:\n      view_name: the name of the View either as a string or a 3-part tuple\n          (projectid, datasetid, name).\n\n    Returns:\n      A View for the Query.\n    \"\"\"\n    # Do the import here to avoid circular dependencies at top-level.\n    from . import _view\n    return _view.View(view_name, self._context).create(self._sql)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noverride help doc to add cell args.", "response": "def format_help(self):\n    \"\"\"Override help doc to add cell args. \"\"\"\n\n    if not self._cell_args:\n      return super(CommandParser, self).format_help()\n    else:\n      # Print the standard argparse info, the cell arg block, and then the epilog\n      # If we don't remove epilog before calling the super, then epilog will\n      # be printed before the 'Cell args' block.\n      epilog = self.epilog\n      self.epilog = None\n      orig_help = super(CommandParser, self).format_help()\n\n      cell_args_help = '\\nCell args:\\n\\n'\n      for cell_arg, v in six.iteritems(self._cell_args):\n        required = 'Required' if v['required'] else 'Optional'\n        cell_args_help += '%s: %s. %s.\\n\\n' % (cell_arg, required, v['help'])\n\n      orig_help += cell_args_help\n      if epilog:\n        orig_help += epilog + '\\n\\n'\n      return orig_help"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_subparser_line_args(self, subparser_prog):\n\n    subparsers = self._get_subparsers()\n    for subparser in subparsers:\n      if subparser_prog == subparser.prog:\n        # Found the subparser.\n        args_to_parse = []\n        for action in subparser._actions:\n          if action.option_strings:\n            for argname in action.option_strings:\n              if argname.startswith('--'):\n                args_to_parse.append(argname[2:])\n        return args_to_parse\n\n    return None", "response": "Get line args of a specified subparser by its prog."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting cell args of a specified subparser by its prog.", "response": "def _get_subparser_cell_args(self, subparser_prog):\n    \"\"\" Get cell args of a specified subparser by its prog.\"\"\"\n\n    subparsers = self._get_subparsers()\n    for subparser in subparsers:\n      if subparser_prog == subparser.prog:\n        return subparser._cell_args\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_cell_argument(self, name, help, required=False):\n\n    for action in self._actions:\n      if action.dest == name:\n        raise ValueError('Arg \"%s\" was added by add_argument already.' % name)\n\n    self._cell_args[name] = {'required': required, 'help': help}", "response": "Add a cell only argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self, line, cell, namespace=None):\n\n    if namespace is None:\n      ipy = IPython.get_ipython()\n      namespace = ipy.user_ns\n\n    # Find which subcommand in the line by comparing line with subcommand progs.\n    # For example, assuming there are 3 subcommands with their progs\n    #   %bq tables\n    #   %bq tables list\n    #   %bq datasets\n    # and the line is \"tables list --dataset proj.myds\"\n    # it will find the second one --- \"tables list\" because it matches the prog and\n    # it is the longest.\n    args = CommandParser.create_args(line, namespace)\n\n    # \"prog\" is a ArgumentParser's path splitted by namspace, such as '%bq tables list'.\n    sub_parsers_progs = [x.prog for x in self._get_subparsers()]\n    matched_progs = []\n    for prog in sub_parsers_progs:\n      # Remove the leading magic such as \"%bq\".\n      match = prog.split()[1:]\n      for i in range(len(args)):\n        if args[i:i + len(match)] == match:\n          matched_progs.append(prog)\n          break\n\n    matched_prog = None\n    if matched_progs:\n      # Get the longest match.\n      matched_prog = max(matched_progs, key=lambda x: len(x.split()))\n\n    # Line args can be provided in cell too. If they are in cell, move them to line\n    # so we can parse them all together.\n    line_args = self._get_subparser_line_args(matched_prog)\n    if line_args:\n      cell_config = None\n      try:\n        cell_config, cell = google.datalab.utils.commands.parse_config_for_selected_keys(\n            cell, line_args)\n      except:\n        # It is okay --- probably because cell is not in yaml or json format.\n        pass\n\n      if cell_config:\n        google.datalab.utils.commands.replace_vars(cell_config, namespace)\n        for arg_name in cell_config:\n          arg_value = cell_config[arg_name]\n          if arg_value is None:\n            continue\n\n          if '--' + arg_name in args:\n            raise ValueError('config item \"%s\" is specified in both cell and line.' % arg_name)\n          if isinstance(arg_value, bool):\n            if arg_value:\n              line += ' --%s' % arg_name\n          else:\n            line += ' --%s %s' % (arg_name, str(cell_config[arg_name]))\n\n    # Parse args again with the new line.\n    args = CommandParser.create_args(line, namespace)\n    args = vars(self.parse_args(args))\n\n    # Parse cell args.\n    cell_config = None\n    cell_args = self._get_subparser_cell_args(matched_prog)\n    if cell_args:\n      try:\n        cell_config, _ = google.datalab.utils.commands.parse_config_for_selected_keys(\n            cell, cell_args)\n      except:\n        # It is okay --- probably because cell is not in yaml or json format.\n        pass\n\n      if cell_config:\n        google.datalab.utils.commands.replace_vars(cell_config, namespace)\n\n      for arg in cell_args:\n        if (cell_args[arg]['required'] and\n           (cell_config is None or cell_config.get(arg, None) is None)):\n          raise ValueError('Cell config \"%s\" is required.' % arg)\n\n    if cell_config:\n      args.update(cell_config)\n\n    return args, cell", "response": "Parses a line and cell into a dictionary of arguments expanding variables from a user namespace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _glob_events_files(self, paths, recursive):\n\n    event_files = []\n    for path in paths:\n      dirs = tf.gfile.Glob(path)\n      dirs = filter(lambda x: tf.gfile.IsDirectory(x), dirs)\n      for dir in dirs:\n        if recursive:\n          dir_files_pair = [(root, filenames) for root, _, filenames in tf.gfile.Walk(dir)]\n        else:\n          dir_files_pair = [(dir, tf.gfile.ListDirectory(dir))]\n\n        for root, filenames in dir_files_pair:\n          file_names = fnmatch.filter(filenames, '*.tfevents.*')\n          file_paths = [os.path.join(root, x) for x in file_names]\n          file_paths = filter(lambda x: not tf.gfile.IsDirectory(x), file_paths)\n          event_files += file_paths\n    return event_files", "response": "Find all tf events files under a list of paths recursively."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist all scalar events in the directory.", "response": "def list_events(self):\n    \"\"\"List all scalar events in the directory.\n\n    Returns:\n      A dictionary. Key is the name of a event. Value is a set of dirs that contain that event.\n    \"\"\"\n    event_dir_dict = collections.defaultdict(set)\n\n    for event_file in self._glob_events_files(self._paths, recursive=True):\n      dir = os.path.dirname(event_file)\n      try:\n        for record in tf_record.tf_record_iterator(event_file):\n          event = event_pb2.Event.FromString(record)\n          if event.summary is None or event.summary.value is None:\n            continue\n          for value in event.summary.value:\n            if value.simple_value is None or value.tag is None:\n              continue\n            event_dir_dict[value.tag].add(dir)\n      except tf.errors.DataLossError:\n        # DataLossError seems to happen sometimes for small logs.\n        # We want to show good records regardless.\n        continue\n    return dict(event_dir_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_events(self, event_names):\n\n    if isinstance(event_names, six.string_types):\n      event_names = [event_names]\n\n    all_events = self.list_events()\n    dirs_to_look = set()\n    for event, dirs in six.iteritems(all_events):\n      if event in event_names:\n        dirs_to_look.update(dirs)\n\n    ret_events = [collections.defaultdict(lambda: pd.DataFrame(columns=['time', 'step', 'value']))\n                  for i in range(len(event_names))]\n    for event_file in self._glob_events_files(dirs_to_look, recursive=False):\n      try:\n        for record in tf_record.tf_record_iterator(event_file):\n          event = event_pb2.Event.FromString(record)\n          if event.summary is None or event.wall_time is None or event.summary.value is None:\n            continue\n\n          event_time = datetime.datetime.fromtimestamp(event.wall_time)\n          for value in event.summary.value:\n            if value.tag not in event_names or value.simple_value is None:\n              continue\n\n            index = event_names.index(value.tag)\n            dir_event_dict = ret_events[index]\n            dir = os.path.dirname(event_file)\n            # Append a row.\n            df = dir_event_dict[dir]\n            df.loc[len(df)] = [event_time, event.step, value.simple_value]\n      except tf.errors.DataLossError:\n        # DataLossError seems to happen sometimes for small logs.\n        # We want to show good records regardless.\n        continue\n\n    for idx, dir_event_dict in enumerate(ret_events):\n      for df in dir_event_dict.values():\n        df.sort_values(by=['time'], inplace=True)\n      ret_events[idx] = dict(dir_event_dict)\n\n    return ret_events", "response": "Get all events as pandas DataFrames given a list of names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting a list of events.", "response": "def plot(self, event_names, x_axis='step'):\n    \"\"\"Plots a list of events. Each event (a dir+event_name) is represetented as a line\n       in the graph.\n    Args:\n      event_names: A list of events to plot. Each event_name may correspond to multiple events,\n          each in a different directory.\n      x_axis: whether to use step or time as x axis.\n    \"\"\"\n\n    if isinstance(event_names, six.string_types):\n      event_names = [event_names]\n\n    events_list = self.get_events(event_names)\n    for event_name, dir_event_dict in zip(event_names, events_list):\n      for dir, df in six.iteritems(dir_event_dict):\n        label = event_name + ':' + dir\n        x_column = df['step'] if x_axis == 'step' else df['time']\n        plt.plot(x_column, df['value'], label=label)\n    plt.legend(loc='best')\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an external table for a GCS object.", "response": "def from_storage(source, source_format='csv', csv_options=None, ignore_unknown_values=False,\n                   max_bad_records=0, compressed=False, schema=None):\n\n    \"\"\" Create an external table for a GCS object.\n\n    Args:\n      source: the URL of the source objects(s). Can include a wildcard '*' at the end of the item\n         name. Can be a single source or a list.\n      source_format: the format of the data, 'csv' or 'json'; default 'csv'.\n      csv_options: For CSV files, the options such as quote character and delimiter.\n      ignore_unknown_values: If True, accept rows that contain values that do not match the schema;\n          the unknown values are ignored (default False).\n      max_bad_records: The maximum number of bad records that are allowed (and ignored) before\n          returning an 'invalid' error in the Job result (default 0).\n      compressed: whether the data is GZ compressed or not (default False). Note that compressed\n          data can be used as a federated table but cannot be loaded into a BQ Table.\n      schema: the schema of the data. This is required for this table to be used as a federated\n          table or to be loaded using a Table object that itself has no schema (default None).\n\n  \"\"\"\n    result = FederatedTable()\n    # Do some sanity checking and concert some params from friendly form to form used by BQ.\n    if source_format == 'csv':\n      result._bq_source_format = 'CSV'\n      if csv_options is None:\n        csv_options = _csv_options.CSVOptions()  # use defaults\n    elif source_format == 'json':\n      if csv_options:\n        raise Exception('CSV options are not support for JSON tables')\n      result._bq_source_format = 'NEWLINE_DELIMITED_JSON'\n    else:\n      raise Exception(\"Invalid source format %s\" % source_format)\n\n    result._source = source if isinstance(source, list) else [source]\n    result._source_format = source_format\n    result._csv_options = csv_options\n    result._ignore_unknown_values = ignore_unknown_values\n    result._max_bad_records = max_bad_records\n    result._compressed = compressed\n    result._schema = schema\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a query argument to a cell magic.", "response": "def _get_query_argument(args, cell, env):\n  \"\"\" Get a query argument to a cell magic.\n\n  The query is specified with args['query']. We look that up and if it is a BQ query\n  object, just return it. If it is a string, build a query object out of it and return\n  that\n\n  Args:\n    args: the dictionary of magic arguments.\n    cell: the cell contents which can be variable value overrides (if args has a 'query'\n        value) or inline SQL otherwise.\n    env: a dictionary that is used for looking up variable values.\n\n  Returns:\n    A Query object.\n  \"\"\"\n  sql_arg = args.get('query', None)\n  if sql_arg is None:\n    # Assume we have inline SQL in the cell\n    if not isinstance(cell, basestring):\n      raise Exception('Expected a --query argument or inline SQL')\n    return bigquery.Query(cell, env=env)\n\n  item = google.datalab.utils.commands.get_notebook_item(sql_arg)\n  if isinstance(item, bigquery.Query):\n    return item\n  else:\n    raise Exception('Expected a query object, got %s.' % type(item))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts query parameters from cell body if provided", "response": "def get_query_parameters(args, cell_body, date_time=datetime.datetime.now()):\n  \"\"\"Extract query parameters from cell body if provided\n  Also validates the cell body schema using jsonschema to catch errors before sending the http\n  request. This validation isn't complete, however; it does not validate recursive schemas,\n  but it acts as a good filter against most simple schemas\n\n  Args:\n    args: arguments passed to the magic cell\n    cell_body: body of the magic cell\n    date_time: The timestamp at which the date-time related parameters need to be resolved.\n\n  Returns:\n    Validated object containing query parameters\n  \"\"\"\n\n  env = google.datalab.utils.commands.notebook_environment()\n  config = google.datalab.utils.commands.parse_config(cell_body, env=env, as_dict=False)\n  sql = args['query']\n  if sql is None:\n    raise Exception('Cannot extract query parameters in non-query cell')\n\n  # Validate query_params\n  if config:\n    jsonschema.validate(config, BigQuerySchema.QUERY_PARAMS_SCHEMA)\n\n  config = config or {}\n  config_parameters = config.get('parameters', [])\n  return bigquery.Query.get_query_parameters(config_parameters, date_time=date_time)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimplement the BigQuery sample magic for sampling queries", "response": "def _sample_cell(args, cell_body):\n  \"\"\"Implements the BigQuery sample magic for sampling queries\n  The supported sytanx is:\n    %%bq sample <args>\n     [<inline SQL>]\n  Args:\n    args: the optional arguments following '%%bq sample'.\n    cell_body: optional contents of the cell\n  Returns:\n    The results of executing the sampling query, or a profile of the sample data.\n  \"\"\"\n\n  env = google.datalab.utils.commands.notebook_environment()\n  config = google.datalab.utils.commands.parse_config(cell_body, env, False) or {}\n  parameters = config.get('parameters') or []\n  if parameters:\n    jsonschema.validate({'parameters': parameters}, BigQuerySchema.QUERY_PARAMS_SCHEMA)\n\n  query = None\n  table = None\n  view = None\n  query_params = None\n\n  if args['query']:\n    query = google.datalab.utils.commands.get_notebook_item(args['query'])\n    if query is None:\n      raise Exception('Cannot find query %s.' % args['query'])\n    query_params = get_query_parameters(args, cell_body)\n\n  elif args['table']:\n    table_name = google.datalab.bigquery.Query.resolve_parameters(args['table'], parameters)\n    table = _get_table(table_name)\n    if not table:\n      raise Exception('Could not find table %s' % args['table'])\n  elif args['view']:\n    view = google.datalab.utils.commands.get_notebook_item(args['view'])\n    if not isinstance(view, bigquery.View):\n      raise Exception('Could not find view %s' % args['view'])\n  else:\n    raise Exception('A query, table, or view is neede to sample')\n\n  # parse comma-separated list of fields\n  fields = args['fields'].split(',') if args['fields'] else None\n  count = int(args['count']) if args['count'] else None\n  percent = int(args['percent']) if args['percent'] else None\n  sampling = Sampling._auto(method=args['method'], fields=fields, count=count, percent=percent,\n                            key_field=args['key_field'], ascending=(args['order'] == 'ascending'))\n\n  context = google.datalab.utils._utils._construct_context_for_args(args)\n\n  if view:\n    query = bigquery.Query.from_view(view)\n  elif table:\n    query = bigquery.Query.from_table(table)\n\n  if args['profile']:\n    results = query.execute(QueryOutput.dataframe(), sampling=sampling,\n                            context=context, query_params=query_params).result()\n  else:\n    results = query.execute(QueryOutput.table(), sampling=sampling, context=context,\n                            query_params=query_params).result()\n\n  if args['verbose']:\n    print(query.sql)\n\n  if args['profile']:\n    return google.datalab.utils.commands.profile_df(results)\n  else:\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimplement the BigQuery cell magic used to dry run BQ queries.", "response": "def _dryrun_cell(args, cell_body):\n  \"\"\"Implements the BigQuery cell magic used to dry run BQ queries.\n\n   The supported syntax is:\n   %%bq dryrun [-q|--sql <query identifier>]\n   [<YAML or JSON cell_body or inline SQL>]\n\n  Args:\n    args: the argument following '%bq dryrun'.\n    cell_body: optional contents of the cell interpreted as YAML or JSON.\n  Returns:\n    The response wrapped in a DryRunStats object\n  \"\"\"\n  query = _get_query_argument(args, cell_body, google.datalab.utils.commands.notebook_environment())\n\n  if args['verbose']:\n    print(query.sql)\n\n  context = google.datalab.utils._utils._construct_context_for_args(args)\n  result = query.dry_run(context=context)\n  return bigquery._query_stats.QueryStats(\n    total_bytes=result['totalBytesProcessed'], is_cached=result['cacheHit'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimplement the Bigquery UDF cell magic for ipython notebooks.", "response": "def _udf_cell(args, cell_body):\n  \"\"\"Implements the Bigquery udf cell magic for ipython notebooks.\n\n  The supported syntax is:\n  %%bq udf --name <var> --language <lang>\n  // @param <name> <type>\n  // @returns <type>\n  // @import <gcs_path>\n  <js function>\n\n  Args:\n    args: the optional arguments following '%%bq udf'.\n    cell_body: the UDF declaration (inputs and outputs) and implementation in javascript.\n  \"\"\"\n  udf_name = args['name']\n  if not udf_name:\n    raise Exception('Declaration must be of the form %%bq udf --name <variable name>')\n\n  # Parse out parameters, return type, and imports\n  param_pattern = r'^\\s*\\/\\/\\s*@param\\s+([<>\\w]+)\\s+([<>\\w,\\s]+)\\s*$'\n  returns_pattern = r'^\\s*\\/\\/\\s*@returns\\s+([<>\\w,\\s]+)\\s*$'\n  import_pattern = r'^\\s*\\/\\/\\s*@import\\s+(\\S+)\\s*$'\n\n  params = re.findall(param_pattern, cell_body, re.MULTILINE)\n  return_type = re.findall(returns_pattern, cell_body, re.MULTILINE)\n  imports = re.findall(import_pattern, cell_body, re.MULTILINE)\n\n  if len(return_type) < 1:\n    raise Exception('UDF return type must be defined using // @returns <type>')\n  if len(return_type) > 1:\n    raise Exception('Found more than one return type definition')\n\n  return_type = return_type[0]\n\n  # Finally build the UDF object\n  udf = bigquery.UDF(udf_name, cell_body, return_type, params, args['language'], imports)\n  google.datalab.utils.commands.notebook_environment()[udf_name] = udf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements the BigQuery datasource cell magic for ipython notebooks.", "response": "def _datasource_cell(args, cell_body):\n  \"\"\"Implements the BigQuery datasource cell magic for ipython notebooks.\n\n  The supported syntax is\n  %%bq datasource --name <var> --paths <url> [--format <CSV|JSON>]\n  <schema>\n\n  Args:\n    args: the optional arguments following '%%bq datasource'\n    cell_body: the datasource's schema in json/yaml\n  \"\"\"\n  name = args['name']\n  paths = args['paths']\n  data_format = (args['format'] or 'CSV').lower()\n  compressed = args['compressed'] or False\n\n  # Get the source schema from the cell body\n  record = google.datalab.utils.commands.parse_config(\n      cell_body, google.datalab.utils.commands.notebook_environment(), as_dict=False)\n\n  jsonschema.validate(record, BigQuerySchema.TABLE_SCHEMA_SCHEMA)\n  schema = bigquery.Schema(record['schema'])\n\n  # Finally build the datasource object\n  datasource = bigquery.ExternalDataSource(source=paths, source_format=data_format,\n                                           compressed=compressed, schema=schema)\n  google.datalab.utils.commands.notebook_environment()[name] = datasource"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _query_cell(args, cell_body):\n  name = args['name']\n  udfs = args['udfs']\n  datasources = args['datasources']\n  subqueries = args['subqueries']\n\n  # Finally build the query object\n  query = bigquery.Query(cell_body, env=IPython.get_ipython().user_ns, udfs=udfs,\n                         data_sources=datasources, subqueries=subqueries)\n\n  # if no name is specified, execute this query instead of defining it\n  if name is None:\n    return query.execute().result()\n  else:\n    google.datalab.utils.commands.notebook_environment()[name] = query", "response": "Implements the BigQuery cell magic for used to build SQL objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _execute_cell(args, cell_body):\n  env = google.datalab.utils.commands.notebook_environment()\n  config = google.datalab.utils.commands.parse_config(cell_body, env, False) or {}\n  parameters = config.get('parameters') or []\n  if parameters:\n    jsonschema.validate({'parameters': parameters}, BigQuerySchema.QUERY_PARAMS_SCHEMA)\n  table_name = google.datalab.bigquery.Query.resolve_parameters(args['table'], parameters)\n\n  query = google.datalab.utils.commands.get_notebook_item(args['query'])\n  if args['verbose']:\n    print(query.sql)\n\n  query_params = get_query_parameters(args, cell_body)\n\n  if args['to_dataframe']:\n    # re-parse the int arguments because they're passed as strings\n    start_row = int(args['dataframe_start_row']) if args['dataframe_start_row'] else None\n    max_rows = int(args['dataframe_max_rows']) if args['dataframe_max_rows'] else None\n    output_options = QueryOutput.dataframe(start_row=start_row, max_rows=max_rows,\n                                           use_cache=not args['nocache'])\n  else:\n    output_options = QueryOutput.table(\n      name=table_name, mode=args['mode'], use_cache=not args['nocache'],\n      allow_large_results=args['large'])\n  context = google.datalab.utils._utils._construct_context_for_args(args)\n  r = query.execute(output_options, context=context, query_params=query_params)\n  return r.result()", "response": "Implements the BigQuery cell magic used to execute BigQuery queries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a variable or table name get a Table if it exists.", "response": "def _get_table(name):\n  \"\"\" Given a variable or table name, get a Table if it exists.\n\n  Args:\n    name: the name of the Table or a variable referencing the Table.\n  Returns:\n    The Table, if found.\n  \"\"\"\n  # If name is a variable referencing a table, use that.\n  item = google.datalab.utils.commands.get_notebook_item(name)\n  if isinstance(item, bigquery.Table):\n    return item\n  # Else treat this as a BQ table name and return the (cached) table if it exists.\n  try:\n    return _existing_table_cache[name]\n  except KeyError:\n    table = bigquery.Table(name)\n    if table.exists():\n      _existing_table_cache[name] = table\n      return table\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dataset_line(args):\n  if args['command'] == 'list':\n    filter_ = args['filter'] if args['filter'] else '*'\n    context = google.datalab.Context.default()\n    if args['project']:\n      context = google.datalab.Context(args['project'], context.credentials)\n    return _render_list([str(dataset) for dataset in bigquery.Datasets(context)\n                         if fnmatch.fnmatch(str(dataset), filter_)])\n\n  elif args['command'] == 'create':\n    try:\n      bigquery.Dataset(args['name']).create(friendly_name=args['friendly'])\n    except Exception as e:\n      print('Failed to create dataset %s: %s' % (args['name'], e))\n\n  elif args['command'] == 'delete':\n    try:\n      bigquery.Dataset(args['name']).delete()\n    except Exception as e:\n      print('Failed to delete dataset %s: %s' % (args['name'], e))", "response": "Implements the BigQuery dataset magic subcommand used to operate on datasets\n  "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements the BigQuery table cell magic subcommand used to operate on tables", "response": "def _table_cell(args, cell_body):\n  \"\"\"Implements the BigQuery table magic subcommand used to operate on tables\n\n   The supported syntax is:\n   %%bq tables <command> <args>\n\n  Commands:\n    {list, create, delete, describe, view}\n\n  Args:\n    args: the optional arguments following '%%bq tables command'.\n    cell_body: optional contents of the cell interpreted as SQL, YAML or JSON.\n  Returns:\n    The HTML rendering for the table of datasets.\n  \"\"\"\n  if args['command'] == 'list':\n    filter_ = args['filter'] if args['filter'] else '*'\n    if args['dataset']:\n      if args['project'] is None:\n        datasets = [bigquery.Dataset(args['dataset'])]\n      else:\n        context = google.datalab.Context(args['project'],\n                                         google.datalab.Context.default().credentials)\n        datasets = [bigquery.Dataset(args['dataset'], context)]\n    else:\n      default_context = google.datalab.Context.default()\n      context = google.datalab.Context(default_context.project_id, default_context.credentials)\n      if args['project']:\n        context.set_project_id(args['project'])\n      datasets = bigquery.Datasets(context)\n\n    tables = []\n    for dataset in datasets:\n      tables.extend([table.full_name\n                     for table in dataset if fnmatch.fnmatch(table.full_name, filter_)])\n\n    return _render_list(tables)\n\n  elif args['command'] == 'create':\n    if cell_body is None:\n      print('Failed to create %s: no schema specified' % args['name'])\n    else:\n      try:\n        record = google.datalab.utils.commands.parse_config(\n            cell_body, google.datalab.utils.commands.notebook_environment(), as_dict=False)\n        jsonschema.validate(record, BigQuerySchema.TABLE_SCHEMA_SCHEMA)\n        schema = bigquery.Schema(record['schema'])\n        bigquery.Table(args['name']).create(schema=schema, overwrite=args['overwrite'])\n      except Exception as e:\n        print('Failed to create table %s: %s' % (args['name'], e))\n\n  elif args['command'] == 'describe':\n    name = args['name']\n    table = _get_table(name)\n    if not table:\n      raise Exception('Could not find table %s' % name)\n\n    html = _repr_html_table_schema(table.schema)\n    return IPython.core.display.HTML(html)\n\n  elif args['command'] == 'delete':\n    try:\n      bigquery.Table(args['name']).delete()\n    except Exception as e:\n      print('Failed to delete table %s: %s' % (args['name'], e))\n\n  elif args['command'] == 'view':\n    name = args['name']\n    table = _get_table(name)\n    if not table:\n      raise Exception('Could not find table %s' % name)\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimplementing the BigQuery extract magic used to extract data from GCS.", "response": "def _extract_cell(args, cell_body):\n  \"\"\"Implements the BigQuery extract magic used to extract query or table data to GCS.\n\n   The supported syntax is:\n     %bq extract <args>\n\n  Args:\n    args: the arguments following '%bigquery extract'.\n  \"\"\"\n\n  env = google.datalab.utils.commands.notebook_environment()\n  config = google.datalab.utils.commands.parse_config(cell_body, env, False) or {}\n  parameters = config.get('parameters')\n  if args['table']:\n    table = google.datalab.bigquery.Query.resolve_parameters(args['table'], parameters)\n    source = _get_table(table)\n    if not source:\n      raise Exception('Could not find table %s' % table)\n\n    csv_delimiter = args['delimiter'] if args['format'] == 'csv' else None\n    path = google.datalab.bigquery.Query.resolve_parameters(args['path'], parameters)\n    job = source.extract(path, format=args['format'], csv_delimiter=csv_delimiter,\n                         csv_header=args['header'], compress=args['compress'])\n  elif args['query'] or args['view']:\n    source_name = args['view'] or args['query']\n    source = google.datalab.utils.commands.get_notebook_item(source_name)\n    if not source:\n      raise Exception('Could not find ' +\n                      ('view ' + args['view'] if args['view'] else 'query ' + args['query']))\n    query = source if args['query'] else bigquery.Query.from_view(source)\n    query_params = get_query_parameters(args, cell_body) if args['query'] else None\n\n    output_options = QueryOutput.file(path=args['path'], format=args['format'],\n                                      csv_delimiter=args['delimiter'],\n                                      csv_header=args['header'], compress=args['compress'],\n                                      use_cache=not args['nocache'])\n    context = google.datalab.utils._utils._construct_context_for_args(args)\n    job = query.execute(output_options, context=context, query_params=query_params)\n  else:\n    raise Exception('A query, table, or view is needed to extract')\n\n  if job.failed:\n    raise Exception('Extract failed: %s' % str(job.fatal_error))\n  elif job.errors:\n    raise Exception('Extract completed with errors: %s' % str(job.errors))\n  return job.result()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimplement the BigQuery load magic used to load data from GCS to a table.", "response": "def _load_cell(args, cell_body):\n  \"\"\"Implements the BigQuery load magic used to load data from GCS to a table.\n\n   The supported syntax is:\n\n       %bq load <optional args>\n\n  Args:\n    args: the arguments following '%bq load'.\n    cell_body: optional contents of the cell interpreted as YAML or JSON.\n  Returns:\n    A message about whether the load succeeded or failed.\n  \"\"\"\n  env = google.datalab.utils.commands.notebook_environment()\n  config = google.datalab.utils.commands.parse_config(cell_body, env, False) or {}\n\n  parameters = config.get('parameters') or []\n  if parameters:\n    jsonschema.validate({'parameters': parameters}, BigQuerySchema.QUERY_PARAMS_SCHEMA)\n  name = google.datalab.bigquery.Query.resolve_parameters(args['table'], parameters)\n\n  table = _get_table(name)\n  if not table:\n    table = bigquery.Table(name)\n\n  if args['mode'] == 'create':\n    if table.exists():\n      raise Exception('table %s already exists; use \"append\" or \"overwrite\" as mode.' % name)\n    if not cell_body or 'schema' not in cell_body:\n      raise Exception('Table does not exist, and no schema specified in cell; cannot load.')\n\n    schema = config['schema']\n    # schema can be an instance of bigquery.Schema.\n    # For example, user can run \"my_schema = bigquery.Schema.from_data(df)\" in a previous cell and\n    # specify \"schema: $my_schema\" in cell input.\n    if not isinstance(schema, bigquery.Schema):\n      jsonschema.validate({'schema': schema}, BigQuerySchema.TABLE_SCHEMA_SCHEMA)\n      schema = bigquery.Schema(schema)\n    table.create(schema=schema)\n  elif not table.exists():\n    raise Exception('table %s does not exist; use \"create\" as mode.' % name)\n\n  csv_options = bigquery.CSVOptions(delimiter=args['delimiter'], skip_leading_rows=args['skip'],\n                                    allow_jagged_rows=not args['strict'], quote=args['quote'])\n  path = google.datalab.bigquery.Query.resolve_parameters(args['path'], parameters)\n  job = table.load(path, mode=args['mode'], source_format=args['format'], csv_options=csv_options,\n                   ignore_unknown_values=not args['strict'])\n  if job.failed:\n    raise Exception('Load failed: %s' % str(job.fatal_error))\n  elif job.errors:\n    raise Exception('Load completed with errors: %s' % str(job.errors))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pipeline_cell(args, cell_body):\n    name = args.get('name')\n    if name is None:\n        raise Exception('Pipeline name was not specified.')\n\n    import google.datalab.utils as utils\n    bq_pipeline_config = utils.commands.parse_config(\n      cell_body, utils.commands.notebook_environment())\n\n    try:\n      airflow_spec = \\\n        google.datalab.contrib.bigquery.commands.get_airflow_spec_from_config(name,\n                                                                              bq_pipeline_config)\n    except AttributeError:\n      return \"Perhaps you're missing: import google.datalab.contrib.bigquery.commands\"\n\n    # If a gcs_dag_bucket is specified, we deploy to it so that the Airflow VM rsyncs it.\n    error_message = ''\n    gcs_dag_bucket = args.get('gcs_dag_bucket')\n    gcs_dag_file_path = args.get('gcs_dag_file_path')\n    if gcs_dag_bucket:\n      try:\n        airflow = google.datalab.contrib.pipeline.airflow.Airflow(gcs_dag_bucket, gcs_dag_file_path)\n        airflow.deploy(name, airflow_spec)\n        error_message += (\"Airflow pipeline successfully deployed! View dashboard for more \"\n                          \"details.\\n\")\n      except AttributeError:\n        return \"Perhaps you're missing: import google.datalab.contrib.pipeline.airflow\"\n\n    location = args.get('location')\n    environment = args.get('environment')\n\n    if location and environment:\n      try:\n        composer = google.datalab.contrib.pipeline.composer.Composer(location, environment)\n        composer.deploy(name, airflow_spec)\n        error_message += (\"Composer pipeline successfully deployed! View dashboard for more \"\n                          \"details.\\n\")\n      except AttributeError:\n        return \"Perhaps you're missing: import google.datalab.contrib.pipeline.composer\"\n\n    if args.get('debug'):\n      error_message += '\\n\\n' + airflow_spec\n\n    return error_message", "response": "Implements the %%bq pipeline subcommand in the %%bq magic."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the parser for the %bq command.", "response": "def _create_bigquery_parser():\n  \"\"\" Create the parser for the %bq magics.\n\n  Note that because we use the func default handler dispatch mechanism of argparse,\n  our handlers can take only one argument which is the parsed args. So we must create closures\n  for the handlers that bind the cell contents and thus must recreate this parser for each\n  cell upon execution.\n  \"\"\"\n  parser = google.datalab.utils.commands.CommandParser(prog='%bq', description=\"\"\"\nExecute various BigQuery-related operations. Use \"%bq <command> -h\"\nfor help on a specific command.\n  \"\"\")\n\n  # This is a bit kludgy because we want to handle some line magics and some cell magics\n  # with the bq command.\n\n  # %bq datasets\n  _add_command(parser, _create_dataset_subparser, _dataset_line, cell_prohibited=True)\n\n  # %bq tables\n  _add_command(parser, _create_table_subparser, _table_cell)\n\n  # %%bq query\n  _add_command(parser, _create_query_subparser, _query_cell)\n\n  # %%bq execute\n  _add_command(parser, _create_execute_subparser, _execute_cell)\n\n  # %bq extract\n  _add_command(parser, _create_extract_subparser, _extract_cell)\n\n  # %%bq sample\n  _add_command(parser, _create_sample_subparser, _sample_cell)\n\n  # %%bq dryrun\n  _add_command(parser, _create_dryrun_subparser, _dryrun_cell)\n\n  # %%bq udf\n  _add_command(parser, _create_udf_subparser, _udf_cell, cell_required=True)\n\n  # %%bq datasource\n  _add_command(parser, _create_datasource_subparser, _datasource_cell, cell_required=True)\n\n  # %bq load\n  _add_command(parser, _create_load_subparser, _load_cell)\n\n  # %bq pipeline\n  _add_command(parser, _create_pipeline_subparser, _pipeline_cell)\n\n  return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bq(line, cell=None):\n  return google.datalab.utils.commands.handle_magic_line(line, cell, _bigquery_parser)", "response": "Implements the bq cell magic for ipython notebooks."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string containing the HTML for the table viewer.", "response": "def _table_viewer(table, rows_per_page=25, fields=None):\n  \"\"\"  Return a table viewer.\n\n    This includes a static rendering of the first page of the table, that gets replaced\n    by the charting code in environments where Javascript is executable and BQ is available.\n\n  Args:\n    table: the table to view.\n    rows_per_page: how many rows to display at one time.\n    fields: an array of field names to display; default is None which uses the full schema.\n  Returns:\n    A string containing the HTML for the table viewer.\n  \"\"\"\n\n  # TODO(gram): rework this to use google.datalab.utils.commands.chart_html\n\n  if not table.exists():\n    raise Exception('Table %s does not exist' % table.full_name)\n\n  if not table.is_listable():\n    return \"Done\"\n\n  _HTML_TEMPLATE = u\"\"\"\n    <div class=\"bqtv\" id=\"{div_id}\">{static_table}</div>\n    <br />{meta_data}<br />\n    <script src=\"/static/components/requirejs/require.js\"></script>\n    <script>\n      require.config({{\n        paths: {{\n          base: '/static/base',\n          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n        }},\n        map: {{\n          '*': {{\n            datalab: 'nbextensions/gcpdatalab'\n          }}\n        }},\n        shim: {{\n          plotly: {{\n            deps: ['d3', 'jquery'],\n            exports: 'plotly'\n          }}\n        }}\n      }});\n\n      require(['datalab/charting', 'datalab/element!{div_id}', 'base/js/events',\n          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n        function(charts, dom, events) {{\n          charts.render('gcharts', dom, events, '{chart_style}', [], {data},\n            {{\n              pageSize: {rows_per_page},\n              cssClassNames:  {{\n                tableRow: 'gchart-table-row',\n                headerRow: 'gchart-table-headerrow',\n                oddTableRow: 'gchart-table-oddrow',\n                selectedTableRow: 'gchart-table-selectedrow',\n                hoverTableRow: 'gchart-table-hoverrow',\n                tableCell: 'gchart-table-cell',\n                headerCell: 'gchart-table-headercell',\n                rowNumberCell: 'gchart-table-rownumcell'\n              }}\n            }},\n            {{source_index: {source_index}, fields: '{fields}'}},\n            0,\n            {total_rows});\n        }}\n      );\n    </script>\n  \"\"\"\n\n  if fields is None:\n    fields = google.datalab.utils.commands.get_field_list(fields, table.schema)\n  div_id = google.datalab.utils.commands.Html.next_id()\n  meta_count = ('rows: %d' % table.length) if table.length >= 0 else ''\n  meta_name = table.full_name if table.job is None else ('job: %s' % table.job.id)\n  if table.job:\n    if table.job.cache_hit:\n      meta_cost = 'cached'\n    else:\n      bytes = bigquery._query_stats.QueryStats._size_formatter(table.job.bytes_processed)\n      meta_cost = '%s processed' % bytes\n    meta_time = 'time: %.1fs' % table.job.total_time\n  else:\n    meta_cost = ''\n    meta_time = ''\n\n  data, total_count = google.datalab.utils.commands.get_data(table, fields, first_row=0,\n                                                             count=rows_per_page)\n\n  if total_count < 0:\n    # The table doesn't have a length metadata property but may still be small if we fetched less\n    # rows than we asked for.\n    fetched_count = len(data['rows'])\n    if fetched_count < rows_per_page:\n      total_count = fetched_count\n\n  chart = 'table' if 0 <= total_count <= rows_per_page else 'paged_table'\n  meta_entries = [meta_count, meta_time, meta_cost, meta_name]\n  meta_data = '(%s)' % (', '.join([entry for entry in meta_entries if len(entry)]))\n\n  return _HTML_TEMPLATE.format(div_id=div_id,\n                               static_table=google.datalab.utils.commands.HtmlBuilder\n                               .render_chart_data(data),\n                               meta_data=meta_data,\n                               chart_style=chart,\n                               source_index=google.datalab.utils.commands\n                               .get_data_source_index(table.full_name),\n                               fields=','.join(fields),\n                               total_rows=total_count,\n                               rows_per_page=rows_per_page,\n                               data=json.dumps(data, cls=google.datalab.utils.JSONEncoder))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a BigQuery SQL UDF javascript object.", "response": "def _build_js(inputs, outputs, name, implementation, support_code):\n    \"\"\"Creates a BigQuery SQL UDF javascript object.\n\n    Args:\n      inputs: a list of (name, type) tuples representing the schema of input.\n      outputs: a list of (name, type) tuples representing the schema of the output.\n      name: the name of the function\n      implementation: a javascript function defining the UDF logic.\n      support_code: additional javascript code that the function can use.\n    \"\"\"\n    # Construct a comma-separated list of input field names\n    # For example, field1,field2,...\n    input_fields = json.dumps([f[0] for f in inputs])\n\n    # Construct a json representation of the output schema\n    # For example, [{'name':'field1','type':'string'},...]\n    output_fields = [{'name': f[0], 'type': f[1]} for f in outputs]\n    output_fields = json.dumps(output_fields, sort_keys=True)\n\n    # Build the JS from the individual bits with proper escaping of the implementation\n    if support_code is None:\n      support_code = ''\n    return ('{code}\\n{name}={implementation};\\nbigquery.defineFunction(\\'{name}\\', {inputs}, '\n            '{outputs}, {name});').format(code=support_code, name=name,\n                                          implementation=implementation, inputs=str(input_fields),\n                                          outputs=str(output_fields))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a sampling query for the SQL object.", "response": "def sampling_query(sql, fields=None, count=5, sampling=None):\n    \"\"\"Returns a sampling query for the SQL object.\n\n    Args:\n      sql: the SQL object to sample\n      fields: an optional list of field names to retrieve.\n      count: an optional count of rows to retrieve which is used if a specific\n          sampling is not specified.\n      sampling: an optional sampling strategy to apply to the table.\n    Returns:\n      A SQL query string for sampling the input sql.\n    \"\"\"\n    if sampling is None:\n      sampling = Sampling.default(count=count, fields=fields)\n    return sampling(sql)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _remove_nonascii(self, df):\n\n    df_copy = df.copy(deep=True)\n    for col in df_copy.columns:\n      if (df_copy[col].dtype == np.dtype('O')):\n        df_copy[col] = df[col].apply(\n          lambda x: re.sub(r'[^\\x00-\\x7f]', r'', x) if isinstance(x, six.string_types) else x)\n\n    return df_copy", "response": "Make copy and remove non - ascii characters from it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self, data):\n\n    import IPython\n\n    if not isinstance(data, dict) or not all(isinstance(v, pd.DataFrame) for v in data.values()):\n      raise ValueError('Expect a dictionary where the values are all dataframes.')\n\n    gfsg = GenericFeatureStatisticsGenerator()\n    data = [{'name': k, 'table': self._remove_nonascii(v)} for k, v in six.iteritems(data)]\n    data_proto = gfsg.ProtoFromDataFrames(data)\n    protostr = base64.b64encode(data_proto.SerializeToString()).decode(\"utf-8\")\n    html_id = 'f' + datalab.utils.commands.Html.next_id()\n\n    HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/gcpdatalab/extern/facets-jupyter.html\" >\n        <facets-overview id=\"{html_id}\"></facets-overview>\n        <script>\n          document.querySelector(\"#{html_id}\").protoInput = \"{protostr}\";\n        </script>\"\"\"\n    html = HTML_TEMPLATE.format(html_id=html_id, protostr=protostr)\n    return IPython.core.display.HTML(html)", "response": "Plots an overview in a list of dataframes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, data, height=1000, render_large_data=False):\n\n    import IPython\n\n    if not isinstance(data, pd.DataFrame):\n      raise ValueError('Expect a DataFrame.')\n\n    if (len(data) > 10000 and not render_large_data):\n      raise ValueError('Facets dive may not work well with more than 10000 rows. ' +\n                       'Reduce data or set \"render_large_data\" to True.')\n\n    jsonstr = data.to_json(orient='records')\n    html_id = 'f' + datalab.utils.commands.Html.next_id()\n    HTML_TEMPLATE = \"\"\"\n        <link rel=\"import\" href=\"/nbextensions/gcpdatalab/extern/facets-jupyter.html\">\n        <facets-dive id=\"{html_id}\" height=\"{height}\"></facets-dive>\n        <script>\n          var data = {jsonstr};\n          document.querySelector(\"#{html_id}\").data = data;\n        </script>\"\"\"\n    html = HTML_TEMPLATE.format(html_id=html_id, jsonstr=jsonstr, height=height)\n    return IPython.core.display.HTML(html)", "response": "Plots a detail view of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ProtoFromDataFrames(self, dataframes):\n    datasets = []\n    for dataframe in dataframes:\n      table = dataframe['table']\n      table_entries = {}\n      for col in table:\n        table_entries[col] = self.NdarrayToEntry(table[col])\n      datasets.append({\n          'entries': table_entries,\n          'size': len(table),\n          'name': dataframe['name']\n      })\n    return self.GetDatasetsProto(datasets)", "response": "Creates a feature statistics proto from a set of pandas dataframes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DtypeToType(self, dtype):\n    if dtype.char in np.typecodes['AllFloat']:\n      return self.fs_proto.FLOAT\n    elif (dtype.char in np.typecodes['AllInteger'] or dtype == np.bool or\n          np.issubdtype(dtype, np.datetime64) or\n          np.issubdtype(dtype, np.timedelta64)):\n      return self.fs_proto.INT\n    else:\n      return self.fs_proto.STRING", "response": "Converts a Numpy dtype to the FeatureNameStatistics. Type proto enum."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DtypeToNumberConverter(self, dtype):\n    if np.issubdtype(dtype, np.datetime64):\n\n      def DatetimesToNumbers(dt_list):\n        return np.array([pd.Timestamp(dt).value for dt in dt_list])\n\n      return DatetimesToNumbers\n    elif np.issubdtype(dtype, np.timedelta64):\n\n      def TimedetlasToNumbers(td_list):\n        return np.array([pd.Timedelta(td).value for td in td_list])\n\n      return TimedetlasToNumbers\n    else:\n      return None", "response": "Converts a Numpy dtype to a converter method if applicable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef NdarrayToEntry(self, x):\n    row_counts = []\n    for row in x:\n      try:\n        rc = np.count_nonzero(~np.isnan(row))\n        if rc != 0:\n          row_counts.append(rc)\n      except TypeError:\n        try:\n          row_counts.append(row.size)\n        except AttributeError:\n          row_counts.append(1)\n\n    data_type = self.DtypeToType(x.dtype)\n    converter = self.DtypeToNumberConverter(x.dtype)\n    flattened = x.ravel()\n    orig_size = len(flattened)\n\n    # Remove all None and nan values and count how many were removed.\n    flattened = flattened[flattened != np.array(None)]\n    if converter:\n      flattened = converter(flattened)\n    if data_type == self.fs_proto.STRING:\n      flattened_temp = []\n      for x in flattened:\n        try:\n          if str(x) != 'nan':\n            flattened_temp.append(x)\n        except UnicodeEncodeError:\n          if x.encode('utf-8') != 'nan':\n            flattened_temp.append(x)\n      flattened = flattened_temp\n    else:\n      flattened = flattened[~np.isnan(flattened)].tolist()\n    missing = orig_size - len(flattened)\n    return {\n        'vals': flattened,\n        'counts': row_counts,\n        'missing': missing,\n        'type': data_type\n    }", "response": "Converts an ndarray to the Entry format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the feature stats proto for the provided datasets.", "response": "def GetDatasetsProto(self, datasets, features=None):\n    \"\"\"Generates the feature stats proto from dictionaries of feature values.\n    Args:\n      datasets: An array of dictionaries, one per dataset, each one containing:\n          - 'entries': The dictionary of features in the dataset from the parsed\n            examples.\n          - 'size': The number of examples parsed for the dataset.\n          - 'name': The name of the dataset.\n      features: A list of strings that is a whitelist of feature names to create\n          feature statistics for. If set to None then all features in the\n            dataset\n          are analyzed. Defaults to None.\n    Returns:\n      The feature statistics proto for the provided datasets.\n    \"\"\"\n    features_seen = set()\n    whitelist_features = set(features) if features else None\n    all_datasets = self.datasets_proto()\n\n    # TODO(jwexler): Add ability to generate weighted feature stats\n    # if there is a specified weight feature in the dataset.\n\n    # Initialize each dataset\n    for dataset in datasets:\n      all_datasets.datasets.add(\n          name=dataset['name'], num_examples=dataset['size'])\n    # This outer loop ensures that for each feature seen in any of the provided\n    # datasets, we check the feature once against all datasets.\n    for outer_dataset in datasets:\n      for key, value in outer_dataset['entries'].items():\n        # If we have a feature whitelist and this feature is not in the\n        # whitelist then do not process it.\n        # If we have processed this feature already, no need to do it again.\n        if ((whitelist_features and key not in whitelist_features) or\n            key in features_seen):\n          continue\n        features_seen.add(key)\n        # Default to type int if no type is found, so that the fact that all\n        # values are missing from this feature can be displayed.\n        feature_type = value['type'] if 'type' in value else self.fs_proto.INT\n        # Process the found feature for each dataset.\n        for j, dataset in enumerate(datasets):\n          feat = all_datasets.datasets[j].features.add(\n              type=feature_type, name=str(key))\n          value = dataset['entries'].get(key)\n          has_data = value is not None and (value['vals'].size != 0\n                                            if isinstance(\n                                                value['vals'], np.ndarray) else\n                                            value['vals'])\n          commonstats = None\n          # For numeric features, calculate numeric statistics.\n          if feat.type in (self.fs_proto.INT, self.fs_proto.FLOAT):\n            featstats = feat.num_stats\n            commonstats = featstats.common_stats\n            if has_data:\n              nums = value['vals']\n              featstats.std_dev = np.asscalar(np.std(nums))\n              featstats.mean = np.asscalar(np.mean(nums))\n              featstats.min = np.asscalar(np.min(nums))\n              featstats.max = np.asscalar(np.max(nums))\n              featstats.median = np.asscalar(np.median(nums))\n              featstats.num_zeros = len(nums) - np.count_nonzero(nums)\n\n              nums = np.array(nums)\n              num_nan = len(nums[np.isnan(nums)])\n              num_posinf = len(nums[np.isposinf(nums)])\n              num_neginf = len(nums[np.isneginf(nums)])\n\n              # Remove all non-finite (including NaN) values from the numeric\n              # values in order to calculate histogram buckets/counts. The\n              # inf values will be added back to the first and last buckets.\n              nums = nums[np.isfinite(nums)]\n              counts, buckets = np.histogram(nums)\n              hist = featstats.histograms.add()\n              hist.type = self.histogram_proto.STANDARD\n              hist.num_nan = num_nan\n              for bucket_count in range(len(counts)):\n                bucket = hist.buckets.add(\n                    low_value=buckets[bucket_count],\n                    high_value=buckets[bucket_count + 1],\n                    sample_count=np.asscalar(counts[bucket_count]))\n                # Add any negative or positive infinities to the first and last\n                # buckets in the histogram.\n                if bucket_count == 0 and num_neginf > 0:\n                  bucket.low_value = float('-inf')\n                  bucket.sample_count += num_neginf\n                elif bucket_count == len(counts) - 1 and num_posinf > 0:\n                  bucket.high_value = float('inf')\n                  bucket.sample_count += num_posinf\n              if not hist.buckets:\n                if num_neginf:\n                  hist.buckets.add(\n                      low_value=float('-inf'),\n                      high_value=float('-inf'),\n                      sample_count=num_neginf)\n                if num_posinf:\n                  hist.buckets.add(\n                      low_value=float('inf'),\n                      high_value=float('inf'),\n                      sample_count=num_posinf)\n              self._PopulateQuantilesHistogram(featstats.histograms.add(),\n                                               nums.tolist())\n          elif feat.type == self.fs_proto.STRING:\n            featstats = feat.string_stats\n            commonstats = featstats.common_stats\n            if has_data:\n              strs = []\n              for item in value['vals']:\n                strs.append(item if hasattr(item, '__len__') else str(item))\n\n              featstats.avg_length = np.mean(np.vectorize(len)(strs))\n              vals, counts = np.unique(strs, return_counts=True)\n              featstats.unique = len(vals)\n              sorted_vals = sorted(zip(counts, vals), reverse=True)\n              for val_index, val in enumerate(sorted_vals):\n                if val[1].dtype.type is np.str_:\n                  printable_val = val[1]\n                else:\n                  try:\n                    printable_val = val[1].decode('UTF-8', 'strict')\n                  except (UnicodeDecodeError, UnicodeEncodeError):\n                    printable_val = '__BYTES_VALUE__'\n                bucket = featstats.rank_histogram.buckets.add(\n                    low_rank=val_index,\n                    high_rank=val_index,\n                    sample_count=np.asscalar(val[0]),\n                    label=printable_val)\n                if val_index < 2:\n                  featstats.top_values.add(\n                      value=bucket.label, frequency=bucket.sample_count)\n          # Add the common stats regardless of the feature type.\n          if has_data:\n            commonstats.num_missing = value['missing']\n            commonstats.num_non_missing = (all_datasets.datasets[j].num_examples\n                                           - featstats.common_stats.num_missing)\n            commonstats.min_num_values = int(np.min(value['counts']).astype(int))\n            commonstats.max_num_values = int(np.max(value['counts']).astype(int))\n            commonstats.avg_num_values = np.mean(value['counts'])\n            if 'feat_lens' in value and value['feat_lens']:\n              self._PopulateQuantilesHistogram(\n                  commonstats.feature_list_length_histogram, value['feat_lens'])\n            self._PopulateQuantilesHistogram(commonstats.num_values_histogram,\n                                             value['counts'])\n          else:\n            commonstats.num_non_missing = 0\n            commonstats.num_missing = all_datasets.datasets[j].num_examples\n\n    return all_datasets"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _PopulateQuantilesHistogram(self, hist, nums):\n    if not nums:\n      return\n    num_quantile_buckets = 10\n    quantiles_to_get = [\n        x * 100 / num_quantile_buckets for x in range(num_quantile_buckets + 1)\n    ]\n    quantiles = np.percentile(nums, quantiles_to_get)\n    hist.type = self.histogram_proto.QUANTILES\n    quantiles_sample_count = float(len(nums)) / num_quantile_buckets\n    for low, high in zip(quantiles, quantiles[1:]):\n      hist.buckets.add(\n          low_value=low, high_value=high, sample_count=quantiles_sample_count)", "response": "Fills in the histogram with quantile information from the provided array."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the input features from a placeholder csv string tensor.", "response": "def serving_from_csv_input(train_config, args, keep_target):\n  \"\"\"Read the input features from a placeholder csv string tensor.\"\"\"\n  examples = tf.placeholder(\n      dtype=tf.string,\n      shape=(None,),\n      name='csv_input_string')\n\n  features = parse_example_tensor(examples=examples,\n                                  train_config=train_config,\n                                  keep_target=keep_target)\n\n  if keep_target:\n    target = features.pop(train_config['target_column'])\n  else:\n    target = None\n  features, target = preprocess_input(\n      features=features,\n      target=target,\n      train_config=train_config,\n      preprocess_output_dir=args.preprocess_output_dir,\n      model_type=args.model_type)\n\n  return input_fn_utils.InputFnOps(features,\n                                   target,\n                                   {'csv_line': examples}\n                                   )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the csv files and return a dict of feature_name to tensor.", "response": "def parse_example_tensor(examples, train_config, keep_target):\n  \"\"\"Read the csv files.\n\n  Args:\n    examples: string tensor\n    train_config: training config\n    keep_target: if true, the target column is expected to exist and it is\n        returned in the features dict.\n\n  Returns:\n    Dict of feature_name to tensor. Target feature is in the dict.\n  \"\"\"\n\n  csv_header = []\n  if keep_target:\n    csv_header = train_config['csv_header']\n  else:\n    csv_header = [name for name in train_config['csv_header']\n                  if name != train_config['target_column']]\n\n  # record_defaults are used by tf.decode_csv to insert defaults, and to infer\n  # the datatype.\n  record_defaults = [[train_config['csv_defaults'][name]]\n                     for name in csv_header]\n  tensors = tf.decode_csv(examples, record_defaults, name='csv_to_tensors')\n\n  # I'm not really sure why expand_dims needs to be called. If using regression\n  # models, it errors without it.\n  tensors = [tf.expand_dims(x, axis=1) for x in tensors]\n\n  tensor_dict = dict(zip(csv_header, tensors))\n  return tensor_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_examples(input_files, batch_size, shuffle, num_epochs=None):\n  files = []\n  for e in input_files:\n    for path in e.split(','):\n      files.extend(file_io.get_matching_files(path))\n  thread_count = multiprocessing.cpu_count()\n\n  # The minimum number of instances in a queue from which examples are drawn\n  # randomly. The larger this number, the more randomness at the expense of\n  # higher memory requirements.\n  min_after_dequeue = 1000\n\n  # When batching data, the queue's capacity will be larger than the batch_size\n  # by some factor. The recommended formula is (num_threads + a small safety\n  # margin). For now, we use a single thread for reading, so this can be small.\n  queue_size_multiplier = thread_count + 3\n\n  # Convert num_epochs == 0 -> num_epochs is None, if necessary\n  num_epochs = num_epochs or None\n\n  # Build a queue of the filenames to be read.\n  filename_queue = tf.train.string_input_producer(files, num_epochs, shuffle)\n\n  example_id, encoded_example = tf.TextLineReader().read_up_to(\n      filename_queue, batch_size)\n\n  if shuffle:\n    capacity = min_after_dequeue + queue_size_multiplier * batch_size\n    return tf.train.shuffle_batch(\n        [example_id, encoded_example],\n        batch_size,\n        capacity,\n        min_after_dequeue,\n        enqueue_many=True,\n        num_threads=thread_count)\n\n  else:\n    capacity = queue_size_multiplier * batch_size\n    return tf.train.batch(\n        [example_id, encoded_example],\n        batch_size,\n        capacity=capacity,\n        enqueue_many=True,\n        num_threads=thread_count)", "response": "Reads examples from a list of input files and returns a list of examples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_estimator(output_dir, train_config, args):\n\n  # Check the requested mode fits the preprocessed data.\n  target_name = train_config['target_column']\n  if is_classification_model(args.model_type) and target_name not in \\\n          train_config['categorical_columns']:\n    raise ValueError('When using a classification model, the target must be a '\n                     'categorical variable.')\n  if is_regression_model(args.model_type) and target_name not in \\\n          train_config['numerical_columns']:\n    raise ValueError('When using a regression model, the target must be a '\n                     'numerical variable.')\n\n  # Check layers used for dnn models.\n  if is_dnn_model(args.model_type) and not args.layer_sizes:\n    raise ValueError('--layer-size* must be used with DNN models')\n  if is_linear_model(args.model_type) and args.layer_sizes:\n    raise ValueError('--layer-size* cannot be used with linear models')\n\n  # Build tf.learn features\n  feature_columns = _tflearn_features(train_config, args)\n\n  # Set how often to run checkpointing in terms of time.\n  config = tf.contrib.learn.RunConfig(\n      save_checkpoints_secs=args.save_checkpoints_secs)\n\n  train_dir = os.path.join(output_dir, 'train')\n  if args.model_type == 'dnn_regression':\n    estimator = tf.contrib.learn.DNNRegressor(\n        feature_columns=feature_columns,\n        hidden_units=args.layer_sizes,\n        config=config,\n        model_dir=train_dir,\n        optimizer=tf.train.AdamOptimizer(\n            args.learning_rate, epsilon=args.epsilon))\n  elif args.model_type == 'linear_regression':\n    estimator = tf.contrib.learn.LinearRegressor(\n        feature_columns=feature_columns,\n        config=config,\n        model_dir=train_dir,\n        optimizer=tf.train.AdamOptimizer(\n            args.learning_rate, epsilon=args.epsilon))\n  elif args.model_type == 'dnn_classification':\n    estimator = tf.contrib.learn.DNNClassifier(\n        feature_columns=feature_columns,\n        hidden_units=args.layer_sizes,\n        n_classes=train_config['vocab_stats'][target_name]['n_classes'],\n        config=config,\n        model_dir=train_dir,\n        optimizer=tf.train.AdamOptimizer(\n            args.learning_rate, epsilon=args.epsilon))\n  elif args.model_type == 'linear_classification':\n    estimator = tf.contrib.learn.LinearClassifier(\n        feature_columns=feature_columns,\n        n_classes=train_config['vocab_stats'][target_name]['n_classes'],\n        config=config,\n        model_dir=train_dir,\n        optimizer=tf.train.AdamOptimizer(\n            args.learning_rate, epsilon=args.epsilon))\n  else:\n    raise ValueError('bad --model-type value')\n\n  return estimator", "response": "Returns a TF learn estimator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preprocess_input(features, target, train_config, preprocess_output_dir,\n                     model_type):\n  \"\"\"Perform some transformations after reading in the input tensors.\n\n  Args:\n    features: dict of feature_name to tensor\n    target: tensor\n    train_config: our training config object\n    preprocess_output_dir: folder should contain the vocab files.\n    model_type: the tf model type.\n\n  Raises:\n    ValueError: if wrong transforms are used\n\n  Returns:\n    New features dict and new target tensor.\n  \"\"\"\n\n  target_name = train_config['target_column']\n  key_name = train_config['key_column']\n\n  # Do the numerical transforms.\n  # Numerical transforms supported for regression/classification\n  # 1) num -> do nothing (identity, default)\n  # 2) num -> scale to -1, 1 (scale)\n  # 3) num -> scale to -a, a (scale with value parameter)\n  with tf.name_scope('numerical_feature_preprocess'):\n    if train_config['numerical_columns']:\n      numerical_analysis_file = os.path.join(preprocess_output_dir,\n                                             NUMERICAL_ANALYSIS)\n      if not file_io.file_exists(numerical_analysis_file):\n        raise ValueError('File %s not found in %s' %\n                         (NUMERICAL_ANALYSIS, preprocess_output_dir))\n\n      numerical_anlysis = json.loads(\n          python_portable_string(\n              file_io.read_file_to_string(numerical_analysis_file)))\n\n      for name in train_config['numerical_columns']:\n        if name == target_name or name == key_name:\n          continue\n\n        transform_config = train_config['transforms'].get(name, {})\n        transform_name = transform_config.get('transform', None)\n        if transform_name == 'scale':\n          value = float(transform_config.get('value', 1.0))\n          features[name] = _scale_tensor(\n              features[name],\n              range_min=numerical_anlysis[name]['min'],\n              range_max=numerical_anlysis[name]['max'],\n              scale_min=-value,\n              scale_max=value)\n        elif transform_name == 'identity' or transform_name is None:\n          pass\n        else:\n          raise ValueError(('For numerical variables, only scale '\n                            'and identity are supported: '\n                            'Error for %s') % name)\n\n  # Do target transform if it exists.\n  if target is not None:\n    with tf.name_scope('target_feature_preprocess'):\n      if target_name in train_config['categorical_columns']:\n        labels = train_config['vocab_stats'][target_name]['labels']\n        table = tf.contrib.lookup.string_to_index_table_from_tensor(labels)\n        target = table.lookup(target)\n        # target = tf.contrib.lookup.string_to_index(target, labels)\n\n  # Do categorical transforms. Only apply vocab mapping. The real\n  # transforms are done with tf learn column features.\n  with tf.name_scope('categorical_feature_preprocess'):\n    for name in train_config['categorical_columns']:\n      if name == key_name or name == target_name:\n        continue\n      transform_config = train_config['transforms'].get(name, {})\n      transform_name = transform_config.get('transform', None)\n\n      if is_dnn_model(model_type):\n        if transform_name == 'embedding' or transform_name == 'one_hot' or transform_name is None:\n          map_vocab = True\n        else:\n          raise ValueError('Unknown transform %s' % transform_name)\n      elif is_linear_model(model_type):\n        if (transform_name == 'one_hot' or transform_name is None):\n          map_vocab = True\n        elif transform_name == 'embedding':\n          map_vocab = False\n        else:\n          raise ValueError('Unknown transform %s' % transform_name)\n      if map_vocab:\n        labels = train_config['vocab_stats'][name]['labels']\n        table = tf.contrib.lookup.string_to_index_table_from_tensor(labels)\n        features[name] = table.lookup(features[name])\n\n  return features, target", "response": "Perform some transformations after reading in the input tensors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscaling a tensor to scale_min to scale_max.", "response": "def _scale_tensor(tensor, range_min, range_max, scale_min, scale_max):\n  \"\"\"Scale a tensor to scale_min to scale_max.\n\n  Args:\n    tensor: input tensor. Should be a numerical tensor.\n    range_min: min expected value for this feature/tensor.\n    range_max: max expected Value.\n    scale_min: new expected min value.\n    scale_max: new expected max value.\n\n  Returns:\n    scaled tensor.\n  \"\"\"\n  if range_min == range_max:\n    return tensor\n\n  float_tensor = tf.to_float(tensor)\n  scaled_tensor = tf.divide((tf.subtract(float_tensor, range_min) *\n                             tf.constant(float(scale_max - scale_min))),\n                            tf.constant(float(range_max - range_min)))\n  shifted_tensor = scaled_tensor + tf.constant(float(scale_min))\n\n  return shifted_tensor"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the tf. learn feature list.", "response": "def _tflearn_features(train_config, args):\n  \"\"\"Builds the tf.learn feature list.\n\n  All numerical features are just given real_valued_column because all the\n  preprocessing transformations are done in preprocess_input. Categoriacl\n  features are processed here depending if the vocab map (from string to int)\n  was applied in preprocess_input.\n\n  Args:\n    train_config: our train config object\n    args: command line args.\n\n  Returns:\n    List of TF lean feature columns.\n\n  Raises:\n    ValueError: if wrong transforms are used for the model type.\n  \"\"\"\n  feature_columns = []\n  target_name = train_config['target_column']\n  key_name = train_config['key_column']\n\n  for name in train_config['numerical_columns']:\n    if name != target_name and name != key_name:\n      feature_columns.append(tf.contrib.layers.real_valued_column(\n          name,\n          dimension=1))\n\n      # Supported transforms:\n      # for DNN\n      # 1) string -> make int -> embedding (embedding)\n      # 2) string -> make int -> one_hot (one_hot, default)\n      # for linear\n      # 1) string -> sparse_column_with_hash_bucket (embedding)\n      # 2) string -> make int -> sparse_column_with_integerized_feature (one_hot, default)\n      # It is unfortunate that tf.layers has different feature transforms if the\n      # model is linear or DNN. This pacakge should not expose to the user that\n      # we are using tf.layers. It is crazy that DNN models support more feature\n      # types (like string -> hash sparse column -> embedding)\n  for name in train_config['categorical_columns']:\n    if name != target_name and name != key_name:\n      transform_config = train_config['transforms'].get(name, {})\n      transform_name = transform_config.get('transform', None)\n\n      if is_dnn_model(args.model_type):\n        if transform_name == 'embedding':\n          sparse = tf.contrib.layers.sparse_column_with_integerized_feature(\n              name,\n              bucket_size=train_config['vocab_stats'][name]['n_classes'])\n          learn_feature = tf.contrib.layers.embedding_column(\n              sparse,\n              dimension=transform_config['embedding_dim'])\n        elif transform_name == 'one_hot' or transform_name is None:\n          sparse = tf.contrib.layers.sparse_column_with_integerized_feature(\n              name,\n              bucket_size=train_config['vocab_stats'][name]['n_classes'])\n          learn_feature = tf.contrib.layers.one_hot_column(sparse)\n        else:\n          raise ValueError(('Unknown transform name. Only \\'embedding\\' '\n                            'and \\'one_hot\\' transforms are supported. Got %s')\n                           % transform_name)\n      elif is_linear_model(args.model_type):\n        if transform_name == 'one_hot' or transform_name is None:\n          learn_feature = tf.contrib.layers.sparse_column_with_integerized_feature(\n              name,\n              bucket_size=train_config['vocab_stats'][name]['n_classes'])\n        elif transform_name == 'embedding':\n          learn_feature = tf.contrib.layers.sparse_column_with_hash_bucket(\n              name,\n              hash_bucket_size=transform_config['embedding_dim'])\n        else:\n          raise ValueError(('Unknown transform name. Only \\'embedding\\' '\n                            'and \\'one_hot\\' transforms are supported. Got %s')\n                           % transform_name)\n\n      # Save the feature\n      feature_columns.append(learn_feature)\n  return feature_columns"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the vocabulary file as a list of strings.", "response": "def get_vocabulary(preprocess_output_dir, name):\n  \"\"\"Loads the vocabulary file as a list of strings.\n\n  Args:\n    preprocess_output_dir: Should contain the file CATEGORICAL_ANALYSIS % name.\n    name: name of the csv column.\n\n  Returns:\n    List of strings.\n\n  Raises:\n    ValueError: if file is missing.\n  \"\"\"\n  vocab_file = os.path.join(preprocess_output_dir, CATEGORICAL_ANALYSIS % name)\n  if not file_io.file_exists(vocab_file):\n    raise ValueError('File %s not found in %s' %\n                     (CATEGORICAL_ANALYSIS % name, preprocess_output_dir))\n\n  labels = python_portable_string(\n      file_io.read_file_to_string(vocab_file)).split('\\n')\n  label_values = [x for x in labels if x]  # remove empty lines\n\n  return label_values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges schema analysis and transforms files into one python object.", "response": "def merge_metadata(preprocess_output_dir, transforms_file):\n  \"\"\"Merge schema, analysis, and transforms files into one python object.\n\n  Args:\n    preprocess_output_dir: the output folder of preprocessing. Should contain\n        the schema, and the numerical and categorical\n        analysis files.\n    transforms_file: the training transforms file.\n\n  Returns:\n    A dict in the form\n    {\n      csv_header: [name1, name2, ...],\n      csv_defaults: {name1: value, name2: value},\n      key_column: name,\n      target_column: name,\n      categorical_columns: []\n      numerical_columns: []\n      transforms: { name1: {transform: scale, value: 2},\n                    name2: {transform: embedding, dim: 50}, ...\n                  }\n      vocab_stats: { name3: {n_classes: 23, labels: ['1', '2', ..., '23']},\n                     name4: {n_classes: 102, labels: ['red', 'blue', ...]}}\n    }\n\n  Raises:\n    ValueError: if one of the input metadata files is wrong.\n  \"\"\"\n  numerical_anlysis_file = os.path.join(preprocess_output_dir,\n                                        NUMERICAL_ANALYSIS)\n  schema_file = os.path.join(preprocess_output_dir, SCHEMA_FILE)\n\n  numerical_anlysis = json.loads(\n      python_portable_string(\n          file_io.read_file_to_string(numerical_anlysis_file)))\n  schema = json.loads(\n      python_portable_string(file_io.read_file_to_string(schema_file)))\n  transforms = json.loads(\n      python_portable_string(file_io.read_file_to_string(transforms_file)))\n\n  result_dict = {}\n  result_dict['csv_header'] = [col_schema['name'] for col_schema in schema]\n  result_dict['key_column'] = None\n  result_dict['target_column'] = None\n  result_dict['categorical_columns'] = []\n  result_dict['numerical_columns'] = []\n  result_dict['transforms'] = {}\n  result_dict['csv_defaults'] = {}\n  result_dict['vocab_stats'] = {}\n\n  # get key column.\n  for name, trans_config in six.iteritems(transforms):\n    if trans_config.get('transform', None) == 'key':\n      result_dict['key_column'] = name\n      break\n  if result_dict['key_column'] is None:\n    raise ValueError('Key transform missing form transfroms file.')\n\n  # get target column.\n  result_dict['target_column'] = schema[0]['name']\n  for name, trans_config in six.iteritems(transforms):\n    if trans_config.get('transform', None) == 'target':\n      result_dict['target_column'] = name\n      break\n  if result_dict['target_column'] is None:\n    raise ValueError('Target transform missing from transforms file.')\n\n  # Get the numerical/categorical columns.\n  for col_schema in schema:\n    col_name = col_schema['name']\n    col_type = col_schema['type'].lower()\n    if col_name == result_dict['key_column']:\n      continue\n\n    if col_type == 'string':\n      result_dict['categorical_columns'].append(col_name)\n    elif col_type == 'integer' or col_type == 'float':\n      result_dict['numerical_columns'].append(col_name)\n    else:\n      raise ValueError('Unsupported schema type %s' % col_type)\n\n  # Get the transforms.\n  for name, trans_config in six.iteritems(transforms):\n    if name != result_dict['target_column'] and name != result_dict['key_column']:\n      result_dict['transforms'][name] = trans_config\n\n  # Get the vocab_stats\n  for name in result_dict['categorical_columns']:\n    if name == result_dict['key_column']:\n      continue\n\n    label_values = get_vocabulary(preprocess_output_dir, name)\n    if name != result_dict['target_column'] and '' not in label_values:\n      label_values.append('')  # append a 'missing' label.\n    n_classes = len(label_values)\n    result_dict['vocab_stats'][name] = {'n_classes': n_classes,\n                                        'labels': label_values}\n\n  # Get the csv_defaults\n  for col_schema in schema:\n    name = col_schema['name']\n    col_type = col_schema['type'].lower()\n    default = transforms.get(name, {}).get('default', None)\n\n    if name == result_dict['target_column']:\n      if name in result_dict['numerical_columns']:\n        default = float(default or 0.0)\n      else:\n        default = default or ''\n    elif name == result_dict['key_column']:\n      if col_type == 'string':\n        default = str(default or '')\n      elif col_type == 'float':\n        default = float(default or 0.0)\n      else:\n        default = int(default or 0)\n    else:\n      if col_type == 'string':\n        default = str(default or '')\n        if default not in result_dict['vocab_stats'][name]['labels']:\n          raise ValueError('Default %s is not in the vocab for %s' %\n                           (default, name))\n      else:\n        default = float(default or numerical_anlysis[name]['mean'])\n\n    result_dict['csv_defaults'][name] = default\n\n  validate_metadata(result_dict)\n  return result_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_metadata(train_config):\n\n  # Make sure we have a default for every column\n  if len(train_config['csv_header']) != len(train_config['csv_defaults']):\n    raise ValueError('Unequal number of columns in input features file and '\n                     'schema file.')\n\n  # Check there are no missing columns. sorted_colums has two copies of the\n  # target column because the target column is also listed in\n  # categorical_columns or numerical_columns.\n  sorted_columns = sorted(train_config['csv_header'] +\n                          [train_config['target_column']])\n\n  sorted_columns2 = sorted(train_config['categorical_columns'] +\n                           train_config['numerical_columns'] +\n                           [train_config['key_column']] +\n                           [train_config['target_column']])\n  if sorted_columns2 != sorted_columns:\n    raise ValueError('Each csv header must be a numerical/categorical type, a '\n                     ' key, or a target.')", "response": "Perform some checks that the trainig config is correct."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_id(credentials=None):\n    project_id = _utils.get_project_id()\n    if project_id is None:\n      projects, _ = Projects(credentials)._retrieve_projects(None, 2)\n      if len(projects) == 1:\n        project_id = projects[0].id\n    return project_id", "response": "Get default project id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_app(state):\n    app = state.app\n\n    app.config.setdefault('SPLIT_ALLOW_MULTIPLE_EXPERIMENTS', False)\n    app.config.setdefault('SPLIT_DB_FAILOVER', False)\n    app.config.setdefault('SPLIT_IGNORE_IP_ADDRESSES', [])\n    app.config.setdefault('SPLIT_ROBOT_REGEX', r\"\"\"\n        (?i)\\b(\n            Baidu|\n            Gigabot|\n            Googlebot|\n            libwww-perl|\n            lwp-trivial|\n            msnbot|\n            SiteUptime|\n            Slurp|\n            WordPress|\n            ZIBB|\n            ZyBorg\n        )\\b\n    \"\"\")\n\n    app.jinja_env.globals.update({\n        'ab_test': ab_test,\n        'finished': finished\n    })\n\n    @app.template_filter()\n    def percentage(number):\n        number *= 100\n        if abs(number) < 10:\n            return \"%.1f%%\" % round(number, 1)\n        else:\n            return \"%d%%\" % round(number)", "response": "Initializes the Flask application for Flask - Split."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntracking a conversion. :param experiment_name: Name of the experiment. :param reset: If set to `True` current user's session is reset so that they may start the test again in the future. If set to `False` the user will always see the alternative they started with. Defaults to `True`.", "response": "def finished(experiment_name, reset=True):\n    \"\"\"\n    Track a conversion.\n\n    :param experiment_name: Name of the experiment.\n    :param reset: If set to `True` current user's session is reset so that they\n        may start the test again in the future.  If set to `False` the user\n        will always see the alternative they started with.  Defaults to `True`.\n    \"\"\"\n    if _exclude_visitor():\n        return\n    redis = _get_redis_connection()\n    try:\n        experiment = Experiment.find(redis, experiment_name)\n        if not experiment:\n            return\n        alternative_name = _get_session().get(experiment.key)\n        if alternative_name:\n            split_finished = set(session.get('split_finished', []))\n            if experiment.key not in split_finished:\n                alternative = Alternative(\n                    redis, alternative_name, experiment_name)\n                alternative.increment_completion()\n            if reset:\n                _get_session().pop(experiment.key, None)\n                try:\n                    split_finished.remove(experiment.key)\n                except KeyError:\n                    pass\n            else:\n                split_finished.add(experiment.key)\n            session['split_finished'] = list(split_finished)\n    except ConnectionError:\n        if not current_app.config['SPLIT_DB_FAILOVER']:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the current visitor is a robot or spider.", "response": "def _is_robot():\n    \"\"\"\n    Return `True` if the current visitor is a robot or spider, or\n    `False` otherwise.\n\n    This function works by comparing the request's user agent with a regular\n    expression.  The regular expression can be configured with the\n    ``SPLIT_ROBOT_REGEX`` setting.\n    \"\"\"\n    robot_regex = current_app.config['SPLIT_ROBOT_REGEX']\n    user_agent = request.headers.get('User-Agent', '')\n    return re.search(robot_regex, user_agent, flags=re.VERBOSE)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting all data for this experiment.", "response": "def reset(self):\n        \"\"\"Delete all data for this experiment.\"\"\"\n        for alternative in self.alternatives:\n            alternative.reset()\n        self.reset_winner()\n        self.increment_version()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self):\n        for alternative in self.alternatives:\n            alternative.delete()\n        self.reset_winner()\n        self.redis.srem('experiments', self.name)\n        self.redis.delete(self.name)\n        self.increment_version()", "response": "Delete this experiment and all its data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_redis_connection():\n    url = current_app.config.get('REDIS_URL', 'redis://localhost:6379')\n    return redis.from_url(url, decode_responses=True)", "response": "Returns a Redis connection based on the Flask application s configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_experiment_winner(experiment):\n    redis = _get_redis_connection()\n    experiment = Experiment.find(redis, experiment)\n    if experiment:\n        alternative_name = request.form.get('alternative')\n        alternative = Alternative(redis, alternative_name, experiment.name)\n        if alternative.name in experiment.alternative_names:\n            experiment.winner = alternative.name\n    return redirect(url_for('.index'))", "response": "Mark an alternative as the winner of the experiment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset_experiment(experiment):\n    redis = _get_redis_connection()\n    experiment = Experiment.find(redis, experiment)\n    if experiment:\n        experiment.reset()\n    return redirect(url_for('.index'))", "response": "Delete all data for an experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_experiment(experiment):\n    redis = _get_redis_connection()\n    experiment = Experiment.find(redis, experiment)\n    if experiment:\n        experiment.delete()\n    return redirect(url_for('.index'))", "response": "Delete an experiment and all its data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving node configuration to tmp_node. json", "response": "def save_config(node, force=False):\n    \"\"\"Saves node configuration\n    if no nodes/hostname.json exists, or force=True, it creates one\n    it also saves to tmp_node.json\n\n    \"\"\"\n    filepath = os.path.join(\"nodes\", env.host_string + \".json\")\n    tmp_filename = 'tmp_{0}.json'.format(env.host_string)\n    files_to_create = [tmp_filename]\n    if not os.path.exists(filepath) or force:\n        # Only save to nodes/ if there is not already a file\n        print \"Saving node configuration to {0}...\".format(filepath)\n        files_to_create.append(filepath)\n    for node_file in files_to_create:\n        with open(node_file, 'w') as f:\n            f.write(json.dumps(node, indent=4, sort_keys=True))\n    return tmp_filename"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_ipaddress(node):\n    if \"ipaddress\" not in node:\n        with settings(hide('stdout'), warn_only=True):\n            output = sudo('ohai -l warn ipaddress')\n        if output.succeeded:\n            try:\n                node['ipaddress'] = json.loads(output)[0]\n            except ValueError:\n                abort(\"Could not parse ohai's output for ipaddress\"\n                      \":\\n  {0}\".format(output))\n            return True\n    return False", "response": "Adds the ipaddress attribute to the given node object if not already present and is correctly given by ohai\n    Returns True if ipaddress is added False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sync_node(node):\n    if node.get('dummy') or 'dummy' in node.get('tags', []):\n        lib.print_header(\"Skipping dummy: {0}\".format(env.host))\n        return False\n    current_node = lib.get_node(node['name'])\n    # Always configure Chef Solo\n    solo.configure(current_node)\n    ipaddress = _get_ipaddress(node)\n    # Everything was configured alright, so save the node configuration\n    # This is done without credentials, so that we keep the node name used\n    # by the user and not the hostname or IP translated by .ssh/config\n    filepath = save_config(node, ipaddress)\n    try:\n        # Synchronize the kitchen directory\n        _synchronize_node(filepath, node)\n        # Execute Chef Solo\n        _configure_node()\n    finally:\n        _node_cleanup()\n    return True", "response": "Builds synchronizes and configures a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms the Synchronize step of a Chef run: Uploads all cookbooks, all roles and all databags to a node and add the patch for data bags Returns the node object of the node which is about to be configured, or None if this node object cannot be found.", "response": "def _synchronize_node(configfile, node):\n    \"\"\"Performs the Synchronize step of a Chef run:\n    Uploads all cookbooks, all roles and all databags to a node and add the\n    patch for data bags\n\n    Returns the node object of the node which is about to be configured,\n    or None if this node object cannot be found.\n\n    \"\"\"\n    msg = \"Synchronizing nodes, environments, roles, cookbooks and data bags...\"\n    if env.parallel:\n        msg = \"[{0}]: {1}\".format(env.host_string, msg)\n    print(msg)\n    # First upload node.json\n    remote_file = '/etc/chef/node.json'\n    put(configfile, remote_file, use_sudo=True, mode=400)\n    with hide('stdout'):\n        sudo('chown root:$(id -g -n root) {0}'.format(remote_file))\n    # Remove local temporary node file\n    os.remove(configfile)\n    # Synchronize kitchen\n    extra_opts = \"-q\"\n    if env.follow_symlinks:\n        extra_opts += \" --copy-links\"\n    ssh_opts = \"\"\n    if env.ssh_config_path:\n        ssh_opts += \" -F %s\" % os.path.expanduser(env.ssh_config_path)\n    if env.encrypted_data_bag_secret:\n        put(env.encrypted_data_bag_secret,\n            \"/etc/chef/encrypted_data_bag_secret\",\n            use_sudo=True,\n            mode=0600)\n        sudo('chown root:$(id -g -n root) /etc/chef/encrypted_data_bag_secret')\n\n    paths_to_sync = ['./data_bags', './roles', './environments']\n    for cookbook_path in cookbook_paths:\n        paths_to_sync.append('./{0}'.format(cookbook_path))\n\n    # Add berksfile directory to sync_list\n    if env.berksfile:\n        paths_to_sync.append(env.berksfile_cookbooks_directory)\n\n    if env.loglevel is \"debug\":\n        extra_opts = \"\"\n\n    if env.gateway:\n        ssh_key_file = '.ssh/' + os.path.basename(' '.join(env.ssh_config.lookup(\n            env.host_string)['identityfile']))\n        ssh_opts += \" \" + env.gateway + \" ssh -o StrictHostKeyChecking=no -i \"\n        ssh_opts += ssh_key_file\n\n    rsync_project(\n        env.node_work_path,\n        ' '.join(paths_to_sync),\n        exclude=('*.svn', '.bzr*', '.git*', '.hg*'),\n        delete=True,\n        extra_opts=extra_opts,\n        ssh_opts=ssh_opts\n    )\n\n    if env.sync_packages_dest_dir and env.sync_packages_local_dir:\n        print(\"Uploading packages from {0} to remote server {2} directory \"\n              \"{1}\").format(env.sync_packages_local_dir,\n                            env.sync_packages_dest_dir, env.host_string)\n        try:\n            rsync_project(\n              env.sync_packages_dest_dir,\n              env.sync_packages_local_dir+\"/*\",\n              exclude=('*.svn', '.bzr*', '.git*', '.hg*'),\n              delete=True,\n              extra_opts=extra_opts,\n              ssh_opts=ssh_opts\n            )\n        except:\n            print(\"Warning: package upload failed. Continuing cooking...\")\n\n    _add_environment_lib()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a dictionary with arbitrary depth out of a key list", "response": "def build_dct(dic, keys, value):\n    \"\"\"Builds a dictionary with arbitrary depth out of a key list\"\"\"\n    key = keys.pop(0)\n    if len(keys):\n        dic.setdefault(key, {})\n        build_dct(dic[key], keys, value)\n    else:\n        # Transform cookbook default attribute strings into proper booleans\n        if value == \"false\":\n            value = False\n        elif value == \"true\":\n            value = True\n        # It's a leaf, assign value\n        dic[key] = deepcopy(value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging two dictionaries recursively dic1 will have preference over dic2", "response": "def update_dct(dic1, dic2):\n    \"\"\"Merges two dictionaries recursively\n    dic2 will have preference over dic1\n\n    \"\"\"\n    for key, val in dic2.items():\n        if isinstance(val, dict):\n            dic1.setdefault(key, {})\n            update_dct(dic1[key], val)\n        else:\n            dic1[key] = val"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd some of Chef s automatic attributes to the node.", "response": "def _add_automatic_attributes(node):\n    \"\"\"Adds some of Chef's automatic attributes:\n        http://wiki.opscode.com/display/chef/Recipes#Recipes\n        -CommonAutomaticAttributes\n\n    \"\"\"\n    node['fqdn'] = node['name']\n    node['hostname'] = node['fqdn'].split('.')[0]\n    node['domain'] = \".\".join(node['fqdn'].split('.')[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge attributes from cookbooks node and roles and returns the merged attributes dict", "response": "def _add_merged_attributes(node, all_recipes, all_roles):\n    \"\"\"Merges attributes from cookbooks, node and roles\n\n    Chef Attribute precedence:\n    http://docs.opscode.com/essentials_cookbook_attribute_files.html#attribute-precedence\n    LittleChef implements, in precedence order:\n        - Cookbook default\n        - Environment default\n        - Role default\n        - Node normal\n        - Role override\n        - Environment override\n\n    NOTE: In order for cookbook attributes to be read, they need to be\n        correctly defined in its metadata.json\n\n    \"\"\"\n    # Get cookbooks from extended recipes\n    attributes = {}\n    for recipe in node['recipes']:\n        # Find this recipe\n        found = False\n        for r in all_recipes:\n            if recipe == r['name']:\n                found = True\n                for attr in r['attributes']:\n                    if r['attributes'][attr].get('type') == \"hash\":\n                        value = {}\n                    else:\n                        value = r['attributes'][attr].get('default')\n                    # Attribute dictionaries are defined as a single\n                    # compound key. Split and build proper dict\n                    build_dct(attributes, attr.split(\"/\"), value)\n        if not found:\n            error = \"Could not find recipe '{0}' while \".format(recipe)\n            error += \"building node data bag for '{0}'\".format(node['name'])\n            abort(error)\n\n    # Get default role attributes\n    for role in node['roles']:\n        for r in all_roles:\n            if role == r['name']:\n                update_dct(attributes, r.get('default_attributes', {}))\n\n    # Get default environment attributes\n    environment = lib.get_environment(node['chef_environment'])\n    update_dct(attributes, environment.get('default_attributes', {}))\n\n    # Get normal node attributes\n    non_attribute_fields = [\n        'id', 'name', 'role', 'roles', 'recipes', 'run_list', 'ipaddress']\n    node_attributes = {}\n    for key in node:\n        if key in non_attribute_fields:\n            continue\n        node_attributes[key] = node[key]\n    update_dct(attributes, node_attributes)\n\n    # Get override role attributes\n    for role in node['roles']:\n        for r in all_roles:\n            if role == r['name']:\n                update_dct(attributes, r.get('override_attributes', {}))\n\n    # Get override environment attributes\n    update_dct(attributes, environment.get('override_attributes', {}))\n\n    # Merge back to the original node object\n    node.update(attributes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a node data bag item for each cookbook.", "response": "def build_node_data_bag():\n    \"\"\"Builds one 'node' data bag item per file found in the 'nodes' directory\n\n    Automatic attributes for a node item:\n        'id': It adds data bag 'id', same as filename but with underscores\n        'name': same as the filename\n        'fqdn': same as the filename (LittleChef filenames should be fqdns)\n        'hostname': Uses the first part of the filename as the hostname\n            (until it finds a period) minus the .json extension\n        'domain': filename minus the first part of the filename (hostname)\n            minus the .json extension\n    In addition, it will contain the merged attributes from:\n        All default cookbook attributes corresponding to the node\n        All attributes found in nodes/<item>.json file\n        Default and override attributes from all roles\n\n    \"\"\"\n    nodes = lib.get_nodes()\n    node_data_bag_path = os.path.join('data_bags', 'node')\n    # In case there are leftovers\n    remove_local_node_data_bag()\n    os.makedirs(node_data_bag_path)\n    all_recipes = lib.get_recipes()\n    all_roles = lib.get_roles()\n    for node in nodes:\n        # Dots are not allowed (only alphanumeric), substitute by underscores\n        node['id'] = node['name'].replace('.', '_')\n\n        # Build extended role list\n        node['role'] = lib.get_roles_in_node(node)\n        node['roles'] = node['role'][:]\n        for role in node['role']:\n            node['roles'].extend(lib.get_roles_in_role(role))\n        node['roles'] = list(set(node['roles']))\n\n        # Build extended recipe list\n        node['recipes'] = lib.get_recipes_in_node(node)\n        # Add recipes found inside each roles in the extended role list\n        for role in node['roles']:\n            node['recipes'].extend(lib.get_recipes_in_role(role))\n        node['recipes'] = list(set(node['recipes']))\n\n        # Add node attributes\n        _add_merged_attributes(node, all_recipes, all_roles)\n        _add_automatic_attributes(node)\n\n        # Save node data bag item\n        with open(os.path.join(\n                  'data_bags', 'node', node['id'] + '.json'), 'w') as f:\n            f.write(json.dumps(node))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_local_node_data_bag():\n    node_data_bag_path = os.path.join('data_bags', 'node')\n    if os.path.exists(node_data_bag_path):\n        shutil.rmtree(node_data_bag_path)", "response": "Removes generated node data_bag locally"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that berks file cookbooks are installed", "response": "def ensure_berksfile_cookbooks_are_installed():\n    \"\"\"Run 'berks vendor' to berksfile cookbooks directory\"\"\"\n    msg = \"Vendoring cookbooks from Berksfile {0} to directory {1}...\"\n    print(msg.format(env.berksfile, env.berksfile_cookbooks_directory))\n\n    run_vendor = True\n    cookbooks_dir = env.berksfile_cookbooks_directory\n    berksfile_lock_path = cookbooks_dir+'/Berksfile.lock'\n\n    berksfile_lock_exists = os.path.isfile(berksfile_lock_path)\n    cookbooks_dir_exists = os.path.isdir(cookbooks_dir)\n\n    if cookbooks_dir_exists and berksfile_lock_exists:\n        berksfile_mtime = os.stat('Berksfile').st_mtime\n        cookbooks_mtime = os.stat(berksfile_lock_path).st_mtime\n        run_vendor = berksfile_mtime > cookbooks_mtime\n\n    if run_vendor:\n        if cookbooks_dir_exists:\n            shutil.rmtree(env.berksfile_cookbooks_directory)\n\n        p = subprocess.Popen(['berks', 'vendor', env.berksfile_cookbooks_directory],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        if env.verbose or p.returncode:\n            print stdout, stderr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves generated node data_bag from the remote node", "response": "def _remove_remote_node_data_bag():\n    \"\"\"Removes generated 'node' data_bag from the remote node\"\"\"\n    node_data_bag_path = os.path.join(env.node_work_path, 'data_bags', 'node')\n    if exists(node_data_bag_path):\n        sudo(\"rm -rf {0}\".format(node_data_bag_path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves remote data bags so it won t leak any sensitive information", "response": "def _remove_remote_data_bags():\n    \"\"\"Remove remote data bags, so it won't leak any sensitive information\"\"\"\n    data_bags_path = os.path.join(env.node_work_path, 'data_bags')\n    if exists(data_bags_path):\n        sudo(\"rm -rf {0}\".format(data_bags_path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the chef_solo_envs cookbook which provides a library that adds the environment. rb to the node s cookbooks", "response": "def _add_environment_lib():\n    \"\"\"Adds the chef_solo_envs cookbook, which provides a library that adds\n    environment attribute compatibility for chef-solo v10\n    NOTE: Chef 10 only\n\n    \"\"\"\n    # Create extra cookbook dir\n    lib_path = os.path.join(env.node_work_path, cookbook_paths[0],\n                            'chef_solo_envs', 'libraries')\n    with hide('running', 'stdout'):\n        sudo('mkdir -p {0}'.format(lib_path))\n    # Add environment patch to the node's cookbooks\n    put(os.path.join(basedir, 'environment.rb'),\n        os.path.join(lib_path, 'environment.rb'), use_sudo=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning resolved hostname using the ssh config", "response": "def _resolve_hostname(name):\n    \"\"\"Returns resolved hostname using the ssh config\"\"\"\n    if env.ssh_config is None:\n        return name\n    elif not os.path.exists(os.path.join(\"nodes\", name + \".json\")):\n        resolved_name = env.ssh_config.lookup(name)['hostname']\n        if os.path.exists(os.path.join(\"nodes\", resolved_name + \".json\")):\n            name = resolved_name\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_environment(name):\n    if name == \"_default\":\n        return env_from_template(name)\n    filename = os.path.join(\"environments\", name + \".json\")\n    try:\n        with open(filename) as f:\n            try:\n                return json.loads(f.read())\n            except ValueError as e:\n                msg = 'LittleChef found the following error in'\n                msg += ' \"{0}\":\\n                {1}'.format(filename, str(e))\n                abort(msg)\n    except IOError:\n        raise FileNotFoundError('File {0} not found'.format(filename))", "response": "Returns a JSON environment file as a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_environments():\n    envs = []\n    for root, subfolders, files in os.walk('environments'):\n        for filename in files:\n            if filename.endswith(\".json\"):\n                path = os.path.join(\n                    root[len('environments'):], filename[:-len('.json')])\n                envs.append(get_environment(path))\n    return sorted(envs, key=lambda x: x['name'])", "response": "Gets all environments found in the environments directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a JSON node file as a dictionary", "response": "def get_node(name, merged=False):\n    \"\"\"Returns a JSON node file as a dictionary\"\"\"\n    if merged:\n        node_path = os.path.join(\"data_bags\", \"node\", name.replace('.', '_') + \".json\")\n    else:\n        node_path = os.path.join(\"nodes\", name + \".json\")\n    if os.path.exists(node_path):\n        # Read node.json\n        with open(node_path, 'r') as f:\n            try:\n                node = json.loads(f.read())\n            except ValueError as e:\n                msg = 'LittleChef found the following error in'\n                msg += ' \"{0}\":\\n                {1}'.format(node_path, str(e))\n                abort(msg)\n    else:\n        print \"Creating new node file '{0}.json'\".format(name)\n        node = {'run_list': []}\n    # Add node name so that we can tell to which node it is\n    node['name'] = name\n    if not node.get('chef_environment'):\n        node['chef_environment'] = '_default'\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nodes(environment=None):\n    if not os.path.exists('nodes'):\n        return []\n    nodes = []\n    for filename in sorted(\n            [f for f in os.listdir('nodes')\n             if (not os.path.isdir(f)\n                 and f.endswith(\".json\") and not f.startswith('.'))]):\n        fqdn = \".\".join(filename.split('.')[:-1])  # Remove .json from name\n        node = get_node(fqdn)\n        if environment is None or node.get('chef_environment') == environment:\n            nodes.append(node)\n    return nodes", "response": "Gets all nodes found in the nodes directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all nodes which include a given role", "response": "def get_nodes_with_role(role_name, environment=None):\n    \"\"\"Get all nodes which include a given role,\n    prefix-searches are also supported\n\n    \"\"\"\n    prefix_search = role_name.endswith(\"*\")\n    if prefix_search:\n        role_name = role_name.rstrip(\"*\")\n    for n in get_nodes(environment):\n        roles = get_roles_in_node(n, recursive=True)\n        if prefix_search:\n            if any(role.startswith(role_name) for role in roles):\n                yield n\n        else:\n            if role_name in roles:\n                yield n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all nodes which include a given tag", "response": "def get_nodes_with_tag(tag, environment=None, include_guests=False):\n    \"\"\"Get all nodes which include a given tag\"\"\"\n    nodes = get_nodes(environment)\n    nodes_mapping = dict((n['name'], n) for n in nodes)\n    for n in nodes:\n        if tag in n.get('tags', []):\n            # Remove from node mapping so it doesn't get added twice by\n            # guest walking below\n            try:\n                del nodes_mapping[n['fqdn']]\n            except KeyError:\n                pass\n            yield n\n            # Walk guest if it is a host\n            if include_guests and n.get('virtualization', {}).get('role') == 'host':\n                for guest in n['virtualization'].get('guests', []):\n                    try:\n                        yield nodes_mapping[guest['fqdn']]\n                    except KeyError:\n                        # we ignore guests which are not in the same\n                        # chef environments than their hosts for now\n                        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all nodes which include a given recipe", "response": "def get_nodes_with_recipe(recipe_name, environment=None):\n    \"\"\"Get all nodes which include a given recipe,\n    prefix-searches are also supported\n\n    \"\"\"\n    prefix_search = recipe_name.endswith(\"*\")\n    if prefix_search:\n        recipe_name = recipe_name.rstrip(\"*\")\n    for n in get_nodes(environment):\n        recipes = get_recipes_in_node(n)\n        for role in get_roles_in_node(n, recursive=True):\n            recipes.extend(get_recipes_in_role(role))\n        if prefix_search:\n            if any(recipe.startswith(recipe_name) for recipe in recipes):\n                yield n\n        else:\n            if recipe_name in recipes:\n                yield n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint all the given nodes", "response": "def print_nodes(nodes, detailed=False):\n    \"\"\"Prints all the given nodes\"\"\"\n    found = 0\n    for node in nodes:\n        found += 1\n        print_node(node, detailed=detailed)\n    print(\"\\nFound {0} node{1}\".format(found, \"s\" if found != 1 else \"\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates metadata. rb and metadata. json files for a given node.", "response": "def _generate_metadata(path, cookbook_path, name):\n    \"\"\"Checks whether metadata.rb has changed and regenerate metadata.json\"\"\"\n    global knife_installed\n    if not knife_installed:\n        return\n    metadata_path_rb = os.path.join(path, 'metadata.rb')\n    metadata_path_json = os.path.join(path, 'metadata.json')\n    if (os.path.exists(metadata_path_rb) and\n            (not os.path.exists(metadata_path_json) or\n             os.stat(metadata_path_rb).st_mtime >\n             os.stat(metadata_path_json).st_mtime)):\n        error_msg = \"Warning: metadata.json for {0}\".format(name)\n        error_msg += \" in {0} is older that metadata.rb\".format(cookbook_path)\n        error_msg += \", cookbook attributes could be out of date\\n\\n\"\n        try:\n            proc = subprocess.Popen(\n                ['knife', 'cookbook', 'metadata', '-o', cookbook_path, name],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            resp, error = proc.communicate()\n            if ('ERROR:' in resp or 'FATAL:' in resp\n                    or 'Generating metadata for' not in resp):\n                if(\"No user specified, pass via -u or specifiy 'node_name'\"\n                        in error):\n                    error_msg += \"You need to have an up-to-date (>=0.10.x)\"\n                    error_msg += \" version of knife installed locally in order\"\n                    error_msg += \" to generate metadata.json.\\nError \"\n                else:\n                    error_msg += \"Unkown error \"\n                error_msg += \"while executing knife to generate \"\n                error_msg += \"metadata.json for {0}\".format(path)\n                print(error_msg)\n                print resp\n            if env.loglevel == 'debug':\n                print \"\\n\".join(resp.split(\"\\n\")[:2])\n        except OSError:\n            knife_installed = False\n            error_msg += \"If you locally install Chef's knife tool, LittleChef\"\n            error_msg += \" will regenerate metadata.json files automatically\\n\"\n            print(error_msg)\n        else:\n            print(\"Generated metadata.json for {0}\\n\".format(path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_recipes_in_cookbook(name):\n    recipes = {}\n    path = None\n    cookbook_exists = False\n    metadata_exists = False\n    for cookbook_path in cookbook_paths:\n        path = os.path.join(cookbook_path, name)\n        path_exists = os.path.exists(path)\n        # cookbook exists if present in any of the cookbook paths\n        cookbook_exists = cookbook_exists or path_exists\n        if not path_exists:\n            continue\n\n        _generate_metadata(path, cookbook_path, name)\n\n        # Now try to open metadata.json\n        try:\n            with open(os.path.join(path, 'metadata.json'), 'r') as f:\n                try:\n                    cookbook = json.loads(f.read())\n                except ValueError as e:\n                    msg = \"Little Chef found the following error in your\"\n                    msg += \" {0} file:\\n  {1}\".format(\n                        os.path.join(path, 'metadata.json'), e)\n                    abort(msg)\n                # Add each recipe defined in the cookbook\n                metadata_exists = True\n                recipe_defaults = {\n                    'description': '',\n                    'version': cookbook.get('version'),\n                    'dependencies': cookbook.get('dependencies', {}).keys(),\n                    'attributes': cookbook.get('attributes', {})\n                }\n                for recipe in cookbook.get('recipes', []):\n                    recipes[recipe] = dict(\n                        recipe_defaults,\n                        name=recipe,\n                        description=cookbook['recipes'][recipe]\n                    )\n            # Cookbook metadata.json was found, don't try next cookbook path\n            # because metadata.json in site-cookbooks has preference\n            break\n        except IOError:\n            # metadata.json was not found, try next cookbook_path\n            pass\n    if not cookbook_exists:\n        abort('Unable to find cookbook \"{0}\"'.format(name))\n    elif not metadata_exists:\n        abort('Cookbook \"{0}\" has no metadata.json'.format(name))\n    # Add recipes found in the 'recipes' directory but not listed\n    # in the metadata\n    for cookbook_path in cookbook_paths:\n        recipes_dir = os.path.join(cookbook_path, name, 'recipes')\n        if not os.path.isdir(recipes_dir):\n            continue\n        for basename in os.listdir(recipes_dir):\n            fname, ext = os.path.splitext(basename)\n            if ext != '.rb':\n                continue\n            if fname != 'default':\n                recipe = '%s::%s' % (name, fname)\n            else:\n                recipe = name\n            if recipe not in recipes:\n                recipes[recipe] = dict(recipe_defaults, name=recipe)\n    # When a recipe has no default recipe (libraries?),\n    # add one so that it is listed\n    if not recipes:\n        recipes[name] = dict(\n            recipe_defaults,\n            name=name,\n            description='This cookbook has no default recipe'\n        )\n    return recipes.values()", "response": "Gets the name of all recipes present in a cookbook Returns a list of dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the name of all recipes present in the run_list of a node", "response": "def get_recipes_in_node(node):\n    \"\"\"Gets the name of all recipes present in the run_list of a node\"\"\"\n    recipes = []\n    for elem in node.get('run_list', []):\n        if elem.startswith(\"recipe\"):\n            recipe = elem.split('[')[1].split(']')[0]\n            recipes.append(recipe)\n    return recipes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_recipes():\n    dirnames = set()\n    for path in cookbook_paths:\n        dirnames.update([d for d in os.listdir(path) if os.path.isdir(\n                            os.path.join(path, d)) and not d.startswith('.')])\n    recipes = []\n    for dirname in dirnames:\n        recipes.extend(get_recipes_in_cookbook(dirname))\n    return sorted(recipes, key=lambda x: x['name'])", "response": "Gets all recipes found in the cookbook directories"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_recipe(recipe):\n    print(colors.yellow(\"\\n{0}\".format(recipe['name'])))\n    print \"  description:  {0}\".format(recipe['description'])\n    print \"  version:      {0}\".format(recipe['version'])\n    print \"  dependencies: {0}\".format(\", \".join(recipe['dependencies']))\n    print \"  attributes:   {0}\".format(\", \".join(recipe['attributes']))", "response": "Pretty prints the given recipe"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_roles_in_node(node, recursive=False, depth=0):\n    LIMIT = 5\n    roles = []\n    for elem in node.get('run_list', []):\n        if elem.startswith(\"role\"):\n            role = elem.split('[')[1].split(']')[0]\n            if role not in roles:\n                roles.append(role)\n                if recursive and depth <= LIMIT:\n                    roles.extend(get_roles_in_node(_get_role(role),\n                                                   recursive=True,\n                                                   depth=depth + 1))\n    return list(set(roles))", "response": "Returns a list of roles found in the run_list of a node"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading and parses a file containing a role", "response": "def _get_role(rolename):\n    \"\"\"Reads and parses a file containing a role\"\"\"\n    path = os.path.join('roles', rolename + '.json')\n    if not os.path.exists(path):\n        abort(\"Couldn't read role file {0}\".format(path))\n    with open(path, 'r') as f:\n        try:\n            role = json.loads(f.read())\n        except ValueError as e:\n            msg = \"Little Chef found the following error in your\"\n            msg += \" {0}.json file:\\n  {1}\".format(rolename, str(e))\n            abort(msg)\n        role['fullname'] = rolename\n        return role"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all roles found in the roles directory", "response": "def get_roles():\n    \"\"\"Gets all roles found in the 'roles' directory\"\"\"\n    roles = []\n    for root, subfolders, files in os.walk('roles'):\n        for filename in files:\n            if filename.endswith(\".json\"):\n                path = os.path.join(\n                    root[len('roles'):], filename[:-len('.json')])\n                roles.append(_get_role(path))\n    return sorted(roles, key=lambda x: x['fullname'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_plugins():\n    if os.path.exists('plugins'):\n        for filename in sorted([f for f in os.listdir('plugins')\n                if not os.path.isdir(f) and f.endswith(\".py\")]):\n            plugin_name = filename[:-3]\n            try:\n                plugin = import_plugin(plugin_name)\n            except SystemExit as e:\n                description = \"Plugin has a syntax error\"\n            else:\n                description = plugin.__doc__ or \"No description found\"\n            yield {plugin_name: description}", "response": "Gets available plugins by looking into the plugins directory and importing them if they are not already there"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_plugin(name):\n    path = os.path.join(\"plugins\", name + \".py\")\n    try:\n        with open(path, 'rb') as f:\n            try:\n                plugin = imp.load_module(\n                    \"p_\" + name, f, name + '.py',\n                    ('.py', 'rb', imp.PY_SOURCE)\n                )\n            except SyntaxError as e:\n                error = \"Found plugin '{0}', but it seems\".format(name)\n                error += \" to have a syntax error: {0}\".format(str(e))\n                abort(error)\n    except IOError:\n        abort(\"Sorry, could not find '{0}.py' in the plugin directory\".format(\n              name))\n    return plugin", "response": "Imports plugin python module"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cookbook_path(cookbook_name):\n    for cookbook_path in cookbook_paths:\n        path = os.path.join(cookbook_path, cookbook_name)\n        if os.path.exists(path):\n            return path\n    raise IOError('Can\\'t find cookbook with name \"{0}\"'.format(cookbook_name))", "response": "Returns the path to the cookbook for the given cookbook name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef global_confirm(question, default=True):\n    if env.abort_on_prompts:\n        return True\n    original_parallel = env.parallel\n    env.parallel = False\n    result = confirm(question, default)\n    env.parallel = original_parallel\n    return result", "response": "Shows a confirmation that applies to all hosts\n    by temporarily disabling parallel execution by temporarily disabling parallel execution in Fabric\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pprint(dic):\n    for key, value in dic.items():\n        print(\"        {0}: {1}\".format(key, value))", "response": "Prints a dictionary with one indentation level"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_margin(length):\n    if length > 23:\n        margin_left = \"\\t\"\n        chars = 1\n    elif length > 15:\n        margin_left = \"\\t\\t\"\n        chars = 2\n    elif length > 7:\n        margin_left = \"\\t\\t\\t\"\n        chars = 3\n    else:\n        margin_left = \"\\t\\t\\t\\t\"\n        chars = 4\n    return margin_left", "response": "Add enough tabs to align in two columns"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install(version):\n    url = \"https://www.chef.io/chef/install.sh\"\n    with hide('stdout', 'running'):\n        local(\"\"\"python -c \"import urllib; print urllib.urlopen('{0}').read()\"'\n              ' > /tmp/install.sh\"\"\".format(url))\n        put('/tmp/install.sh', '/tmp/install.sh')\n        print(\"Downloading and installing Chef {0}...\".format(version))\n        with hide('stdout'):\n            sudo(\"\"\"bash /tmp/install.sh -v {0}\"\"\".format(version))\n            sudo('rm /tmp/install.sh')", "response": "Install the Chef using the omnibus installer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure(current_node=None):\n    current_node = current_node or {}\n    # Ensure that the /tmp/chef-solo/cache directory exist\n    cache_dir = \"{0}/cache\".format(env.node_work_path)\n    # First remote call, could go wrong\n    try:\n        cache_exists = exists(cache_dir)\n    except EOFError as e:\n        abort(\"Could not login to node, got: {0}\".format(e))\n    if not cache_exists:\n        with settings(hide('running', 'stdout'), warn_only=True):\n            output = sudo('mkdir -p {0}'.format(cache_dir))\n        if output.failed:\n            error = \"Could not create {0} dir. \".format(env.node_work_path)\n            error += \"Do you have sudo rights?\"\n            abort(error)\n    # Change ownership of /tmp/chef-solo/ so that we can rsync\n    with hide('running', 'stdout'):\n        with settings(warn_only=True):\n            output = sudo(\n                'chown -R {0} {1}'.format(env.user, env.node_work_path))\n        if output.failed:\n            error = \"Could not modify {0} dir. \".format(env.node_work_path)\n            error += \"Do you have sudo rights?\"\n            abort(error)\n    # Set up chef solo configuration\n    logging_path = os.path.dirname(LOGFILE)\n    if not exists(logging_path):\n        sudo('mkdir -p {0}'.format(logging_path))\n    if not exists('/etc/chef'):\n        sudo('mkdir -p /etc/chef')\n    # Set parameters and upload solo.rb template\n    reversed_cookbook_paths = cookbook_paths[:]\n    reversed_cookbook_paths.reverse()\n    cookbook_paths_list = '[{0}]'.format(', '.join(\n        ['\"{0}/{1}\"'.format(env.node_work_path, x)\n            for x in reversed_cookbook_paths]))\n    data = {\n        'node_work_path': env.node_work_path,\n        'cookbook_paths_list': cookbook_paths_list,\n        'environment': current_node.get('chef_environment', '_default'),\n        'verbose': \"true\" if env.verbose else \"false\",\n        'http_proxy': env.http_proxy,\n        'https_proxy': env.https_proxy\n    }\n    with settings(hide('everything')):\n        try:\n            upload_template('solo.rb.j2', '/etc/chef/solo.rb',\n                            context=data, use_sudo=True, backup=False,\n                            template_dir=BASEDIR, use_jinja=True, mode=0400)\n        except SystemExit:\n            error = (\"Failed to upload '/etc/chef/solo.rb'\\nThis \"\n                     \"can happen when the deployment user does not have a \"\n                     \"home directory, which is needed as a temporary location\")\n            abort(error)\n    with hide('stdout'):\n        sudo('chown root:$(id -g -n root) {0}'.format('/etc/chef/solo.rb'))", "response": "Configure chef - solo based files"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_ip(text):\n    ip_matches = re.findall(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', text)\n    ip = ip_matches[0] if ip_matches else None\n    return ip", "response": "Extract an IPv4 IP from a text string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute the node s virtualization command", "response": "def execute(node):\n    \"\"\"Uses ohai to get virtualization information which is then saved to then\n    node file\n\n    \"\"\"\n    with hide('everything'):\n        virt = json.loads(sudo('ohai virtualization'))\n    if not len(virt) or virt[0][1] != \"host\":\n        # It may work for virtualization solutions other than Xen\n        print(\"This node is not a Xen host, doing nothing\")\n        return\n    node['virtualization'] = {\n        'role': 'host',\n        'system': 'xen',\n        'vms': [],\n    }\n    # VMs\n    with hide('everything'):\n        vm_list = sudo(\"xm list\")\n    for vm in vm_list.split(\"\\n\")[2:]:\n        data = vm.split()\n        if len(data) != 6:\n            break\n        node['virtualization']['vms'].append({\n            'fqdn': data[0], 'RAM': data[2], 'cpus': data[3]})\n    print(\"Found {0} VMs for this Xen host\".format(\n          len(node['virtualization']['vms'])))\n    # Save node file and remove the returned temp file\n    del node['name']\n    os.remove(chef.save_config(node, True))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_kitchen():\n    def _mkdir(d, content=\"\"):\n        if not os.path.exists(d):\n            os.mkdir(d)\n            # Add a README so that it can be added to version control\n            readme_path = os.path.join(d, 'README')\n            if not os.path.exists(readme_path):\n                with open(readme_path, \"w\") as readme:\n                    print >> readme, content\n            print \"{0}/ directory created...\".format(d)\n\n    content = \"# The /nodes directory contains your nodes as JSON files \"\n    content += \"representing a node.\\n\"\n    content += \"# Example node file `nodes/myfqdn.json`:\\n\"\n    data = {\n        \"chef_environment\": \"production\",\n        \"apt\": {\"cacher_port\": 3143},\n        \"run_list\": [\"recipe[apt]\"]\n    }\n    content += \"{0}\".format(json.dumps(data, indent=2))\n    _mkdir(\"nodes\", content)\n    _mkdir(\"roles\")\n    _mkdir(\"data_bags\")\n    _mkdir(\"environments\")\n    for cookbook_path in littlechef.cookbook_paths:\n        _mkdir(cookbook_path)\n    # Add skeleton config file\n    if not os.path.exists(littlechef.CONFIGFILE):\n        with open(littlechef.CONFIGFILE, 'w') as configfh:\n            print >> configfh, \"[userinfo]\"\n            print >> configfh, \"user = \"\n            print >> configfh, \"password = \"\n            print >> configfh, \"keypair-file = \"\n            print >> configfh, \"ssh-config = \"\n            print >> configfh, \"encrypted_data_bag_secret = \"\n            print >> configfh, \"[kitchen]\"\n            print >> configfh, \"node_work_path = /tmp/chef-solo/\"\n            print \"{0} file created...\".format(littlechef.CONFIGFILE)", "response": "Create a new Kitchen."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconfigure a list of nodes that have the given role in their run list", "response": "def nodes_with_role(rolename):\n    \"\"\"Configures a list of nodes that have the given role in their run list\"\"\"\n    nodes = [n['name'] for n in\n             lib.get_nodes_with_role(rolename, env.chef_environment)]\n    if not len(nodes):\n        print(\"No nodes found with role '{0}'\".format(rolename))\n        sys.exit(0)\n    return node(*nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures a list of nodes that have the given recipe in their run list", "response": "def nodes_with_recipe(recipename):\n    \"\"\"Configures a list of nodes that have the given recipe in their run list\n    \"\"\"\n    nodes = [n['name'] for n in\n             lib.get_nodes_with_recipe(recipename, env.chef_environment)]\n    if not len(nodes):\n        print(\"No nodes found with recipe '{0}'\".format(recipename))\n        sys.exit(0)\n    return node(*nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a list of nodes that have the given tag assigned and calls node", "response": "def nodes_with_tag(tag):\n    \"\"\"Sets a list of nodes that have the given tag assigned and calls node()\"\"\"\n    nodes = lib.get_nodes_with_tag(tag, env.chef_environment,\n                                   littlechef.include_guests)\n    nodes = [n['name'] for n in nodes]\n    if not len(nodes):\n        print(\"No nodes found with tag '{0}'\".format(tag))\n        sys.exit(0)\n    return node(*nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects and configures a list of nodes.", "response": "def node(*nodes):\n    \"\"\"Selects and configures a list of nodes. 'all' configures all nodes\"\"\"\n    chef.build_node_data_bag()\n    if not len(nodes) or nodes[0] == '':\n        abort('No node was given')\n    elif nodes[0] == 'all':\n        # Fetch all nodes and add them to env.hosts\n        for node in lib.get_nodes(env.chef_environment):\n            env.hosts.append(node['name'])\n        if not len(env.hosts):\n            abort('No nodes found in /nodes/')\n        message = \"Are you sure you want to configure all nodes ({0})\".format(\n            len(env.hosts))\n        if env.chef_environment:\n            message += \" in the {0} environment\".format(env.chef_environment)\n        message += \"?\"\n        if not __testing__:\n            if not lib.global_confirm(message):\n                abort('Aborted by user')\n    else:\n        # A list of nodes was given\n        env.hosts = list(nodes)\n    env.all_hosts = list(env.hosts)  # Shouldn't be needed\n\n    # Check whether another command was given in addition to \"node:\"\n    if not(littlechef.__cooking__ and\n            'node:' not in sys.argv[-1] and\n            'nodes_with_role:' not in sys.argv[-1] and\n            'nodes_with_recipe:' not in sys.argv[-1] and\n            'nodes_with_tag:' not in sys.argv[-1]):\n        # If user didn't type recipe:X, role:Y or deploy_chef,\n        # configure the nodes\n        with settings():\n            execute(_node_runner)\n        chef.remove_local_node_data_bag()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _node_runner():\n    env.host_string = lib.get_env_host_string()\n    node = lib.get_node(env.host_string)\n\n    _configure_fabric_for_platform(node.get(\"platform\"))\n\n    if __testing__:\n        print \"TEST: would now configure {0}\".format(env.host_string)\n    else:\n        lib.print_header(\"Configuring {0}\".format(env.host_string))\n        if env.autodeploy_chef and not chef.chef_test():\n            deploy_chef(ask=\"no\")\n        chef.sync_node(node)", "response": "This is only used by node so that we can execute in parallel"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deploy_chef(ask=\"yes\", version=\"11\"):\n    env.host_string = lib.get_env_host_string()\n    if ask == \"no\" or littlechef.noninteractive:\n        print(\"Deploying Chef using omnibus installer version: ...\".format(version))\n    else:\n        message = ('\\nAre you sure you want to install Chef version:'\n                   '{0} on node {1}?'.format(version, env.host_string))\n        if not confirm(message):\n            abort('Aborted by user')\n\n    lib.print_header(\"Configuring Chef Solo on {0}\".format(env.host_string))\n\n    if not __testing__:\n        solo.install(version)\n        solo.configure()\n\n        # Build a basic node file if there isn't one already\n        # with some properties from ohai\n        with settings(hide('stdout'), warn_only=True):\n            output = sudo('ohai -l warn')\n        if output.succeeded:\n            try:\n                ohai = json.loads(output)\n            except ValueError:\n                abort(\"Could not parse ohai's output\"\n                      \":\\n  {0}\".format(output))\n            node = {\"run_list\": []}\n            for attribute in [\"ipaddress\", \"platform\", \"platform_family\",\n                              \"platform_version\"]:\n                if ohai.get(attribute):\n                    node[attribute] = ohai[attribute]\n            chef.save_config(node)", "response": "Deploy a Chef Solo node to a node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies the given recipe to a node", "response": "def recipe(recipe):\n    \"\"\"Apply the given recipe to a node\n    Sets the run_list to the given recipe\n    If no nodes/hostname.json file exists, it creates one\n\n    \"\"\"\n    env.host_string = lib.get_env_host_string()\n    lib.print_header(\n        \"Applying recipe '{0}' on node {1}\".format(recipe, env.host_string))\n\n    # Create configuration and sync node\n    data = lib.get_node(env.host_string)\n    data[\"run_list\"] = [\"recipe[{0}]\".format(recipe)]\n    if not __testing__:\n        if env.autodeploy_chef and not chef.chef_test():\n            deploy_chef(ask=\"no\")\n        chef.sync_node(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes the given command on the node", "response": "def ssh(name):\n    \"\"\"Executes the given command\"\"\"\n    env.host_string = lib.get_env_host_string()\n    print(\"\\nExecuting the command '{0}' on node {1}...\".format(\n          name, env.host_string))\n    # Execute remotely using either the sudo or the run fabric functions\n    with settings(hide(\"warnings\"), warn_only=True):\n        if name.startswith(\"sudo \"):\n            sudo(name[5:])\n        else:\n            run(name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plugin(name):\n    env.host_string = lib.get_env_host_string()\n    plug = lib.import_plugin(name)\n    lib.print_header(\"Executing plugin '{0}' on \"\n                     \"{1}\".format(name, env.host_string))\n    node = lib.get_node(env.host_string)\n    if node == {'run_list': []}:\n        node['name'] = env.host_string\n    plug.execute(node)\n    print(\"Finished executing plugin\")", "response": "Executes the selected plugin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing all nodes which have assigned a given tag", "response": "def list_nodes_with_tag(tag):\n    \"\"\"Show all nodes which have assigned a given tag\"\"\"\n    lib.print_nodes(lib.get_nodes_with_tag(tag, env.chef_environment,\n                                           littlechef.include_guests))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_recipes():\n    for recipe in lib.get_recipes():\n        margin_left = lib.get_margin(len(recipe['name']))\n        print(\"{0}{1}{2}\".format(\n            recipe['name'], margin_left, recipe['description']))", "response": "Show a list of all available recipes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow a list of all available roles", "response": "def list_roles():\n    \"\"\"Show a list of all available roles\"\"\"\n    for role in lib.get_roles():\n        margin_left = lib.get_margin(len(role['fullname']))\n        print(\"{0}{1}{2}\".format(\n            role['fullname'], margin_left,\n            role.get('description', '(no description)')))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlooking around and return True or False based on whether we are in a kitchen", "response": "def _check_appliances():\n    \"\"\"Looks around and return True or False based on whether we are in a\n    kitchen\n    \"\"\"\n    filenames = os.listdir(os.getcwd())\n    missing = []\n    for dirname in ['nodes', 'environments', 'roles', 'cookbooks', 'data_bags']:\n        if (dirname not in filenames) or (not os.path.isdir(dirname)):\n            missing.append(dirname)\n    return (not bool(missing)), missing"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _readconfig():\n    config = ConfigParser.SafeConfigParser()\n    try:\n        found = config.read(littlechef.CONFIGFILE)\n    except ConfigParser.ParsingError as e:\n        abort(str(e))\n    if not len(found):\n        try:\n            found = config.read(['config.cfg', 'auth.cfg'])\n        except ConfigParser.ParsingError as e:\n            abort(str(e))\n        if len(found):\n            print('\\nDeprecationWarning: deprecated config file name \\'{0}\\'.'\n                  ' Use {1}'.format(found[0], littlechef.CONFIGFILE))\n        else:\n            abort('No {0} file found in the current '\n                  'directory'.format(littlechef.CONFIGFILE))\n\n    in_a_kitchen, missing = _check_appliances()\n    missing_str = lambda m: ' and '.join(', '.join(m).rsplit(', ', 1))\n    if not in_a_kitchen:\n        abort(\"Couldn't find {0}. \"\n              \"Are you executing 'fix' outside of a kitchen?\\n\"\n              \"To create a new kitchen in the current directory \"\n              \" type 'fix new_kitchen'\".format(missing_str(missing)))\n\n    # We expect an ssh_config file here,\n    # and/or a user, (password/keyfile) pair\n    try:\n        env.ssh_config_path = config.get('userinfo', 'ssh-config')\n    except ConfigParser.NoSectionError:\n        abort('You need to define a \"userinfo\" section'\n              ' in the config file. Refer to the README for help '\n              '(http://github.com/tobami/littlechef)')\n    except ConfigParser.NoOptionError:\n        env.ssh_config_path = None\n\n    if env.ssh_config_path:\n        env.ssh_config = _SSHConfig()\n        env.ssh_config_path = os.path.expanduser(env.ssh_config_path)\n        env.use_ssh_config = True\n        try:\n            env.ssh_config.parse(open(env.ssh_config_path))\n        except IOError:\n            abort(\"Couldn't open the ssh-config file \"\n                  \"'{0}'\".format(env.ssh_config_path))\n        except Exception:\n            abort(\"Couldn't parse the ssh-config file \"\n                  \"'{0}'\".format(env.ssh_config_path))\n    else:\n        env.ssh_config = None\n\n    # check for a gateway\n    try:\n        env.gateway = config.get('connection', 'gateway')\n    except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):\n        env.gateway = None\n\n    # check for http_proxy which will be put into solo.rb\n    try:\n        env.http_proxy = config.get('connection', 'http_proxy')\n    except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):\n        env.http_proxy = None\n\n    try:\n        env.https_proxy = config.get('connection', 'https_proxy')\n    except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):\n        env.https_proxy = None\n\n    try:\n        env.remove_data_bags = config.get('userinfo', 'remove_data_bags')\n    except ConfigParser.NoOptionError:\n        env.remove_data_bags = False\n        \n    # Check for an encrypted_data_bag_secret file and set the env option\n    try:\n        env.encrypted_data_bag_secret = config.get('userinfo',\n                                                   'encrypted_data_bag_secret')\n    except ConfigParser.NoOptionError:\n        env.encrypted_data_bag_secret = None\n\n    if env.encrypted_data_bag_secret:\n        env.encrypted_data_bag_secret = os.path.expanduser(\n            env.encrypted_data_bag_secret)\n        try:\n            open(env.encrypted_data_bag_secret)\n        except IOError as e:\n            abort(\"Failed to open encrypted_data_bag_secret file at \"\n                  \"'{0}'\".format(env.encrypted_data_bag_secret))\n\n    try:\n        sudo_prefix = config.get('ssh', 'sudo_prefix', raw=True)\n    except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):\n        pass\n    else:\n        env.sudo_prefix = sudo_prefix\n\n    try:\n        env.user = config.get('userinfo', 'user')\n    except ConfigParser.NoOptionError:\n        if not env.ssh_config_path:\n            msg = 'You need to define a user in the \"userinfo\" section'\n            msg += ' of {0}. Refer to the README for help'\n            msg += ' (http://github.com/tobami/littlechef)'\n            abort(msg.format(littlechef.CONFIGFILE))\n        user_specified = False\n    else:\n        user_specified = True\n\n    try:\n        env.password = config.get('userinfo', 'password') or None\n    except ConfigParser.NoOptionError:\n        pass\n\n    try:\n        # If keypair-file is empty, assign None or fabric will try to read key\n        env.key_filename = config.get('userinfo', 'keypair-file') or None\n    except ConfigParser.NoOptionError:\n        pass\n\n    if (user_specified and not env.password and not env.key_filename\n            and not env.ssh_config):\n        abort('You need to define a password, keypair file, or ssh-config '\n              'file in {0}'.format(littlechef.CONFIGFILE))\n\n    # Node's Chef Solo working directory for storing cookbooks, roles, etc.\n    try:\n        env.node_work_path = os.path.expanduser(config.get('kitchen',\n                                                'node_work_path'))\n    except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):\n        env.node_work_path = littlechef.node_work_path\n    else:\n        if not env.node_work_path:\n            abort('The \"node_work_path\" option cannot be empty')\n\n    # Follow symlinks\n    try:\n        env.follow_symlinks = config.getboolean('kitchen', 'follow_symlinks')\n    except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):\n        env.follow_symlinks = False\n\n    try:\n        env.berksfile = config.get('kitchen', 'berksfile')\n    except (ConfigParser.NoSectionError, ConfigParser.NoOptionError) as e:\n        env.berksfile = None\n    else:\n        try:\n            env.berksfile_cookbooks_directory = config.get('kitchen', 'berksfile_cookbooks_directory')\n            littlechef.cookbook_paths.append(env.berksfile_cookbooks_directory)\n        except (ConfigParser.NoSectionError, ConfigParser.NoOptionError) as e:\n            if env.berksfile:\n                env.berksfile_cookbooks_directory = tempfile.mkdtemp('littlechef-berks')\n                littlechef.cookbook_paths.append(env.berksfile_cookbooks_directory)\n            else:\n                env.berksfile_cookbooks_directory = None\n        chef.ensure_berksfile_cookbooks_are_installed()\n\n    # Upload Directory\n    try:\n        env.sync_packages_dest_dir = config.get('sync-packages',\n                                                'dest-dir')\n    except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):\n        env.sync_packages_dest_dir = None\n\n    # Local Directory\n    try:\n        env.sync_packages_local_dir = config.get('sync-packages',\n                                                 'local-dir')\n    except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):\n        env.sync_packages_local_dir = None\n\n    try:\n        env.autodeploy_chef = config.get('userinfo', 'autodeploy_chef') or None\n    except ConfigParser.NoOptionError:\n        env.autodeploy_chef = None", "response": "Configures environment variables and environment variables for a single item in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_ticket(self, ticket=None, **kwargs):\n        if not ticket:\n            ticket = self.create_ticket_str()\n        if 'service' in kwargs:\n            kwargs['service'] = clean_service_url(kwargs['service'])\n        if 'expires' not in kwargs:\n            expires = now() + timedelta(seconds=self.model.TICKET_EXPIRE)\n            kwargs['expires'] = expires\n        t = self.create(ticket=ticket, **kwargs)\n        logger.debug(\"Created %s %s\" % (t.name, t.ticket))\n        return t", "response": "Create a new Ticket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_ticket_str(self, prefix=None):\n        if not prefix:\n            prefix = self.model.TICKET_PREFIX\n        return \"%s-%d-%s\" % (prefix, int(time.time()),\n                             get_random_string(length=self.model.TICKET_RAND_LEN))", "response": "Generate a sufficiently opaque ticket string to ensure the ticket is not guessable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_ticket(self, ticket, service, renew=False, require_https=False):\n        if not ticket:\n            raise InvalidRequest(\"No ticket string provided\")\n\n        if not self.model.TICKET_RE.match(ticket):\n            raise InvalidTicket(\"Ticket string %s is invalid\" % ticket)\n\n        try:\n            t = self.get(ticket=ticket)\n        except self.model.DoesNotExist:\n            raise InvalidTicket(\"Ticket %s does not exist\" % ticket)\n\n        if t.is_consumed():\n            raise InvalidTicket(\"%s %s has already been used\" %\n                                (t.name, ticket))\n        if t.is_expired():\n            raise InvalidTicket(\"%s %s has expired\" % (t.name, ticket))\n\n        if not service:\n            raise InvalidRequest(\"No service identifier provided\")\n\n        if require_https and not is_scheme_https(service):\n            raise InvalidService(\"Service %s is not HTTPS\" % service)\n\n        if not service_allowed(service):\n            raise InvalidService(\"Service %s is not a valid %s URL\" %\n                                 (service, t.name))\n\n        try:\n            if not match_service(t.service, service):\n                raise InvalidService(\"%s %s for service %s is invalid for \"\n                        \"service %s\" % (t.name, ticket, t.service, service))\n        except AttributeError:\n            pass\n\n        try:\n            if renew and not t.is_primary():\n                raise InvalidTicket(\"%s %s was not issued via primary \"\n                                    \"credentials\" % (t.name, ticket))\n        except AttributeError:\n            pass\n\n        logger.debug(\"Validated %s %s\" % (t.name, ticket))\n        return t", "response": "Validate a ticket string and service identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes invalid tickets for all available TCs.", "response": "def delete_invalid_tickets(self):\n        \"\"\"\n        Delete consumed or expired ``Ticket``s that are not referenced\n        by other ``Ticket``s. Invalid tickets are no longer valid for\n        authentication and can be safely deleted.\n\n        A custom management command is provided that executes this method\n        on all applicable models by running ``manage.py cleanupcas``.\n        \"\"\"\n        for ticket in self.filter(Q(consumed__isnull=False) |\n                                  Q(expires__lte=now())).order_by('-expires'):\n            try:\n                ticket.delete()\n            except models.ProtectedError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef consume_tickets(self, user):\n        for ticket in self.filter(user=user, consumed__isnull=True,\n                                  expires__gt=now()):\n            ticket.consume()", "response": "Consume all valid Tickets for a user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_sign_out(self, user):\n        session = Session()\n        for ticket in self.filter(user=user, consumed__gte=user.last_login):\n            ticket.request_sign_out(session=session)", "response": "Send a single logout request to each service accessed by user. This is called at logout when single logout is enabled."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a POST request to the ServiceTicket s logout URL to s logout request sign - out.", "response": "def request_sign_out(self, session=requests):\n        \"\"\"\n        Send a POST request to the ``ServiceTicket``s logout URL to\n        request sign-out.\n        \"\"\"\n        if logout_allowed(self.service):\n            request = SingleSignOutRequest(context={'ticket': self})\n            url = get_logout_url(self.service) or self.service\n            session.post(url, data={'logoutRequest': request.render_content()})\n            logger.info(\"Single sign-out request sent to %s\" % url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new ProxyGrantingTicket.", "response": "def create_ticket(self, service, pgturl, **kwargs):\n        \"\"\"\n        When a ``pgtUrl`` parameter is provided to ``/serviceValidate`` or\n        ``/proxyValidate``, attempt to create a new ``ProxyGrantingTicket``.\n        If validation succeeds, create and return the ``ProxyGrantingTicket``.\n        If validation fails, return ``None``.\n        \"\"\"\n        pgtid = self.create_ticket_str()\n        pgtiou = self.create_ticket_str(prefix=self.model.IOU_PREFIX)\n        try:\n            self.validate_callback(service, pgturl, pgtid, pgtiou)\n        except ValidationError as e:\n            logger.warning(\"%s %s\" % (e.code, e))\n            return None\n        else:\n            # pgtUrl validation succeeded, so create a new PGT with the\n            # previously generated ticket strings\n            return super(ProxyGrantingTicketManager, self).create_ticket(ticket=pgtid, iou=pgtiou, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying the provided proxy callback URL.", "response": "def validate_callback(self, service, pgturl, pgtid, pgtiou):\n        \"\"\"Verify the provided proxy callback URL.\"\"\"\n        if not proxy_allowed(service):\n            raise UnauthorizedServiceProxy(\"%s is not authorized to use proxy authentication\" % service)\n\n        if not is_scheme_https(pgturl):\n            raise InvalidProxyCallback(\"Proxy callback %s is not HTTPS\" % pgturl)\n\n        if not proxy_callback_allowed(service, pgturl):\n            raise InvalidProxyCallback(\"%s is not an authorized proxy callback URL\" % pgturl)\n\n        # Verify that the SSL certificate is valid\n        verify = os.environ.get('REQUESTS_CA_BUNDLE', True)\n        try:\n            requests.get(pgturl, verify=verify, timeout=5)\n        except requests.exceptions.SSLError:\n            raise InvalidProxyCallback(\"SSL certificate validation failed for proxy callback %s\" % pgturl)\n        except requests.exceptions.RequestException as e:\n            raise InvalidProxyCallback(e)\n\n        # Callback certificate appears valid, so send the ticket strings\n        pgturl = add_query_params(pgturl, {'pgtId': pgtid, 'pgtIou': pgtiou})\n        try:\n            response = requests.get(pgturl, verify=verify, timeout=5)\n        except requests.exceptions.RequestException as e:\n            raise InvalidProxyCallback(e)\n\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise InvalidProxyCallback(\"Proxy callback %s returned %s\" % (pgturl, e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_backends():\n    backends = []\n    backend_paths = getattr(\n        settings, 'MAMA_CAS_SERVICE_BACKENDS',\n        ['mama_cas.services.backends.SettingsBackend']\n    )\n    for backend_path in backend_paths:\n        backend = import_string(backend_path)()\n        backends.append(backend)\n    return backends", "response": "Retrieve the list of configured service backends."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntesting if a given attribute is allowed according to the current set of service backends.", "response": "def _is_allowed(attr, *args):\n    \"\"\"\n    Test if a given attribute is allowed according to the\n    current set of configured service backends.\n    \"\"\"\n    for backend in _get_backends():\n        try:\n            if getattr(backend, attr)(*args):\n                return True\n        except AttributeError:\n            raise NotImplementedError(\"%s.%s.%s() not implemented\" % (\n                backend.__class__.__module__, backend.__class__.__name__, attr)\n            )\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the given URL is a valid service.", "response": "def _is_valid_service_url(url):\n    \"\"\"Access services list from ``MAMA_CAS_VALID_SERVICES``.\"\"\"\n    valid_services = getattr(settings, 'MAMA_CAS_VALID_SERVICES', ())\n    if not valid_services:\n        return True\n    warnings.warn(\n        'The MAMA_CAS_VALID_SERVICES setting is deprecated. Services '\n        'should be configured using MAMA_CAS_SERVICES.', DeprecationWarning)\n    for service in [re.compile(s) for s in valid_services]:\n        if service.match(url):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the dotted path of the matching backend.", "response": "def get_backend_path(service):\n    \"\"\"Return the dotted path of the matching backend.\"\"\"\n    for backend in _get_backends():\n        try:\n            if backend.service_allowed(service):\n                return \"%s.%s\" % (backend.__class__.__module__, backend.__class__.__name__)\n        except AttributeError:\n            raise NotImplementedError(\"%s.%s.service_allowed() not implemented\" % (\n                backend.__class__.__module__, backend.__class__.__name__)\n            )\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_callbacks(service):\n    callbacks = list(getattr(settings, 'MAMA_CAS_ATTRIBUTE_CALLBACKS', []))\n    if callbacks:\n        warnings.warn(\n            'The MAMA_CAS_ATTRIBUTE_CALLBACKS setting is deprecated. Service callbacks '\n            'should be configured using MAMA_CAS_SERVICES.', DeprecationWarning)\n\n    for backend in _get_backends():\n        try:\n            callbacks.extend(backend.get_callbacks(service))\n        except AttributeError:\n            raise NotImplementedError(\"%s.%s.get_callbacks() not implemented\" % (\n                backend.__class__.__module__, backend.__class__.__name__)\n            )\n    return callbacks", "response": "Get the list of callbacks for a given service identifier."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the configured logout URL for a given service identifier.", "response": "def get_logout_url(service):\n    \"\"\"Get the configured logout URL for a given service identifier, if any.\"\"\"\n    for backend in _get_backends():\n        try:\n            return backend.get_logout_url(service)\n        except AttributeError:\n            raise NotImplementedError(\"%s.%s.get_logout_url() not implemented\" % (\n                backend.__class__.__module__, backend.__class__.__name__)\n            )\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logout_allowed(service):\n    if hasattr(settings, 'MAMA_CAS_SERVICES'):\n        return _is_allowed('logout_allowed', service)\n\n    if hasattr(settings, 'MAMA_CAS_ENABLE_SINGLE_SIGN_OUT'):\n        warnings.warn(\n            'The MAMA_CAS_ENABLE_SINGLE_SIGN_OUT setting is deprecated. SLO '\n            'should be configured using MAMA_CAS_SERVICES.', DeprecationWarning)\n    return getattr(settings, 'MAMA_CAS_ENABLE_SINGLE_SIGN_OUT', False)", "response": "Check if a given service identifier should be sent a logout request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef proxy_callback_allowed(service, pgturl):\n    if hasattr(settings, 'MAMA_CAS_SERVICES'):\n        return _is_allowed('proxy_callback_allowed', service, pgturl)\n    return _is_valid_service_url(service)", "response": "Check if a given proxy callback is allowed for the given service identifier."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean(self):\n        username = self.cleaned_data.get('username')\n        password = self.cleaned_data.get('password')\n\n        if username and password:\n            try:\n                self.user = authenticate(request=self.request, username=username, password=password)\n            except Exception:\n                logger.exception(\"Error authenticating %s\" % username)\n                error_msg = _('Internal error while authenticating user')\n                raise forms.ValidationError(error_msg)\n\n            if self.user is None:\n                logger.warning(\"Failed authentication for %s\" % username)\n                error_msg = _('The username or password is not correct')\n                raise forms.ValidationError(error_msg)\n            else:\n                if not self.user.is_active:\n                    logger.warning(\"User account %s is disabled\" % username)\n                    error_msg = _('This user account is disabled')\n                    raise forms.ValidationError(error_msg)\n\n        return self.cleaned_data", "response": "Pass the username and password to the active\n        authentication backends and verify the user account is not disabled."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_username(self):\n        username = self.cleaned_data.get('username').split('@')[0]\n        if not username:\n            raise forms.ValidationError(_('Invalid username provided'))\n        return username", "response": "Returns the username of the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ns(self, prefix, tag):\n        return etree.QName(self.prefixes[prefix], tag)", "response": "Given a prefix and an XML tag output the qualified name of that tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_service_ticket(service, ticket, pgturl=None, renew=False, require_https=False):\n    logger.debug(\"Service validation request received for %s\" % ticket)\n\n    # Check for proxy tickets passed to /serviceValidate\n    if ticket and ticket.startswith(ProxyTicket.TICKET_PREFIX):\n        raise InvalidTicketSpec('Proxy tickets cannot be validated with /serviceValidate')\n\n    st = ServiceTicket.objects.validate_ticket(ticket, service, renew=renew, require_https=require_https)\n    attributes = get_attributes(st.user, st.service)\n\n    if pgturl is not None:\n        logger.debug(\"Proxy-granting ticket request received for %s\" % pgturl)\n        pgt = ProxyGrantingTicket.objects.create_ticket(service, pgturl, user=st.user, granted_by_st=st)\n    else:\n        pgt = None\n    return st, attributes, pgt", "response": "Validate a service ticket string. Return a ServiceTicket and an optional ProxyGrantingTicket or ValidationError."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate a proxy ticket string. Return a 4 - tuple containing a ProxyTicket an optional ProxyGrantingTicket and a list of proxies through which authentication proceeded.", "response": "def validate_proxy_ticket(service, ticket, pgturl=None):\n    \"\"\"\n    Validate a proxy ticket string. Return a 4-tuple containing a\n    ``ProxyTicket``, an optional ``ProxyGrantingTicket`` and a list\n    of proxies through which authentication proceeded, or a\n    ``ValidationError`` if ticket validation failed.\n    \"\"\"\n    logger.debug(\"Proxy validation request received for %s\" % ticket)\n\n    pt = ProxyTicket.objects.validate_ticket(ticket, service)\n    attributes = get_attributes(pt.user, pt.service)\n\n    # Build a list of all services that proxied authentication,\n    # in reverse order of which they were traversed\n    proxies = [pt.service]\n    prior_pt = pt.granted_by_pgt.granted_by_pt\n    while prior_pt:\n        proxies.append(prior_pt.service)\n        prior_pt = prior_pt.granted_by_pgt.granted_by_pt\n\n    if pgturl is not None:\n        logger.debug(\"Proxy-granting ticket request received for %s\" % pgturl)\n        pgt = ProxyGrantingTicket.objects.create_ticket(service, pgturl, user=pt.user, granted_by_pt=pt)\n    else:\n        pgt = None\n    return pt, attributes, pgt, proxies"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_proxy_granting_ticket(pgt, target_service):\n    logger.debug(\"Proxy ticket request received for %s using %s\" % (target_service, pgt))\n\n    pgt = ProxyGrantingTicket.objects.validate_ticket(pgt, target_service)\n    pt = ProxyTicket.objects.create_ticket(service=target_service, user=pgt.user, granted_by_pgt=pgt)\n    return pt", "response": "Validate a proxy granting ticket string. Return an ordered pair\n    containing a ProxyTicket or a ValidationError if validation failed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_attributes(user, service):\n    attributes = {}\n    for path in get_callbacks(service):\n        callback = import_string(path)\n        attributes.update(callback(user, service))\n    return attributes", "response": "Returns a dictionary of user attributes from the set of configured\n    callback functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logout_user(request):\n    logger.debug(\"Logout request received for %s\" % request.user)\n    if is_authenticated(request.user):\n        ServiceTicket.objects.consume_tickets(request.user)\n        ProxyTicket.objects.consume_tickets(request.user)\n        ProxyGrantingTicket.objects.consume_tickets(request.user)\n\n        ServiceTicket.objects.request_sign_out(request.user)\n\n        logger.info(\"Single sign-on session ended for %s\" % request.user)\n        logout(request)\n        messages.success(request, _('You have been successfully logged out'))", "response": "End a single sign - on session for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all available user name related fields and methods.", "response": "def user_name_attributes(user, service):\n    \"\"\"Return all available user name related fields and methods.\"\"\"\n    attributes = {}\n    attributes['username'] = user.get_username()\n    attributes['full_name'] = user.get_full_name()\n    attributes['short_name'] = user.get_short_name()\n    return attributes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_model_attributes(user, service):\n    ignore_fields = ['id', 'password']\n    attributes = {}\n    for field in user._meta.fields:\n        if field.name not in ignore_fields:\n            attributes[field.name] = getattr(user, field.name)\n    return attributes", "response": "Return all the user model attributes that are not in the list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninjecting additional query parameters into an existing URL.", "response": "def add_query_params(url, params):\n    \"\"\"\n    Inject additional query parameters into an existing URL. If\n    parameters already exist with the same name, they will be\n    overwritten. Parameters with empty values are ignored. Return\n    the modified URL as a string.\n    \"\"\"\n    def encode(s):\n        return force_bytes(s, settings.DEFAULT_CHARSET)\n    params = dict([(encode(k), encode(v)) for k, v in params.items() if v])\n\n    parts = list(urlparse(url))\n    query = dict(parse_qsl(parts[4]))\n    query.update(params)\n    parts[4] = urlencode(query)\n    return urlunparse(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncleaning up the service URL.", "response": "def clean_service_url(url):\n    \"\"\"\n    Return only the scheme, hostname (with optional port) and path\n    components of the parameter URL.\n    \"\"\"\n    parts = urlparse(url)\n    return urlunparse((parts.scheme, parts.netloc, parts.path, '', '', ''))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_service(service1, service2):\n    s1, s2 = urlparse(service1), urlparse(service2)\n    try:\n        return (s1.scheme, s1.netloc, s1.path) == (s2.scheme, s2.netloc, s2.path)\n    except ValueError:\n        return False", "response": "Compare two service URLs. Return True if the scheme hostname port and path match."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef redirect(to, *args, **kwargs):\n    params = kwargs.pop('params', {})\n\n    try:\n        to = reverse(to, args=args, kwargs=kwargs)\n    except NoReverseMatch:\n        if '/' not in to and '.' not in to:\n            to = reverse('cas_login')\n        elif not service_allowed(to):\n            raise PermissionDenied()\n\n    if params:\n        to = add_query_params(to, params)\n\n    logger.debug(\"Redirecting to %s\" % to)\n    return HttpResponseRedirect(to)", "response": "Redirect to a URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the configuration for a given service and setting.", "response": "def get_config(self, service, setting):\n        \"\"\"\n        Access the configuration for a given service and setting. If the\n        service is not found, return a default value.\n        \"\"\"\n        try:\n            return self.get_service(service)[setting]\n        except KeyError:\n            return getattr(self, setting + '_DEFAULT')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_status(self, status_value, message=None):\n        status = etree.Element('Status')\n        status_code = etree.SubElement(status, 'StatusCode')\n        status_code.set('Value', 'samlp:' + status_value)\n        if message:\n            status_message = etree.SubElement(status, 'StatusMessage')\n            status_message.text = message\n        return status", "response": "Build a Status XML block for a SAML 1. 1 Response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a SAML 1. 1 Assertion XML block.", "response": "def get_assertion(self, ticket, attributes):\n        \"\"\"\n        Build a SAML 1.1 Assertion XML block.\n        \"\"\"\n        assertion = etree.Element('Assertion')\n        assertion.set('xmlns', 'urn:oasis:names:tc:SAML:1.0:assertion')\n        assertion.set('AssertionID', self.generate_id())\n        assertion.set('IssueInstant', self.instant())\n        assertion.set('Issuer', 'localhost')\n        assertion.set('MajorVersion', '1')\n        assertion.set('MinorVersion', '1')\n        assertion.append(self.get_conditions(ticket.service))\n        subject = self.get_subject(ticket.user.get_username())\n        if attributes:\n            assertion.append(self.get_attribute_statement(subject, attributes))\n        assertion.append(self.get_authentication_statement(subject, ticket))\n\n        return assertion"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a SAML 1. 1 Assertion. AudienceRestrictionCondition XML block for a SAML 1. 1 Assertion.", "response": "def get_conditions(self, service_id):\n        \"\"\"\n        Build a Conditions XML block for a SAML 1.1 Assertion.\n        \"\"\"\n        conditions = etree.Element('Conditions')\n        conditions.set('NotBefore', self.instant())\n        conditions.set('NotOnOrAfter', self.instant(offset=30))\n        restriction = etree.SubElement(conditions, 'AudienceRestrictionCondition')\n        audience = etree.SubElement(restriction, 'Audience')\n        audience.text = service_id\n        return conditions"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds an AttributeStatement XML block for a SAML 1. 1 Assertion.", "response": "def get_attribute_statement(self, subject, attributes):\n        \"\"\"\n        Build an AttributeStatement XML block for a SAML 1.1 Assertion.\n        \"\"\"\n        attribute_statement = etree.Element('AttributeStatement')\n        attribute_statement.append(subject)\n        for name, value in attributes.items():\n            attribute = etree.SubElement(attribute_statement, 'Attribute')\n            attribute.set('AttributeName', name)\n            attribute.set('AttributeNamespace', self.namespace)\n            if isinstance(value, list):\n                for v in value:\n                    attribute_value = etree.SubElement(attribute, 'AttributeValue')\n                    attribute_value.text = force_text(v)\n            else:\n                attribute_value = etree.SubElement(attribute, 'AttributeValue')\n                attribute_value.text = force_text(value)\n        return attribute_statement"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds an AuthenticationStatement XML block for a SAML 1. 1 Assertion.", "response": "def get_authentication_statement(self, subject, ticket):\n        \"\"\"\n        Build an AuthenticationStatement XML block for a SAML 1.1\n        Assertion.\n        \"\"\"\n        authentication_statement = etree.Element('AuthenticationStatement')\n        authentication_statement.set('AuthenticationInstant',\n                                     self.instant(instant=ticket.consumed))\n        authentication_statement.set('AuthenticationMethod',\n                                     self.authn_method_password)\n        authentication_statement.append(subject)\n        return authentication_statement"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a Subject XML block for a SAML 1. 1 AuthenticationStatement or AttributeStatement.", "response": "def get_subject(self, identifier):\n        \"\"\"\n        Build a Subject XML block for a SAML 1.1\n        AuthenticationStatement or AttributeStatement.\n        \"\"\"\n        subject = etree.Element('Subject')\n        name = etree.SubElement(subject, 'NameIdentifier')\n        name.text = identifier\n        subject_confirmation = etree.SubElement(subject, 'SubjectConfirmation')\n        method = etree.SubElement(subject_confirmation, 'ConfirmationMethod')\n        method.text = self.confirmation_method\n        return subject"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a string is a bytes type or not", "response": "def is_bytes(string):\n    \"\"\"Check if a string is a bytes instance\n\n    :param Union[str, bytes] string: A string that may be string or bytes like\n    :return: Whether the provided string is a bytes type or not\n    :rtype: bool\n    \"\"\"\n    if six.PY3 and isinstance(string, (bytes, memoryview, bytearray)):  # noqa\n        return True\n    elif six.PY2 and isinstance(string, (buffer, bytearray)):  # noqa\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes a filesystem path to the proper filesystem encoding", "response": "def fs_encode(path):\n    \"\"\"\n    Encode a filesystem path to the proper filesystem encoding\n\n    :param Union[str, bytes] path: A string-like path\n    :returns: A bytes-encoded filesystem path representation\n    \"\"\"\n\n    path = _get_path(path)\n    if path is None:\n        raise TypeError(\"expected a valid path to encode\")\n    if isinstance(path, six.text_type):\n        if six.PY2:\n            return b\"\".join(\n                (\n                    _byte(ord(c) - 0xDC00)\n                    if 0xDC00 <= ord(c) <= 0xDCFF\n                    else c.encode(_fs_encoding, _fs_encode_errors)\n                )\n                for c in path\n            )\n        return path.encode(_fs_encoding, _fs_encode_errors)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding a filesystem path using the proper filesystem encoding", "response": "def fs_decode(path):\n    \"\"\"\n    Decode a filesystem path using the proper filesystem encoding\n\n    :param path: The filesystem path to decode from bytes or string\n    :return: The filesystem path, decoded with the determined encoding\n    :rtype: Text\n    \"\"\"\n\n    path = _get_path(path)\n    if path is None:\n        raise TypeError(\"expected a valid path to decode\")\n    if isinstance(path, six.binary_type):\n        if six.PY2:\n            from array import array\n\n            indexes = _invalid_utf8_indexes(array(str(\"B\"), path))\n            return \"\".join(\n                chunk.decode(_fs_encoding, _fs_decode_errors)\n                for chunk in _chunks(path, indexes)\n            )\n        return path.decode(_fs_encoding, _fs_decode_errors)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partialclass(cls, *args, **kwargs):\n\n    name_attrs = [\n        n\n        for n in (getattr(cls, name, str(cls)) for name in (\"__name__\", \"__qualname__\"))\n        if n is not None\n    ]\n    name_attrs = name_attrs[0]\n    type_ = type(\n        name_attrs, (cls,), {\"__init__\": partialmethod(cls.__init__, *args, **kwargs)}\n    )\n    # Swiped from attrs.make_class\n    try:\n        type_.__module__ = sys._getframe(1).f_globals.get(\"__name__\", \"__main__\")\n    except (AttributeError, ValueError):  # pragma: no cover\n        pass  # pragma: no cover\n    return type_", "response": "Returns a partially instantiated class containing the given class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_bytes(string, encoding=\"utf-8\", errors=None):\n\n    unicode_name = get_canonical_encoding_name(\"utf-8\")\n    if not errors:\n        if get_canonical_encoding_name(encoding) == unicode_name:\n            if six.PY3 and os.name == \"nt\":\n                errors = \"surrogatepass\"\n            else:\n                errors = \"surrogateescape\" if six.PY3 else \"ignore\"\n        else:\n            errors = \"strict\"\n    if isinstance(string, bytes):\n        if get_canonical_encoding_name(encoding) == unicode_name:\n            return string\n        else:\n            return string.decode(unicode_name).encode(encoding, errors)\n    elif isinstance(string, memoryview):\n        return bytes(string)\n    elif not isinstance(string, six.string_types):\n        try:\n            if six.PY3:\n                return six.text_type(string).encode(encoding, errors)\n            else:\n                return bytes(string)\n        except UnicodeEncodeError:\n            if isinstance(string, Exception):\n                return b\" \".join(to_bytes(arg, encoding, errors) for arg in string)\n            return six.text_type(string).encode(encoding, errors)\n    else:\n        return string.encode(encoding, errors)", "response": "Force a value to bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_text(string, encoding=\"utf-8\", errors=None):\n\n    unicode_name = get_canonical_encoding_name(\"utf-8\")\n    if not errors:\n        if get_canonical_encoding_name(encoding) == unicode_name:\n            if six.PY3 and os.name == \"nt\":\n                errors = \"surrogatepass\"\n            else:\n                errors = \"surrogateescape\" if six.PY3 else \"ignore\"\n        else:\n            errors = \"strict\"\n    if issubclass(type(string), six.text_type):\n        return string\n    try:\n        if not issubclass(type(string), six.string_types):\n            if six.PY3:\n                if isinstance(string, bytes):\n                    string = six.text_type(string, encoding, errors)\n                else:\n                    string = six.text_type(string)\n            elif hasattr(string, \"__unicode__\"):\n                string = six.text_type(string)\n            else:\n                string = six.text_type(bytes(string), encoding, errors)\n        else:\n            string = string.decode(encoding, errors)\n    except UnicodeDecodeError:\n        string = \" \".join(to_text(arg, encoding, errors) for arg in string)\n    return string", "response": "Force a value to a text - type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a stream wrap it in a StreamWrapper instance and return the wrapped stream.", "response": "def get_wrapped_stream(stream, encoding=None, errors=\"replace\"):\n    \"\"\"\n    Given a stream, wrap it in a `StreamWrapper` instance and return the wrapped stream.\n\n    :param stream: A stream instance to wrap\n    :param str encoding: The encoding to use for the stream\n    :param str errors: The error handler to use, default \"replace\"\n    :returns: A new, wrapped stream\n    :rtype: :class:`StreamWrapper`\n    \"\"\"\n\n    if stream is None:\n        raise TypeError(\"must provide a stream to wrap\")\n    stream = _get_binary_buffer(stream)\n    if stream is not None and encoding is None:\n        encoding = \"utf-8\"\n    if not encoding:\n        encoding = get_output_encoding(stream)\n    else:\n        encoding = get_canonical_encoding_name(encoding)\n    return StreamWrapper(stream, encoding, errors, line_buffering=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a unicode stream wrapper around sys. stdout or sys. stderr.", "response": "def get_text_stream(stream=\"stdout\", encoding=None):\n    \"\"\"Retrieve a unicode stream wrapper around **sys.stdout** or **sys.stderr**.\n\n    :param str stream: The name of the stream to wrap from the :mod:`sys` module.\n    :param str encoding: An optional encoding to use.\n    :return: A new :class:`~vistir.misc.StreamWrapper` instance around the stream\n    :rtype: `vistir.misc.StreamWrapper`\n    \"\"\"\n\n    stream_map = {\"stdin\": sys.stdin, \"stdout\": sys.stdout, \"stderr\": sys.stderr}\n    if os.name == \"nt\" or sys.platform.startswith(\"win\"):\n        from ._winconsole import _get_windows_console_stream, _wrap_std_stream\n\n    else:\n        _get_windows_console_stream = lambda *args: None  # noqa\n        _wrap_std_stream = lambda *args: None  # noqa\n\n    if six.PY2 and stream != \"stdin\":\n        _wrap_std_stream(stream)\n    sys_stream = stream_map[stream]\n    windows_console = _get_windows_console_stream(sys_stream, encoding, None)\n    if windows_console is not None:\n        return windows_console\n    return get_wrapped_stream(sys_stream, encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_with_text_stream(stream_name):\n    new_stream = TEXT_STREAMS.get(stream_name)\n    if new_stream is not None:\n        new_stream = new_stream()\n        setattr(sys, stream_name, new_stream)\n    return None", "response": "Given a stream name replace the target stream with a text - converted equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the given text to the provided stream or sys. stdout by default.", "response": "def echo(text, fg=None, bg=None, style=None, file=None, err=False, color=None):\n    \"\"\"Write the given text to the provided stream or **sys.stdout** by default.\n\n    Provides optional foreground and background colors from the ansi defaults:\n    **grey**, **red**, **green**, **yellow**, **blue**, **magenta**, **cyan**\n    or **white**.\n\n    Available styles include **bold**, **dark**, **underline**, **blink**, **reverse**,\n    **concealed**\n\n    :param str text: Text to write\n    :param str fg: Foreground color to use (default: None)\n    :param str bg: Foreground color to use (default: None)\n    :param str style: Style to use (default: None)\n    :param stream file: File to write to (default: None)\n    :param bool color: Whether to force color (i.e. ANSI codes are in the text)\n    \"\"\"\n\n    if file and not hasattr(file, \"write\"):\n        raise TypeError(\"Expected a writable stream, received {0!r}\".format(file))\n    if not file:\n        if err:\n            file = _text_stderr()\n        else:\n            file = _text_stdout()\n    if text and not isinstance(text, (six.string_types, bytes, bytearray)):\n        text = six.text_type(text)\n    text = \"\" if not text else text\n    if isinstance(text, six.text_type):\n        text += \"\\n\"\n    else:\n        text += b\"\\n\"\n    if text and six.PY3 and is_bytes(text):\n        buffer = _get_binary_buffer(file)\n        if buffer is not None:\n            file.flush()\n            buffer.write(text)\n            buffer.flush()\n            return\n    if text and not is_bytes(text):\n        can_use_color = _can_use_color(file, color=color)\n        if any([fg, bg, style]):\n            text = colorize(text, fg=fg, bg=bg, attrs=style)\n        if not can_use_color or (os.name == \"nt\" and not _wrap_for_color):\n            text = ANSI_REMOVAL_RE.sub(\"\", text)\n        elif os.name == \"nt\" and _wrap_for_color:\n            file = _wrap_for_color(file, color=color)\n    if text:\n        file.write(text)\n    file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_stream_handle(stream=sys.stdout):\n    handle = stream\n    if os.name == \"nt\":\n        from ._winconsole import get_stream_handle as get_win_stream_handle\n\n        return get_win_stream_handle(stream)\n    return handle", "response": "Get the OS appropriate handle for the corresponding output stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hide_cursor(stream=sys.stdout):\n\n    handle = get_stream_handle(stream=stream)\n    if os.name == \"nt\":\n        from ._winconsole import hide_cursor\n\n        hide_cursor()\n    else:\n        handle.write(\"\\033[?25l\")\n        handle.flush()", "response": "Hide the console cursor on the given stream"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean(ctx):\n    ctx.run(f\"python setup.py clean\")\n    dist = ROOT.joinpath(\"dist\")\n    print(f\"[clean] Removing {dist}\")\n    if dist.exists():\n        shutil.rmtree(str(dist))", "response": "Clean previously built package artifacts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister the surrogateescape error handler on Python 3", "response": "def register_surrogateescape():\n    \"\"\"\n    Registers the surrogateescape error handler on Python 2 (only)\n    \"\"\"\n    if six.PY3:\n        return\n    try:\n        codecs.lookup_error(FS_ERRORS)\n    except LookupError:\n        codecs.register_error(FS_ERRORS, surrogateescape_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_write_bit(fn):\n    # type: (str) -> None\n    \"\"\"\n    Set read-write permissions for the current user on the target path.  Fail silently\n    if the path doesn't exist.\n\n    :param str fn: The target filename or path\n    :return: None\n    \"\"\"\n\n    fn = fs_encode(fn)\n    if not os.path.exists(fn):\n        return\n    file_stat = os.stat(fn).st_mode\n    os.chmod(fn, file_stat | stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n    if os.name == \"nt\":\n        from ._winconsole import get_current_user\n\n        user_sid = get_current_user()\n        icacls_exe = _find_icacls_exe() or \"icacls\"\n        from .misc import run\n        if user_sid:\n            _, err = run([icacls_exe, \"/grant\", \"{0}:WD\".format(user_sid), \"''{0}''\".format(fn), \"/T\", \"/C\", \"/Q\"])\n            if not err:\n                return\n\n    if not os.path.isdir(fn):\n        for path in [fn, os.path.dirname(fn)]:\n            try:\n                os.chflags(path, 0)\n            except AttributeError:\n                pass\n        return None\n    for root, dirs, files in os.walk(fn, topdown=False):\n        for dir_ in [os.path.join(root, d) for d in dirs]:\n            set_write_bit(dir_)\n        for file_ in [os.path.join(root, f) for f in files]:\n            set_write_bit(file_)", "response": "Sets the read - write bit for the current user on the target path. Fail silently\n    Set read - write permissions for the current user on the target path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef arg_to_dict(arg):\n    if arg is None:\n        arg = []\n    try:\n        arg = dict(arg)\n    except ValueError:\n        arg = dict.fromkeys(list(arg), {})\n\n    return arg", "response": "Convert an argument that can be None list tuple or dict to dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict representation of a model.", "response": "def asdict(model, exclude=None, exclude_underscore=None, exclude_pk=None,\n           follow=None, include=None, only=None, method='asdict', **kwargs):\n    \"\"\"Get a dict from a model\n\n    Using the `method` parameter makes it possible to have multiple methods\n    that formats the result.\n\n    Additional keyword arguments will be passed to all relationships that are\n    followed. This can be used to pass on things like request or context.\n\n    :param follow: List or dict of relationships that should be followed.\n            If the parameter is a dict the value should be a dict of \\\n            keyword arguments. Currently it follows InstrumentedList, \\\n            MappedCollection and regular 1:1, 1:m, m:m relationships. Follow \\\n            takes an extra argument, 'method', which is the method that \\\n            should be used on the relation. It also takes the extra argument \\\n            'parent' which determines where the relationships data should be \\\n            added in the response dict. If 'parent' is set the relationship \\\n            will be added with it's own key as a child to `parent`.\n    :param exclude: List of properties that should be excluded, will be \\\n            merged with `model.dictalchemy_exclude`\n    :param exclude_pk: If True any column that refers to the primary key will \\\n            be excluded.\n    :param exclude_underscore: Overides `model.dictalchemy_exclude_underscore`\\\n            if set\n    :param include: List of properties that should be included. Use this to \\\n            allow python properties to be called. This list will be merged \\\n            with `model.dictalchemy_asdict_include` or \\\n            `model.dictalchemy_include`.\n    :param only: List of properties that should be included. This will \\\n            override everything else except `follow`.\n    :param method: Name of the method that is currently called. This will be \\\n            the default method used in 'follow' unless another method is\\\n            set.\n\n    :raises: :class:`dictalchemy.errors.MissingRelationError` \\\n            if `follow` contains a non-existent relationship.\n    :raises: :class:`dictalchemy.errors.UnsupportedRelationError` If `follow` \\\n            contains an existing relationship that currently isn't supported.\n\n    :returns: dict\n\n    \"\"\"\n\n    follow = arg_to_dict(follow)\n\n    info = inspect(model)\n\n    columns = [c.key for c in info.mapper.column_attrs]\n    synonyms = [c.key for c in info.mapper.synonyms]\n\n    if only:\n        attrs = only\n    else:\n        exclude = exclude or []\n        exclude += getattr(model, 'dictalchemy_exclude',\n                           constants.default_exclude) or []\n        if exclude_underscore is None:\n            exclude_underscore = getattr(model,\n                                         'dictalchemy_exclude_underscore',\n                                         constants.default_exclude_underscore)\n        if exclude_underscore:\n            # Exclude all properties starting with underscore\n            exclude += [k.key for k in info.mapper.attrs if k.key[0] == '_']\n        if exclude_pk is True:\n            exclude += [c.key for c in info.mapper.primary_key]\n\n        include = (include or []) + (getattr(model,\n                                             'dictalchemy_asdict_include',\n                                             getattr(model,\n                                                     'dictalchemy_include',\n                                                     None)) or [])\n        attrs = [k for k in columns + synonyms + include if k not in exclude]\n\n    data = dict([(k, getattr(model, k)) for k in attrs])\n\n    for (rel_key, orig_args) in follow.iteritems():\n\n        try:\n            rel = getattr(model, rel_key)\n        except AttributeError:\n            raise errors.MissingRelationError(rel_key)\n\n        args = copy.deepcopy(orig_args)\n        method = args.pop('method', method)\n        args['method'] = method\n        args.update(copy.copy(kwargs))\n\n        if hasattr(rel, method):\n            rel_data = getattr(rel, method)(**args)\n        elif isinstance(rel, (list, _AssociationList)):\n            rel_data = []\n\n            for child in rel:\n                if hasattr(child, method):\n                    rel_data.append(getattr(child, method)(**args))\n                else:\n                    try:\n                        rel_data.append(dict(child))\n                        # TypeError is for non-dictable children\n                    except TypeError:\n                        rel_data.append(copy.copy(child))\n\n        elif isinstance(rel, dict):\n            rel_data = {}\n\n            for (child_key, child) in rel.iteritems():\n                if hasattr(child, method):\n                    rel_data[child_key] = getattr(child, method)(**args)\n                else:\n                    try:\n                        rel_data[child_key] = dict(child)\n                    except ValueError:\n                        rel_data[child_key] = copy.copy(child)\n\n        elif isinstance(rel, (AppenderMixin, Query)):\n            rel_data = []\n\n            for child in rel.all():\n                if hasattr(child, method):\n                    rel_data.append(getattr(child, method)(**args))\n                else:\n                    rel_data.append(dict(child))\n\n        elif rel is None:\n            rel_data = None\n        else:\n            raise errors.UnsupportedRelationError(rel_key)\n\n        ins_key = args.pop('parent', None)\n\n        if ins_key is None:\n            data[rel_key] = rel_data\n        else:\n            if ins_key not in data:\n                data[ins_key] = {}\n\n            data[ins_key][rel_key] = rel_data\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromdict(model, data, exclude=None, exclude_underscore=None,\n             allow_pk=None, follow=None, include=None, only=None):\n    \"\"\"Update a model from a dict\n\n    Works almost identically as :meth:`dictalchemy.utils.asdict`. However, it\n    will not create missing instances or update collections.\n\n    This method updates the following properties on a model:\n\n    * Simple columns\n    * Synonyms\n    * Simple 1-m relationships\n\n    :param data: dict of data\n    :param exclude: list of properties that should be excluded\n    :param exclude_underscore: If True underscore properties will be excluded,\\\n            if set to None model.dictalchemy_exclude_underscore will be used.\n    :param allow_pk: If True any column that refers to the primary key will \\\n            be excluded. Defaults model.dictalchemy_fromdict_allow_pk or \\\n            dictable.constants.fromdict_allow_pk. If set to True a primary \\\n            key can still be excluded with the `exclude` parameter.\n    :param follow: Dict of relations that should be followed, the key is the \\\n            arguments passed to the relation. Relations only works on simple \\\n            relations, not on lists.\n    :param include: List of properties that should be included. This list \\\n            will override anything in the exclude list. It will not override \\\n            allow_pk.\n    :param only: List of the only properties that should be set. This \\\n            will not override `allow_pk` or `follow`.\n\n    :raises: :class:`dictalchemy.errors.DictalchemyError` If a primary key is \\\n            in data and allow_pk is False\n\n    :returns: The model\n\n    \"\"\"\n\n    follow = arg_to_dict(follow)\n\n    info = inspect(model)\n    columns = [c.key for c in info.mapper.column_attrs]\n    synonyms = [c.key for c in info.mapper.synonyms]\n    relations = [c.key for c in info.mapper.relationships]\n    primary_keys = [c.key for c in info.mapper.primary_key]\n\n    if allow_pk is None:\n        allow_pk = getattr(model, 'dictalchemy_fromdict_allow_pk',\n                           constants.default_fromdict_allow_pk)\n\n    if only:\n        valid_keys = only\n    else:\n        exclude = exclude or []\n        exclude += getattr(model, 'dictalchemy_exclude',\n                           constants.default_exclude) or []\n        if exclude_underscore is None:\n            exclude_underscore = getattr(model,\n                                         'dictalchemy_exclude_underscore',\n                                         constants.default_exclude_underscore)\n\n        if exclude_underscore:\n            # Exclude all properties starting with underscore\n            exclude += [k.key for k in info.mapper.attrs if k.key[0] == '_']\n\n        include = (include or []) + (getattr(model,\n                                             'dictalchemy_fromdict_include',\n                                             getattr(model,\n                                                     'dictalchemy_include',\n                                                     None)) or [])\n        valid_keys = [k for k in columns + synonyms\n                      if k not in exclude] + include\n\n    # Keys that will be updated\n    update_keys = set(valid_keys) & set(data.keys())\n\n    # Check for primary keys\n    data_primary_key= update_keys & set(primary_keys)\n    if len(data_primary_key) and not allow_pk:\n        msg = (\"Primary keys({0}) cannot be updated by fromdict.\"\n               \"Set 'dictalchemy_fromdict_allow_pk' to True in your Model\"\n               \" or pass 'allow_pk=True'.\").format(','.join(data_primary_key))\n        raise errors.DictalchemyError(msg)\n\n    # Update columns and synonyms\n    for k in update_keys:\n        setattr(model, k, data[k])\n\n    # Update simple relations\n    for (k, args) in follow.iteritems():\n        if k not in data:\n            continue\n        if k not in relations:\n            raise errors.MissingRelationError(k)\n        rel = getattr(model, k)\n        if hasattr(rel, 'fromdict'):\n            rel.fromdict(data[k], **args)\n\n    return model", "response": "Update a model from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_class_dictable(\n        cls,\n        exclude=constants.default_exclude,\n        exclude_underscore=constants.default_exclude_underscore,\n        fromdict_allow_pk=constants.default_fromdict_allow_pk,\n        include=None,\n        asdict_include=None,\n        fromdict_include=None):\n    \"\"\"Make a class dictable\n\n    Useful for when the Base class is already defined, for example when using\n    Flask-SQLAlchemy.\n\n    Warning: This method will overwrite existing attributes if they exists.\n\n    :param exclude: Will be set as dictalchemy_exclude on the class\n    :param exclude_underscore: Will be set as dictalchemy_exclude_underscore \\\n            on the class\n    :param fromdict_allow_pk: Will be set as dictalchemy_fromdict_allow_pk\\\n            on the class\n    :param include: Will be set as dictalchemy_include on the class.\n    :param asdict_include: Will be set as `dictalchemy_asdict_include` on the \\\n            class. If not None it will override `dictalchemy_include`.\n    :param fromdict_include: Will be set as `dictalchemy_fromdict_include` on \\\n            the class. If not None it will override `dictalchemy_include`.\n\n    :returns: The class\n    \"\"\"\n\n    setattr(cls, 'dictalchemy_exclude', exclude)\n    setattr(cls, 'dictalchemy_exclude_underscore', exclude_underscore)\n    setattr(cls, 'dictalchemy_fromdict_allow_pk', fromdict_allow_pk)\n    setattr(cls, 'asdict', asdict)\n    setattr(cls, 'fromdict', fromdict)\n    setattr(cls, '__iter__', iter)\n    setattr(cls, 'dictalchemy_include', include)\n    setattr(cls, 'dictalchemy_asdict_include', asdict_include)\n    setattr(cls, 'dictalchemy_fromdict_include', fromdict_include)\n    return cls", "response": "Make a class dictable by setting the base class attributes to the class attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_GFF_attribute_string(attrStr, extra_return_first_value=False):\n    if attrStr.endswith(\"\\n\"):\n        attrStr = attrStr[:-1]\n    d = {}\n    first_val = \"_unnamed_\"\n    for (i, attr) in itertools.izip(\n            itertools.count(),\n            _HTSeq.quotesafe_split(attrStr)):\n        if _re_attr_empty.match(attr):\n            continue\n        if attr.count('\"') not in (0, 2):\n            raise ValueError(\n                \"The attribute string seems to contain mismatched quotes.\")\n        mo = _re_attr_main.match(attr)\n        if not mo:\n            raise ValueError(\"Failure parsing GFF attribute line\")\n        val = mo.group(2)\n        if val.startswith('\"') and val.endswith('\"'):\n            val = val[1:-1]\n        d[intern(mo.group(1))] = intern(val)\n        if extra_return_first_value and i == 0:\n            first_val = val\n    if extra_return_first_value:\n        return (d, first_val)\n    else:\n        return d", "response": "Parses a GFF attribute string and returns it as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_feature_dict(feature_sequence):\n\n    res = {}\n    for f in feature_sequence:\n        if f.type not in res:\n            res[f.type] = {}\n        res_ftype = res[f.type]\n        if f.name not in res_ftype:\n            res_ftype[f.name] = [f]\n        else:\n            res_ftype[f.name].append(f)\n    return res", "response": "This function creates a dict of all the feature types and names in the feature_sequence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pair_SAM_alignments(\n        alignments,\n        bundle=False,\n        primary_only=False):\n    '''Iterate over SAM aligments, name-sorted paired-end\n\n    Args:\n        alignments (iterator of SAM/BAM alignments): the alignments to wrap\n        bundle (bool): if True, bundle all alignments from one read pair into a\n            single yield. If False (default), each pair of alignments is\n            yielded separately.\n        primary_only (bool): for each read, consider only the primary line\n            (SAM flag 0x900 = 0). The SAM specification requires one and only\n            one of those for each read.\n\n    Yields:\n        2-tuples with each pair of alignments or, if bundle==True, each bundled\n        list of alignments.\n    '''\n\n    mate_missing_count = [0]\n\n    def process_list(almnt_list):\n        '''Transform a list of alignment with the same read name into pairs\n\n        Args:\n            almnt_list (list): alignments to process\n\n        Yields:\n            each pair of alignments.\n\n        This function is needed because each line of a BAM file is not a read\n        but an alignment. For uniquely mapped and unmapped reads, those two are\n        the same. For multimapped reads, however, there can be more than one\n        alignment for each read. Also, it is normal for a mapper to uniquely\n        map one read and multimap its mate.\n\n        This function goes down the list of alignments for a given read name\n        and tries to find the first mate. So if read 1 is uniquely mapped but\n        read 2 is mapped 4 times, only (read 1, read 2 - first occurrence) will\n        yield; the other 3 alignments of read 2 are ignored.\n        '''\n\n\n        while len(almnt_list) > 0:\n            a1 = almnt_list.pop(0)\n            # Find its mate\n            for a2 in almnt_list:\n                if a1.pe_which == a2.pe_which:\n                    continue\n                if a1.aligned != a2.mate_aligned or a1.mate_aligned != a2.aligned:\n                    continue\n                if not (a1.aligned and a2.aligned):\n                    break\n                if a1.iv.chrom == a2.mate_start.chrom and a1.iv.start == a2.mate_start.pos and \\\n                   a2.iv.chrom == a1.mate_start.chrom and a2.iv.start == a1.mate_start.pos:\n                    break\n            else:\n                if a1.mate_aligned:\n                    mate_missing_count[0] += 1\n                    if mate_missing_count[0] == 1:\n                        warnings.warn(\n                            \"Read \" + a1.read.name + \" claims to have an aligned mate \" +\n                            \"which could not be found in an adjacent line.\")\n                a2 = None\n            if a2 is not None:\n                almnt_list.remove(a2)\n            if a1.pe_which == \"first\":\n                yield (a1, a2)\n            else:\n                assert a1.pe_which == \"second\"\n                yield (a2, a1)\n\n    almnt_list = []\n    current_name = None\n    for almnt in alignments:\n        if not almnt.paired_end:\n            raise ValueError(\n                \"'pair_alignments' needs a sequence of paired-end alignments\")\n        if almnt.pe_which == \"unknown\":\n            raise ValueError(\n                \"Paired-end read found with 'unknown' 'pe_which' status.\")\n        # FIXME: almnt.not_primary_alignment currently means secondary\n        if primary_only and (almnt.not_primary_alignment or almnt.supplementary):\n            continue\n        if almnt.read.name == current_name:\n            almnt_list.append(almnt)\n        else:\n            if bundle:\n                yield list(process_list(almnt_list))\n            else:\n                for p in process_list(almnt_list):\n                    yield p\n            current_name = almnt.read.name\n            almnt_list = [almnt]\n    if bundle:\n        yield list(process_list(almnt_list))\n    else:\n        for p in process_list(almnt_list):\n            yield p\n    if mate_missing_count[0] > 1:\n        warnings.warn(\"%d reads with missing mate encountered.\" %\n                      mate_missing_count[0])", "response": "Iterate over SAM aligments and yield the list of pairs of a specific read name and alignment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct a new StepVector of the given length.", "response": "def create( cls, length = sys.maxint, typecode = 'd', start_index = 0 ):\n      \"\"\"Construct a StepVector of the given length, with indices starting\n      at the given start_index and counting up to (but not including)\n      start_index + length.\n\n      The typecode may be:\n        'd' for float values (C type 'double'),\n        'i' for int values,\n        'b' for Boolean values,\n        'O' for arbitrary Python objects as value.\n\n      The vector is initialized with the value zero (or, for typecode 'O',\n      with None).\n      \"\"\"\n      if typecode == 'd':\n         swigclass = _StepVector_float\n      elif typecode == 'i':\n         swigclass = _StepVector_int\n      elif typecode == 'b':\n         swigclass = _StepVector_bool\n      elif typecode == 'O':\n         swigclass = _StepVector_obj\n      else:\n         raise ValueError, \"unsupported typecode\"\n      obj = cls()\n      obj._typecode = typecode\n      obj._swigobj = swigclass( )    \n      obj.start = start_index\n      obj.stop = start_index + length\n      return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over SAM aligments with buffer position - sorted paired - end and return a list of 2 - tuples with each pair of alignments.", "response": "def pair_SAM_alignments_with_buffer(\n        alignments,\n        max_buffer_size=30000000,\n        primary_only=False):\n    '''Iterate over SAM aligments with buffer, position-sorted paired-end\n\n    Args:\n        alignments (iterator of SAM/BAM alignments): the alignments to wrap\n        max_buffer_size (int): maxmal numer of alignments to keep in memory.\n        primary_only (bool): for each read, consider only the primary line\n            (SAM flag 0x900 = 0). The SAM specification requires one and only\n            one of those for each read.\n\n    Yields:\n        2-tuples with each pair of alignments.\n    '''\n\n    almnt_buffer = {}\n    ambiguous_pairing_counter = 0\n    for almnt in alignments:\n        if not almnt.paired_end:\n            raise ValueError(\n                \"Sequence of paired-end alignments expected, but got single-end alignment.\")\n        if almnt.pe_which == \"unknown\":\n            raise ValueError(\n                \"Cannot process paired-end alignment found with 'unknown' 'pe_which' status.\")\n        # FIXME: almnt.not_primary_alignment currently means secondary\n        if primary_only and (almnt.not_primary_alignment or almnt.supplementary):\n            continue\n\n        matekey = (\n            almnt.read.name,\n            \"second\" if almnt.pe_which == \"first\" else \"first\",\n            almnt.mate_start.chrom if almnt.mate_aligned else None,\n            almnt.mate_start.pos if almnt.mate_aligned else None,\n            almnt.iv.chrom if almnt.aligned else None,\n            almnt.iv.start if almnt.aligned else None,\n            -almnt.inferred_insert_size if almnt.aligned and almnt.mate_aligned else None)\n\n        if matekey in almnt_buffer:\n            if len(almnt_buffer[matekey]) == 1:\n                mate = almnt_buffer[matekey][0]\n                del almnt_buffer[matekey]\n            else:\n                mate = almnt_buffer[matekey].pop(0)\n                if ambiguous_pairing_counter == 0:\n                    ambiguous_pairing_first_occurance = matekey\n                ambiguous_pairing_counter += 1\n            if almnt.pe_which == \"first\":\n                yield (almnt, mate)\n            else:\n                yield (mate, almnt)\n        else:\n            almntkey = (\n                almnt.read.name, almnt.pe_which,\n                almnt.iv.chrom if almnt.aligned else None,\n                almnt.iv.start if almnt.aligned else None,\n                almnt.mate_start.chrom if almnt.mate_aligned else None,\n                almnt.mate_start.pos if almnt.mate_aligned else None,\n                almnt.inferred_insert_size if almnt.aligned and almnt.mate_aligned else None)\n            if almntkey not in almnt_buffer:\n                almnt_buffer[almntkey] = [almnt]\n            else:\n                almnt_buffer[almntkey].append(almnt)\n            if len(almnt_buffer) > max_buffer_size:\n                raise ValueError(\n                    \"Maximum alignment buffer size exceeded while pairing SAM alignments.\")\n\n    if len(almnt_buffer) > 0:\n        warnings.warn(\n            \"Mate records missing for %d records; first such record: %s.\" %\n            (len(almnt_buffer), str(list(almnt_buffer.values())[0][0])))\n        for almnt_list in list(almnt_buffer.values()):\n            for almnt in almnt_list:\n                if almnt.pe_which == \"first\":\n                    yield (almnt, None)\n                else:\n                    yield (None, almnt)\n\n    if ambiguous_pairing_counter > 0:\n        warnings.warn(\n            \"Mate pairing was ambiguous for %d records; mate key for first such record: %s.\" %\n            (ambiguous_pairing_counter, str(ambiguous_pairing_first_occurance)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that v8 src is present and up - to - date.", "response": "def ensure_v8_src():\n    \"\"\" Ensure that v8 src are presents and up-to-date\n    \"\"\"\n    path = local_path('v8')\n\n    if not os.path.isdir(path):\n        fetch_v8(path)\n    else:\n        update_v8(path)\n\n    checkout_v8_version(local_path(\"v8/v8\"), V8_VERSION)\n    dependencies_sync(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all the files to copy", "response": "def get_filenames(directory):\n    \"\"\"Get all the file to copy\"\"\"\n    for filename in os.listdir(directory):\n        if re.search(r\"cp\\d{2}mu?-manylinux1_\\S+\\.whl\", filename):\n            yield filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies the file and put the correct tag", "response": "def copy_file(filename):\n    \"\"\"Copy the file and put the correct tag\"\"\"\n\n    print(\"Updating file %s\" % filename)\n    out_dir = os.path.abspath(DIRECTORY)\n\n    tags = filename[:-4].split(\"-\")\n\n    tags[-2] = tags[-2].replace(\"m\", \"\")\n\n    new_name = \"-\".join(tags) + \".whl\"\n    wheel_flag = \"-\".join(tags[2:])\n\n    with InWheelCtx(os.path.join(DIRECTORY, filename)) as ctx:\n        info_fname = os.path.join(_dist_info_dir(ctx.path), 'WHEEL')\n        infos = pkginfo.read_pkg_info(info_fname)\n        print(\"Changing Tag %s to %s\" % (infos[\"Tag\"], wheel_flag))\n        del infos['Tag']\n        infos.add_header('Tag', wheel_flag)\n        pkginfo.write_pkg_info(info_fname, infos)\n\n        ctx.out_wheel = os.path.join(out_dir, new_name)\n\n        print(\"Saving new wheel into %s\" % ctx.out_wheel)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a value is a valid unicode string", "response": "def is_unicode(value):\n    \"\"\" Check if a value is a valid unicode string, compatible with python 2 and python 3\n\n    >>> is_unicode(u'foo')\n    True\n    >>> is_unicode(u'\u270c')\n    True\n    >>> is_unicode(b'foo')\n    False\n    >>> is_unicode(42)\n    False\n    >>> is_unicode(('abc',))\n    False\n    \"\"\"\n    python_version = sys.version_info[0]\n\n    if python_version == 2:\n        return isinstance(value, unicode)\n    elif python_version == 3:\n        return isinstance(value, str)\n    else:\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the given JS value", "response": "def execute(self, js_str, timeout=0, max_memory=0):\n        \"\"\" Exec the given JS value \"\"\"\n\n        wrapped = \"(function(){return (%s)})()\" % js_str\n        return self.eval(wrapped, timeout, max_memory)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate the JavaScript string and return the result.", "response": "def eval(self, js_str, timeout=0, max_memory=0):\n        \"\"\" Eval the JavaScript string \"\"\"\n\n        if is_unicode(js_str):\n            bytes_val = js_str.encode(\"utf8\")\n        else:\n            bytes_val = js_str\n\n        res = None\n        self.lock.acquire()\n        try:\n            res = self.ext.mr_eval_context(self.ctx,\n                                           bytes_val,\n                                           len(bytes_val),\n                                           ctypes.c_ulong(timeout),\n                                           ctypes.c_size_t(max_memory))\n\n            if bool(res) is False:\n                raise JSConversionException()\n            python_value = res.contents.to_python()\n            return python_value\n        finally:\n            self.lock.release()\n            if res is not None:\n                self.free(res)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call(self, identifier, *args, **kwargs):\n\n        encoder = kwargs.get('encoder', None)\n        timeout = kwargs.get('timeout', 0)\n        max_memory = kwargs.get('max_memory', 0)\n\n        json_args = json.dumps(args, separators=(',', ':'), cls=encoder)\n        js = \"{identifier}.apply(this, {json_args})\"\n        return self.eval(js.format(identifier=identifier, json_args=json_args), timeout, max_memory)", "response": "Call the named function with provided arguments"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the heap snapshot", "response": "def heap_snapshot(self):\n        \"\"\" Return heap snapshot \"\"\"\n\n        self.lock.acquire()\n        res = self.ext.mr_heap_snapshot(self.ctx)\n        self.lock.release()\n\n        python_value = res.contents.to_python()\n        self.free(res)\n        return python_value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an object as native Python", "response": "def to_python(self):\n        \"\"\" Return an object as native Python \"\"\"\n\n        result = None\n        if self.type == PythonTypes.null:\n            result = None\n        elif self.type == PythonTypes.bool:\n            result = self.value == 1\n        elif self.type == PythonTypes.integer:\n            if self.value is None:\n                result = 0\n            else:\n                result = ctypes.c_int32(self.value).value\n        elif self.type == PythonTypes.double:\n            result = self._double_value()\n        elif self.type == PythonTypes.str_utf8:\n            buf = ctypes.c_char_p(self.value)\n            ptr = ctypes.cast(buf, ctypes.POINTER(ctypes.c_char))\n            result = ptr[0:self.len].decode(\"utf8\")\n        elif self.type == PythonTypes.array:\n            if self.len == 0:\n                return []\n            ary = []\n            ary_addr = ctypes.c_void_p.from_address(self.value)\n            ptr_to_ary = ctypes.pointer(ary_addr)\n            for i in range(self.len):\n                pval = PythonValue.from_address(ptr_to_ary[i])\n                ary.append(pval.to_python())\n            result = ary\n        elif self.type == PythonTypes.hash:\n            if self.len == 0:\n                return {}\n            res = {}\n            hash_ary_addr = ctypes.c_void_p.from_address(self.value)\n            ptr_to_hash = ctypes.pointer(hash_ary_addr)\n            for i in range(self.len):\n                pkey = PythonValue.from_address(ptr_to_hash[i*2])\n                pval = PythonValue.from_address(ptr_to_hash[i*2+1])\n                res[pkey.to_python()] = pval.to_python()\n            result = res\n        elif self.type == PythonTypes.function:\n            result = JSFunction()\n        elif self.type == PythonTypes.parse_exception:\n            msg = ctypes.c_char_p(self.value).value\n            raise JSParseException(msg)\n        elif self.type == PythonTypes.execute_exception:\n            msg = ctypes.c_char_p(self.value).value\n            raise JSEvalException(msg.decode('utf-8', errors='replace'))\n        elif self.type == PythonTypes.oom_exception:\n            msg = ctypes.c_char_p(self.value).value\n            raise JSOOMException(msg)\n        elif self.type == PythonTypes.timeout_exception:\n            msg = ctypes.c_char_p(self.value).value\n            raise JSTimeoutException(msg)\n        elif self.type == PythonTypes.date:\n            timestamp = self._double_value()\n            # JS timestamp are milliseconds, in python we are in seconds\n            result = datetime.datetime.utcfromtimestamp(timestamp / 1000.)\n        else:\n            raise WrongReturnTypeException(\"unknown type %d\" % self.type)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef libv8_object(object_name):\n\n    filename = join(V8_LIB_DIRECTORY, 'out.gn/x64.release/obj/{}'.format(object_name))\n\n    if not isfile(filename):\n        filename = join(local_path('vendor/v8/out.gn/libv8/obj/{}'.format(object_name)))\n\n    if not isfile(filename):\n        filename = join(V8_LIB_DIRECTORY, 'out.gn/x64.release/obj/{}'.format(object_name))\n\n    return filename", "response": "Return a path for object_name which is OS independent\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_static_lib_paths():\n    libs = []\n    is_linux = sys.platform.startswith('linux')\n    if is_linux:\n        libs += ['-Wl,--start-group']\n    libs += get_raw_static_lib_path()\n    if is_linux:\n        libs += ['-Wl,--end-group']\n    return libs", "response": "Return the required static libraries path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompiling manually the py_mini_racer extension bypass setuptools", "response": "def build_extension(self, ext):\n        \"\"\" Compile manually the py_mini_racer extension, bypass setuptools\n        \"\"\"\n        try:\n            if not is_v8_built():\n                self.run_command('build_v8')\n\n            self.debug = True\n            if V8_PATH:\n                dest_filename = join(self.build_lib, \"py_mini_racer\")\n                copy_file(V8_PATH, dest_filename, verbose=self.verbose, dry_run=self.dry_run)\n            else:\n                build_ext.build_extension(self, ext)\n\n        except Exception as e:\n            traceback.print_exc()\n\n            # Alter message\n            err_msg = \"\"\"py_mini_racer failed to build, ensure you have an up-to-date pip (>= 8.1) to use the wheel instead\n            To update pip: 'pip install -U pip'\n            See also: https://github.com/sqreen/PyMiniRacer#binary-builds-availability\n\n            Original error: %s\"\"\"\n\n            raise Exception(err_msg % repr(e))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming the provided string using babel. transform", "response": "def babel_transform(es_string):\n    \"\"\" Transform the provided string using babel.transform \"\"\"\n\n    path_to_babel = os.path.join(os.path.dirname(__file__), '..', 'tests',\n                                 'fixtures', 'babel.js')\n\n    es6 = '[1,2,3].map(n => n + 1);'\n\n    babel_source = open(path_to_babel, \"r\").read()\n\n    # Initializes PyMiniRacer\n    ctx = py_mini_racer.MiniRacer()\n\n    # Parse babel\n    ctx.eval(\"\"\"var self = this; %s \"\"\" % babel_source)\n\n    # Transform stuff :)\n    val = \"babel.transform(`%s`)['code']\" % es_string\n    res = ctx.eval(val)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntag Gifs Endpoint. Returns a list of gifs for a given tag (alias to `/gif/search`). This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.gifs_categories_category_tag_get(api_key, category, tag, callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :param str api_key: Giphy API Key. (required) :param str category: Filters results by category. (required) :param str tag: Filters results by tag. (required) :param int limit: The maximum number of records to return. :param int offset: An optional results offset. Defaults to 0. :return: InlineResponse2005 If the method is called asynchronously, returns the request thread.", "response": "def gifs_categories_category_tag_get(self, api_key, category, tag, **kwargs):\n        \"\"\"\n        Tagged Gifs Endpoint.\n        Returns a list of gifs for a given tag (alias to `/gif/search`).\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please define a `callback` function\n        to be invoked when receiving the response.\n        >>> def callback_function(response):\n        >>>     pprint(response)\n        >>>\n        >>> thread = api.gifs_categories_category_tag_get(api_key, category, tag, callback=callback_function)\n\n        :param callback function: The callback function\n            for asynchronous request. (optional)\n        :param str api_key: Giphy API Key. (required)\n        :param str category: Filters results by category. (required)\n        :param str tag: Filters results by tag. (required)\n        :param int limit: The maximum number of records to return.\n        :param int offset: An optional results offset. Defaults to 0.\n        :return: InlineResponse2005\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.gifs_categories_category_tag_get_with_http_info(api_key, category, tag, **kwargs)\n        else:\n            (data) = self.gifs_categories_category_tag_get_with_http_info(api_key, category, tag, **kwargs)\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets GIF by ID Endpoint Returns a GIF given that GIF's unique ID This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.gifs_gif_id_get(api_key, gif_id, callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :param str api_key: Giphy API Key. (required) :param str gif_id: Filters results by specified GIF ID. (required) :return: InlineResponse2001 If the method is called asynchronously, returns the request thread.", "response": "def gifs_gif_id_get(self, api_key, gif_id, **kwargs):\n        \"\"\"\n        Get GIF by ID Endpoint\n        Returns a GIF given that GIF's unique ID\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please define a `callback` function\n        to be invoked when receiving the response.\n        >>> def callback_function(response):\n        >>>     pprint(response)\n        >>>\n        >>> thread = api.gifs_gif_id_get(api_key, gif_id, callback=callback_function)\n\n        :param callback function: The callback function\n            for asynchronous request. (optional)\n        :param str api_key: Giphy API Key. (required)\n        :param str gif_id: Filters results by specified GIF ID. (required)\n        :return: InlineResponse2001\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.gifs_gif_id_get_with_http_info(api_key, gif_id, **kwargs)\n        else:\n            (data) = self.gifs_gif_id_get_with_http_info(api_key, gif_id, **kwargs)\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gifs_search_get(self, api_key, q, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.gifs_search_get_with_http_info(api_key, q, **kwargs)\n        else:\n            (data) = self.gifs_search_get_with_http_info(api_key, q, **kwargs)\n            return data", "response": "Search Endpoint Giphy GIFs for a word or phrase."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gifs_trending_get(self, api_key, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.gifs_trending_get_with_http_info(api_key, **kwargs)\n        else:\n            (data) = self.gifs_trending_get_with_http_info(api_key, **kwargs)\n            return data", "response": "This method returns the current set of GIFs currently trending online. Hand curated by the GIPHY editorial team."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stickers_random_get(self, api_key, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.stickers_random_get_with_http_info(api_key, **kwargs)\n        else:\n            (data) = self.stickers_random_get_with_http_info(api_key, **kwargs)\n            return data", "response": "Random Sticker Endpoint\n        Returns a random GIF limited by tag. Excluding the tag parameter will return a random GIF from the GIPHY catalog."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stickers_trending_get(self, api_key, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.stickers_trending_get_with_http_info(api_key, **kwargs)\n        else:\n            (data) = self.stickers_trending_get_with_http_info(api_key, **kwargs)\n            return data", "response": "This method returns the current set of GIPHY stickers currently trending online."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_api(self, resource_path, method,\n                 path_params=None, query_params=None, header_params=None,\n                 body=None, post_params=None, files=None,\n                 response_type=None, auth_settings=None, callback=None,\n                 _return_http_data_only=None, collection_formats=None, _preload_content=True,\n                 _request_timeout=None):\n        \"\"\"\n        Makes the HTTP request (synchronous) and return the deserialized data.\n        To make an async request, define a function for callback.\n\n        :param resource_path: Path to method endpoint.\n        :param method: Method to call.\n        :param path_params: Path parameters in the url.\n        :param query_params: Query parameters in the url.\n        :param header_params: Header parameters to be\n            placed in the request header.\n        :param body: Request body.\n        :param post_params dict: Request post form parameters,\n            for `application/x-www-form-urlencoded`, `multipart/form-data`.\n        :param auth_settings list: Auth Settings names for the request.\n        :param response: Response data type.\n        :param files dict: key -> filename, value -> filepath,\n            for `multipart/form-data`.\n        :param callback function: Callback function for asynchronous request.\n            If provide this parameter,\n            the request will be called asynchronously.\n        :param _return_http_data_only: response data without head status code and headers\n        :param collection_formats: dict of collection formats for path, query,\n            header, and post parameters.\n        :param _preload_content: if False, the urllib3.HTTPResponse object will be returned without\n                                 reading/decoding response data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of (connection, read) timeouts.\n        :return:\n            If provide parameter callback,\n            the request will be called asynchronously.\n            The method will return the request thread.\n            If parameter callback is None,\n            then the method will return the response directly.\n        \"\"\"\n        if callback is None:\n            return self.__call_api(resource_path, method,\n                                   path_params, query_params, header_params,\n                                   body, post_params, files,\n                                   response_type, auth_settings, callback,\n                                   _return_http_data_only, collection_formats, _preload_content, _request_timeout)\n        else:\n            thread = threading.Thread(target=self.__call_api,\n                                      args=(resource_path, method,\n                                            path_params, query_params,\n                                            header_params, body,\n                                            post_params, files,\n                                            response_type, auth_settings,\n                                            callback, _return_http_data_only,\n                                            collection_formats, _preload_content, _request_timeout))\n        thread.start()\n        return thread", "response": "Makes a HTTP request to the remote server and returns the deserialized data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_params_for_auth(self, headers, querys, auth_settings):\n        config = Configuration()\n\n        if not auth_settings:\n            return\n\n        for auth in auth_settings:\n            auth_setting = config.auth_settings().get(auth)\n            if auth_setting:\n                if not auth_setting['value']:\n                    continue\n                elif auth_setting['in'] == 'header':\n                    headers[auth_setting['key']] = auth_setting['value']\n                elif auth_setting['in'] == 'query':\n                    querys.append((auth_setting['key'], auth_setting['value']))\n                else:\n                    raise ValueError(\n                        'Authentication token must be in `query` or `header`'\n                    )", "response": "Updates header and query params based on authentication settings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_dag_params(self) -> Dict[str, Any]:\n        try:\n            dag_params: Dict[str, Any] = utils.merge_configs(self.dag_config, self.default_config)\n        except Exception as e:\n            raise Exception(f\"Failed to merge config with default config, err: {e}\")\n        dag_params[\"dag_id\"]: str = self.dag_name\n        try:\n            # ensure that default_args dictionary contains key \"start_date\" with \"datetime\" value in specified timezone\n            dag_params[\"default_args\"][\"start_date\"]: datetime = utils.get_start_date(\n                date_value=dag_params[\"default_args\"][\"start_date\"],\n                timezone=dag_params[\"default_args\"].get(\"timezone\", \"UTC\"),\n            )\n        except KeyError as e:\n            raise Exception(f\"{self.dag_name} config is missing start_date, err: {e}\")\n        return dag_params", "response": "Returns a dictionary of dag parameters"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_task(operator: str, task_params: Dict[str, Any]) -> BaseOperator:\n        try:\n            # class is a Callable https://stackoverflow.com/a/34578836/3679900\n            operator_obj: Callable[..., BaseOperator] = import_string(operator)\n        except Exception as e:\n            raise Exception(f\"Failed to import operator: {operator}. err: {e}\")\n        try:\n            task: BaseOperator = operator_obj(**task_params)\n        except Exception as e:\n            raise Exception(f\"Failed to create {operator_obj} task. err: {e}\")\n        return task", "response": "Takes an operator and params and creates an instance of that operator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build(self) -> Dict[str, Union[str, DAG]]:\n        dag_params: Dict[str, Any] = self.get_dag_params()\n        dag: DAG = DAG(\n            dag_id=dag_params[\"dag_id\"],\n            schedule_interval=dag_params[\"schedule_interval\"],\n            description=dag_params.get(\"description\", \"\"),\n            max_active_runs=dag_params.get(\n                \"max_active_runs\",\n                configuration.conf.getint(\"core\", \"max_active_runs_per_dag\"),\n            ),\n            default_args=dag_params.get(\"default_args\", {}),\n        )\n        tasks: Dict[str, Dict[str, Any]] = dag_params[\"tasks\"]\n\n        # create dictionary to track tasks and set dependencies\n        tasks_dict: Dict[str, BaseOperator] = {}\n        for task_name, task_conf in tasks.items():\n            task_conf[\"task_id\"]: str = task_name\n            operator: str = task_conf[\"operator\"]\n            task_conf[\"dag\"]: DAG = dag\n            params: Dict[str, Any] = {k: v for k, v in task_conf.items() if k not in SYSTEM_PARAMS}\n            task: BaseOperator = DagBuilder.make_task(operator=operator, task_params=params)\n            tasks_dict[task.task_id]: BaseOperator = task\n\n        # set task dependencies after creating tasks\n        for task_name, task_conf in tasks.items():\n            if task_conf.get(\"dependencies\"):\n                source_task: BaseOperator = tasks_dict[task_name]\n                for dep in task_conf[\"dependencies\"]:\n                    dep_task: BaseOperator = tasks_dict[dep]\n                    source_task.set_upstream(dep_task)\n\n        return {\"dag_id\": dag_params[\"dag_id\"], \"dag\": dag}", "response": "Creates a DAG from the parameters and returns a dictionary with dag_id and DAG object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_start_date(date_value: Union[str, datetime, date], timezone: str = \"UTC\") -> datetime:\n    try:\n        local_tz: pendulum.Timezone = pendulum.timezone(timezone)\n    except Exception as e:\n        raise Exception(f\"Failed to create timezone; err: {e}\")\n    if isinstance(date_value, date):\n        return datetime.combine(date=date_value, time=datetime.min.time()).replace(tzinfo=local_tz)\n    if isinstance(date_value, datetime):\n        return date_value.replace(tzinfo=local_tz)\n    rel_delta: timedelta = get_time_delta(date_value)\n    now: datetime = (\n        datetime.today()\n            .replace(hour=0, minute=0, second=0, microsecond=0)\n            .replace(tzinfo=local_tz)\n    )\n    if not rel_delta:\n        return now\n    return (now - rel_delta)", "response": "Generates valid start_date for a DAG."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_time_delta(time_string: str) -> timedelta:\n    rel_time: Pattern = re.compile(\n        pattern=r\"((?P<hours>\\d+?)\\s+hour)?((?P<minutes>\\d+?)\\s+minute)?((?P<seconds>\\d+?)\\s+second)?((?P<days>\\d+?)\\s+day)?\",\n        # noqa\n        flags=re.IGNORECASE,\n    )\n    parts: Optional[Match[AnyStr]] = rel_time.match(string=time_string)\n    if not parts:\n        raise Exception(f\"Invalid relative time: {time_string}\")\n    # https://docs.python.org/3/library/re.html#re.Match.groupdict\n    parts: Dict[str, str] = parts.groupdict()\n    time_params = {}\n    if all(value == None for value in parts.values()):\n        raise Exception(f\"Invalid relative time: {time_string}\")\n    for time_unit, magnitude in parts.items():\n        if magnitude:\n            time_params[time_unit]: int = int(magnitude)\n    return timedelta(**time_params)", "response": "Converts a time string to a timedelta object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_configs(config: Dict[str, Any], default_config: Dict[str, Any]) -> Dict[str, Any]:\n    for key in default_config:\n        if key in config:\n            if isinstance(config[key], dict) and isinstance(default_config[key], dict):\n                merge_configs(config[key], default_config[key])\n        else:\n            config[key]: Any = default_config[key]\n    return config", "response": "Merges a default config with a DAG config. Used to set default values\n    for a group of DAGs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_config(config_filepath: str) -> Dict[str, Any]:\n        try:\n            config: Dict[str, Any] = yaml.load(stream=open(config_filepath, \"r\"))\n        except Exception as e:\n            raise Exception(f\"Invalid DAG Factory config file; err: {e}\")\n        return config", "response": "Loads YAML config file to dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns configuration for each the DAG in factory", "response": "def get_dag_configs(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Returns configuration for each the DAG in factory\n\n        :returns: dict with configuration for dags\n        \"\"\"\n        return {dag: self.config[dag] for dag in self.config.keys() if dag != \"default\"}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_dags(self, globals: Dict[str, Any]) -> None:\n        dag_configs: Dict[str, Dict[str, Any]] = self.get_dag_configs()\n        default_config: Dict[str, Any] = self.get_default_config()\n\n        for dag_name, dag_config in dag_configs.items():\n            dag_builder: DagBuilder = DagBuilder(dag_name=dag_name,\n                                                 dag_config=dag_config,\n                                                 default_config=default_config)\n            try:\n                dag: Dict[str, Union[str, DAG]] = dag_builder.build()\n            except Exception as e:\n                raise Exception(\n                    f\"Failed to generate dag {dag_name}. make sure config is properly populated. err:{e}\"\n                )\n            globals[dag[\"dag_id\"]]: DAG = dag[\"dag\"]", "response": "Generates DAGs from YAML config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arm(self, value):\n        if value:\n            return api.request_system_arm(self.blink, self.network_id)\n\n        return api.request_system_disarm(self.blink, self.network_id)", "response": "Arm or disarm system."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves events from server.", "response": "def get_events(self, **kwargs):\n        \"\"\"Retrieve events from server.\"\"\"\n        force = kwargs.pop('force', False)\n        response = api.request_sync_events(self.blink,\n                                           self.network_id,\n                                           force=force)\n        try:\n            return response['event']\n        except (TypeError, KeyError):\n            _LOGGER.error(\"Could not extract events: %s\",\n                          response,\n                          exc_info=True)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all blink cameras and pulls their most recent status.", "response": "def refresh(self, force_cache=False):\n        \"\"\"Get all blink cameras and pulls their most recent status.\"\"\"\n        self.network_info = api.request_network_status(self.blink,\n                                                       self.network_id)\n        self.check_new_videos()\n        for camera_name in self.cameras.keys():\n            camera_id = self.cameras[camera_name].camera_id\n            camera_info = self.get_camera_info(camera_id)\n            self.cameras[camera_name].update(camera_info,\n                                             force_cache=force_cache)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if new videos since last refresh.", "response": "def check_new_videos(self):\n        \"\"\"Check if new videos since last refresh.\"\"\"\n        resp = api.request_videos(self.blink,\n                                  time=self.blink.last_refresh,\n                                  page=0)\n\n        for camera in self.cameras.keys():\n            self.motion[camera] = False\n\n        try:\n            info = resp['videos']\n        except (KeyError, TypeError):\n            _LOGGER.warning(\"Could not check for motion. Response: %s\", resp)\n            return False\n\n        for entry in info:\n            try:\n                name = entry['camera_name']\n                clip = entry['address']\n                timestamp = entry['created_at']\n                self.motion[name] = True\n                self.last_record[name] = {'clip': clip, 'time': timestamp}\n            except KeyError:\n                _LOGGER.debug(\"No new videos since last refresh.\")\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attributes(self):\n        attributes = {\n            'name': self.name,\n            'camera_id': self.camera_id,\n            'serial': self.serial,\n            'temperature': self.temperature,\n            'temperature_c': self.temperature_c,\n            'temperature_calibrated': self.temperature_calibrated,\n            'battery': self.battery,\n            'thumbnail': self.thumbnail,\n            'video': self.clip,\n            'motion_enabled': self.motion_enabled,\n            'motion_detected': self.motion_detected,\n            'wifi_strength': self.wifi_strength,\n            'network_id': self.sync.network_id,\n            'sync_module': self.sync.name,\n            'last_record': self.last_record\n        }\n        return attributes", "response": "Return dictionary of all camera attributes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a picture with camera to create a new thumbnail.", "response": "def snap_picture(self):\n        \"\"\"Take a picture with camera to create a new thumbnail.\"\"\"\n        return api.request_new_image(self.sync.blink,\n                                     self.network_id,\n                                     self.camera_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the camera info.", "response": "def update(self, config, force_cache=False, **kwargs):\n        \"\"\"Update camera info.\"\"\"\n        # force = kwargs.pop('force', False)\n        self.name = config['name']\n        self.camera_id = str(config['id'])\n        self.network_id = str(config['network_id'])\n        self.serial = config['serial']\n        self.motion_enabled = config['enabled']\n        self.battery_voltage = config['battery_voltage']\n        self.battery_state = config['battery_state']\n        self.temperature = config['temperature']\n        self.wifi_strength = config['wifi_strength']\n\n        # Retrieve calibrated temperature from special endpoint\n        resp = api.request_camera_sensors(self.sync.blink,\n                                          self.network_id,\n                                          self.camera_id)\n        try:\n            self.temperature_calibrated = resp['temp']\n        except KeyError:\n            self.temperature_calibrated = self.temperature\n            _LOGGER.warning(\"Could not retrieve calibrated temperature.\")\n\n        # Check if thumbnail exists in config, if not try to\n        # get it from the homescreen info in teh sync module\n        # otherwise set it to None and log an error\n        new_thumbnail = None\n        if config['thumbnail']:\n            thumb_addr = config['thumbnail']\n        else:\n            thumb_addr = self.get_thumb_from_homescreen()\n\n        if thumb_addr is not None:\n            new_thumbnail = \"{}{}.jpg\".format(self.sync.urls.base_url,\n                                              thumb_addr)\n\n        try:\n            self.motion_detected = self.sync.motion[self.name]\n        except KeyError:\n            self.motion_detected = False\n\n        clip_addr = None\n        if self.name in self.sync.last_record:\n            clip_addr = self.sync.last_record[self.name]['clip']\n            self.last_record = self.sync.last_record[self.name]['time']\n            self.clip = \"{}{}\".format(self.sync.urls.base_url,\n                                      clip_addr)\n\n        # If the thumbnail or clip have changed, update the cache\n        update_cached_image = False\n        if new_thumbnail != self.thumbnail or self._cached_image is None:\n            update_cached_image = True\n        self.thumbnail = new_thumbnail\n\n        update_cached_video = False\n        if self._cached_video is None or self.motion_detected:\n            update_cached_video = True\n\n        if new_thumbnail is not None and (update_cached_image or force_cache):\n            self._cached_image = api.http_get(self.sync.blink,\n                                              url=self.thumbnail,\n                                              stream=True,\n                                              json=False)\n        if clip_addr is not None and (update_cached_video or force_cache):\n            self._cached_video = api.http_get(self.sync.blink,\n                                              url=self.clip,\n                                              stream=True,\n                                              json=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting image to file.", "response": "def image_to_file(self, path):\n        \"\"\"\n        Write image to file.\n\n        :param path: Path to write file\n        \"\"\"\n        _LOGGER.debug(\"Writing image from %s to %s\", self.name, path)\n        response = self._cached_image\n        if response.status_code == 200:\n            with open(path, 'wb') as imgfile:\n                copyfileobj(response.raw, imgfile)\n        else:\n            _LOGGER.error(\"Cannot write image to file, response %s\",\n                          response.status_code,\n                          exc_info=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite video to file.", "response": "def video_to_file(self, path):\n        \"\"\"Write video to file.\n\n        :param path: Path to write file\n        \"\"\"\n        _LOGGER.debug(\"Writing video from %s to %s\", self.name, path)\n        response = self._cached_video\n        if response is None:\n            _LOGGER.error(\"No saved video exist for %s.\",\n                          self.name,\n                          exc_info=True)\n            return\n        with open(path, 'wb') as vidfile:\n            copyfileobj(response.raw, vidfile)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_thumb_from_homescreen(self):\n        for device in self.sync.homescreen['devices']:\n            try:\n                device_type = device['device_type']\n                device_name = device['name']\n                device_thumb = device['thumbnail']\n                if device_type == 'camera' and device_name == self.name:\n                    return device_thumb\n            except KeyError:\n                pass\n        _LOGGER.error(\"Could not find thumbnail for camera %s\",\n                      self.name,\n                      exc_info=True)\n        return None", "response": "Retrieve thumbnail from homescreen."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating blink - compatible timestamp.", "response": "def get_time(time_to_convert=None):\n    \"\"\"Create blink-compatible timestamp.\"\"\"\n    if time_to_convert is None:\n        time_to_convert = time.time()\n    return time.strftime(TIMESTAMP_FORMAT, time.localtime(time_to_convert))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_dicts(dict_a, dict_b):\n    duplicates = [val for val in dict_a if val in dict_b]\n    if duplicates:\n        _LOGGER.warning((\"Duplicates found during merge: %s. \"\n                         \"Renaming is recommended.\"), duplicates)\n    return {**dict_a, **dict_b}", "response": "Merge two dictionaries into one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to refresh auth token and links.", "response": "def attempt_reauthorization(blink):\n    \"\"\"Attempt to refresh auth token and links.\"\"\"\n    _LOGGER.info(\"Auth token expired, attempting reauthorization.\")\n    headers = blink.get_auth_token(is_retry=True)\n    return headers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef http_req(blink, url='http://example.com', data=None, headers=None,\n             reqtype='get', stream=False, json_resp=True, is_retry=False):\n    \"\"\"\n    Perform server requests and check if reauthorization neccessary.\n\n    :param blink: Blink instance\n    :param url: URL to perform request\n    :param data: Data to send (default: None)\n    :param headers: Headers to send (default: None)\n    :param reqtype: Can be 'get' or 'post' (default: 'get')\n    :param stream: Stream response? True/FALSE\n    :param json_resp: Return JSON response? TRUE/False\n    :param is_retry: Is this a retry attempt? True/FALSE\n    \"\"\"\n    if reqtype == 'post':\n        req = Request('POST', url, headers=headers, data=data)\n    elif reqtype == 'get':\n        req = Request('GET', url, headers=headers)\n    else:\n        _LOGGER.error(\"Invalid request type: %s\", reqtype)\n        raise BlinkException(ERROR.REQUEST)\n\n    prepped = req.prepare()\n\n    try:\n        response = blink.session.send(prepped, stream=stream, timeout=10)\n        if json_resp and 'code' in response.json():\n            if is_retry:\n                _LOGGER.error(\"Cannot obtain new token for server auth.\")\n                return None\n            else:\n                headers = attempt_reauthorization(blink)\n                if not headers:\n                    raise exceptions.ConnectionError\n                return http_req(blink, url=url, data=data, headers=headers,\n                                reqtype=reqtype, stream=stream,\n                                json_resp=json_resp, is_retry=True)\n    except (exceptions.ConnectionError, exceptions.Timeout):\n        _LOGGER.info(\"Cannot connect to server with url %s.\", url)\n        if not is_retry:\n            headers = attempt_reauthorization(blink)\n            return http_req(blink, url=url, data=data, headers=headers,\n                            reqtype=reqtype, stream=stream,\n                            json_resp=json_resp, is_retry=True)\n        _LOGGER.error(\"Endpoint %s failed. Possible issue with Blink servers.\",\n                      url)\n        return None\n\n    if json_resp:\n        return response.json()\n\n    return response", "response": "Perform HTTP request to the specified URL and check if reauthorization neccessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self):\n        if self._username is None or self._password is None:\n            if not self.login():\n                return\n        elif not self.get_auth_token():\n            return\n\n        camera_list = self.get_cameras()\n        networks = self.get_ids()\n        for network_name, network_id in networks.items():\n            if network_id not in camera_list.keys():\n                camera_list[network_id] = {}\n                _LOGGER.warning(\"No cameras found for %s\", network_name)\n            sync_module = BlinkSyncModule(self,\n                                          network_name,\n                                          network_id,\n                                          camera_list[network_id])\n            sync_module.start()\n            self.sync[network_name] = sync_module\n        self.cameras = self.merge_cameras()", "response": "Perform full system setup.\n\n        Method logs in and sets auth token, urls, and ids for future requests.\n        Essentially this is just a wrapper function for ease of use."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef login(self):\n        self._username = input(\"Username:\")\n        self._password = getpass.getpass(\"Password:\")\n        if self.get_auth_token():\n            _LOGGER.debug(\"Login successful!\")\n            return True\n        _LOGGER.warning(\"Unable to login with %s.\", self._username)\n        return False", "response": "Prompt user for username and password."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_auth_token(self, is_retry=False):\n        if not isinstance(self._username, str):\n            raise BlinkAuthenticationException(ERROR.USERNAME)\n        if not isinstance(self._password, str):\n            raise BlinkAuthenticationException(ERROR.PASSWORD)\n\n        login_urls = [LOGIN_URL, OLD_LOGIN_URL, LOGIN_BACKUP_URL]\n\n        response = self.login_request(login_urls, is_retry=is_retry)\n\n        if not response:\n            return False\n\n        self._host = \"{}.{}\".format(self.region_id, BLINK_URL)\n        self._token = response['authtoken']['authtoken']\n        self.networks = response['networks']\n\n        self._auth_header = {'Host': self._host,\n                             'TOKEN_AUTH': self._token}\n        self.urls = BlinkURLHandler(self.region_id)\n\n        return self._auth_header", "response": "Retrieve the authentication token from Blink."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a login request.", "response": "def login_request(self, login_urls, is_retry=False):\n        \"\"\"Make a login request.\"\"\"\n        try:\n            login_url = login_urls.pop(0)\n        except IndexError:\n            _LOGGER.error(\"Could not login to blink servers.\")\n            return False\n\n        _LOGGER.info(\"Attempting login with %s\", login_url)\n\n        response = api.request_login(self,\n                                     login_url,\n                                     self._username,\n                                     self._password,\n                                     is_retry=is_retry)\n        try:\n            if response.status_code != 200:\n                response = self.login_request(login_urls)\n            response = response.json()\n            (self.region_id, self.region), = response['region'].items()\n\n        except AttributeError:\n            _LOGGER.error(\"Login API endpoint failed with response %s\",\n                          response,\n                          exc_info=True)\n            return False\n\n        except KeyError:\n            _LOGGER.warning(\"Could not extract region info.\")\n            self.region_id = 'piri'\n            self.region = 'UNKNOWN'\n\n        self._login_url = login_url\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the network ID and Account ID.", "response": "def get_ids(self):\n        \"\"\"Set the network ID and Account ID.\"\"\"\n        response = api.request_networks(self)\n        all_networks = []\n        network_dict = {}\n        for network, status in self.networks.items():\n            if status['onboarded']:\n                all_networks.append('{}'.format(network))\n                network_dict[status['name']] = network\n\n        # For the first onboarded network we find, grab the account id\n        for resp in response['networks']:\n            if str(resp['id']) in all_networks:\n                self.account_id = resp['account_id']\n                break\n\n        self.network_ids = all_networks\n        return network_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves a camera list for each onboarded network.", "response": "def get_cameras(self):\n        \"\"\"Retrieve a camera list for each onboarded network.\"\"\"\n        response = api.request_homescreen(self)\n        try:\n            all_cameras = {}\n            for camera in response['cameras']:\n                camera_network = str(camera['network_id'])\n                camera_name = camera['name']\n                camera_id = camera['id']\n                camera_info = {'name': camera_name, 'id': camera_id}\n                if camera_network not in all_cameras:\n                    all_cameras[camera_network] = []\n\n                all_cameras[camera_network].append(camera_info)\n            return all_cameras\n        except KeyError:\n            _LOGGER.error(\"Initialization failue. Could not retrieve cameras.\")\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a system refresh.", "response": "def refresh(self, force_cache=False):\n        \"\"\"\n        Perform a system refresh.\n\n        :param force_cache: Force an update of the camera cache\n        \"\"\"\n        if self.check_if_ok_to_update() or force_cache:\n            for sync_name, sync_module in self.sync.items():\n                _LOGGER.debug(\"Attempting refresh of sync %s\", sync_name)\n                sync_module.refresh(force_cache=force_cache)\n            if not force_cache:\n                # Prevents rapid clearing of motion detect property\n                self.last_refresh = int(time.time())\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_if_ok_to_update(self):\n        current_time = int(time.time())\n        last_refresh = self.last_refresh\n        if last_refresh is None:\n            last_refresh = 0\n        if current_time >= (last_refresh + self.refresh_rate):\n            return True\n        return False", "response": "Check if the user is ok to update the resource cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges all sync camera dicts into one.", "response": "def merge_cameras(self):\n        \"\"\"Merge all sync camera dicts into one.\"\"\"\n        combined = CaseInsensitiveDict({})\n        for sync in self.sync:\n            combined = merge_dicts(combined, self.sync[sync].cameras)\n        return combined"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_videos(self, path, since=None, camera='all', stop=10):\n        if since is None:\n            since_epochs = self.last_refresh\n        else:\n            parsed_datetime = parse(since, fuzzy=True)\n            since_epochs = parsed_datetime.timestamp()\n\n        formatted_date = get_time(time_to_convert=since_epochs)\n        _LOGGER.info(\"Retrieving videos since %s\", formatted_date)\n\n        if not isinstance(camera, list):\n            camera = [camera]\n\n        for page in range(1, stop):\n            response = api.request_videos(self, time=since_epochs, page=page)\n            _LOGGER.debug(\"Processing page %s\", page)\n            try:\n                result = response['videos']\n                if not result:\n                    raise IndexError\n            except (KeyError, IndexError):\n                _LOGGER.info(\"No videos found on page %s. Exiting.\", page)\n                break\n\n            self._parse_downloaded_items(result, camera, path)", "response": "Download all videos from server since specified time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs-in request. :param blink: Blink instance. :param url: Login url. :param username: Blink username. :param password: Blink password. :param is_retry: Is this part of a re-authorization attempt?", "response": "def request_login(blink, url, username, password, is_retry=False):\n    \"\"\"\n    Login request.\n\n    :param blink: Blink instance.\n    :param url: Login url.\n    :param username: Blink username.\n    :param password: Blink password.\n    :param is_retry: Is this part of a re-authorization attempt?\n    \"\"\"\n    headers = {\n        'Host': DEFAULT_URL,\n        'Content-Type': 'application/json'\n    }\n    data = dumps({\n        'email': username,\n        'password': password,\n        'client_specifier': 'iPhone 9.2 | 2.2 | 222'\n    })\n    return http_req(blink, url=url, headers=headers, data=data,\n                    json_resp=False, reqtype='post', is_retry=is_retry)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef request_networks(blink):\n    url = \"{}/networks\".format(blink.urls.base_url)\n    return http_get(blink, url)", "response": "Request all networks information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrequest network information. :param blink: Blink instance. :param network: Sync module network id.", "response": "def request_network_status(blink, network):\n    \"\"\"\n    Request network information.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id.\n    \"\"\"\n    url = \"{}/network/{}\".format(blink.urls.base_url, network)\n    return http_get(blink, url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting sync module info.", "response": "def request_syncmodule(blink, network):\n    \"\"\"\n    Request sync module info.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id.\n    \"\"\"\n    url = \"{}/network/{}/syncmodules\".format(blink.urls.base_url, network)\n    return http_get(blink, url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\narm system. :param blink: Blink instance. :param network: Sync module network id.", "response": "def request_system_arm(blink, network):\n    \"\"\"\n    Arm system.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id.\n    \"\"\"\n    url = \"{}/network/{}/arm\".format(blink.urls.base_url, network)\n    return http_post(blink, url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef request_system_disarm(blink, network):\n    url = \"{}/network/{}/disarm\".format(blink.urls.base_url, network)\n    return http_post(blink, url)", "response": "Disarm system.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request_command_status(blink, network, command_id):\n    url = \"{}/network/{}/command/{}\".format(blink.urls.base_url,\n                                            network,\n                                            command_id)\n    return http_get(blink, url)", "response": "Request command status.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id.\n    :param command_id: Command id to check."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request_sync_events(blink, network):\n    url = \"{}/events/network/{}\".format(blink.urls.base_url, network)\n    return http_get(blink, url)", "response": "Request sync events from sync module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting total video count.", "response": "def request_video_count(blink):\n    \"\"\"Request total video count.\"\"\"\n    url = \"{}/api/v2/videos/count\".format(blink.urls.base_url)\n    return http_get(blink, url)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting videos from a Blink instance.", "response": "def request_videos(blink, time=None, page=0):\n    \"\"\"\n    Perform a request for videos.\n\n    :param blink: Blink instance.\n    :param time: Get videos since this time.  In epoch seconds.\n    :param page: Page number to get videos from.\n    \"\"\"\n    timestamp = get_time(time)\n    url = \"{}/api/v2/videos/changed?since={}&page={}\".format(\n        blink.urls.base_url, timestamp, page)\n    return http_get(blink, url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef request_cameras(blink, network):\n    url = \"{}/network/{}/cameras\".format(blink.urls.base_url, network)\n    return http_get(blink, url)", "response": "Request all camera information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrequest camera sensor info for one camera.", "response": "def request_camera_sensors(blink, network, camera_id):\n    \"\"\"\n    Request camera sensor info for one camera.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id.\n    :param camera_id: Camera ID of camera to request sesnor info from.\n    \"\"\"\n    url = \"{}/network/{}/camera/{}/signals\".format(blink.urls.base_url,\n                                                   network,\n                                                   camera_id)\n    return http_get(blink, url)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request_motion_detection_enable(blink, network, camera_id):\n    url = \"{}/network/{}/camera/{}/enable\".format(blink.urls.base_url,\n                                                  network,\n                                                  camera_id)\n    return http_post(blink, url)", "response": "Request motion detection for a camera."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef http_get(blink, url, stream=False, json=True, is_retry=False):\n    if blink.auth_header is None:\n        raise BlinkException(ERROR.AUTH_TOKEN)\n    _LOGGER.debug(\"Making GET request to %s\", url)\n    return http_req(blink, url=url, headers=blink.auth_header,\n                    reqtype='get', stream=stream, json_resp=json,\n                    is_retry=is_retry)", "response": "Perform an http get request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform an http post request.", "response": "def http_post(blink, url, is_retry=False):\n    \"\"\"\n    Perform an http post request.\n\n    :param url: URL to perfom post request.\n    :param is_retry: Is this part of a re-auth attempt?\n    \"\"\"\n    if blink.auth_header is None:\n        raise BlinkException(ERROR.AUTH_TOKEN)\n    _LOGGER.debug(\"Making POST request to %s\", url)\n    return http_req(blink, url=url, headers=blink.auth_header,\n                    reqtype='post', is_retry=is_retry)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef average_filter(input, size=None, footprint=None, output=None, mode=\"reflect\", cval=0.0, origin=0):\n    footprint = __make_footprint(input, size, footprint)\n    filter_size = footprint.sum()\n    \n    output = _get_output(output, input)\n    sum_filter(input, footprint=footprint, output=output, mode=mode, cval=cval, origin=origin)\n    output /= filter_size\n    return output", "response": "r Calculates the multi - dimensional average filter for a single - dimensional array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef otsu (img, bins=64):\n    # cast bins parameter to int\n    bins = int(bins)\n    \n    # cast img parameter to scipy arrax\n    img = numpy.asarray(img)\n    \n    # check supplied parameters\n    if bins <= 1:\n        raise AttributeError('At least a number two bins have to be provided.')\n    \n    # determine initial threshold and threshold step-length\n    steplength = (img.max() - img.min()) / float(bins)\n    initial_threshold = img.min() + steplength\n    \n    # initialize best value variables\n    best_bcv = 0\n    best_threshold = initial_threshold\n    \n    # iterate over the thresholds and find highest between class variance\n    for threshold in numpy.arange(initial_threshold, img.max(), steplength):\n        mask_fg = (img >= threshold)\n        mask_bg = (img < threshold)\n        \n        wfg = numpy.count_nonzero(mask_fg)\n        wbg = numpy.count_nonzero(mask_bg)\n        \n        if 0 == wfg or 0 == wbg: continue\n        \n        mfg = img[mask_fg].mean()\n        mbg = img[mask_bg].mean()\n        \n        bcv = wfg * wbg * math.pow(mbg - mfg, 2)\n        \n        if bcv > best_bcv:\n            best_bcv = bcv\n            best_threshold = threshold\n        \n    return best_threshold", "response": "r Otsu s method to find the optimal threshold separating an image into fore - and background."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef triangular_membership(bin_center, bin_width, smoothness = 0.5):\n    if smoothness > 0.5: raise AttributeError('the triangular/trapezium membership functions supports only smoothnesses between 1/10 and 1/2.')\n    if smoothness < 0.5: return trapezoid_membership(bin_center, bin_width, smoothness)\n    \n    a = bin_center - bin_width\n    b = float(bin_center)\n    c = bin_center + bin_width\n    \n    def fun(x):\n        if x < a or x > c: return 0\n        elif x <= b: return (x-a)/(b-a)\n        else: return (c-x)/(c-b)\n    return fun", "response": "r Returns a new fuzzy histogram with the given bin center and width."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef immerkaer_local(input, size, output=None, mode=\"reflect\", cval=0.0):\n    output = _ni_support._get_output(output, input)\n    footprint = numpy.asarray([1] * size)\n    \n    # build nd-kernel to acquire square root of sum of squared elements\n    kernel = [1, -2, 1]\n    for _ in range(input.ndim - 1):\n        kernel = numpy.tensordot(kernel, [1, -2, 1], 0)\n    divider = numpy.square(numpy.abs(kernel)).sum() # 36 for 1d, 216 for 3D, etc.\n    \n    # compute laplace of input\n    laplace = separable_convolution(input, [1, -2, 1], numpy.double, mode, cval)\n    \n    # compute factor\n    factor = numpy.sqrt(numpy.pi / 2.) * 1. / ( numpy.sqrt(divider) * numpy.power(footprint.size, laplace.ndim) )\n    \n    # locally sum laplacian values\n    separable_convolution(numpy.abs(laplace), footprint, output, mode, cval)\n    \n    output *= factor\n    \n    return output", "response": "r Immerkaer local noise filter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef immerkaer(input, mode=\"reflect\", cval=0.0):\n    # build nd-kernel to acquire square root of sum of squared elements\n    kernel = [1, -2, 1]\n    for _ in range(input.ndim - 1):\n        kernel = numpy.tensordot(kernel, [1, -2, 1], 0)\n    divider = numpy.square(numpy.abs(kernel)).sum() # 36 for 1d, 216 for 3D, etc.\n    \n    # compute laplace of input and derive noise sigma\n    laplace = separable_convolution(input, [1, -2, 1], None, mode, cval)\n    factor = numpy.sqrt(numpy.pi / 2.) * 1. / ( numpy.sqrt(divider) * numpy.prod(laplace.shape) )\n    sigma = factor * numpy.abs(laplace).sum()\n    \n    return sigma", "response": "r Immerkaer estimate of the global noise of the input image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef separable_convolution(input, weights, output=None, mode=\"reflect\", cval=0.0, origin=0):\n    input = numpy.asarray(input)\n    output = _ni_support._get_output(output, input)\n    axes = list(range(input.ndim))\n    if len(axes) > 0:\n        convolve1d(input, weights, axes[0], output, mode, cval, origin)\n        for ii in range(1, len(axes)):\n            convolve1d(output, weights, axes[ii], output, mode, cval, origin)\n    else:\n        output[...] = input[...]\n    return output", "response": "r Separable convolution of a single kernel to a n - dimensional array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit code at plt. show", "response": "def split_code_at_show(text):\n    \"\"\"\n    Split code at plt.show()\n\n    \"\"\"\n\n    parts = []\n    is_doctest = contains_doctest(text)\n\n    part = []\n    for line in text.split(\"\\n\"):\n        if (not is_doctest and line.strip() == 'plt.show()') or \\\n               (is_doctest and line.strip() == '>>> plt.show()'):\n            part.append(line)\n            parts.append(\"\\n\".join(part))\n            part = []\n        else:\n            part.append(line)\n    if \"\\n\".join(part).strip():\n        parts.append(\"\\n\".join(part))\n    return parts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if original is out - of - date wrt derived.", "response": "def out_of_date(original, derived):\n    \"\"\"\n    Returns True if derivative is out-of-date wrt original,\n    both of which are full file paths.\n    \"\"\"\n    return (not os.path.exists(derived)\n            or os.stat(derived).st_mtime < os.stat(original).st_mtime)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a single or multi - figure from a code file.", "response": "def makefig(code, code_path, output_dir, output_base, config):\n    \"\"\"\n    Run a pyplot script *code* and save the images under *output_dir*\n    with file names derived from *output_base*\n\n    \"\"\"\n\n    # -- Parse format list\n    default_dpi = {'png': 80, 'hires.png': 200, 'pdf': 50}\n    formats = []\n    for fmt in config.plot_formats:\n        if isinstance(fmt, str):\n            formats.append((fmt, default_dpi.get(fmt, 80)))\n        elif type(fmt) in (tuple, list) and len(fmt)==2:\n            formats.append((str(fmt[0]), int(fmt[1])))\n        else:\n            raise PlotError('invalid image format \"%r\" in plot_formats' % fmt)\n\n    # -- Try to determine if all images already exist\n\n    code_pieces = split_code_at_show(code)\n\n    # Look for single-figure output files first\n    all_exists = True\n    img = ImageFile(output_base, output_dir)\n    for format, dpi in formats:\n        if out_of_date(code_path, img.filename(format)):\n            all_exists = False\n            break\n        img.formats.append(format)\n\n    if all_exists:\n        return [(code, [img])]\n\n    # Then look for multi-figure output files\n    results = []\n    all_exists = True\n    for i, code_piece in enumerate(code_pieces):\n        images = []\n        for j in range(1000):\n            img = ImageFile('%s_%02d_%02d' % (output_base, i, j), output_dir)\n            for format, dpi in formats:\n                if out_of_date(code_path, img.filename(format)):\n                    all_exists = False\n                    break\n                img.formats.append(format)\n\n            # assume that if we have one, we have them all\n            if not all_exists:\n                all_exists = (j > 0)\n                break\n            images.append(img)\n        if not all_exists:\n            break\n        results.append((code_piece, images))\n\n    if all_exists:\n        return results\n\n    # -- We didn't find the files, so build them\n\n    results = []\n    ns = {}\n\n    for i, code_piece in enumerate(code_pieces):\n        # Clear between runs\n        plt.close('all')\n\n        # Run code\n        run_code(code_piece, code_path, ns)\n\n        # Collect images\n        images = []\n        fig_managers = _pylab_helpers.Gcf.get_all_fig_managers()\n        for j, figman in enumerate(fig_managers):\n            if len(fig_managers) == 1 and len(code_pieces) == 1:\n                img = ImageFile(output_base, output_dir)\n            else:\n                img = ImageFile(\"%s_%02d_%02d\" % (output_base, i, j),\n                                output_dir)\n            images.append(img)\n            for format, dpi in formats:\n                try:\n                    figman.canvas.figure.savefig(img.filename(format), dpi=dpi)\n                except exceptions.BaseException as err:\n                    raise PlotError(traceback.format_exc())\n                img.formats.append(format)\n\n        # Results\n        results.append((code_piece, images))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nproviding additional validation of the arguments collected by argparse.", "response": "def getArguments(parser):\n    \"Provides additional validation of the arguments collected by argparse.\"\n    args = parser.parse_args()\n    # parse volume and adapt to zero-indexing\n    try:\n        def _to_int_or_none(string):\n            if 0 == len(string): return None\n            return int(string)\n        def _to_int_or_none_double (string):\n            if 0 == len(string): return [None, None]\n            return list(map(_to_int_or_none, string.split(':')))        \n        args.volume = list(map(_to_int_or_none_double, args.volume.split(',')))\n        args.volume = [(x[0], x[1]) for x in args.volume]\n    except (ValueError, IndexError) as e:\n        raise ArgumentError('Maleformed volume parameter \"{}\", see description with -h flag.'.format(args.volume), e)\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contrast(image, mask = slice(None)):\n    image = numpy.asarray(image)\n    \n    # set default mask or apply given mask\n    if not type(mask) is slice:\n        if not type(mask[0] is slice):\n            mask = numpy.array(mask, copy=False, dtype = numpy.bool)\n    image = image[mask]\n    \n    standard_deviation = numpy.std(image)\n    kurtosis = stats.kurtosis(image, axis=None, bias=True, fisher=False)\n    n = 0.25 # The value n=0.25 is recommended as the best for discriminating the textures.  \n    \n    Fcon = standard_deviation / (kurtosis**n) \n    \n    return Fcon", "response": "r Returns the contrast of the texture."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef directionality(image, min_distance = 4, threshold = 0.1, voxelspacing = None, mask = slice(None)):\n    image = numpy.asarray(image)\n    ndim = image.ndim\n    # set default mask or apply given mask\n    if not type(mask) is slice:\n        if not type(mask[0] is slice):\n            mask = numpy.array(mask, copy=False, dtype = numpy.bool)\n    image = image[mask]\n           \n    # set default voxel spacing if not suppliec\n    if None == voxelspacing:\n        voxelspacing = tuple([1.] * ndim)\n        \n    if len(voxelspacing) != ndim:\n        print(\"Voxel spacing and image dimensions do not fit.\")\n        return None\n   \n   # Calculate amount of combinations: n choose k, normalizing factor r and voxel spacing.    \n    n = (factorial(ndim)/(2*factorial(ndim-2)))\n    pi1_2 = numpy.pi/2.0\n    r=1.0 / (pi1_2**2)\n    vs = [slice(None,None,numpy.rint(ii)) for ii in voxelspacing]\n   \n    # Allocate memory, define constants\n    Fdir = numpy.empty(n)\n\n    # calculate differences by using Sobel-filter. (Maybe other filter kernel like Prewitt will do a better job)\n    E = [sobel(image, axis=ndim-1-i) for i in range(ndim)]\n    \n    # The edge strength e(x,y) is used for thresholding.\n    e = sum(E) / float(ndim)\n    border = [numpy.percentile(e, 1),numpy.percentile(e, 99)]\n    e[e < border[0]] = 0\n    e[e > border[1]] = border[1]\n    e -= border[0]\n    e /= border[1]\n    em = e > threshold\n        \n    for i in range(n):\n        A = numpy.arctan((E[(i + (ndim+i)/ndim) % ndim][vs]) / (E[i%ndim][vs]+numpy.spacing(1))) # [0 , pi/2]\n        A = A[em[vs]]\n        # Calculate number of bins for the histogram. Watch out, this is just a work around! \n        # @TODO: Write a more stable code to prevent for minimum and maximum repetition when the same value in the Histogram appears multiple times in a row. Example: image = numpy.zeros([10,10]), image[:,::3] = 1\n        bins = numpy.unique(A).size + min_distance        \n        H = numpy.histogram(A, bins = bins, density=True)[0] # [0 , 1]\n        H[H < numpy.percentile(H,1)] = 0.0\n        H_peaks, H_valleys, H_range = find_valley_range(H)\n        summe = 0.0\n        for idx_ap in range(len(H_peaks)):\n            for range_idx in range( H_valleys[idx_ap], H_valleys[idx_ap]+H_range[idx_ap]):\n                a=range_idx % len(H)\n                summe += (((pi1_2*a)/bins - (pi1_2 * H_peaks[idx_ap])/bins) **2) * H[a]\n        Fdir[i] = 1.0 - r * summe \n        \n    return Fdir", "response": "r Returns the directionality of an image in a specific image layer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef local_maxima(vector,min_distance = 4, brd_mode = \"wrap\"):\n    fits = gaussian_filter(numpy.asarray(vector,dtype=numpy.float32),1., mode=brd_mode)\n    for ii in range(len(fits)):\n        if fits[ii] == fits[ii-1]:\n            fits[ii-1] = 0.0\n    maxfits     = maximum_filter(fits, size=min_distance, mode=brd_mode)\n    maxima_mask = fits == maxfits\n    maximum     = numpy.transpose(maxima_mask.nonzero())\n    return numpy.asarray(maximum)", "response": "Internal finder for local maxima."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef local_minima(vector,min_distance = 4, brd_mode = \"wrap\"):\n    fits = gaussian_filter(numpy.asarray(vector,dtype=numpy.float32),1., mode=brd_mode)\n    for ii in range(len(fits)):\n        if fits[ii] == fits[ii-1]:\n            fits[ii-1] = numpy.pi/2.0\n    minfits = minimum_filter(fits, size=min_distance, mode=brd_mode)\n    minima_mask = fits == minfits\n    minima = numpy.transpose(minima_mask.nonzero())\n    return numpy.asarray(minima)", "response": "Internal finder for local minima."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate and returns the argparse parser object.", "response": "def getParser():\n    \"Creates and returns the argparse parser object.\"\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=__description__)\n    parser.add_argument('input', help='the input image')\n    parser.add_argument('output', help='the output image')\n    parser.add_argument('shape', type=argparseu.sequenceOfIntegersGt, help='the desired shape in colon-separated values, e.g. 255,255,32')\n\n    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', help='verbose output')\n    parser.add_argument('-d', dest='debug', action='store_true', help='Display debug information.')\n    parser.add_argument('-f', '--force', dest='force', action='store_true', help='overwrite existing files')\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gauss_xminus1d(img, sigma, dim=2):\n    img = numpy.array(img, copy=False)\n    return xminus1d(img, gaussian_filter, dim, sigma=sigma)", "response": "r Applies a X - 1D gauss to a copy of a XD image slicing it along dim and returns a new array containing the smoothed image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize(vector, cutoffp = (0, 100), model = False):\n    vector = numpy.array(vector, dtype=numpy.float)\n    \n    # add a singleton dimension if required\n    if 1 == vector.ndim:\n        vector = vector[:, None]\n    \n    # compute lower and upper range border of each row using the supplied percentiles\n    minp, maxp = numpy.percentile(vector, cutoffp, 0)\n    \n    # shift outliers to fit range\n    for i in range(vector.shape[1]):\n        vector[:,i][vector[:,i] < minp[i]] = minp[i]\n        vector[:,i][vector[:,i] > maxp[i]] = maxp[i]\n    \n    # normalize\n    minv = vector.min(0)\n    vector -= minv\n    maxv = vector.max(0)\n    vector /= maxv\n    \n    if not model:\n        return vector\n    else:\n        return vector, (minp, maxp, minv, maxv)", "response": "r Normalizes the input vector."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef join(*vectors):\n    # check supplied arguments\n    if len(vectors) < 2:\n        return vectors[0]\n\n    # process supplied arguments\n    vectors = list(vectors)\n    for i in range(len(vectors)):\n        vectors[i] = numpy.array(vectors[i], copy=False)\n        if vectors[i].ndim == 1:\n            vectors[i] = numpy.array([vectors[i]], copy=False).T\n    \n    # treat single-value cases special (no squeezing)\n    if 1 == len(vectors[0]):\n        return numpy.concatenate(vectors, 1)\n    \n    return numpy.squeeze(numpy.concatenate(vectors, 1))", "response": "r Returns a new sequence of vectors that are combined together."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a graph - cut ready graph from a set of foreground and background voxels.", "response": "def graph_from_voxels(fg_markers,\n                        bg_markers,\n                        regional_term = False,\n                        boundary_term = False,\n                        regional_term_args = False,\n                        boundary_term_args = False):\n    \"\"\"\n    Create a graph-cut ready graph to segment a nD image using the voxel neighbourhood.\n    \n    Create a `~medpy.graphcut.maxflow.GraphDouble` object for all voxels of an image with a\n    :math:`ndim * 2` neighbourhood.\n    \n    Every voxel of the image is regarded as a node. They are connected to their immediate\n    neighbours via arcs. If to voxels are neighbours is determined using\n    :math:`ndim*2`-connectedness (e.g. :math:`3*2=6` for 3D). In the next step the arcs weights\n    (n-weights) are computed using the supplied ``boundary_term`` function\n    (see :mod:`~medpy.graphcut.energy_voxel` for a selection).\n    \n    Implicitly the graph holds two additional nodes: the source and the sink, so called\n    terminal nodes. These are connected with all other nodes through arcs of an initial\n    weight (t-weight) of zero.\n    All voxels that are under the foreground markers are considered to be tightly bound\n    to the source: The t-weight of the arc from source to these nodes is set to a maximum\n    value. The same goes for the background markers: The covered voxels receive a maximum\n    (`~medpy.graphcut.graph.GCGraph.MAX`) t-weight for their arc towards the sink.\n    \n    All other t-weights are set using the supplied ``regional_term`` function\n    (see :mod:`~medpy.graphcut.energy_voxel` for a selection).\n    \n    Parameters\n    ----------\n    fg_markers : ndarray\n        The foreground markers as binary array of the same shape as the original image.\n    bg_markers : ndarray\n        The background markers as binary array of the same shape as the original image.\n    regional_term : function\n        This can be either `False`, in which case all t-weights are set to 0, except for\n        the nodes that are directly connected to the source or sink; or a function, in\n        which case the supplied function is used to compute the t_edges. It has to\n        have the following signature *regional_term(graph, regional_term_args)*, and is\n        supposed to compute (source_t_weight, sink_t_weight) for all voxels of the image\n        and add these to the passed `~medpy.graphcut.graph.GCGraph` object. The weights\n        have only to be computed for nodes where they do not equal zero. Additional\n        parameters can be passed to the function via the ``regional_term_args`` parameter.\n    boundary_term : function\n        This can be either `False`, in which case all n-edges, i.e. between all nodes\n        that are not source or sink, are set to 0; or a function, in which case the\n        supplied function is used to compute the edge weights. It has to have the\n        following signature *boundary_term(graph, boundary_term_args)*, and is supposed\n        to compute the edges between the graphs nodes and to add them to the supplied\n        `~medpy.graphcut.graph.GCGraph` object. Additional parameters can be passed to\n        the function via the ``boundary_term_args`` parameter.\n    regional_term_args : tuple\n        Use this to pass some additional parameters to the ``regional_term`` function.\n    boundary_term_args : tuple    \n        Use this to pass some additional parameters to the ``boundary_term`` function.\n    \n    Returns\n    -------\n    graph : `~medpy.graphcut.maxflow.GraphDouble`\n        The created graph, ready to execute the graph-cut.\n    \n    Raises\n    ------\n    AttributeError\n        If an argument is malformed.\n    FunctionError\n        If one of the supplied functions returns unexpected results.\n    \n    Notes\n    -----\n    If a voxel is marked as both, foreground and background, the background marker\n    is given higher priority.\n     \n    All arcs whose weight is not explicitly set are assumed to carry a weight of zero.\n    \"\"\"\n    # prepare logger\n    logger = Logger.getInstance()\n    \n    # prepare result graph\n    logger.debug('Assuming {} nodes and {} edges for image of shape {}'.format(fg_markers.size, __voxel_4conectedness(fg_markers.shape), fg_markers.shape)) \n    graph = GCGraph(fg_markers.size, __voxel_4conectedness(fg_markers.shape))\n    \n    logger.info('Performing attribute tests...')\n    \n    # check, set and convert all supplied parameters\n    fg_markers = scipy.asarray(fg_markers, dtype=scipy.bool_)\n    bg_markers = scipy.asarray(bg_markers, dtype=scipy.bool_)\n    \n    # set dummy functions if not supplied\n    if not regional_term: regional_term = __regional_term_voxel\n    if not boundary_term: boundary_term = __boundary_term_voxel\n    \n    # check supplied functions and their signature\n    if not hasattr(regional_term, '__call__') or not 2 == len(inspect.getargspec(regional_term)[0]):\n        raise AttributeError('regional_term has to be a callable object which takes two parameter.')\n    if not hasattr(boundary_term, '__call__') or not 2 == len(inspect.getargspec(boundary_term)[0]):\n        raise AttributeError('boundary_term has to be a callable object which takes two parameters.')\n\n    logger.debug('#nodes={}, #hardwired-nodes source/sink={}/{}'.format(fg_markers.size,\n                                                                        len(fg_markers.ravel().nonzero()[0]),\n                                                                        len(bg_markers.ravel().nonzero()[0])))\n    \n    # compute the weights of all edges from the source and to the sink i.e.\n    # compute the weights of the t_edges Wt\n    logger.info('Computing and adding terminal edge weights...')\n    regional_term(graph, regional_term_args)\n\n    # compute the weights of the edges between the neighbouring nodes i.e.\n    # compute the weights of the n_edges Wr\n    logger.info('Computing and adding inter-node edge weights...')\n    boundary_term(graph, boundary_term_args)\n    \n    # collect all voxels that are under the foreground resp. background markers i.e.\n    # collect all nodes that are connected to the source resp. sink\n    logger.info('Setting terminal weights for the markers...')\n    if not 0 == scipy.count_nonzero(fg_markers):\n        graph.set_source_nodes(fg_markers.ravel().nonzero()[0])\n    if not 0 == scipy.count_nonzero(bg_markers):\n        graph.set_sink_nodes(bg_markers.ravel().nonzero()[0])    \n    \n    return graph.get_graph()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a graph - cut ready graph from a label image.", "response": "def graph_from_labels(label_image,\n                        fg_markers,\n                        bg_markers,\n                        regional_term = False,\n                        boundary_term = False,\n                        regional_term_args = False,\n                        boundary_term_args = False):\n    \"\"\"\n    Create a graph-cut ready graph to segment a nD image using the region neighbourhood.\n    \n    Create a `~medpy.graphcut.maxflow.GraphDouble` object for all regions of a nD label\n    image.\n    \n    Every region of the label image is regarded as a node. They are connected to their\n    immediate neighbours by arcs. If to regions are neighbours is determined using\n    :math:`ndim*2`-connectedness (e.g. :math:`3*2=6` for 3D).\n    In the next step the arcs weights (n-weights) are computed using the supplied\n    ``boundary_term`` function (see :mod:`~medpy.graphcut.energy_voxel` for a selection).\n    \n    Implicitly the graph holds two additional nodes: the source and the sink, so called\n    terminal nodes. These are connected with all other nodes through arcs of an initial\n    weight (t-weight) of zero.\n    All regions that are under the foreground markers are considered to be tightly bound\n    to the source: The t-weight of the arc from source to these nodes is set to a maximum \n    value. The same goes for the background markers: The covered regions receive a\n    maximum (`~medpy.graphcut.graph.GCGraph.MAX`) t-weight for their arc towards the sink.\n    \n    All other t-weights are set using the supplied ``regional_term`` function\n    (see :mod:`~medpy.graphcut.energy_voxel` for a selection).\n    \n    Parameters\n    ----------\n    label_image: ndarray\n        The label image as an array cwhere each voxel carries the id of the region it\n        belongs to. Note that the region labels have to start from 1 and be continuous\n        (can be achieved with `~medpy.filter.label.relabel`).\n    fg_markers : ndarray\n        The foreground markers as binary array of the same shape as the original image.\n    bg_markers : ndarray\n        The background markers as binary array of the same shape as the original image.\n    regional_term : function\n        This can be either `False`, in which case all t-weights are set to 0, except for\n        the nodes that are directly connected to the source or sink; or a function, in\n        which case the supplied function is used to compute the t_edges. It has to\n        have the following signature *regional_term(graph, regional_term_args)*, and is\n        supposed to compute (source_t_weight, sink_t_weight) for all regions of the image\n        and add these to the passed `~medpy.graphcut.graph.GCGraph` object. The weights\n        have only to be computed for nodes where they do not equal zero. Additional\n        parameters can be passed to the function via the ``regional_term_args`` parameter.\n    boundary_term : function\n        This can be either `False`, in which case all n-edges, i.e. between all nodes\n        that are not source or sink, are set to 0; or a function, in which case the\n        supplied function is used to compute the edge weights. It has to have the\n        following signature *boundary_term(graph, boundary_term_args)*, and is supposed\n        to compute the edges between all adjacent regions of the image and to add them\n        to the supplied `~medpy.graphcut.graph.GCGraph` object. Additional parameters\n        can be passed to the function via the ``boundary_term_args`` parameter.\n    regional_term_args : tuple\n        Use this to pass some additional parameters to the ``regional_term`` function.\n    boundary_term_args : tuple    \n        Use this to pass some additional parameters to the ``boundary_term`` function.\n\n    Returns\n    -------\n    graph : `~medpy.graphcut.maxflow.GraphDouble`\n        The created graph, ready to execute the graph-cut.\n    \n    Raises\n    ------\n    AttributeError\n        If an argument is malformed.\n    FunctionError\n        If one of the supplied functions returns unexpected results.\n    \n    Notes\n    -----\n    If a voxel is marked as both, foreground and background, the background marker\n    is given higher priority.\n     \n    All arcs whose weight is not explicitly set are assumed to carry a weight of zero.    \n    \"\"\"    \n    # prepare logger\n    logger = Logger.getInstance()\n    \n    logger.info('Performing attribute tests...')\n    \n    # check, set and convert all supplied parameters\n    label_image = scipy.asarray(label_image)\n    fg_markers = scipy.asarray(fg_markers, dtype=scipy.bool_)\n    bg_markers = scipy.asarray(bg_markers, dtype=scipy.bool_)\n    \n    __check_label_image(label_image)\n    \n    # set dummy functions if not supplied\n    if not regional_term: regional_term = __regional_term_label\n    if not boundary_term: boundary_term = __boundary_term_label\n    \n    # check supplied functions and their signature\n    if not hasattr(regional_term, '__call__') or not 3 == len(inspect.getargspec(regional_term)[0]):\n        raise AttributeError('regional_term has to be a callable object which takes three parameters.')\n    if not hasattr(boundary_term, '__call__') or not 3 == len(inspect.getargspec(boundary_term)[0]):\n        raise AttributeError('boundary_term has to be a callable object which takes three parameters.')    \n    \n    logger.info('Determining number of nodes and edges.')\n    \n    # compute number of nodes and edges\n    nodes = len(scipy.unique(label_image))\n    # POSSIBILITY 1: guess the number of edges (in the best situation is faster but requires a little bit more memory. In the worst is slower.)\n    edges = 10 * nodes\n    logger.debug('guessed: #nodes={} nodes / #edges={}'.format(nodes, edges))\n    # POSSIBILITY 2: compute the edges (slow)\n    #edges = len(__compute_edges(label_image))\n    #logger.debug('computed: #nodes={} nodes / #edges={}'.format(nodes, edges))\n        \n    # prepare result graph\n    graph = GCGraph(nodes, edges)\n                                        \n    logger.debug('#hardwired-nodes source/sink={}/{}'.format(len(scipy.unique(label_image[fg_markers])),\n                                                             len(scipy.unique(label_image[bg_markers]))))\n                 \n    #logger.info('Extracting the regions bounding boxes...')\n    # extract the bounding boxes\n    #bounding_boxes = find_objects(label_image)\n        \n    # compute the weights of all edges from the source and to the sink i.e.\n    # compute the weights of the t_edges Wt\n    logger.info('Computing and adding terminal edge weights...')\n    #regions = set(graph.get_nodes()) - set(graph.get_source_nodes()) - set(graph.get_sink_nodes())\n    regional_term(graph, label_image, regional_term_args) # bounding boxes indexed from 0 # old version: regional_term(graph, label_image, regions, bounding_boxes, regional_term_args)\n\n    # compute the weights of the edges between the neighbouring nodes i.e.\n    # compute the weights of the n_edges Wr\n    logger.info('Computing and adding inter-node edge weights...')\n    boundary_term(graph, label_image, boundary_term_args)\n    \n    # collect all regions that are under the foreground resp. background markers i.e.\n    # collect all nodes that are connected to the source resp. sink\n    logger.info('Setting terminal weights for the markers...')\n    graph.set_source_nodes(scipy.unique(label_image[fg_markers] - 1)) # requires -1 to adapt to node id system\n    graph.set_sink_nodes(scipy.unique(label_image[bg_markers] - 1))\n    \n    return graph.get_graph()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the number of edges for the supplied image shape assuming 4-connectedness. The name of the function has historical reasons. Essentially it returns the number of edges assuming 4-connectedness only for 2D. For 3D it assumes 6-connectedness, etc. @param shape the shape of the image @type shape sequence @return the number of edges @rtype int", "response": "def __voxel_4conectedness(shape):\n    \"\"\"\n    Returns the number of edges for the supplied image shape assuming 4-connectedness.\n    \n    The name of the function has historical reasons. Essentially it returns the number\n    of edges assuming 4-connectedness only for 2D. For 3D it assumes 6-connectedness,\n    etc.\n    \n    @param shape the shape of the image\n    @type shape sequence\n    @return the number of edges\n    @rtype int\n    \"\"\"\n    shape = list(shape)\n    while 1 in shape: shape.remove(1) # empty resp. 1-sized dimensions have to be removed (equal to scipy.squeeze on the array)\n    return int(round(sum([(dim - 1)/float(dim) for dim in shape]) * scipy.prod(shape)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nproviding additional validation of the arguments collected by argparse.", "response": "def getArguments(parser):\n    \"Provides additional validation of the arguments collected by argparse.\"\n    args = parser.parse_args()\n    # get the number of dimensions in the image\n    if args.example:\n        args.example_image, args.example_header = load(args.example)\n        dimensions = args.example_image.ndim\n    else:\n        dimensions = len(args.shape)\n    \n    # check and, if required, modify the spacing argument\n    if isinstance(args.spacing, int):\n        args.spacing = [args.spacing] * dimensions\n    elif len(args.spacing) != dimensions:\n        raise argparse.ArgumentTypeError('the grid spacing ({}) must contain the same number of elements as the output image has dimensions ({})'.format(','.join(map(str, args.spacing)), dimensions))\n    \n    # check further arguments\n    if args.offset and len(args.offset) != dimensions:\n        raise argparse.ArgumentTypeError('the offset ({}) must contain the same number of elements as the output image has dimensions ({})'.format(','.join(map(str, args.offset)), dimensions))\n    if args.pixelspacing and len(args.pixelspacing) != dimensions:\n        raise argparse.ArgumentTypeError('the supplied pixel spacing ({}) must contain the same number of elements as the output image has dimensions ({})'.format(','.join(map(str, args.pixelspacing)), dimensions))\n    \n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getParser():\n    \"Creates and returns the argparse parser object.\"\n    # text\n    epilog =\"\"\"\nexamples:\n  %(prog)s -e example.nii grid.nii 10\n      Generates an empty image with the same attributes as example.nii, overlays it\n      with a regular grid of width 10 voxels and saves it as grid.nii.\n  %(prog)s -e example.nii grid.nii 10,11,12 -r\n      Same as above, but with an irregular grid and using real world coordinates\n      (i.e. taking the voxel spacing of the image into account).\n  %(prog)s -s 100,200 grid.nii 10,2 -p 0.5,3 \n      Generates a 10x2 spaced grid in a 100x200 image with a voxel spacing of 0.5x3.\n  %(prog)s -s 100,100,50 grid.nii 5,5,0 \n      Generates a 100x100x50 3D volume but fills it only with a regular 5x5 2D grid\n      over the first two dimensions.  \n\"\"\"\n    \n    # command line argument parser\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, \n                                     description=__description__, epilog=epilog)\n    parser.add_argument('output', help='Generated grid volume.')\n    parser.add_argument('spacing', type=list_of_integers_or_int, help='The grid spacing. Can be a single digit for regular spacing in all dimensions or a colon-separated list of N integers, where N is the number of dimension in the generated volume. To skip the grid in one dimension, simply supply a 0 for it.')\n    \n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument('-e', '--example', dest='example', help='Option 1/2: Supply an image to create the grid volume by example (i.e. with same shape, voxel spacing and offset).')\n    group.add_argument('-s', '--shape', type=list_of_integers, dest='shape', help='Option 2/2: Supply a colon-separated list of integers that constitute the target volumes shape.')\n    \n    parser.add_argument('-p', '--pixel-spacing', type=list_of_floats, dest='pixelspacing', help='Set the pixel spacing of the target volume by supplying a colon-separated list of N numbers, where N is the number of dimension in the generated volume.')\n    parser.add_argument('-o', '--offset', type=list_of_floats, dest='offset', help='Set offset of the target volume by supplying a colon-separated list of N numbers, where N is the number of dimension in the generated volume.')\n\n    parser.add_argument('-r', '--real', dest='real', action='store_true', help='Spacing is given in real world coordinates, rather than voxels. For this to make a difference, either the -e switch or the -p switch must be set.')\n    \n    parser.add_argument('-v', dest='verbose', action='store_true', help='Display more information.')\n    parser.add_argument('-d', dest='debug', action='store_true', help='Display debug information.')\n    parser.add_argument('-f', '--force', dest='force', action='store_true', help='Silently override existing output images.')\n    return parser", "response": "Creates and returns the argparse parser object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting an integer marker image into two binary images containing the foreground and background markers respectively.", "response": "def split_marker(marker, fg_id = 1, bg_id = 2):\n    \"\"\"\n    Splits an integer marker image into two binary image containing the foreground and\n    background markers respectively.\n    All encountered 1's are hereby treated as foreground, all 2's as background, all 0's\n    as neutral marker and all others are ignored.\n    This behaviour can be changed by supplying the fg_id and/or bg_id parameters.\n    \n    Parameters\n    ----------\n    marker : ndarray\n        The marker image.\n    fg_id : integer\n        The value that should be treated as foreground.\n    bg_id : integer\n        The value that should be treated as background.\n    \n    Returns\n    -------\n    fgmarkers, bgmarkers : nadarray\n        The fore- and background markers as boolean images.\n    \"\"\"\n    img_marker = scipy.asarray(marker)\n    \n    img_fgmarker = scipy.zeros(img_marker.shape, scipy.bool_)\n    img_fgmarker[img_marker == fg_id] = True\n    \n    img_bgmarker = scipy.zeros(img_marker.shape, scipy.bool_)\n    img_bgmarker[img_marker == bg_id] = True\n    \n    return img_fgmarker, img_bgmarker"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a graph cut on the original volume and returns the number of sub - volumes that are split into smaller edges.", "response": "def graphcut_split(graphcut_function, regions, gradient, foreground, background, minimal_edge_length = 100, overlap = 10, processes = None):\n    \"\"\"\n    Executes a graph cut by splitting the original volume into a number of sub-volumes of\n    a minimal edge length. These are then processed in subprocesses.\n    \n    This can be significantly faster than the traditional graph cuts, but should be\n    used with, as it can lead to different results. To minimize this effect, the overlap\n    parameter allows control over how much the respective sub-volumes should overlap.\n    \n    Parameters\n    ----------\n    graphcut_function : function\n        The graph cut to use (e.g. `graphcut_stawiaski`).\n    regions : ndarray\n        The regions image / label map.\n    gradient : ndarray\n        The gradient image.\n    foreground : ndarray\n        The foreground markers.\n    background : ndarray\n        The background markers.\n    minimal_edge_length : integer\n        The minimal edge length of the sub-volumes in voxels.\n    overlap : integer\n        The overlap (in voxels) between the generated sub-volumes.\n    processes : integer or None\n        The number of processes to run simultaneously, if not supplied, will be the same\n        as the number of processors.\n        \n    Returns\n    -------\n    segmentation : ndarray\n        The graph-cut segmentation result as boolean array.\n    \"\"\"\n    # initialize logger\n    logger = Logger.getInstance()\n    \n    # ensure that input images are scipy arrays\n    img_region = scipy.asarray(regions)\n    img_gradient = scipy.asarray(gradient)\n    img_fg = scipy.asarray(foreground, dtype=scipy.bool_)\n    img_bg = scipy.asarray(background, dtype=scipy.bool_)\n    \n    # ensure correctness of supplied images\n    if not (img_region.shape == img_gradient.shape == img_fg.shape == img_bg.shape): raise ArgumentError('All supplied images must be of the same shape.')    \n    \n    # check and eventually enhance input parameters\n    if minimal_edge_length < 10: raise ArgumentError('A minimal edge length smaller than 10 is not supported.')\n    if overlap < 0: raise ArgumentError('A negative overlap is not supported.')\n    if overlap >= minimal_edge_length: raise ArgumentError('The overlap is not allowed to exceed the minimal edge length.')\n    \n    # compute how to split the volumes into sub-volumes i.e. determine step-size for each image dimension\n    shape = list(img_region.shape)\n    steps = [x // minimal_edge_length for x in shape]\n    steps = [1 if 0 == x else x for x in steps] # replace zeros by ones\n    stepsizes = [math.ceil(x / y) for x, y in zip(shape, steps)]\n    logger.debug('Using a minimal edge length of {}, a sub-volume size of {} was determined from the shape {}, which means {} sub-volumes.'.format(minimal_edge_length, stepsizes, shape, reduce(lambda x, y: x*y, steps)))\n    \n    # control step-sizes to definitely cover the whole image\n    covered_shape = [x * y for x, y in zip(steps, stepsizes)]\n    for c, o in zip(covered_shape, shape):\n        if c < o: raise Exception(\"The computed sub-volumes do not cover the complete image!\")\n            \n    # iterate over the steps and extract subvolumes according to the stepsizes\n    slicer_steps = [list(range(0, int(step * stepsize), int(stepsize))) for step, stepsize in zip(steps, stepsizes)]\n    slicers = [[slice(_from, _from + _offset + overlap) for _from, _offset in zip(slicer_step, stepsizes)] for slicer_step in itertools.product(*slicer_steps)]\n    subvolumes_input = [(img_region[slicer],\n                         img_gradient[slicer],\n                         img_fg[slicer],\n                         img_bg[slicer]) for slicer in slicers]\n    \n    # execute the graph cuts and collect results\n    subvolumes_output = graphcut_subprocesses(graphcut_function, subvolumes_input, processes)\n    \n    # put back data together\n    img_result = scipy.zeros(img_region.shape, dtype=scipy.bool_)\n    for slicer, subvolume in zip(slicers, subvolumes_output):\n        sslicer_antioverlap = [slice(None)] * img_result.ndim\n        \n        # treat overlap area using logical-and (&)\n        for dim in range(img_result.ndim):\n            if 0 == slicer[dim].start: continue\n            sslicer_antioverlap[dim] = slice(overlap, None)\n            sslicer_overlap = [slice(None)] * img_result.ndim\n            sslicer_overlap[dim] = slice(0, overlap)\n            img_result[slicer][sslicer_overlap] = scipy.logical_and(img_result[slicer][sslicer_overlap], subvolume[sslicer_overlap])\n            \n        # treat remainder through assignment\n        img_result[slicer][sslicer_antioverlap] = subvolume[sslicer_antioverlap]\n    \n    return img_result.astype(scipy.bool_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef graphcut_subprocesses(graphcut_function, graphcut_arguments, processes = None):\n    # initialize logger\n    logger = Logger.getInstance()\n    \n    # check and eventually enhance input parameters\n    if not processes: processes = multiprocessing.cpu_count()\n    if not int == type(processes) or processes <= 0: raise ArgumentError('The number processes can not be zero or negative.')\n    \n    logger.debug('Executing graph cuts in {} subprocesses.'.format(multiprocessing.cpu_count()))\n    \n    # creates subprocess pool and execute\n    pool = multiprocessing.Pool(processes)\n    results = pool.map(graphcut_function, graphcut_arguments)\n    \n    return results", "response": "Executes multiple graph cuts in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a Stawiaski label graph cut on the supplied regions.", "response": "def graphcut_stawiaski(regions, gradient = False, foreground = False, background = False):\n    \"\"\"\n    Executes a Stawiaski label graph cut.\n    \n    Parameters\n    ----------\n    regions : ndarray\n        The regions image / label map.\n    gradient : ndarray\n        The gradient image.\n    foreground : ndarray\n        The foreground markers.\n    background : ndarray\n        The background markers.\n        \n    Returns\n    -------\n    segmentation : ndarray\n        The graph-cut segmentation result as boolean array.\n        \n    Raises\n    ------\n    ArgumentError\n        When the supplied data is erroneous.\n    \"\"\"\n    # initialize logger\n    logger = Logger.getInstance()\n    \n    # unpack images if required\n    # !TODO: This is an ugly hack, especially since it can be seen inside the function definition\n    # How to overcome this, since I can not use a wrapper function as the whole thing must be pickable\n    if not gradient and not foreground and not background: \n        regions, gradient, foreground, background = regions\n    \n    # ensure that input images are scipy arrays\n    img_region = scipy.asarray(regions)\n    img_gradient = scipy.asarray(gradient)\n    img_fg = scipy.asarray(foreground, dtype=scipy.bool_)\n    img_bg = scipy.asarray(background, dtype=scipy.bool_)\n    \n    # ensure correctness of supplied images\n    if not (img_region.shape == img_gradient.shape == img_fg.shape == img_bg.shape): raise ArgumentError('All supplied images must be of the same shape.')\n\n    # recompute the label ids to start from id = 1\n    img_region = relabel(img_region)\n    \n    # generate graph\n    gcgraph = graph_from_labels(img_region, img_fg, img_bg, boundary_term = boundary_stawiaski, boundary_term_args = (img_gradient))\n    \n    # execute min-cut\n    maxflow = gcgraph.maxflow() # executes the cut and returns the maxflow value\n    \n    logger.debug('Graph-cut terminated successfully with maxflow of {}.'.format(maxflow))\n    \n    # apply results to the region image\n    mapping = [0] # no regions with id 1 exists in mapping, entry used as padding\n    mapping.extend([0 if gcgraph.termtype.SINK == gcgraph.what_segment(int(x) - 1) else 1 for x in scipy.unique(img_region)])\n    img_results = relabel_map(img_region, mapping)\n    \n    return img_results.astype(scipy.bool_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nzoom the provided image by the supplied factor in the supplied dimension.", "response": "def zoom(image, factor, dimension, hdr = False, order = 3):\n    \"\"\"\n    Zooms the provided image by the supplied factor in the supplied dimension.\n    The factor is an integer determining how many slices should be put between each\n    existing pair.\n    If an image header (hdr) is supplied, its voxel spacing gets updated.\n    Returns the image and the updated header or false.\n    \"\"\"\n    # check if supplied dimension is valid\n    if dimension >= image.ndim:\n        raise argparse.ArgumentError('The supplied zoom-dimension {} exceeds the image dimensionality of 0 to {}.'.format(dimension, image.ndim - 1))\n    \n    # get logger\n    logger = Logger.getInstance()\n\n    logger.debug('Old shape = {}.'.format(image.shape))\n\n    # perform the zoom\n    zoom = [1] * image.ndim\n    zoom[dimension] = (image.shape[dimension] + (image.shape[dimension] - 1) * factor) / float(image.shape[dimension])\n    logger.debug('Reshaping with = {}.'.format(zoom))\n    image = interpolation.zoom(image, zoom, order=order)\n        \n    logger.debug('New shape = {}.'.format(image.shape))\n    \n    if hdr:\n        new_spacing = list(header.get_pixel_spacing(hdr))\n        new_spacing[dimension] = new_spacing[dimension] / float(factor + 1)\n        logger.debug('Setting pixel spacing from {} to {}....'.format(header.get_pixel_spacing(hdr), new_spacing))\n        header.set_pixel_spacing(hdr, tuple(new_spacing))\n    \n    return image, hdr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_phantom_module(xml_file):\n    import lxml.etree as etree\n\n    object_cache = {}\n\n    tree = etree.parse(xml_file)\n    root = tree.getroot()\n\n    # Sort items so that\n    # - Base classes come before classes inherited from them\n    # - Modules come before their contents\n    all_nodes = dict([(n.attrib['id'], n) for n in root])\n    \n    def _get_bases(node, recurse=False):\n        bases = [x.attrib['ref'] for x in node.findall('base')]\n        if recurse:\n            j = 0\n            while True:\n                try:\n                    b = bases[j]\n                except IndexError: break\n                if b in all_nodes:\n                    bases.extend(_get_bases(all_nodes[b]))\n                j += 1\n        return bases\n\n    type_index = ['module', 'class', 'callable', 'object']\n    \n    def base_cmp(a, b):\n        x = cmp(type_index.index(a.tag), type_index.index(b.tag))\n        if x != 0: return x\n\n        if a.tag == 'class' and b.tag == 'class':\n            a_bases = _get_bases(a, recurse=True)\n            b_bases = _get_bases(b, recurse=True)\n            x = cmp(len(a_bases), len(b_bases))\n            if x != 0: return x\n            if a.attrib['id'] in b_bases: return -1\n            if b.attrib['id'] in a_bases: return 1\n        \n        return cmp(a.attrib['id'].count('.'), b.attrib['id'].count('.'))\n\n    nodes = root.getchildren()\n    nodes.sort(base_cmp)\n\n    # Create phantom items\n    for node in nodes:\n        name = node.attrib['id']\n        doc = (node.text or '').decode('string-escape') + \"\\n\"\n        if doc == \"\\n\": doc = \"\"\n\n        # create parent, if missing\n        parent = name\n        while True:\n            parent = '.'.join(parent.split('.')[:-1])\n            if not parent: break\n            if parent in object_cache: break\n            obj = imp.new_module(parent)\n            object_cache[parent] = obj\n            sys.modules[parent] = obj\n\n        # create object\n        if node.tag == 'module':\n            obj = imp.new_module(name)\n            obj.__doc__ = doc\n            sys.modules[name] = obj\n        elif node.tag == 'class':\n            bases = [object_cache[b] for b in _get_bases(node)\n                     if b in object_cache]\n            bases.append(object)\n            init = lambda self: None\n            init.__doc__ = doc\n            obj = type(name, tuple(bases), {'__doc__': doc, '__init__': init})\n            obj.__name__ = name.split('.')[-1]\n        elif node.tag == 'callable':\n            funcname = node.attrib['id'].split('.')[-1]\n            argspec = node.attrib.get('argspec')\n            if argspec:\n                argspec = re.sub('^[^(]*', '', argspec)\n                doc = \"%s%s\\n\\n%s\" % (funcname, argspec, doc)\n            obj = lambda: 0\n            obj.__argspec_is_invalid_ = True\n            if sys.version_info[0] >= 3:\n                obj.__name__ = funcname\n            else:\n                obj.__name__ = funcname\n            obj.__name__ = name\n            obj.__doc__ = doc\n            if inspect.isclass(object_cache[parent]):\n                obj.__objclass__ = object_cache[parent]\n        else:\n            class Dummy(object): pass\n            obj = Dummy()\n            obj.__name__ = name\n            obj.__doc__ = doc\n            if inspect.isclass(object_cache[parent]):\n                obj.__get__ = lambda: None\n        object_cache[name] = obj\n\n        if parent:\n            if inspect.ismodule(object_cache[parent]):\n                obj.__module__ = parent\n                setattr(object_cache[parent], name.split('.')[-1], obj)\n\n    # Populate items\n    for node in root:\n        obj = object_cache.get(node.attrib['id'])\n        if obj is None: continue\n        for ref in node.findall('ref'):\n            if node.tag == 'class':\n                if ref.attrib['ref'].startswith(node.attrib['id'] + '.'):\n                    setattr(obj, ref.attrib['name'],\n                            object_cache.get(ref.attrib['ref']))\n            else:\n                setattr(obj, ref.attrib['name'],\n                        object_cache.get(ref.attrib['ref']))", "response": "Imports a phantom module from an XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef regional_probability_map(graph, xxx_todo_changeme):\n    (probability_map, alpha) = xxx_todo_changeme\n    probability_map = scipy.asarray(probability_map)\n    probabilities = numpy.vstack([(probability_map * alpha).flat,\n                                  ((1 - probability_map) * alpha).flat]).T\n    graph.set_tweights_all(probabilities)", "response": "r This function calculates the probability of a given regional term based on a given probability atlas image."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef boundary_difference_linear(graph, xxx_todo_changeme2):\n    (original_image, spacing) = xxx_todo_changeme2\n    original_image = scipy.asarray(original_image)\n    \n    # compute maximum (possible) intensity difference\n    max_intensity_difference = float(abs(original_image.max() - original_image.min()))\n    \n    def boundary_term_linear(intensities):\n        \"\"\"\n        Implementation of a linear boundary term computation over an array.\n        \"\"\"\n        # normalize the intensity distances to the interval (0, 1]\n        intensities /= max_intensity_difference\n        #difference_to_neighbour[difference_to_neighbour > 1] = 1 # this line should not be required, but might be due to rounding errors\n        intensities = (1. - intensities) # reverse weights such that high intensity difference lead to small weights and hence more likely to a cut at this edge\n        intensities[intensities == 0.] = sys.float_info.min # required to avoid zero values\n        return intensities\n    \n    __skeleton_difference(graph, original_image, boundary_term_linear, spacing)", "response": "r Returns a regional term that processes adjacent voxels difference values using a linear relationship."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef boundary_maximum_exponential(graph, xxx_todo_changeme3):\n    (gradient_image, sigma, spacing) = xxx_todo_changeme3\n    gradient_image = scipy.asarray(gradient_image)\n    \n    def boundary_term_exponential(intensities):\n        \"\"\"\n        Implementation of a exponential boundary term computation over an array.\n        \"\"\"\n        # apply exp-(x**2/sigma**2)\n        intensities = scipy.power(intensities, 2)\n        intensities /= math.pow(sigma, 2)\n        intensities *= -1\n        intensities = scipy.exp(intensities)\n        intensities[intensities <= 0] = sys.float_info.min\n        return intensities\n    \n    __skeleton_maximum(graph, gradient_image, boundary_term_exponential, spacing)", "response": "r Returns the maximum value of the adjacent voxels using an exponential relationship."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef boundary_difference_exponential(graph, xxx_todo_changeme4):\n    (original_image, sigma, spacing) = xxx_todo_changeme4\n    original_image = scipy.asarray(original_image)\n    \n    def boundary_term_exponential(intensities):\n        \"\"\"\n        Implementation of a exponential boundary term computation over an array.\n        \"\"\"\n        # apply exp-(x**2/sigma**2)\n        intensities = scipy.power(intensities, 2)\n        intensities /= math.pow(sigma, 2)\n        intensities *= -1\n        intensities = scipy.exp(intensities)\n        intensities[intensities <= 0] = sys.float_info.min\n        return intensities\n    \n    __skeleton_difference(graph, original_image, boundary_term_exponential, spacing)", "response": "r Returns a new graph that processes adjacent voxels difference values using an exponential relationship."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef boundary_maximum_power(graph, xxx_todo_changeme7):\n    (gradient_image, sigma, spacing) = xxx_todo_changeme7\n    gradient_image = scipy.asarray(gradient_image)\n    \n    def boundary_term_power(intensities):\n        \"\"\"\n        Implementation of a power boundary term computation over an array.\n        \"\"\"\n        # apply (1 / (1  + x))^sigma\n        intensities = 1. / (intensities + 1)\n        intensities = scipy.power(intensities, sigma)\n        intensities[intensities <= 0] = sys.float_info.min\n        return intensities\n    \n    __skeleton_maximum(graph, gradient_image, boundary_term_power, spacing)", "response": "Returns the maximum power of the boundary term."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __skeleton_maximum(graph, image, boundary_term, spacing):\n    def intensity_maximum(neighbour_one, neighbour_two):\n        \"\"\"\n        Takes two voxel arrays constituting neighbours and computes the maximum between\n        their intensities.\n        \"\"\"\n        return scipy.maximum(neighbour_one, neighbour_two)\n        \n    __skeleton_base(graph, numpy.abs(image), boundary_term, intensity_maximum, spacing)", "response": "Calculates the maximum intensity of a skeleton given an image and a function that returns the maximum intensity of the neighbouring voxels."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbases of the skeleton for voxel based boundary term calculation. This function holds the low level procedures shared by nearly all boundary terms. @param graph An initialized graph.GCGraph object @type graph.GCGraph @param image The image containing the voxel intensity values @type image numpy.ndarray @param boundary_term A function to compute the boundary term over an array of absolute intensity differences @type boundary_term function @param neighbourhood_function A function that takes two arrays of neighbouring pixels and computes an intensity term from them that is returned as a single array of the same shape @type neighbourhood_function function @param spacing A sequence containing the slice spacing used for weighting the computed neighbourhood weight value for different dimensions. If False, no distance based weighting of the graph edges is performed. @param spacing sequence | False", "response": "def __skeleton_base(graph, image, boundary_term, neighbourhood_function, spacing):\n    \"\"\"\n    Base of the skeleton for voxel based boundary term calculation.\n    \n    This function holds the low level procedures shared by nearly all boundary terms.\n    \n    @param graph An initialized graph.GCGraph object\n    @type graph.GCGraph\n    @param image The image containing the voxel intensity values\n    @type image numpy.ndarray\n    @param boundary_term A function to compute the boundary term over an array of\n                           absolute intensity differences\n    @type boundary_term function\n    @param neighbourhood_function A function that takes two arrays of neighbouring pixels\n                                  and computes an intensity term from them that is\n                                  returned as a single array of the same shape\n    @type neighbourhood_function function\n    @param spacing A sequence containing the slice spacing used for weighting the\n                   computed neighbourhood weight value for different dimensions. If\n                   False, no distance based weighting of the graph edges is performed.\n    @param spacing sequence | False\n    \"\"\"\n    image = scipy.asarray(image)\n    image = image.astype(scipy.float_)\n\n    # iterate over the image dimensions and for each create the appropriate edges and compute the associated weights\n    for dim in range(image.ndim):\n        # construct slice-objects for the current dimension\n        slices_exclude_last = [slice(None)] * image.ndim\n        slices_exclude_last[dim] = slice(-1)\n        slices_exclude_first = [slice(None)] * image.ndim\n        slices_exclude_first[dim] = slice(1, None)\n        # compute difference between all layers in the current dimensions direction\n        neighbourhood_intensity_term = neighbourhood_function(image[slices_exclude_last], image[slices_exclude_first])\n        # apply boundary term\n        neighbourhood_intensity_term = boundary_term(neighbourhood_intensity_term)\n        # compute key offset for relative key difference\n        offset_key = [1 if i == dim else 0 for i in range(image.ndim)]\n        offset = __flatten_index(offset_key, image.shape)\n        # generate index offset function for index dependent offset\n        idx_offset_divider = (image.shape[dim] - 1) * offset\n        idx_offset = lambda x: int(x / idx_offset_divider) * offset\n        \n        # weight the computed distanced in dimension dim by the corresponding slice spacing provided\n        if spacing: neighbourhood_intensity_term /= spacing[dim]\n        \n        for key, value in enumerate(neighbourhood_intensity_term.ravel()):\n            # apply index dependent offset\n            key += idx_offset(key) \n            # add edges and set the weight\n            graph.set_nweight(key, key + offset, value, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __flatten_index(pos, shape):\n    res = 0\n    acc = 1\n    for pi, si in zip(reversed(pos), reversed(shape)):\n        res += pi * acc\n        acc *= si\n    return res", "response": "This function takes a three dimensional index x y z and computes the index required to access the same element in the flattened version of the array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mutual_information(i1, i2, bins=256):\n    # pre-process function arguments\n    i1 = numpy.asarray(i1)\n    i2 = numpy.asarray(i2)\n    \n    # validate function arguments\n    if not i1.shape == i2.shape:\n        raise ArgumentError('the two supplied array-like sequences i1 and i2 must be of the same shape')\n    \n    # compute i1 and i2 histogram range\n    i1_range = __range(i1, bins)\n    i2_range = __range(i2, bins)\n    \n    # compute joined and separated normed histograms\n    i1i2_hist, _, _ = numpy.histogram2d(i1.flatten(), i2.flatten(), bins=bins, range=[i1_range, i2_range]) # Note: histogram2d does not flatten array on its own\n    i1_hist, _ = numpy.histogram(i1, bins=bins, range=i1_range)\n    i2_hist, _ = numpy.histogram(i2, bins=bins, range=i2_range)\n    \n    # compute joined and separated entropy\n    i1i2_entropy = __entropy(i1i2_hist)\n    i1_entropy = __entropy(i1_hist)\n    i2_entropy = __entropy(i2_hist)\n    \n    # compute and return the mutual information distance\n    return i1_entropy + i2_entropy - i1i2_entropy", "response": "r Computes the mutual information between two images."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __range(a, bins):\n    '''Compute the histogram range of the values in the array a according to\n    scipy.stats.histogram.'''\n    a = numpy.asarray(a)\n    a_max = a.max()\n    a_min = a.min()\n    s = 0.5 * (a_max - a_min) / float(bins - 1)\n    return (a_min - s, a_max + s)", "response": "Compute the histogram range of the values in the array a according to\n    scipy. stats. histogram."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __entropy(data):\n    '''Compute entropy of the flattened data set (e.g. a density distribution).'''\n    # normalize and convert to float\n    data = data/float(numpy.sum(data))\n    # for each grey-value g with a probability p(g) = 0, the entropy is defined as 0, therefore we remove these values and also flatten the histogram\n    data = data[numpy.nonzero(data)]\n    # compute entropy\n    return -1. * numpy.sum(data * numpy.log2(data))", "response": "Compute the entropy of the flattened data set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprovide additional validation of the arguments collected by argparse.", "response": "def getArguments(parser):\n    \"Provides additional validation of the arguments collected by argparse.\"\n    args = parser.parse_args()\n    if not '{}' in args.output:\n        raise argparse.ArgumentError(args.output, 'The output argument string must contain the sequence \"{}\".')\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef centerdistance_xdminus1(image, dim, voxelspacing = None, mask = slice(None)):\n    # pre-process arguments\n    if type(image) == tuple or type(image) == list:\n        image = image[0]\n    \n    if type(dim) is int:\n        dims = [dim]\n    else:\n        dims = list(dim)\n        \n    # check arguments\n    if len(dims) >= image.ndim - 1:\n        raise ArgumentError('Applying a sub-volume extraction of depth {} on a image of dimensionality {} would lead to invalid images of dimensionality <= 1.'.format(len(dims), image.ndim))\n    for dim in dims:\n        if dim >= image.ndim:\n            raise ArgumentError('Invalid dimension index {} supplied for image(s) of shape {}.'.format(dim, image.shape))\n    \n    # extract desired sub-volume\n    slicer = [slice(None)] * image.ndim\n    for dim in dims: slicer[dim] = slice(1)\n    subvolume = numpy.squeeze(image[slicer])\n    \n    # compute centerdistance for sub-volume and reshape to original sub-volume shape (note that normalization and mask are not passed on in this step)\n    o = centerdistance(subvolume, voxelspacing).reshape(subvolume.shape)\n    \n    # re-establish original shape by copying the resulting array multiple times\n    for dim in sorted(dims):\n        o = numpy.asarray([o] * image.shape[dim])\n        o = numpy.rollaxis(o, 0, dim + 1)\n        \n    # extract intensities / centerdistance values, applying normalization and mask in this step\n    return intensities(o, mask)", "response": "r Returns the centerdistance of the image minus one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef indices(image, voxelspacing = None, mask = slice(None)):\n    if type(image) == tuple or type(image) == list:\n        image = image[0]\n        \n    if not type(mask) is slice:\n        mask = numpy.array(mask, copy=False, dtype=numpy.bool)\n        \n    if voxelspacing is None:\n        voxelspacing = [1.] * image.ndim\n\n    return join(*[a[mask].ravel() * vs for a, vs in zip(numpy.indices(image.shape), voxelspacing)])", "response": "r Returns the indices of the voxels in the image that are voxel - wise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gaussian_gradient_magnitude(image, sigma = 5, voxelspacing = None, mask = slice(None)):\n    return _extract_feature(_extract_gaussian_gradient_magnitude, image, mask, sigma = sigma, voxelspacing = voxelspacing)", "response": "r Computes the gradient magnitude of the image using gaussian derivates and returns the intensity values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the multi - dimensional median filter of the given image.", "response": "def median(image, size = 5, voxelspacing = None, mask = slice(None)):\n    \"\"\"\n    Computes the multi-dimensional median filter and returns the resulting values per\n    voxel.\n    \n    Optionally a binary mask can be supplied to select the voxels for which the feature\n    should be extracted.\n    \n    Parameters\n    ----------\n    image : array_like or list/tuple of array_like \n        A single image or a list/tuple of images (for multi-spectral case).\n    size : number or sequence of numbers\n        Size of the structuring element. Can be given given for each axis as a sequence,\n        or as a single number, in which case it is equal for all axes. Note that the\n        voxel spacing of the image is taken into account, the given values are treated\n        as mm.\n    voxelspacing : sequence of floats\n        The side-length of each voxel.\n    mask : array_like\n        A binary mask for the image.\n        \n    Returns\n    -------\n    median : ndarray\n        Multi-dimesnional median filtered version of the input images.\n    \n    \"\"\"\n    return _extract_feature(_extract_median, image, mask, size = size, voxelspacing = voxelspacing)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef local_histogram(image, bins=19, rang=\"image\", cutoffp=(0.0, 100.0), size=None, footprint=None, output=None, mode=\"ignore\", origin=0, mask=slice(None)):\n    return _extract_feature(_extract_local_histogram, image, mask, bins=bins, rang=rang, cutoffp=cutoffp, size=size, footprint=footprint, output=output, mode=mode, origin=origin)", "response": "r Computes a multi - dimensional local histogram of the image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_median(image, mask = slice(None), size = 1, voxelspacing = None):\n    # set voxel spacing\n    if voxelspacing is None:\n        voxelspacing = [1.] * image.ndim\n        \n    # determine structure element size in voxel units\n    size = _create_structure_array(size, voxelspacing)\n        \n    return _extract_intensities(median_filter(image, size), mask)", "response": "Internal single - image version of median."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _extract_gaussian_gradient_magnitude(image, mask = slice(None), sigma = 1, voxelspacing = None):\n    # set voxel spacing\n    if voxelspacing is None:\n        voxelspacing = [1.] * image.ndim\n        \n    # determine gaussian kernel size in voxel units\n    sigma = _create_structure_array(sigma, voxelspacing)\n        \n    return _extract_intensities(scipy_gaussian_gradient_magnitude(image, sigma), mask)", "response": "Internal single - image version of gaussian_gradient_magnitude"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extract_mask_distance(image, mask = slice(None), voxelspacing = None):\n    if isinstance(mask, slice):\n        mask = numpy.ones(image.shape, numpy.bool)\n    \n    distance_map = distance_transform_edt(mask, sampling=voxelspacing)\n    \n    return _extract_intensities(distance_map, mask)", "response": "Internal single - image version of mask_distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_centerdistance(image, mask = slice(None), voxelspacing = None):\n    image = numpy.array(image, copy=False)\n    \n    if None == voxelspacing:\n        voxelspacing = [1.] * image.ndim\n        \n    # get image center and an array holding the images indices\n    centers = [(x - 1) / 2. for x in image.shape]\n    indices = numpy.indices(image.shape, dtype=numpy.float)\n    \n    # shift to center of image and correct spacing to real world coordinates\n    for dim_indices, c, vs in zip(indices, centers, voxelspacing):\n        dim_indices -= c\n        dim_indices *= vs\n        \n    # compute euclidean distance to image center\n    return numpy.sqrt(numpy.sum(numpy.square(indices), 0))[mask].ravel()", "response": "Internal single - image version of centerdistance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extract_intensities(image, mask = slice(None)):\n    return numpy.array(image, copy=True)[mask].ravel()", "response": "Internal single - image version of intensities."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a structure array to image space.", "response": "def _create_structure_array(structure_array, voxelspacing):\n    \"\"\"\n    Convenient function to take a structure array (single number valid for all dimensions\n    or a sequence with a distinct number for each dimension) assumed to be in mm and\n    returns a structure array (a sequence) adapted to the image space using the supplied\n    voxel spacing.\n    \"\"\"\n    try:\n        structure_array = [s / float(vs) for s, vs in zip(structure_array, voxelspacing)]\n    except TypeError:\n        structure_array = [structure_array / float(vs) for vs in voxelspacing]\n    \n    return structure_array"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_feature(fun, image, mask = slice(None), **kwargs):\n    if not type(mask) is slice:\n        mask = numpy.array(mask, copy=False, dtype=numpy.bool)\n    \n    if type(image) == tuple or type(image) == list:\n        return join(*[fun(i, mask, **kwargs) for i in image])\n    else:\n        return fun(image, mask, **kwargs)", "response": "This function is used to extract features from the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getArguments(parser):\n    \"Provides additional validation of the arguments collected by argparse.\"\n    args = parser.parse_args()    \n    # check output image exists if override not forced\n    if not args.force:\n        if os.path.exists(args.output + args.image[-4:]):\n            raise ArgumentError('The supplied output file {} already exists. Run -f/force flag to override.'.format(args.output))\n\n    return args", "response": "Provides additional validation of the arguments collected by argparse."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nindents a piece of text according to the current indentation level", "response": "def _fill(self, text = \"\"):\n        \"Indent a piece of text, according to the current indentation level\"\n        if self._do_indent:\n            self._write(\"\\n\"+\"    \"*self._indent + text)\n        else:\n            self._write(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dispatch(self, tree):\n        \"_dispatcher function, _dispatching tree type T to method _T.\"\n        if isinstance(tree, list):\n            for t in tree:\n                self._dispatch(t)\n            return\n        meth = getattr(self, \"_\"+tree.__class__.__name__)\n        if tree.__class__.__name__ == 'NoneType' and not self._do_indent:\n            return\n        meth(tree)", "response": "_dispatcher function _dispatching tree type T to method _T."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _AssAttr(self, t):\n        self._dispatch(t.expr)\n        self._write('.'+t.attrname)", "response": "Handle assigning an attribute of an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _Assign(self, t):\n        self._fill()\n        for target in t.nodes:\n            self._dispatch(target)\n            self._write(\" = \")\n        self._dispatch(t.expr)\n        if not self._do_indent:\n            self._write('; ')", "response": "Internal assignment for the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the left hand side of an expression.", "response": "def _AssTuple(self, t):\n        \"\"\" Tuple on left hand side of an expression.\n        \"\"\"\n\n        # _write each elements, separated by a comma.\n        for element in t.nodes[:-1]:\n            self._dispatch(element)\n            self._write(\", \")\n\n        # Handle the last one without writing comma\n        last_element = t.nodes[-1]\n        self._dispatch(last_element)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _AugAssign(self, t):\n\n        self._fill()\n        self._dispatch(t.node)\n        self._write(' '+t.op+' ')\n        self._dispatch(t.expr)\n        if not self._do_indent:\n            self._write(';')", "response": "AugAssign is a helper function for assignment of a set of keys."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _CallFunc(self, t):\n        self._dispatch(t.node)\n        self._write(\"(\")\n        comma = False\n        for e in t.args:\n            if comma: self._write(\", \")\n            else: comma = True\n            self._dispatch(e)\n        if t.star_args:\n            if comma: self._write(\", \")\n            else: comma = True\n            self._write(\"*\")\n            self._dispatch(t.star_args)\n        if t.dstar_args:\n            if comma: self._write(\", \")\n            else: comma = True\n            self._write(\"**\")\n            self._dispatch(t.dstar_args)\n        self._write(\")\")", "response": "Internal function for parsing a function call."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _From(self, t):\n        # fixme: Are From and ImportFrom handled differently?\n        self._fill(\"from \")\n        self._write(t.modname)\n        self._write(\" import \")\n        for i, (name,asname) in enumerate(t.names):\n            if i != 0:\n                self._write(\", \")\n            self._write(name)\n            if asname is not None:\n                self._write(\" as \"+asname)", "response": "Handle from xyz import foo bar as baz."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling getting an attribute of an object", "response": "def _Getattr(self, t):\n        \"\"\" Handle getting an attribute of an object\n        \"\"\"\n        if isinstance(t.expr, (Div, Mul, Sub, Add)):\n            self._write('(')\n            self._dispatch(t.expr)\n            self._write(')')\n        else:\n            self._dispatch(t.expr)\n            \n        self._write('.'+t.attrname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles import xyz. foo.", "response": "def _Import(self, t):\n        \"\"\" Handle \"import xyz.foo\".\n        \"\"\"\n        self._fill(\"import \")\n        \n        for i, (name,asname) in enumerate(t.names):\n            if i != 0:\n                self._write(\", \")\n            self._write(name)\n            if asname is not None:\n                self._write(\" as \"+asname)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef existingDirectory(string):\n    if not os.path.isdir(string):\n        argparse.ArgumentTypeError('{} is not a valid directory.'.format(string))\n    return string", "response": "A custom type for the argparse commandline parser."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests a sequences values to be in strictly ascending order.", "response": "def __sequenceAscendingStrict(l):\n    \"Test a sequences values to be in strictly ascending order.\"\n    it = iter(l)\n    next(it)\n    if not all(b > a for a, b in zip(l, it)):\n        raise argparse.ArgumentTypeError('All values must be given in strictly ascending order.')\n    return l"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __compute_stdrange(self, images):\n        if not 'auto' in self.__stdrange:\n            return self.__stdrange\n        \n        copl, copu = self.__cutoffp\n        \n        # collect cutoff + landmark percentile segments and image mean intensity values \n        s = []\n        m = []\n        for idx, i in enumerate(images):\n            li = numpy.percentile(i, [copl] + self.__landmarkp + [copu])\n            \n            s.append(numpy.asarray(li)[1:] - numpy.asarray(li)[:-1])\n            m.append(i.mean())\n            \n            # treat single intensity accumulation error\n            if 0 in s[-1]:\n                raise SingleIntensityAccumulationError('Image no.{} shows an unusual single-intensity accumulation that leads to a situation where two percentile values are equal. This situation is usually caused, when the background has not been removed from the image. Another possibility would be to reduce the number of landmark percentiles landmarkp or to change their distribution.'.format(idx))\n            \n        # select the maximum and minimum of each percentile segment over all images\n        maxs = numpy.max(s, 0)\n        mins = numpy.min(s, 0)\n        \n        # divide them pairwise\n        divs = numpy.divide(numpy.asarray(maxs, dtype=numpy.float), mins) \n        \n        # compute interval range according to generalized theorem 2 of [1]\n        intv = numpy.sum(maxs) + numpy.max(divs)\n        \n        # compute mean intensity value over all images (assuming equal size)\n        im = numpy.mean(m)\n        \n        # return interval with borders according to settings\n        if 'auto' == self.__stdrange[0] and 'auto' == self.__stdrange[1]:\n            return im - intv / 2, im + intv / 2\n        elif 'auto' == self.__stdrange[0]:\n            return self.__stdrange[1] - intv, self.__stdrange[1]\n        else:\n            return self.__stdrange[0], self.__stdrange[0] + intv", "response": "r Returns a common standard intensity range over a number of images."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking whether the image from which the supplied landmarks were extracted is transformed to learned standard intensity space without loss of the image.", "response": "def __check_mapping(self, landmarks):\n        \"\"\"\n        Checks whether the image, from which the supplied landmarks were extracted, can\n        be transformed to the learned standard intensity space without loss of\n        information.\n        \"\"\"\n        sc_udiff = numpy.asarray(self.__sc_umaxs)[1:] - numpy.asarray(self.__sc_umins)[:-1]\n        l_diff = numpy.asarray(landmarks)[1:] - numpy.asarray(landmarks)[:-1]\n        return numpy.all(sc_udiff > numpy.asarray(l_diff))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether a number n is inside the interval l and r.", "response": "def is_in_interval(n, l, r, border = 'included'):\n        \"\"\"\n        Checks whether a number is inside the interval l, r.\n        \"\"\"\n        if 'included' == border:\n            return (n >= l) and (n <= r)\n        elif 'excluded' == border:\n            return (n > l) and (n < r)\n        else:\n            raise ValueError('borders must be either \\'included\\' or \\'excluded\\'')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef are_in_interval(s, l, r, border = 'included'):\n        return numpy.all([IntensityRangeStandardization.is_in_interval(x, l, r, border) for x in s])", "response": "Checks whether all number in sequence s lie inside the interval formed by\n        l and r."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef linear_model(x, y):\n        x1, x2 = x\n        y1, y2 = y\n        m = (y2 - y1) / (x2 - x1)\n        b = y1 - (m * x1)\n        return lambda x: m * x + b", "response": "Returns a linear model transformation function fitted on the two supplied points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ght_alternative (img, template, indices):\n    # cast template to bool and img to numpy array\n    img = numpy.asarray(img)\n    template = numpy.asarray(template).astype(numpy.bool)\n    \n    # check supplied parameters\n    if img.ndim != template.ndim:\n        raise AttributeError('The supplied image and template must be of the same dimensionality.')\n    if not numpy.all(numpy.greater_equal(img.shape, template.shape)):\n        raise AttributeError('The supplied template is bigger than the image. This setting makes no sense for a hough transform.')\n    \n    # pad the original image\n    img_padded = pad(img, footprint=template, mode='constant')\n    \n    # prepare the hough image\n    if numpy.bool == img.dtype:\n        img_hough = numpy.zeros(img.shape, numpy.int32)\n    else:\n        img_hough = numpy.zeros(img.shape, img.dtype)\n        \n    # iterate over the pixels, apply the template center to each of these and save the sum into the hough image\n    for idx_hough in indices:\n        idx_hough = tuple(idx_hough)\n        slices_img_padded = [slice(idx_hough[i], None) for i in range(img_hough.ndim)]\n        img_hough[idx_hough] = sum(img_padded[slices_img_padded][template])     \n        \n    return img_hough", "response": "This function returns a new image with the same shape and template as the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef template_sphere (radius, dimensions):\n    if int(dimensions) != dimensions:\n        raise TypeError('The supplied dimension parameter must be of type integer.')\n    dimensions = int(dimensions)\n    \n    return template_ellipsoid(dimensions * [radius * 2])", "response": "r Returns a spherical binary structure of a of the supplied radius and the supplied dimension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the supplied graph into a dimacs file.", "response": "def graph_to_dimacs(g, f):\n    \"\"\"\n    Persists the supplied graph in valid dimacs format into the file.\n    \n    Parameters\n    ----------\n    g : `~medpy.graphcut.graph.Graph`\n        A graph object to persist.\n    f : file\n        A file-like object.\n    \"\"\"\n    # write comments\n    f.write('c Created by medpy\\n')\n    f.write('c Oskar Maier, oskar.maier@googlemail.com\\n')\n    f.write('c\\n')\n    \n    # write problem\n    f.write('c problem line\\n')\n    f.write('p max {} {}\\n'.format(g.get_node_count() + 2, len(g.get_edges()))) # +2 as terminal nodes also count in dimacs format # no-nodes / no-edges\n    \n    # denote source and sink\n    f.write('c source descriptor\\n')\n    f.write('n 1 s\\n')\n    f.write('c sink descriptor\\n')\n    f.write('n 2 t\\n')\n    \n    # write terminal arcs (t-weights)\n    f.write('c terminal arcs (t-weights)\\n')\n    for node, weight in list(g.get_tweights().items()):\n        # Note: the nodes ids of the graph start from 1, but 1 and 2 are reserved for source and sink respectively, therefore add 2\n        if not 0 == weight[0]: # 0 weights are implicit\n            f.write('a 1 {} {}\\n'.format(node + 2, weight[0]))\n        if not 0 == weight[1]: # 0 weights are implicit\n            f.write('a {} 2 {}\\n'.format(node + 2, weight[1]))\n    \n    # write inter-node arcs (n-weights)\n    f.write('c inter-node arcs (n-weights)\\n')\n    for edge, weight in list(g.get_nweights().items()):\n        if not 0 == weight[0]: # 0 weights are implicit\n            f.write('a {} {} {}\\n'.format(edge[0] + 2, edge[1] + 2, weight[0]))\n        # reversed weights have to follow directly in the next line\n        if not 0 == weight[1]: # 0 weights are implicit\n            f.write('a {} {} {}\\n'.format(edge[1] + 2, edge[0] + 2, weight[1]))\n            \n    # end comment\n    f.write('c end-of-file')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the object has a class or superclass with the given class name.", "response": "def looks_like_issubclass(obj, classname):\n    \"\"\" Return True if the object has a class or superclass with the given class\n    name.\n\n    Ignores old-style classes.\n    \"\"\"\n    t = obj\n    if t.__name__ == classname:\n        return True\n    for klass in t.__mro__:\n        if klass.__name__ == classname:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the k - nearest neighbours for each observation.", "response": "def mkneighbors_graph(observations, n_neighbours, metric, mode='connectivity', metric_params = None):\n    \"\"\"\n    Computes the (weighted) graph of mutual k-Neighbors for observations.\n    \n    Notes\n    -----\n    The distance between an observation and itself is never computed and instead set to\n    ``numpy.inf``. I.e. only in the case of k>=n_observations or when the ``metric``\n    returns ``numpy.inf``, the returned graph can contain loops.\n    \n    Parameters\n    ----------\n    observations : sequence\n        Sequence of observations.\n    n_neighbours : int\n        Maximum number of neighbours for each sample.\n    metric : function\n        The distance metric taking two observations and returning a numeric value > 0.\n    mode : {'connectivity', 'distance', 'both'}, optional\n        Type of returned matrix: 'connectivity' will return the connectivity matrix with\n        ones and zeros, in 'distance' the edges are distances between points, while\n        'both' returns a (connectivity, distance) tuple.\n    metric_params : dict, optional  (default = None)\n            Additional keyword arguments for the metric function.\n            \n    Returns\n    -------\n    mkneighbors_graph : ndarray\n        Sparse matrix in CSR format, shape = [n_observations, n_observations].\n        mkneighbors_graph[i, j] is assigned the weight of edge that connects i to j.\n        Might contain ``numpy.inf`` values.\n\n    \"\"\"\n    # compute their pairwise-distances\n    pdists = pdist(observations, metric)\n\n    # get the k nearest neighbours for each patch \n    k_nearest_nbhs = numpy.argsort(pdists)[:,:n_neighbours]\n    \n    # create a mask denoting the k nearest neighbours in image_pdist\n    k_nearest_mutual_nbhs_mask = numpy.zeros(pdists.shape, numpy.bool)\n    for _mask_row, _nbhs_row in zip(k_nearest_mutual_nbhs_mask, k_nearest_nbhs):\n        _mask_row[_nbhs_row] = True\n        \n    # and with transposed to remove non-mutual nearest neighbours\n    k_nearest_mutual_nbhs_mask &= k_nearest_mutual_nbhs_mask.T\n    \n    # set distance not in the mutual k nearest neighbour set to zero\n    pdists[~k_nearest_mutual_nbhs_mask] = 0\n    \n    # check for edges with zero-weight\n    if numpy.any(pdists[k_nearest_mutual_nbhs_mask] == 0):\n        warnings.warn('The graph contains at least one edge with a weight of \"0\".')\n        \n    if 'connectivity' == mode:\n        return csr_matrix(k_nearest_mutual_nbhs_mask)\n    elif 'distance' == mode:\n        return csr_matrix(pdists)\n    else:\n        return csr_matrix(k_nearest_mutual_nbhs_mask), csr_matrix(pdists)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pdist(objects, dmeasure, diagval = numpy.inf):\n    out = numpy.zeros([len(objects)] * 2, numpy.float)\n    numpy.fill_diagonal(out, diagval)\n    for idx1, idx2 in combinations(list(range(len(objects))), 2):\n        out[idx1, idx2] = dmeasure(objects[idx1], objects[idx2])\n    return out + out.T", "response": "Compute the pair - wise distances between arbitrary objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noverriding the parent method to adapt the formatting string to the level.", "response": "def setLevel(self, level):\n        r\"\"\"Overrides the parent method to adapt the formatting string to the level.\n        \n        Parameters\n        ----------\n        level : int\n            The new log level to set. See the logging levels in the logging module for details.\n            \n        Examples\n        --------\n        >>> import logging\n        >>> Logger.setLevel(logging.DEBUG)\n        \"\"\"\n        if logging.DEBUG >= level:\n            formatter = logging.Formatter(\"%(asctime)s [%(levelname)-8s] %(message)s (in %(module)s.%(funcName)s:%(lineno)s)\", \n                                          \"%d.%m.%Y %H:%M:%S\") \n            self._handler.setFormatter(formatter)\n        else:\n            formatter = logging.Formatter(\"%(asctime)s [%(levelname)-8s] %(message)s\", \n                                          \"%d.%m.%Y %H:%M:%S\") \n            self._handler.setFormatter(formatter)\n            \n        NativeLogger.setLevel(self, level)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef xminus1d(img, fun, dim, *args, **kwargs):\n    slicer = [slice(None)] * img.ndim\n    output = []\n    for slid in range(img.shape[dim]):\n        slicer[dim] = slice(slid, slid + 1)\n        output.append(fun(numpy.squeeze(img[slicer]), *args, **kwargs))\n    return numpy.rollaxis(numpy.asarray(output), 0, dim + 1)", "response": "r Applies the function fun along all X - 1D dimensional volumes of the images img and dimension dim."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef intersection(i1, h1, i2, h2):\n        \n        # compute image bounding boxes in real-world coordinates\n        os1 = numpy.asarray(header.get_offset(h1))\n        ps1 = numpy.asarray(header.get_pixel_spacing(h1))\n        bb1 = (os1, numpy.asarray(i1.shape) * ps1 + os1)\n        \n        \n        os2 = numpy.asarray(header.get_offset(h2))\n        ps2 = numpy.asarray(header.get_pixel_spacing(h2))\n        bb2 = (os2, numpy.asarray(i2.shape) * ps2 + os2)\n        \n        # compute intersection\n        ib = (numpy.maximum(bb1[0], bb2[0]), numpy.minimum(bb1[1], bb2[1]))\n        \n        # transfer intersection to respective image coordinates image\n        ib1 = [ ((ib[0] - os1) / numpy.asarray(ps1)).astype(numpy.int), ((ib[1] - os1) / numpy.asarray(ps1)).astype(numpy.int) ]\n        ib2 = [ ((ib[0] - os2) / numpy.asarray(ps2)).astype(numpy.int), ((ib[1] - os2) / numpy.asarray(ps2)).astype(numpy.int) ]\n        \n        # ensure that both sub-volumes are of same size (might be affected by rounding errors); only reduction allowed\n        s1 = ib1[1] - ib1[0]\n        s2 = ib2[1] - ib2[0]\n        d1 = s1 - s2\n        d1[d1 > 0] = 0\n        d2 = s2 - s1\n        d2[d2 > 0] = 0\n        ib1[1] -= d1\n        ib2[1] -= d2\n        \n        # compute new image offsets (in real-world coordinates); averaged to account for rounding errors due to world-to-voxel mapping\n        nos1 = ib1[0] * ps1 + os1 # real offset for image 1\n        nos2 = ib2[0] * ps2 + os2 # real offset for image 2\n        nos = numpy.average([nos1, nos2], 0)\n        \n        # build slice lists\n        sl1 = [slice(l, u) for l, u in zip(*ib1)]\n        sl2 = [slice(l, u) for l, u in zip(*ib2)]\n        \n        return i1[sl1], i2[sl2], nos", "response": "r Returns the intersection of two images in real - world coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __make_footprint(input, size, footprint):\n    \"Creates a standard footprint element ala scipy.ndimage.\"\n    if footprint is None:\n        if size is None:\n            raise RuntimeError(\"no footprint or filter size provided\")\n        sizes = _ni_support._normalize_sequence(size, input.ndim)\n        footprint = numpy.ones(sizes, dtype=bool)\n    else:\n        footprint = numpy.asarray(footprint, dtype=bool)\n    return footprint", "response": "Creates a standard footprint element ala scipy. ndimage."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef boundary_difference_of_means(graph, label_image, original_image): # label image is not required to hold continuous ids or to start from 1\n    # convert to arrays if necessary\n    label_image = scipy.asarray(label_image)\n    original_image = scipy.asarray(original_image)\n    \n    if label_image.flags['F_CONTIGUOUS']: # strangely one this one is required to be ctype ordering\n        label_image = scipy.ascontiguousarray(label_image)\n        \n    __check_label_image(label_image)\n    \n    # create a lookup-table that translates from a label id to its position in the sorted unique vector\n    labels_unique = scipy.unique(label_image)\n    \n    # compute the mean intensities of all regions\n    # Note: Bug in mean implementation: means over labels is only computed if the indexes are also supplied\n    means = scipy.ndimage.measurements.mean(original_image, labels=label_image, index=labels_unique)\n    \n    # compute the maximum possible intensity difference\n    max_difference = float(abs(min(means) - max(means)))\n\n    # create a lookup table that relates region ids to their respective intensity values\n    means = dict(list(zip(labels_unique, means)))\n\n    # get the adjuncancy of the labels\n    edges = __compute_edges(label_image)\n    \n    # compute the difference of means for each adjunct region and add it as a tuple to the dictionary\n    if 0. == max_difference: # special case when the divider is zero and therefore all values can be assured to equal zero\n        for edge in edges:\n            graph.set_nweight(edge[0] - 1, edge[1] - 1, sys.float_info.min, sys.float_info.min)\n    else:    \n        # compute the difference of means for each adjunct region and add it as a tuple to the dictionary\n        for edge in edges:\n            value = max(1. - abs(means[edge[0]] - means[edge[1]]) / max_difference, sys.float_info.min)\n            graph.set_nweight(edge[0] - 1, edge[1] - 1, value, value)", "response": "r Returns a graph that computes the difference of means between adjacent image regions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef boundary_stawiaski(graph, label_image, gradient_image): # label image is not required to hold continuous ids or to start from 1\n    # convert to arrays if necessary\n    label_image = scipy.asarray(label_image)\n    gradient_image = scipy.asarray(gradient_image)\n    \n    if label_image.flags['F_CONTIGUOUS']: # strangely, this one is required to be ctype ordering\n        label_image = scipy.ascontiguousarray(label_image)\n        \n    __check_label_image(label_image)\n        \n    for dim in range(label_image.ndim):\n        # prepare slicer for all minus last and all minus first \"row\"\n        slicer_from = [slice(None)] * label_image.ndim\n        slicer_to = [slice(None)] * label_image.ndim\n        slicer_from[dim] = slice(None, -1)\n        slicer_to[dim] = slice(1, None)\n        # slice views of keys\n        keys_from = label_image[slicer_from]\n        keys_to = label_image[slicer_to]\n        # determine not equal keys\n        valid_edges = keys_from != keys_to\n        # determine largest gradient\n        gradient_max = numpy.maximum(numpy.abs(gradient_image[slicer_from]), numpy.abs(gradient_image[slicer_to]))[valid_edges]\n        # determine key order\n        keys_max = numpy.maximum(keys_from, keys_to)[valid_edges]\n        keys_min = numpy.minimum(keys_from, keys_to)[valid_edges]\n        # set edges / nweights\n        for k1, k2, val in zip(keys_min, keys_max, gradient_max):\n            weight = math.pow(1./(1. + val), 2) # weight contribution of a single pixel\n            weight = max(weight, sys.float_info.min)\n            graph.set_nweight(k1 - 1 , k2 - 1, weight, weight)", "response": "r Calculates the stawiaski boundary term for a given graph and label image."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the edge neighbourhoods of the n - dimensional structure.", "response": "def __compute_edges_nd(label_image):\n    \"\"\"\n    Computes the region neighbourhood defined by a star shaped n-dimensional structuring\n    element (as returned by scipy.ndimage.generate_binary_structure(ndim, 1)) for the\n    supplied region/label image.\n    Note The returned set contains neither duplicates, nor self-references\n    (i.e. (id_1, id_1)), nor reversed references (e.g. (id_1, id_2) and (id_2, id_1).\n    \n    @param label_image An image with labeled regions (nD).\n    @param return A set with tuples denoting the edge neighbourhood.\n    \"\"\"\n    Er = set()\n   \n    def append(v1, v2):\n        if v1 != v2:\n            Er.update([(min(v1, v2), max(v1, v2))])\n        \n    vappend = scipy.vectorize(append)\n   \n    for dim in range(label_image.ndim):\n        slices_x = []\n        slices_y = []\n        for di in range(label_image.ndim):\n            slices_x.append(slice(None, -1 if di == dim else None))\n            slices_y.append(slice(1 if di == dim else None, None))\n        vappend(label_image[slices_x], label_image[slices_y])\n        \n    return Er"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the label image for consistent labelling starting from 1.", "response": "def __check_label_image(label_image):\n    \"\"\"Check the label image for consistent labelling starting from 1.\"\"\"\n    encountered_indices = scipy.unique(label_image)\n    expected_indices = scipy.arange(1, label_image.max() + 1)\n    if not encountered_indices.size == expected_indices.size or \\\n       not (encountered_indices == expected_indices).all():\n        raise AttributeError('The supplied label image does either not contain any regions or they are not labeled consecutively starting from 1.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and returns the argparse parser object.", "response": "def getParser():\n    \"Creates and returns the argparse parser object.\"\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,\n                                     description=__description__)\n    parser.add_argument('output', help='Target volume.')\n    parser.add_argument('inputs', nargs='+', help='Source volume(s).')\n    parser.add_argument('-o', '--operation', dest='operation', choices=['sum', 'avg', 'max', 'min'], default='avg', help='Combinatorial operation to conduct.')\n    parser.add_argument('-v', dest='verbose', action='store_true', help='Display more information.')\n    parser.add_argument('-d', dest='debug', action='store_true', help='Display debug information.')\n    parser.add_argument('-f', dest='force', action='store_true', help='Silently override existing output images.')\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _str_member_list(self, name):\n        out = []\n        if self[name]:\n            out += ['.. rubric:: %s' % name, '']\n            prefix = getattr(self, '_name', '')\n\n            if prefix:\n                prefix = '~%s.' % prefix\n\n            autosum = []\n            others = []\n            for param, param_type, desc in self[name]:\n                param = param.strip()\n\n                # Check if the referenced member can have a docstring or not\n                param_obj = getattr(self._obj, param, None)\n                if not (isinstance(param_obj, collections.Callable)\n                        or isinstance(param_obj, property)\n                        or inspect.isgetsetdescriptor(param_obj)):\n                    param_obj = None\n\n                if param_obj and (pydoc.getdoc(param_obj) or not desc):\n                    # Referenced object has a docstring\n                    autosum += [\"   %s%s\" % (prefix, param)]\n                else:\n                    others.append((param, param_type, desc))\n\n            if autosum:\n                out += ['.. autosummary::']\n                if self.class_members_toctree:\n                    out += ['   :toctree:']\n                out += [''] + autosum\n\n            if others:\n                maxlen_0 = max(3, max([len(x[0]) for x in others]))\n                hdr = sixu(\"=\")*maxlen_0 + sixu(\"  \") + sixu(\"=\")*10\n                fmt = sixu('%%%ds  %%s  ') % (maxlen_0,)\n                out += ['', hdr]\n                for param, param_type, desc in others:\n                    desc = sixu(\" \").join(x.strip() for x in desc).strip()\n                    if param_type:\n                        desc = \"(%s) %s\" % (param_type, desc)\n                    out += [fmt % (param.strip(), desc)]\n                out += [hdr]\n            out += ['']\n        return out", "response": "Generate a list of members of the class where possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef relabel(label_image, start = 1):\n    label_image = scipy.asarray(label_image)\n    mapping = {}\n    rav = label_image.ravel()\n    for i in range(len(rav)):\n        if not rav[i] in mapping:\n            mapping[rav[i]] = start\n            start += 1\n        rav[i] = mapping[rav[i]]\n    return rav.reshape(label_image.shape)", "response": "r Relabels the regions of a label image."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlikes xd_iterator but the fun return values are always passed on to the next and only the last returned.", "response": "def __xd_iterator_pass_on(arr, view, fun):\n    \"\"\"\n    Like xd_iterator, but the fun return values are always passed on to the next and only the last returned.\n    \"\"\"\n    # create list of iterations\n    iterations = [[None] if dim in view else list(range(arr.shape[dim])) for dim in range(arr.ndim)]\n     \n    # iterate, create slicer, execute function and collect results\n    passon = None\n    for indices in itertools.product(*iterations):\n        slicer = [slice(None) if idx is None else slice(idx, idx + 1) for idx in indices]\n        passon = fun(scipy.squeeze(arr[slicer]), passon)\n        \n    return passon"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the bounding box of an binary objects hole (assuming only one in existence.", "response": "def __extract_bbox(arr, bb_old):\n    \"Extracts the bounding box of an binary objects hole (assuming only one in existence).\"\n    hole = ndimage.binary_fill_holes(arr)- arr\n    bb_list = ndimage.find_objects(ndimage.binary_dilation(hole, iterations = 1))\n    if 0 == len(bb_list): return bb_old\n    else: bb = bb_list[0]\n    \n    if not bb_old: return list(bb)\n    \n    for i in range(len(bb_old)):\n        bb_old[i] = slice(min(bb_old[i].start, bb[i].start),\n                          max(bb_old[i].stop, bb[i].stop))\n    return bb_old"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_sink_nodes(self, sink_nodes):\n        if max(sink_nodes) >= self.__nodes or min(sink_nodes) < 0:\n            raise ValueError('Invalid node id of {} or {}. Valid values are 0 to {}.'.format(max(sink_nodes), min(sink_nodes), self.__nodes - 1))\n        # set the node-to-sink weights (t-weights)\n        for snode in sink_nodes:\n            self.__graph.add_tweights(int(snode), 0, self.MAX)", "response": "r Set multiple sink nodes and compute their t - weights."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_nweight(self, node_from, node_to, weight_there, weight_back):\n        if node_from >= self.__nodes or node_from < 0:\n            raise ValueError('Invalid node id (node_from) of {}. Valid values are 0 to {}.'.format(node_from, self.__nodes - 1))\n        elif node_to >= self.__nodes or node_to < 0:\n            raise ValueError('Invalid node id (node_to) of {}. Valid values are 0 to {}.'.format(node_to, self.__nodes - 1))\n        elif node_from == node_to:\n            raise ValueError('The node_from ({}) can not be equal to the node_to ({}) (self-connections are forbidden in graph cuts).'.format(node_from, node_to))\n        elif weight_there <= 0 or weight_back <= 0:\n            raise ValueError('Negative or zero weights are not allowed.')\n        self.__graph.sum_edge(int(node_from), int(node_to), float(weight_there), float(weight_back))", "response": "r This method sets a single n - weight edge - weight."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_tweights(self, tweights):\n        for node, weight in list(tweights.items()):\n            self.set_tweight(node, weight[0], weight[1])", "response": "r Sets multiple t - weights to the current collection of t - weights overwriting already existing ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_tweights_all(self, tweights):\n        for node, (twsource, twsink) in enumerate(tweights):\n            self.set_tweight(node, twsource, twsink)", "response": "r Set all t - weights at once."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getArguments(parser):\n    \"Provides additional validation of the arguments collected by argparse.\"\n    args = parser.parse_args()\n    if args.width <= 0:\n        raise argparse.ArgumentError(args.width, 'The contour width must be a positive number.')\n    return args", "response": "Provides additional validation of the arguments collected by argparse."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndepreciating synonym of ~medpy. io. header. set_voxel_spacing.", "response": "def set_pixel_spacing(hdr, spacing):\n    r\"\"\"Depreciated synonym of `~medpy.io.header.set_voxel_spacing`.\"\"\"\n    warnings.warn('get_pixel_spacing() is depreciated, use set_voxel_spacing() instead', category=DeprecationWarning)\n    set_voxel_spacing(hdr, spacing)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy_meta_data(hdr_to, hdr_from):\n    warnings.warn('copy_meta_data() is depreciated and may be removed in future versions', category=DeprecationWarning)\n    logger = Logger.getInstance()\n    try:\n        set_pixel_spacing(hdr_to, get_pixel_spacing(hdr_from))\n    except AttributeError as e:\n        logger.warning('The voxel spacing could not be set correctly. Signaled error: {}'.format(e))\n    try:\n        set_offset(hdr_to, get_offset(hdr_from))\n    except AttributeError as e:\n        logger.warning('The image offset could not be set correctly. Signaled error: {}'.format(e))", "response": "r Copy image meta data from one header to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies all stored meta information info to an sitk Image.", "response": "def copy_to(self, sitkimage):\n        \"\"\"\n        Copy all stored meta information info to an sitk Image.\n\n        Note that only the spacing and the offset/origin information\n        are guaranteed to be preserved, although the method also\n        tries to copy other meta information such as DICOM tags.\n\n        Parameters\n        ----------\n        sitkimage : sitk.Image\n            the sitk Image object to which to copy the information\n\n        Returns\n        -------\n        sitkimage : sitk.Image\n            the passed sitk Image object\n        \"\"\"\n        if self.sitkimage is not None:\n            for k in self.sitkimage.GetMetaDataKeys():\n                sitkimage.SetMetaData(k, self.sitkimage.GetMetaData(k))\n\n        ndim = len(sitkimage.GetSize())\n        spacing, offset, direction = self.get_info_consistent(ndim)\n            \n        sitkimage.SetSpacing(spacing)\n        sitkimage.SetOrigin(offset)\n        sitkimage.SetDirection(tuple(direction.flatten()))\n        \n        return sitkimage"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the main meta - data information adapted to the supplied image dimensionality.", "response": "def get_info_consistent(self, ndim):\n        \"\"\"\n        Returns the main meta-data information adapted to the supplied\n        image dimensionality.\n\n        It will try to resolve inconsistencies and other conflicts,\n        altering the information avilable int he most plausible way.\n\n        Parameters\n        ----------\n        ndim : int\n            image's dimensionality\n        \n        Returns\n        -------\n        spacing : tuple of floats\n        offset : tuple of floats\n        direction : ndarray\n        \"\"\"\n        if ndim > len(self.spacing):\n            spacing = self.spacing + (1.0, ) * (ndim - len(self.spacing))\n        else:\n            spacing = self.spacing[:ndim]\n\n        if ndim > len(self.offset):\n            offset = self.offset + (0.0, ) * (ndim - len(self.offset))\n        else:\n            offset = self.offset[:ndim]\n\n        if ndim > self.direction.shape[0]:\n            direction = np.identity(ndim)\n            direction[:self.direction.shape[0], :self.direction.shape[0]] = self.direction\n        else:\n            direction = self.direction[:ndim, :ndim]\n        \n        return spacing, offset, direction"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dc(result, reference):\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n    \n    intersection = numpy.count_nonzero(result & reference)\n    \n    size_i1 = numpy.count_nonzero(result)\n    size_i2 = numpy.count_nonzero(reference)\n    \n    try:\n        dc = 2. * intersection / float(size_i1 + size_i2)\n    except ZeroDivisionError:\n        dc = 0.0\n    \n    return dc", "response": "r Computes the Dice coefficient between two binary objects in two images."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the Jaccard coefficient between two binary objects in one image.", "response": "def jc(result, reference):\n    \"\"\"\n    Jaccard coefficient\n    \n    Computes the Jaccard coefficient between the binary objects in two images.\n    \n    Parameters\n    ----------\n    result: array_like\n            Input data containing objects. Can be any type but will be converted\n            into binary: background where 0, object everywhere else.\n    reference: array_like\n            Input data containing objects. Can be any type but will be converted\n            into binary: background where 0, object everywhere else.\n\n    Returns\n    -------\n    jc: float\n        The Jaccard coefficient between the object(s) in `result` and the\n        object(s) in `reference`. It ranges from 0 (no overlap) to 1 (perfect overlap).\n    \n    Notes\n    -----\n    This is a real metric. The binary images can therefore be supplied in any order.\n    \"\"\"\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n    \n    intersection = numpy.count_nonzero(result & reference)\n    union = numpy.count_nonzero(result | reference)\n    \n    jc = float(intersection) / float(union)\n    \n    return jc"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef precision(result, reference):\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n        \n    tp = numpy.count_nonzero(result & reference)\n    fp = numpy.count_nonzero(result & ~reference)\n    \n    try:\n        precision = tp / float(tp + fp)\n    except ZeroDivisionError:\n        precision = 0.0\n    \n    return precision", "response": "Calculates the precision between two binary datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recall(result, reference):\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n        \n    tp = numpy.count_nonzero(result & reference)\n    fn = numpy.count_nonzero(~result & reference)\n\n    try:\n        recall = tp / float(tp + fn)\n    except ZeroDivisionError:\n        recall = 0.0\n    \n    return recall", "response": "Returns the recall between two binary datasets."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef specificity(result, reference):\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n       \n    tn = numpy.count_nonzero(~result & ~reference)\n    fp = numpy.count_nonzero(result & ~reference)\n\n    try:\n        specificity = tn / float(tn + fp)\n    except ZeroDivisionError:\n        specificity = 0.0\n    \n    return specificity", "response": "Returns the specificity between two binary datasets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hd(result, reference, voxelspacing=None, connectivity=1):\n    hd1 = __surface_distances(result, reference, voxelspacing, connectivity).max()\n    hd2 = __surface_distances(reference, result, voxelspacing, connectivity).max()\n    hd = max(hd1, hd2)\n    return hd", "response": "Computes the Hausdorff distance between two binary objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hd95(result, reference, voxelspacing=None, connectivity=1):\n    hd1 = __surface_distances(result, reference, voxelspacing, connectivity)\n    hd2 = __surface_distances(reference, result, voxelspacing, connectivity)\n    hd95 = numpy.percentile(numpy.hstack((hd1, hd2)), 95)\n    return hd95", "response": "Calculates the 95th percentile of the Hausdorff Distance between two binary objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assd(result, reference, voxelspacing=None, connectivity=1):\n    assd = numpy.mean( (asd(result, reference, voxelspacing, connectivity), asd(reference, result, voxelspacing, connectivity)) )\n    return assd", "response": "Calculates the average symmetric surface distance between two binary objects in the result and reference."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef asd(result, reference, voxelspacing=None, connectivity=1):\n    sds = __surface_distances(result, reference, voxelspacing, connectivity)\n    asd = sds.mean()\n    return asd", "response": "Calculates the average surface distance between two binary objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ravd(result, reference):\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n        \n    vol1 = numpy.count_nonzero(result)\n    vol2 = numpy.count_nonzero(reference)\n    \n    if 0 == vol2:\n        raise RuntimeError('The second supplied array does not contain any binary object.')\n    \n    return (vol1 - vol2) / float(vol2)", "response": "Compute relative absolute volume difference between two binary objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef volume_change_correlation(results, references):\n    results = numpy.atleast_2d(numpy.array(results).astype(numpy.bool))\n    references = numpy.atleast_2d(numpy.array(references).astype(numpy.bool))\n    \n    results_volumes = numpy.asarray([numpy.count_nonzero(r) for r in results])\n    references_volumes = numpy.asarray([numpy.count_nonzero(r) for r in references])\n    \n    results_volumes_changes = results_volumes[1:] - results_volumes[:-1]\n    references_volumes_changes = references_volumes[1:] - references_volumes[:-1] \n    \n    return pearsonr(results_volumes_changes, references_volumes_changes)", "response": "r Computes the linear correlation of change in binary object volume between two binary images supplied."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef obj_assd(result, reference, voxelspacing=None, connectivity=1):\n    assd = numpy.mean( (obj_asd(result, reference, voxelspacing, connectivity), obj_asd(reference, result, voxelspacing, connectivity)) )\n    return assd", "response": "This function calculates the average symmetric surface distance between two binary objects in the result and reference."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef obj_asd(result, reference, voxelspacing=None, connectivity=1):\n    sds = list()\n    labelmap1, labelmap2, _a, _b, mapping = __distinct_binary_object_correspondences(result, reference, connectivity)\n    slicers1 = find_objects(labelmap1)\n    slicers2 = find_objects(labelmap2)\n    for lid2, lid1 in list(mapping.items()):\n        window = __combine_windows(slicers1[lid1 - 1], slicers2[lid2 - 1])\n        object1 = labelmap1[window] == lid1\n        object2 = labelmap2[window] == lid2\n        sds.extend(__surface_distances(object1, object2, voxelspacing, connectivity))\n    asd = numpy.mean(sds)\n    return asd", "response": "This function calculates the average surface distance between all distinct binary objects in result and reference."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __distinct_binary_object_correspondences(reference, result, connectivity=1):\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n    \n    # binary structure\n    footprint = generate_binary_structure(result.ndim, connectivity)\n    \n    # label distinct binary objects\n    labelmap1, n_obj_result = label(result, footprint)\n    labelmap2, n_obj_reference = label(reference, footprint)\n    \n    # find all overlaps from labelmap2 to labelmap1; collect one-to-one relationships and store all one-two-many for later processing\n    slicers = find_objects(labelmap2) # get windows of labelled objects\n    mapping = dict() # mappings from labels in labelmap2 to corresponding object labels in labelmap1\n    used_labels = set() # set to collect all already used labels from labelmap2\n    one_to_many = list() # list to collect all one-to-many mappings\n    for l1id, slicer in enumerate(slicers): # iterate over object in labelmap2 and their windows\n        l1id += 1 # labelled objects have ids sarting from 1\n        bobj = (l1id) == labelmap2[slicer] # find binary object corresponding to the label1 id in the segmentation\n        l2ids = numpy.unique(labelmap1[slicer][bobj]) # extract all unique object identifiers at the corresponding positions in the reference (i.e. the mapping)\n        l2ids = l2ids[0 != l2ids] # remove background identifiers (=0)\n        if 1 == len(l2ids): # one-to-one mapping: if target label not already used, add to final list of object-to-object mappings and mark target label as used\n            l2id = l2ids[0]\n            if not l2id in used_labels:\n                mapping[l1id] = l2id\n                used_labels.add(l2id)\n        elif 1 < len(l2ids): # one-to-many mapping: store relationship for later processing\n            one_to_many.append((l1id, set(l2ids)))\n            \n    # process one-to-many mappings, always choosing the one with the least labelmap2 correspondences first\n    while True:\n        one_to_many = [(l1id, l2ids - used_labels) for l1id, l2ids in one_to_many] # remove already used ids from all sets\n        one_to_many = [x for x in one_to_many if x[1]] # remove empty sets\n        one_to_many = sorted(one_to_many, key=lambda x: len(x[1])) # sort by set length\n        if 0 == len(one_to_many):\n            break\n        l2id = one_to_many[0][1].pop() # select an arbitrary target label id from the shortest set\n        mapping[one_to_many[0][0]] = l2id # add to one-to-one mappings \n        used_labels.add(l2id) # mark target label as used\n        one_to_many = one_to_many[1:] # delete the processed set from all sets\n    \n    return labelmap1, labelmap2, n_obj_result, n_obj_reference, mapping", "response": "This function returns a 1 - to - 1 mapping from the labelled objects in result to the labelled objects in reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the surface distances between two binary objects in result and their nearest partner surface voxel of a binary object in reference.", "response": "def __surface_distances(result, reference, voxelspacing=None, connectivity=1):\n    \"\"\"\n    The distances between the surface voxel of binary objects in result and their\n    nearest partner surface voxel of a binary object in reference.\n    \"\"\"\n    result = numpy.atleast_1d(result.astype(numpy.bool))\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\n    if voxelspacing is not None:\n        voxelspacing = _ni_support._normalize_sequence(voxelspacing, result.ndim)\n        voxelspacing = numpy.asarray(voxelspacing, dtype=numpy.float64)\n        if not voxelspacing.flags.contiguous:\n            voxelspacing = voxelspacing.copy()\n            \n    # binary structure\n    footprint = generate_binary_structure(result.ndim, connectivity)\n    \n    # test for emptiness\n    if 0 == numpy.count_nonzero(result): \n        raise RuntimeError('The first supplied array does not contain any binary object.')\n    if 0 == numpy.count_nonzero(reference): \n        raise RuntimeError('The second supplied array does not contain any binary object.')    \n            \n    # extract only 1-pixel border line of objects\n    result_border = result ^ binary_erosion(result, structure=footprint, iterations=1)\n    reference_border = reference ^ binary_erosion(reference, structure=footprint, iterations=1)\n    \n    # compute average surface distance        \n    # Note: scipys distance transform is calculated only inside the borders of the\n    #       foreground objects, therefore the input has to be reversed\n    dt = distance_transform_edt(~reference_border, sampling=voxelspacing)\n    sds = dt[result_border]\n    \n    return sds"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncombine two windows into a single window.", "response": "def __combine_windows(w1, w2):\n    \"\"\"\n    Joins two windows (defined by tuple of slices) such that their maximum\n    combined extend is covered by the new returned window.\n    \"\"\"\n    res = []\n    for s1, s2 in zip(w1, w2):\n        res.append(slice(min(s1.start, s2.start), max(s1.stop, s2.stop)))\n    return tuple(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef applyslicer(array, slicer, pmask, cval = 0):\n        l = len(slicer)\n        patch = numpy.zeros(list(pmask.shape[:l]) + list(array.shape[l:]), array.dtype)\n        if not 0 == cval: patch.fill(cval)\n        sliced = array[slicer]\n        patch[pmask] = sliced.reshape([numpy.prod(sliced.shape[:l])] + list(sliced.shape[l:]))\n        return patch", "response": "r Apply a slicer to an array of the same dimensionality as the iterator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd `nslices` slices between all slices of the binary image `img` along dimension `dim` respecting the original slice values to be situated in the middle of each slice. Extrapolation situations are handled by simple repeating. Interpolation of new slices is performed using shape based interpolation. Parameters ---------- img : array_like A n-dimensional image. dim : int The dimension along which to add slices. nslices : int The number of slices to add. Must be an even number. Returns ------- out : ndarray The re-sampled image.", "response": "def shape_based_slice_interpolation(img, dim, nslices):\n    \"\"\"\n    Adds `nslices` slices between all slices of the binary image `img` along dimension\n    `dim` respecting the original slice values to be situated in the middle of each\n    slice. Extrapolation situations are handled by simple repeating.\n    \n    Interpolation of new slices is performed using shape based interpolation.\n    \n    Parameters\n    ----------\n    img : array_like\n        A n-dimensional image.\n    dim : int\n        The dimension along which to add slices.\n    nslices : int\n        The number of slices to add. Must be an even number.\n    \n    Returns\n    -------\n    out : ndarray\n        The re-sampled image.\n    \"\"\"\n    # check arguments\n    if not 0 == nslices % 2:\n        raise ValueError('nslices must be an even number')\n    \n    out = None\n    slicer = [slice(None)] * img.ndim\n    chunk_full_shape = list(img.shape)\n    chunk_full_shape[dim] = nslices + 2\n     \n    for sl1, sl2 in zip(numpy.rollaxis(img, dim)[:-1], numpy.rollaxis(img, dim)[1:]):\n        if 0 == numpy.count_nonzero(sl1) and 0 == numpy.count_nonzero(sl2):\n            chunk = numpy.zeros(chunk_full_shape, dtype=numpy.bool)\n        else:\n            chunk = shape_based_slice_insertation_object_wise(sl1, sl2, dim, nslices)\n        if out is None:\n            out = numpy.delete(chunk, -1, dim)\n        else:\n            out = numpy.concatenate((out, numpy.delete(chunk, -1, dim)), dim)\n    \n    slicer[dim] = numpy.newaxis    \n    out = numpy.concatenate((out, sl2[slicer]), dim)\n    \n    slicer[dim] = slice(0, 1)\n    for _ in range(nslices // 2):\n        out = numpy.concatenate((img[slicer], out), dim)\n    slicer[dim] = slice(-1, None)\n    for _ in range(nslices // 2):\n        out = numpy.concatenate((out, img[slicer]), dim)\n        \n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping to apply shape_based_slice_insertation for each binary object separately to ensure correct extrapolation behaviour.", "response": "def shape_based_slice_insertation_object_wise(sl1, sl2, dim, nslices, order=3):\n    \"\"\"\n    Wrapper to apply `shape_based_slice_insertation()` for each binary object\n    separately to ensure correct extrapolation behaviour.\n    \"\"\"\n    out = None\n    sandwich = numpy.concatenate((sl1[numpy.newaxis], sl2[numpy.newaxis]), 0)\n    label_image, n_labels = label(sandwich)\n    for lid in range(1, n_labels + 1):\n        _sl1, _sl2 = label_image == lid\n        _out = shape_based_slice_insertation(_sl1, _sl2, dim, nslices, order=3)\n        if out is None:\n            out = _out\n        else:\n            out |= _out\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shape_based_slice_insertation(sl1, sl2, dim, nslices, order=3):\n    sl1 = sl1.astype(numpy.bool)\n    sl2 = sl2.astype(numpy.bool)\n    \n    # extrapolation through erosion\n    if 0 == numpy.count_nonzero(sl1):\n        slices = [sl1]\n        for _ in range(nslices / 2):\n            slices.append(numpy.zeros_like(sl1))\n        for i in range(1, nslices / 2 + nslices % 2 + 1)[::-1]:\n            slices.append(binary_erosion(sl2, iterations=i))\n        slices.append(sl2)\n        return numpy.rollaxis(numpy.asarray(slices), 0, dim + 1)\n        #return numpy.asarray([sl.T for sl in slices]).T\n    elif 0 ==numpy.count_nonzero(sl2):\n        slices = [sl1]\n        for i in range(1, nslices / 2 + 1):\n            slices.append(binary_erosion(sl1, iterations=i))\n        for _ in range(0, nslices / 2 + nslices % 2):\n            slices.append(numpy.zeros_like(sl2))\n        slices.append(sl2)\n        return numpy.rollaxis(numpy.asarray(slices), 0, dim + 1)\n        #return numpy.asarray([sl.T for sl in slices]).T\n    \n    # interpolation shape based \n    # note: distance_transform_edt shows strange behaviour for ones-arrays\n    dt1 = distance_transform_edt(~sl1) - distance_transform_edt(sl1)\n    dt2 = distance_transform_edt(~sl2) - distance_transform_edt(sl2)\n    \n    slicer = [slice(None)] * dt1.ndim\n    slicer = slicer[:dim] + [numpy.newaxis] + slicer[dim:]\n    out = numpy.concatenate((dt1[slicer], dt2[slicer]), axis=dim)\n    zoom_factors = [1] * dt1.ndim\n    zoom_factors = zoom_factors[:dim] + [(nslices + 2)/2.] + zoom_factors[dim:]\n    out = zoom(out, zoom_factors, order=order)\n    \n    return out <= 0", "response": "This function creates a new array of size nslices with the same shape as sl1 and sl2 along the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getArguments(parser):\n    \"Provides additional validation of the arguments collected by argparse.\"\n    args = parser.parse_args()\n    if args.order < 0 or args.order > 5:\n        parser.error('The order has to be a number between 0 and 5.')   \n    return args", "response": "Provides additional validation of the arguments collected by argparse."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide additional validation of the arguments collected by argparse.", "response": "def getArguments(parser):\n    \"Provides additional validation of the arguments collected by argparse.\"\n    args = parser.parse_args()\n    \n    # check mutual exlusive and reaquired arguments\n    if args.lmodel and args.smodel:\n        parser.error('only one of --load-model and --save-model can be supplied, as they decide on whether to apply the application or the training mode')\n    if not args.lmodel and not args.smodel:\n        parser.error('exactly one of --load-model or --save-model has to be supplied')\n    \n    # application mode\n    if args.lmodel:\n        if not os.path.isfile(args.lmodel):\n            parser.error('the supplied model file {} does not exist'.format(args.lmodel))\n        if not args.simages:\n            parser.error('--save-images must be supplied when running the application mode')\n    \n    # training mode\n    if args.smodel:\n        if not args.landmarkp in ('L2', 'L3', 'L4'):\n            args.landmarkp = sequenceOfIntegersGeAscendingStrict(args.landmarkp)\n        if not 'auto' == args.stdspace:\n            args.stdspace = sequenceOfIntegersGeAscendingStrict(args.stdspace)\n        if not args.force and os.path.isfile(args.smodel):\n            parser.error('the target model file {} already exists'.format(args.smodel))\n        \n    # others\n    if args.simages:\n        if not os.path.isdir(args.simages):\n            parser.error('--save-images must be a valid directory')\n    if args.masks and len(args.masks) != len(args.images):\n        parser.error('the same number of masks must be passed to --masks as images have been supplied') \n    \n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getParser():\n    \"Creates and returns the argparse parser object.\"\n    parser = argparse.ArgumentParser(description=__description__, formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('images', nargs='+', help='The images used for training (in the learning case) or to transform (in the transformation case)')\n\n    apply_group = parser.add_argument_group('apply an existing model')\n    apply_group.add_argument('--load-model', dest='lmodel', default=False, help='Location of the pickled intensity range model to load. Activated application mode.')\n    \n    train_group = parser.add_argument_group('train a new model and save and/or apply it')\n    train_group.add_argument('--save-model', dest='smodel', default=False, help='Save the trained model under this name as a pickled object (should end in .pkl). Activates training mode.')\n    train_group.add_argument('--cutoffp', dest='cutoffp', type=sequenceOfIntegersGeAscendingStrict, default='1,99', help='Colon-separated lower and upper cut-off percentile values to exclude intensity outliers during the model training.')\n    train_group.add_argument('--landmarkp', dest='landmarkp', default='L4', help='The landmark percentiles, based on which to train the model. Can be L2, L3, L4 or a colon-separated, ordered list of percentiles.')\n    train_group.add_argument('--stdspace', dest='stdspace', default='auto', help='Two colon-separated intensity values to roughly define the average intensity space to learn. In most cases should be left set to \\'auto\\'')\n    \n    shared_group = parser.add_argument_group('shared arguments')\n    shared_group.add_argument('--save-images', dest='simages', default=False, help='Save the transformed images under this location. Required for the application mode, optional for the learning mode.')\n    shared_group.add_argument('--threshold', type=float, default=0, help='All voxel with an intensity > threshold are considered as foreground. Supply either this or a mask for each image.')\n    shared_group.add_argument('--masks', nargs='+', help='A number of binary foreground mask, one for each image. Alternative to supplying a threshold. Overrides the threshold parameter if supplied.')\n    shared_group.add_argument('--ignore', dest='ignore', action='store_true', help='Ignore possible loss of information during the intensity transformation. Should only be used when you know what you are doing.')\n    \n    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', help='Verbose output')\n    parser.add_argument('-d', '--debug', dest='debug', action='store_true', help='Display debug information.')\n    parser.add_argument('-f', '--force', dest='force', action='store_true', help='Overwrite existing files (both model and images)')\n    return parser", "response": "Creates and returns the argparse parser object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __minowski_low_positive_integer_p(h1, h2, p = 2): # 11..43 us for p = 1..24 \\w 100 bins\n    mult = scipy.absolute(h1 - h2)\n    dif = mult\n    for _ in range(p - 1): dif = scipy.multiply(dif, mult)\n    return math.pow(scipy.sum(dif), 1./p)", "response": "Return the Minowski distance for positive integer < p"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef manhattan(h1, h2): # # 7 us @array, 31 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    return scipy.sum(scipy.absolute(h1 - h2))", "response": "r A function that calculates manhattan distance between two histograms."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef euclidean(h1, h2): # 9 us @array, 33 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    return math.sqrt(scipy.sum(scipy.square(scipy.absolute(h1 - h2))))", "response": "r Euclidean distance between two histograms."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chebyshev(h1, h2): # 12 us @array, 36 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    return max(scipy.absolute(h1 - h2))", "response": "r Returns the Chebyshev distance between two histograms h1 and h2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chebyshev_neg(h1, h2): # 12 us @array, 36 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    return min(scipy.absolute(h1 - h2))", "response": "r Returns the Chebyshev negative distance between two histograms h1 and h2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relative_deviation(h1, h2): # 18 us @array, 42 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    numerator = math.sqrt(scipy.sum(scipy.square(h1 - h2)))\n    denominator = (math.sqrt(scipy.sum(scipy.square(h1))) + math.sqrt(scipy.sum(scipy.square(h2)))) / 2.\n    return numerator / denominator", "response": "r Returns the relative deviation between two histograms h1 and h2."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef relative_bin_deviation(h1, h2): # 79 us @array, 104 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    numerator = scipy.sqrt(scipy.square(h1 - h2))\n    denominator = (scipy.sqrt(scipy.square(h1)) + scipy.sqrt(scipy.square(h2))) / 2.\n    old_err_state = scipy.seterr(invalid='ignore') # divide through zero only occurs when the bin is zero in both histograms, in which case the division is 0/0 and leads to (and should lead to) 0\n    result = numerator / denominator\n    scipy.seterr(**old_err_state)\n    result[scipy.isnan(result)] = 0 # faster than scipy.nan_to_num, which checks for +inf and -inf also\n    return scipy.sum(result)", "response": "r Calculates the relative bin - wise deviation between two histograms h1 and h2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kullback_leibler(h1, h2): # 83 us @array, 109 us @list \\w 100 bins\n    old_err_state = scipy.seterr(divide='raise')\n    try:\n        h1, h2 = __prepare_histogram(h1, h2)\n        result = (__kullback_leibler(h1, h2) + __kullback_leibler(h2, h1)) / 2.\n        scipy.seterr(**old_err_state)\n        return result\n    except FloatingPointError:\n        scipy.seterr(**old_err_state)\n        raise ValueError('h1 can only contain zero values where h2 also contains zero values and vice-versa')", "response": "r Compute how inefficient it would be code one histogram into another."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __kullback_leibler(h1, h2): # 36.3 us\n    result = h1.astype(scipy.float_)\n    mask = h1 != 0\n    result[mask] = scipy.multiply(h1[mask], scipy.log(h1[mask] / h2[mask]))\n    return scipy.sum(result)", "response": "The actual KL implementation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef jensen_shannon(h1, h2): # 85 us @array, 110 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    s = (h1 + h2) / 2.\n    return __kullback_leibler(h1, s) / 2. + __kullback_leibler(h2, s) / 2.", "response": "r Returns the Jensen - Shannon divergence between two histograms h1 and h2."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef correlate(h1, h2): # 31 us @array, 55 us @list \\w 100 bins\n    h1, h2 = __prepare_histogram(h1, h2)\n    h1m = h1 - scipy.sum(h1) / float(h1.size)\n    h2m = h2 - scipy.sum(h2) / float(h2.size)\n    a = scipy.sum(scipy.multiply(h1m, h2m))\n    b = math.sqrt(scipy.sum(scipy.square(h1m)) * scipy.sum(scipy.square(h2m)))\n    return 0 if 0 == b else a / b", "response": "r Returns the correlation between two histograms h1 and h2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef quadratic_forms(h1, h2):\n    h1, h2 = __prepare_histogram(h1, h2)\n    A = __quadratic_forms_matrix_euclidean(h1, h2)\n    return math.sqrt((h1-h2).dot(A.dot(h1-h2)))", "response": "r Returns the quadratic form of the two histograms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the histograms to scipy. ndarrays if required.", "response": "def __prepare_histogram(h1, h2):\n    \"\"\"Convert the histograms to scipy.ndarrays if required.\"\"\"\n    h1 = h1 if scipy.ndarray == type(h1) else scipy.asarray(h1)\n    h2 = h2 if scipy.ndarray == type(h2) else scipy.asarray(h2)\n    if h1.shape != h2.shape or h1.size != h2.size:\n        raise ValueError('h1 and h2 must be of same shape and size')\n    return h1, h2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find():\n    spark_home = os.environ.get('SPARK_HOME', None)\n\n    if not spark_home:\n        for path in [\n            '/usr/local/opt/apache-spark/libexec', # OS X Homebrew\n            '/usr/lib/spark/', # AWS Amazon EMR\n            '/usr/local/spark/', # common linux path for spark\n            '/opt/spark/', # other common linux path for spark\n            # Any other common places to look?\n        ]:\n            if os.path.exists(path):\n                spark_home = path\n                break\n\n    if not spark_home:\n        raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n                         \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\n    return spark_home", "response": "Find a local spark installation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_rc(spark_home, spark_python, py4j):\n\n    bashrc_location = os.path.expanduser(\"~/.bashrc\")\n\n    if os.path.isfile(bashrc_location):\n        with open(bashrc_location, 'a') as bashrc:\n            bashrc.write(\"\\n# Added by findspark\\n\")\n            bashrc.write(\"export SPARK_HOME=\" + spark_home + \"\\n\")\n            bashrc.write(\"export PYTHONPATH=\" + spark_python + \":\" +\n                         py4j + \":$PYTHONPATH\\n\\n\")", "response": "Changes the bashrc file to be used by findspark."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a startup file to the current IPython profile to import pyspark.", "response": "def edit_ipython_profile(spark_home, spark_python, py4j):\n    \"\"\"Adds a startup file to the current IPython profile to import pyspark.\n\n    The startup file sets the required environment variables and imports pyspark.\n\n    Parameters\n    ----------\n    spark_home : str\n        Path to Spark installation.\n    spark_python : str\n        Path to python subdirectory of Spark installation.\n    py4j : str\n        Path to py4j library.\n    \"\"\"\n    from IPython import get_ipython\n    ip = get_ipython()\n\n    if ip:\n        profile_dir = ip.profile_dir.location\n    else:\n        from IPython.utils.path import locate_profile\n        profile_dir = locate_profile()\n\n    startup_file_loc = os.path.join(profile_dir, \"startup\", \"findspark.py\")\n\n    with open(startup_file_loc, 'w') as startup_file:\n        #Lines of code to be run when IPython starts\n        startup_file.write(\"import sys, os\\n\")\n        startup_file.write(\"os.environ['SPARK_HOME'] = '\" + spark_home + \"'\\n\")\n        startup_file.write(\"sys.path[:0] = \" + str([spark_python, py4j]) + \"\\n\")\n        startup_file.write(\"import pyspark\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking pyspark importable. Sets environment variables and adds dependencies to sys.path. If no Spark location is provided, will try to find an installation. Parameters ---------- spark_home : str, optional, default = None Path to Spark installation, will try to find automatically if not provided. python_path : str, optional, default = None Path to Python for Spark workers (PYSPARK_PYTHON), will use the currently running Python if not provided. edit_rc : bool, optional, default = False Whether to attempt to persist changes by appending to shell config. edit_profile : bool, optional, default = False Whether to create an IPython startup file to automatically configure and import pyspark.", "response": "def init(spark_home=None, python_path=None, edit_rc=False, edit_profile=False):\n    \"\"\"Make pyspark importable.\n\n    Sets environment variables and adds dependencies to sys.path.\n    If no Spark location is provided, will try to find an installation.\n\n    Parameters\n    ----------\n    spark_home : str, optional, default = None\n        Path to Spark installation, will try to find automatically\n        if not provided.\n    python_path : str, optional, default = None\n        Path to Python for Spark workers (PYSPARK_PYTHON),\n        will use the currently running Python if not provided.\n    edit_rc : bool, optional, default = False\n        Whether to attempt to persist changes by appending to shell\n        config.\n    edit_profile : bool, optional, default = False\n        Whether to create an IPython startup file to automatically\n        configure and import pyspark.\n    \"\"\"\n\n    if not spark_home:\n        spark_home = find()\n\n    if not python_path:\n        python_path = os.environ.get('PYSPARK_PYTHON', sys.executable)\n\n    # ensure SPARK_HOME is defined\n    os.environ['SPARK_HOME'] = spark_home\n\n    # ensure PYSPARK_PYTHON is defined\n    os.environ['PYSPARK_PYTHON'] = python_path\n\n    if not os.environ.get(\"PYSPARK_SUBMIT_ARGS\", None):\n        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = ''\n\n    # add pyspark to sys.path\n    spark_python = os.path.join(spark_home, 'python')\n    py4j = glob(os.path.join(spark_python, 'lib', 'py4j-*.zip'))[0]\n    sys.path[:0] = [spark_python, py4j]\n\n    if edit_rc:\n        change_rc(spark_home, spark_python, py4j)\n\n    if edit_profile:\n        edit_ipython_profile(spark_home, spark_python, py4j)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_to_submit_args(s):\n    new_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\") + (\" %s\" % s)\n    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = new_args\n    return new_args", "response": "Adds string s to the PYSPARK_SUBMIT_ARGS env var"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_packages(packages):\n\n    #if the parameter is a string, convert to a single element list\n    if isinstance(packages,str):\n        packages = [packages]\n\n    _add_to_submit_args(\"--packages \"+ \",\".join(packages)  +\" pyspark-shell\")", "response": "Add external packages to the pyspark interpreter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding external jars to the pyspark interpreter.", "response": "def add_jars(jars):\n    \"\"\"Add external jars to the pyspark interpreter.\n\n    Set the PYSPARK_SUBMIT_ARGS properly.\n\n    Parameters\n    ----------\n    jars: list of path to jars in string format\n    \"\"\"\n\n    #if the parameter is a string, convert to a single element list\n    if isinstance(jars,str):\n        jars = [jars]\n\n    _add_to_submit_args(\"--jars \"+ \",\".join(jars)  +\" pyspark-shell\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the given console command definition into a dict.", "response": "def parse(cls, expression):\n        \"\"\"\n        Parse the given console command definition into a dict.\n\n        :param expression: The expression to parse\n        :type expression: str\n\n        :rtype: dict\n        \"\"\"\n        parsed = {\"name\": None, \"arguments\": [], \"options\": []}\n\n        if not expression.strip():\n            raise ValueError(\"Console command signature is empty.\")\n\n        expression = expression.replace(os.linesep, \"\")\n\n        matches = re.match(r\"[^\\s]+\", expression)\n\n        if not matches:\n            raise ValueError(\"Unable to determine command name from signature.\")\n\n        name = matches.group(0)\n        parsed[\"name\"] = name\n\n        tokens = re.findall(r\"\\{\\s*(.*?)\\s*\\}\", expression)\n\n        if tokens:\n            parsed.update(cls._parameters(tokens))\n\n        return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parameters(cls, tokens):\n        arguments = []\n        options = []\n\n        for token in tokens:\n            if not token.startswith(\"--\"):\n                arguments.append(cls._parse_argument(token))\n            else:\n                options.append(cls._parse_option(token))\n\n        return {\"arguments\": arguments, \"options\": options}", "response": "Extracts all of the parameters from the tokens."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_argument(cls, token):\n        description = \"\"\n        validator = None\n\n        if \" : \" in token:\n            token, description = tuple(token.split(\" : \", 2))\n\n            token = token.strip()\n\n            description = description.strip()\n\n        # Checking validator:\n        matches = re.match(r\"(.*)\\((.*?)\\)\", token)\n        if matches:\n            token = matches.group(1).strip()\n            validator = matches.group(2).strip()\n\n        if token.endswith(\"?*\"):\n            return _argument(\n                token.rstrip(\"?*\"),\n                Argument.MULTI_VALUED | Argument.OPTIONAL,\n                description,\n                None,\n            )\n        elif token.endswith(\"*\"):\n            return _argument(\n                token.rstrip(\"*\"),\n                Argument.MULTI_VALUED | Argument.REQUIRED,\n                description,\n                None,\n            )\n        elif token.endswith(\"?\"):\n            return _argument(token.rstrip(\"?\"), Argument.OPTIONAL, description, None)\n\n        matches = re.match(r\"(.+)=(.+)\", token)\n        if matches:\n            return _argument(\n                matches.group(1), Argument.OPTIONAL, description, matches.group(2)\n            )\n\n        return _argument(token, Argument.REQUIRED, description, None)", "response": "Parses an argument expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_option(cls, token):\n        description = \"\"\n        validator = None\n\n        if \" : \" in token:\n            token, description = tuple(token.split(\" : \", 2))\n\n            token = token.strip()\n\n            description = description.strip()\n\n        # Checking validator:\n        matches = re.match(r\"(.*)\\((.*?)\\)\", token)\n        if matches:\n            token = matches.group(1).strip()\n            validator = matches.group(2).strip()\n\n        shortcut = None\n\n        matches = re.split(r\"\\s*\\|\\s*\", token, 2)\n\n        if len(matches) > 1:\n            shortcut = matches[0].lstrip(\"-\")\n            token = matches[1]\n        else:\n            token = token.lstrip(\"-\")\n\n        default = None\n        mode = Option.NO_VALUE\n\n        if token.endswith(\"=*\"):\n            mode = Option.MULTI_VALUED\n            token = token.rstrip(\"=*\")\n        elif token.endswith(\"=?*\"):\n            mode = Option.MULTI_VALUED\n            token = token.rstrip(\"=?*\")\n        elif token.endswith(\"=?\"):\n            mode = Option.OPTIONAL_VALUE\n            token = token.rstrip(\"=?\")\n        elif token.endswith(\"=\"):\n            mode = Option.REQUIRED_VALUE\n            token = token.rstrip(\"=\")\n\n        matches = re.match(r\"(.+)(=[?*]*)(.+)\", token)\n        if matches:\n            token = matches.group(1)\n            operator = matches.group(2)\n            default = matches.group(3)\n\n            if operator == \"=*\":\n                mode = Option.REQUIRED_VALUE | Option.MULTI_VALUED\n            elif operator == \"=?*\":\n                mode = Option.MULTI_VALUED\n            elif operator == \"=?\":\n                mode = Option.OPTIONAL_VALUE\n            elif operator == \"=\":\n                mode = Option.REQUIRED_VALUE\n\n        return _option(token, shortcut, mode, description, default)", "response": "Parses an option expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add(self, command):  # type: (BaseCommand) -> Application\n        self.add_command(command.config)\n        command.set_application(self)\n\n        return self", "response": "Adds a command object to the application."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures the console command using a fluent definition.", "response": "def _configure_using_fluent_definition(self):\n        \"\"\"\n        Configure the console command using a fluent definition.\n        \"\"\"\n        definition = Parser.parse(self.signature)\n\n        self._config.set_name(definition[\"name\"])\n\n        for name, flags, description, default in definition[\"arguments\"]:\n            self._config.add_argument(name, flags, description, default)\n\n        for long_name, short_name, flags, description, default in definition[\"options\"]:\n            self._config.add_option(long_name, short_name, flags, description, default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the value of a command argument.", "response": "def argument(self, key=None):\n        \"\"\"\n        Get the value of a command argument.\n        \"\"\"\n        if key is None:\n            return self._args.arguments()\n\n        return self._args.argument(key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the value of a command option.", "response": "def option(self, key=None):\n        \"\"\"\n        Get the value of a command option.\n        \"\"\"\n        if key is None:\n            return self._args.options()\n\n        return self._args.option(key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef confirm(self, question, default=False, true_answer_regex=\"(?i)^y\"):\n        return self._io.confirm(question, default, true_answer_regex)", "response": "Confirm a question with the user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ask(self, question, default=None):\n        if isinstance(question, Question):\n            return self._io.ask_question(question)\n\n        return self._io.ask(question, default)", "response": "Ask the user for input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef choice(self, question, choices, default=None, attempts=None, multiple=False):\n        question = ChoiceQuestion(question, choices, default)\n\n        question.set_max_attempts(attempts)\n        question.set_multi_select(multiple)\n\n        return self._io.ask_question(question)", "response": "Ask the user for a single choice from a list of answers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_question(self, question, type=None, **kwargs):\n        if not type:\n            return Question(question, **kwargs)\n\n        if type == \"choice\":\n            return ChoiceQuestion(question, **kwargs)\n\n        if type == \"confirmation\":\n            return ConfirmationQuestion(question, **kwargs)", "response": "Creates a new Question object of specified type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Table instance.", "response": "def table(self, header=None, rows=None, style=None):\n        \"\"\"\n        Return a Table instance.\n        \"\"\"\n        if style is not None:\n            style = self.TABLE_STYLES[style]\n\n        table = Table(style)\n\n        if header:\n            table.set_header_row(header)\n\n        if rows:\n            table.set_rows(rows)\n\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders a textual table.", "response": "def render_table(self, headers, rows, style=None):\n        \"\"\"\n        Format input to textual table.\n        \"\"\"\n        table = self.table(headers, rows, style)\n\n        table.render(self._io)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, text, style=None):\n        if style:\n            styled = \"<%s>%s</>\" % (style, text)\n        else:\n            styled = text\n\n        self._io.write(styled)", "response": "Writes a string without a new line."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a string as information output.", "response": "def line(self, text, style=None, verbosity=None):\n        \"\"\"\n        Write a string as information output.\n        \"\"\"\n        if style:\n            styled = \"<%s>%s</>\" % (style, text)\n        else:\n            styled = text\n\n        self._io.write_line(styled, verbosity)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef line_error(self, text, style=None, verbosity=None):\n        if style:\n            styled = \"<%s>%s</>\" % (style, text)\n        else:\n            styled = text\n\n        self._io.error_line(styled, verbosity)", "response": "Write a string as information output to stderr."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new progress indicator.", "response": "def progress_indicator(self, fmt=None, interval=100, values=None):\n        \"\"\"\n        Creates a new progress indicator.\n        \"\"\"\n        return ProgressIndicator(self.io, fmt, interval, values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_style(self, name, fg=None, bg=None, options=None):\n        style = Style(name)\n        if fg is not None:\n            style.fg(fg)\n\n        if bg is not None:\n            style.bg(bg)\n\n        if options is not None:\n            if \"bold\" in options:\n                style.bold()\n\n            if \"underline\" in options:\n                style.underlined()\n\n        self._io.output.formatter.add_style(style)\n        self._io.error_output.formatter.add_style(style)", "response": "Adds a new style to the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef overwrite(self, text, size=None):\n        self._io.overwrite(text, size=size)", "response": "Overwrites the current line with the given text."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_login_password(site_name=\"github.com\",\n                       netrc_file=\"~/.netrc\",\n                       git_credential_file=\"~/.git-credentials\"):\n    \"\"\"Read a .netrc file and return login/password for LWN.\"\"\"\n    try:\n        n = netrc.netrc(os.path.expanduser(netrc_file))\n    except OSError:\n        pass\n    else:\n        if site_name in n.hosts:\n            return n.hosts[site_name][0], n.hosts[site_name][2]\n\n    try:\n        with open(os.path.expanduser(git_credential_file)) as f:\n            for line in f:\n                parsed = parse.urlparse(line.strip())\n                if parsed.hostname == site_name:\n                    return (parse.unquote(parsed.username),\n                            parse.unquote(parsed.password))\n    except OSError:\n        pass\n\n    return None, None", "response": "Read a. netrc file and return login and password for LWN."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_github_hostname_user_repo_from_url(url):\n    parsed = parse.urlparse(url)\n    if parsed.netloc == '':\n        # Probably ssh\n        host, sep, path = parsed.path.partition(\":\")\n        if \"@\" in host:\n            username, sep, host = host.partition(\"@\")\n    else:\n        path = parsed.path[1:].rstrip('/')\n        host = parsed.netloc\n    user, repo = path.split(\"/\", 1)\n    return host, user, repo[:-4] if repo.endswith('.git') else repo", "response": "Return hostname user and repository to fork from."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget title and message summary for patches between 2 commits.", "response": "def git_get_title_and_message(begin, end):\n    \"\"\"Get title and message summary for patches between 2 commits.\n\n    :param begin: first commit to look at\n    :param end: last commit to look at\n    :return: number of commits, title, message\n    \"\"\"\n    titles = git_get_log_titles(begin, end)\n    title = \"Pull request for \" + end\n    if len(titles) == 1:\n        title = titles[0]\n    pr_template = find_pull_request_template()\n    if pr_template:\n        message = get_pr_template_message(pr_template)\n    else:\n        if len(titles) == 1:\n            message = git_get_commit_body(end)\n        else:\n            message = \"\\n\".join(titles)\n\n    return (len(titles), title, message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_term(func):\n\n    @functools.wraps(func)\n    def on_receive_function(self, data):\n        if self.storage.term < data['term']:\n            self.storage.update({\n                'term': data['term']\n            })\n            if not isinstance(self, Follower):\n                self.state.to_follower()\n\n        if self.storage.term > data['term'] and not data['type'].endswith('_response'):\n            response = {\n                'type': '{}_response'.format(data['type']),\n                'term': self.storage.term,\n                'success': False\n            }\n            asyncio.ensure_future(self.state.send(response, data['sender']), loop=self.loop)\n            return\n\n        return func(self, data)\n    return on_receive_function", "response": "Validate the term of the current page."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_commit_index(func):\n\n    @functools.wraps(func)\n    def wrapped(self, *args, **kwargs):\n        for not_applied in range(self.log.last_applied + 1, self.log.commit_index + 1):\n            self.state_machine.apply(self.log[not_applied]['command'])\n            self.log.last_applied += 1\n\n            try:\n                self.apply_future.set_result(not_applied)\n            except (asyncio.futures.InvalidStateError, AttributeError):\n                pass\n\n        return func(self, *args, **kwargs)\n    return wrapped", "response": "Decorator to apply to State Machine everything up to commit index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def append_entries(self, destination=None):\n\n        # Send AppendEntries RPC to destination if specified or broadcast to everyone\n        destination_list = [destination] if destination else self.state.cluster\n        for destination in destination_list:\n            data = {\n                'type': 'append_entries',\n\n                'term': self.storage.term,\n                'leader_id': self.id,\n                'commit_index': self.log.commit_index,\n\n                'request_id': self.request_id\n            }\n\n            next_index = self.log.next_index[destination]\n            prev_index = next_index - 1\n\n            if self.log.last_log_index >= next_index:\n                data['entries'] = [self.log[next_index]]\n\n            else:\n                data['entries'] = []\n\n            data.update({\n                'prev_log_index': prev_index,\n                'prev_log_term': self.log[prev_index]['term'] if self.log and prev_index else 0\n            })\n\n            asyncio.ensure_future(self.state.send(data, destination), loop=self.loop)", "response": "AppendEntries RPC to all nodes in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting to log & send AppendEntries RPC", "response": "async def execute_command(self, command):\n        \"\"\"Write to log & send AppendEntries RPC\"\"\"\n        self.apply_future = asyncio.Future(loop=self.loop)\n\n        entry = self.log.write(self.storage.term, command)\n        asyncio.ensure_future(self.append_entries(), loop=self.loop)\n\n        await self.apply_future"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nincrements current term vote for herself & send vote requests", "response": "def start(self):\n        \"\"\"Increment current term, vote for herself & send vote requests\"\"\"\n        self.storage.update({\n            'term': self.storage.term + 1,\n            'voted_for': self.id\n        })\n\n        self.vote_count = 1\n        self.request_vote()\n        self.election_timer.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrequests Vote RPC \u2014 gather votes", "response": "def request_vote(self):\n        \"\"\"RequestVote RPC \u2014 gather votes\n        Arguments:\n            term \u2014 candidate\u2019s term\n            candidate_id \u2014 candidate requesting vote\n            last_log_index \u2014 index of candidate\u2019s last log entry\n            last_log_term \u2014 term of candidate\u2019s last log entry\n        \"\"\"\n        data = {\n            'type': 'request_vote',\n\n            'term': self.storage.term,\n            'candidate_id': self.id,\n            'last_log_index': self.log.last_log_index,\n            'last_log_term': self.log.last_log_term\n        }\n        self.state.broadcast(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if vote was granted and if it is we got majority and if it is we can become Leader", "response": "def on_receive_request_vote_response(self, data):\n        \"\"\"Receives response for vote request.\n        If the vote was granted then check if we got majority and may become Leader\n        \"\"\"\n\n        if data.get('vote_granted'):\n            self.vote_count += 1\n\n            if self.state.is_majority(self.vote_count):\n                self.state.to_leader()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_receive_append_entries(self, data):\n        if self.storage.term == data['term']:\n            self.state.to_follower()", "response": "When we discover a Leader with the same term step down"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the term and voted_for attributes.", "response": "def init_storage(self):\n        \"\"\"Set current term to zero upon initialization & voted_for to None\"\"\"\n        if not self.storage.exists('term'):\n            self.storage.update({\n                'term': 0,\n            })\n\n        self.storage.update({\n            'voted_for': None\n        })"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def wait_for_election_success(cls):\n        if cls.leader is None:\n            cls.leader_future = asyncio.Future(loop=cls.loop)\n            await cls.leader_future", "response": "Await this function must have a leader"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def wait_until_leader(cls, node_id):\n        if node_id is None:\n            raise ValueError('Node id can not be None!')\n\n        if cls.get_leader() != node_id:\n            cls.wait_until_leader_id = node_id\n            cls.wait_until_leader_future = asyncio.Future(loop=cls.loop)\n            await cls.wait_until_leader_future\n\n            cls.wait_until_leader_id = None\n            cls.wait_until_leader_future = None", "response": "Await this function if you want to do nothing until node_id becomes a leader"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def register(*address_list, cluster=None, loop=None):\n\n    loop = loop or asyncio.get_event_loop()\n    for address in address_list:\n        host, port = address.rsplit(':', 1)\n        node = Node(address=(host, int(port)), loop=loop)\n        await node.start()\n\n        for address in cluster:\n            host, port = address.rsplit(':', 1)\n            port = int(port)\n\n            if (host, port) != (node.host, node.port):\n                node.update_cluster((host, port))", "response": "Start Raft node with given address list and cluster."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def send(self, data, destination):\n        if isinstance(destination, str):\n            host, port = destination.split(':')\n            destination = host, int(port)\n\n        await self.requests.put({\n            'data': data,\n            'destination': destination\n        })", "response": "Sends data to destination Node\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef broadcast(self, data):\n        for destination in self.cluster:\n            asyncio.ensure_future(self.send(data, destination), loop=self.loop)", "response": "Sends data to all Nodes in cluster"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns true if the line appears to be a foreign key definition", "response": "def is_foreign_key(line):\n    \"\"\"\n    :param line: a line from the table definition\n    :return: true if the line appears to be a foreign key definition\n    \"\"\"\n    arrow_position = line.find('->')\n    return arrow_position >= 0 and not any(c in line[:arrow_position] for c in '\"#\\'')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compile_foreign_key(line, context, attributes, primary_key, attr_sql, foreign_key_sql, index_sql):\n    # Parse and validate\n    from .table import Table\n    from .expression import Projection\n\n    new_style = True   # See issue #436.  Old style to be deprecated in a future release\n    try:\n        result = foreign_key_parser.parseString(line)\n    except pp.ParseException:\n        try:\n            result = foreign_key_parser_old.parseString(line)\n        except pp.ParseBaseException as err:\n            raise DataJointError('Parsing error in line \"%s\". %s.' % (line, err)) from None\n        else:\n            new_style = False\n    try:\n        ref = eval(result.ref_table, context)\n    except Exception if new_style else NameError:\n        raise DataJointError('Foreign key reference %s could not be resolved' % result.ref_table)\n\n    options = [opt.upper() for opt in result.options]\n    for opt in options:  # check for invalid options\n        if opt not in {'NULLABLE', 'UNIQUE'}:\n            raise DataJointError('Invalid foreign key option \"{opt}\"'.format(opt=opt))\n    is_nullable = 'NULLABLE' in options\n    is_unique = 'UNIQUE' in options\n    if is_nullable and primary_key is not None:\n        raise DataJointError('Primary dependencies cannot be nullable in line \"{line}\"'.format(line=line))\n\n    if not new_style:\n        if not isinstance(ref, type) or not issubclass(ref, Table):\n            raise DataJointError('Foreign key reference %r must be a valid query' % result.ref_table)\n\n    if isinstance(ref, type) and issubclass(ref, Table):\n        ref = ref()\n\n    # check that dependency is of supported type\n    if (not isinstance(ref, (Table, Projection)) or len(ref.restriction) or\n            (isinstance(ref, Projection) and (not isinstance(ref._arg, Table) or len(ref._arg.restriction)))):\n        raise DataJointError('Dependency \"%s\" is not supported (yet). Use a base table or its projection.' %\n                             result.ref_table)\n\n    if not new_style:\n        # for backward compatibility with old-style dependency declarations.  See issue #436\n        if not isinstance(ref, Table):\n            DataJointError('Dependency \"%s\" is not supported. Check documentation.' % result.ref_table)\n        if not all(r in ref.primary_key for r in result.ref_attrs):\n            raise DataJointError('Invalid foreign key attributes in \"%s\"' % line)\n        try:\n            raise DataJointError('Duplicate attributes \"{attr}\" in \"{line}\"'.format(\n                attr=next(attr for attr in result.new_attrs if attr in attributes),\n                line=line))\n        except StopIteration:\n            pass   # the normal outcome\n\n        # Match the primary attributes of the referenced table to local attributes\n        new_attrs = list(result.new_attrs)\n        ref_attrs = list(result.ref_attrs)\n\n        # special case, the renamed attribute is implicit\n        if new_attrs and not ref_attrs:\n            if len(new_attrs) != 1:\n                raise DataJointError('Renamed foreign key must be mapped to the primary key in \"%s\"' % line)\n            if len(ref.primary_key) == 1:\n                # if the primary key has one attribute, allow implicit renaming\n                ref_attrs = ref.primary_key\n            else:\n                # if only one primary key attribute remains, then allow implicit renaming\n                ref_attrs = [attr for attr in ref.primary_key if attr not in attributes]\n                if len(ref_attrs) != 1:\n                    raise DataJointError('Could not resovle which primary key attribute should be referenced in \"%s\"' % line)\n\n        if len(new_attrs) != len(ref_attrs):\n            raise DataJointError('Mismatched attributes in foreign key \"%s\"' % line)\n\n        if ref_attrs:\n            # convert to projected dependency\n            ref = ref.proj(**dict(zip(new_attrs, ref_attrs)))\n\n    # declare new foreign key attributes\n    base = ref._arg if isinstance(ref, Projection) else ref   # base reference table\n    for attr, ref_attr in zip(ref.primary_key, base.primary_key):\n        if attr not in attributes:\n            attributes.append(attr)\n            if primary_key is not None:\n                primary_key.append(attr)\n            attr_sql.append(\n                base.heading[ref_attr].sql.replace(ref_attr, attr, 1).replace('NOT NULL ', '', int(is_nullable)))\n\n    # declare the foreign key\n    foreign_key_sql.append(\n        'FOREIGN KEY (`{fk}`) REFERENCES {ref} (`{pk}`) ON UPDATE CASCADE ON DELETE RESTRICT'.format(\n            fk='`,`'.join(ref.primary_key),\n            pk='`,`'.join(base.primary_key),\n            ref=base.full_table_name))\n\n    # declare unique index\n    if is_unique:\n        index_sql.append('UNIQUE INDEX ({attrs})'.format(attrs='`,`'.join(ref.primary_key)))", "response": "Compiles a foreign key line into a list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing declaration and create new SQL table accordingly.", "response": "def declare(full_table_name, definition, context):\n    \"\"\"\n    Parse declaration and create new SQL table accordingly.\n\n    :param full_table_name: full name of the table\n    :param definition: DataJoint table definition\n    :param context: dictionary of objects that might be referred to in the table.\n    \"\"\"\n\n    table_name = full_table_name.strip('`').split('.')[1]\n    if len(table_name) > MAX_TABLE_NAME_LENGTH:\n        raise DataJointError(\n            'Table name `{name}` exceeds the max length of {max_length}'.format(\n                name=table_name,\n                max_length=MAX_TABLE_NAME_LENGTH))\n    # split definition into lines\n    definition = re.split(r'\\s*\\n\\s*', definition.strip())\n    # check for optional table comment\n    table_comment = definition.pop(0)[1:].strip() if definition[0].startswith('#') else ''\n    in_key = True  # parse primary keys\n    primary_key = []\n    attributes = []\n    attribute_sql = []\n    foreign_key_sql = []\n    index_sql = []\n    uses_external = False\n\n    for line in definition:\n        if line.startswith('#'):  # additional comments are ignored\n            pass\n        elif line.startswith('---') or line.startswith('___'):\n            in_key = False  # start parsing dependent attributes\n        elif is_foreign_key(line):\n            compile_foreign_key(line, context, attributes,\n                                primary_key if in_key else None,\n                                attribute_sql, foreign_key_sql, index_sql)\n        elif re.match(r'^(unique\\s+)?index[^:]*$', line, re.I):   # index\n            compile_index(line, index_sql)\n        else:\n            name, sql, is_external = compile_attribute(line, in_key, foreign_key_sql)\n            uses_external = uses_external or is_external\n            if in_key and name not in primary_key:\n                primary_key.append(name)\n            if name not in attributes:\n                attributes.append(name)\n                attribute_sql.append(sql)\n    # compile SQL\n    if not primary_key:\n        raise DataJointError('Table must have a primary key')\n\n    return (\n        'CREATE TABLE IF NOT EXISTS %s (\\n' % full_table_name +\n        ',\\n'.join(attribute_sql + ['PRIMARY KEY (`' + '`,`'.join(primary_key) + '`)'] + foreign_key_sql + index_sql) +\n        '\\n) ENGINE=InnoDB, COMMENT \"%s\"' % table_comment), uses_external"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts attribute definition from DataJoint format to SQL", "response": "def compile_attribute(line, in_key, foreign_key_sql):\n    \"\"\"\n    Convert attribute definition from DataJoint format to SQL\n\n    :param line: attribution line\n    :param in_key: set to True if attribute is in primary key set\n    :param foreign_key_sql:\n    :returns: (name, sql, is_external) -- attribute name and sql code for its declaration\n    \"\"\"\n    try:\n        match = attribute_parser.parseString(line+'#', parseAll=True)\n    except pp.ParseException as err:\n        raise DataJointError('Declaration error in position {pos} in line:\\n  {line}\\n{msg}'.format(\n            line=err.args[0], pos=err.args[1], msg=err.args[2]))\n    match['comment'] = match['comment'].rstrip('#')\n    if 'default' not in match:\n        match['default'] = ''\n    match = {k: v.strip() for k, v in match.items()}\n    match['nullable'] = match['default'].lower() == 'null'\n    accepted_datatype = r'time|date|year|enum|(var)?char|float|real|double|decimal|numeric|' \\\n                        r'(tiny|small|medium|big)?int|bool|' \\\n                        r'(tiny|small|medium|long)?blob|external|attach'\n    if re.match(accepted_datatype, match['type'], re.I) is None:\n        raise DataJointError('DataJoint does not support datatype \"{type}\"'.format(**match))\n\n    literals = ['CURRENT_TIMESTAMP']   # not to be enclosed in quotes\n    if match['nullable']:\n        if in_key:\n            raise DataJointError('Primary key attributes cannot be nullable in line %s' % line)\n        match['default'] = 'DEFAULT NULL'  # nullable attributes default to null\n    else:\n        if match['default']:\n            quote = match['default'].upper() not in literals and match['default'][0] not in '\"\\''\n            match['default'] = ('NOT NULL DEFAULT ' +\n                                ('\"%s\"' if quote else \"%s\") % match['default'])\n        else:\n            match['default'] = 'NOT NULL'\n    match['comment'] = match['comment'].replace('\"', '\\\\\"')   # escape double quotes in comment\n\n    is_external = match['type'].startswith('external')\n    is_attachment = match['type'].startswith('attachment')\n    if not is_external:\n        sql = ('`{name}` {type} {default}' + (' COMMENT \"{comment}\"' if match['comment'] else '')).format(**match)\n    else:\n        # process externally stored attribute\n        if in_key:\n            raise DataJointError('External attributes cannot be primary in:\\n%s' % line)\n        store_name = match['type'].split('-')\n        if store_name[0] != 'external':\n            raise DataJointError('External store types must be specified as \"external\" or \"external-<name>\"')\n        store_name = '-'.join(store_name[1:])\n        if store_name != '' and not store_name.isidentifier():\n            raise DataJointError(\n                'The external store name `{type}` is invalid. Make like a python identifier.'.format(**match))\n        if len(store_name) > STORE_NAME_LENGTH:\n            raise DataJointError(\n                'The external store name `{type}` is too long. Must be <={max_len} characters.'.format(\n                    max_len=STORE_NAME_LENGTH, **match))\n        if not match['default'] in ('DEFAULT NULL', 'NOT NULL'):\n            raise DataJointError('The only acceptable default value for an external field is null in:\\n%s' % line)\n        if match['type'] not in config:\n            raise DataJointError('The external store `{type}` is not configured.'.format(**match))\n\n        # append external configuration name to the end of the comment\n        sql = '`{name}` {hash_type} {default} COMMENT \":{type}:{comment}\"'.format(\n            hash_type=HASH_DATA_TYPE, **match)\n        foreign_key_sql.append(\n            \"FOREIGN KEY (`{name}`) REFERENCES {{external_table}} (`hash`) \"\n            \"ON UPDATE RESTRICT ON DELETE RESTRICT\".format(**match))\n\n    return match['name'], sql, is_external"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreserve a job for computation.", "response": "def reserve(self, table_name, key):\n        \"\"\"\n        Reserve a job for computation.  When a job is reserved, the job table contains an entry for the\n        job key, identified by its hash. When jobs are completed, the entry is removed.\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :return: True if reserved job successfully. False = the jobs is already taken\n        \"\"\"\n        job = dict(\n            table_name=table_name,\n            key_hash=key_hash(key),\n            status='reserved',\n            host=platform.node(),\n            pid=os.getpid(),\n            connection_id=self.connection.connection_id,\n            key=key,\n            user=self._user)\n        try:\n            self.insert1(job, ignore_extra_fields=True)\n        except DuplicateError:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog a completed job.", "response": "def complete(self, table_name, key):\n        \"\"\"\n        Log a completed job.  When a job is completed, its reservation entry is deleted.\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        \"\"\"\n        job_key = dict(table_name=table_name, key_hash=key_hash(key))\n        (self & job_key).delete_quick()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog an error message.", "response": "def error(self, table_name, key, error_message, error_stack=None):\n        \"\"\"\n        Log an error message.  The job reservation is replaced with an error entry.\n        if an error occurs, leave an entry describing the problem\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :param error_message: string error message\n        :param error_stack: stack trace\n        \"\"\"\n        if len(error_message) > ERROR_MESSAGE_LENGTH:\n            error_message = error_message[:ERROR_MESSAGE_LENGTH-len(TRUNCATION_APPENDIX)] + TRUNCATION_APPENDIX\n        job_key = dict(table_name=table_name, key_hash=key_hash(key))\n        self.insert1(\n            dict(job_key,\n                 status=\"error\",\n                 host=platform.node(),\n                 pid=os.getpid(),\n                 connection_id=self.connection.connection_id,\n                 user=self._user,\n                 key=key,\n                 error_message=error_message,\n                 error_stack=error_stack),\n            replace=True, ignore_extra_fields=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ordered_dir(klass):\n    attr_list = list()\n    for c in reversed(klass.mro()):\n        attr_list.extend(e for e in (\n            c._ordered_class_members if hasattr(c, '_ordered_class_members') else c.__dict__)\n            if e not in attr_list)\n    return attr_list", "response": "Returns a list of attributes declared in the class including inherited ones similar to dir build - in function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_virtual_module(module_name, schema_name, create_schema=False, create_tables=False, connection=None):\n    module = types.ModuleType(module_name)\n    _schema = Schema(schema_name, create_schema=create_schema, create_tables=create_tables, connection=connection)\n    _schema.spawn_missing_classes(context=module.__dict__)\n    module.__dict__['schema'] = _schema\n    return module", "response": "Creates a python module with the given name from the given schema on the server and automatically adds classes to the tables in the schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the size of the entire schema in bytes", "response": "def size_on_disk(self):\n        \"\"\"\n        :return: size of the entire schema in bytes\n        \"\"\"\n        return int(self.connection.query(\n            \"\"\"\n            SELECT SUM(data_length + index_length)\n            FROM information_schema.tables WHERE table_schema='{db}'\n            \"\"\".format(db=self.database)).fetchone()[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spawn_missing_classes(self, context=None):\n        if context is None:\n            if self.context is not None:\n                context = self.context\n            else:\n                # if context is missing, use the calling namespace\n                frame = inspect.currentframe().f_back\n                context = frame.f_locals\n                del frame\n        tables = [\n            row[0] for row in self.connection.query('SHOW TABLES in `%s`' % self.database)\n            if lookup_class_name('`{db}`.`{tab}`'.format(db=self.database, tab=row[0]), context, 0) is None]\n        master_classes = (Lookup, Manual, Imported, Computed)\n        part_tables = []\n        for table_name in tables:\n            class_name = to_camel_case(table_name)\n            if class_name not in context:\n                try:\n                    cls = next(cls for cls in master_classes if re.fullmatch(cls.tier_regexp, table_name))\n                except StopIteration:\n                    if re.fullmatch(Part.tier_regexp, table_name):\n                        part_tables.append(table_name)\n                else:\n                    # declare and decorate master relation classes\n                    context[class_name] = self(type(class_name, (cls,), dict()))\n\n        # attach parts to masters\n        for table_name in part_tables:\n            groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n            class_name = to_camel_case(groups['part'])\n            try:\n                master_class = context[to_camel_case(groups['master'])]\n            except KeyError:\n                raise DataJointError('The table %s does not follow DataJoint naming conventions' % table_name)\n            part_class = type(class_name, (Part,), dict(definition=...))\n            part_class._master = master_class\n            self.process_relation_class(part_class, context=context, assert_declared=True)\n            setattr(master_class, class_name, part_class)", "response": "Spawn the missing python user relation classes from tables in the schema and places them into the context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef drop(self, force=False):\n        if not self.exists:\n            logger.info(\"Schema named `{database}` does not exist. Doing nothing.\".format(database=self.database))\n        elif (not config['safemode'] or\n              force or\n              user_choice(\"Proceed to delete entire schema `%s`?\" % self.database, default='no') == 'yes'):\n            logger.info(\"Dropping `{database}`.\".format(database=self.database))\n            try:\n                self.connection.query(\"DROP DATABASE `{database}`\".format(database=self.database))\n                logger.info(\"Schema `{database}` was dropped successfully.\".format(database=self.database))\n            except pymysql.OperationalError:\n                raise DataJointError(\"An attempt to drop schema `{database}` \"\n                                     \"has failed. Check permissions.\".format(database=self.database))", "response": "Drop the associated schema if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exists(self):\n        cur = self.connection.query(\"SHOW DATABASES LIKE '{database}'\".format(database=self.database))\n        return cur.rowcount > 0", "response": "Returns true if the associated schema exists on the server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing the relation class.", "response": "def process_relation_class(self, relation_class, context, assert_declared=False):\n        \"\"\"\n        assign schema properties to the relation class and declare the table\n        \"\"\"\n        relation_class.database = self.database\n        relation_class._connection = self.connection\n        relation_class._heading = Heading()\n        # instantiate the class, declare the table if not already\n        instance = relation_class()\n        is_declared = instance.is_declared\n        if not is_declared:\n            if not self.create_tables or assert_declared:\n                raise DataJointError('Table not declared %s' % instance.table_name)\n            else:\n                instance.declare(context)\n        is_declared = is_declared or instance.is_declared\n\n        # fill values in Lookup tables from their contents property\n        if isinstance(instance, Lookup) and hasattr(instance, 'contents') and is_declared:\n            contents = list(instance.contents)\n            if len(contents) > len(instance):\n                if instance.heading.has_autoincrement:\n                    warnings.warn(\n                        'Contents has changed but cannot be inserted because {table} has autoincrement.'.format(\n                            table=instance.__class__.__name__))\n                else:\n                    instance.insert(contents, skip_duplicates=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the jobs table", "response": "def jobs(self):\n        \"\"\"\n        schema.jobs provides a view of the job reservation table for the schema\n        :return: jobs table\n        \"\"\"\n        if self._jobs is None:\n            self._jobs = JobTable(self.connection, self.database)\n        return self._jobs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef external_table(self):\n        if self._external is None:\n            self._external = ExternalTable(self.connection, self.database)\n        return self._external", "response": "Returns the external table for the schema"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a table name in the form schema_name. table_name. depth return the class name found in the context or None if not found.", "response": "def lookup_class_name(name, context, depth=3):\n    \"\"\"\n    given a table name in the form `schema_name`.`table_name`, find its class in the context.\n    :param name: `schema_name`.`table_name`\n    :param context: dictionary representing the namespace\n    :param depth: search depth into imported modules, helps avoid infinite recursion.\n    :return: class name found in the context or None if not found\n    \"\"\"\n    # breadth-first search\n    nodes = [dict(context=context, context_name='', depth=depth)]\n    while nodes:\n        node = nodes.pop(0)\n        for member_name, member in node['context'].items():\n            if not member_name.startswith('_'):  # skip IPython's implicit variables\n                if inspect.isclass(member) and issubclass(member, Table):\n                    if member.full_table_name == name:   # found it!\n                        return '.'.join([node['context_name'],  member_name]).lstrip('.')\n                    try:  # look for part tables\n                        parts = member._ordered_class_members\n                    except AttributeError:\n                        pass  # not a UserTable -- cannot have part tables.\n                    else:\n                        for part in (getattr(member, p) for p in parts if p[0].isupper() and hasattr(member, p)):\n                            if inspect.isclass(part) and issubclass(part, Table) and part.full_table_name == name:\n                                return '.'.join([node['context_name'], member_name, part.__name__]).lstrip('.')\n                elif node['depth'] > 0 and inspect.ismodule(member) and member.__name__ != 'datajoint':\n                    try:\n                        nodes.append(\n                            dict(context=dict(inspect.getmembers(member)),\n                                 context_name=node['context_name'] + '.' + member_name,\n                                 depth=node['depth']-1))\n                    except ImportError:\n                        pass  # could not import, so do not attempt\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef heading(self):\n        if self._heading is None:\n            self._heading = Heading()  # instance-level heading\n        if not self._heading:  # lazy loading of heading\n            if self.connection is None:\n                raise DataJointError(\n                    'DataJoint class is missing a database connection. '\n                    'Missing schema decorator on the class? (e.g. @schema)')\n            else:\n                self._heading.init_from_database(self.connection, self.database, self.table_name)\n        return self._heading", "response": "Returns the table heading."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef declare(self, context=None):\n        try:\n            sql, uses_external = declare(self.full_table_name, self.definition, context)\n            if uses_external:\n                sql = sql.format(external_table=self.external_table.full_table_name)\n            self.connection.query(sql)\n        except pymysql.OperationalError as error:\n            # skip if no create privilege\n            if error.args[0] == server_error_codes['command denied']:\n                logger.warning(error.args[1])\n            else:\n                raise\n        else:\n            self._log('Declared ' + self.full_table_name)", "response": "Declare the table in the schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parents(self, primary=None):\n        return self.connection.dependencies.parents(self.full_table_name, primary)", "response": "Returns a dict of tables referenced with this node s foreign keys."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef children(self, primary=None):\n        return self.connection.dependencies.children(self.full_table_name, primary)", "response": "Returns a dict of tables with foreign keys referencing this node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the table is declared in the schema.", "response": "def is_declared(self):\n        \"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return self.connection.query(\n            'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                database=self.database, table_name=self.table_name)).rowcount > 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninserting a collection of rows into the table.", "response": "def insert(self, rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None):\n        \"\"\"\n        Insert a collection of rows.\n\n        :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence,\n            or a query expression with the same heading as table self.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: applies only in auto-populated tables. Set True to insert outside populate calls.\n\n        Example::\n        >>> relation.insert([\n        >>>     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        >>>     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n\n        if isinstance(rows, pandas.DataFrame):\n            rows = rows.to_records()\n\n        # prohibit direct inserts into auto-populated tables\n        if not (allow_direct_insert or getattr(self, '_allow_insert', True)):  # _allow_insert is only present in AutoPopulate\n            raise DataJointError(\n                'Auto-populate tables can only be inserted into from their make methods during populate calls. (see allow_direct_insert)')\n\n        heading = self.heading\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):   # instantiate if a class\n            rows = rows()\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found.  To ignore extra attributes in insert, set ignore_extra_fields=True.\" %\n                        next(name for name in rows.heading if name not in heading))\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in heading)\n            query = '{command} INTO {table} ({fields}) {select}{duplicate}'.format(\n                command='REPLACE' if replace else 'INSERT',\n                fields='`' + '`,`'.join(fields) + '`',\n                table=self.full_table_name,\n                select=rows.make_sql(select_fields=fields),\n                duplicate=(' ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`'.format(\n                    table=self.full_table_name, pk=self.primary_key[0])\n                           if skip_duplicates else ''))\n            self.connection.query(query)\n            return\n\n        if heading.attributes is None:\n            logger.warning('Could not access table {table}'.format(table=self.full_table_name))\n            return\n\n        field_list = None  # ensures that all rows have the same attributes in the same order as the first row.\n\n        def make_row_to_insert(row):\n            \"\"\"\n            :param row:  A tuple to insert\n            :return: a dict with fields 'names', 'placeholders', 'values'\n            \"\"\"\n\n            def make_placeholder(name, value):\n                \"\"\"\n                For a given attribute `name` with `value`, return its processed value or value placeholder\n                as a string to be included in the query and the value, if any, to be submitted for\n                processing by mysql API.\n                :param name:\n                :param value:\n                \"\"\"\n                if ignore_extra_fields and name not in heading:\n                    return None\n                if heading[name].is_external:\n                    placeholder, value = '%s', self.external_table.put(heading[name].type, value)\n                elif heading[name].is_blob:\n                    if value is None:\n                        placeholder, value = 'NULL', None\n                    else:\n                        placeholder, value = '%s', pack(value)\n                elif heading[name].numeric:\n                    if value is None or value == '' or np.isnan(np.float(value)):  # nans are turned into NULLs\n                        placeholder, value = 'NULL', None\n                    else:\n                        placeholder, value = '%s', (str(int(value) if isinstance(value, bool) else value))\n                else:\n                    placeholder = '%s'\n                return name, placeholder, value\n\n            def check_fields(fields):\n                \"\"\"\n                Validates that all items in `fields` are valid attributes in the heading\n                :param fields: field names of a tuple\n                \"\"\"\n                if field_list is None:\n                    if not ignore_extra_fields:\n                        for field in fields:\n                            if field not in heading:\n                                raise KeyError(u'`{0:s}` is not in the table heading'.format(field))\n                elif set(field_list) != set(fields).intersection(heading.names):\n                    raise DataJointError('Attempt to insert rows with different fields')\n\n            if isinstance(row, np.void):  # np.array\n                check_fields(row.dtype.fields)\n                attributes = [make_placeholder(name, row[name])\n                              for name in heading if name in row.dtype.fields]\n            elif isinstance(row, collections.abc.Mapping):  # dict-based\n                check_fields(row)\n                attributes = [make_placeholder(name, row[name]) for name in heading if name in row]\n            else:  # positional\n                try:\n                    if len(row) != len(heading):\n                        raise DataJointError(\n                            'Invalid insert argument. Incorrect number of attributes: '\n                            '{given} given; {expected} expected'.format(\n                                given=len(row), expected=len(heading)))\n                except TypeError:\n                    raise DataJointError('Datatype %s cannot be inserted' % type(row))\n                else:\n                    attributes = [make_placeholder(name, value) for name, value in zip(heading, row)]\n            if ignore_extra_fields:\n                attributes = [a for a in attributes if a is not None]\n\n            assert len(attributes), 'Empty tuple'\n            row_to_insert = dict(zip(('names', 'placeholders', 'values'), zip(*attributes)))\n            nonlocal field_list\n            if field_list is None:\n                # first row sets the composition of the field list\n                field_list = row_to_insert['names']\n            else:\n                #  reorder attributes in row_to_insert to match field_list\n                order = list(row_to_insert['names'].index(field) for field in field_list)\n                row_to_insert['names'] = list(row_to_insert['names'][i] for i in order)\n                row_to_insert['placeholders'] = list(row_to_insert['placeholders'][i] for i in order)\n                row_to_insert['values'] = list(row_to_insert['values'][i] for i in order)\n\n            return row_to_insert\n\n        rows = list(make_row_to_insert(row) for row in rows)\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command='REPLACE' if replace else 'INSERT',\n                    destination=self.from_clause,\n                    fields='`,`'.join(field_list),\n                    placeholders=','.join('(' + ','.join(row['placeholders']) + ')' for row in rows),\n                    duplicate=(' ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`'.format(pk=self.primary_key[0])\n                               if skip_duplicates else ''))\n                self.connection.query(query, args=list(\n                    itertools.chain.from_iterable((v for v in r['values'] if v is not None) for r in rows)))\n            except (OperationalError, InternalError, IntegrityError) as err:\n                if err.args[0] == server_error_codes['command denied']:\n                    raise DataJointError('Command denied:  %s' % err.args[1]) from None\n                elif err.args[0] == server_error_codes['unknown column']:\n                    # args[1] -> Unknown column 'extra' in 'field list'\n                    raise DataJointError(\n                        '{} : To ignore extra fields, set ignore_extra_fields=True in insert.'.format(err.args[1])\n                    ) from None\n                elif err.args[0] == server_error_codes['duplicate entry']:\n                    raise DuplicateError(\n                        '{} : To ignore duplicate entries, set skip_duplicates=True in insert.'.format(err.args[1])\n                    ) from None\n                else:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the table without cascading and without user prompt.", "response": "def delete_quick(self, get_count=False):\n        \"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = 'DELETE FROM ' + self.full_table_name + self.where_clause\n        self.connection.query(query)\n        count = self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0] if get_count else None\n        self._log(query[:255])\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the contents of the table and its dependent tables recursively.", "response": "def delete(self, verbose=True):\n        \"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        conn = self.connection\n        already_in_transaction = conn.in_transaction\n        safe = config['safemode']\n        if already_in_transaction and safe:\n            raise DataJointError('Cannot delete within a transaction in safemode. '\n                                 'Set dj.config[\"safemode\"] = False or complete the ongoing transaction first.')\n        graph = conn.dependencies\n        graph.load()\n        delete_list = collections.OrderedDict(\n            (name, _rename_map(next(iter(graph.parents(name).items()))) if name.isdigit() else FreeTable(conn, name))\n            for name in graph.descendants(self.full_table_name))\n\n        # construct restrictions for each relation\n        restrict_by_me = set()\n        # restrictions: Or-Lists of restriction conditions for each table.\n        # Uncharacteristically of Or-Lists, an empty entry denotes \"delete everything\".\n        restrictions = collections.defaultdict(list)\n        # restrict by self\n        if self.restriction:\n            restrict_by_me.add(self.full_table_name)\n            restrictions[self.full_table_name].append(self.restriction)  # copy own restrictions\n        # restrict by renamed nodes\n        restrict_by_me.update(table for table in delete_list if table.isdigit())  # restrict by all renamed nodes\n        # restrict by secondary dependencies\n        for table in delete_list:\n            restrict_by_me.update(graph.children(table, primary=False))   # restrict by any non-primary dependents\n\n        # compile restriction lists\n        for name, table in delete_list.items():\n            for dep in graph.children(name):\n                # if restrict by me, then restrict by the entire relation otherwise copy restrictions\n                restrictions[dep].extend([table] if name in restrict_by_me else restrictions[name])\n\n        # apply restrictions\n        for name, table in delete_list.items():\n            if not name.isdigit() and restrictions[name]:  # do not restrict by an empty list\n                table.restrict([\n                    r.proj() if isinstance(r, FreeTable) else (\n                        delete_list[r[0]].proj(**{a: b for a, b in r[1]['attr_map'].items()})\n                        if isinstance(r, _rename_map) else r)\n                    for r in restrictions[name]])\n        if safe:\n            print('About to delete:')\n\n        if not already_in_transaction:\n            self.connection.start_transaction()\n        total = 0\n        try:\n            for name, table in reversed(list(delete_list.items())):\n                if not name.isdigit():\n                    count = table.delete_quick(get_count=True)\n                    total += count\n                    if (verbose or safe) and count:\n                        print('{table}: {count} items'.format(table=name, count=count))\n        except:\n            # Delete failed, perhaps due to insufficient privileges. Cancel transaction.\n            if not already_in_transaction:\n                self.connection.cancel_transaction()\n            raise\n        else:\n            assert not (already_in_transaction and safe)\n            if not total:\n                print('Nothing to delete')\n                if not already_in_transaction:\n                    self.connection.cancel_transaction()\n            else:\n                if already_in_transaction:\n                    if verbose:\n                        print('The delete is pending within the ongoing transaction.')\n                else:\n                    if not safe or user_choice(\"Proceed?\", default='no') == 'yes':\n                        self.connection.commit_transaction()\n                        if verbose or safe:\n                            print('Committed.')\n                    else:\n                        self.connection.cancel_transaction()\n                        if verbose or safe:\n                            print('Cancelled deletes.')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef drop_quick(self):\n        if self.is_declared:\n            query = 'DROP TABLE %s' % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\"Nothing to drop: table %s is not declared\" % self.full_table_name)", "response": "Drop the table associated with this relation without cascading and without user prompt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef drop(self):\n        if self.restriction:\n            raise DataJointError('A relation with an applied restriction condition cannot be dropped.'\n                                 ' Call drop() on the unrestricted Table.')\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [table for table in self.connection.dependencies.descendants(self.full_table_name)\n                  if not table.isdigit()]\n        if config['safemode']:\n            for table in tables:\n                print(table, '(%d tuples)' % len(FreeTable(self.connection, table)))\n            do_drop = user_choice(\"Proceed?\", default='no') == 'yes'\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            print('Tables dropped.  Restart kernel.')", "response": "Drop the table and all tables that reference it recursively."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef size_on_disk(self):\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name), as_dict=True).fetchone()\n        return ret['Data_length'] + ret['Index_length']", "response": "Returns the size of data and indices on the storage device."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef describe(self, context=None, printout=True):\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents()\n        in_key = True\n        definition = ('# ' + self.heading.table_info['comment'] + '\\n'\n                      if self.heading.table_info['comment'] else '')\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += '---\\n'\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in list(parents.items()):  # need list() to force a copy\n                if attr.name in fk_props['attr_map']:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props['attr_map']):\n                        parents.pop(parent_name)\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props['attr_map']))\n                        except KeyError:\n                            index_props = ''\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = ' [{}]'.format(', '.join(index_props)) if index_props else ''\n\n                        if not parent_name.isdigit():\n                            # simple foreign key\n                            definition += '->{props} {class_name}\\n'.format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context) or parent_name)\n                        else:\n                            # projected foreign key\n                            parent_name = list(self.connection.dependencies.in_edges(parent_name))[0][0]\n                            lst = [(attr, ref) for attr, ref in fk_props['attr_map'].items() if ref != attr]\n                            definition += '->{props} {class_name}.proj({proj_list})\\n'.format(\n                                attr_list=', '.join(r[0] for r in lst),\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context) or parent_name,\n                                proj_list=','.join('{}=\"{}\"'.format(a,b) for a, b in lst))\n                            attributes_declared.update(fk_props['attr_map'])\n            if do_include:\n                attributes_declared.add(attr.name)\n                name = attr.name.lstrip('_')  # for external\n                definition += '%-20s : %-28s %s\\n' % (\n                    name if attr.default is None else '%s=%s' % (name, attr.default),\n                    '%s%s' % (attr.type, ' auto_increment' if attr.autoincrement else ''),\n                    '# ' + attr.comment if attr.comment else '')\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += '{unique}INDEX ({attrs})\\n'.format(\n                unique='UNIQUE ' if v['unique'] else '',\n                attrs=', '.join(k))\n        if printout:\n            print(definition)\n        return definition", "response": "Returns the string that describes the relation using DataJoint DDL."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates a field in an existing tuple.", "response": "def _update(self, attrname, value=None):\n        \"\"\"\n            Updates a field in an existing tuple. This is not a datajoyous operation and should not be used\n            routinely. Relational database maintain referential integrity on the level of a tuple. Therefore,\n            the UPDATE operator can violate referential integrity. The datajoyous way to update information is\n            to delete the entire tuple and insert the entire update tuple.\n\n            Safety constraints:\n               1. self must be restricted to exactly one tuple\n               2. the update attribute must not be in primary key\n\n            Example\n\n            >>> (v2p.Mice() & key).update('mouse_dob',   '2011-01-01')\n            >>> (v2p.Mice() & key).update( 'lens')   # set the value to NULL\n\n        \"\"\"\n        if len(self) != 1:\n            raise DataJointError('Update is only allowed on one tuple at a time')\n        if attrname not in self.heading:\n            raise DataJointError('Invalid attribute name')\n        if attrname in self.heading.primary_key:\n            raise DataJointError('Cannot update a key value.')\n\n        attr = self.heading[attrname]\n\n        if attr.is_blob:\n            value = pack(value)\n            placeholder = '%s'\n        elif attr.numeric:\n            if value is None or np.isnan(np.float(value)):  # nans are turned into NULLs\n                placeholder = 'NULL'\n                value = None\n            else:\n                placeholder = '%s'\n                value = str(int(value) if isinstance(value, bool) else value)\n        else:\n            placeholder = '%s'\n        command = \"UPDATE {full_table_name} SET `{attrname}`={placeholder} {where_clause}\".format(\n            full_table_name=self.from_clause,\n            attrname=attrname,\n            placeholder=placeholder,\n            where_clause=self.where_clause)\n        self.connection.query(command, args=(value, ) if value is not None else ())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if expressions rel1 and rel2 are join - compatible.", "response": "def assert_join_compatibility(rel1, rel2):\n    \"\"\"\n    Determine if expressions rel1 and rel2 are join-compatible.  To be join-compatible, the matching attributes\n    in the two expressions must be in the primary key of one or the other expression.\n    Raises an exception if not compatible.\n    :param rel1: A QueryExpression object\n    :param rel2: A QueryExpression object\n    \"\"\"\n    for rel in (rel1, rel2):\n        if not isinstance(rel, (U, QueryExpression)):\n            raise DataJointError('Object %r is not a QueryExpression and cannot be joined.' % rel)\n    if not isinstance(rel1, U) and not isinstance(rel2, U):  # dj.U is always compatible\n        try:\n            raise DataJointError(\"Cannot join query expressions on dependent attribute `%s`\" % next(r for r in set(\n                rel1.heading.dependent_attributes).intersection(rel2.heading.dependent_attributes)))\n        except StopIteration:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntranslate the input arg into the equivalent SQL condition string or a boolean value.", "response": "def _make_condition(self, arg):\n        \"\"\"\n        Translate the input arg into the equivalent SQL condition (a string)\n        :param arg: any valid restriction object.\n        :return: an SQL condition string or a boolean value.\n        \"\"\"\n        def prep_value(v):\n            return str(v) if isinstance(v, (datetime.date, datetime.datetime, datetime.time, decimal.Decimal)) else v\n\n        negate = False\n        while isinstance(arg, Not):\n            negate = not negate\n            arg = arg.restriction\n        template = \"NOT (%s)\" if negate else \"%s\"\n\n        # restrict by string\n        if isinstance(arg, str):\n            return template % arg.strip().replace(\"%\", \"%%\")  # escape % in strings, see issue #376\n\n        # restrict by AndList\n        if isinstance(arg, AndList):\n            # omit all conditions that evaluate to True\n            items = [item for item in (self._make_condition(i) for i in arg) if item is not True]\n            if any(item is False for item in items):\n                return negate  # if any item is False, the whole thing is False\n            if not items:\n                return not negate   # and empty AndList is True\n            return template % ('(' + ') AND ('.join(items) + ')')\n\n        # restriction by dj.U evaluates to True\n        if isinstance(arg, U):\n            return not negate\n\n        # restrict by boolean\n        if isinstance(arg, bool):\n            return negate != arg\n\n        # restrict by a mapping such as a dict -- convert to an AndList of string equality conditions\n        if isinstance(arg, collections.abc.Mapping):\n            return template % self._make_condition(\n                AndList('`%s`=%r' % (k, prep_value(v)) for k, v in arg.items() if k in self.heading))\n\n        # restrict by a numpy record -- convert to an AndList of string equality conditions\n        if isinstance(arg, np.void):\n            return template % self._make_condition(\n                AndList(('`%s`=%r' % (k, prep_value(arg[k])) for k in arg.dtype.fields if k in self.heading)))\n\n        # restrict by a QueryExpression subclass -- triggers instantiation\n        if inspect.isclass(arg) and issubclass(arg, QueryExpression):\n            arg = arg()\n\n        # restrict by another expression (aka semijoin and antijoin)\n        if isinstance(arg, QueryExpression):\n            assert_join_compatibility(self, arg)\n            common_attributes = [q for q in arg.heading.names if q in self.heading.names]\n            return (\n                # without common attributes, any non-empty set matches everything\n                (not negate if arg else negate) if not common_attributes\n                else '({fields}) {not_}in ({subquery})'.format(\n                    fields='`' + '`,`'.join(common_attributes) + '`',\n                    not_=\"not \" if negate else \"\",\n                    subquery=arg.make_sql(common_attributes)))\n\n        # restrict by pandas.DataFrames\n        if isinstance(arg, pandas.DataFrame):\n            arg = arg.to_records()   # convert to np.recarray\n\n        # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList\n        try:\n            or_list = [self._make_condition(q) for q in arg]\n        except TypeError:\n            raise DataJointError('Invalid restriction type %r' % arg)\n        else:\n            or_list = [item for item in or_list if item is not False]  # ignore all False conditions\n            if any(item is True for item in or_list):  # if any item is True, the whole thing is True\n                return not negate\n            return template % ('(%s)' % ' OR '.join(or_list)) if or_list else negate"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting self. restriction to the SQL WHERE clause", "response": "def where_clause(self):\n        \"\"\"\n        convert self.restriction to the SQL WHERE clause\n        \"\"\"\n        cond = self._make_condition(self.restriction)\n        return '' if cond is True else ' WHERE %s' % cond"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_select_fields(self, select_fields=None):\n        return self.heading.as_sql if select_fields is None else self.heading.project(select_fields).as_sql", "response": "Returns the select fields for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an entity set representing the result of the aggregation and projection of entities from self and group by the given attributes.", "response": "def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n        \"\"\"\n        Aggregation/projection operator\n        :param group:  an entity set whose entities will be grouped per entity of `self`\n        :param attributes: attributes of self to include in the result\n        :param keep_all_rows: True = preserve the number of elements in the result (equivalent of LEFT JOIN in SQL)\n        :param named_attributes: renamings and computations on attributes of self and group\n        :return: an entity set representing the result of the aggregation/projection operator of entities from `group`\n        per entity of `self`\n        \"\"\"\n        return GroupBy.create(self, group, keep_all_rows=keep_all_rows,\n                              attributes=attributes, named_attributes=named_attributes)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restrict(self, restriction):\n        assert is_true(restriction) or not self.heading.expressions or isinstance(self, GroupBy), \\\n            \"Cannot restrict a projection with renamed attributes in place.\"\n        self.restriction.append(restriction)\n        return self", "response": "In - place restriction. Restricts the result to a subset of the input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tail(self, limit=25, **fetch_kwargs):\n        return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]", "response": "Returns the last few entries from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attributes_in_restriction(self):\n        return set(name for name in self.heading.names\n                   if re.search(r'\\b' + name + r'\\b', self.where_clause))", "response": "Returns a list of attributes that are probably used in the restriction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef preview(self, limit=None, width=None):\n        heading = self.heading\n        rel = self.proj(*heading.non_blobs)\n        if limit is None:\n            limit = config['display.limit']\n        if width is None:\n            width = config['display.width']\n        tuples = rel.fetch(limit=limit+1, format=\"array\")\n        has_more = len(tuples) > limit\n        tuples = tuples[:limit]\n        columns = heading.names\n        widths = {f: min(max([len(f)] +\n            [len(str(e)) for e in tuples[f]] if f in tuples.dtype.names else [len('=BLOB=')]) + 4, width) for f in columns}\n        templates = {f: '%%-%d.%ds' % (widths[f], widths[f]) for f in columns}\n        return (\n            ' '.join([templates[f] % ('*' + f if f in rel.primary_key else f) for f in columns]) + '\\n' +\n            ' '.join(['+' + '-' * (widths[column] - 2) + '+' for column in columns]) + '\\n' +\n            '\\n'.join(' '.join(templates[f] % (tup[f] if f in tup.dtype.names else '=BLOB=')\n                for f in columns) for tup in tuples) +\n            ('\\n   ...\\n' if has_more else '\\n') +\n            (' (Total: %d)\\n' % len(rel) if config['display.show_tuple_count'] else ''))", "response": "Returns a preview of the contents of the query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a cursor of the current record set.", "response": "def cursor(self, offset=0, limit=None, order_by=None, as_dict=False):\n        \"\"\"\n        See expression.fetch() for input description.\n        :return: query cursor\n        \"\"\"\n        if offset and limit is None:\n            raise DataJointError('limit is required when offset is set')\n        sql = self.make_sql()\n        if order_by is not None:\n            sql += ' ORDER BY ' + ', '.join(order_by)\n        if limit is not None:\n            sql += ' LIMIT %d' % limit + (' OFFSET %d' % offset if offset else \"\")\n        logger.debug(sql)\n        return self.connection.query(sql, as_dict=as_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecide when a Join argument needs to be wrapped in a subquery", "response": "def make_argument_subquery(arg):\n        \"\"\"\n        Decide when a Join argument needs to be wrapped in a subquery\n        \"\"\"\n        return Subquery.create(arg) if isinstance(arg, (GroupBy, Projection)) or arg.restriction else arg"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new projection object from the given arguments.", "response": "def create(cls, arg, attributes=None, named_attributes=None, include_primary_key=True):\n        \"\"\"\n        :param arg: The QueryExression to be projected\n        :param attributes:  attributes to select\n        :param named_attributes:  new attributes to create by renaming or computing\n        :param include_primary_key:  True if the primary key must be included even if it's not in attributes.\n        :return: the resulting Projection object\n        \"\"\"\n        obj = cls()\n        obj._connection = arg.connection\n        named_attributes = {k: v.strip() for k, v in named_attributes.items()}  # clean up values\n        obj._distinct = arg.distinct\n        if include_primary_key:  # include primary key of the QueryExpression\n            attributes = (list(a for a in arg.primary_key if a not in named_attributes.values()) +\n                          list(a for a in attributes if a not in arg.primary_key))\n        else:\n            # make distinct if the primary key is not completely selected\n            obj._distinct = obj._distinct or not set(arg.primary_key).issubset(\n                set(attributes) | set(named_attributes.values()))\n        if obj._distinct or cls._need_subquery(arg, attributes, named_attributes):\n            obj._arg = Subquery.create(arg)\n            obj._heading = obj._arg.heading.project(attributes, named_attributes)\n            if not include_primary_key:\n                obj._heading = obj._heading.extend_primary_key(attributes)\n        else:\n            obj._arg = arg\n            obj._heading = obj._arg.heading.project(attributes, named_attributes)\n            obj &= arg.restriction  # copy restriction when no subquery\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecides whether the argument needs to be wrapped in a subquery.", "response": "def _need_subquery(arg, attributes, named_attributes):\n        \"\"\"\n        Decide whether the projection argument needs to be wrapped in a subquery\n        \"\"\"\n        if arg.heading.expressions or arg.distinct:  # argument has any renamed (computed) attributes\n            return True\n        restricting_attributes = arg.attributes_in_restriction()\n        return (not restricting_attributes.issubset(attributes) or  # if any restricting attribute is projected out or\n                any(v.strip() in restricting_attributes for v in named_attributes.values()))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(cls, arg):\n        obj = cls()\n        obj._connection = arg.connection\n        obj._heading = arg.heading.make_subquery_heading()\n        obj._arg = arg\n        return obj", "response": "construct a subquery from arg"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aggr(self, group, **named_attributes):\n        return (\n            GroupBy.create(self, group=group, keep_all_rows=False, attributes=(), named_attributes=named_attributes)\n            if self.primary_key else\n            Projection.create(group, attributes=(), named_attributes=named_attributes, include_primary_key=False))", "response": "This method creates a derived aggregation of the type U ( attr1 attr2 ) and returns the derived query expression that is used to aggregate the values of the group."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprompts the user for confirmation.", "response": "def user_choice(prompt, choices=(\"yes\", \"no\"), default=None):\n    \"\"\"\n    Prompts the user for confirmation.  The default value, if any, is capitalized.\n\n    :param prompt: Information to display to the user.\n    :param choices: an iterable of possible choices.\n    :param default: default choice\n    :return: the user's choice\n    \"\"\"\n    assert default is None or default in choices\n    choice_list = ', '.join((choice.title() if choice == default else choice for choice in choices))\n    response = None\n    while response not in choices:\n        response = input(prompt + ' [' + choice_list + ']: ')\n        response = response.lower() if response else default\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_camel_case(s):\n\n    def to_upper(match):\n        return match.group(0)[-1].upper()\n\n    return re.sub('(^|[_\\W])+[a-zA-Z]', to_upper, s)", "response": "Convert names with under score ( _ ) separation into CamelCase names."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_camel_case(s):\n\n    def convert(match):\n        return ('_' if match.groups()[0] else '') + match.group(0).lower()\n\n    if not re.match(r'[A-Z][a-zA-Z0-9]*', s):\n        raise DataJointError(\n            'ClassName must be alphanumeric in CamelCase, begin with a capital letter')\n    return re.sub(r'(\\B[A-Z])|(\\b[A-Z])', convert, s)", "response": "Convert names in camel case into underscore separated names\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef safe_write(filename, blob):\n    temp_file = filename + '.saving'\n    with open(temp_file, 'bw') as f:\n        f.write(blob)\n    os.rename(temp_file, filename)", "response": "A two - step write."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dicts(recarray):\n    for rec in recarray:\n        yield dict(zip(recarray.dtype.names, rec.tolist()))", "response": "convert record array to a list of dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef keys(self, **kwargs):\n        warnings.warn('Use of `rel.fetch.keys()` notation is deprecated. '\n                      'Please use `rel.fetch(\"KEY\")` or `rel.fetch(dj.key)` for equivalent result', stacklevel=2)\n        yield from self._expression.proj().fetch(as_dict=True, **kwargs)", "response": "DEPRECATED - Use fetch. keys instead of fetch."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a base64 - encoded long hash of the given list of buffers.", "response": "def long_hash(*buffers):\n    \"\"\"\n    :param buffer: a binary buffer (e.g. serialized blob)\n    :return: 43-character base64 ASCII rendition SHA-256\n    \"\"\"\n    hashed = hashlib.sha256()\n    for buffer in buffers:\n        hashed.update(buffer)\n    return to_ascii(hashed.digest())[0:43]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef short_hash(*buffers):\n    hashed = hashlib.sha1()\n    for buffer in buffers:\n        hashed.update(buffer)\n    return to_ascii(hashed.digest())[:8]", "response": "short_hash - Returns the first 8 characters of base64 ASCII rendition SHA - 1 of the given list of buffers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a persistent connection object to be shared by multiple modules.", "response": "def conn(host=None, user=None, password=None, init_fun=None, reset=False):\n    \"\"\"\n    Returns a persistent connection object to be shared by multiple modules.\n    If the connection is not yet established or reset=True, a new connection is set up.\n    If connection information is not provided, it is taken from config which takes the\n    information from dj_local_conf.json. If the password is not specified in that file\n    datajoint prompts for the password.\n\n    :param host: hostname\n    :param user: mysql user\n    :param password: mysql password\n    :param init_fun: initialization function\n    :param reset: whether the connection should be reset or not\n    \"\"\"\n    if not hasattr(conn, 'connection') or reset:\n        host = host if host is not None else config['database.host']\n        user = user if user is not None else config['database.user']\n        password = password if password is not None else config['database.password']\n        if user is None:  # pragma: no cover\n            user = input(\"Please enter DataJoint username: \")\n        if password is None:  # pragma: no cover\n            password = getpass(prompt=\"Please enter DataJoint password: \")\n        init_fun = init_fun if init_fun is not None else config['connection.init_function']\n        conn.connection = Connection(host, user, password, init_fun)\n    return conn.connection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self):\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', '.*deprecated.*')\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                         \"STRICT_ALL_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\",\n                charset=config['connection.charset'],\n                **self.conn_info)\n        self._conn.autocommit(True)", "response": "Connects to the database server."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a mysql query and return the cursor objects.", "response": "def query(self, query, args=(), as_dict=False, suppress_warnings=True, reconnect=None):\n        \"\"\"\n        Execute the specified query and return the tuple generator (cursor).\n\n        :param query: mysql query\n        :param args: additional arguments for the client.cursor\n        :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                        query results as dictionary.\n        :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n        \"\"\"\n        if reconnect is None:\n            reconnect = config['database.reconnect']\n\n        cursor = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n        cur = self._conn.cursor(cursor=cursor)\n\n        logger.debug(\"Executing SQL:\" + query[0:300])\n        try:\n            with warnings.catch_warnings():\n                if suppress_warnings:\n                    # suppress all warnings arising from underlying SQL library\n                    warnings.simplefilter(\"ignore\")\n                cur.execute(query, args)\n        except (err.InterfaceError, err.OperationalError) as e:\n            if is_connection_error(e) and reconnect:\n                warnings.warn(\"Mysql server has gone away. Reconnecting to the server.\")\n                self.connect()\n                if self._in_transaction:\n                    self.cancel_transaction()\n                    raise DataJointError(\"Connection was lost during a transaction.\")\n                else:\n                    logger.debug(\"Re-executing SQL\")\n                    cur = self.query(query, args=args, as_dict=as_dict, suppress_warnings=suppress_warnings, reconnect=False)\n            else:\n                logger.debug(\"Caught InterfaceError/OperationalError.\")\n                raise\n        except err.ProgrammingError as e:\n            if e.args[0] == server_error_codes['parse error']:\n                raise DataJointError(\"\\n\".join((\n                    \"Error in query:\", query,\n                    \"Please check spelling, syntax, and existence of tables and attributes.\",\n                    \"When restricting a relation by a condition in a string, enclose attributes in backquotes.\"\n                ))) from None\n        return cur"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in_transaction(self):\n        self._in_transaction = self._in_transaction and self.is_connected\n        return self._in_transaction", "response": "Returns True if there is an open transaction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_transaction(self):\n        if self.in_transaction:\n            raise DataJointError(\"Nested connections are not supported.\")\n        self.query('START TRANSACTION WITH CONSISTENT SNAPSHOT')\n        self._in_transaction = True\n        logger.info(\"Transaction started\")", "response": "Starts a transaction error.\n\n        :raise DataJointError: if there is an ongoing transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads all loaded schemas and add nodes and foreign keys to the graph.", "response": "def load(self):\n        \"\"\"\n        Load dependencies for all loaded schemas.\n        This method gets called before any operation that requires dependencies: delete, drop, populate, progress.\n        \"\"\"\n\n        # reload from scratch to prevent duplication of renamed edges\n        self.clear()\n\n        # load primary key info\n        keys = self._conn.query(\"\"\"\n                SELECT\n                    concat('`', table_schema, '`.`', table_name, '`') as tab, column_name\n                FROM information_schema.key_column_usage\n                WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\"\n                \"\"\".format(schemas=\"','\".join(self._conn.schemas)))\n        pks = defaultdict(set)\n        for key in keys:\n            pks[key[0]].add(key[1])\n\n        # add nodes to the graph\n        for n, pk in pks.items():\n            self.add_node(n, primary_key=pk)\n\n        # load foreign keys\n        keys = self._conn.query(\"\"\"\n        SELECT constraint_name,\n            concat('`', table_schema, '`.`', table_name, '`') as referencing_table,\n            concat('`', referenced_table_schema, '`.`',  referenced_table_name, '`') as referenced_table,\n            column_name, referenced_column_name\n        FROM information_schema.key_column_usage\n        WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR\n            referenced_table_schema is not NULL AND table_schema in ('{schemas}'))\n        \"\"\".format(schemas=\"','\".join(self._conn.schemas)), as_dict=True)\n        fks = defaultdict(lambda: dict(attr_map=OrderedDict()))\n        for key in keys:\n            d = fks[(key['constraint_name'], key['referencing_table'], key['referenced_table'])]\n            d['referencing_table'] = key['referencing_table']\n            d['referenced_table'] = key['referenced_table']\n            d['attr_map'][key['column_name']] = key['referenced_column_name']\n\n        # add edges to the graph\n        for fk in fks.values():\n            props = dict(\n                primary=all(attr in pks[fk['referencing_table']] for attr in fk['attr_map']),\n                attr_map=fk['attr_map'],\n                aliased=any(k != v for k, v in fk['attr_map'].items()),\n                multi=not all(a in fk['attr_map'] for a in pks[fk['referencing_table']]))\n            if not props['aliased']:\n                self.add_edge(fk['referenced_table'], fk['referencing_table'], **props)\n            else:\n                # for aliased dependencies, add an extra node in the format '1', '2', etc\n                alias_node = '%d' % next(self._node_alias_count)\n                self.add_node(alias_node)\n                self.add_edge(fk['referenced_table'], alias_node, **props)\n                self.add_edge(alias_node, fk['referencing_table'], **props)\n\n        if not nx.is_directed_acyclic_graph(self):  # pragma: no cover\n            raise DataJointError('DataJoint can only work with acyclic dependencies')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parents(self, table_name, primary=None):\n        return dict(p[::2] for p in self.in_edges(table_name, data=True)\n                    if primary is None or p[2]['primary'] == primary)", "response": "returns a dict of tables referenced by the foreign keys of the table_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef children(self, table_name, primary=None):\n        return dict(p[1:3] for p in self.out_edges(table_name, data=True)\n                    if primary is None or p[2]['primary'] == primary)", "response": "Returns a dict of tables referencing the table through foreign keys."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all tables that are descendants of the given full_table_name.", "response": "def descendants(self, full_table_name):\n        \"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        nodes = self.subgraph(\n            nx.algorithms.dag.descendants(self, full_table_name))\n\n        return [full_table_name] + list(\n            nx.algorithms.dag.topological_sort(nodes))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all ancestors of the given full_table_name.", "response": "def ancestors(self, full_table_name):\n        \"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        nodes = self.subgraph(\n            nx.algorithms.dag.ancestors(self, full_table_name))\n        return [full_table_name] + list(reversed(list(\n            nx.algorithms.dag.topological_sort(nodes))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean(self, exclude, max_count=None):\n        return self.client.remove_objects(self.bucket, itertools.islice(\n            (x.object_name for x in self.client.list_objects(self.bucket, self.remote_path + '/')\n             if x not in exclude), max_count))", "response": "Delete all objects except for those in the exclude\n        list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, filename, verbose=False):\n        with open(filename, 'w') as fid:\n            json.dump(self._conf, fid, indent=4)\n        if verbose:\n            print('Saved settings in ' + filename)", "response": "Saves the current settings in JSON format to the given file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the current configuration from a local JSON file.", "response": "def load(self, filename):\n        \"\"\"\n        Updates the setting from config file in JSON format.\n        :param filename: filename of the local JSON settings file. If None, the local config file is used.\n        \"\"\"\n        if filename is None:\n            filename = LOCALCONFIG\n        with open(filename, 'r') as fid:\n            self._conf.update(json.load(fid))\n        self.add_history('Updated from config file: %s' % filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_global(self, verbose=False):\n        self.save(os.path.expanduser(os.path.join('~', GLOBALCONFIG)), verbose)", "response": "Saves the current configuration in the global config file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert namedtuple to dict.", "response": "def todict(self):\n        \"\"\"Convert namedtuple to dict.\"\"\"\n        return OrderedDict((name, self[i]) for i, name in enumerate(self._fields))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the SQL code for the object.", "response": "def sql(self):\n        \"\"\"\n        Convert primary key attribute tuple into its SQL CREATE TABLE clause.\n        Default values are not reflected.\n        This is used for declaring foreign keys in referencing tables\n        :return: SQL code\n        \"\"\"\n        assert self.in_key and not self.nullable   # primary key attributes are never nullable\n        return '`{name}` {type} NOT NULL COMMENT \"{comment}\"'.format(\n            name=self.name, type=self.type, comment=self.comment)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_dtype(self):\n        return np.dtype(dict(\n            names=self.names,\n            formats=[v.dtype for v in self.attributes.values()]))", "response": "represent the heading as a numpy dtype"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string representation of the table as SQL", "response": "def as_sql(self):\n        \"\"\"\n        represent heading as SQL field list\n        \"\"\"\n        return ','.join('`%s`' % name if self.attributes[name].sql_expression is None\n                        else '%s as `%s`' % (self.attributes[name].sql_expression, name)\n                        for name in self.names)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_from_database(self, conn, database, table_name):\n        info = conn.query('SHOW TABLE STATUS FROM `{database}` WHERE name=\"{table_name}\"'.format(\n            table_name=table_name, database=database), as_dict=True).fetchone()\n        if info is None:\n            if table_name == '~log':\n                logger.warning('Could not create the ~log table')\n                return\n            else:\n                raise DataJointError('The table `{database}`.`{table_name}` is not defined.'.format(\n                    table_name=table_name, database=database))\n        self.table_info = {k.lower(): v for k, v in info.items()}\n\n        cur = conn.query(\n            'SHOW FULL COLUMNS FROM `{table_name}` IN `{database}`'.format(\n                table_name=table_name, database=database), as_dict=True)\n\n        attributes = cur.fetchall()\n\n        rename_map = {\n            'Field': 'name',\n            'Type': 'type',\n            'Null': 'nullable',\n            'Default': 'default',\n            'Key': 'in_key',\n            'Comment': 'comment'}\n\n        fields_to_drop = ('Privileges', 'Collation')\n\n        # rename and drop attributes\n        attributes = [{rename_map[k] if k in rename_map else k: v\n                       for k, v in x.items() if k not in fields_to_drop}\n                      for x in attributes]\n\n        numeric_types = {\n            ('float', False): np.float64,\n            ('float', True): np.float64,\n            ('double', False): np.float64,\n            ('double', True): np.float64,\n            ('tinyint', False): np.int64,\n            ('tinyint', True): np.int64,\n            ('smallint', False): np.int64,\n            ('smallint', True): np.int64,\n            ('mediumint', False): np.int64,\n            ('mediumint', True): np.int64,\n            ('int', False): np.int64,\n            ('int', True): np.int64,\n            ('bigint', False): np.int64,\n            ('bigint', True): np.uint64}\n\n        sql_literals = ['CURRENT_TIMESTAMP']\n\n        # additional attribute properties\n        for attr in attributes:\n            # process external attributes\n            split_comment = attr['comment'].split(':')\n            attr['is_external'] = len(split_comment) >= 3 and split_comment[1].startswith('external')\n            if attr['is_external']:\n                attr['comment'] = ':'.join(split_comment[2:])\n                attr['type'] = split_comment[1]\n\n            attr['nullable'] = (attr['nullable'] == 'YES')\n            attr['in_key'] = (attr['in_key'] == 'PRI')\n            attr['autoincrement'] = bool(re.search(r'auto_increment', attr['Extra'], flags=re.IGNORECASE))\n            attr['type'] = re.sub(r'int\\(\\d+\\)', 'int', attr['type'], count=1)   # strip size off integers\n            attr['numeric'] = bool(re.match(r'(tiny|small|medium|big)?int|decimal|double|float', attr['type']))\n            attr['string'] = bool(re.match(r'(var)?char|enum|date|year|time|timestamp', attr['type']))\n            attr['is_blob'] = attr['is_external'] or bool(re.match(r'(tiny|medium|long)?blob', attr['type']))\n            attr['database'] = database\n\n            if attr['string'] and attr['default'] is not None and attr['default'] not in sql_literals:\n                attr['default'] = '\"%s\"' % attr['default']\n\n            if attr['nullable']:   # nullable fields always default to null\n                attr['default'] = 'null'\n\n            attr['sql_expression'] = None\n            if not (attr['numeric'] or attr['string'] or attr['is_blob']):\n                raise DataJointError('Unsupported field type {field} in `{database}`.`{table_name}`'.format(\n                    field=attr['type'], database=database, table_name=table_name))\n            attr.pop('Extra')\n\n            # fill out dtype. All floats and non-nullable integers are turned into specific dtypes\n            attr['dtype'] = object\n            if attr['numeric']:\n                is_integer = bool(re.match(r'(tiny|small|medium|big)?int', attr['type']))\n                is_float = bool(re.match(r'(double|float)', attr['type']))\n                if is_integer and not attr['nullable'] or is_float:\n                    is_unsigned = bool(re.match('\\sunsigned', attr['type'], flags=re.IGNORECASE))\n                    t = attr['type']\n                    t = re.sub(r'\\(.*\\)', '', t)    # remove parentheses\n                    t = re.sub(r' unsigned$', '', t)   # remove unsigned\n                    assert (t, is_unsigned) in numeric_types, 'dtype not found for type %s' % t\n                    attr['dtype'] = numeric_types[(t, is_unsigned)]\n        self.attributes = OrderedDict([(q['name'], Attribute(**q)) for q in attributes])\n\n        # Read and tabulate secondary indexes\n        keys = defaultdict(dict)\n        for item in conn.query('SHOW KEYS FROM `{db}`.`{tab}`'.format(db=database, tab=table_name), as_dict=True):\n            if item['Key_name'] != 'PRIMARY':\n                keys[item['Key_name']][item['Seq_in_index']] = dict(\n                    column=item['Column_name'],\n                    unique=(item['Non_unique'] == 0),\n                    nullable=item['Null'].lower() == 'yes')\n        self.indexes = {\n            tuple(item[k]['column'] for k in sorted(item.keys())):\n                dict(unique=item[1]['unique'],\n                     nullable=any(v['nullable'] for v in item.values()))\n            for item in keys.values()}", "response": "Initialize the object from a database table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new Heading object that is a copy of the current Heading with the attributes of the existing attributes.", "response": "def project(self, attribute_list, named_attributes=None, force_primary_key=None):\n        \"\"\"\n        derive a new heading by selecting, renaming, or computing attributes.\n        In relational algebra these operators are known as project, rename, and extend.\n        :param attribute_list:  the full list of existing attributes to include\n        :param force_primary_key:  attributes to force to be converted to primary\n        :param named_attributes:  dictionary of renamed attributes\n        \"\"\"\n        try:  # check for missing attributes\n            raise DataJointError('Attribute `%s` is not found' % next(a for a in attribute_list if a not in self.names))\n        except StopIteration:\n            if named_attributes is None:\n                named_attributes = {}\n            if force_primary_key is None:\n                force_primary_key = set()\n            rename_map = {v: k for k, v in named_attributes.items() if v in self.attributes}\n\n            # copied and renamed attributes\n            copy_attrs = (dict(self.attributes[k].todict(),\n                               in_key=self.attributes[k].in_key or k in force_primary_key,\n                               **({'name': rename_map[k], 'sql_expression': '`%s`' % k} if k in rename_map else {}))\n                          for k in self.attributes if k in rename_map or k in attribute_list)\n            compute_attrs = (dict(default_attribute_properties, name=new_name, sql_expression=expr)\n                             for new_name, expr in named_attributes.items() if expr not in rename_map)\n\n            return Heading(chain(copy_attrs, compute_attrs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef join(self, other):\n        return Heading(\n            [self.attributes[name].todict() for name in self.primary_key] +\n            [other.attributes[name].todict() for name in other.primary_key if name not in self.primary_key] +\n            [self.attributes[name].todict() for name in self.dependent_attributes if name not in other.primary_key] +\n            [other.attributes[name].todict() for name in other.dependent_attributes if name not in self.primary_key])", "response": "Join two headings into a new one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_subquery_heading(self):\n        return Heading(dict(v.todict(), sql_expression=None) for v in self.attributes.values())", "response": "Create a new heading with removed attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new heading in which the primary key also includes new_attributes.", "response": "def extend_primary_key(self, new_attributes):\n        \"\"\"\n        Create a new heading in which the primary key also includes new_attributes.\n        :param new_attributes: new attributes to be added to the primary key.\n        \"\"\"\n        try:  # check for missing attributes\n            raise DataJointError('Attribute `%s` is not found' % next(a for a in new_attributes if a not in self.names))\n        except StopIteration:\n            return Heading(dict(v.todict(), in_key=v.in_key or v.name in new_attributes)\n                           for v in self.attributes.values())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pack_dict(obj):\n    obj = OrderedDict(obj)\n    blob = b'S'\n    blob += np.array((1, 1), dtype=np.uint64).tostring()\n    blob += np.array(len(obj), dtype=np.uint32).tostring()\n\n    # write out field names\n    for k in obj:\n        blob += pack_string(k)\n\n    for k, v in obj.items():\n        blob_part = pack_obj(v)\n        blob += np.array(len(blob_part), dtype=np.uint64).tostring()\n        blob += blob_part\n\n    return blob", "response": "Serializes a dictionary into a singular structure array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef squeeze(self, array):\n        if not self._squeeze:\n            return array\n        array = array.copy()\n        array = array.squeeze()\n        if array.ndim == 0:\n            array = array[()]\n        return array", "response": "Simplify the given array as much as possible - squeeze out all singletons and convert a zero dimensional array into array scalar\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a string from the internal buffer.", "response": "def read_string(self, advance=True):\n        \"\"\"\n        Read a string terminated by null byte '\\0'. The returned string\n        object is ASCII decoded, and will not include the terminating null byte.\n        \"\"\"\n        target = self._blob.find(b'\\0', self.pos)\n        assert target >= self._pos\n        data = self._blob[self._pos:target]\n        if advance:\n            self._pos = target + 1\n        return data.decode('ascii')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_value(self, dtype='uint64', count=1, advance=True):\n        data = np.frombuffer(self._blob, dtype=dtype, count=count, offset=self.pos)\n        if advance:\n            # probably the same thing as data.nbytes * 8\n            self._pos += data.dtype.itemsize * data.size\n        if count == 1:\n            data = data[0]\n        return data", "response": "Read one or more scalars of the indicated dtype."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nputting an object in an external store", "response": "def put(self, store, obj):\n        \"\"\"\n        put an object in external store\n        \"\"\"\n        spec = self._get_store_spec(store)\n        blob = pack(obj)\n        blob_hash = long_hash(blob) + store[len('external-'):]\n        if spec['protocol'] == 'file':\n            folder = os.path.join(spec['location'], self.database)\n            full_path = os.path.join(folder, blob_hash)\n            if not os.path.isfile(full_path):\n                try:\n                    safe_write(full_path, blob)\n                except FileNotFoundError:\n                    os.makedirs(folder)\n                    safe_write(full_path, blob)\n        elif spec['protocol'] == 's3':\n            S3Folder(database=self.database, **spec).put(blob_hash, blob)\n        else:\n            raise DataJointError('Unknown external storage protocol {protocol} for {store}'.format(\n                store=store, protocol=spec['protocol']))\n\n        # insert tracking info\n        self.connection.query(\n            \"INSERT INTO {tab} (hash, size) VALUES ('{hash}', {size}) \"\n            \"ON DUPLICATE KEY UPDATE timestamp=CURRENT_TIMESTAMP\".format(\n                tab=self.full_table_name,\n                hash=blob_hash,\n                size=len(blob)))\n        return blob_hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, blob_hash):\n        if blob_hash is None:\n            return None\n        store = blob_hash[STORE_HASH_LENGTH:]\n        store = 'external' + ('-' if store else '') + store\n\n        cache_folder = config.get('cache', None)\n\n        blob = None\n        if cache_folder:\n            try:\n                with open(os.path.join(cache_folder, blob_hash), 'rb') as f:\n                    blob = f.read()\n            except FileNotFoundError:\n                pass\n\n        if blob is None:\n            spec = self._get_store_spec(store)\n            if spec['protocol'] == 'file':\n                full_path = os.path.join(spec['location'], self.database, blob_hash)\n                try:\n                    with open(full_path, 'rb') as f:\n                        blob = f.read()\n                except FileNotFoundError:\n                    raise DataJointError('Lost access to external blob %s.' % full_path) from None\n            elif spec['protocol'] == 's3':\n                try:\n                    blob = S3Folder(database=self.database, **spec).get(blob_hash)\n                except TypeError:\n                    raise DataJointError('External store {store} configuration is incomplete.'.format(store=store))\n            else:\n                raise DataJointError('Unknown external storage protocol \"%s\"' % spec['protocol'])\n\n            if cache_folder:\n                if not os.path.exists(cache_folder):\n                    os.makedirs(cache_folder)\n                safe_write(os.path.join(cache_folder, blob_hash), blob)\n\n        return unpack(blob)", "response": "Get an object from an external store."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef references(self):\n        return self.connection.query(\"\"\"\n        SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name\n        FROM information_schema.key_column_usage\n        WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\"\n        \"\"\".format(tab=self.table_name, db=self.database), as_dict=True)", "response": "Returns a generator of table names and their referencing columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_garbage(self):\n        self.connection.query(\n            \"DELETE FROM `{db}`.`{tab}` WHERE \".format(tab=self.table_name, db=self.database) +\n            \" AND \".join(\n                'hash NOT IN (SELECT {column_name} FROM {referencing_table})'.format(**ref)\n                for ref in self.references) or \"TRUE\")\n        print('Deleted %d items' % self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0])", "response": "Delete items that are no longer referenced."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_store(self, store, display_progress=True):\n        spec = self._get_store_spec(store)\n        progress = tqdm if display_progress else lambda x: x\n        if spec['protocol'] == 'file':\n            folder = os.path.join(spec['location'], self.database)\n            delete_list = set(os.listdir(folder)).difference(self.fetch('hash'))\n            print('Deleting %d unused items from %s' % (len(delete_list), folder), flush=True)\n            for f in progress(delete_list):\n                os.remove(os.path.join(folder, f))\n        elif spec['protocol'] == 's3':\n            try:\n                S3Folder(database=self.database, **spec).clean(self.fetch('hash'))\n            except TypeError:\n                raise DataJointError('External store {store} configuration is incomplete.'.format(store=store))", "response": "Clean unused data in an external storage repository from unused blobs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if error e pertains to a connection issue", "response": "def is_connection_error(e):\n    \"\"\"\n    Checks if error e pertains to a connection issue\n    \"\"\"\n    return (isinstance(e, err.InterfaceError) and e.args[0] == \"(0, '')\") or\\\n        (isinstance(e, err.OperationalError) and e.args[0] in operation_error_codes.values())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef key_source(self):\n        def parent_gen(self):\n            if self.target.full_table_name not in self.connection.dependencies:\n                self.connection.dependencies.load()\n            for parent_name, fk_props in self.target.parents(primary=True).items():\n                if not parent_name.isdigit():  # simple foreign key\n                    yield FreeTable(self.connection, parent_name).proj()\n                else:\n                    grandparent = list(self.connection.dependencies.in_edges(parent_name))[0][0]\n                    yield FreeTable(self.connection, grandparent).proj(**{\n                        attr: ref for attr, ref in fk_props['attr_map'].items() if ref != attr})\n\n        if self._key_source is None:\n            parents = parent_gen(self)\n            try:\n                self._key_source = next(parents)\n            except StopIteration:\n                raise DataJointError('A relation must have primary dependencies for auto-populate to work') from None\n            for q in parents:\n                self._key_source *= q\n        return self._key_source", "response": "Returns the relation whose primary key values are passed sequentially to the\n                make method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the relation containing the keys to be computed based on the keys in the target table.", "response": "def _jobs_to_do(self, restrictions):\n        \"\"\"\n        :return: the relation containing the keys to be computed (derived from self.key_source)\n        \"\"\"\n        if self.restriction:\n            raise DataJointError('Cannot call populate on a restricted table. '\n                                 'Instead, pass conditions to populate() as arguments.')\n        todo = self.key_source\n        if not isinstance(todo, QueryExpression):\n            raise DataJointError('Invalid key_source value')\n        # check if target lacks any attributes from the primary key of key_source\n        try:\n            raise DataJointError(\n                'The populate target lacks attribute %s from the primary key of key_source' % next(\n                    name for name in todo.heading.primary_key if name not in self.target.heading))\n        except StopIteration:\n            pass\n        return (todo & AndList(restrictions)).proj()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreporting the progress of populating the table", "response": "def progress(self, *restrictions, display=True):\n        \"\"\"\n        report progress of populating the table\n        :return: remaining, total -- tuples to be populated\n        \"\"\"\n        todo = self._jobs_to_do(restrictions)\n        total = len(todo)\n        remaining = len(todo - self.target)\n        if display:\n            print('%-20s' % self.__class__.__name__,\n                  'Completed %d of %d (%2.1f%%)   %s' % (\n                      total - remaining, total, 100 - 100 * remaining / (total+1e-12),\n                      datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S')), flush=True)\n        return remaining, total"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkilling all processes in the processlist.", "response": "def kill(restriction=None, connection=None):  # pragma: no cover\n    \"\"\"\n    view and kill database connections.\n    :param restriction: restriction to be applied to processlist\n    :param connection: a datajoint.Connection object. Default calls datajoint.conn()\n\n    Restrictions are specified as strings and can involve any of the attributes of\n    information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.\n\n    Examples:\n        dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\".\n        dj.kill('TIME > 600') lists only connections older than 10 minutes.\n    \"\"\"\n\n    if connection is None:\n        connection = conn()\n\n    query = 'SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()' + (\n        \"\" if restriction is None else ' AND (%s)' % restriction)\n\n    while True:\n        print('  ID USER         STATE         TIME  INFO')\n        print('+--+ +----------+ +-----------+ +--+')\n        cur = connection.query(query, as_dict=True)\n        for process in cur:\n            try:\n                print('{ID:>4d} {USER:<12s} {STATE:<12s} {TIME:>5d}  {INFO}'.format(**process))\n            except TypeError:\n                print(process)\n        response = input('process to kill or \"q\" to quit > ')\n        if response == 'q':\n            break\n        if response:\n            try:\n                pid = int(response)\n            except ValueError:\n                pass  # ignore non-numeric input\n            else:\n                try:\n                    connection.query('kill %d' % pid)\n                except pymysql.err.InternalError:\n                    print('Process not found')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef table_name(cls):\n        if cls._prefix is None:\n            raise AttributeError('Class prefix is not defined!')\n        return cls._prefix + from_camel_case(cls.__name__)", "response": "Returns the table name of the mysql table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecompress a complete Brotli - compressed string.", "response": "def decompress(data):\n    \"\"\"\n    Decompress a complete Brotli-compressed string.\n\n    :param data: A bytestring containing Brotli-compressed data.\n    \"\"\"\n    d = Decompressor()\n    data = d.decompress(data)\n    d.finish()\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compress(data,\n             mode=DEFAULT_MODE,\n             quality=lib.BROTLI_DEFAULT_QUALITY,\n             lgwin=lib.BROTLI_DEFAULT_WINDOW,\n             lgblock=0,\n             dictionary=b''):\n    \"\"\"\n    Compress a string using Brotli.\n\n    .. versionchanged:: 0.5.0\n       Added ``mode``, ``quality``, `lgwin``, ``lgblock``, and ``dictionary``\n       parameters.\n\n    :param data: A bytestring containing the data to compress.\n    :type data: ``bytes``\n\n    :param mode: The encoder mode.\n    :type mode: :class:`BrotliEncoderMode` or ``int``\n\n    :param quality: Controls the compression-speed vs compression-density\n        tradeoffs. The higher the quality, the slower the compression. The\n        range of this value is 0 to 11.\n    :type quality: ``int``\n\n    :param lgwin: The base-2 logarithm of the sliding window size. The range of\n        this value is 10 to 24.\n    :type lgwin: ``int``\n\n    :param lgblock: The base-2 logarithm of the maximum input block size. The\n        range of this value is 16 to 24. If set to 0, the value will be set\n        based on ``quality``.\n    :type lgblock: ``int``\n\n    :param dictionary: A pre-set dictionary for LZ77. Please use this with\n        caution: if a dictionary is used for compression, the same dictionary\n        **must** be used for decompression!\n    :type dictionary: ``bytes``\n\n    :returns: The compressed bytestring.\n    :rtype: ``bytes``\n    \"\"\"\n    # This method uses private variables on the Compressor object, and\n    # generally does a whole lot of stuff that's not supported by the public\n    # API. The goal here is to minimise the number of allocations and copies\n    # we have to do. Users should prefer this method over the Compressor if\n    # they know they have single-shot data.\n    compressor = Compressor(\n        mode=mode,\n        quality=quality,\n        lgwin=lgwin,\n        lgblock=lgblock,\n        dictionary=dictionary\n    )\n    compressed_data = compressor._compress(data, lib.BROTLI_OPERATION_FINISH)\n    assert lib.BrotliEncoderIsFinished(compressor._encoder) == lib.BROTLI_TRUE\n    assert (\n        lib.BrotliEncoderHasMoreOutput(compressor._encoder) == lib.BROTLI_FALSE\n    )\n    return compressed_data", "response": "Compress a string using Brotli."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_parameter(encoder, parameter, parameter_name, val):\n    rc = lib.BrotliEncoderSetParameter(encoder, parameter, val)\n\n    if parameter == lib.BROTLI_PARAM_MODE:\n        _validate_mode(val)\n    elif parameter == lib.BROTLI_PARAM_QUALITY:\n        _validate_quality(val)\n    elif parameter == lib.BROTLI_PARAM_LGWIN:\n        _validate_lgwin(val)\n    elif parameter == lib.BROTLI_PARAM_LGBLOCK:\n        _validate_lgblock(val)\n    else:  # pragma: no cover\n        raise RuntimeError(\"Unexpected parameter!\")\n\n    # This block is defensive: I see no way to hit it, but as long as the\n    # function returns a value we can live in hope that the brotli folks will\n    # enforce their own constraints.\n    if rc != lib.BROTLI_TRUE:  # pragma: no cover\n        raise Error(\n            \"Error setting parameter %s: %d\" % (parameter_name, val)\n        )", "response": "This helper function sets a specific parameter in the encoder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compress(self, data, operation):\n        # The 'algorithm' for working out how big to make this buffer is from\n        # the Brotli source code, brotlimodule.cc.\n        original_output_size = int(\n            math.ceil(len(data) + (len(data) >> 2) + 10240)\n        )\n        available_out = ffi.new(\"size_t *\")\n        available_out[0] = original_output_size\n        output_buffer = ffi.new(\"uint8_t []\", available_out[0])\n        ptr_to_output_buffer = ffi.new(\"uint8_t **\", output_buffer)\n        input_size = ffi.new(\"size_t *\", len(data))\n        input_buffer = ffi.new(\"uint8_t []\", data)\n        ptr_to_input_buffer = ffi.new(\"uint8_t **\", input_buffer)\n\n        rc = lib.BrotliEncoderCompressStream(\n            self._encoder,\n            operation,\n            input_size,\n            ptr_to_input_buffer,\n            available_out,\n            ptr_to_output_buffer,\n            ffi.NULL\n        )\n        if rc != lib.BROTLI_TRUE:  # pragma: no cover\n            raise Error(\"Error encountered compressing data.\")\n\n        assert not input_size[0]\n\n        size_of_output = original_output_size - available_out[0]\n        return ffi.buffer(output_buffer, size_of_output)[:]", "response": "This private method compresses some data in a given mode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flush(self):\n        chunks = []\n        chunks.append(self._compress(b'', lib.BROTLI_OPERATION_FLUSH))\n\n        while lib.BrotliEncoderHasMoreOutput(self._encoder) == lib.BROTLI_TRUE:\n            chunks.append(self._compress(b'', lib.BROTLI_OPERATION_FLUSH))\n\n        return b''.join(chunks)", "response": "Flush the compressor. This will emit the remaining output data, but\n        will not destroy the compressor. It can be used, for example, to ensure\n        that given chunks of content will decompress immediately."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef finish(self):\n        chunks = []\n        while lib.BrotliEncoderIsFinished(self._encoder) == lib.BROTLI_FALSE:\n            chunks.append(self._compress(b'', lib.BROTLI_OPERATION_FINISH))\n\n        return b''.join(chunks)", "response": "Finish the compressor. This will emit the remaining output data and\n        transition the compressor to a completed state. The compressor cannot\n        be used again after this point, and must be replaced."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecompresses part of a complete Brotli - compressed string.", "response": "def decompress(self, data):\n        \"\"\"\n        Decompress part of a complete Brotli-compressed string.\n\n        :param data: A bytestring containing Brotli-compressed data.\n        :returns: A bytestring containing the decompressed data.\n        \"\"\"\n        chunks = []\n\n        available_in = ffi.new(\"size_t *\", len(data))\n        in_buffer = ffi.new(\"uint8_t[]\", data)\n        next_in = ffi.new(\"uint8_t **\", in_buffer)\n\n        while True:\n            # Allocate a buffer that's hopefully overlarge, but if it's not we\n            # don't mind: we'll spin around again.\n            buffer_size = 5 * len(data)\n            available_out = ffi.new(\"size_t *\", buffer_size)\n            out_buffer = ffi.new(\"uint8_t[]\", buffer_size)\n            next_out = ffi.new(\"uint8_t **\", out_buffer)\n\n            rc = lib.BrotliDecoderDecompressStream(self._decoder,\n                                                   available_in,\n                                                   next_in,\n                                                   available_out,\n                                                   next_out,\n                                                   ffi.NULL)\n\n            # First, check for errors.\n            if rc == lib.BROTLI_DECODER_RESULT_ERROR:\n                error_code = lib.BrotliDecoderGetErrorCode(self._decoder)\n                error_message = lib.BrotliDecoderErrorString(error_code)\n                raise Error(\n                    \"Decompression error: %s\" % ffi.string(error_message)\n                )\n\n            # Next, copy the result out.\n            chunk = ffi.buffer(out_buffer, buffer_size - available_out[0])[:]\n            chunks.append(chunk)\n\n            if rc == lib.BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT:\n                assert available_in[0] == 0\n                break\n            elif rc == lib.BROTLI_DECODER_RESULT_SUCCESS:\n                break\n            else:\n                # It's cool if we need more output, we just loop again.\n                assert rc == lib.BROTLI_DECODER_RESULT_NEEDS_MORE_OUTPUT\n\n        return b''.join(chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinishes the decompressor. As the decompressor decompresses eagerly, this will never actually emit any data. However, it will potentially throw errors if a truncated or damaged data stream has been used. Note that, once this method is called, the decompressor is no longer safe for further use and must be thrown away.", "response": "def finish(self):\n        \"\"\"\n        Finish the decompressor. As the decompressor decompresses eagerly, this\n        will never actually emit any data. However, it will potentially throw\n        errors if a truncated or damaged data stream has been used.\n\n        Note that, once this method is called, the decompressor is no longer\n        safe for further use and must be thrown away.\n        \"\"\"\n        assert (\n            lib.BrotliDecoderHasMoreOutput(self._decoder) == lib.BROTLI_FALSE\n        )\n        if lib.BrotliDecoderIsFinished(self._decoder) == lib.BROTLI_FALSE:\n            raise Error(\"Decompression error: incomplete compressed stream.\")\n\n        return b''"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a tuple of hue saturation value to a tuple of red green blue.", "response": "def hsv_to_rgb(hsv):\n    \"\"\"Converts a tuple of hue, saturation, value to a tuple of red, green blue.\n    Hue should be an angle from 0.0 to 359.0.  Saturation and value should be a\n    value from 0.0 to 1.0, where saturation controls the intensity of the hue and\n    value controls the brightness.\n    \"\"\"\n    # Algorithm adapted from http://www.cs.rit.edu/~ncs/color/t_convert.html\n    h, s, v = hsv\n    if s == 0:\n        return (v, v, v)\n    h /= 60.0\n    i = math.floor(h)\n    f = h-i\n    p = v*(1.0-s)\n    q = v*(1.0-s*f)\n    t = v*(1.0-s*(1.0-f))\n    if i == 0:\n        return (v, t, p)\n    elif i == 1:\n        return (q, v, p)\n    elif i == 2:\n        return (p, v, t)\n    elif i == 3:\n        return (p, q, v)\n    elif i == 4:\n        return (t, p, v)\n    else:\n        return (v, p, q)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_cursor(self, col, row):\n        # Clamp row to the last row of the display.\n        if row > self._lines:\n            row = self._lines - 1\n        # Set location.\n        self.write8(LCD_SETDDRAMADDR | (col + LCD_ROW_OFFSETS[row]))", "response": "Move the cursor to an explicit column and row position."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable_display(self, enable):\n        if enable:\n            self.displaycontrol |= LCD_DISPLAYON\n        else:\n            self.displaycontrol &= ~LCD_DISPLAYON\n        self.write8(LCD_DISPLAYCONTROL | self.displaycontrol)", "response": "Enable or disable the display. Set enable to True to enable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_cursor(self, show):\n        if show:\n            self.displaycontrol |= LCD_CURSORON\n        else:\n            self.displaycontrol &= ~LCD_CURSORON\n        self.write8(LCD_DISPLAYCONTROL | self.displaycontrol)", "response": "Show or hide the cursor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef blink(self, blink):\n        if blink:\n            self.displaycontrol |= LCD_BLINKON\n        else:\n            self.displaycontrol &= ~LCD_BLINKON\n        self.write8(LCD_DISPLAYCONTROL | self.displaycontrol)", "response": "Turn on or off cursor blinking."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset text direction left to right.", "response": "def set_left_to_right(self):\n        \"\"\"Set text direction left to right.\"\"\"\n        self.displaymode |= LCD_ENTRYLEFT\n        self.write8(LCD_ENTRYMODESET | self.displaymode)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_right_to_left(self):\n        self.displaymode &= ~LCD_ENTRYLEFT\n        self.write8(LCD_ENTRYMODESET | self.displaymode)", "response": "Set text direction right to left."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets autoscroll to True.", "response": "def autoscroll(self, autoscroll):\n        \"\"\"Autoscroll will 'right justify' text from the cursor if set True,\n        otherwise it will 'left justify' the text.\n        \"\"\"\n        if autoscroll:\n            self.displaymode |= LCD_ENTRYSHIFTINCREMENT\n        else:\n            self.displaymode &= ~LCD_ENTRYSHIFTINCREMENT\n        self.write8(LCD_ENTRYMODESET | self.displaymode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting text to display.", "response": "def message(self, text):\n        \"\"\"Write text to display.  Note that text can include newlines.\"\"\"\n        line = 0\n        # Iterate through each character.\n        for char in text:\n            # Advance to next line if character is a new line.\n            if char == '\\n':\n                line += 1\n                # Move to left or right side depending on text direction.\n                col = 0 if self.displaymode & LCD_ENTRYLEFT > 0 else self._cols-1\n                self.set_cursor(col, line)\n            # Write the character to the display.\n            else:\n                self.write8(ord(char), True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_backlight(self, backlight):\n        if self._backlight is not None:\n            if self._pwm_enabled:\n                self._pwm.set_duty_cycle(self._backlight, self._pwm_duty_cycle(backlight))\n            else:\n                self._gpio.output(self._backlight, self._blpol if backlight else not self._blpol)", "response": "Enable or disable the backlight."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites 8 - bit value in character or data mode.", "response": "def write8(self, value, char_mode=False):\n        \"\"\"Write 8-bit value in character or data mode.  Value should be an int\n        value from 0-255, and char_mode is True if character data or False if\n        non-character data (default).\n        \"\"\"\n        # One millisecond delay to prevent writing too quickly.\n        self._delay_microseconds(1000)\n        # Set character / data bit.\n        self._gpio.output(self._rs, char_mode)\n        # Write upper 4 bits.\n        self._gpio.output_pins({ self._d4: ((value >> 4) & 1) > 0,\n                                 self._d5: ((value >> 5) & 1) > 0,\n                                 self._d6: ((value >> 6) & 1) > 0,\n                                 self._d7: ((value >> 7) & 1) > 0 })\n        self._pulse_enable()\n        # Write lower 4 bits.\n        self._gpio.output_pins({ self._d4: (value        & 1) > 0,\n                                 self._d5: ((value >> 1) & 1) > 0,\n                                 self._d6: ((value >> 2) & 1) > 0,\n                                 self._d7: ((value >> 3) & 1) > 0 })\n        self._pulse_enable()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_char(self, location, pattern):\n        # only position 0..7 are allowed\n        location &= 0x7\n        self.write8(LCD_SETCGRAMADDR | (location << 3))\n        for i in range(8):\n            self.write8(pattern[i], char_mode=True)", "response": "Fill one of the first 8 CGRAM locations with custom characters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_color(self, red, green, blue):\n        if self._pwm_enabled:\n            # Set duty cycle of PWM pins.\n            rdc, gdc, bdc = self._rgb_to_duty_cycle((red, green, blue))\n            self._pwm.set_duty_cycle(self._red, rdc)\n            self._pwm.set_duty_cycle(self._green, gdc)\n            self._pwm.set_duty_cycle(self._blue, bdc)\n        else:\n            # Set appropriate backlight pins based on polarity and enabled colors.\n            self._gpio.output_pins({self._red:   self._blpol if red else not self._blpol,\n                                    self._green: self._blpol if green else not self._blpol,\n                                    self._blue:  self._blpol if blue else not self._blpol })", "response": "Sets the color of the current object to provided red green and blue values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_pressed(self, button):\n        if button not in set((SELECT, RIGHT, DOWN, UP, LEFT)):\n            raise ValueError('Unknown button, must be SELECT, RIGHT, DOWN, UP, or LEFT.')\n        return self._mcp.input(button) == GPIO.LOW", "response": "Return True if the provided button is pressed False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions to obtain the Dictionary that represents this object.", "response": "def getResults(self, parFound = None):\n        ''' \n            Function to obtain the Dictionarythat represents this object.\n            \n            :param parFound:    values to return.\n\n            :return:    The output format will be like:\n                [{\"type\" : \"i3visio.email\", \"value\": \"foo@bar.com\", \"attributes\": [] }, {\"type\" : \"i3visio.email\", \"value\": \"bar@foo.com\", \"attributes\": [] }]\n        '''\n        # Defining a dictionary\n        results = []\n        # Defining a dictionary inside with a couple of fields: reg_exp for the regular expression and found_exp for the expressions found.\n        #results[self.name] = {\"reg_exp\" : self.reg_exp, \"found_exp\" : parFound}\n        #results[self.name] = parFound\n        if len(parFound ) >0:\n            for found in parFound:\n                aux = {}\n                aux[\"type\"] = self.getEntityType(found)\n                aux[\"value\"] = self.getValue(found)\n                aux[\"attributes\"] = self.getAttributes(found)\n                results.append(aux)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generateTabularData(res, oldTabularData = {}, isTerminal=False, canUnicode=True):\n    def _grabbingNewHeader(h):\n        \"\"\"\n        Updates the headers to be general.\n\n        Changing the starting @ for a '_' and changing the \"i3visio.\" for\n        \"i3visio_\". Changed in 0.9.4+.\n\n        Args:\n        -----\n            h: A header to be sanitised.\n\n        Returns:\n        --------\n            string: The modified header.\n        \"\"\"\n        if h[0] == \"@\":\n            h = h.replace(\"@\", \"_\")\n        elif \"i3visio.\" in h:\n            h = h.replace(\"i3visio.\", \"i3visio_\")\n        return h\n\n    # Entities allowed for the output in terminal\n    allowedInTerminal = [\n        \"i3visio_alias\",\n        \"i3visio_uri\",\n        \"i3visio_platform\",\n        \"i3visio_email\",\n        \"i3visio_ipv4\",\n        \"i3visio_phone\",\n        \"i3visio_dni\",\n        \"i3visio_domain\",\n        \"i3visio_platform_leaked\",\n        #\"_source\"\n    ]\n    # List of profiles found\n    values = {}\n    headers = [\"_id\"]\n    try:\n        if not isTerminal:\n            # Recovering the headers in the first line of the old Data\n            headers = oldTabularData[\"OSRFramework\"][0]\n        else:\n            # Recovering only the printable headers if in Terminal mode\n            oldHeaders = oldTabularData[\"OSRFramework\"][0]\n            headers = []\n            for h in oldHeaders:\n                h = _grabbingNewHeader(h)\n                if h in allowedInTerminal:\n                    # Set to simplify the table shown in mailfy for leaked platforms\n                    if h in [\"i3visio_domain\", \"i3visio_alias\"] and \"_source\" in old_headers:\n                        pass\n                    else:\n                        headers.append(h)\n        # Changing the starting @ for a '_' and changing the \"i3visio.\" for \"i3visio_\". Changed in 0.9.4+\n        for i, h in enumerate(headers):\n            h = _grabbingNewHeader(h)\n            # Replacing the header\n            headers[i] = h\n    except:\n        # No previous files... Easy...\n        headers = [\"_id\"]\n\n    # We are assuming that we received a list of profiles.\n    for p in res:\n        # Creating the dictionaries\n        values[p[\"value\"]] = {}\n        attributes = p[\"attributes\"]\n        # Processing all the attributes found\n        for a in attributes:\n            # Grabbing the type in the new format\n            h = _grabbingNewHeader(a[\"type\"])\n\n            # Default behaviour for the output methods\n            if not isTerminal:\n                values[p[\"value\"]][h] = a[\"value\"]\n                # Appending the column if not already included\n                if str(h) not in headers:\n                    headers.append(str(h))\n            # Specific table construction for the terminal output\n            else:\n                if h in allowedInTerminal:\n                    values[p[\"value\"]][h] = a[\"value\"]\n                    # Appending the column if not already included\n                    if str(h) not in headers:\n                        headers.append(str(h))\n\n    data = {}\n    # Note that each row should be a list!\n    workingSheet = []\n\n    # Appending the headers\n    workingSheet.append(headers)\n\n    # First, we will iterate through the previously stored values\n    try:\n        for dataRow in oldTabularData[\"OSRFramework\"][1:]:\n            # Recovering the previous data\n            newRow = []\n            for cell in dataRow:\n                newRow.append(cell)\n\n            # Now, we will fill the rest of the cells with \"N/A\" values\n            for i in range(len(headers)-len(dataRow)):\n                # Printing a Not Applicable value\n                newRow.append(\"[N/A]\")\n\n            # Appending the newRow to the data structure\n            workingSheet.append(newRow)\n    except Exception, e:\n        # No previous value found!\n        pass\n\n    # After having all the previous data stored an updated... We will go through the rest:\n    for prof in values.keys():\n        # Creating an empty structure\n        newRow = []\n        for i, col in enumerate(headers):\n            try:\n                if col == \"_id\":\n                    newRow.append(len(workingSheet))\n                else:\n                    if canUnicode:\n                        newRow.append(unicode(values[prof][col]))\n                    else:\n                        newRow.append(str(values[prof][col]))\n            except UnicodeEncodeError as e:\n                # Printing that an error was found\n                newRow.append(\"[WARNING: Unicode Encode]\")\n            except:\n                # Printing that this is not applicable value\n                newRow.append(\"[N/A]\")\n        # Appending the newRow to the data structure\n        workingSheet.append(newRow)\n\n    # Storing the workingSheet onto the data structure to be stored\n    data.update({\"OSRFramework\": workingSheet})\n\n    return data", "response": "This function generates the tabular data for the current structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef usufyToCsvExport(d, fPath):\n\n    from pyexcel_io import get_data\n    try:\n        oldData = {\"OSRFramework\": get_data(fPath) }\n    except:\n        # No information has been recovered\n        oldData = {\"OSRFramework\":[]}\n\n    # Generating the new tabular data.\n    tabularData = _generateTabularData(d, oldData)\n\n    from pyexcel_io import save_data\n    # Storing the file\n    # NOTE: when working with CSV files it is no longer a dict because it is a one-sheet-format\n    save_data(fPath, tabularData[\"OSRFramework\"])", "response": "Workaround to export to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef usufyToOdsExport(d, fPath):\n    from pyexcel_ods import get_data\n    try:\n        #oldData = get_data(fPath)\n        # A change in the API now returns only an array of arrays if there is only one sheet.\n        oldData = {\"OSRFramework\": get_data(fPath) }\n    except:\n        # No information has been recovered\n        oldData = {\"OSRFramework\":[]}\n\n    # Generating the new tabular data\n    tabularData = _generateTabularData(d, oldData)\n\n    from pyexcel_ods import save_data\n    # Storing the file\n    save_data(fPath, tabularData)", "response": "Workaround to export to a. ods file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef usufyToXlsxExport(d, fPath):\n    from pyexcel_xlsx import get_data\n    try:\n        #oldData = get_data(fPath)\n        # A change in the API now returns only an array of arrays if there is only one sheet.\n        oldData = {\"OSRFramework\": get_data(fPath) }\n    except:\n        # No information has been recovered\n        oldData = {\"OSRFramework\":[]}\n\n    # Generating the new tabular data\n    tabularData = _generateTabularData(d, oldData)\n\n    from pyexcel_xlsx import save_data\n    # Storing the file\n    save_data(fPath, tabularData)", "response": "Workaround to export to a. xlsx file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the i3visio - like graph data from the given data.", "response": "def _generateGraphData(data, oldData=nx.Graph()):\n    \"\"\"\n    Processing the data from i3visio structures to generate nodes and edges\n\n    This function uses the networkx graph library. It will create a new node\n    for each and i3visio.<something> entities while it will add properties for\n    all the attribute starting with \"@\".\n\n    Args:\n    -----\n        d: The i3visio structures containing a list of\n        oldData: A graph structure representing the previous information.\n\n    Returns:\n    --------\n        A graph structure representing the updated information.\n    \"\"\"\n    def _addNewNode(ent, g):\n        \"\"\"\n        Wraps the creation of a node\n\n        Args:\n        -----\n            ent:   The hi3visio-like entities to be used as the identifier.\n                ent = {\n                    \"value\":\"i3visio\",\n                    \"type\":\"i3visio.alias,\n                }\n            g:   The graph in which the entity will be stored.\n\n        Returns:\n        -------\n            The label used to represent this element.\n        \"\"\"\n        try:\n            label = unicode(ent[\"value\"])\n        except UnicodeEncodeError as e:\n            # Printing that an error was found\n            label = str(ent[\"value\"])\n        g.add_node(label)\n        g.node[label][\"type\"] = ent[\"type\"]\n        return label\n\n    def _processAttributes(elems, g):\n        \"\"\"\n        Function that processes a list of elements to obtain new attributes.\n\n        Args:\n        -----\n            elems: List of i3visio-like entities.\n            g: The graph in which the entity will be stored.\n\n        Returns:\n        --------\n            newAtts: Dict of attributes (to be stored as attributes for the\n                given entity).\n            newEntities: List of new Entities (to be stored as attributes for\n                the given entity).\n        \"\"\"\n        newAtts = {}\n        newEntities= []\n\n        for att in elems:\n            # If it is an attribute\n            if att[\"type\"][0] == \"@\":\n                # Removing the @ and the  _ of the attributes\n                attName = str(att[\"type\"][1:]).replace('_', '')\n                try:\n                    newAtts[attName] = int(att[\"value\"])\n                except:\n                    newAtts[attName] = att[\"value\"]\n            elif att[\"type\"][:8] == \"i3visio.\":\n                # Creating a dict to represent the pair: type, value entity.\n                ent = {\n                    \"value\":att[\"value\"],\n                    \"type\":att[\"type\"].replace(\"i3visio.\", \"i3visio_\"),\n                }\n                # Appending the new Entity to the entity list\n                newEntities.append(ent)\n\n                # Appending the new node\n                hashLabel = _addNewNode(ent, g)\n\n                # Make this recursive to link the attributes in each and every att\n                newAttsInAttributes, newEntitiesInAttributes = _processAttributes(att[\"attributes\"], g)\n\n                # Updating the attributes to the current entity\n                g.node[hashLabel].update(newAttsInAttributes)\n\n                # Creating the edges (the new entities have also been created in the _processAttributes\n                for new in newEntitiesInAttributes:\n                    graphData.add_edge(hashLabel, json.dumps(new))\n                    try:\n                        # Here, we would add the properties of the edge\n                        #graphData.edge[hashLabel][json.dumps(new)][\"@times_seen\"] +=1\n                        pass\n                    except:\n                        # If the attribute does not exist, we would initialize it\n                        #graphData.edge[hashLabel][json.dumps(new)][\"@times_seen\"] = 1\n                        pass\n            else:\n                # An unexpected type\n                pass\n\n        return newAtts, newEntities\n\n    graphData = oldData\n    # Iterating through the results\n    for elem in data:\n        # Creating a dict to represent the pair: type, value entity.\n        ent = {\n            \"value\":elem[\"value\"],\n            \"type\":elem[\"type\"],\n        }\n\n        # Appending the new node\n        new_node = _addNewNode(ent, graphData)\n\n        # Processing the attributes to grab the attributes (starting with \"@...\" and entities)\n        newAtts, newEntities = _processAttributes(elem[\"attributes\"], graphData)\n\n        # Updating the attributes to the current entity\n        graphData.node[new_node].update(newAtts)\n\n        # Creating the edges (the new entities have also been created in the _processAttributes\n        for other_node in newEntities:\n            # Serializing the second entity\n            serEnt = json.dumps(new_node)\n\n            try:\n                other_node = unicode(other_node[\"value\"])\n            except UnicodeEncodeError as e:\n                # Printing that an error was found\n                other_node = str(other_node[\"value\"])\n\n            # Adding the edge\n            graphData.add_edge(new_node, other_node)\n            try:\n                # Here, we would add the properties of the edge\n                #graphData.edge[hashLabel][hashLabelSeconds][\"times_seen\"] +=1\n                pass\n            except:\n                # If the attribute does not exist, we would initialize it\n                #graphData.edge[hashLabel][hashLabelSeconds][\"times_seen\"] = 1\n                pass\n\n    return graphData"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef usufyToGmlExport(d, fPath):\n    # Reading the previous gml file\n    try:\n        oldData=nx.read_gml(fPath)\n    except UnicodeDecodeError as e:\n        print(\"UnicodeDecodeError:\\t\" + str(e))\n        print(\"Something went wrong when reading the .gml file relating to the decoding of UNICODE.\")\n        import time as time\n        fPath+=\"_\" +str(time.time())\n        print(\"To avoid losing data, the output file will be renamed to use the timestamp as:\\n\" + fPath + \"_\" + str(time.time()))\n        print()\n        # No information has been recovered\n        oldData = nx.Graph()\n    except Exception as e:\n        # No information has been recovered\n        oldData = nx.Graph()\n\n    newGraph = _generateGraphData(d, oldData)\n\n    # Writing the gml file\n    nx.write_gml(newGraph,fPath)", "response": "Workaround to export data to a. gml file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef usufyToPngExport(d, fPath):\n    newGraph = _generateGraphData(d)\n\n    import matplotlib.pyplot as plt\n    # Writing the png file\n    nx.draw(newGraph)\n    plt.savefig(fPath)", "response": "Workaround to export to a png file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fileToMD5(filename, block_size=256*128, binary=False):\n    md5 = hashlib.md5()\n    with open(filename,'rb') as f:\n        for chunk in iter(lambda: f.read(block_size), b''):\n             md5.update(chunk)\n    if not binary:\n        return md5.hexdigest()\n    return md5.digest()", "response": "A function that calculates the MD5 hash of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the current Datetime with a given format", "response": "def getCurrentStrDatetime():\n    \"\"\"\n    Generating the current Datetime with a given format\n\n    Returns:\n    --------\n        string: The string of a date.\n    \"\"\"\n    # Generating current time\n    i = datetime.datetime.now()\n    strTime = \"%s-%s-%s_%sh%sm\" % (i.year, i.month, i.day, i.hour, i.minute)\n    return strTime"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getFilesFromAFolder(path):\n    from os import listdir\n    from os.path import isfile, join\n    #onlyfiles = [ f for f in listdir(path) if isfile(join(path,f)) ]\n    onlyFiles = []\n    for f in listdir(path):\n        if isfile(join(path, f)):\n            onlyFiles.append(f)\n    return onlyFiles", "response": "Returns a list of all the files in a folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef urisToBrowser(uris=[], autoraise=True):\n\n    # Cloning stdout (1) and stderr (2)\n    savout1 = os.dup(1)\n    savout2 = os.dup(2)\n\n    # Closing them\n    os.close(1)\n    os.close(2)\n    os.open(os.devnull, os.O_RDWR)\n\n    try:\n        for uri in uris:\n            # Opening the Tor URI using onion.cab proxy\n            if \".onion\" in uri:\n                wb.open(uri.replace(\".onion\", \".onion.city\"), new=2, autoraise=autoraise)\n            else:\n                wb.open(uri, new=2, autoraise=autoraise)\n    finally:\n        # Reopening them...\n        os.dup2(savout1, 1)\n        os.dup2(savout2, 2)", "response": "Method that launches the URI in the default browser of the system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef openResultsInBrowser(res):\n    print(emphasis(\"\\n\\tOpening URIs in the default web browser...\"))\n\n    urisToBrowser([\"https://github.com/i3visio/osrframework\"], autoraise=False)\n    # Waiting 2 seconds to confirm that the browser is opened and prevent the OS from opening several windows\n    time.sleep(2)\n\n    uris = []\n    for r in res:\n        for att in r[\"attributes\"]:\n            if att[\"type\"] == \"i3visio.uri\":\n                uris.append(att[\"value\"])\n\n    urisToBrowser(uris)", "response": "Method that opens the URIs in the default web browser and opens them in the windows"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef colorize(text, messageType=None):\n    formattedText = str(text)\n    # Set colors\n    if \"ERROR\" in messageType:\n        formattedText = colorama.Fore.RED + formattedText\n    elif \"WARNING\" in messageType:\n        formattedText = colorama.Fore.YELLOW + formattedText\n    elif \"SUCCESS\" in messageType:\n        formattedText = colorama.Fore.GREEN + formattedText\n    elif \"INFO\" in messageType:\n        formattedText = colorama.Fore.BLUE + formattedText\n\n    # Set emphashis mode\n    if \"BOLD\" in messageType:\n        formattedText = colorama.Style.BRIGHT + formattedText\n\n    return formattedText + colorama.Style.RESET_ALL", "response": "Function that colorizes a message."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expandEntitiesFromEmail(e):\n    # Grabbing the email\n    email = {}\n    email[\"type\"] = \"i3visio.email\"\n    email[\"value\"] = e\n    email[\"attributes\"] = []\n\n    # Grabbing the alias\n    alias = {}\n    alias[\"type\"] = \"i3visio.alias\"\n    alias[\"value\"] = e.split(\"@\")[0]\n    alias[\"attributes\"] = []\n\n    # Grabbing the domain\n    domain= {}\n    domain[\"type\"] = \"i3visio.domain\"\n    domain[\"value\"] = e.split(\"@\")[1]\n    domain[\"attributes\"] = []\n\n    return [email, alias, domain]", "response": "Method that receives an email and creates linked entities"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the total number of TLDs in the TLD table.", "response": "def getNumberTLD():\n    \"\"\"\n    Counting the total number of TLD being processed.\n    \"\"\"\n    total = 0\n    for typeTld in TLD.keys():\n        total+= len(TLD[typeTld])\n    return total"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getWhoisInfo(domain):\n    new = []\n\n    # Grabbing the aliases\n    try:\n        emails = {}\n        emails[\"type\"] = \"i3visio.alias\"\n        emails[\"value\"] = str(domain.split(\".\")[0])\n        emails[\"attributes\"] = []\n        new.append(emails)\n    except:\n        pass\n\n    info = whois.whois(domain)\n\n    if info.status == None:\n        raise Exception(\"UnknownDomainError: \" + domain + \" could not be resolved.\")\n\n    # Grabbing the emails\n    try:\n        emails = {}\n        emails[\"type\"] = \"i3visio.email\"\n        if type(info.emails) is not list:\n            aux = [info.emails]\n            emails[\"value\"] = json.dumps(aux)\n        else:\n            emails[\"value\"] = json.dumps(info.emails)\n        emails[\"attributes\"] = []\n        new.append(emails)\n    except:\n        pass\n\n    # Grabbing the country\n    try:\n        tmp = {}\n        tmp[\"type\"] = \"i3visio.location.country\"\n        tmp[\"value\"] = str(info.country)\n        tmp[\"attributes\"] = []\n        new.append(tmp)\n    except:\n        pass\n\n    # Grabbing the regitrar\n    try:\n        tmp = {}\n        tmp[\"type\"] = \"i3visio.registrar\"\n        tmp[\"value\"] = str(info.registrar)\n        tmp[\"attributes\"] = []\n        new.append(tmp)\n    except:\n        pass\n\n    # Grabbing the regitrar\n    try:\n        tmp = {}\n        tmp[\"type\"] = \"i3visio.fullname\"\n        try:\n            tmp[\"value\"] = str(info.name)\n        except:\n            tmp[\"value\"] = info.name\n        tmp[\"attributes\"] = []\n        new.append(tmp)\n    except:\n        pass\n\n    return new", "response": "Method that trie to recover the whois info from a domain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef createDomains(tlds, nicks=None, nicksFile=None):\n    domain_candidates = []\n    if nicks != None:\n        for n in nicks:\n            for t in tlds:\n                tmp = {\n                    \"domain\" : n + t[\"tld\"],\n                    \"type\" : t[\"type\"],\n                    \"tld\": t[\"tld\"]\n                }\n                domain_candidates.append(tmp)\n    elif nicksFile != None:\n        with open(nicksFile, \"r\") as iF:\n            nicks = iF.read().splitlines()\n            for n in nicks:\n                for t in tlds:\n                    tmp = {\n                        \"domain\" : n + t[\"tld\"],\n                        \"type\" : t[\"type\"],\n                        \"tld\": t[\"tld\"]\n                    }\n                    domain_candidates.append(tmp)\n    return domain_candidates", "response": "Method that globally permits to generate the domains to be checked."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pool_function(domain, launchWhois=False):\n    is_valid = True\n    try:\n        ipv4 = socket.gethostbyname(domain[\"domain\"])\n\n        # Check if this ipv4 normally throws false positives\n        if isBlackListed(ipv4):\n            return {\"platform\" : str(domain), \"status\": \"ERROR\", \"data\": {}}\n\n        #If we arrive here... The domain exists!!\n        aux = {}\n        aux[\"type\"] = \"i3visio.result\"\n        aux[\"value\"] = \"Domain Info - \" + domain[\"domain\"]\n        aux[\"attributes\"] = []\n\n        # Performing whois info and adding if necessary\n        try:\n            if domain[\"type\"] != \"global\" and launchWhois:\n                aux[\"attributes\"] = getWhoisInfo(domain[\"domain\"])\n        except Exception as e:\n            # If something happened... Well, we'll return an empty attributes array.\n            pass\n\n        tmp = {}\n        tmp[\"type\"] = \"i3visio.domain\"\n        tmp[\"value\"] =  domain[\"domain\"]\n        tmp[\"attributes\"] = []\n\n        aux[\"attributes\"].append(tmp)\n\n        tmp = {}\n        tmp[\"type\"] = \"i3visio.tld_type\"\n        tmp[\"value\"] =  domain[\"type\"]\n        tmp[\"attributes\"] = []\n\n        aux[\"attributes\"].append(tmp)\n\n        tmp = {}\n        tmp[\"type\"] = \"i3visio.ipv4\"\n        tmp[\"value\"] =  ipv4\n        tmp[\"attributes\"] = []\n\n        aux[\"attributes\"].append(tmp)\n\n        return {\"platform\" : str(domain), \"status\": \"DONE\", \"data\": aux}\n    except Exception as e:\n        return {\"platform\" : str(domain), \"status\": \"ERROR\", \"data\": {}}", "response": "This function is used to create a new page wrapper for the domain. It is used to create a new page wrapper for all the threads of getPageWrapper."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef performSearch(domains=[], nThreads=16, launchWhois=False):\n    results = []\n\n    # Using threads in a pool if we are not running the program in main\n    args = []\n\n    # Returning None if no valid domain has been returned\n    if len(domains) == 0:\n        return results\n\n    # If the process is executed by the current app, we use the Processes. It is faster than pools.\n    if nThreads <= 0 or nThreads > len(domains):\n        nThreads = len(domains)\n\n    # Launching the Pool\n    # ------------------\n    # Example catched from: https://stackoverflow.com/questions/11312525/catch-ctrlc-sigint-and-exit-multiprocesses-gracefully-in-python\n    try:\n        original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n        pool = Pool(nThreads)\n        signal.signal(signal.SIGINT, original_sigint_handler)\n    except ValueError:\n        # To avoid: ValueError: signal only works in main thread\n        pool = Pool(nThreads)\n\n    poolResults = []\n    try:\n        def log_result(result):\n            # This is called whenever foo_pool(i) returns a result.\n            # result_list is modified only by the main process, not the pool workers.\n            poolResults.append(result)\n\n        for d in domains:\n            # We need to create all the arguments that will be needed\n            parameters = (d, launchWhois,)\n            pool.apply_async(pool_function, args=parameters, callback=log_result )\n\n        # Waiting for results to be finished\n        while len(poolResults) < len(domains):\n            pass\n        # Closing normal termination\n        pool.close()\n    except KeyboardInterrupt:\n        print(general.warning(\"\\nProcess manually stopped by the user. Terminating workers.\\n\"))\n        pool.terminate()\n        print(general.warning(\"The following domains were not processed:\"))\n        pending_tld = \"\"\n        for d in domains:\n            processed = False\n            for processedDomain in poolResults:\n                if str(d) == processedDomain[\"platform\"]:\n                    processed = True\n                    break\n            if not processed:\n                print(general.warning(\"\\t- \" + str(d[\"domain\"])))\n                pending_tld += \" \" + str(d[\"tld\"])\n        print(general.warning(\"[!] If you want to relaunch the app with these domains you can always run the command with: \"))\n        print(general.warning(\"\\t domainfy ... -t none -u \" + pending_tld))\n        print(general.warning(\"[!] If you prefer to avoid these platforms you can manually evade them for whatever reason with: \"))\n        print(general.warning(\"\\t domainfy ... -x \" + pending_tld))\n    pool.join()\n\n    # Processing the results\n    # ----------------------\n    for serArray in poolResults:\n        data = serArray[\"data\"]\n        # We need to recover the results and check if they are not an empty json or None\n        if data != None and data != {}:\n            results.append(data)\n    return results", "response": "Perform the mail verification process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(params=None):\n    if params == None:\n        parser = getParser()\n        args = parser.parse_args(params)\n    else:\n        args = params\n\n    results = []\n    if not args.quiet:\n        print(general.title(banner.text))\n\n    sayingHello = \"\"\"\n    Domainfy | Copyright (C) Yaiza Rubio & F\u00e9lix Brezo (i3visio) 2014-2018\n\nThis program comes with ABSOLUTELY NO WARRANTY. This is free software, and you\nare welcome to redistribute it under certain conditions. For additional info,\nvisit <{}>.\n\"\"\".format(general.LICENSE_URL)\n    print(general.info(sayingHello))\n\n    if args.license:\n        general.showLicense()\n    else:\n        # Processing the options returned to remove the \"all\" option\n        tlds = []\n        if \"all\" in args.tlds:\n            for typeTld in TLD.keys():\n                for tld in TLD[typeTld]:\n                    if tld not in args.exclude:\n                        tlds.append({ \"tld\" : tld, \"type\" : typeTld })\n        elif \"none\" in args.tlds:\n            pass\n        else:\n            for typeTld in TLD.keys():\n                if typeTld in args.tlds:\n                    for tld in TLD[typeTld]:\n                        if tld not in args.exclude:\n                            tlds.append({ \"tld\" : tld, \"type\" : typeTld })\n\n        for new in args.user_defined:\n            if new not in args.exclude:\n                tlds.append({\"tld\": new, \"type\": \"user_defined\"})\n\n        if args.nicks:\n            domains = createDomains(tlds, nicks=args.nicks)\n        else:\n            # nicks_file\n            domains = createDomains(tlds, nicksFile=args.nicks_file)\n\n        # Showing the execution time...\n        if not args.quiet:\n            startTime= dt.datetime.now()\n            print(\"{}\\tTrying to resolve {} domain(s)\u2026\\n\".format(str(startTime), general.emphasis(str(len(domains)))))\n            if len(domains) > 200:\n                print(\"\"\"        Note that a full '-t all' search may take around 3.5 mins. If that's too \n        long for you, try narrowing the search using '-t cc' or similar arguments. \n        Otherwise, just wait and keep calm!\n                \"\"\")\n            print(general.emphasis(\"\\tPress <Ctrl + C> to stop...\\n\"))\n\n        # Perform searches, using different Threads\n        results = performSearch(domains, args.threads, args.whois)\n\n        # Trying to store the information recovered\n        if args.output_folder != None:\n            if not os.path.exists(args.output_folder):\n                os.makedirs(args.output_folder)\n            # Grabbing the results\n            fileHeader = os.path.join(args.output_folder, args.file_header)\n            for ext in args.extension:\n                # Generating output files\n                general.exportUsufy(results, ext, fileHeader)\n\n        # Showing the information gathered if requested\n        if not args.quiet:\n            now = dt.datetime.now()\n            print(\"\\n{}\\tResults obtained:\\n\".format(str(now)))\n            try:\n                print(general.success(general.usufyToTextExport(results)))\n            except:\n                print(general.warning(\"\\nSomething happened when exporting the results. The Json will be shown instead:\\n\"))\n                print(general.warning(json.dumps(results, indent=2)))\n\n            now = dt.datetime.now()\n            print(\"\\n\" + str(now) + \"\\tYou can find all the information collected in the following files:\")\n            for ext in args.extension:\n                # Showing the output files\n                print(\"\\t\" + general.emphasis(fileHeader + \".\" + ext))\n\n        # Showing the execution time...\n        if not args.quiet:\n            # Showing the execution time...\n            endTime= dt.datetime.now()\n            print(\"\\n{}\\tFinishing execution...\\n\".format(endTime))\n            print(\"Total time used:\\t\" + general.emphasis(str(endTime-startTime)))\n            print(\"Average seconds/query:\\t\" + general.emphasis(str((endTime-startTime).total_seconds()/len(domains))) +\" seconds\\n\")\n\n            # Urging users to place an issue on Github...\n            print(banner.footer)\n\n    if params:\n        return results", "response": "This function is called by the main function of the i3visio application. It is called by the main function of the i3visio application. It is called by the main function of the i3visio application."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getEntitiesByRegexp(data = None, listRegexp = None, verbosity=1, logFolder=\"./logs\"):\n    logSet.setupLogger(loggerName=\"osrframework.entify\", verbosity=verbosity, logFolder=logFolder)\n    logInstance = logging.getLogger(\"osrframework.entify\")\n    if listRegexp == None:\n        listRegexp = regexp_selection.getAllRegexp()\n\n    foundExpr = []\n\n    for r in listRegexp:\n        foundExpr += r.findExp(data)\n\n    return foundExpr", "response": "Method to obtain the list of available entities by regular expressions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scanFolderForRegexp(folder = None, listRegexp = None, recursive = False, verbosity=1, logFolder= \"./logs\", quiet=False):\n    logSet.setupLogger(loggerName=\"osrframework.entify\", verbosity=verbosity, logFolder=logFolder)\n    logger = logging.getLogger(\"osrframework.entify\")\n\n    logger.info(\"Scanning the folder: \" + folder)\n    results = []\n\n    #onlyfiles = []\n    #for f in listdir(args.input_folder):\n    #    if isfile(join(args.input_folder, f)):\n    #        onlyfiles.append(f)\n    onlyfiles = [ f for f in listdir(folder) if isfile(join(folder,f)) ]\n\n    for i, f in enumerate(onlyfiles):\n        filePath = join(folder,f)\n        logger.debug(\"Looking for regular expressions in: \" + filePath)\n        if not quiet:\n            print(str(i) + \"/\" + str(len(onlyfiles)) + \"\\tLooking for regular expressions in: \" + filePath)\n        with open(filePath, \"r\") as tempF:\n            # reading data\n            foundExpr = getEntitiesByRegexp(data = tempF.read(), listRegexp = listRegexp)\n            logger.debug(\"Updating the \" + str(len(foundExpr)) + \" results found on: \" + filePath)\n            aux = {}\n            aux[\"type\"] = \"i3visio.uri\"\n            aux[\"value\"] = filePath\n            aux[\"attributes\"] = foundExpr\n            results.append(aux)\n\n    if recursive:\n        onlyfolders = [ f for f in listdir(folder) if isdir(join(folder,f)) ]\n        for f in onlyfolders:\n            folderPath = join(folder, f)\n            logger.debug(\"Looking for additional in the folder: \"+ folderPath)\n            results.update(scanFolderForRegexp(folder = folderPath,listRegexp = listRegexp, recursive = recursive))\n\n    # Printing the information if not in quiet mode\n    if not quiet:\n        print(general.success(json.dumps(results, indent=2)))\n\n    return results", "response": "This method scans the folder for the specified regular expressions and returns a list of Available objects containing the regular expressions found in the folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(params=None):\n    if params == None:\n        parser = getParser()\n        args = parser.parse_args(params)\n    else:\n        args = params\n\n    results = []\n\n    # Recovering the logger\n    # Calling the logger when being imported\n    logSet.setupLogger(loggerName=\"osrframework.entify\", verbosity=args.verbose, logFolder=args.logfolder)\n    # From now on, the logger can be recovered like this:\n    logger = logging.getLogger(\"osrframework.entify\")\n\n    logger.info(\"Selecting the regular expressions to be analysed...\")\n\n    if not args.quiet:\n        print(general.title(banner.text))\n\n    sayingHello = \"\"\"\n      Entify | Copyright (C) Yaiza Rubio & F\u00e9lix Brezo (i3visio) 2014-2018\n\nThis program comes with ABSOLUTELY NO WARRANTY. This is free software, and you\nare welcome to redistribute it under certain conditions. For additional info,\nvisit <{}>.\n\"\"\".format(general.LICENSE_URL)\n    print(general.info(sayingHello))\n    \n    if args.license:\n        general.showLicense()\n    else:\n        listRegexp = []\n        if args.regexp:\n            listRegexp = regexp_selection.getRegexpsByName(args.regexp)\n        elif args.new_regexp:\n            for i, r in enumerate(args.new_regexp):\n                listRegexp.append(RegexpObject(name = \"NewRegexp\"+str(i), reg_exp = args.new_regexp))\n\n        if not args.web:\n            results = scanFolderForRegexp(folder = args.input_folder, listRegexp= listRegexp, recursive = args.recursive, verbosity=args.verbose, logFolder= args.logfolder, quiet=args.quiet)\n        else:\n            results = scanResource(uri = args.web, listRegexp= listRegexp, verbosity=args.verbose, logFolder= args.logfolder)\n        logger.info(\"Logging the results:\\n\" + json.dumps(results, indent=2, sort_keys=True))\n\n        # Trying to store the information recovered\n        if args.output_folder != None:\n            # Verifying an output folder was selected\n            logger.debug(\"Preparing the output folder...\")\n            if not os.path.exists(args.output_folder):\n                logger.warning(\"The output folder \\'\" + args.output_folder + \"\\' does not exist. The system will try to create it.\")\n                os.makedirs(args.output_folder)\n\n            # Grabbing the results\n            fileHeader = os.path.join(args.output_folder, args.file_header)\n            for ext in args.extension:\n                # Generating output files\n                general.exportUsufy(results, ext, fileHeader)\n\n        # Showing the information gathered if requested\n        if not args.quiet:\n            now = dt.datetime.now()\n            print(\"\\n{}\\tResults obtained:\\n\".format(str(now)))\n            print(general.success(general.usufyToTextExport(results)))\n\n            now = dt.datetime.now()\n            print(str(now) + \"\\tYou can find all the information collected in the following files:\")\n            for ext in args.extension:\n                # Showing the output files\n                print(\"\\t-\" + general.emphasis(fileHeader + \".\" + ext))\n\n            # Urging users to place an issue on Github...\n            print(banner.footer)\n\n    if params:\n        return results", "response": "This function is called by osrfjs to launch phonefy. It is called by osrfjs to launch phonefy."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies a mailfy query in this platform.", "response": "def check_mailfy(self, query, kwargs={}):\n        \"\"\"\n        Verifying a mailfy query in this platform.\n\n        This might be redefined in any class inheriting from Platform. The only\n        condition is that any of this should return a dictionary as defined.\n\n        Args:\n        -----\n            query: The element to be searched.\n            kwargs: Dictionary with extra parameters. Just in case.\n\n        Return:\n        -------\n            Returns the collected data if exists or None if not.\n        \"\"\"\n        import requests\n\n        s = requests.Session()\n\n        # Getting the first response to grab the csrf_token\n        r1 = s.get('https://www.infojobs.net')\n\n        # Launching the query to Instagram\n        r2 = s.post(\n            'https://www.infojobs.net/candidate/profile/check-email-registered.xhtml',\n            data={\"email\": query},\n        )\n\n        if '{\"email_is_secure\":true,\"email\":true}' in r2.text:\n            return r2.text\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef processMailList(platformNames=[], emails=[]):\n    # Grabbing the <Platform> objects\n    platforms = platform_selection.getPlatformsByName(platformNames, mode=\"mailfy\")\n\n    results = []\n    for e in emails:\n        for pla in platforms:\n            # This returns a json.txt!\n            entities = pla.getInfo(query=e, mode=\"mailfy\")\n            if entities != {}:\n                results += json.loads(entities)\n    return results", "response": "Method to perform the email search on the platforms and return a list of verified emails."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pool_function(args):\n    is_valid = True\n\n    try:\n        checker = emailahoy.VerifyEmail()\n        status, message = checker.verify_email_smtp(args, from_host='gmail.com', from_email='sample@gmail.com')\n        if status == 250:\n            print(\"\\t[*] Verification of '{}' status: {}. Details:\\n\\t\\t{}\".format(general.success(args), general.success(\"SUCCESS ({})\".format(str(status))), message.replace('\\n', '\\n\\t\\t')))\n            is_valid = True\n        else:\n            print(\"\\t[*] Verification of '{}' status: {}. Details:\\n\\t\\t{}\".format(general.error(args), general.error(\"FAILED ({})\".format(str(status))), message.replace('\\n', '\\n\\t\\t')))\n            is_valid = False\n    except Exception, e:\n        print(general.warning(\"WARNING. An error was found when performing the search. You can omit this message.\\n\" + str(e)))\n        is_valid = False\n\n    aux = {}\n    aux[\"type\"] = \"i3visio.profile\"\n    aux[\"value\"] = \"Email - \" + args\n    aux[\"attributes\"] =  general.expandEntitiesFromEmail(args)\n    platform = aux[\"attributes\"][2][\"value\"].title()\n    aux[\"attributes\"].append({\n            \"type\": \"i3visio.platform\",\n            \"value\": platform,\n            \"attributes\": []\n        }\n    )\n\n    if is_valid:\n        return {\"platform\": platform, \"status\": \"DONE\", \"data\": aux}\n    else:\n        return {\"platform\": platform, \"status\": \"DONE\", \"data\": {}}", "response": "This function is used to be able to launch all the threads. It will return a dictionary representing whether the verification was ended successfully."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(params=None):\n    if params == None:\n        parser = getParser()\n        args = parser.parse_args(params)\n    else:\n        args = params\n\n    results = []\n\n    if not args.quiet:\n        print(general.title(banner.text))\n\n        sayingHello = \"\"\"\n          Mailfy | Copyright (C) Yaiza Rubio & F\u00e9lix Brezo (i3visio) 2014-2018\n\n    This program comes with ABSOLUTELY NO WARRANTY. This is free software, and you\n    are welcome to redistribute it under certain conditions. For additional info,\n    visit <{}>.\n    \"\"\".format(general.LICENSE_URL)\n        print(general.info(sayingHello))\n        # Displaying a warning if this is being run in a windows system\n        if sys.platform == 'win32':\n            print(general.warning(\"\"\"OSRFramework has detected that you are running mailfy.py in a Windows system.\nAs the \"emailahoy\" library is NOT working properly there, \"validate_email\" will\nbe used instead. Verification may be slower though.\"\"\"))\n\n    if args.license:\n        general.showLicense()\n    else:\n        # processing only the given domains and excluding the ones provided\n        extra_domains = []\n\n        for d in args.domains:\n            if d not in args.exclude and not d == \"all\":\n                extra_domains.append(d)\n\n        # Two different arrays are mantained since there are some domains that cannot be safely verified\n        if args.create_emails:\n            potentially_existing_emails = grabEmails(\n                nicksFile=args.create_emails,\n                domains=EMAIL_DOMAINS + extra_domains,\n                excludeDomains=args.exclude\n            )\n            potentially_leaked_emails = grabEmails(\n                nicksFile=args.create_emails,\n                domains=LEAKED_DOMAINS + extra_domains,\n                excludeDomains=args.exclude\n            )\n        else:\n            potentially_existing_emails = grabEmails(\n                emails=args.emails,\n                emailsFile=args.emails_file,\n                nicks=args.nicks,\n                nicksFile=args.nicks_file,\n                domains=EMAIL_DOMAINS + extra_domains,\n                excludeDomains=args.exclude\n            )\n            potentially_leaked_emails = grabEmails(\n                emails=args.emails,\n                emailsFile=args.emails_file,\n                nicks=args.nicks,\n                nicksFile=args.nicks_file,\n                domains=LEAKED_DOMAINS + extra_domains,\n                excludeDomains=args.exclude\n            )\n\n        emails = list(set(potentially_leaked_emails + potentially_existing_emails))\n\n        # Showing the execution time...\n        if not args.quiet:\n            startTime = dt.datetime.now()\n            print(\"{}\\tStarting search of {} different emails:\\n{}\\n\".format(\n                str(startTime),\n                general.emphasis(str(len(emails))),\n                json.dumps(emails, indent=2, sort_keys=True))\n            )\n\n        if not args.quiet:\n            now = dt.datetime.now()\n            print(\"\\n{}\\tStep 1. Trying to determine if the emails provided do exist...\\n\".format(str(now)))\n            print(general.emphasis(\"\\tPress <Ctrl + C> to stop...\\n\"))\n\n        # Perform searches, using different Threads\n        results = performSearch(potentially_existing_emails, nThreads=args.threads)\n\n        if not args.quiet:\n            now = dt.datetime.now()\n            print(\"\\n{}\\tStep 2. Checking if the emails have been used to register socialmedia accounts...\\n\".format(str(now)))\n            print(general.emphasis(\"\\tPress <Ctrl + C> to stop...\\n\"))\n\n        registered = processMailList(platformNames=args.platforms, emails=potentially_existing_emails)\n        results += registered\n\n        if not args.quiet:\n            if len(results) > 0:\n                for r in registered:\n                    print(\"\\t[*] Registered account found: {}\".format(general.success(r[\"value\"])))\n            else:\n                print(\"\\t[*] Registered account found: {}\".format(general.error(\"None\")))\n\n            now = dt.datetime.now()\n            print(\"\\n{}\\tStep 3. Verifying if the provided emails have  been leaked somewhere?\\n\".format(str(now)))\n            print(general.emphasis(\"\\tPress <Ctrl + C> to stop...\\n\"))\n\n        # Verify the existence of the mails found as leaked emails.\n        for query in potentially_leaked_emails:\n            # Iterate through the different leak platforms\n            leaks = hibp.checkIfEmailWasHacked(query)\n\n            if len(leaks) > 0:\n                if not args.quiet:\n                    if len(leaks) > 0:\n                        print(\"\\t[*] '{}' has been found in at least {} different leaks.\".format(general.success(query), general.success(str(len(leaks)))))\n                    else:\n                        print(\"\\t[*] '{}' has NOT been found in any leak.\".format(general.error(query)))\n            else:\n                if not args.quiet:\n                    print(\"\\t[*] '{}' has NOT been found on any leak yet.\".format(general.error(query)))\n\n            results += leaks\n\n        # Trying to store the information recovered\n        if args.output_folder != None:\n            if not os.path.exists(args.output_folder):\n                os.makedirs(args.output_folder)\n            # Grabbing the results\n            fileHeader = os.path.join(args.output_folder, args.file_header)\n            for ext in args.extension:\n                # Generating output files\n                general.exportUsufy(results, ext, fileHeader)\n\n        # Showing the information gathered if requested\n        if not args.quiet:\n            now = dt.datetime.now()\n            print(\"\\n{}\\tResults obtained:\\n\".format(str(now)))\n            print(general.success(general.usufyToTextExport(results)))\n\n            now = dt.datetime.now()\n            print(\"\\n\" + str(now) + \"\\tYou can find all the information collected in the following files:\")\n            for ext in args.extension:\n                # Showing the output files\n                print(general.emphasis(\"\\t\" + fileHeader + \".\" + ext))\n\n        # Showing the execution time...\n        if not args.quiet:\n            endTime= dt.datetime.now()\n            print(\"\\n\" + str(endTime) +\"\\tFinishing execution...\\n\")\n            print(\"Total time used:\\t\" + general.emphasis(str(endTime-startTime)))\n            print(\"Average seconds/query:\\t\" + general.emphasis(str((endTime-startTime).total_seconds()/len(emails))) +\" seconds\\n\")\n\n        if not args.quiet:\n            # Urging users to place an issue on Github...\n            print(banner.footer)\n\n    if params:\n        return results", "response": "Main function to launch phonefy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recoverURL(self, url):\n        # Configuring user agents...\n        self.setUserAgent()\n\n        # Configuring proxies\n        if \"https://\" in url:\n            self.setProxy(protocol = \"https\")\n        else:\n            self.setProxy(protocol = \"http\")\n\n        # Giving special treatment for .onion platforms\n        if \".onion\" in url:\n            try:\n                # TODO: configuring manually the tor bundle\n                pass\n            except:\n                # TODO: capturing the error and eventually trying the tor2web approach\n                #url = url.replace(\".onion\", \".tor2web.org\")\n                pass\n            url = url.replace(\".onion\", \".onion.cab\")\n\n        # Opening the resource\n        try:\n            recurso = self.br.open(url)\n        except:\n            # Something happened. Maybe the request was forbidden?\n            return None\n\n        html = recurso.read()\n\n        return html", "response": "This method is used to recover a resource from a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the password for a url.", "response": "def setNewPassword(self, url, username, password):\n        \"\"\"\n            Public method to manually set the credentials for a url in the browser.\n        \"\"\"\n        self.br.add_password(url, username, password)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a proxy for the browser.", "response": "def setProxy(self, protocol=\"http\"):\n        \"\"\"\n            Public method to set a proxy for the browser.\n        \"\"\"\n        # Setting proxy\n        try:\n            new = { protocol: self.proxies[protocol]}\n            self.br.set_proxies( new )\n        except:\n            # No proxy defined for that protocol\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery the user and user list", "response": "def main(args):\n    \"\"\"\n        Query manager.\n    \"\"\"\n    # Creating the instance\n    tAW = TwitterAPIWrapper()\n\n    # Selecting the query to be launched\n    if args.type == \"get_all_docs\":\n        results = tAW.get_all_docs(args.query)\n\n    elif args.type == \"get_user\":\n        results = tAW.get_user(args.query)\n\n    elif args.type == \"get_followers\":\n        results = tAW.get_followers(args.query)\n\n        print \"... %s followers downloaded... \" % (len(results))\n        #write the csv\n        with open('%s_followers.csv' % args.query, 'wb') as f:\n            writer = csv.writer(f)\n            for r in results:\n                writer.writerow([args.query,str(r)])\n\n    elif args.type == \"get_friends\":\n        results = tAW.get_friends(args.query)\n        print \"... %s friends downloaded... \" % (len(results))\n        #write the csv\n        with open('%s_friends.csv' % args.query, 'wb') as f:\n            writer = csv.writer(f)\n            for r in results:\n                writer.writerow([args.query,str(r)])\n\n    elif args.type == \"search_users\":\n        results = tAW.search_users(args.query)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to the tweepy API.", "response": "def _connectToAPI(self):\n        \"\"\"\n            :return: A tweepy.API object that performs the queries\n        \"\"\"\n        #authorize twitter, initialize tweepy\n        auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n        auth.set_access_token(self.access_key, self.access_secret)\n        api = tweepy.API(auth)\n        return api"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _rate_limit_status(self, api=None, mode=None):\n        if api == None:\n            api = self.connectToAPI()\n\n        if mode == None:\n            print json.dumps(api.rate_limit_status(), indent=2)\n            raw_input(\"<Press ENTER>\")\n        else:\n            # Testing if we have enough queries\n            while True:\n                allLimits = api.rate_limit_status()\n                if mode == \"get_user\":\n                    limit = allLimits[\"resources\"][\"users\"][\"/users/show/:id\"][\"limit\"]\n                    remaining = allLimits[\"resources\"][\"users\"][\"/users/show/:id\"][\"remaining\"]\n                    reset = allLimits[\"resources\"][\"users\"][\"/users/show/:id\"][\"reset\"]\n                elif mode == \"get_followers\":\n                    limit = allLimits[\"resources\"][\"followers\"][\"/followers/ids\"][\"limit\"]\n                    remaining = allLimits[\"resources\"][\"followers\"][\"/followers/ids\"][\"remaining\"]\n                    reset = allLimits[\"resources\"][\"followers\"][\"/followers/ids\"][\"reset\"]\n                elif mode == \"get_friends\":\n                    limit = allLimits[\"resources\"][\"friends\"][\"/friends/ids\"][\"limit\"]\n                    remaining = allLimits[\"resources\"][\"friends\"][\"/friends/ids\"][\"remaining\"]\n                    reset = allLimits[\"resources\"][\"friends\"][\"/friends/ids\"][\"reset\"]\n                elif mode == \"search_users\":\n                    limit = allLimits[\"resources\"][\"users\"][\"/users/search\"][\"limit\"]\n                    remaining = allLimits[\"resources\"][\"users\"][\"/users/search\"][\"remaining\"]\n                    reset = allLimits[\"resources\"][\"users\"][\"/users/search\"][\"reset\"]\n                else:\n                    remaining = 1\n                \"\"\"elif mode == \"get_all_docs\":\n                    limit = allLimits[\"resources\"]REPLACEME[\"limit\"]\n                    remaining = allLimits[\"resources\"]REPLACEME[\"remaining\"]\n                    reset = allLimits[\"resources\"]REPLACEME[\"reset\"]\"\"\"\n                \"\"\"elif mode == \"get_users\":\n                    limit = allLimits[\"resources\"]REPLACEME[\"limit\"]\n                    remaining = allLimits[\"resources\"]REPLACEME[\"remaining\"]\n                    reset = allLimits[\"resources\"]REPLACEME[\"reset\"] \"\"\"\n                \"\"\"else:\n                    remaining = 1\"\"\"\n                # Checking if we have enough remaining queries\n                if remaining > 0:\n                    #raw_input(str(remaining) + \" queries yet...\")\n                    break\n                else:\n                    waitTime = 60\n                    print \"No more queries remaining, sleeping for \" + str(waitTime) +\" seconds...\"\n                    time.sleep(waitTime)\n\n        return 0", "response": "Verify the API limits and return the status of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting tweepy.User to a i3visio-like user. This will process the returned JSON object that the API returns to transform it to the i3visio-like format. A sample answer is copied now when testing it to the @i3visio user in Twitter. { \"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"profile_text_color\": \"333333\", \"default_profile_image\": false, \"id\": 2594815981, \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"verified\": false, \"profile_location\": null, \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/491716630292881408/FBqYf9qv_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": { \"url\": { \"urls\": [ { \"url\": \"http://t.co/Vus95W8ub6\", \"indices\": [ 0, 22 ], \"expanded_url\": \"http://www.i3visio.com\", \"display_url\": \"i3visio.com\" } ] }, \"description\": { \"urls\": [ { \"url\": \"http://t.co/SGty7or6SQ\", \"indices\": [ 30, 52 ], \"expanded_url\": \"http://github.com/i3visio/osrframework\", \"display_url\": \"github.com/i3visio/osrfra\\u2026\" } ] } }, \"followers_count\": 21, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"2594815981\", \"profile_background_color\": \"C0DEED\", \"listed_count\": 5, \"status\": { \"lang\": \"es\", \"favorited\": false, \"entities\": { \"symbols\": [], \"user_mentions\": [], \"hashtags\": [], \"urls\": [] }, \"contributors\": null, \"truncated\": false, \"text\": \"Podemos confirmar que Alpify, aunque acabe en ...fy no es una aplicaci\\u00f3n nuestra. ;) \\u00a1A aprovechar lo que queda de domingo!\", \"created_at\": \"Sun Aug 16 17:35:37 +0000 2015\", \"retweeted\": true, \"in_reply_to_status_id_str\": null, \"coordinates\": null, \"in_reply_to_user_id_str\": null, \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_screen_name\": null, \"id_str\": \"632968969662689280\", \"place\": null, \"retweet_count\": 1, \"geo\": null, \"id\": 632968969662689280, \"favorite_count\": 0, \"in_reply_to_user_id\": null }, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 56, \"description\": \"Leading OSRFramework project (http://t.co/SGty7or6SQ) for researching in Open Sources. #security #osint #socialengineering\", \"friends_count\": 10, \"location\": \"Espa\\u00f1a\", \"profile_link_color\": \"0084B4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/491716630292881408/FBqYf9qv_normal.png\", \"following\": true, \"geo_enabled\": false, \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"name\": \"i3visio\", \"lang\": \"en\", \"profile_background_tile\": false, \"favourites_count\": 6, \"screen_name\": \"i3visio\", \"notifications\": false, \"url\": \"http://t.co/Vus95W8ub6\", \"created_at\": \"Sun Jun 29 13:27:20 +0000 2014\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false } :param jUser: A Json representing the information of a profile as returned by the API. :return: Dict in i3visio-like format.", "response": "def _processUser(self, jUser):\n        \"\"\"\n            Convert tweepy.User to a i3visio-like user. This will process the returned JSON object that the API returns to transform it to the i3visio-like format. A sample answer is copied now when testing it to the @i3visio user in Twitter.\n{\n  \"follow_request_sent\": false,\n  \"has_extended_profile\": false,\n  \"profile_use_background_image\": true,\n  \"profile_text_color\": \"333333\",\n  \"default_profile_image\": false,\n  \"id\": 2594815981,\n  \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\",\n  \"verified\": false,\n  \"profile_location\": null,\n  \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/491716630292881408/FBqYf9qv_normal.png\",\n  \"profile_sidebar_fill_color\": \"DDEEF6\",\n  \"entities\": {\n    \"url\": {\n      \"urls\": [\n        {\n          \"url\": \"http://t.co/Vus95W8ub6\",\n          \"indices\": [\n            0,\n            22\n          ],\n          \"expanded_url\": \"http://www.i3visio.com\",\n          \"display_url\": \"i3visio.com\"\n        }\n      ]\n    },\n    \"description\": {\n      \"urls\": [\n        {\n          \"url\": \"http://t.co/SGty7or6SQ\",\n          \"indices\": [\n            30,\n            52\n          ],\n          \"expanded_url\": \"http://github.com/i3visio/osrframework\",\n          \"display_url\": \"github.com/i3visio/osrfra\\u2026\"\n        }\n      ]\n    }\n  },\n  \"followers_count\": 21,\n  \"profile_sidebar_border_color\": \"C0DEED\",\n  \"id_str\": \"2594815981\",\n  \"profile_background_color\": \"C0DEED\",\n  \"listed_count\": 5,\n  \"status\": {\n    \"lang\": \"es\",\n    \"favorited\": false,\n    \"entities\": {\n      \"symbols\": [],\n      \"user_mentions\": [],\n      \"hashtags\": [],\n      \"urls\": []\n    },\n    \"contributors\": null,\n    \"truncated\": false,\n    \"text\": \"Podemos confirmar que Alpify, aunque acabe en ...fy no es una aplicaci\\u00f3n nuestra. ;) \\u00a1A aprovechar lo que queda de domingo!\",\n    \"created_at\": \"Sun Aug 16 17:35:37 +0000 2015\",\n    \"retweeted\": true,\n    \"in_reply_to_status_id_str\": null,\n    \"coordinates\": null,\n    \"in_reply_to_user_id_str\": null,\n    \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\",\n    \"in_reply_to_status_id\": null,\n    \"in_reply_to_screen_name\": null,\n    \"id_str\": \"632968969662689280\",\n    \"place\": null,\n    \"retweet_count\": 1,\n    \"geo\": null,\n    \"id\": 632968969662689280,\n    \"favorite_count\": 0,\n    \"in_reply_to_user_id\": null\n  },\n  \"is_translation_enabled\": false,\n  \"utc_offset\": null,\n  \"statuses_count\": 56,\n  \"description\": \"Leading OSRFramework project (http://t.co/SGty7or6SQ) for researching in Open Sources. #security #osint #socialengineering\",\n  \"friends_count\": 10,\n  \"location\": \"Espa\\u00f1a\",\n  \"profile_link_color\": \"0084B4\",\n  \"profile_image_url\": \"http://pbs.twimg.com/profile_images/491716630292881408/FBqYf9qv_normal.png\",\n  \"following\": true,\n  \"geo_enabled\": false,\n  \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\",\n  \"name\": \"i3visio\",\n  \"lang\": \"en\",\n  \"profile_background_tile\": false,\n  \"favourites_count\": 6,\n  \"screen_name\": \"i3visio\",\n  \"notifications\": false,\n  \"url\": \"http://t.co/Vus95W8ub6\",\n  \"created_at\": \"Sun Jun 29 13:27:20 +0000 2014\",\n  \"contributors_enabled\": false,\n  \"time_zone\": null,\n  \"protected\": false,\n  \"default_profile\": true,\n  \"is_translator\": false\n}\n\n            :param jUser:   A Json representing the information of a profile as returned by the API.\n\n\n            :return: Dict in i3visio-like format.\n        \"\"\"\n        #raw_input(json.dumps(jUser, indent=2))\n        r = {}\n        r[\"type\"] = \"i3visio.profile\"\n        r[\"value\"] = self.platformName + \" - \" + jUser[\"screen_name\"]\n        r[\"attributes\"] = []\n\n        # Appending platform URI\n        \"\"\"aux = {}\n        aux[\"type\"] = \"i3visio.uri\"\n        aux[\"value\"] = qURL\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux) \"\"\"\n        # Appending the id\n        aux = {}\n        aux[\"type\"] = \"@twitter_id\"\n        aux[\"value\"] = jUser[\"id_str\"]\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n\n        # Appending the alias\n        aux = {}\n        aux[\"type\"] = \"i3visio.alias\"\n        aux[\"value\"] = jUser[\"screen_name\"]\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending fullname\n        aux = {}\n        aux[\"type\"] = \"i3visio.fullname\"\n        aux[\"value\"] = jUser[\"name\"]\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending description\n        aux = {}\n        aux[\"type\"] = \"i3visio.text\"\n        aux[\"value\"] = jUser[\"description\"] if jUser[\"description\"] != \"\" else \"[N/A]\"\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending platform name\n        aux = {}\n        aux[\"type\"] = \"i3visio.platform\"\n        aux[\"value\"] = self.platformName\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending location\n        aux = {}\n        aux[\"type\"] = \"i3visio.location\"\n        aux[\"value\"] = jUser[\"location\"] if jUser[\"location\"] != \"\" else \"[N/A]\"\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending profile_location\n        aux = {}\n        aux[\"type\"] = \"i3visio.location.current\"\n        aux[\"value\"] = jUser[\"profile_location\"] if jUser[\"profile_location\"] != None else \"[N/A]\"\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending uri homepage\n        try:\n            urls = jUser[\"entities\" ][\"url\"][\"urls\"]\n            for url in urls:\n                aux = {}\n                aux[\"type\"] = \"i3visio.uri.homepage\"\n                aux[\"value\"] = url[\"expanded_url\"] if url[\"expanded_url\"] != None else \"[N/A]\"\n                aux[\"attributes\"] = []\n                r[\"attributes\"].append(aux)\n        except Exception as e:\n            #Something happenned when parsing the URLS\n            aux = {}\n            aux[\"type\"] = \"i3visio.uri.homepage\"\n            aux[\"value\"] = \"[N/A]\"\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n        # Appending profile uri homepage\n        try:\n            aux = {}\n            aux[\"type\"] = \"i3visio.uri.image.profile\"\n            aux[\"value\"] = jUser[\"profile_image_url\"] if jUser[\"profile_image_url\"] != None else \"[N/A]\"\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n        except Exception as e:\n            #Something happenned when parsing the Profile URL\n            aux = {}\n            aux[\"type\"] = \"i3visio.uri.image.profile\"\n            aux[\"value\"] = \"[N/A]\"\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n        # Appending uri background\n        try:\n            aux = {}\n            aux[\"type\"] = \"i3visio.uri.image.background\"\n            aux[\"value\"] = jUser[\"profile_background_image_url\"] if jUser[\"profile_background_image_url\"] != None else \"[N/A]\"\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n        except Exception as e:\n            #Something happenned when parsing the background URL\n            aux = {}\n            aux[\"type\"] = \"i3visio.uri.image.background\"\n            aux[\"value\"] = \"[N/A]\"\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n        # Appending created_at\n        aux = {}\n        aux[\"type\"] = \"@created_at\"\n        aux[\"value\"] = jUser[\"created_at\"]\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending friends_count\n        aux = {}\n        aux[\"type\"] = \"@friends_count\"\n        aux[\"value\"] = str(jUser[\"friends_count\"])\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending followers_count\n        aux = {}\n        aux[\"type\"] = \"@followers_count\"\n        aux[\"value\"] = str(jUser[\"followers_count\"])\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending protected\n        aux = {}\n        aux[\"type\"] = \"@protected\"\n        aux[\"value\"] = str(jUser[\"protected\"]).lower()\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending geo_enabled\n        aux = {}\n        aux[\"type\"] = \"@geo_enabled\"\n        aux[\"value\"] = str(jUser[\"geo_enabled\"]).lower()\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending language\n        aux = {}\n        aux[\"type\"] = \"@language\"\n        aux[\"value\"] = jUser[\"lang\"]\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending time_zone\n        aux = {}\n        aux[\"type\"] = \"@time_zone\"\n        aux[\"value\"] = jUser[\"time_zone\"] if jUser[\"time_zone\"] != None else \"[N/A]\"\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending verified\n        aux = {}\n        aux[\"type\"] = \"@verified\"\n        aux[\"value\"] = str(jUser[\"verified\"]).lower()\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending listed_count\n        aux = {}\n        aux[\"type\"] = \"@listed_count\"\n        aux[\"value\"] = str(jUser[\"listed_count\"])\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending publications_count\n        aux = {}\n        aux[\"type\"] = \"@publications_count\"\n        aux[\"value\"] = str(jUser[\"statuses_count\"])\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending favourites_count\n        aux = {}\n        aux[\"type\"] = \"@favourites_count\"\n        aux[\"value\"] = str(jUser[\"favourites_count\"])\n        aux[\"attributes\"] = []\n        r[\"attributes\"].append(aux)\n        # Appending suspended\n        try:\n            aux = {}\n            aux[\"type\"] = \"@suspended\"\n            aux[\"value\"] = str(jUser[\"suspended\"]).lower()\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n        except:\n            pass\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all the tweets that have been emitted by a user.", "response": "def get_all_docs(self, screen_name):\n        \"\"\"\n            Method to get all the tweets emitted by a user.\n\n            :param screen_name: The Twitter username.\n\n            :return:    List of tweets.\n        \"\"\"\n        def _getNewTweets(api, screen_name,count=200, oldest=None, waitTime=60):\n            \"\"\"\n                MEthod that recovers the new tweets or waits until the number of remaining calls has been freed.\n\n                :param api:     A valid and connected api.\n                :param screen_name: screen_name of the user to monitor.\n                :param count:   Number of tweets to grab per iteration.\n                :param oldes:  Oldest tweet to grab in this iteration.\n                :param waitTime:    Number of seconds to wait between tries.\n\n                :return:  List of new_tweets\n            \"\"\"\n            # Verifying the limits of the API\n            #self._rate_limit_status(api=api, mode=\"get_all_docs\")\n\n            waiting = True\n            while waiting == True:\n                try:\n                    if oldest != None:\n                        # We have to update the oldest id\n                        new_tweets = api.user_timeline(screen_name=screen_name, count=count, max_id=oldest)\n                    else:\n                        new_tweets = api.user_timeline(screen_name=screen_name, count=count)\n                    waiting = False\n                    #save most recent tweets\n\n                except Exception as e:\n                    # Error... We will have to wait\n                    #waiting = True\n                    print str(e)\n                    #print(traceback.format_exc())\n                    print \"No more queries remaining, sleeping for \" + str(waitTime) +\" seconds...\"\n                    time.sleep(waitTime)\n\n            return new_tweets\n\n        # Connecting to the API\n        api = self._connectToAPI()\n\n        #initialize a list to hold all the tweepy Tweets\n        alltweets = []\n\n        #make initial request for most recent tweets (200 is the maximum allowed count)\n        \"\"\"waiting = True\n        while waiting == True:\n            try:\n                new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n                waiting = False\n            except:\n                # Error... We will have to wait\n                waiting = True\n                time.sleep(waitTime)  \"\"\"\n        new_tweets = _getNewTweets(api, screen_name)\n\n        alltweets.extend(new_tweets)\n        # Storing manually all the json representation for the tweets\n        jTweets = []\n        for n in new_tweets:\n            jTweets.append(n._json)\n        if len(alltweets) > 0:\n            #save the id of the oldest tweet less one\n            oldest = alltweets[-1].id - 1\n\n            #keep grabbing tweets until there are no tweets left to grab\n            while len(new_tweets) > 0:\n                print \"Getting tweets before %s\" % (oldest)\n\n                \"\"\" #all subsequent requests use the max_id param to prevent duplicates\n                waiting = True\n                while waiting == True:\n                    try:\n                        # We have to update the oldest id\n                        new_tweets = api.user_timeline(screen_name = screen_name,count=200, max_id=oldest)\n                        waiting = False\n                        #save most recent tweets\n\n                    except:\n                        # Error... We will have to wait\n                        waiting = True\n                        print \"No more queries remaining, sleeping for \" + str(waitTime) +\" seconds...\"\n                        time.sleep(waitTime)  \"\"\"\n\n                new_tweets = _getNewTweets(api, screen_name, oldest=oldest)\n\n                # Extending the list of tweets\n                alltweets.extend(new_tweets)\n\n                #update the id of the oldest tweet less one\n                oldest = alltweets[-1].id - 1\n                print \"... %s tweets downloaded so far\" % (len(alltweets))\n                # Storing manually all the json representation for the tweets\n                for n in new_tweets:\n                    jTweets.append(n._json)\n        else:\n            # Verifying the limits of the API\n            print json.dumps(self._rate_limit_status(api=api, mode=\"get_all_docs\"), indent =2)\n\n        #transform the tweepy tweets into a 2D array that will populate the csv\n        outtweets = []\n        # This is how it is represented\n        \"\"\"\n          \"status\": {\n            \"lang\": \"es\",\n            \"favorited\": false,\n            \"entities\": {\n              \"symbols\": [],\n              \"user_mentions\": [],\n              \"hashtags\": [],\n              \"urls\": []\n            },\n            \"contributors\": null,\n            \"truncated\": false,\n            \"text\": \"Podemos confirmar que Alpify, aunque acabe en ...fy no es una aplicaci\\u00f3n nuestra. ;) \\u00a1A aprovechar lo que queda de domingo!\",\n            \"created_at\": \"Sun Aug 16 17:35:37 +0000 2015\",\n            \"retweeted\": true,\n            \"in_reply_to_status_id_str\": null,\n            \"coordinates\": null,\n            \"in_reply_to_user_id_str\": null,\n            \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\",\n            \"in_reply_to_status_id\": null,\n            \"in_reply_to_screen_name\": null,\n            \"id_str\": \"632968969662689280\",\n            \"place\": null,\n            \"retweet_count\": 1,\n            \"geo\": null,\n            \"id\": 632968969662689280,\n            \"favorite_count\": 0,\n            \"in_reply_to_user_id\": null\n          },\n        \"\"\"\n        for tweet in jTweets:\n            row =[]\n            row.append(tweet[\"id_str\"])\n            row.append(tweet[\"created_at\"])\n            row.append(tweet[\"text\"].encode(\"utf-8\"))\n            row.append(tweet[\"source\"])\n            row.append(tweet[\"coordinates\"])\n            row.append(tweet[\"retweet_count\"])\n            row.append(tweet[\"favorite_count\"])\n            row.append(tweet[\"lang\"])\n            row.append(tweet[\"place\"])\n            row.append(tweet[\"geo\"])\n            row.append(tweet[\"id\"])\n            row.append(screen_name)\n\n            # URLS\n            urls = []\n            \"\"\"\n            [\n                {\n                  \"url\": \"http://t.co/SGty7or6SQ\",\n                  \"indices\": [\n                    30,\n                    52\n                  ],\n                  \"expanded_url\": \"http://github.com/i3visio/osrframework\",\n                  \"display_url\": \"github.com/i3visio/osrfra\\u2026\"\n                }\n            ]\n            \"\"\"\n            for u in tweet[\"entities\"][\"urls\"]:\n                urls.append(u[\"expanded_url\"])\n            # Creating the string value for the cell\n            str_urls =\"\"\n            if len(urls) == 0:\n                str_urls = \"[N/A]\"\n            else:\n                for i, u in enumerate(urls):\n                    str_urls += u\n                    # Appending a separator\n                    if i+1 <> len(urls):\n                        str_urls+= \"|\"\n            row.append(str_urls.encode('utf-8'))\n\n            # TODO: Extract Mentions\n            mentions = []\n            \"\"\" \"user_mentions\": [\n              {\n                \"id\": 66345537,\n                \"indices\": [\n                  0,\n                  10\n                ],\n                \"id_str\": \"66345537\",\n                \"screen_name\": \"muchotomy\",\n                \"name\": \"Tomy\"\n              },\n            \"\"\"\n            for a in tweet[\"entities\"][\"user_mentions\"]:\n                mentions.append(a[\"screen_name\"])\n            # Creating the string value for the cell\n            str_mentions =\"\"\n            if len(mentions) == 0:\n                str_mentions = \"[N/A]\"\n            else:\n                for i, m in enumerate(mentions):\n                    str_mentions += m\n                    # Appending a separator\n                    if i+1 <> len(mentions):\n                        str_mentions+= \"|\"\n            row.append(str_mentions.encode('utf-8'))\n\n            # Appending the row to the output\n            outtweets.append(row)\n\n        # Writing the csv\n        with open('%s_tweets.csv' % screen_name, 'wb') as f:\n            writer = csv.writer(f)\n            # Writing the headers\n            writer.writerow([\n                \"_tweet_id\",\n                \"_tweet_created_at\",\n                \"_tweet_text\",\n                \"_tweet_source\",\n                \"_tweet_coordinates\",\n                \"_tweet_retweet_count\",\n                \"_tweet_favourite_count\",\n                \"_tweet_lang\",\n                \"i3visio_location\",\n                \"_tweet_geo\",\n                \"_twitter_id\",\n                \"i3visio_alias\",\n                \"i3visio_uri\",\n                \"i3visio_alias_mentions\",\n            ])\n            # Writing the rows\n            #writer.writerows(outtweets)\n            for o in outtweets:\n                try:\n                    writer.writerow(o)\n                except:\n                    print o\n\n        return jTweets"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_friends(self, query):\n        # Connecting to the API\n        api = self._connectToAPI()\n\n        # Verifying the limits of the API\n        self._rate_limit_status(api=api, mode=\"get_friends\")\n\n        # Making the call to the API\n        try:\n            friends_ids = api.friends_ids(query)\n        except:\n            return []\n\n        \"\"\"res = []\n        # Extracting the information from each profile\n        for a in aux:\n            us= self.getUser(a)\n            res.append(self._processUser(us))\"\"\"\n\n        return friends_ids", "response": "Method to get the friends of a user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_user(self, screen_name):\n        # Connecting to the API\n        api = self._connectToAPI()\n\n        # Verifying the limits of the API\n        self._rate_limit_status(api=api, mode=\"get_user\")\n\n        aux = []\n        try:\n            user = api.get_user(screen_name)\n            # Iterate through the results using user._json\n            aux.append(user._json)\n        except tweepy.error.TweepError as e:\n            pass\n\n        res = []\n        # Extracting the information from each profile\n        for a in aux:\n            res.append(self._processUser(a))\n        return res", "response": "Method to perform the usufy searches."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform the searchfy searches.", "response": "def search_users(self, query, n=20, maxUsers=60):\n        \"\"\"\n            Method to perform the searchfy searches.\n\n            :param query:   Query to be performed.\n            :param n:   Number of results per query.\n            :param maxUsers:    Max. number of users to be recovered.\n\n            :return:    List of users\n        \"\"\"\n        # Connecting to the API\n        api = self._connectToAPI()\n\n        # Verifying the limits of the API\n        self._rate_limit_status(api=api, mode=\"search_users\")\n\n        aux = []\n\n        page = 0\n\n        # print \"Getting page %s of new users...\" % page+1\n        # Making the call to the API\n        try:\n            newUsers = api.search_users(query, n, page)\n\n            for n in newUsers:\n                aux.append(n._json)\n\n            #keep grabbing tweets until there are no tweets left to grab\n            while len(aux) < maxUsers & len(newUsers)>0:\n                page+=1\n                print \"Getting page %s of new users...\" % page\n\n                # Grabbing new Users\n                newUsers = api.search_users(query, n, page)\n\n                # Save the users found\n                aux.extend(newUsers)\n        except:\n            pass\n\n        res = []\n        # Extracting the information from each profile\n        for a in aux:\n            res.append(self._processUser(a))\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_categories(categories):\n        if not set(categories) <= Source.categories:\n            invalid = list(set(categories) - Source.categories)\n            raise ValueError('Invalid categories: %s' % invalid)", "response": "Take an iterable of source categories and raise ValueError if some \n        of them are invalid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkIfHashIsCracked(hash=None):\n\n\tapiURL = \"http://md5db.net/api/\" + str(hash).lower()\n\n\ttry:\n\n\t\t# Getting the result of the query from MD5db.net\n\t\tdata = urllib2.urlopen(apiURL).read()\n\t\treturn data\n\n\texcept:\n\t\t# No information was found, then we return a null entity\n\t\treturn []", "response": "This method checks if the given hash is stored in the MD5db website."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fuzzUsufy(fDomains = None, fFuzzStruct = None):\n    if fFuzzStruct == None:\n        # Loading these structures by default\n        fuzzingStructures = [\n            \"http://<DOMAIN>/<USERNAME>\",\n            \"http://<DOMAIN>/~<USERNAME>\",\n            \"http://<DOMAIN>/?action=profile;user=<USERNAME>\",\n            \"http://<DOMAIN>/causes/author/<USERNAME>\",\n            \"http://<DOMAIN>/channel/<USERNAME>\",\n            \"http://<DOMAIN>/community/profile/<USERNAME>\",\n            \"http://<DOMAIN>/component/comprofiler/userprofiler/<USERNAME>\",\n            \"http://<DOMAIN>/details/@<USERNAME>\",\n            \"http://<DOMAIN>/foros/member.php?username=<USERNAME>\",\n            \"http://<DOMAIN>/forum/member/<USERNAME>\",\n            \"http://<DOMAIN>/forum/member.php?username=<USERNAME>\",\n            \"http://<DOMAIN>/forum/profile.php?mode=viewprofile&u=<USERNAME>\",\n            \"http://<DOMAIN>/home/<USERNAME>\",\n            \"http://<DOMAIN>/index.php?action=profile;user=<USERNAME>\",\n            \"http://<DOMAIN>/member_profile.php?u=<USERNAME>\",\n            \"http://<DOMAIN>/member.php?username=<USERNAME>\",\n            \"http://<DOMAIN>/members/?username=<USERNAME>\",\n            \"http://<DOMAIN>/members/<USERNAME>\",\n            \"http://<DOMAIN>/members/view/<USERNAME>\",\n            \"http://<DOMAIN>/mi-espacio/<USERNAME>\",\n            \"http://<DOMAIN>/u<USERNAME>\",\n            \"http://<DOMAIN>/u/<USERNAME>\",\n            \"http://<DOMAIN>/user-<USERNAME>\",\n            \"http://<DOMAIN>/user/<USERNAME>\",\n            \"http://<DOMAIN>/user/<USERNAME>.html\",\n            \"http://<DOMAIN>/users/<USERNAME>\",\n            \"http://<DOMAIN>/usr/<USERNAME>\",\n            \"http://<DOMAIN>/usuario/<USERNAME>\",\n            \"http://<DOMAIN>/usuarios/<USERNAME>\",\n            \"http://<DOMAIN>/en/users/<USERNAME>\",\n            \"http://<DOMAIN>/people/<USERNAME>\",\n            \"http://<DOMAIN>/profil/<USERNAME>\",\n            \"http://<DOMAIN>/profile/<USERNAME>\",\n            \"http://<DOMAIN>/profile/page/<USERNAME>\",\n            \"http://<DOMAIN>/rapidforum/index.php?action=profile;user=<USERNAME>\",\n            \"http://<DOMAIN>/social/usuarios/<USERNAME>\",\n            \"http://<USERNAME>.<DOMAIN>\",\n            \"http://<USERNAME>.<DOMAIN>/user/\"\n        ]\n    else:\n        try:\n            fuzzingStructures = fFuzzStruct.read().splitlines()\n        except:\n            print(\"Usufy could NOT open the following file: \" + fFuzzStruct)\n\n    res = {}\n\n    lines = fDomains.read().splitlines()\n\n    # Going through all the lines\n    for l in lines:\n        domain = l.split()[0]\n        print(\"Performing tests for\" + domain + \"...\")\n\n        # selecting the number of nicks to be tested in this domain\n        nick = l.split()[1]\n\n        # possibleURLs found\n        possibleURL = []\n\n        for struct in fuzzingStructures:\n            # initiating list\n            urlToTry = struct.replace(\"<DOMAIN>\", domain)\n            test = urlToTry.replace(\"<USERNAME>\", nick.lower())\n            print(\"Processing \"+ test + \"...\")\n            i3Browser = browser.Browser()\n            try:\n                html = i3Browser.recoverURL(test)\n                if nick in html:\n                    possibleURL.append(test)\n                    print(general.success(\"\\tPossible usufy found!!!\\n\"))\n            except:\n                print(\"The resource could not be downloaded.\")\n\n        res[domain] = possibleURL\n\n    print(json.dumps(res, indent = 2))\n    return res", "response": "Method to guess the usufy path against a list of domains or subdomains."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pool_function(p, nick, rutaDescarga, avoidProcessing=True, avoidDownload=True, verbosity=1):\n    try:\n        #res = getPageWrapper(p, nick, rutaDescarga, avoidProcessing, avoidDownload, outQueue)\n        res = p.getInfo(\n            query=nick,\n            mode=\"usufy\",\n            process=True\n        )\n        return {\"platform\" : str(p), \"status\": \"Ok\", \"data\": res}\n\n    except Exception as e:\n        if (isinstance(e, OSRFrameworkError) and verbosity >= 1) and (isinstance(e, OSRFrameworkException) and verbosity >= 2):\n            print(str(e))\n        return {\"platform\" : str(p), \"status\": e, \"data\": e.generic}", "response": "Wrapper for getting information from the system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef processNickList(nicks, platforms=None, rutaDescarga=\"./\", avoidProcessing=True, avoidDownload=True, nThreads=12, verbosity=1, logFolder=\"./logs\"):\n    if platforms == None:\n        platforms = platform_selection.getAllPlatformNames(\"usufy\")\n\n    # Defining the output results variable\n    res = []\n    # Processing the whole list of terms...\n    for nick in nicks:\n        # If the process is executed by the current app, we use the Processes. It is faster than pools.\n        if nThreads <= 0 or nThreads > len(platforms):\n            nThreads = len(platforms)\n\n        # Using threads in a pool if we are not running the program in main\n        # Example catched from: https://stackoverflow.com/questions/11312525/catch-ctrlc-sigint-and-exit-multiprocesses-gracefully-in-python\n        try:\n            original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n            pool = Pool(nThreads)\n            signal.signal(signal.SIGINT, original_sigint_handler)\n        except ValueError:\n            # To avoid: ValueError: signal only works in main thread\n            pool = Pool(nThreads)\n\n        poolResults = []\n        try:\n            def log_result(result):\n                # This is called whenever foo_pool(i) returns a result.\n                # result_list is modified only by the main process, not the pool workers.\n                poolResults.append(result)\n\n            for plat in platforms:\n                # We need to create all the arguments that will be needed\n                parameters = ( plat, nick, rutaDescarga, avoidProcessing, avoidDownload, verbosity)\n                pool.apply_async (pool_function, args=parameters, callback=log_result,)\n\n            # Waiting for results to be finished\n            while len(poolResults) < len(platforms):\n                time.sleep(1)\n\n            # Closing normal termination\n            pool.close()\n        except KeyboardInterrupt:\n            print(general.warning(\"\\n[!] Process manually stopped by the user. Terminating workers.\\n\"))\n            pool.terminate()\n            print(general.warning(\"[!] The following platforms were not processed:\"))\n            pending = \"\"\n            for p in platforms:\n                processed = False\n                for processedPlatform in poolResults:\n                    if str(p) == processedPlatform[\"platform\"]:\n                        processed = True\n                        break\n                if not processed:\n                    print(\"\\t- \" + str(p))\n                    pending += \" \" + str(p).lower()\n            print(\"\\n\")\n            print(general.warning(\"If you want to relaunch the app with these platforms you can always run the command with: \"))\n            print(\"\\t usufy ... -p \" + general.emphasis(pending))\n            print(\"\\n\")\n            print(general.warning(\"If you prefer to avoid these platforms you can manually evade them for whatever reason with: \"))\n            print(\"\\t usufy ... -x \" + general.emphasis(pending))\n            print(\"\\n\")\n        pool.join()\n\n        # Collecting the results\n        profiles = []\n        errors = {}\n        warnings = {}\n\n        for info in poolResults:\n            if info[\"status\"] == \"Ok\":\n                array = json.loads(info[\"data\"])\n                for r in array:\n                    if r != \"{}\":\n                        profiles.append(r)\n            else:\n                e = info[\"status\"]\n                if isinstance(e, OSRFrameworkError):\n                    aux = errors.get(e.__class__.__name__, {})\n                    aux[\"info\"] = info[\"data\"]\n                    aux[\"counter\"] = aux.get(\"counter\", 0) + 1\n                    errors[e.__class__.__name__] = aux\n                else:\n                    aux = warnings.get(e.__class__.__name__, {})\n                    aux[\"info\"] = info[\"data\"]\n                    aux[\"counter\"] = aux.get(\"counter\", 0) + 1\n                    warnings[e.__class__.__name__] = aux\n        res += profiles\n\n        if errors:\n            now = dt.datetime.now()\n            print(\"\\n{}\\tSome errors where found in the process:\".format(now))\n            for key, value in errors.items():\n                print(textwrap.fill(\"- {} (found: {}). Details:\".format(general.error(key), general.error(value[\"counter\"])), 90, initial_indent=\"\\t\"))\n                print(textwrap.fill(\"\\t{}\".format(value[\"info\"]), 80, initial_indent=\"\\t\"))\n\n        if warnings and verbosity >= 2:\n            now = dt.datetime.now()\n            print(\"\\n{}\\tSome warnings where found in the process:\".format(now))\n            for key, value in warnings.items():\n                print(textwrap.fill(\"- {} (found: {}). Details:\".format(general.warning(key), general.warning(value[\"counter\"])), 90, initial_indent=\"\\t\"))\n                print(textwrap.fill(\"\\t{}\".format(value[\"info\"]), 80, initial_indent=\"\\t\"))\n\n    return res", "response": "This method processes a list of nicks and returns a dictionary that contains the information for each profile associated with the nick."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prepare_filtering_params(domain=None, category=None, \r\n                                  sponsored_source=None, has_field=None,\r\n                                  has_fields=None, query_params_match=None, \r\n                                  query_person_match=None, **kwargs):\r\n        \"\"\"Transform the params to the API format, return a list of params.\"\"\"\r\n        if query_params_match not in (None, True):\r\n            raise ValueError('query_params_match can only be `True`')\r\n        if query_person_match not in (None, True):\r\n            raise ValueError('query_person_match can only be `True`')\r\n        \r\n        params = []\r\n        if domain is not None:\r\n            params.append('domain:%s' % domain)\r\n        if category is not None:\r\n            Source.validate_categories([category])\r\n            params.append('category:%s' % category)\r\n        if sponsored_source is not None:\r\n            params.append('sponsored_source:%s' % sponsored_source)\r\n        if query_params_match is not None:\r\n            params.append('query_params_match')\r\n        if query_person_match is not None:\r\n            params.append('query_person_match')\r\n        has_fields = has_fields or []\r\n        if has_field is not None:\r\n            has_fields.append(has_field)\r\n        for has_field in has_fields:\r\n            params.append('has_field:%s' % has_field.__name__)\r\n        return params", "response": "Transform the params to the API format return a list of params."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new and filter to the list of records that match the given criteria.", "response": "def add_records_filter(self, domain=None, category=None, \r\n                           sponsored_source=None, has_fields=None, \r\n                           query_params_match=None, query_person_match=None):\r\n        \"\"\"Add a new \"and\" filter for the records returned in the response.\r\n        \r\n        IMPORTANT: This method can be called multiple times per request for \r\n        adding multiple \"and\" filters, each of these \"and\" filters is \r\n        interpreted as \"or\" with the other filters.\r\n        For example:\r\n        \r\n        >>> from osrframework.thirdparties.pipl_com.lib.search import SearchAPIRequest\r\n        >>> from osrframework.thirdparties.pipl_com.lib import Phone, Job\r\n        >>> request = SearchAPIRequest('samplekey', username='eric123')\r\n        >>> request.add_records_filter(domain='linkedin', has_fields=[Phone])\r\n        >>> request.add_records_filter(has_fields=[Phone, Job])\r\n        \r\n        The above request is only for records that are:\r\n        (from LinkedIn AND has a phone) OR (has a phone AND has a job).\r\n        Records that don't match this rule will not come back in the response.\r\n        \r\n        Please note that in case there are too many results for the query, \r\n        adding filters to the request can significantly improve the number of\r\n        useful results; when you define which records interest you, you'll\r\n        get records that would have otherwise be cut-off by the limit on the\r\n        number of records per query.\r\n        \r\n        Args:\r\n        \r\n        domain -- str, for example \"linkedin.com\", you may also use \"linkedin\"\r\n                  but note that it'll match \"linkedin.*\" and \"*.linkedin.*\" \r\n                  (any sub-domain and any TLD).\r\n        category -- str, any one of the categories defined in\r\n                    osrframework.thirdparties.pipl_com.lib.source.Source.categories.\r\n        sponsored_source -- bool, True means you want just the records that \r\n                            come from a sponsored source and False means you \r\n                            don't want these records.\r\n        has_fields -- A list of fields classes from osrframework.thirdparties.pipl_com.lib.fields, \r\n                      records must have content in all these fields.\r\n                      For example: [Name, Phone] means you only want records \r\n                      that has at least one name and at least one phone.\r\n        query_params_match -- True is the only possible value and it means you \r\n                              want records that match all the params you passed \r\n                              in the query.\r\n        query_person_match -- True is the only possible value and it means you\r\n                              want records that are the same person you \r\n                              queried by (only records with \r\n                              query_person_match == 1.0, see the documentation \r\n                              of record.query_person_match for more details).\r\n        \r\n        ValueError is raised in any case of an invalid parameter.\r\n        \r\n        \"\"\"\r\n        params = SearchAPIRequest._prepare_filtering_params(**locals())\r\n        if params:\r\n            self._filter_records_by.append(' AND '.join(params))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending a new priority rule for the records returned in the response.", "response": "def append_priority_rule(self, domain=None, category=None, \r\n                             sponsored_source=None, has_field=None, \r\n                             query_params_match=None, query_person_match=None):\r\n        \"\"\"Append a new priority rule for the records returned in the response.\r\n        \r\n        IMPORTANT: This method can be called multiple times per request for \r\n        adding multiple priority rules, each call can be with only one argument\r\n        and the order of the calls matter (the first rule added is the highest \r\n        priority, the second is second priority etc).\r\n        For example:\r\n        \r\n        >>> from osrframework.thirdparties.pipl_com.lib.search import SearchAPIRequest\r\n        >>> from osrframework.thirdparties.pipl_com.lib import Phone\r\n        >>> request = SearchAPIRequest('samplekey', username='eric123')\r\n        >>> request.append_priority_rule(domain='linkedin')\r\n        >>> request.append_priority_rule(has_field=Phone)\r\n        \r\n        In the response to the above request records from LinkedIn will be \r\n        returned before records that aren't from LinkedIn and records with \r\n        phone will be returned before records without phone. \r\n        \r\n        Please note that in case there are too many results for the query,\r\n        adding priority rules to the request does not only affect the order \r\n        of the records but can significantly improve the number of useful \r\n        results; when you define which records interest you, you'll get records\r\n        that would have otherwise be cut-off by the limit on the number\r\n        of records per query.  \r\n\r\n        Args:\r\n        \r\n        domain -- str, for example \"linkedin.com\", \"linkedin\" is also possible \r\n                  and it'll match \"linkedin.*\".\r\n        category -- str, any one of the categories defined in\r\n                    osrframework.thirdparties.pipl_com.lib.source.Source.categories.\r\n        sponsored_source -- bool, True will bring the records that \r\n                            come from a sponsored source first and False \r\n                            will bring the non-sponsored records first.\r\n        has_fields -- A field class from osrframework.thirdparties.pipl_com.lib.fields.\r\n                      For example: has_field=Phone means you want to give \r\n                      a priority to records that has at least one phone.\r\n        query_params_match -- True is the only possible value and it means you \r\n                              want to give a priority to records that match all \r\n                              the params you passed in the query.\r\n        query_person_match -- True is the only possible value and it means you\r\n                              want to give a priority to records with higher\r\n                              query_person_match (see the documentation of \r\n                              record.query_person_match for more details).\r\n                     \r\n        ValueError is raised in any case of an invalid parameter.\r\n        \r\n        \"\"\"\r\n        params = SearchAPIRequest._prepare_filtering_params(**locals())\r\n        if len(params) > 1:\r\n            raise ValueError('The function should be called with one argument')\r\n        if params:\r\n            self._prioritize_records_by.append(params[0])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_query_params(self, strict=True):\r\n        if not (self.api_key or default_api_key):\r\n            raise ValueError('API key is missing')\r\n        if strict and self.query_params_mode not in (None, 'and', 'or'):\r\n            raise ValueError('query_params_match should be one of \"and\"/\"or\"')\r\n        if not self.person.is_searchable:\r\n            raise ValueError('No valid name/username/phone/email in request')\r\n        if strict and self.person.unsearchable_fields:\r\n            raise ValueError('Some fields are unsearchable: %s' \r\n                             % self.person.unsearchable_fields)", "response": "Validate the query parameters for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, strict_validation=True):\r\n        self.validate_query_params(strict=strict_validation)\r\n        query = {\r\n            'key': self.api_key or default_api_key,\r\n            'person': self.person.to_json(),\r\n            'query_params_mode': self.query_params_mode,\r\n            'exact_name': self.exact_name,\r\n            'prioritize_records_by': ','.join(self._prioritize_records_by),\r\n            'filter_records_by': self._filter_records_by,\r\n        }\r\n        request = urllib2.Request(url=SearchAPIRequest.BASE_URL, data=urllib.urlencode(query, True), headers=SearchAPIRequest.HEADERS)\r\n        try:\r\n            json_response = urllib2.urlopen(request).read()\r\n        except urllib2.HTTPError as e:\r\n            json_error = e.read()\r\n            if not json_error:\r\n                raise e\r\n            try:\r\n                raise SearchAPIError.from_json(json_error)\r\n            except ValueError:\r\n                raise e\r\n        return SearchAPIResponse.from_json(json_response)", "response": "Send the request and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict with the records grouped by the key returned by key_function.", "response": "def group_records(self, key_function):\r\n        \"\"\"Return a dict with the records grouped by the key returned by \r\n        `key_function`.\r\n        \r\n        `key_function` takes a record and returns the value from the record to\r\n        group by (see examples in the group_records_by_* methods below).\r\n        \r\n        The return value is a dict, a key in this dict is a key returned by\r\n        `key_function` and the value is a list of all the records with this key.\r\n        \r\n        \"\"\"\r\n        sorted_records = sorted(self.records, key=key_function)\r\n        grouped_records = itertools.groupby(sorted_records, key=key_function)\r\n        return dict([(key, list(group)) for key, group in grouped_records])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef group_records_by_domain(self):\r\n        key_function = lambda record: record.source.domain\r\n        return self.group_records(key_function)", "response": "Return the records grouped by the domain they came from."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef group_records_by_category(self):\r\n        Source.validate_categories(categories)\r\n        key_function = lambda record: record.source.category\r\n        return self.group_records(key_function)", "response": "Return the records grouped by the category of their source."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_dict(d):\r\n        warnings_ = d.get('warnings', [])\r\n        query = d.get('query') or None\r\n        if query:\r\n            query = Person.from_dict(query)\r\n        person = d.get('person') or None\r\n        if person:\r\n            person = Person.from_dict(person)\r\n        records = d.get('records')\r\n        if records:\r\n            records = [Record.from_dict(record) for record in records]\r\n        suggested_searches = d.get('suggested_searches')\r\n        if suggested_searches:\r\n            suggested_searches = [Record.from_dict(record) \r\n                                  for record in suggested_searches]\r\n        return SearchAPIResponse(query=query, person=person, records=records, \r\n                                 suggested_searches=suggested_searches,\r\n                                 warnings_=warnings_)", "response": "Transform the dict to a response object and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict representation of the response.", "response": "def to_dict(self):\r\n        \"\"\"Return a dict representation of the response.\"\"\"\r\n        d = {}\r\n        if self.warnings:\r\n            d['warnings'] = self.warnings\r\n        if self.query is not None:\r\n            d['query'] = self.query.to_dict()\r\n        if self.person is not None:\r\n            d['person'] = self.person.to_dict()\r\n        if self.records:\r\n            d['records'] = [record.to_dict() for record in self.records]\r\n        if self.suggested_searches:\r\n            d['suggested_searches'] = [record.to_dict() \r\n                                       for record in self.suggested_searches]\r\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_type(self, type_):\r\n        if type_ is not None and type_ not in self.types_set:\r\n            raise ValueError('Invalid type for %s:%s' % (self.__class__, type_))", "response": "Take an str / unicode type_ and raise a ValueError if it s not a valid type for the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming the dict to a field object and return the field.", "response": "def from_dict(cls, d):\r\n        \"\"\"Transform the dict to a field object and return the field.\"\"\"\r\n        kwargs = {}\r\n        for key, val in d.iteritems():\r\n            if key.startswith('display'): # includes phone.display_international\r\n                continue\r\n            if key.startswith('@'):\r\n                key = key[1:]\r\n            if key == 'type':\r\n                key = 'type_'\r\n            elif key == 'valid_since':\r\n                val = str_to_datetime(val)\r\n            elif key == 'date_range':\r\n                val = DateRange.from_dict(val)\r\n            kwargs[key.encode('ascii')] = val\r\n        return cls(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dict representation of the field.", "response": "def to_dict(self):\r\n        \"\"\"Return a dict representation of the field.\"\"\"\r\n        d = {}\r\n        if self.valid_since is not None:\r\n            d['@valid_since'] = datetime_to_str(self.valid_since)\r\n        for attr_list, prefix in [(self.attributes, '@'), (self.children, '')]:\r\n            for attr in attr_list:\r\n                value = getattr(self, attr)\r\n                if isinstance(value, Serializable):\r\n                    value = value.to_dict()\r\n                if value or isinstance(value, (bool, int, long)):\r\n                    d[prefix + attr] = value\r\n        if hasattr(self, 'display') and self.display:\r\n            d['display'] = self.display\r\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef state_full(self):\r\n    \r\n        if self.is_valid_state:\r\n            return STATES[self.country.upper()].get(self.state.upper())", "response": "unicode the full name of the state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_text(text):\r\n        number = int(filter(unicode.isdigit, unicode(text)))\r\n        return Phone(number=number)", "response": "Return a new instance of the class Phone object with the number from text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_dict(cls, d):\r\n        phone = super(cls, cls).from_dict(d)\r\n        phone.display = d.get('display', u'')\r\n        phone.display_international = d.get('display_international', u'')\r\n        return phone", "response": "Extend Field. from_dict set display and display_international attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self):\r\n        d = Field.to_dict(self)\r\n        if self.display_international:\r\n            d['display_international'] = self.display_international\r\n        return d", "response": "Extend Field. to_dict take the display_international attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef display(self):\r\n        if self.title and self.organization:\r\n            disp = self.title + u' at ' + self.organization\r\n        else:\r\n            disp = self.title or self.organization or None\r\n        if disp and self.industry:\r\n            if self.date_range is not None:\r\n                disp += u' (%s, %d-%d)' % ((self.industry,) + \\\r\n                                           self.date_range.years_range)\r\n            else:\r\n                disp += u' (%s)' % self.industry\r\n        else:\r\n            disp = ((disp or u'') + u' ' + (self.industry or u'')).strip()\r\n            if disp and self.date_range is not None:\r\n                disp += u' (%d-%d)' % self.date_range.years_range\r\n        return disp", "response": "A unicode value with the object s data to be used for displaying \r\n            the object in your application."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef age(self):\r\n        if self.date_range is None:\r\n            return\r\n        dob = self.date_range.middle\r\n        today = datetime.date.today()\r\n        if (today.month, today.day) < (dob.month, dob.day):\r\n            return today.year - dob.year - 1\r\n        else:\r\n            return today.year - dob.year", "response": "int the estimated age of the person."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef age_range(self):\r\n        if self.date_range is None:\r\n            return None, None\r\n        start_date = DateRange(self.date_range.start, self.date_range.start)\r\n        end_date = DateRange(self.date_range.end, self.date_range.end)\r\n        start_age = DOB(date_range=end_date).age\r\n        end_age = DOB(date_range=start_date).age\r\n        return start_age, end_age", "response": "A tuple of two ints - the minimum and maximum age of the person."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_birth_year(birth_year):\r\n        if birth_year <= 0:\r\n            raise ValueError('birth_year must be positive')\r\n        date_range = DateRange.from_years_range(birth_year, birth_year)\r\n        return DOB(date_range=date_range)", "response": "Take a person s birth year and return a new DOB object \r\n        suitable for him."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a person s birth date and return a new DOB object suitable for him.", "response": "def from_birth_date(birth_date):\r\n        \"\"\"Take a person's birth date (datetime.date) and return a new DOB \r\n        object suitable for him.\"\"\"\r\n        if birth_date > datetime.date.today():\r\n            raise ValueError('birth_date can\\'t be in the future')\r\n        date_range = DateRange(birth_date, birth_date)\r\n        return DOB(date_range=date_range)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a person s minimal and maximal age and return a new DOB object suitable for him.", "response": "def from_age_range(start_age, end_age):\r\n        \"\"\"Take a person's minimal and maximal age and return a new DOB object \r\n        suitable for him.\"\"\"\r\n        if start_age < 0 or end_age < 0:\r\n            raise ValueError('start_age and end_age can\\'t be negative')\r\n        \r\n        if start_age > end_age:\r\n            start_age, end_age = end_age, start_age\r\n            \r\n        today = datetime.date.today()\r\n        \r\n        try:\r\n            start_date = today.replace(year=today.year - end_age - 1)\r\n        except ValueError:  # February 29\r\n            start_date = today.replace(year=today.year - end_age - 1, day=28)\r\n        start_date += datetime.timedelta(days=1)\r\n        \r\n        try:\r\n            end_date = today.replace(year=today.year - start_age)\r\n        except ValueError:  # February 29\r\n            end_date = today.replace(year=today.year - start_age, day=28)\r\n        \r\n        date_range = DateRange(start_date, end_date)\r\n        return DOB(date_range=date_range)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_dict(cls, d):\r\n        relationship = super(cls, cls).from_dict(d)\r\n        if relationship.name is not None:\r\n            relationship.name = Name.from_dict(relationship.name)\r\n        return relationship", "response": "Extend Field. from_dict and also load the name from the dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms a range of years ( two ints ) to a DateRange object.", "response": "def from_years_range(start_year, end_year):\r\n        \"\"\"Transform a range of years (two ints) to a DateRange object.\"\"\"\r\n        start = datetime.date(start_year, 1 , 1)\r\n        end = datetime.date(end_year, 12 , 31)\r\n        return DateRange(start, end)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_dict(d):\r\n        start = d.get('start')\r\n        end = d.get('end')\r\n        if not (start and end):\r\n            raise ValueError('DateRange must have both start and end')\r\n        start = str_to_date(start)\r\n        end = str_to_date(end)\r\n        return DateRange(start, end)", "response": "Transform the dict to a DateRange object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self):\r\n        d = {}\r\n        d['start'] = date_to_str(self.start)\r\n        d['end'] = date_to_str(self.end)\r\n        return d", "response": "Transform the date - range to a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning that performs the enumeration itself.", "response": "def enumerateURL(urlDict, outputFolder, startIndex= 0, maxErrors = 100):\r\n    \"\"\"\r\n    Function that performs the enumeration itself.\r\n    \"\"\"\r\n    for i, url in enumerate(urlDict.keys()):\r\n        # Grabbing domain name:\r\n        domain = re.findall(\"://(.*)/\", url)[0]\r\n\r\n        # Defining the starting index\r\n        index = startIndex\r\n\r\n        # The app will stop when this value reaches maxErrors\r\n        consecutiveErrors = 0\r\n\r\n        i3Browser = browser.Browser()\r\n\r\n        # Main loop that checks if the maximum number of errors has been reached\r\n        while consecutiveErrors <= maxErrors:\r\n            # creating the new URL to download\r\n            newQuery = url.replace(\"<INDEX>\", str(index))\r\n            print(newQuery)\r\n            # Downloading the file\r\n            try:\r\n                data = i3Browser.recoverURL(newQuery)\r\n\r\n                filename = domain.replace(\"/\", \"|\") + \"_\" + \"-profile_\" + str(index).rjust(10, \"0\") +\".html\"\r\n\r\n                if urlDict[url] != None:\r\n                    if urlDict[url] in data:\r\n                        print(general.info(\"Storing resource as:\\t\" + filename + \"...\"))\r\n                        # The profile was found  so we will store it:\r\n                        with open( outputFolder + \"/\" + filename, \"w\") as oF:\r\n                            oF.write(data)\r\n                else:\r\n                    # The profile was found  so we will store it:\r\n                    print(general.info(\"Storing resource as:\\t\" + filename + \"...\"))\r\n                    with open( outputFolder + \"/\" + filename, \"w\") as oF:\r\n                        oF.write(data)\r\n            except:\r\n                pass\r\n                #logger.error(\"The resource could not be downloaded.\")\r\n\r\n            index+=1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify a usufy query in this platform. This might be redefined in any class inheriting from Platform. Args: ----- query: The element to be searched. Return: ------- A list of elements to be appended.", "response": "def do_usufy(self, query, **kwargs):\n        \"\"\"\n        Verifying a usufy query in this platform.\n\n        This might be redefined in any class inheriting from Platform.\n\n        Args:\n        -----\n            query: The element to be searched.\n\n        Return:\n        -------\n            A list of elements to be appended.\n        \"\"\"\n        # Trying to interact with the API Wrapper\n        try:\n            self.wrapperAPI = TwitterAPIWrapper()\n\n            results = self.wrapperAPI.get_user(query)\n\n            for r in results:\n                # Manually appending the URL\n                aux = {}\n                aux[\"type\"]=\"i3visio.uri\"\n                alias=r[\"value\"].split(' - ')[1]\n                aux[\"value\"]= self.createURL(word=alias, mode=\"usufy\")\n                aux[\"attributes\"]= []\n                r[\"attributes\"].append(aux)\n\n        # Standard execution\n        except Exception, e:\n            return super(Twitter, self).do_usufy(query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify a usufy query in this platform. This might be redefined in any class inheriting from Platform. Args: ----- query: The element to be searched. Return: ------- A list of elements to be appended.", "response": "def do_searchfy(self, query, **kwargs):\n        \"\"\"\n        Verifying a usufy query in this platform.\n\n        This might be redefined in any class inheriting from Platform.\n\n        Args:\n        -----\n            query: The element to be searched.\n\n        Return:\n        -------\n            A list of elements to be appended.\n        \"\"\"\n        # Trying to interact with the API Wrapper\n        try:\n            results = self.wrapperAPI.search_users(query)\n            # Manually appending the URL\n            for r in results:\n                aux = {}\n                aux[\"type\"]=\"i3visio.uri\"\n                alias=r[\"value\"].split(' - ')[1]\n                qURL = self.createURL(word=alias, mode=\"usufy\")\n                aux[\"value\"]= qURL\n                aux[\"attributes\"]= []\n                r[\"attributes\"].append(aux)\n\n        # Standard execution\n        except Exception, e:\n            return super(Twitter, self).do_searchfy(query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of tuples containing the login and password for the current application.", "response": "def returnListOfCreds():\n    '''\n        :return:\n            A list of tuples containing in the first the name of the platform,\n            as read from the accounts.cfg file in the application folder. E. g.:\n\n            listCreds.append((\"<platform>\", \"<username>\", \"<password>\"))\n    '''\n    listCreds = []\n    # If a accounts.cfg has not been found, creating it by copying from default\n    configPath = os.path.join(configuration.getConfigPath()[\"appPath\"], \"accounts.cfg\")\n\n    # Checking if the configuration file exists\n    if not os.path.exists(configPath):\n        # Copy the data from the default folder\n        defaultConfigPath = os.path.join(configuration.getConfigPath()[\"appPathDefaults\"], \"accounts.cfg\")\n\n        try:\n            with open(defaultConfigPath) as iF:\n                cont = iF.read()\n                with open(configPath, \"w\") as oF:\n                    oF.write(cont)\n        except Exception, e:\n            raise errors.ConfigurationFileNotFoundError(configPath, defaultConfigPath);\n            return listCreds\n\n    # Reading the configuration file\n    config = ConfigParser.ConfigParser()\n    config.read(configPath)\n\n    # Iterating through all the sections, which contain the platforms\n    for platform in config.sections():\n        # Initializing values\n        creds = {}\n\n        incomplete = False\n\n        # Iterating through parametgers\n        for (param, value) in config.items(platform):\n            if value == '':\n                incomplete = True\n                break\n            creds[param] = value\n\n        # Appending credentials if possible\n        try:\n            if not incomplete:\n                listCreds.append((platform, creds[\"login\"], creds[\"password\"]))\n        except:\n            pass\n\n    return listCreds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getResults(uri):\n    '''\n        Method that recovers the text for each result in infobel.com\n        \n        :param uri: Infobel uri\n        \n        :return:    A list of textual information to be processed\n    '''\n    # Using i3visio browser to avoid certain issues...\n    i3Browser = browser.Browser()\n\n    data = i3Browser.recoverURL(uri)        \n\n    # Strings to be searched\n    regExp = \"<!-- Results -->(.*)<!-- /Results -->\"\n    # re.DOTALL is needed to match any character INCLUDING \\n\n    results = re.findall(regExp, data, re.DOTALL)\n    \n    return results", "response": "Method that recovers the text for each result in infobel. com\n        \n        and returns a list of textual information to be processed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef checkIfEmailWasHacked(email=None, sleepSeconds=1):\n    # Sleeping just a little bit\n    time.sleep(sleepSeconds)\n\n    print(\"\\t[*] Bypassing Cloudflare Restriction...\")\n    ua = 'osrframework 0.18'\n    useragent = {'User-Agent': ua}\n    cookies, user_agent = cfscrape.get_tokens('https://haveibeenpwned.com/api/v2/breachedaccount/test@example.com', user_agent=ua)\n\n    leaks = []\n\n    apiURL = \"https://haveibeenpwned.com/api/v2/breachedaccount/{}\".format(email)\n\n    # Accessing the HIBP API\n    time.sleep(sleepSeconds)\n    # Building API query\n    data = requests.get(\n        apiURL,\n        headers=useragent,\n        cookies=cookies,\n        verify=True\n    ).text\n\n    # Reading the text data onto python structures\n    try:\n        jsonData = json.loads(data)\n\n        for e in jsonData:\n            # Building the i3visio like structure\n            new = {}\n            new[\"value\"] = \"(HIBP) \" + e.get(\"Name\") + \" - \" + email\n            new[\"type\"] = \"i3visio.profile\"\n            new[\"attributes\"] = [\n                {\n                    \"type\": \"i3visio.platform_leaked\",\n                    \"value\": e.get(\"Name\"),\n                    \"attributes\": []\n                },\n                {\n                    \"type\": \"@source\",\n                    \"value\": \"haveibeenpwned.com\",\n                    \"attributes\": []\n                },\n                {\n                    \"type\": \"@source_uri\",\n                    \"value\": apiURL,\n                    \"attributes\": []\n                },\n                {\n                    \"type\": \"@pwn_count\",\n                    \"value\": e.get(\"PwnCount\"),\n                    \"attributes\": []\n                },\n                {\n                    \"type\": \"@added_date\",\n                    \"value\": e.get(\"AddedDate\"),\n                    \"attributes\": []\n                },\n                {\n                    \"type\": \"@breach_date\",\n                    \"value\": e.get(\"BreachDate\"),\n                    \"attributes\": []\n                },\n                {\n                    \"type\": \"@description\",\n                    \"value\": e.get(\"Description\"),\n                    \"attributes\": []\n                }\n            ] + general.expandEntitiesFromEmail(email)\n            leaks.append(new)\n    except ValueError:\n        return []\n    except Exception:\n        print(\"ERROR: Something happenned when using HIBP API.\")\n        return []\n\n    return leaks", "response": "Method that checks if the given email is stored in the HIBP website."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checkInfo(email=None, username=None, api_key=None):\n    ''' \n        Method that checks if the given hash is stored in the pipl.com website. \n\n        :param email: queries to be launched.\n        :param api_key: api_key to be used in pipl.com. If not provided, the API key will be searched in the config_api_keys.py file.\n\n        :return:    Python structure for the Json received. It has the following structure:\n    '''\n    # This is for i3visio\n    if api_key==None:\n        #api_key = raw_input(\"Insert the API KEY here:\\t\")\n        allKeys = config_api_keys.returnListOfAPIKeys()\n        try: \n            api_key = allKeys[\"pipl_com\"]\n        except:\n            # API_Key not found. The samplekey will be used but it has a limit of 10 queries/day.\n            api_key = \"samplekey\"            \n\n    results = {}\n    results[\"person\"] = []\n    results[\"records\"] = []    \n\n    if username != None:\n        request = SearchAPIRequest( username=username, api_key=api_key)\n        person, records = launchRequest(request)\n        # Appending the results \n        results[\"person\"].append(person)\n        results[\"records\"].append(records)        \n    if email != None:\n        request = SearchAPIRequest( email=email, api_key=api_key)\n        person, records = launchRequest(request)\n        # Appending the results \n        results[\"person\"].append(person)\n        results[\"records\"].append(records)        \n    return results", "response": "Method that checks if the given hash is stored in the pipl. com website."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef launchRequest(request):\n    '''\n        Method to launch a given request.\n        \n        :param request: The request object.\n        \n        :return:    A dictionary containinf the results of the person and a list of dicts containing the references for the record.\n    '''\n    person = {}\n    records = []\n   \n    try:\n        response = request.send()\n        # Trying to recover a person object. This is a dict:\n        try:\n            person = (response.person).to_dict()\n        except:\n            pass                \n        # Trying to recover a list of record objects. This is a list dicts\n        try:\n            aux = response.records \n            records = [r.to_dict() for r in aux]\n        except:\n            pass\n    except SearchAPIError as e:\n        print e.http_status_code, e    \n        \n    return  person, records", "response": "Method to launch a given request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the logger to be used for the whole app. This method may be invoked if required by the launcher to update the verbosity syntax. :param loggerName: Name of the package or app that is going to use this logger. :param logFolder: Path to the folder where the information will be logged. :param verbosity: Level of verbosity to be used: - 0: Only errors. - 1: Standard output. - 2: Verbose level with rich outputs. :return: The logger already created.", "response": "def setupLogger(loggerName=\"osrframework\", logFolder=\"./logs\", verbosity=0):\n    \"\"\" \n        Returns the logger to be used for the whole app. This method may be invoked if required by the launcher to update the verbosity syntax.\n    \n        :param loggerName:    Name of the package or app that is going to use this logger.\n        :param logFolder:    Path to the folder where the information will be logged.\n        :param verbosity:    Level of verbosity to be used: \n            - 0:    Only errors.\n            - 1:    Standard output.\n            - 2:    Verbose level with rich outputs.\n            \n        :return:    The logger already created.\n    \"\"\"\n    logger = logging.getLogger(loggerName)\n\n    # create a logging format\n    loginFormat = '%(asctime)s [%(filename)s] - %(levelname)s:\\n\\t%(message)s\\n'\n\n    formatter = logging.Formatter(loginFormat)\n\n    # first, defining the type of standard output and verbosity \n    if verbosity == 0:\n        logging.basicConfig(level=logging.ERROR, format=loginFormat)\n    elif verbosity == 1:\n        logging.basicConfig(level=logging.INFO, format=loginFormat)\n    elif verbosity == 2:\n        logging.basicConfig(level=logging.DEBUG, format=loginFormat)\n\n    # trying to store the logfile\n    try:\n        # verifying if the logs folder exist\n        logFolder = os.path.join(os.path.dirname(os.path.realpath(__file__)), logFolder)\n        if not os.path.exists(logFolder):\n            os.makedirs(logFolder)\n        # create a file handler\n        logFile = os.path.join(logFolder, loggerName+\".log\")\n    \n        # This adds a handler to write on a file\n        handler = logging.FileHandler(logFile)\n        handler.setLevel(logging.DEBUG)\n        formatterLogFile = logging.Formatter(loginFormat)\n        handler.setFormatter(formatterLogFile)\n    \n        # add the handlers to the logger\n        logger.addHandler(handler)\n    except:\n        logger.warning(\"The log file could not be created. No log will be stored for this session.\")\n\n    # Notifying correctly import\n    #logger.debug(loggerName+ \" successfully imported.\")\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify a mailfy query in this platform. This might be redefined in any class inheriting from Platform. The only condition is that any of this should return an equivalent array. Args: ----- query: The element to be searched. Return: ------- A list of elements to be appended. A sample output format is as follows: [ { \"attributes\": [ { \"attributes\": [], \"type\": \"i3visio.email\", \"value\": \"contacto@i3visio.com\" }, { \"attributes\": [], \"type\": \"i3visio.alias\", \"value\": \"contacto\" }, { \"attributes\": [], \"type\": \"i3visio.domain\", \"value\": \"i3visio.com\" }, { \"attributes\": [], \"type\": \"i3visio.platform\", \"value\": \"Twitter\" } ], \"type\": \"i3visio.profile\", \"value\": \"Twitter - contacto@i3visio.com\" } ]", "response": "def do_mailfy(self, query):\n        \"\"\"\n        Verifying a mailfy query in this platform.\n\n        This might be redefined in any class inheriting from Platform. The only\n        condition is that any of this should return an equivalent array.\n\n        Args:\n        -----\n            query: The element to be searched.\n\n        Return:\n        -------\n            A list of elements to be appended. A sample output format is as follows:\n            [\n              {\n                \"attributes\": [\n                  {\n                    \"attributes\": [],\n                    \"type\": \"i3visio.email\",\n                    \"value\": \"contacto@i3visio.com\"\n                  },\n                  {\n                    \"attributes\": [],\n                    \"type\": \"i3visio.alias\",\n                    \"value\": \"contacto\"\n                  },\n                  {\n                    \"attributes\": [],\n                    \"type\": \"i3visio.domain\",\n                    \"value\": \"i3visio.com\"\n                  },\n                  {\n                    \"attributes\": [],\n                    \"type\": \"i3visio.platform\",\n                    \"value\": \"Twitter\"\n                  }\n                ],\n                \"type\": \"i3visio.profile\",\n                \"value\": \"Twitter - contacto@i3visio.com\"\n              }\n            ]\n        \"\"\"\n        from bs4 import BeautifulSoup\n        #LINKEDIN-------------------------------------------------\n        r = br.open('https://www.linkedin.com/')\n        br.select_form(nr=0)\n        br.form[\"session_key\"] = query\n        br.form[\"session_password\"] = \"123456\"\n        br.submit()\n        respuestaURL = br.response().geturl()\n        if \"captcha\" in respuestaURL:\n        \tprint \"|--[INFO][LinkedIn][Captcha][>] Captcha detect!\"\n        else:\n        \tpass\n        html = br.response().read()\n        #print \"|--[INFO][LinkedIn][URL][>] \" + respuestaLI\n        soup = BeautifulSoup(html, \"html.parser\")\n        for span in soup.findAll(\"span\", {\"class\", \"error\"}):\n        \tdata = remove_tags(str(span))\n        \tif \"password\" in data:\n        \t\tprint \"|--[INFO][LinkedIn][CHECK][>] The account exist...\"\n        \t\tif state == 1:\n        \t\t\tprint colores.blue + \"|--[INFO][LinkedIn][CHECK][>] it's possible to hack it !!!\" + colores.normal\n        \tif \"recognize\" in data:\n                print \"|--[INFO][LinkedIn][CHECK][>] The account doesn't exist...\"\n\n        data = self.launchQueryForMode(query=query, mode=\"mailfy\")\n\n        expandedEntities = general.expandEntitiesFromEmail(query)\n\n        if self.somethingFound(data, mode=\"mailfy\"):\n            r = {\n                \"type\": \"i3visio.profile\",\n                \"value\": self.platformName + \" - \" + query,\n                \"attributes\": expandedEntities + [\n                    {\n                        \"type\": \"i3visio.platform\",\n                        \"value\": self.platformName,\n                        \"attributes\": []\n                    }\n                ]\n            }\n            return [r]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting the given URL and return the response page.", "response": "def get_page(url):\r\n    \"\"\"\r\n    Request the given URL and return the response page, using the cookie jar.\r\n\r\n    @type  url: str\r\n    @param url: URL to retrieve.\r\n\r\n    @rtype:  str\r\n    @return: Web page retrieved for the given URL.\r\n\r\n    @raise IOError: An exception is raised on error.\r\n    @raise urllib2.URLError: An exception is raised on error.\r\n    @raise urllib2.HTTPError: An exception is raised on error.\r\n    \"\"\"\r\n    request = Request(url)\r\n    request.add_header('User-Agent',\r\n                       'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0)')\r\n    cookie_jar.add_cookie_header(request)\r\n    response = urlopen(request)\r\n    cookie_jar.extract_cookies(response, request)\r\n    html = response.read()\r\n    response.close()\r\n    cookie_jar.save()\r\n    return html"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for URLs using Google.", "response": "def search(query, tld='com', lang='en', num=10, start=0, stop=None, pause=2.0,\r\n           only_standard=False):\r\n    \"\"\"\r\n    Search the given query string using Google.\r\n\r\n    @type  query: str\r\n    @param query: Query string. Must NOT be url-encoded.\r\n\r\n    @type  tld: str\r\n    @param tld: Top level domain.\r\n\r\n    @type  lang: str\r\n    @param lang: Languaje.\r\n\r\n    @type  num: int\r\n    @param num: Number of results per page.\r\n\r\n    @type  start: int\r\n    @param start: First result to retrieve.\r\n\r\n    @type  stop: int\r\n    @param stop: Last result to retrieve.\r\n        Use C{None} to keep searching forever.\r\n\r\n    @type  pause: float\r\n    @param pause: Lapse to wait between HTTP requests.\r\n        A lapse too long will make the search slow, but a lapse too short may\r\n        cause Google to block your IP. Your mileage may vary!\r\n\r\n    @type  only_standard: bool\r\n    @param only_standard: If C{True}, only returns the standard results from\r\n        each page. If C{False}, it returns every possible link from each page,\r\n        except for those that point back to Google itself. Defaults to C{False}\r\n        for backwards compatibility with older versions of this module.\r\n\r\n    @rtype:  generator\r\n    @return: Generator (iterator) that yields found URLs. If the C{stop}\r\n        parameter is C{None} the iterator will loop forever.\r\n    \"\"\"\r\n\r\n    # Lazy import of BeautifulSoup.\r\n    # Try to use BeautifulSoup 4 if available, fall back to 3 otherwise.\r\n    global BeautifulSoup\r\n    if BeautifulSoup is None:\r\n        try:\r\n            from bs4 import BeautifulSoup\r\n        except ImportError:\r\n            from BeautifulSoup import BeautifulSoup\r\n\r\n    # Set of hashes for the results found.\r\n    # This is used to avoid repeated results.\r\n    hashes = set()\r\n\r\n    # Prepare the search string.\r\n    query = quote_plus(query)\r\n\r\n    # Grab the cookie from the home page.\r\n    get_page(url_home % vars())\r\n\r\n    # Prepare the URL of the first request.\r\n    if start:\r\n        if num == 10:\r\n            url = url_next_page % vars()\r\n        else:\r\n            url = url_next_page_num % vars()\r\n    else:\r\n        if num == 10:\r\n            url = url_search % vars()\r\n        else:\r\n            url = url_search_num % vars()\r\n\r\n    # Loop until we reach the maximum result, if any (otherwise, loop forever).\r\n    while not stop or start < stop:\r\n\r\n        # Sleep between requests.\r\n        time.sleep(pause)\r\n\r\n        # Request the Google Search results page.\r\n        html = get_page(url)\r\n\r\n        # Parse the response and process every anchored URL.\r\n        soup = BeautifulSoup(html)\r\n        anchors = soup.find(id='search').findAll('a')\r\n        for a in anchors:\r\n\r\n            # Leave only the \"standard\" results if requested.\r\n            # Otherwise grab all possible links.\r\n            if only_standard and (\r\n                        not a.parent or a.parent.name.lower() != \"h3\"):\r\n                continue\r\n\r\n            # Get the URL from the anchor tag.\r\n            try:\r\n                link = a['href']\r\n            except KeyError:\r\n                continue\r\n\r\n            # Filter invalid links and links pointing to Google itself.\r\n            link = filter_result(link)\r\n            if not link:\r\n                continue\r\n\r\n            # Discard repeated results.\r\n            h = hash(link)\r\n            if h in hashes:\r\n                continue\r\n            hashes.add(h)\r\n\r\n            # Yield the result.\r\n            yield link\r\n\r\n        # End if there are no more results.\r\n        if not soup.find(id='nav'):\r\n            break\r\n\r\n        # Prepare the URL for the next request.\r\n        start += num\r\n        if num == 10:\r\n            url = url_next_page % vars()\r\n        else:\r\n            url = url_next_page_num % vars()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef processSearch(query, tld='com', lang='en', num=10, start=0, stop=50, pause=2.0, only_standard=False):\r\n    ''' \r\n        Method that recovers the URI for a search returning an i3visio-compliant json object.\r\n        \r\n        :return:    A json-like object.\r\n\r\n    '''\r\n    uriList = search(query, tld=tld, lang=lang, num=int(num), start=int(start), stop=int(stop), pause=float(pause),only_standard=only_standard)       \r\n    \r\n    # Dictionary containing the URI objects\r\n    results = []\r\n    \r\n    # Building the objects\r\n    for uri in uriList:\r\n        aux = {}\r\n        aux[\"type\"] = \"i3visio.uri\"\r\n        aux[\"value\"] = uri        \r\n        aux[\"attributes\"] = []\r\n        results.append(aux)\r\n\r\n    return results", "response": "A method that recovers the URI for a search returning a json - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_fields(self, fields):\n        for field in fields:\n            cls = field.__class__\n            try:\n                container = FieldsContainer.class_container[cls]\n            except KeyError:\n                raise ValueError('Object of type %s is an invalid field' % cls)\n            getattr(self, container).append(field)", "response": "Add the fields to their corresponding container."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the fields from the dict return a list with all the fields", "response": "def fields_from_dict(d):\n        \"\"\"Load the fields from the dict, return a list with all the fields.\"\"\"\n        class_container = FieldsContainer.class_container\n        fields = [field_cls.from_dict(field_dict) \n                  for field_cls, container in class_container.iteritems()\n                  for field_dict in d.get(container, [])]\n        return fields"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fields_to_dict(self):\n        d = {}\n        for container in FieldsContainer.class_container.values():\n            fields = getattr(self, container)\n            if fields:\n                d[container] = [field.to_dict() for field in fields]\n        return d", "response": "Transform the object to a dict and return the dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_dict(d):\n        query_params_match = d.get('@query_params_match')\n        query_person_match = d.get('@query_person_match')\n        valid_since = d.get('@valid_since')\n        if valid_since:\n            valid_since = str_to_datetime(valid_since)\n        source = Source.from_dict(d.get('source', {}))\n        fields = Record.fields_from_dict(d)\n        return Record(source=source, fields=fields, \n                      query_params_match=query_params_match, \n                      query_person_match=query_person_match, \n                      valid_since=valid_since)", "response": "Transform the dict to a record object and return the record."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict representation of the record.", "response": "def to_dict(self):\n        \"\"\"Return a dict representation of the record.\"\"\"\n        d = {}\n        if self.query_params_match is not None:\n            d['@query_params_match'] = self.query_params_match\n        if self.query_person_match is not None:\n            d['@query_person_match'] = self.query_person_match\n        if self.valid_since is not None:\n            d['@valid_since'] = datetime_to_str(self.valid_since)\n        if self.source is not None:\n            d['source'] = self.source.to_dict()\n        d.update(self.fields_to_dict())\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_searchable(self):\n        filter_func = lambda field: field.is_searchable\n        return bool(filter(filter_func, self.names) or \n                    filter(filter_func, self.emails) or\n                    filter(filter_func, self.phones) or\n                    filter(filter_func, self.usernames))", "response": "A bool value that indicates whether the person has enough data and\n        can be sent as a query to the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform the dict to a person object and return the person.", "response": "def from_dict(d):\n        \"\"\"Transform the dict to a person object and return the person.\"\"\"\n        query_params_match = d.get('@query_params_match')\n        sources = [Source.from_dict(source) for source in d.get('sources', [])]\n        fields = Person.fields_from_dict(d)\n        return Person(fields=fields, sources=sources,\n                      query_params_match=query_params_match)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self):\n        d = {}\n        if self.query_params_match is not None:\n            d['@query_params_match'] = self.query_params_match\n        if self.sources:\n            d['sources'] = [source.to_dict() for source in self.sources]\n        d.update(self.fields_to_dict())\n        return d", "response": "Return a dict representation of the person."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef processPhoneList(platformNames=[], numbers=[], excludePlatformNames=[]):\n    # Grabbing the <Platform> objects\n    platforms = platform_selection.getPlatformsByName(platformNames, mode=\"phonefy\", excludePlatformNames=excludePlatformNames)\n\n    results = []\n    for num in numbers:\n        for pla in platforms:\n            # This returns a json.txt!\n            entities = pla.getInfo(query=num, process=True, mode=\"phonefy\")\n            if entities != {}:\n                results+=json.loads(entities)\n    return results", "response": "Method to perform search on a series of numbers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef launchQueryForMode(self, query=None, mode=None):\n        # Creating the query URL for that mode\n        qURL = self.createURL(word=query, mode=mode)\n        i3Browser = browser.Browser()\n\n        try:\n            # Check if it needs creds\n            if self.needsCredentials[mode]:\n                self._getAuthenticated(i3Browser, qURL)\n                data = i3Browser.recoverURL(qURL)\n            else:\n                # Accessing the resources\n                data = i3Browser.recoverURL(qURL)\n            return data\n        except KeyError:\n            print(general.error(\"[*] '{}' is not a valid mode for this wrapper ({}).\".format(mode, self.__class__.__name__)))\n\n        return None", "response": "Method that launches an i3Browser to collect data for a specific mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a JSON string of the information for a given query.", "response": "def getInfo(self, query=None, process=False, mode=\"phonefy\", qURI=None):\n        \"\"\"\n        Method that checks the presence of a given query and recovers the first list of complains.\n\n        Args:\n        -----\n            query: Query to verify.\n            process: Calling the processing function.\n            mode: Mode to be executed.\n            qURI: A query to be checked.\n\n        Return:\n        -------\n            Python structure for the html processed.\n\n        Raises:\n        -------\n            NoCredentialsException.\n            NotImplementedModeError.\n            BadImplementationError.\n        \"\"\"\n        results = []\n        data = \"\"\n\n        if self._modeIsValid(mode=mode) and self._isValidQuery(query, mode=mode):\n            if mode in [\"mailfy\", \"phonefy\", \"searchfy\", \"usufy\"]:\n                try:\n                    results = getattr(self, \"do_{}\".format(mode))(query)\n                except AttributeError as e:\n                    raise NotImplementedModeError(str(self), mode)\n\n        return json.dumps(results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _modeIsValid(self, mode):\n        try:\n            # Suport for version 2 of wrappers\n            return mode in self.modes.keys()\n        except AttributeError as e:\n            # Legacy for mantaining old wrappers\n            if mode in self.isValidMode.keys():\n                if mode in self.isValidMode.keys():\n                    return True\n        return False", "response": "Returns True if the mode is a correct option to be used."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getAuthenticated(self, browser, url):\n        # check if we have creds\n        try:\n            if len(self.creds) > 0:\n                # TODO: in choosing a cred there is an uneeded nesting of arrays\n                c = random.choice(self.creds)[0]\n                # adding the credential\n                browser.setNewPassword(url, c.user, c.password)\n                return True\n            else:\n                raise NoCredentialsException(str(self))\n        except AttributeError as e:\n            raise BadImplementationError(str(e))", "response": "This method is used to get the authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies if something was found.", "response": "def _somethingFound(self, data, mode=\"phonefy\"):\n        \"\"\"\n        Verifying if something was found.\n\n        Args:\n        -----\n            data: Data where the self.notFoundText will be searched.\n            mode: Mode to be executed.\n\n        Return:\n        -------\n            True if exists.\n        \"\"\"\n        if data:\n            try:\n                for text in self.notFoundText[mode]:\n                    if text in data:\n                        return False\n                return True\n            except AttributeError as e:\n                # Update to version 2 of the wrappers.\n                verifier = self.modes.get(mode)\n                if verifier:\n                    if verifier.get(\"not_found_text\", \"\") in data:\n                        return False\n                    else:\n                        return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies a mailfy query in this platform. This might be redefined in any class inheriting from Platform. The only condition is that any of this should return a dictionary as defined. Args: ----- query: The element to be searched. kwargs: Dictionary with extra parameters. Just in case. Return: ------- Returns the collected data if exists or None if not.", "response": "def check_mailfy(self, query, kwargs={}):\n        \"\"\"\n        Verifying a mailfy query in this platform.\n\n        This might be redefined in any class inheriting from Platform. The only\n        condition is that any of this should return a dictionary as defined.\n\n        Args:\n        -----\n            query: The element to be searched.\n            kwargs: Dictionary with extra parameters. Just in case.\n\n        Return:\n        -------\n            Returns the collected data if exists or None if not.\n        \"\"\"\n        data = self.launchQueryForMode(query=query, mode=\"mailfy\")\n        if self._somethingFound(data, mode=\"mailfy\"):\n            return data\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_mailfy(self, query, **kwargs):\n        if self.check_mailfy(query, kwargs):\n            expandedEntities = general.expandEntitiesFromEmail(query)\n            r = {\n                \"type\": \"i3visio.profile\",\n                \"value\": self.platformName + \" - \" + query,\n                \"attributes\": expandedEntities + [\n                    {\n                        \"type\": \"i3visio.platform\",\n                        \"value\": self.platformName,\n                        \"attributes\": []\n                    }\n                ]\n            }\n            return [r]\n        return []", "response": "This function checks a mailfy query in this platform and returns a list of elements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_searchfy(self, query, kwargs={}):\n        data = self.launchQueryForMode(query=query, mode=\"searchfy\")\n        if self._somethingFound(data, mode=\"searchfy\"):\n            return data\n        return None", "response": "This function checks if a mailfy query is available in this platform and returns the collected data if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_searchfy(self, query, **kwargs):\n        results = []\n        print(\"[*] Launching search using the {} module...\".format(self.__class__.__name__))\n        test = self.check_searchfy(query, kwargs)\n\n        if test:\n            try:\n                # Recovering all the found aliases in the traditional way\n                ids = re.findall(self.searchfyAliasRegexp, test, re.DOTALL)\n            except:\n                # Version 2 of the wrappers\n                verifier = self.modes.get(mode)\n                \n                if verifier and verifier.get(\"alias_extractor\"):\n                    ids = re.findall(verifier.get(\"alias_extractor\"), test, re.DOTALL)\n                else:\n                    return []\n                    \n            for j, alias in enumerate(ids):\n                r = {\n                    \"type\": \"i3visio.profile\",\n                    \"value\": self.platformName + \" - \" + alias,\n                    \"attributes\": []\n                }\n\n                # Appending platform name\n                aux = {}\n                aux[\"type\"] = \"i3visio.platform\"\n                aux[\"value\"] = self.platformName\n                aux[\"attributes\"] = []\n                r[\"attributes\"].append(aux)\n                \n                # Appending the alias\n                aux = {}\n                aux[\"type\"] = \"i3visio.alias\"\n                aux[\"value\"] = alias\n                aux[\"attributes\"] = []\n                r[\"attributes\"].append(aux)\n                \n                # Appending the query performed to grab this items\n                aux = {}\n                aux[\"type\"] = \"i3visio.search\"\n                aux[\"value\"] = query\n                aux[\"attributes\"] = []\n                r[\"attributes\"].append(aux)\n                \n                # Appending platform URI\n                try:                    \n                    aux = {}\n                    aux[\"type\"] = \"i3visio.uri\"\n                    uri = self.createURL(word=alias, mode=\"usufy\")\n                    aux[\"value\"] = uri\n                    aux[\"attributes\"] = []\n                    r[\"attributes\"].append(aux)\n                except NameError:\n                    pass\n\n                # Appending the result to results: in this case only one profile will be grabbed\"\"\"\n                results.append(r)\n        return results", "response": "This function is called by the searchfy module to find the requested elements in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_phonefy(self, query, kwargs={}):\n        data = self.launchQueryForMode(query=query, mode=\"phonefy\")\n        if self._somethingFound(data, mode=\"phonefy\"):\n            return data\n        return None", "response": "This function checks if a mailfy query is available in this platform and returns the collected data if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify a phonefy query in this platform. This might be redefined in any class inheriting from Platform. Args: ----- query: The element to be searched. Return: ------- A list of elements to be appended.", "response": "def do_phonefy(self, query, **kwargs):\n        \"\"\"\n        Verifying a phonefy query in this platform.\n\n        This might be redefined in any class inheriting from Platform.\n\n        Args:\n        -----\n            query: The element to be searched.\n\n        Return:\n        -------\n            A list of elements to be appended.\n        \"\"\"\n        results = []\n\n        test = self.check_phonefy(query, kwargs)\n\n        if test:\n            r = {\n                \"type\": \"i3visio.phone\",\n                \"value\": self.platformName + \" - \" + query,\n                \"attributes\": []\n            }\n\n            try:\n                aux = {\n                    \"type\": \"i3visio.uri\",\n                    \"value\": self.createURL(query, mode=\"phonefy\"),\n                    \"attributes\": []\n                }\n                r[\"attributes\"].append(aux)\n            except:\n                pass\n\n            aux = {\n                \"type\": \"i3visio.platform\",\n                \"value\": self.platformName,\n                \"attributes\": []\n            }\n            r[\"attributes\"].append(aux)\n\n            # V2 of the wrappers\n            r[\"attributes\"] += self.process_phonefy(test)\n            results.append(r)\n            \n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_usufy(self, query, **kwargs):\n        data = self.launchQueryForMode(query=query, mode=\"usufy\")\n        if self._somethingFound(data, mode=\"usufy\"):\n            return data\n        return None", "response": "This function checks if a mailfy query is available in this platform and returns the collected data if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_usufy(self, query, **kwargs):\n        results = []\n\n        test = self.check_usufy(query, **kwargs)\n\n        if test:\n            r = {\n                \"type\": \"i3visio.profile\",\n                \"value\": self.platformName + \" - \" + query,\n                \"attributes\": []\n            }\n\n            # Appending platform URI\n            aux = {}\n            aux[\"type\"] = \"i3visio.uri\"\n            aux[\"value\"] = self.createURL(word=query, mode=\"usufy\")\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n            # Appending the alias\n            aux = {}\n            aux[\"type\"] = \"i3visio.alias\"\n            aux[\"value\"] = query\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n            # Appending platform name\n            aux = {}\n            aux[\"type\"] = \"i3visio.platform\"\n            aux[\"value\"] = self.platformName\n            aux[\"attributes\"] = []\n            r[\"attributes\"].append(aux)\n\n            r[\"attributes\"] += self.process_usufy(test)\n\n            results.append(r)\n        return results", "response": "Verify a usufy query in this platform and return a list of the elements that are found in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAllRegexp():\n    '''\n        Method that recovers ALL the list of <RegexpObject> classes to be processed....\n\n        :return:    Returns a list [] of <RegexpObject> classes.\n    '''\n\n    listAll = []\n\n    ############################################################################\n    ############################################################################\n\n    # --------------------------------------------------------------------------\n    # Dinamically collecting all the \"official\" modules\n    # --------------------------------------------------------------------------\n\n    # A list that will contain all of the module names\n    all_modules = []\n\n    # Grabbing all the module names\n    for _, name, _ in pkgutil.iter_modules(osrframework.patterns.__path__):\n        all_modules.append(\"osrframework.patterns.\" + name)\n\n    # Iterating through all the module names to grab them\n    for moduleName in all_modules:\n        # Importing the module\n        my_module = importlib.import_module(moduleName)\n\n        # Getting all the classNames.\n        classNames = [m[0] for m in inspect.getmembers(my_module, inspect.isclass) if m[1].__module__ == moduleName]\n\n        # Dinamically grabbing the first class of the module. IT SHOULD BE ALONE!\n        MyClass = getattr(my_module, classNames[0])\n\n        # Instantiating the object\n        newInstance = MyClass()\n\n        # Adding to the list!\n        listAll.append(newInstance)\n\n    # --------------------------------------------------------------------------\n    # Loading user-defined wrappers under [OSRFrameworkHOME]/plugins/patterns/\n    # --------------------------------------------------------------------------\n\n    # Creating the application paths\n    paths = configuration.getConfigPath()\n\n    newPath = os.path.abspath(paths[\"appPathPatterns\"])\n\n    # Inserting in the System Path\n    if not newPath in sys.path:\n        sys.path.append(newPath)\n\n    userImportedModules = {}\n\n    for module in os.listdir(newPath):\n        if module[-3:] == '.py':\n            current = module.replace('.py', '')\n            userImportedModules[current] = __import__(current)\n\n    del newPath\n\n    userClasses = []\n\n    # Iterating through all the files\n    for userModule in userImportedModules.keys():\n\n        my_module = userImportedModules[userModule]\n        # Getting all the classNames.\n        classNames = [m[0] for m in inspect.getmembers(my_module, inspect.isclass) if m[1].__module__ == userModule]\n\n        # Dinamically grabbing the first class of the module. IT SHOULD BE ALONE!\n        MyClass = getattr(my_module, classNames[0])\n\n        # Instantiating the object\n        newInstance = MyClass()\n\n        # Adding to the list!\n        userClasses.append(newInstance)\n\n    # --------------------------------------------------------------------------\n    # Overwriting original modules with the user plugins\n    # --------------------------------------------------------------------------\n    listToAdd = []\n    for userClass in userClasses:\n        for i, officialClass in enumerate(listAll):\n            # Checking if the name is the same\n            if str(userClass) == str(officialClass):\n                # Replacing the official module if a user module exists for it\n                listAll[i] = userClass\n            else:\n                if userClass not in listToAdd:\n                    # Appending the new class\n                    listToAdd.append(userClass)\n\n    # Merging listAll and listToAdd\n    listAll = listAll + listToAdd\n    ############################################################################\n    ############################################################################\n\n    return listAll", "response": "Method that recovers ALL the list of regexp objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef checkIfHashIsCracked(hash=None, api_key=None):\n\t''' \n\t\tMethod that checks if the given hash is stored in the md5crack.com website. \n\n\t\t:param hash:\thash to verify.\n\t\t:param api_key:\tapi_key to be used in md5crack.com. If not provided, the API key will be searched in the config_api_keys.py file.\n\n\t\t:return:\tPython structure for the Json received. It has the following structure:\n\t        {\n              \"phrase\": \"4d186321c1a7f0f354b297e8914ab240\",\n              \"code\": 6,\n              \"parsed\": \"hola\",\n              \"response\": \"The MD5 hash was cracked.\"\n            }\n\t'''\n\t# This is for i3visio\n\tif api_key==None:\n\t\t#api_key = raw_input(\"Insert the API KEY here:\\t\")\n\t\tallKeys = config_api_keys.returnListOfAPIKeys()\n\t\ttry: \n\t\t\tapi_key_data = allKeys[\"md5crack_com\"]\n\t\t\tapi_key = api_key_data[\"api_key\"]\n\t\texcept:\n\t\t\t# API_Key not found\n\t\t\treturn {}\t\t\t\n\n\tapiURL = \"http://api.md5crack.com/crack/\"+ api_key +\"/\" + hash\n\n\t# Accessing the HIBP API\n\tdata = urllib2.urlopen(apiURL).read()\n\tif \"\\\"parsed\\\":null\" in data:\n\t\tdata = data.replace(\"\\\"parsed\\\":null\", \"\\\"parsed\\\":\\\"\\\"\")\n\t\n\t# Reading the text data onto python structures\n\tjsonData = json.loads(data)\n\t#print json.dumps(jsonData, indent = 2)\n\treturn jsonData", "response": "This function checks if the given hash is cracked in the HIBP API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef somethingFound(self,data,mode=\"phonefy\"):\n        ''' \n            Verifying if something was found. Note that this method needed to be rewritten as in Spoj we need to look for a text which APPEARS instead of looking for a text that does NOT appear.\n            \n            :param data:    Data where the self.notFoundText will be searched.\n            :param mode:    Mode to be executed.            \n            \n            :return: Returns True if exists.\n        '''\n        #try:\n        for text in self.notFoundText[mode]:\n            if text in data:\n                # This is the change with regards to the standard behaviour!\n                return True\n        return False", "response": "Returns True if something was found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef changePermissionsRecursively(path, uid, gid):\n    os.chown(path, uid, gid)\n    for item in os.listdir(path):\n        itempath = os.path.join(path, item)\n        if os.path.isfile(itempath):\n            # Setting owner\n            try:\n                os.chown(itempath, uid, gid)\n            except Exception as e:\n                # If this crashes it may be because we are running the\n                # application in Windows systems, where os.chown does NOT work.\n                pass\n            # Setting permissions\n            os.chmod(itempath, 600)\n        elif os.path.isdir(itempath):\n            # Setting owner\n            try:\n                os.chown(itempath, uid, gid)\n            except Exception as e:\n                # If this crashes it may be because we are running the\n                # application in Windows systems, where os.chown does NOT work.\n                pass\n            # Setting permissions\n            os.chmod(itempath, 6600)\n            # Recursive function to iterate the files\n            changePermissionsRecursively(itempath, uid, gid)", "response": "Function to recursively change the user id and group id of the files in the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getBitcoinAddressDetails(address=None):\n    ''' \n        Method that checks the presence of a Bitcoin Address in blockchain.info:\n{\n  \"total_sent\": 41301084, \n  \"total_received\": 52195147, \n  \"final_balance\": 10894063, \n  \"address\": \"1APKyS2TEdFMjXjJfMCgavFtoWuv2QNXTw\", \n  \"hash160\": \"66f21efc754af07e87913db46bf24df2eb0d5075\", \n...\n}\n\n        :param address: Bitcoin address to verify.\n\n        :return:        Python structure for the json received. If nothing was found, it will return an empty dictionary.\n    '''\n    try:\n        apiURL = \"https://blockchain.info/rawaddr/\" + str(address)\n\n        # Accessing the HIBP API\n        data = urllib2.urlopen(apiURL).read()\n\n        # Reading the text data onto python structures\n        jsonData = json.loads(data)\n        return jsonData\n    except:\n        # No information was found, then we return a null entity\n        return {}", "response": "Method that checks the presence of a Bitcoin Address in blockchain. info. Returns a dictionary containing the json data received from the HIBP API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(params=None):\n    if params == None:\n        parser = getParser()\n        args = parser.parse_args(params)\n    else:\n        args = params\n        \n    results = []\n\n    print(general.title(banner.text))\n\n    sayingHello = \"\"\"\n     Searchfy | Copyright (C) Yaiza Rubio & F\u00e9lix Brezo (i3visio) 2014-2018\n\nThis program comes with ABSOLUTELY NO WARRANTY. This is free software, and you\nare welcome to redistribute it under certain conditions. For additional info,\nvisit <{}>.\n\"\"\".format(general.LICENSE_URL)\n    print(general.info(sayingHello))\n\n    if args.license:\n        general.showLicense()\n    else:\n        # Showing the execution time...\n        startTime= dt.datetime.now()\n        print(str(startTime) + \"\\tStarting search in different platform(s)... Relax!\\n\")\n        print(general.emphasis(\"\\tPress <Ctrl + C> to stop...\\n\"))\n        # Performing the search\n        try:\n            results = performSearch(platformNames=args.platforms, queries=args.queries, process=args.process, excludePlatformNames=args.exclude)\n        except KeyboardInterrupt:\n            print(general.error(\"\\n[!] Process manually stopped by the user. Workers terminated without providing any result.\\n\"))\n            results = []\n\n        # Generating summary files for each ...\n        if args.extension:\n            # Verifying if the outputPath exists\n            if not os.path.exists (args.output_folder):\n                os.makedirs(args.output_folder)\n\n            # Grabbing the results\n            fileHeader = os.path.join(args.output_folder, args.file_header)\n\n            # Iterating through the given extensions to print its values\n            for ext in args.extension:\n                # Generating output files\n                general.exportUsufy(results, ext, fileHeader)\n\n        # Printing the results if requested\n        now = dt.datetime.now()\n        print(\"\\n{}\\tResults obtained:\\n\".format(str(now)))\n        print(general.success(general.usufyToTextExport(results)))\n\n        if args.web_browser:\n            general.openResultsInBrowser(results)\n\n        now = dt.datetime.now()\n        print(\"\\n{date}\\tYou can find all the information collected in the following files:\".format(date=str(now)))\n        for ext in args.extension:\n            # Showing the output files\n            print(\"\\t\" + general.emphasis(fileHeader + \".\" + ext))\n\n        # Showing the execution time...\n        endTime= dt.datetime.now()\n        print(\"\\n{date}\\tFinishing execution...\\n\".format(date=str(endTime)))\n        print(\"Total time used:\\t\" + general.emphasis(str(endTime-startTime)))\n        print(\"Average seconds/query:\\t\" + general.emphasis(str((endTime-startTime).total_seconds()/len(args.platforms))) +\" seconds\\n\")\n\n        # Urging users to place an issue on Github...\n        print(banner.footer)\n\n    if params:\n        return results", "response": "Main function to launch usufy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getAllPlatformNames(mode):\n    # Recovering all the possible platforms installed\n    platOptions = []\n    if mode in [\"phonefy\", \"usufy\", \"searchfy\", \"mailfy\"]:\n        allPlatforms = getAllPlatformObjects(mode=mode)\n        # Defining the platOptions\n        for p in allPlatforms:\n            try:\n                # E. g.: to use wikipedia instead of wikipedia_ca and so on\n                parameter = p.parameterName\n            except:\n                parameter = p.platformName.lower()\n\n            if parameter not in platOptions:\n                platOptions.append(parameter)\n    elif mode == \"domainfy\":\n        platOptions = osrframework.domainfy.TLD.keys()\n\n    platOptions =  sorted(set(platOptions))\n    platOptions.insert(0, 'all')\n    return platOptions", "response": "Method that returns the whole list of available parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getPlatformsByName(platformNames=['all'], mode=None, tags=[], excludePlatformNames=[]):\n\n    allPlatformsList = getAllPlatformObjects(mode)\n\n    platformList = []\n\n    # Tags has priority over platform\n    if \"all\" in platformNames and len(tags) == 0:\n        # Last condition: checking if \"all\" has been provided\n        for plat in allPlatformsList:\n            if str(plat.platformName).lower() not in excludePlatformNames:\n                platformList.append(plat)\n        return platformList\n    else:\n        # going through the regexpList\n        for name in platformNames:\n            if name not in excludePlatformNames:\n                for plat in allPlatformsList:\n                    # Verifying if the parameter was provided\n                    if name == str(plat.platformName).lower():\n                        platformList.append(plat)\n                        break\n\n                    # We need to perform additional checks to verify the Wikipedia platforms, which are called with a single parameter\n                    try:\n                        if name == str(plat.parameterName).lower():\n                            platformList.append(plat)\n                            break\n                    except:\n                        pass\n\n                    # Verifying if any of the platform tags match the original tag\n                    for t in plat.tags:\n                        if t in tags:\n                            platformList.append(plat)\n                            break\n    # If the platformList is empty, we will return all\n    if platformList == []:\n        return allPlatformsList\n    else:\n        return platformList", "response": "Method that recovers the names of the possible platforms in a given list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAllPlatformNamesByTag (mode = None):\n    tags = {}\n\n    allPlatformsList = getAllPlatformObjects(mode)\n\n    # Iterating the list of platforms to collect the tags\n    for plat in allPlatformsList:\n        # Grabbing the tags and providing them\n        for t in plat.tags:\n            if t not in tags.keys():\n                tags[t] = [str(plat)]\n            else:\n                tags[t].append(str(plat))\n\n    return tags", "response": "Returns the platforms in the framework grouped by tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAllPlatformObjects(mode = None):\n\n    listAll = []\n\n    ############################################################################\n    ############################################################################\n\n    # --------------------------------------------------------------------------\n    # Dinamically collecting all the \"official\" modules\n    # --------------------------------------------------------------------------\n\n    # A list that will contain all of the module names\n    all_modules = []\n\n    # Grabbing all the module names\n    for _, name, _ in pkgutil.iter_modules(osrframework.wrappers.__path__):\n        all_modules.append(\"osrframework.wrappers.\" + name)\n\n    # Iterating through all the module names to grab them\n    for moduleName in all_modules:\n        # Importing the module\n        my_module = importlib.import_module(moduleName)\n\n        # Getting all the classNames.\n        classNames = [m[0] for m in inspect.getmembers(my_module, inspect.isclass) if m[1].__module__ == moduleName]\n\n        # Dinamically grabbing the first class of the module. IT SHOULD BE ALONE!\n        MyClass = getattr(my_module, classNames[0])\n\n        # Instantiating the object\n        newInstance = MyClass()\n\n        # Adding to the list!\n        listAll.append(newInstance)\n\n    # --------------------------------------------------------------------------\n    # Loading user-defined wrappers under [OSRFrameworkHOME]/plugins/wrappers/\n    # --------------------------------------------------------------------------\n\n    # Creating the application paths\n    paths = configuration.getConfigPath()\n\n    newPath = os.path.abspath(paths[\"appPathWrappers\"])\n\n    # Inserting in the System Path\n    if not newPath in sys.path:\n        sys.path.append(newPath)\n\n    userImportedModules = {}\n\n    for module in os.listdir(newPath):\n        if module[-3:] == '.py':\n            current = module.replace('.py', '')\n            userImportedModules[current] = __import__(current)\n\n    del newPath\n\n    userClasses = []\n\n    # Iterating through all the files\n    for userModule in userImportedModules.keys():\n\n        my_module = userImportedModules[userModule]\n        # Getting all the classNames.\n        classNames = [m[0] for m in inspect.getmembers(my_module, inspect.isclass) if m[1].__module__ == userModule]\n\n        # Dinamically grabbing the first class of the module. IT SHOULD BE ALONE!\n        MyClass = getattr(my_module, classNames[0])\n\n        # Instantiating the object\n        newInstance = MyClass()\n\n        # Adding to the list!\n        userClasses.append(newInstance)\n\n    # --------------------------------------------------------------------------\n    # Overwriting original modules with the user plugins\n    # --------------------------------------------------------------------------\n    listToAdd = []\n    for userClass in userClasses:\n        overwritten = False\n        for i, officialClass in enumerate(listAll):\n            # Checking if the name is the same\n            if str(userClass) == str(officialClass):\n                # Replacing the official module if a user module exists for it\n                listAll[i] = userClass\n                # We stop iterating this loop\n                overwritten = True\n                break\n        if not overwritten:\n            # Appending the new class\n            listToAdd.append(userClass)\n\n    # Merging listAll and listToAdd\n    listAll = listAll + listToAdd\n    ############################################################################\n    ############################################################################\n\n    creds = credentials.getCredentials()\n\n    for p in listAll:\n        # Verify if there are credentials to be loaded\n        if p.platformName.lower() in creds.keys():\n            p.setCredentials(creds[p.platformName.lower()])\n\n    if mode == None:\n        return listAll\n    else:\n        # We are returning only those platforms which are required by the mode.\n        selected = []\n        for p in listAll:\n            try:\n                if p.isValidMode[mode]:\n                    selected.append(p)\n            except:\n                pass\n        return selected", "response": "Method that recovers ALL the list of platforms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_json(cls, json_str):\r\n        d = json.loads(json_str)\r\n        return cls.from_dict(d)", "response": "Deserialize the object from a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createEmails(nicks=None, nicksFile=None):\n    candidate_emails = set()\n    if nicks != None:\n        for n in nicks:\n            for e in email_providers.domains:\n                candidate_emails.add(\"{}@{}\".format(n, e))\n    elif nicksFile != None:\n        with open(nicksFile, \"r\") as iF:\n            nicks = iF.read().splitlines()\n            for n in nicks:\n                for e in email_providers.domains:\n                    candidate_emails.add(\"{}@{}\".format(n, e))\n    return candidate_emails", "response": "Method that globally permits to generate the emails to be checked."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(params=None):\n    if params == None:\n        parser = getParser()\n        args = parser.parse_args(params)\n    else:\n        args = params\n\n    results = []\n    if not args.quiet:\n        print(general.title(banner.text))\n\n        sayingHello = \"\"\"\n     Checkfy | Copyright (C) Yaiza Rubio & F\u00e9lix Brezo (i3visio) 2014-2018\n\nThis program comes with ABSOLUTELY NO WARRANTY. This is free software, and you\nare welcome to redistribute it under certain conditions. For additional info,\nvisit <{}>.\n\"\"\".format(general.LICENSE_URL)\n    print(general.info(sayingHello))\n\n    if args.license:\n        general.showLicense()\n    else:\n        # Processing the options returned to remove the \"all\" option\n        if args.nicks:\n            emails = createEmails(nicks=args.nicks)\n        else:\n            # nicks_file\n            emails = createEmails(nicksFile=args.nicks_file)\n\n        # Showing the execution time...\n        if not args.quiet:\n            startTime= dt.datetime.now()\n            print(str(startTime) + \"\\tTrying to identify possible emails \" + general.emphasis(str(len(emails))) + \" email(s)... Relax!\\n\")\n            print(general.emphasis(\"\\tPress <Ctrl + C> to stop...\\n\"))\n\n        print(args.pattern)\n        if args.type == \"twitter\":\n            pattern = args.pattern.replace(\".\", \"\\.\")\n            pattern = pattern.replace(\"*\", \".\")\n            pattern = \"^{}$\".format(pattern)\n        elif args.type == \"regexp\":\n            pattern = args.pattern\n\n        # Perform searches, using different Threads\n        results = verifyEmails(emails, pattern)\n\n        # Sorting list\n        results.sort()\n        print(\"\\nProcess finished.\")\n        print(\"\\nValidated emails:\\n\")\n        print(general.success(json.dumps(results, indent=2, sort_keys=True)))\n        print(\"\\nUp to \" + general.emphasis(str(len(results))) + \" possible emails foundd.\\n\")\n\n\n        # Trying to store the information recovered\n        if args.output_folder != None:\n            if not os.path.exists(args.output_folder):\n                os.makedirs(args.output_folder)\n\n            outputPath = os.path.join(args.output_folder, \"possible_emails.txt\")\n\n            print(\"Writing the results onto the file:\\n\\t\" + general.emphasis(outputPath))\n\n            with open(outputPath, \"w\") as oF:\n                for r in results:\n                    oF.write(r+\"\\n\")\n\n        # Showing the execution time...\n        if not args.quiet:\n            # Showing the execution time...\n            endTime= dt.datetime.now()\n            print(\"\\n\" + str(endTime) +\"\\tFinishing execution...\\n\")\n            print(\"Total time used:\\t\" + general.emphasis(str(endTime-startTime)))\n            print(\"Average seconds/query:\\t\" + general.emphasis(str((endTime-startTime).total_seconds()/len(emails))) +\" seconds\\n\")\n\n            # Urging users to place an issue on Github...\n            print(banner.footer)\n\n    if params:\n        return results", "response": "This function is called by the main function of the phonefy application. It is called by the main function of the phonefy application. It is called by the main function of the phonefy application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_thumbnail_url(image_url, height, width, favicon_domain=None, \n                           zoom_face=True, api_key=None):\n    \"\"\"Take an image URL and generate a thumbnail URL for that image.\n    \n    Args:\n    \n    image_url -- unicode (or utf8 encoded str), URL of the image you want to \n                 thumbnail.   \n    height -- int, requested thumbnail height in pixels, maximum 500.\n    width -- int, requested thumbnail width in pixels, maximum 500.\n    favicon_domain -- unicode (or utf8 encoded str), optional, the domain of \n                      the website where the image came from, the favicon will \n                      be added to the corner of the thumbnail, recommended for \n                      copyright reasones.\n                      IMPORTANT: Don't assume that the domain of the website is\n                      the domain from `image_url`, it's possible that \n                      domain1.com hosts its images on domain2.com.\n    zoom_face -- bool, indicates whether you want the thumbnail to zoom on the \n                 face in the image (in case there is a face) or not.\n    api_key -- str, a valid API key (use \"samplekey\" for experimenting).\n               Note that you can set a default API key\n               (osrframework.thirdparties.pipl_com.lib.thumbnail.default_api_key = '<your_key>') instead of \n               passing your key in each call.\n    \n    ValueError is raised in case of illegal parameters.\n    \n    Example (thumbnail URL from an image URL):\n    \n    >>> from osrframework.thirdparties.pipl_com.lib.thumbnail import generate_thumbnail_url\n    >>> image_url = 'http://a7.twimg.com/a/ab76f.jpg'\n    >>> generate_thumbnail_url(image_url, 100, 100, \n                               favicon_domain='twitter.com',\n                               api_key='samplekey')\n    'http://api.pipl.com/thumbnail/v2/?key=samplekey&\n    favicon_domain=twitter.com&height=100&width=100&zoom_face=True&\n    image_url=http%3A%2F%2Fa7.twimg.com%2Fa%2Fab76f.jpg'\n    \n    Example (thumbnail URL from a record that came in the response of our \n    Search API):\n    \n    >>> from osrframework.thirdparties.pipl_com.lib.thumbnail import generate_thumbnail_url\n    >>> generate_thumbnail_url(record.images[0].url, 100, 100, \n                               favicon_domain=record.source.domain,\n                               api_key='samplekey')\n    'http://api.pipl.com/thumbnail/v2/?key=samplekey&\n    favicon_domain=twitter.com&height=100&width=100&zoom_face=True&\n    image_url=http%3A%2F%2Fa7.twimg.com%2Fa%2Fab76f.jpg'\n    \n    \"\"\"\n    if not (api_key or default_api_key):\n        raise ValueError('A valid API key is required')\n    if not Image(url=image_url).is_valid_url:\n        raise ValueError('image_url is not a valid URL')\n    if not (0 < height <= MAX_PIXELS and 0 < width <= MAX_PIXELS):\n        raise ValueError('height/width must be between 0 and %d' % MAX_PIXELS)\n    query = {\n        'key': to_utf8(api_key or default_api_key),\n        'image_url': urllib.unquote(to_utf8(image_url)),\n        'height': height,\n        'width': width,\n        'favicon_domain': to_utf8(favicon_domain or ''),\n        'zoom_face': zoom_face,\n    }\n    return BASE_URL + urllib.urlencode(query)", "response": "Generate a thumbnail URL from an image URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef checkIPFromAlias(alias=None):\n    ''' \n\t\tMethod that checks if the given alias is currently connected to Skype and returns its IP address. \n\n\t\t:param alias:\tAlias to be searched.\n\n\t\t:return:\tPython structure for the Json received. It has the following structure:\n\t        {\n              \"type\": \"i3visio.ip\",\n              \"value\": \"1.1.1.1\",\n              \"attributes\" : []\n            }\n\t'''\n    headers = {\n    \"Content-type\": \"text/html\",\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Encoding\": \" gzip, deflate\",\n    \"Accept-Language\": \" es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3\",\n    \"Connection\": \"keep-alive\",\n    \"DNT\": \"1\",\n    \"Host\": \"www.resolvethem.com\",\n    \"Referer\": \"http://www.resolvethem.com/index.php\",\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; rv:38.0) Gecko/20100101 Firefox/38.0\",\n    \"Content-Length\": \"26\",\n    \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }    \n    \n    req = requests.post(\"http://www.resolvethem.com/index.php\",headers=headers,data={'skypeUsername': alias,'submit':''})\n    # Data returned\n    data = req.content\n    # Compilation of the regular expression\n    p = re.compile(\"class='alert alert-success'>([0-9\\.]*)<\")\n    allMatches = p.findall(data)\n    if len(allMatches)> 0:\n        jsonData = {}\n        jsonData[\"type\"]=\"i3visio.ip\"\n        jsonData[\"value\"]=allMatches[0]\n        jsonData[\"attributes\"]=[]     \n        return jsonData\n    return {}", "response": "This method checks if the given alias is currently connected to Skype and returns its IP address."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the credentials from a file with the following structure.", "response": "def getCredentials():\n    ''' \n        Recovering the credentials from a file with the following structure:\n        \n        :return: A dictionary with the following struture:\n            { \"platform1\": [C1<Credential>, C2<Credential>], \"platform2\": [C3<Credential>]}\n    '''\n    logger = logging.getLogger(\"osrframework.utils\")\n    # Dictionary of lists:\n    #     {'Twitter': {cred1, cred2, ...}}\n    creds = {} \n    try:\n        credsTuples = c_creds.returnListOfCreds()\n\n        for cTuple in credsTuples:\n            plat, user, password = cTuple\n\n            c = Credential(user, password)\n\n            if plat not in creds.keys():\n                creds[plat] = [c]\n            else:\n                creds[plat] = creds[plat].append(c)\n        logger.info(str(len(credsTuples)) + \" credentials have been loaded.\")    \n        return creds\n    except Exception, e:\n        logger.error(\"WARNING. Something happened when loading credentials.\")\n        logger.error(str(e))        \n        logger.debug(\"No credentials were loaded.\")    \n    return {}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_mailfy(self, query, kwargs={}):\n        import re\n        import requests\n\n        s = requests.Session()\n\n        # Getting the first response to grab the csrf_token\n        r1 = s.get(\"https://www.instagram.com\")\n        csrf_token = re.findall(\"csrf_token\", r1.text)[0]\n\n        # Launching the query to Instagram\n        r2 = s.post(\n            'https://www.instagram.com/accounts/web_create_ajax/attempt/',\n            data={\"email\": query},\n            headers={\"X-CSRFToken\": csrf_token}\n        )\n\n        if '{\"email\": [{\"message\": \"Another account is using' in r2.text:\n            return r2.text\n        else:\n            return None", "response": "This function checks if a specific element in this platform is mailfy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _enrich_link(self, glossary):\n        try:\n            Model = apps.get_model(*glossary['link']['model'].split('.'))\n            obj = Model.objects.get(pk=glossary['link']['pk'])\n            glossary['link'].update(identifier=str(obj))\n        except (KeyError, ObjectDoesNotExist):\n            pass", "response": "Enrich the dict glossary [ link ] with an identifier onto the model\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextending the form for the given plugin with the form SharableCascadeForm", "response": "def get_form(self, request, obj=None, **kwargs):\n        \"\"\"\n        Extend the form for the given plugin with the form SharableCascadeForm\n        \"\"\"\n        Form = type(str('ExtSharableForm'), (SharableCascadeForm, kwargs.pop('form', self.form)), {})\n        Form.base_fields['shared_glossary'].limit_choices_to = dict(plugin_type=self.__class__.__name__)\n        kwargs.update(form=Form)\n        return super(SharableGlossaryMixin, self).get_form(request, obj, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of HTML elements that can be rendered together with all its source elements.", "response": "def get_picture_elements(instance):\n    \"\"\"\n    Create a context, used to render a <picture> together with all its ``<source>`` elements:\n    It returns a list of HTML elements, each containing the information to render a ``<source>``\n    element.\n    The purpose of this HTML entity is to display images with art directions. For normal images use\n    the ``<img>`` element.\n    \"\"\"\n\n    if hasattr(instance, 'image') and hasattr(instance.image, 'exif'):\n        aspect_ratio = compute_aspect_ratio(instance.image)\n    elif 'image' in instance.glossary and 'width' in instance.glossary['image']: \n        aspect_ratio = compute_aspect_ratio_with_glossary(instance.glossary)\n    else:\n        # if accessing the image file fails or fake image fails, abort here\n        logger.warning(\"Unable to compute aspect ratio of image '{}'\".format(instance.image))\n        return\n\n    # container_max_heights = instance.glossary.get('container_max_heights', {})\n    resize_options = instance.glossary.get('resize_options', {})\n    crop = 'crop' in resize_options\n    upscale = 'upscale' in resize_options\n    if 'subject_location' in resize_options and hasattr(instance.image, 'subject_location'):\n        subject_location = instance.image.subject_location\n    else:\n        subject_location = None\n    max_width = 0\n    max_zoom = 0\n    elements = []\n    for bp, media_query in instance.glossary['media_queries'].items():\n        width, media = media_query['width'], media_query['media']\n        max_width = max(max_width, width)\n        size = None\n        try:\n            image_height = parse_responsive_length(instance.glossary['responsive_heights'][bp])\n        except KeyError:\n            image_height = (None, None)\n        if image_height[0]:  # height was given in px\n            size = (int(width), image_height[0])\n        elif image_height[1]:  # height was given in %\n            size = (int(width), int(round(width * aspect_ratio * image_height[1])))\n        try:\n            zoom = int(\n                instance.glossary['responsive_zoom'][bp].strip().rstrip('%')\n            )\n        except (AttributeError, KeyError, ValueError):\n            zoom = 0\n        max_zoom = max(max_zoom, zoom)\n        if size is None:\n            # as fallback, adopt height to current width\n            size = (int(width), int(round(width * aspect_ratio)))\n        elem = {'tag': 'source', 'size': size, 'zoom': zoom, 'crop': crop,\n                'upscale': upscale, 'subject_location': subject_location, 'media': media}\n        if 'high_resolution' in resize_options:\n            elem['size2'] = (size[0] * 2, size[1] * 2)\n        elements.append(elem)\n\n    # add a fallback image for old browsers which can't handle the <source> tags inside a <picture> element\n    if image_height[1]:\n        size = (int(max_width), int(round(max_width * aspect_ratio * image_height[1])))\n    else:\n        size = (int(max_width), int(round(max_width * aspect_ratio)))\n    elements.append({'tag': 'img', 'size': size, 'zoom': max_zoom, 'crop': crop,\n                     'upscale': upscale, 'subject_location': subject_location})\n    return elements"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict of min - and max - values for the given column.", "response": "def get_min_max_bounds(self):\n        \"\"\"\n        Return a dict of min- and max-values for the given column.\n        This is required to estimate the bounds of images.\n        \"\"\"\n        bound = Bound(999999.0, 0.0)\n        for bp in Breakpoint:\n            bound.extend(self.get_bound(bp))\n        return {'min': bound.min, 'max': bound.max}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Django Proxy Model on the fly.", "response": "def create_proxy_model(name, model_mixins, base_model, attrs=None, module=None):\n    \"\"\"\n    Create a Django Proxy Model on the fly, to be used by any Cascade Plugin.\n    \"\"\"\n    from django.apps import apps\n\n    class Meta:\n        proxy = True\n        app_label = 'cmsplugin_cascade'\n\n    name = str(name + 'Model')\n    try:\n        Model = apps.get_registered_model(Meta.app_label, name)\n    except LookupError:\n        bases = model_mixins + (base_model,)\n        attrs = dict(attrs or {}, Meta=Meta, __module__=module)\n        Model = type(name, bases, attrs)\n        fake_proxy_models[name] = bases\n    return Model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_parent_classes_transparent(cls, slot, page, instance=None):\n        parent_classes = super(CascadePluginBase, cls).get_parent_classes(slot, page, instance)\n        if parent_classes is None:\n            if cls.get_require_parent(slot, page) is False:\n                return\n            parent_classes = []\n\n        # add all plugins marked as 'transparent', since they all are potential parents\n        parent_classes = set(parent_classes)\n        parent_classes.update(TransparentContainer.get_plugins())\n        return list(parent_classes)", "response": "Return all parent classes including those marked as transparent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextend the number of children so that the parent object contains wanted children.", "response": "def extend_children(self, parent, wanted_children, child_class, child_glossary=None):\n        \"\"\"\n        Extend the number of children so that the parent object contains wanted children.\n        No child will be removed if wanted_children is smaller than the current number of children.\n        \"\"\"\n        from cms.api import add_plugin\n        current_children = parent.get_num_children()\n        for _ in range(current_children, wanted_children):\n            child = add_plugin(parent.placeholder, child_class, parent.language, target=parent)\n            if isinstance(child_glossary, dict):\n                child.glossary.update(child_glossary)\n            child.save()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_form(self, request, obj=None, **kwargs):\n        widgets = kwargs.pop('widgets', {})\n        labels = kwargs.pop('labels', {})\n        glossary_fields = kwargs.pop('glossary_fields', self.glossary_fields)\n        widgets.update(glossary=JSONMultiWidget(glossary_fields))\n        labels.update(glossary='')  # remove label for glossary, since each subfields provides a label itself\n        kwargs.update(widgets=widgets, labels=labels)\n        form = super(CascadePluginBase, self).get_form(request, obj, **kwargs)\n        # help_text can not be cleared using an empty string in modelform_factory\n        form.base_fields['glossary'].help_text = ''\n        if request.method == 'POST':\n            is_shared = bool(request.POST.get('shared_glossary'))\n            for field in glossary_fields:\n                if not (is_shared and field.name in self.sharable_fields):\n                    form.base_fields['glossary'].validators.append(field.run_validators)\n        form.glossary_fields = glossary_fields\n        return form", "response": "Build the form used for changing the model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_parent_instance(self, request=None, obj=None):\n        try:\n            parent_id = obj.parent_id\n        except AttributeError:\n            try:\n                # TODO: self.parent presumably is not used anymore in CMS-3.4, because it doesn't\n                # make sense anyway, since the plugin instances shall know their parents, not the\n                # plugins.\n                parent_id = self.parent.id\n            except AttributeError:\n                if request:\n                    parent_id = request.GET.get('plugin_parent', None)\n                    if parent_id is None:\n                        from cms.models import CMSPlugin\n                        try:\n                            parent_id = CMSPlugin.objects.filter(id=request.resolver_match.args[0]\n                                                                 ).only(\"parent_id\").order_by('?').first().parent_id\n                        except (AttributeError, IndexError):\n                            parent_id = None\n                else:\n                    parent_id = None\n        for model in CascadeModelBase._get_cascade_elements():\n            try:\n                return model.objects.get(id=parent_id)\n            except model.DoesNotExist:\n                continue", "response": "Get the parent model instance corresponding to this plugin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_previous_instance(self, obj):\n        ordered_siblings = obj.get_siblings().filter(placeholder=obj.placeholder).order_by('position')\n        pos = list(ordered_siblings).index(obj.cmsplugin_ptr)\n        if pos > 0:\n            prev_sibling = ordered_siblings[pos - 1]\n            return prev_sibling.get_bound_plugin()", "response": "Return the previous plugin instance for the given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the next plugin instance for the given object.", "response": "def get_next_instance(self, obj):\n        \"\"\"\n        Return the next plugin instance for the given object.\n        This differs from `obj.get_next_sibling()` which returns an unsorted sibling.\n        \"\"\"\n        ordered_siblings = obj.get_siblings().filter(placeholder=obj.placeholder).order_by('position')\n        pos = list(ordered_siblings).index(obj.cmsplugin_ptr)\n        if pos < ordered_siblings.count() - 1:\n            next_sibling = ordered_siblings[pos + 1]\n            return next_sibling.get_bound_plugin()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef in_edit_mode(self, request, placeholder):\n        toolbar = getattr(request, 'toolbar', None)\n        edit_mode = getattr(toolbar, 'edit_mode', False) and getattr(placeholder, 'is_editable', True)\n        if edit_mode:\n            edit_mode = placeholder.has_change_permission(request.user)\n        return edit_mode", "response": "Returns True if the plugin is in edit mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a context returning the tags to render an image.", "response": "def get_image_tags(instance):\n    \"\"\"\n    Create a context returning the tags to render an <img ...> element with\n    ``sizes``, ``srcset``, a fallback ``src`` and if required inline styles.\n    \"\"\"\n    if hasattr(instance, 'image') and hasattr(instance.image, 'exif'):\n        aspect_ratio = compute_aspect_ratio(instance.image)\n    elif 'image' in instance.glossary and 'width' in instance.glossary['image']: \n        aspect_ratio = compute_aspect_ratio_with_glossary(instance.glossary)\n        instance.glossary['ramdom_svg_color'] = 'hsl({}, 30%, 80%, 0.8)'.format( str(random.randint(0, 360)))\n    else:\n        # if accessing the image file fails or fake image fails, abort here\n        logger.warning(\"Unable to compute aspect ratio of image '{}'\".format(instance.image))\n        return\n\n    is_responsive = 'img-fluid' in instance.glossary.get('image_shapes', [])\n    resize_options = instance.glossary.get('resize_options', {})\n    crop = 'crop' in resize_options\n    upscale = 'upscale' in resize_options\n    if 'subject_location' in resize_options and hasattr(instance.image, 'subject_location'):\n        subject_location = instance.image.subject_location\n    else:\n        subject_location = None\n    tags = {'sizes': [], 'srcsets': {}, 'is_responsive': is_responsive, 'extra_styles': {}}\n    if is_responsive:\n        image_width = parse_responsive_length(instance.glossary.get('image_width_responsive') or '100%')\n        assert(image_width[1]), \"The given image has no valid width\"\n        if image_width[1] != 1.0:\n            tags['extra_styles'].update({'max-width': '{:.0f}%'.format(100 * image_width[1])})\n    else:\n        image_width = parse_responsive_length(instance.glossary['image_width_fixed'])\n        if not image_width[0]:\n            image_width = (instance.image.width, image_width[1])\n    try:\n        image_height = parse_responsive_length(instance.glossary['image_height'])\n    except KeyError:\n        image_height = (None, None)\n    if is_responsive:\n        column_bounds_min = instance.glossary['column_bounds']['min']\n        if 'high_resolution' in resize_options:\n            column_bounds_max = 2 * instance.glossary['column_bounds']['max']\n        else:\n            column_bounds_max = instance.glossary['column_bounds']['max']\n        num_steps = min(int((column_bounds_max - column_bounds_min) / app_settings.RESPONSIVE_IMAGE_STEP_SIZE),\n                        app_settings.RESPONSIVE_IMAGE_MAX_STEPS)\n        step_width, max_width = (column_bounds_max - column_bounds_min) / num_steps, 0\n        for step in range(0, num_steps + 1):\n            width = round(column_bounds_min + step_width * step)\n            max_width = max(max_width, width)\n            size = get_image_size(width, image_height, aspect_ratio)\n            key = '{0}w'.format(*size)\n            tags['srcsets'][key] = {'size': size, 'crop': crop, 'upscale': upscale,\n                                    'subject_location': subject_location}\n        tags['sizes'] = instance.glossary['media_queries'].values()\n        # use an existing image as fallback for the <img ...> element\n        if not max_width > 0:\n            logger.warning('image tags: image max width is zero')\n        size = (int(round(max_width)), int(round(max_width * aspect_ratio)))\n    else:\n        size = get_image_size(image_width[0], image_height, aspect_ratio)\n        if 'high_resolution' in resize_options:\n            tags['srcsets']['1x'] = {'size': size, 'crop': crop, 'upscale': upscale,\n                                     'subject_location': subject_location}\n            tags['srcsets']['2x'] = dict(tags['srcsets']['1x'], size=(size[0] * 2, size[1] * 2))\n    tags['src'] = {'size': size, 'crop': crop, 'upscale': upscale, 'subject_location': subject_location}\n    return tags"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_previous_open_tag(self, obj):\n        prev_instance = self.get_previous_instance(obj)\n        if prev_instance and prev_instance.plugin_type == self.__class__.__name__:\n            return prev_instance.glossary.get('open_tag')", "response": "Return the open tag of the previous sibling."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the given element_id is unique for the current page.", "response": "def check_unique_element_id(cls, instance, element_id):\n        \"\"\"\n        Check for uniqueness of the given element_id for the current page.\n        Return None if instance is not yet associated with a page.\n        \"\"\"\n        try:\n            element_ids = instance.placeholder.page.cascadepage.glossary.get('element_ids', {})\n        except (AttributeError, ObjectDoesNotExist):\n            pass\n        else:\n            if element_id:\n                for key, value in element_ids.items():\n                    if str(key) != str(instance.pk) and element_id == value:\n                        msg = _(\"The element ID '{}' is not unique for this page.\")\n                        raise ValidationError(msg.format(element_id))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nemulate what a Python set does but keeping the element s order.", "response": "def remove_duplicates(lst):\n    \"\"\"\n    Emulate what a Python ``set()`` does, but keeping the element's order.\n    \"\"\"\n    dset = set()\n    return [l for l in lst if l not in dset and not dset.add(l)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrectifying partial form field.", "response": "def rectify_partial_form_field(base_field, partial_form_fields):\n    \"\"\"\n    In base_field reset the attributes label and help_text, since they are overriden by the\n    partial field. Additionally, from the list, or list of lists of partial_form_fields\n    append the bound validator methods to the given base field.\n    \"\"\"\n    base_field.label = ''\n    base_field.help_text = ''\n    for fieldset in partial_form_fields:\n        if not isinstance(fieldset, (list, tuple)):\n            fieldset = [fieldset]\n        for field in fieldset:\n            base_field.validators.append(field.run_validators)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the given model exists and if not raise a Validation error", "response": "def validate_link(link_data):\n    \"\"\"\n    Check if the given model exists, otherwise raise a Validation error\n    \"\"\"\n    from django.apps import apps\n\n    try:\n        Model = apps.get_model(*link_data['model'].split('.'))\n        Model.objects.get(pk=link_data['pk'])\n    except Model.DoesNotExist:\n        raise ValidationError(_(\"Unable to link onto '{0}'.\").format(Model.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a string containing a length definition in pixels or percent and returns a tuple of the first element is the length in pixels and the second element is the length in percent divided by 100.", "response": "def parse_responsive_length(responsive_length):\n    \"\"\"\n    Takes a string containing a length definition in pixels or percent and parses it to obtain\n    a computational length. It returns a tuple where the first element is the length in pixels and\n    the second element is its length in percent divided by 100.\n    Note that one of both returned elements is None.\n    \"\"\"\n    responsive_length = responsive_length.strip()\n    if responsive_length.endswith('px'):\n        return (int(responsive_length.rstrip('px')), None)\n    elif responsive_length.endswith('%'):\n        return (None, float(responsive_length.rstrip('%')) / 100)\n    return (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_css_classes(cls, instance):\n        css_classes = []\n        if hasattr(cls, 'default_css_class'):\n            css_classes.append(cls.default_css_class)\n        for attr in getattr(cls, 'default_css_attributes', []):\n            css_class = instance.glossary.get(attr)\n            if isinstance(css_class, six.string_types):\n                css_classes.append(css_class)\n            elif isinstance(css_class, list):\n                css_classes.extend(css_class)\n        return css_classes", "response": "Returns a list of CSS classes to be added as class =... to the current HTML tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_inline_styles(cls, instance):\n        inline_styles = getattr(cls, 'default_inline_styles', {})\n        css_style = instance.glossary.get('inline_styles')\n        if css_style:\n            inline_styles.update(css_style)\n        return inline_styles", "response": "Returns a dictionary of CSS attributes to be added as style =... to the current HTML tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_html_tag_attributes(cls, instance):\n        attributes = getattr(cls, 'html_tag_attributes', {})\n        return dict((attr, instance.glossary.get(key, '')) for key, attr in attributes.items())", "response": "Returns a dictionary of attributes which shall be added to the current HTML tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile_validation_pattern(self, units=None):\n        if units is None:\n            units = list(self.POSSIBLE_UNITS)\n        else:\n            for u in units:\n                if u not in self.POSSIBLE_UNITS:\n                    raise ValidationError('{} is not a valid unit for a size field'.format(u))\n        regex = re.compile(r'^(-?\\d+)({})$'.format('|'.join(units)))\n        endings = (' %s ' % ugettext(\"or\")).join(\"'%s'\" % u.replace('%', '%%') for u in units)\n        params = {'label': '%(label)s', 'value': '%(value)s', 'field': '%(field)s', 'endings': endings}\n        return regex, self.invalid_message % params", "response": "Compile the validation pattern for the size field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_nodes(self, request):\n        nodes = []\n        docsmap_file = os.path.join(settings.SPHINX_DOCS_ROOT, 'docsmap.json')\n        if not os.path.exists(docsmap_file):\n            return nodes\n        with io.open(docsmap_file) as fh:\n            docs_map = json.load(fh, encoding='utf-8')\n\n        for counter, items in enumerate(docs_map.items(), 1):\n            bits = items[0].split('/')\n            if len(bits) == 1 and bits[0] == 'index' or len(bits) == 2 and bits[1] != 'index':\n                continue\n            node = NavigationNode(\n                title=items[1],\n                url=reverse_lazy('sphinx-documentation', args=(bits[0],)),\n                id=counter,\n            )\n            nodes.append(node)\n        return nodes", "response": "This method returns a list of NavigationNode objects that are used to build the menu tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unset_required_for(cls, sharable_fields):\n        if 'link_content' in cls.base_fields and 'link_content' not in sharable_fields:\n            cls.base_fields['link_content'].required = False\n        if 'link_type' in cls.base_fields and 'link' not in sharable_fields:\n            cls.base_fields['link_type'].required = False", "response": "Deactivate required fields for this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assure_relation(cls, cms_page):\n        try:\n            cms_page.cascadepage\n        except cls.DoesNotExist:\n            cls.objects.create(extended_object=cms_page)", "response": "Assure that we have a foreign key relation pointing from CascadePage onto CMSPage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreduce the number of breakpoints in the widget of the named glossary_field.", "response": "def reduce_breakpoints(plugin, field_name, request=None, obj=None):\n    \"\"\"\n    Narrow down the number of breakpoints in the widget of the named glossary_field. This is useful\n    in case the container was defined with a subset of these breakpoints: xs, sm, md, lg.\n    \"\"\"\n    if not isinstance(plugin, CascadePluginBase):\n        raise ValueError('Plugin is not of type CascadePluginBase')\n    parent_instance = plugin.get_parent_instance(request, obj)\n    if not parent_instance:\n        return\n    complete_glossary = parent_instance.get_complete_glossary()\n    if 'breakpoints' not in complete_glossary:\n        return\n    try:\n        # find the glossary_field named field_name and restrict its breakpoint to the available ones\n        widget = [f for f in plugin.glossary_fields if f.name == field_name][0].widget\n    except IndexError:\n        return\n    if not isinstance(widget, widgets.MultiWidget):\n        raise ValueError('Widget for glossary_field {0} is not a multiple value field')\n    temp = [(l, widget.widgets[k]) for k, l in enumerate(widget.labels) if l in complete_glossary['breakpoints']]\n    widget.labels, widget.widgets = (list(t) for t in zip(*temp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_media_queries(element):\n    parent_glossary = element.get_parent_glossary()\n    # compute the max width and the required media queries for each chosen breakpoint\n    element.glossary['container_max_widths'] = max_widths = {}\n    element.glossary['media_queries'] = media_queries = {}\n    breakpoints = element.glossary.get('breakpoints', parent_glossary.get('breakpoints', []))\n    last_index = len(breakpoints) - 1\n    fluid = element.glossary.get('fluid')\n    for index, bp in enumerate(breakpoints):\n        try:\n            key = 'container_fluid_max_widths' if fluid else 'container_max_widths'\n            max_widths[bp] = parent_glossary[key][bp]\n        except KeyError:\n            max_widths[bp] = BS3_BREAKPOINTS[bp][4 if fluid else 3]\n        if last_index > 0:\n            if index == 0:\n                next_bp = breakpoints[1]\n                media_queries[bp] = ['(max-width: {0}px)'.format(BS3_BREAKPOINTS[next_bp][0])]\n            elif index == last_index:\n                media_queries[bp] = ['(min-width: {0}px)'.format(BS3_BREAKPOINTS[bp][0])]\n            else:\n                next_bp = breakpoints[index + 1]\n                media_queries[bp] = ['(min-width: {0}px)'.format(BS3_BREAKPOINTS[bp][0]),\n                                     '(max-width: {0}px)'.format(BS3_BREAKPOINTS[next_bp][0])]", "response": "Compute the current media queries for each chosen breakpoint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_image_tags(context, instance, options):\n    if hasattr(instance, 'image') and hasattr(instance.image, 'exif'):\n        aspect_ratio = compute_aspect_ratio(instance.image)\n    elif 'image' in instance.glossary and 'width' in instance.glossary['image']: \n        aspect_ratio = compute_aspect_ratio_with_glossary(instance.glossary)\n        instance.glossary['ramdom_svg_color'] = 'hsl({}, 30%, 80%, 0.8)'.format( str(random.randint(0, 360)))\n    else:\n        # if accessing the image file fails or fake image fails, abort here\n        logger.warning(\"Unable to compute aspect ratio of image '{}'\".format(instance.image))\n        return\n\n    is_responsive = options.get('is_responsive', False)\n    resize_options = options.get('resize_options', {})\n    crop = 'crop' in resize_options\n    upscale = 'upscale' in resize_options\n    if hasattr(instance.image, 'subject_location'):\n        subject_location = instance.image.subject_location and 'subject_location' in resize_options\n    else:\n        subject_location = None\n    resolutions = (False, True) if 'high_resolution' in resize_options else (False,)\n    tags = {'sizes': [], 'srcsets': {}, 'is_responsive': is_responsive, 'extra_styles': {}}\n    if is_responsive:\n        image_width = parse_responsive_length(options.get('image_width_responsive') or '100%')\n        assert(image_width[1]), \"The given image has no valid width\"\n        if image_width[1] != 1.0:\n            tags['extra_styles'].update({'max-width': '{:.0f}%'.format(100 * image_width[1])})\n    else:\n        image_width = parse_responsive_length(options['image_width_fixed'])\n        if not image_width[0]:\n            image_width = (instance.image.width, image_width[1])\n    try:\n        image_height = parse_responsive_length(options['image_height'])\n    except KeyError:\n        image_height = (None, None)\n    set_defaults(options)\n    if is_responsive:\n        max_width = 0\n        for bp in options['breakpoints']:\n            if bp not in options['container_max_widths']:\n                continue\n            width = int(image_width[1] * options['container_max_widths'][bp])\n            max_width = max(max_width, width)\n            size = get_image_size(width, image_height, aspect_ratio)\n            if bp in options['media_queries']:\n                tags['sizes'].append('{0} {1}px'.format(' and '.join(options['media_queries'][bp]), width))\n            for high_res in resolutions:\n                if high_res:\n                    size = (size[0] * 2, size[1] * 2)\n                key = '{0}w'.format(size[0])\n                tags['srcsets'][key] = {'size': size, 'crop': crop, 'upscale': upscale,\n                                        'subject_location': subject_location}\n        # use an existing image as fallback for the <img ...> element\n        if not max_width > 0:\n            logger.warning('image tags: image max width is zero')\n        size = (int(round(max_width)), int(round(max_width * aspect_ratio)))\n    else:\n        size = get_image_size(image_width[0], image_height, aspect_ratio)\n        if len(resolutions) > 1:\n            for high_res in resolutions:\n                if high_res:\n                    tags['srcsets']['2x'] = {'size': (size[0] * 2, size[1] * 2), 'crop': crop,\n                        'upscale': upscale, 'subject_location': subject_location}\n                else:\n                    tags['srcsets']['1x'] = {'size': size, 'crop': crop,\n                        'upscale': upscale, 'subject_location': subject_location}\n    tags['src'] = {'size': size, 'crop': crop, 'upscale': upscale,\n                   'subject_location': subject_location}\n    return tags", "response": "Create a context returning the tags to render an image."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of HTML elements that can be rendered together with all its source elements.", "response": "def get_picture_elements(context, instance):\n    \"\"\"\n    Create a context, used to render a <picture> together with all its ``<source>`` elements:\n    It returns a list of HTML elements, each containing the information to render a ``<source>``\n    element.\n    The purpose of this HTML entity is to display images with art directions. For normal images use\n    the ``<img>`` element.\n    \"\"\"\n    if hasattr(instance, 'image') and hasattr(instance.image, 'exif'):\n        aspect_ratio = compute_aspect_ratio(instance.image)\n    elif 'image' in instance.glossary and 'width' in instance.glossary['image']: \n        aspect_ratio = compute_aspect_ratio_with_glossary(instance.glossary)\n        instance.glossary['ramdom_svg_color'] = 'hsl({}, 30%, 80%, 0.8)'.format( str(random.randint(0, 360)))\n    else:\n        # if accessing the image file fails or fake image fails, abort here\n        logger.warning(\"Unable to compute aspect ratio of image '{}'\".format(instance.image))\n        return\n\n    complete_glossary = instance.get_complete_glossary()\n    container_max_heights = complete_glossary.get('container_max_heights', {})\n    resize_options = instance.glossary.get('resize_options', {})\n    crop = 'crop' in resize_options\n    upscale = 'upscale' in resize_options\n    if hasattr(instance.image, 'subject_location'):\n        subject_location = instance.image.subject_location and 'subject_location' in resize_options\n    else:\n        subject_location = None\n    max_width = 0\n    max_zoom = 0\n    elements = []\n    for bp in complete_glossary['breakpoints']:\n        try:\n            width = float(complete_glossary['container_max_widths'][bp])\n        except KeyError:\n            width = 0\n        max_width = max(max_width, round(width))\n        size = None\n        try:\n            image_height = parse_responsive_length(instance.glossary['responsive_heights'][bp])\n        except KeyError:\n            image_height = (None, None)\n        if image_height[0]:  # height was given in px\n            size = (int(width), image_height[0])\n        elif image_height[1]:  # height was given in %\n            size = (int(width), int(round(width * aspect_ratio * image_height[1])))\n        elif bp in container_max_heights:\n            container_height = parse_responsive_length(container_max_heights[bp])\n            if container_height[0]:\n                size = (int(width), container_height[0])\n            elif container_height[1]:\n                size = (int(width), int(round(width * aspect_ratio * container_height[1])))\n        try:\n            zoom = int(\n                instance.glossary['responsive_zoom'][bp].strip().rstrip('%')\n            )\n        except (AttributeError, KeyError, ValueError):\n            zoom = 0\n        max_zoom = max(max_zoom, zoom)\n        if size is None:\n            # as fallback, adopt height to current width\n            size = (int(width), int(round(width * aspect_ratio)))\n        try:\n            media_queries = complete_glossary['media_queries'][bp][:]\n        except KeyError:\n            media_queries = []\n        media = ' and '.join(media_queries)\n        elem = {'tag': 'source', 'size': size, 'zoom': zoom, 'crop': crop,\n                'upscale': upscale, 'subject_location': subject_location, 'media': media}\n        if 'high_resolution' in resize_options:\n            elem['size2'] = (size[0] * 2, size[1] * 2)\n        elements.append(elem)\n\n    # add a fallback image for old browsers which can't handle the <picture> element\n    if image_height[1]:\n        size = (int(max_width), int(round(max_width * aspect_ratio * image_height[1])))\n    else:\n        size = (int(max_width), int(round(max_width * aspect_ratio)))\n    elements.append({'tag': 'img', 'size': size, 'zoom': max_zoom, 'crop': crop,\n                     'upscale': upscale, 'subject_location': subject_location})\n    return elements"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrestricts child classes of Card to one of each", "response": "def get_child_classes(cls, slot, page, instance=None):\n        \"\"\"Restrict child classes of Card to one of each: Header, Body and Footer\"\"\"\n        child_classes = super(BootstrapCardPlugin, cls).get_child_classes(slot, page, instance)\n        # allow only one child of type Header, Body, Footer\n        for child in instance.get_children():\n            if child.plugin_type in child_classes:\n                child_classes.remove(child.plugin_type)\n        return child_classes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_form(self, request, obj=None, **kwargs):\n        kwargs.update(widgets={\n            'plugin_type': widgets.Select(choices=self.plugins_for_site),\n            'css_classes': JSONMultiWidget(self.classname_fields),\n            'inline_styles': JSONMultiWidget(self.style_fields)\n        })\n        form = super(PluginExtraFieldsAdmin, self).get_form(request, obj, **kwargs)\n        rectify_partial_form_field(form.base_fields['css_classes'], self.classname_fields)\n        form.classname_fields = self.classname_fields\n        rectify_partial_form_field(form.base_fields['inline_styles'], self.style_fields)\n        form.style_fields = self.style_fields\n        return form", "response": "Build the form used for changing the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_element_ids(self, prefix_id):\n        if isinstance(self.widget, widgets.MultiWidget):\n            ids = ['{0}_{1}_{2}'.format(prefix_id, self.name, field_name) for field_name in self.widget]\n        elif isinstance(self.widget, (widgets.SelectMultiple, widgets.RadioSelect)):\n            ids = ['{0}_{1}_{2}'.format(prefix_id, self.name, k) for k in range(len(self.widget.choices))]\n        else:\n            ids = ['{0}_{1}'.format(prefix_id, self.name)]\n        return ids", "response": "Returns a single or a list of element ids one for each input widget of this field\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_django_migrations_module(module_name):\n    import imp\n    try:\n        module_info = imp.find_module(module_name)\n        module = imp.load_module(module_name, *module_info)\n        imp.find_module('migrations_django', module.__path__)\n        return module_name + '.migrations_django'\n    except ImportError:\n        return module_name + '.migrations'", "response": "Tries to locate a module_name. migrations_django and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_context_override(self, request):\n        context_override = super(EmulateUserModelMixin, self).get_context_override(request)\n        try:\n            if request.user.is_staff:\n                user = self.UserModel.objects.get(pk=request.session['emulate_user_id'])\n                context_override.update(user=user)\n        except (self.UserModel.DoesNotExist, KeyError):\n            pass\n        return context_override", "response": "Override the request object with an emulated user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pre_migrate(cls, sender=None, **kwargs):\n        ContentType = apps.get_model('contenttypes', 'ContentType')\n        try:\n            queryset = ContentType.objects.filter(app_label=sender.label)\n            for ctype in queryset.exclude(model__in=sender.get_proxy_models().keys()):\n                model = ctype.model_class()\n                if model is None:\n                    sender.revoke_permissions(ctype)\n                    ContentType.objects.get(app_label=sender.label, model=ctype).delete()\n        except DatabaseError:\n            return", "response": "Remove all proxy models from contenttypes and contenttypes that are not in proxy models."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding contenttypes and permissions for missing proxy_models.", "response": "def post_migrate(cls, sender=None, **kwargs):\n        \"\"\"\n        Iterate over fake_proxy_models and add contenttypes and permissions for missing proxy\n        models, if this has not been done by Django yet\n        \"\"\"\n        ContentType = apps.get_model('contenttypes', 'ContentType')\n\n        for model_name, proxy_model in sender.get_proxy_models().items():\n            ctype, created = ContentType.objects.get_or_create(app_label=sender.label, model=model_name)\n            if created:\n                sender.grant_permissions(proxy_model)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the default permissions for the just added proxy model", "response": "def grant_permissions(self, proxy_model):\n        \"\"\"\n        Create the default permissions for the just added proxy model\n        \"\"\"\n        ContentType = apps.get_model('contenttypes', 'ContentType')\n        try:\n            Permission = apps.get_model('auth', 'Permission')\n        except LookupError:\n            return\n\n        # searched_perms will hold the permissions we're looking for as (content_type, (codename, name))\n        searched_perms = []\n        ctype = ContentType.objects.get_for_model(proxy_model)\n        for perm in self.default_permissions:\n            searched_perms.append((\n                '{0}_{1}'.format(perm, proxy_model._meta.model_name),\n                \"Can {0} {1}\".format(perm, proxy_model._meta.verbose_name_raw)\n            ))\n\n        all_perms = set(Permission.objects.filter(\n            content_type=ctype,\n        ).values_list(\n            'content_type', 'codename'\n        ))\n        permissions = [\n            Permission(codename=codename, name=name, content_type=ctype)\n            for codename, name in searched_perms if (ctype.pk, codename) not in all_perms\n        ]\n        Permission.objects.bulk_create(permissions)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove all permissions for the content type ctype", "response": "def revoke_permissions(self, ctype):\n        \"\"\"\n        Remove all permissions for the content type to be removed\n        \"\"\"\n        ContentType = apps.get_model('contenttypes', 'ContentType')\n        try:\n            Permission = apps.get_model('auth', 'Permission')\n        except LookupError:\n            return\n\n        codenames = ['{0}_{1}'.format(perm, ctype) for perm in self.default_permissions]\n        cascade_element = apps.get_model(self.label, 'cascadeelement')\n        element_ctype = ContentType.objects.get_for_model(cascade_element)\n        Permission.objects.filter(content_type=element_ctype, codename__in=codenames).delete()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_file(file, array):\n\n    try:\n        array.tofile(file)\n    except (TypeError, IOError, UnsupportedOperation):\n        # tostring actually returns bytes\n        file.write(array.tostring())", "response": "Wrapper around ndarray. toFile to support any file - like object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a list of objects to a TDMS file.", "response": "def write_segment(self, objects):\n        \"\"\" Write a segment of data to a TDMS file\n\n        :param objects: A list of TdmsObject instances to write\n        \"\"\"\n        segment = TdmsSegment(objects)\n        segment.write(self._file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps around np. fromfile to support any file - like object", "response": "def fromfile(file, dtype, count, *args, **kwargs):\n    \"\"\"Wrapper around np.fromfile to support any file-like object\"\"\"\n\n    try:\n        return np.fromfile(file, dtype=dtype, count=count, *args, **kwargs)\n    except (TypeError, IOError, UnsupportedOperation):\n        return np.frombuffer(\n            file.read(count * np.dtype(dtype).itemsize),\n            dtype=dtype, count=count, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a property from a segment s metadata", "response": "def read_property(f, endianness=\"<\"):\n    \"\"\" Read a property from a segment's metadata \"\"\"\n\n    prop_name = types.String.read(f, endianness)\n    prop_data_type = types.tds_data_types[types.Uint32.read(f, endianness)]\n    value = prop_data_type.read(f, endianness)\n    log.debug(\"Property %s: %r\", prop_name, value)\n    return prop_name, value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread string raw data", "response": "def read_string_data(file, number_values, endianness):\n    \"\"\" Read string raw data\n\n        This is stored as an array of offsets\n        followed by the contiguous string data.\n    \"\"\"\n    offsets = [0]\n    for i in range(number_values):\n        offsets.append(types.Uint32.read(file, endianness))\n    strings = []\n    for i in range(number_values):\n        s = file.read(offsets[i + 1] - offsets[i])\n        strings.append(s.decode('utf-8'))\n    return strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a path into group and channel name components", "response": "def path_components(path):\n    \"\"\"Convert a path into group and channel name components\"\"\"\n\n    def yield_components(path):\n        # Iterate over each character and the next character\n        chars = zip_longest(path, path[1:])\n        try:\n            # Iterate over components\n            while True:\n                c, n = next(chars)\n                if c != '/':\n                    raise ValueError(\"Invalid path, expected \\\"/\\\"\")\n                elif (n is not None and n != \"'\"):\n                    raise ValueError(\"Invalid path, expected \\\"'\\\"\")\n                else:\n                    # Consume \"'\" or raise StopIteration if at the end\n                    next(chars)\n                component = []\n                # Iterate over characters in component name\n                while True:\n                    c, n = next(chars)\n                    if c == \"'\" and n == \"'\":\n                        component += \"'\"\n                        # Consume second \"'\"\n                        next(chars)\n                    elif c == \"'\":\n                        yield \"\".join(component)\n                        break\n                    else:\n                        component += c\n        except StopIteration:\n            return\n\n    return list(yield_components(path))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a TDMS object from the file.", "response": "def object(self, *path):\n        \"\"\"Get a TDMS object from the file\n\n        :param path: The object group and channel. Providing no channel\n            returns a group object, and providing no channel or group\n            will return the root object.\n        :rtype: :class:`TdmsObject`\n\n        For example, to get the root object::\n\n            object()\n\n        To get a group::\n\n            object(\"group_name\")\n\n        To get a channel::\n\n            object(\"group_name\", \"channel_name\")\n        \"\"\"\n\n        object_path = self._path(*path)\n        try:\n            return self.objects[object_path]\n        except KeyError:\n            raise KeyError(\"Invalid object path: %s\" % object_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef groups(self):\n\n        # Split paths into components and take the first (group) component.\n        object_paths = (\n            path_components(path)\n            for path in self.objects)\n        group_names = (path[0] for path in object_paths if len(path) > 0)\n\n        # Use an ordered dict as an ordered set to find unique\n        # groups in order.\n        groups_set = OrderedDict()\n        for group in group_names:\n            groups_set[group] = None\n        return list(groups_set)", "response": "Return the names of groups in the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef group_channels(self, group):\n\n        path = self._path(group)\n        return [\n            self.objects[p]\n            for p in self.objects\n            if p.startswith(path + '/')]", "response": "Returns a list of channel objects for the given group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_dataframe(self, time_index=False, absolute_time=False):\n\n        import pandas as pd\n\n        dataframe_dict = OrderedDict()\n        for key, value in self.objects.items():\n            if value.has_data:\n                index = value.time_track(absolute_time) if time_index else None\n                dataframe_dict[key] = pd.Series(data=value.data, index=index)\n        return pd.DataFrame.from_dict(dataframe_dict)", "response": "Converts the TDMS file to a Pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_hdf(self, filepath, mode='w', group='/'):\n        import h5py\n\n        # Groups in TDMS are mapped to the first level of the HDF5 hierarchy\n\n        # Channels in TDMS are then mapped to the second level of the HDF5\n        # hierarchy, under the appropriate groups.\n\n        # Properties in TDMS are mapped to attributes in HDF5.\n        # These all exist under the appropriate, channel group etc.\n\n        h5file = h5py.File(filepath, mode)\n\n        container_group = None\n        if group in h5file:\n            container_group = h5file[group]\n        else:\n            container_group = h5file.create_group(group)\n\n        # First write the properties at the root level\n        try:\n            root = self.object()\n            for property_name, property_value in root.properties.items():\n                container_group.attrs[property_name] = property_value\n        except KeyError:\n            # No root object present\n            pass\n\n        # Now iterate through groups and channels,\n        # writing the properties and data\n        for group_name in self.groups():\n            try:\n                group = self.object(group_name)\n\n                # Write the group's properties\n                for prop_name, prop_value in group.properties.items():\n                    container_group[group_name].attrs[prop_name] = prop_value\n\n            except KeyError:\n                # No group object present\n                pass\n\n            # Write properties and data for each channel\n            for channel in self.group_channels(group_name):\n                for prop_name, prop_value in channel.properties.items():\n                    container_group.attrs[prop_name] = prop_value\n\n                container_group[group_name+'/'+channel.channel] = channel.data\n\n        return h5file", "response": "Converts the TDMS object into an HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the metadata section of a TdmsFile and update object information.", "response": "def read_metadata(self, f, objects, previous_segment=None):\n        \"\"\"Read segment metadata section and update object information\"\"\"\n\n        if not self.toc[\"kTocMetaData\"]:\n            try:\n                self.ordered_objects = previous_segment.ordered_objects\n            except AttributeError:\n                raise ValueError(\n                    \"kTocMetaData is not set for segment but \"\n                    \"there is no previous segment\")\n            self.calculate_chunks()\n            return\n        if not self.toc[\"kTocNewObjList\"]:\n            # In this case, there can be a list of new objects that\n            # are appended, or previous objects can also be repeated\n            # if their properties change\n            self.ordered_objects = [\n                copy(o) for o in previous_segment.ordered_objects]\n\n        log.debug(\"Reading metadata at %d\", f.tell())\n\n        # First four bytes have number of objects in metadata\n        num_objects = types.Int32.read(f, self.endianness)\n\n        for obj in range(num_objects):\n            # Read the object path\n            object_path = types.String.read(f, self.endianness)\n\n            # If this is a new segment for an existing object,\n            # reuse the existing object, otherwise,\n            # create a new object and add it to the object dictionary\n            if object_path in objects:\n                obj = objects[object_path]\n            else:\n                obj = TdmsObject(object_path, self.tdms_file)\n                objects[object_path] = obj\n\n            # Add this segment object to the list of segment objects,\n            # re-using any properties from previous segments.\n            updating_existing = False\n            if not self.toc[\"kTocNewObjList\"]:\n                # Search for the same object from the previous segment\n                # object list.\n                obj_index = [\n                    i for i, o in enumerate(self.ordered_objects)\n                    if o.tdms_object is obj]\n                if len(obj_index) > 0:\n                    updating_existing = True\n                    log.debug(\"Updating object in segment list\")\n                    obj_index = obj_index[0]\n                    segment_obj = self.ordered_objects[obj_index]\n            if not updating_existing:\n                if obj._previous_segment_object is not None:\n                    log.debug(\"Copying previous segment object\")\n                    segment_obj = copy(obj._previous_segment_object)\n                else:\n                    log.debug(\"Creating a new segment object\")\n                    segment_obj = _TdmsSegmentObject(obj, self.endianness)\n                self.ordered_objects.append(segment_obj)\n            # Read the metadata for this object, updating any\n            # data structure information and properties.\n            segment_obj._read_metadata(f)\n            obj._previous_segment_object = segment_obj\n\n        self.calculate_chunks()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_chunks(self):\n\n        if self.toc['kTocDAQmxRawData']:\n            # chunks defined differently for DAQmxRawData format\n            try:\n                data_size = next(\n                    o.number_values * o.raw_data_width\n                    for o in self.ordered_objects\n                    if o.has_data and o.number_values * o.raw_data_width > 0)\n            except StopIteration:\n                data_size = 0\n        else:\n            data_size = sum([\n                o.data_size\n                for o in self.ordered_objects if o.has_data])\n\n        total_data_size = self.next_segment_offset - self.raw_data_offset\n        if data_size < 0 or total_data_size < 0:\n            raise ValueError(\"Negative data size\")\n        elif data_size == 0:\n            # Sometimes kTocRawData is set, but there isn't actually any data\n            if total_data_size != data_size:\n                raise ValueError(\n                    \"Zero channel data size but data length based on \"\n                    \"segment offset is %d.\" % total_data_size)\n            self.num_chunks = 0\n            return\n        chunk_remainder = total_data_size % data_size\n        if chunk_remainder == 0:\n            self.num_chunks = int(total_data_size // data_size)\n\n            # Update data count for the overall tdms object\n            # using the data count for this segment.\n            for obj in self.ordered_objects:\n                if obj.has_data:\n                    obj.tdms_object.number_values += (\n                        obj.number_values * self.num_chunks)\n\n        else:\n            log.warning(\n                \"Data size %d is not a multiple of the \"\n                \"chunk size %d. Will attempt to read last chunk\" %\n                (total_data_size, data_size))\n            self.num_chunks = 1 + int(total_data_size // data_size)\n\n            self.final_chunk_proportion = (\n                float(chunk_remainder) / float(data_size))\n\n            for obj in self.ordered_objects:\n                if obj.has_data:\n                    obj.tdms_object.number_values += (\n                        obj.number_values * (self.num_chunks - 1) + int(\n                            obj.number_values * self.final_chunk_proportion))", "response": "Calculates the number of chunks the data is in based on the number of objects in the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads signal data from file f.", "response": "def read_raw_data(self, f):\n        \"\"\"Read signal data from file\"\"\"\n\n        if not self.toc[\"kTocRawData\"]:\n            return\n\n        f.seek(self.data_position)\n\n        total_data_size = self.next_segment_offset - self.raw_data_offset\n        log.debug(\n            \"Reading %d bytes of data at %d in %d chunks\" %\n            (total_data_size, f.tell(), self.num_chunks))\n\n        for chunk in range(self.num_chunks):\n            if self.toc[\"kTocInterleavedData\"]:\n                log.debug(\"Data is interleaved\")\n                data_objects = [o for o in self.ordered_objects if o.has_data]\n                # If all data types have numpy types and all the lengths are\n                # the same, then we can read all data at once with numpy,\n                # which is much faster\n                all_numpy = all(\n                    (o.data_type.nptype is not None for o in data_objects))\n                same_length = (len(\n                    set((o.number_values for o in data_objects))) == 1)\n                if (all_numpy and same_length):\n                    self._read_interleaved_numpy(f, data_objects)\n                else:\n                    self._read_interleaved(f, data_objects)\n            else:\n                object_data = {}\n                log.debug(\"Data is contiguous\")\n                for obj in self.ordered_objects:\n                    if obj.has_data:\n                        if (chunk == (self.num_chunks - 1) and\n                                self.final_chunk_proportion != 1.0):\n                            number_values = int(\n                                obj.number_values *\n                                self.final_chunk_proportion)\n                        else:\n                            number_values = obj.number_values\n                        object_data[obj.path] = (\n                            obj._read_values(f, number_values))\n\n                for obj in self.ordered_objects:\n                    if obj.has_data:\n                        obj.tdms_object._update_data(object_data[obj.path])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread interleaved data into numpy arrays.", "response": "def _read_interleaved_numpy(self, f, data_objects):\n        \"\"\"Read interleaved data where all channels have a numpy type\"\"\"\n\n        log.debug(\"Reading interleaved data all at once\")\n        # Read all data into 1 byte unsigned ints first\n        all_channel_bytes = data_objects[0].raw_data_width\n        if all_channel_bytes == 0:\n            all_channel_bytes = sum((o.data_type.size for o in data_objects))\n        log.debug(\"all_channel_bytes: %d\", all_channel_bytes)\n        number_bytes = int(all_channel_bytes * data_objects[0].number_values)\n        combined_data = fromfile(f, dtype=np.uint8, count=number_bytes)\n        # Reshape, so that one row is all bytes for all objects\n        combined_data = combined_data.reshape(-1, all_channel_bytes)\n        # Now set arrays for each channel\n        data_pos = 0\n        for (i, obj) in enumerate(data_objects):\n            byte_columns = tuple(\n                range(data_pos, obj.data_type.size + data_pos))\n            log.debug(\"Byte columns for channel %d: %s\", i, byte_columns)\n            # Select columns for this channel, so that number of values will\n            # be number of bytes per point * number of data points.\n            # Then use ravel to flatten the results into a vector.\n            object_data = combined_data[:, byte_columns].ravel()\n            # Now set correct data type, so that the array length should\n            # be correct\n            object_data.dtype = (\n                np.dtype(obj.data_type.nptype).newbyteorder(self.endianness))\n            obj.tdms_object._update_data(object_data)\n            data_pos += obj.data_type.size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_interleaved(self, f, data_objects):\n\n        log.debug(\"Reading interleaved data point by point\")\n        object_data = {}\n        points_added = {}\n        for obj in data_objects:\n            object_data[obj.path] = obj._new_segment_data()\n            points_added[obj.path] = 0\n        while any([points_added[o.path] < o.number_values\n                  for o in data_objects]):\n            for obj in data_objects:\n                if points_added[obj.path] < obj.number_values:\n                    object_data[obj.path][points_added[obj.path]] = (\n                        obj._read_value(f))\n                    points_added[obj.path] += 1\n        for obj in data_objects:\n            obj.tdms_object._update_data(object_data[obj.path])", "response": "Read interleaved data that doesn t have a numpy type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef time_track(self, absolute_time=False, accuracy='ns'):\n\n        try:\n            increment = self.property('wf_increment')\n            offset = self.property('wf_start_offset')\n        except KeyError:\n            raise KeyError(\"Object does not have time properties available.\")\n\n        periods = len(self._data)\n\n        relative_time = np.linspace(\n            offset,\n            offset + (periods - 1) * increment,\n            periods)\n\n        if not absolute_time:\n            return relative_time\n\n        try:\n            start_time = self.property('wf_start_time')\n        except KeyError:\n            raise KeyError(\n                \"Object does not have start time property available.\")\n\n        try:\n            unit_correction = {\n                's': 1e0,\n                'ms': 1e3,\n                'us': 1e6,\n                'ns': 1e9,\n            }[accuracy]\n        except KeyError:\n            raise KeyError(\"Invalid accuracy: {0}\".format(accuracy))\n\n        # Because numpy only knows ints as its date datatype,\n        # convert to accuracy.\n        time_type = \"timedelta64[{0}]\".format(accuracy)\n        return (np.datetime64(start_time) +\n                (relative_time * unit_correction).astype(time_type))", "response": "Return an array of time or independent variable for this channel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _initialise_data(self, memmap_dir=None):\n\n        if self.number_values == 0:\n            pass\n        elif self.data_type.nptype is None:\n            self._data = []\n        else:\n            if memmap_dir:\n                memmap_file = tempfile.NamedTemporaryFile(\n                    mode='w+b', prefix=\"nptdms_\", dir=memmap_dir)\n                self._data = np.memmap(\n                    memmap_file.file,\n                    mode='w+',\n                    shape=(self.number_values,),\n                    dtype=self.data_type.nptype)\n            else:\n                self._data = np.zeros(\n                    self.number_values, dtype=self.data_type.nptype)\n            self._data_insert_position = 0\n        if self._data is not None:\n            log.debug(\"Allocated %d sample slots for %s\", len(self._data),\n                      self.path)\n        else:\n            log.debug(\"Allocated no space for %s\", self.path)", "response": "Initialise data array to zeros"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the object data with a new array of data", "response": "def _update_data(self, new_data):\n        \"\"\"Update the object data with a new array of data\"\"\"\n\n        log.debug(\"Adding %d data points to data for %s\" %\n                  (len(new_data), self.path))\n        if self._data is None:\n            self._data = new_data\n        else:\n            if self.data_type.nptype is not None:\n                data_pos = (\n                    self._data_insert_position,\n                    self._data_insert_position + len(new_data))\n                self._data_insert_position += len(new_data)\n                self._data[data_pos[0]:data_pos[1]] = new_data\n            else:\n                self._data.extend(new_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the TDMS object to a Pandas DataFrame.", "response": "def as_dataframe(self, absolute_time=False):\n        \"\"\"\n        Converts the TDMS object to a DataFrame\n\n        :param absolute_time: Whether times should be absolute rather than\n            relative to the start time.\n        :return: The TDMS object data.\n        :rtype: pandas.DataFrame\n        \"\"\"\n\n        import pandas as pd\n\n        # When absolute_time is True,\n        # use the wf_start_time as offset for the time_track()\n        try:\n            time = self.time_track(absolute_time)\n        except KeyError:\n            time = None\n        if self.channel is None:\n            return pd.DataFrame.from_items(\n                [(ch.channel, pd.Series(ch.data))\n                    for ch in self.tdms_file.group_channels(self.group)])\n        else:\n            return pd.DataFrame(self._data, index=time, columns=[self.path])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data(self):\n        if self._data is None:\n            # self._data is None if data segment is empty\n            return np.empty((0, 1))\n        if self._data_scaled is None:\n            scale = scaling.get_scaling(self)\n            if scale is None:\n                self._data_scaled = self._data\n            else:\n                self._data_scaled = scale.scale(self._data)\n\n        return self._data_scaled", "response": "Returns NumPy array containing data for this object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_metadata(self, f, endianness):\n        self.data_type = types.tds_data_types[0xFFFFFFFF]\n        self.dimension = types.Uint32.read(f, endianness)\n        # In TDMS format version 2.0, 1 is the only valid value for dimension\n        if self.dimension != 1:\n            log.warning(\"Data dimension is not 1\")\n        self.chunk_size = types.Uint64.read(f, endianness)\n        # size of vector of format changing scalers\n        self.scaler_vector_length = types.Uint32.read(f, endianness)\n        # Size of the vector\n        log.debug(\"mxDAQ format scaler vector size '%d'\" %\n                  (self.scaler_vector_length,))\n        if self.scaler_vector_length > 1:\n            log.error(\"mxDAQ multiple format changing scalers not implemented\")\n\n        for idx in range(self.scaler_vector_length):\n            # WARNING: This code overwrites previous values with new\n            # values.  At this time NI provides no documentation on\n            # how to use these scalers and sample TDMS files do not\n            # include more than one of these scalers.\n            self.scaler_data_type_code = types.Uint32.read(f, endianness)\n            self.scaler_data_type = (\n                types.tds_data_types[self.scaler_data_type_code])\n\n            # more info for format changing scaler\n            self.scaler_raw_buffer_index = types.Uint32.read(f, endianness)\n            self.scaler_raw_byte_offset = types.Uint32.read(f, endianness)\n            self.scaler_sample_format_bitmap = types.Uint32.read(f, endianness)\n            self.scale_id = types.Uint32.read(f, endianness)\n\n        raw_data_widths_length = types.Uint32.read(f, endianness)\n        self.raw_data_widths = np.zeros(raw_data_widths_length, dtype=np.int32)\n        for cnt in range(raw_data_widths_length):\n            self.raw_data_widths[cnt] = types.Uint32.read(f, endianness)", "response": "Reads the metadata for a DAQmx raw segment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_metadata(self, f):\n\n        raw_data_index = types.Uint32.read(f, self.endianness)\n\n        log.debug(\"Reading metadata for object %s\", self.tdms_object.path)\n\n        # Object has no data in this segment\n        if raw_data_index == 0xFFFFFFFF:\n            log.debug(\"Object has no data in this segment\")\n            self.has_data = False\n            # Leave number_values and data_size as set previously,\n            # as these may be re-used by later segments.\n        # Data has same structure as previously\n        elif raw_data_index == 0x00000000:\n            log.debug(\n                \"Object has same data structure as in the previous segment\")\n            self.has_data = True\n        elif raw_data_index == 0x00001269 or raw_data_index == 0x00001369:\n            # This is a DAQmx raw data segment.\n            #    0x00001269 for segment containing Format Changing scaler.\n            #    0x00001369 for segment containing Digital Line scaler.\n            if raw_data_index == 0x00001369:\n                # special scaling for DAQ's digital input lines?\n                log.warning(\"DAQmx with Digital Line scaler has not tested\")\n\n            # DAQmx raw data format metadata has its own class\n            self.has_data = True\n            self.tdms_object.has_data = True\n\n            info = self._read_metadata_mx(f)\n            self.dimension = info.dimension\n            self.data_type = info.data_type\n            # DAQmx format has special chunking\n            self.data_size = info.chunk_size\n            self.number_values = info.chunk_size\n            # segment reading code relies on a single consistent raw\n            # data width so assert that there is only one.\n            assert(len(info.raw_data_widths) == 1)\n            self.raw_data_width = info.raw_data_widths[0]\n            # fall through and read properties\n        else:\n            # Assume metadata format is legacy TDMS format.\n            # raw_data_index gives the length of the index information.\n            self.has_data = True\n            self.tdms_object.has_data = True\n\n            # Read the data type\n            try:\n                self.data_type = types.tds_data_types[\n                    types.Uint32.read(f, self.endianness)]\n            except KeyError:\n                raise KeyError(\"Unrecognised data type\")\n            if (self.tdms_object.data_type is not None and\n                    self.data_type != self.tdms_object.data_type):\n                raise ValueError(\n                    \"Segment object doesn't have the same data \"\n                    \"type as previous segments.\")\n            else:\n                self.tdms_object.data_type = self.data_type\n            log.debug(\"Object data type: %r\", self.tdms_object.data_type)\n\n            if (self.tdms_object.data_type.size is None and\n                    self.tdms_object.data_type != types.String):\n                raise ValueError(\n                    \"Unsupported data type: %r\" % self.tdms_object.data_type)\n\n            # Read data dimension\n            self.dimension = types.Uint32.read(f, self.endianness)\n            # In TDMS version 2.0, 1 is the only valid value for dimension\n            if self.dimension != 1:\n                log.warning(\"Data dimension is not 1\")\n\n            # Read number of values\n            self.number_values = types.Uint64.read(f, self.endianness)\n\n            # Variable length data types have total size\n            if self.data_type in (types.String, ):\n                self.data_size = types.Uint64.read(f, self.endianness)\n            else:\n                self.data_size = (\n                    self.number_values *\n                    self.data_type.size * self.dimension)\n\n            log.debug(\n                \"Object number of values in segment: %d\", self.number_values)\n\n        # Read data properties\n        num_properties = types.Uint32.read(f, self.endianness)\n        log.debug(\"Reading %d properties\", num_properties)\n        for i in range(num_properties):\n            prop_name, value = read_property(f, self.endianness)\n            self.tdms_object.properties[prop_name] = value", "response": "Read the object metadata and update the object information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_value(self, file):\n\n        if self.data_type.nptype is not None:\n            dtype = (np.dtype(self.data_type.nptype).newbyteorder(\n                self.endianness))\n            return fromfile(file, dtype=dtype, count=1)\n        return self.data_type.read(file, self.endianness)", "response": "Read a single value from the given file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_values(self, file, number_values):\n\n        if self.data_type.nptype is not None:\n            dtype = (np.dtype(self.data_type.nptype).newbyteorder(\n                self.endianness))\n            return fromfile(file, dtype=dtype, count=number_values)\n        elif self.data_type == types.String:\n            return read_string_data(file, number_values, self.endianness)\n        data = self._new_segment_data()\n        for i in range(number_values):\n            data[i] = self.data_type.read(file, self.endianness)\n        return data", "response": "Read all values for this object from a contiguous segment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new array to read the data of the current section into", "response": "def _new_segment_data(self):\n        \"\"\"Return a new array to read the data of the current section into\"\"\"\n\n        if self.data_type.nptype is not None:\n            return np.zeros(self.number_values, dtype=self.data_type.nptype)\n        else:\n            return [None] * self.number_values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetect build of SNPs.", "response": "def detect_build(snps):\n    \"\"\" Detect build of SNPs.\n\n    Use the coordinates of common SNPs to identify the build / assembly of a genotype file\n    that is being loaded.\n\n    Notes\n    -----\n    rs3094315 : plus strand in 36, 37, and 38\n    rs11928389 : plus strand in 36, minus strand in 37 and 38\n    rs2500347 : plus strand in 36 and 37, minus strand in 38\n    rs964481 : plus strand in 36, 37, and 38\n    rs2341354 : plus strand in 36, 37, and 38\n\n    Parameters\n    ----------\n    snps : pandas.DataFrame\n        SNPs to add\n\n    Returns\n    -------\n    int\n        detected build of SNPs, else None\n\n    References\n    ----------\n    ..[1] Yates et. al. (doi:10.1093/bioinformatics/btu613),\n      http://europepmc.org/search/?query=DOI:10.1093/bioinformatics/btu613\n    ..[2] Zerbino et. al. (doi.org/10.1093/nar/gkx1098), https://doi.org/10.1093/nar/gkx1098\n    ..[3] Sherry ST, Ward MH, Kholodov M, Baker J, Phan L, Smigielski EM, Sirotkin K.\n      dbSNP: the NCBI database of genetic variation. Nucleic Acids Res. 2001 Jan 1;29(1):308-11.\n    ..[4] Database of Single Nucleotide Polymorphisms (dbSNP). Bethesda (MD): National Center\n      for Biotechnology Information, National Library of Medicine. dbSNP accession: rs3094315,\n      rs11928389, rs2500347, rs964481, and rs2341354 (dbSNP Build ID: 151). Available from:\n      http://www.ncbi.nlm.nih.gov/SNP/\n    \"\"\"\n\n    def lookup_build_with_snp_pos(pos, s):\n        try:\n            return s.loc[s == pos].index[0]\n        except:\n            return None\n\n    build = None\n\n    rsids = [\"rs3094315\", \"rs11928389\", \"rs2500347\", \"rs964481\", \"rs2341354\"]\n    df = pd.DataFrame(\n        {\n            36: [742429, 50908372, 143649677, 27566744, 908436],\n            37: [752566, 50927009, 144938320, 27656823, 918573],\n            38: [817186, 50889578, 148946169, 27638706, 983193],\n        },\n        index=rsids,\n    )\n\n    for rsid in rsids:\n        if rsid in snps.index:\n            build = lookup_build_with_snp_pos(snps.loc[rsid].pos, df.loc[rsid])\n\n        if build is not None:\n            break\n\n    return build"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_chromosomes(snps):\n\n    if isinstance(snps, pd.DataFrame):\n        return list(pd.unique(snps[\"chrom\"]))\n    else:\n        return []", "response": "Returns a list of chromosomes in the given SNPs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string representation of the chromosomes of the given SNPs.", "response": "def get_chromosomes_summary(snps):\n    \"\"\" Summary of the chromosomes of SNPs.\n\n    Parameters\n    ----------\n    snps : pandas.DataFrame\n\n    Returns\n    -------\n    str\n        human-readable listing of chromosomes (e.g., '1-3, MT'), empty str if no chromosomes\n    \"\"\"\n\n    if isinstance(snps, pd.DataFrame):\n        chroms = list(pd.unique(snps[\"chrom\"]))\n\n        int_chroms = [int(chrom) for chrom in chroms if chrom.isdigit()]\n        str_chroms = [chrom for chrom in chroms if not chrom.isdigit()]\n\n        # https://codereview.stackexchange.com/a/5202\n        def as_range(iterable):\n            l = list(iterable)\n            if len(l) > 1:\n                return \"{0}-{1}\".format(l[0], l[-1])\n            else:\n                return \"{0}\".format(l[0])\n\n        # create str representations\n        int_chroms = \", \".join(\n            as_range(g)\n            for _, g in groupby(int_chroms, key=lambda n, c=count(): n - next(c))\n        )\n        str_chroms = \", \".join(str_chroms)\n\n        if int_chroms != \"\" and str_chroms != \"\":\n            int_chroms += \", \"\n\n        return int_chroms + str_chroms\n    else:\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef determine_sex(\n    snps, y_snps_not_null_threshold=0.1, heterozygous_x_snps_threshold=0.01\n):\n    \"\"\" Determine sex from SNPs using thresholds.\n\n    Parameters\n    ----------\n    snps : pandas.DataFrame\n    y_snps_not_null_threshold : float\n        percentage Y SNPs that are not null; above this threshold, Male is determined\n    heterozygous_x_snps_threshold : float\n        percentage heterozygous X SNPs; above this threshold, Female is determined\n\n    Returns\n    -------\n    str\n        'Male' or 'Female' if detected, else empty str\n    \"\"\"\n\n    if isinstance(snps, pd.DataFrame):\n        y_snps = len(snps.loc[(snps[\"chrom\"] == \"Y\")])\n\n        if y_snps > 0:\n            y_snps_not_null = len(\n                snps.loc[(snps[\"chrom\"] == \"Y\") & (snps[\"genotype\"].notnull())]\n            )\n\n            if y_snps_not_null / y_snps > y_snps_not_null_threshold:\n                return \"Male\"\n            else:\n                return \"Female\"\n\n        x_snps = len(snps.loc[snps[\"chrom\"] == \"X\"])\n\n        if x_snps == 0:\n            return \"\"\n\n        heterozygous_x_snps = len(\n            snps.loc[\n                (snps[\"chrom\"] == \"X\")\n                & (snps[\"genotype\"].notnull())\n                & (snps[\"genotype\"].str[0] != snps[\"genotype\"].str[1])\n            ]\n        )\n\n        if heterozygous_x_snps / x_snps > heterozygous_x_snps_threshold:\n            return \"Female\"\n        else:\n            return \"Male\"\n    else:\n        return \"\"", "response": "Determine sex from SNPs using thresholds."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort SNPs based on ordered chromosome list and position.", "response": "def sort_snps(snps):\n    \"\"\" Sort SNPs based on ordered chromosome list and position. \"\"\"\n\n    sorted_list = sorted(snps[\"chrom\"].unique(), key=_natural_sort_key)\n\n    # move PAR and MT to the end of the dataframe\n    if \"PAR\" in sorted_list:\n        sorted_list.remove(\"PAR\")\n        sorted_list.append(\"PAR\")\n\n    if \"MT\" in sorted_list:\n        sorted_list.remove(\"MT\")\n        sorted_list.append(\"MT\")\n\n    # convert chrom column to category for sorting\n    # https://stackoverflow.com/a/26707444\n    snps[\"chrom\"] = snps[\"chrom\"].astype(\n        CategoricalDtype(categories=sorted_list, ordered=True)\n    )\n\n    # sort based on ordered chromosome list and position\n    snps = snps.sort_values([\"chrom\", \"pos\"])\n\n    # convert chromosome back to object\n    snps[\"chrom\"] = snps[\"chrom\"].astype(object)\n\n    return snps"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict of summary information for the current object.", "response": "def get_summary(self):\n        \"\"\" Get summary of ``SNPs``.\n\n        Returns\n        -------\n        dict\n            summary info, else None if ``SNPs`` is not valid\n        \"\"\"\n        if not self.is_valid():\n            return None\n        else:\n            return {\n                \"source\": self.source,\n                \"assembly\": self.assembly,\n                \"build\": self.build,\n                \"build_detected\": self.build_detected,\n                \"snp_count\": self.snp_count,\n                \"chromosomes\": self.chromosomes_summary,\n                \"sex\": self.sex,\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_23andme(file):\n        df = pd.read_csv(\n            file,\n            comment=\"#\",\n            sep=\"\\t\",\n            na_values=\"--\",\n            names=[\"rsid\", \"chrom\", \"pos\", \"genotype\"],\n            index_col=0,\n            dtype={\"chrom\": object},\n        )\n\n        return sort_snps(df), \"23andMe\"", "response": "Read and parse 23andMe file and return pandas. DataFrame with genetic data sorted for use with Lineage"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_ftdna(file):\n        df = pd.read_csv(\n            file,\n            skiprows=1,\n            na_values=\"--\",\n            names=[\"rsid\", \"chrom\", \"pos\", \"genotype\"],\n            index_col=0,\n            dtype={\"chrom\": object},\n        )\n\n        # remove incongruous data\n        df = df.drop(df.loc[df[\"chrom\"] == \"0\"].index)\n        df = df.drop(\n            df.loc[df.index == \"RSID\"].index\n        )  # second header for concatenated data\n\n        # if second header existed, pos dtype will be object (should be np.int64)\n        df[\"pos\"] = df[\"pos\"].astype(np.int64)\n\n        return sort_snps(df), \"FTDNA\"", "response": "Read and parse Family Tree DNA file and return a pandas. DataFrame with the genetic data sorted for use with Lineage"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_ftdna_famfinder(file):\n        df = pd.read_csv(\n            file,\n            comment=\"#\",\n            na_values=\"-\",\n            names=[\"rsid\", \"chrom\", \"pos\", \"allele1\", \"allele2\"],\n            index_col=0,\n            dtype={\"chrom\": object},\n        )\n\n        # create genotype column from allele columns\n        df[\"genotype\"] = df[\"allele1\"] + df[\"allele2\"]\n\n        # delete allele columns\n        # http://stackoverflow.com/a/13485766\n        del df[\"allele1\"]\n        del df[\"allele2\"]\n\n        return sort_snps(df), \"FTDNA\"", "response": "Read and parse Family Tree DNA ( FAMFinder ) file and return a pandas. DataFrame containing the individual s genetic data normalized for use with Lineage."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_ancestry(file):\n        df = pd.read_csv(\n            file,\n            comment=\"#\",\n            header=0,\n            sep=\"\\t\",\n            na_values=0,\n            names=[\"rsid\", \"chrom\", \"pos\", \"allele1\", \"allele2\"],\n            index_col=0,\n            dtype={\"chrom\": object},\n        )\n\n        # create genotype column from allele columns\n        df[\"genotype\"] = df[\"allele1\"] + df[\"allele2\"]\n\n        # delete allele columns\n        # http://stackoverflow.com/a/13485766\n        del df[\"allele1\"]\n        del df[\"allele2\"]\n\n        # https://redd.it/5y90un\n        df.ix[np.where(df[\"chrom\"] == \"23\")[0], \"chrom\"] = \"X\"\n        df.ix[np.where(df[\"chrom\"] == \"24\")[0], \"chrom\"] = \"Y\"\n        df.ix[np.where(df[\"chrom\"] == \"25\")[0], \"chrom\"] = \"PAR\"\n        df.ix[np.where(df[\"chrom\"] == \"26\")[0], \"chrom\"] = \"MT\"\n\n        return sort_snps(df), \"AncestryDNA\"", "response": "Read and parse Ancestry. com file and return pandas. DataFrame with individual s genetic data normalized for use with Lineage."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading and parse CSV generated by lineage.", "response": "def _read_lineage_csv(file, comments):\n        \"\"\" Read and parse CSV file generated by lineage.\n\n        Parameters\n        ----------\n        file : str\n            path to file\n        comments : str\n            comments at beginning of file\n\n        Returns\n        -------\n        pandas.DataFrame\n            individual's genetic data normalized for use with `lineage`\n        str\n            name of data source(s)\n        \"\"\"\n        source = \"\"\n        for comment in comments.split(\"\\n\"):\n            if \"Source(s):\" in comment:\n                source = comment.split(\"Source(s):\")[1].strip()\n                break\n\n        df = pd.read_csv(\n            file,\n            comment=\"#\",\n            header=0,\n            na_values=\"--\",\n            names=[\"rsid\", \"chrom\", \"pos\", \"genotype\"],\n            index_col=0,\n            dtype={\"chrom\": object, \"pos\": np.int64},\n        )\n\n        return sort_snps(df), source"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading and parse a generic CSV file.", "response": "def _read_generic_csv(file):\n        \"\"\" Read and parse generic CSV file.\n\n        Notes\n        -----\n        Assumes columns are 'rsid', 'chrom' / 'chromosome', 'pos' / 'position', and 'genotype';\n        values are comma separated; unreported genotypes are indicated by '--'; and one header row\n        precedes data. For example:\n\n            rsid,chromosome,position,genotype\n            rs1,1,1,AA\n            rs2,1,2,CC\n            rs3,1,3,--\n\n        Parameters\n        ----------\n        file : str\n            path to file\n\n        Returns\n        -------\n        pandas.DataFrame\n            individual's genetic data normalized for use with `lineage`\n        str\n            name of data source\n        \"\"\"\n        df = pd.read_csv(\n            file,\n            skiprows=1,\n            na_values=\"--\",\n            names=[\"rsid\", \"chrom\", \"pos\", \"genotype\"],\n            index_col=0,\n            dtype={\"chrom\": object, \"pos\": np.int64},\n        )\n\n        return sort_snps(df), \"generic\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _assign_par_snps(self):\n        rest_client = EnsemblRestClient(server=\"https://api.ncbi.nlm.nih.gov\")\n        for rsid in self.snps.loc[self.snps[\"chrom\"] == \"PAR\"].index.values:\n            if \"rs\" in rsid:\n                try:\n                    id = rsid.split(\"rs\")[1]\n                    response = rest_client.perform_rest_action(\n                        \"/variation/v0/beta/refsnp/\" + id\n                    )\n\n                    if response is not None:\n                        for item in response[\"primary_snapshot_data\"][\n                            \"placements_with_allele\"\n                        ]:\n                            if \"NC_000023\" in item[\"seq_id\"]:\n                                assigned = self._assign_snp(rsid, item[\"alleles\"], \"X\")\n                            elif \"NC_000024\" in item[\"seq_id\"]:\n                                assigned = self._assign_snp(rsid, item[\"alleles\"], \"Y\")\n                            else:\n                                assigned = False\n\n                            if assigned:\n                                if not self.build_detected:\n                                    self.build = self._extract_build(item)\n                                    self.build_detected = True\n                                continue\n\n                except Exception as err:\n                    print(err)", "response": "Assign PAR SNPs to the X or Y chromosome using SNP position."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_chromosomes(one_chrom_match, two_chrom_match, cytobands, path, title, build):\n    # Height of each chromosome\n    chrom_height = 1.25\n\n    # Spacing between consecutive chromosomes\n    chrom_spacing = 1\n\n    # Decide which chromosomes to use\n    chromosome_list = [\"chr%s\" % i for i in range(1, 23)]\n    chromosome_list.append(\"chrY\")\n    chromosome_list.append(\"chrX\")\n\n    # Keep track of the y positions for chromosomes, and the center of each chromosome\n    # (which is where we'll put the ytick labels)\n    ybase = 0\n    chrom_ybase = {}\n    chrom_centers = {}\n\n    # Iterate in reverse so that items in the beginning of `chromosome_list` will\n    # appear at the top of the plot\n    for chrom in chromosome_list[::-1]:\n        chrom_ybase[chrom] = ybase\n        chrom_centers[chrom] = ybase + chrom_height / 2.0\n        ybase += chrom_height + chrom_spacing\n\n    # Colors for different chromosome stains\n    color_lookup = {\n        \"gneg\": (202 / 255, 202 / 255, 202 / 255),  # background\n        \"one_chrom\": (0 / 255, 176 / 255, 240 / 255),\n        \"two_chrom\": (66 / 255, 69 / 255, 121 / 255),\n        \"centromere\": (1, 1, 1, 0.6),\n    }\n\n    df = _patch_chromosomal_features(cytobands, one_chrom_match, two_chrom_match)\n\n    # Add a new column for colors\n    df[\"colors\"] = df[\"gie_stain\"].apply(lambda x: color_lookup[x])\n\n    # Width, height (in inches)\n    figsize = (6.5, 9)\n\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(111)\n\n    # Now all we have to do is call our function for the chromosome data...\n    for collection in _chromosome_collections(df, chrom_ybase, chrom_height):\n        ax.add_collection(collection)\n\n    # Axes tweaking\n    ax.set_yticks([chrom_centers[i] for i in chromosome_list])\n    ax.set_yticklabels(chromosome_list)\n    ax.margins(0.01)\n    ax.axis(\"tight\")\n\n    handles = []\n\n    # setup legend\n    if len(one_chrom_match) > 0:\n        one_chrom_patch = patches.Patch(\n            color=color_lookup[\"one_chrom\"], label=\"One chromosome shared\"\n        )\n        handles.append(one_chrom_patch)\n\n    if len(two_chrom_match) > 0:\n        two_chrom_patch = patches.Patch(\n            color=color_lookup[\"two_chrom\"], label=\"Two chromosomes shared\"\n        )\n        handles.append(two_chrom_patch)\n\n    no_match_patch = patches.Patch(color=color_lookup[\"gneg\"], label=\"No shared DNA\")\n    handles.append(no_match_patch)\n\n    centromere_patch = patches.Patch(\n        color=(234 / 255, 234 / 255, 234 / 255), label=\"Centromere\"\n    )\n    handles.append(centromere_patch)\n\n    plt.legend(handles=handles, loc=\"lower right\", bbox_to_anchor=(0.95, 0.05))\n\n    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n    plt.xlabel(\"Build \" + str(build) + \" Chromosome Position\", fontsize=10)\n    print(\"Saving \" + os.path.relpath(path))\n    plt.tight_layout()\n    plt.savefig(path)", "response": "Plot the chromosomes of the given set of segments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _patch_chromosomal_features(cytobands, one_chrom_match, two_chrom_match):\n    chromosomes = cytobands[\"chrom\"].unique()\n\n    df = pd.DataFrame()\n\n    for chromosome in chromosomes:\n        chromosome_length = np.max(\n            cytobands[cytobands[\"chrom\"] == chromosome][\"end\"].values\n        )\n\n        # get all markers for this chromosome\n        one_chrom_match_markers = [\n            marker for marker in one_chrom_match if marker[\"chrom\"] == chromosome\n        ]\n        two_chrom_match_markers = [\n            marker for marker in two_chrom_match if marker[\"chrom\"] == chromosome\n        ]\n\n        # background of chromosome\n        df = df.append(\n            {\n                \"chrom\": chromosome,\n                \"start\": 0,\n                \"end\": chromosome_length,\n                \"gie_stain\": \"gneg\",\n            },\n            ignore_index=True,\n        )\n\n        # add markers for shared DNA on one chromosome\n        for marker in one_chrom_match_markers:\n            df = df.append(\n                {\n                    \"chrom\": chromosome,\n                    \"start\": marker[\"start\"],\n                    \"end\": marker[\"end\"],\n                    \"gie_stain\": marker[\"gie_stain\"],\n                },\n                ignore_index=True,\n            )\n\n        # add markers for shared DNA on both chromosomes\n        for marker in two_chrom_match_markers:\n            df = df.append(\n                {\n                    \"chrom\": chromosome,\n                    \"start\": marker[\"start\"],\n                    \"end\": marker[\"end\"],\n                    \"gie_stain\": marker[\"gie_stain\"],\n                },\n                ignore_index=True,\n            )\n\n        # add centromeres\n        for item in cytobands.loc[\n            (cytobands[\"chrom\"] == chromosome) & (cytobands[\"gie_stain\"] == \"acen\")\n        ].itertuples():\n            df = df.append(\n                {\n                    \"chrom\": chromosome,\n                    \"start\": item.start,\n                    \"end\": item.end,\n                    \"gie_stain\": \"centromere\",\n                },\n                ignore_index=True,\n            )\n\n    return df", "response": "Patch the start and stop positions of particular features on each chromosome segment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_dir(path):\n    # https://stackoverflow.com/a/5032238\n    try:\n        os.makedirs(path, exist_ok=True)\n    except Exception as err:\n        print(err)\n        return False\n\n    if os.path.exists(path):\n        return True\n    else:\n        return False", "response": "Create directory specified by path if it doesn t already exist."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_df_as_csv(df, path, filename, comment=None, **kwargs):\n    if isinstance(df, pd.DataFrame) and len(df) > 0:\n        try:\n            if not create_dir(path):\n                return \"\"\n\n            destination = os.path.join(path, filename)\n\n            print(\"Saving \" + os.path.relpath(destination))\n\n            s = (\n                \"# Generated by lineage v{}, https://github.com/apriha/lineage\\n\"\n                \"# Generated at {} UTC\\n\"\n            )\n\n            s = s.format(\n                __version__, datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n            )\n\n            if isinstance(comment, str):\n                s += comment\n\n            with open(destination, \"w\") as f:\n                f.write(s)\n\n            # https://stackoverflow.com/a/29233924/4727627\n            with open(destination, \"a\") as f:\n                df.to_csv(f, na_rep=\"--\", **kwargs)\n\n            return destination\n        except Exception as err:\n            print(err)\n            return \"\"\n    else:\n        print(\"no data to save...\")\n        return \"\"", "response": "Save dataframe to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind discordant SNPs between two or three individuals.", "response": "def find_discordant_snps(\n        self, individual1, individual2, individual3=None, save_output=False\n    ):\n        \"\"\" Find discordant SNPs between two or three individuals.\n\n        Parameters\n        ----------\n        individual1 : Individual\n            reference individual (child if `individual2` and `individual3` are parents)\n        individual2 : Individual\n            comparison individual\n        individual3 : Individual\n            other parent if `individual1` is child and `individual2` is a parent\n        save_output : bool\n            specifies whether to save output to a CSV file in the output directory\n\n        Returns\n        -------\n        pandas.DataFrame\n            discordant SNPs and associated genetic data\n\n        References\n        ----------\n        ..[1] David Pike, \"Search for Discordant SNPs in Parent-Child\n          Raw Data Files,\" David Pike's Utilities,\n          http://www.math.mun.ca/~dapike/FF23utils/pair-discord.php\n        ..[2] David Pike, \"Search for Discordant SNPs when given data\n          for child and both parents,\" David Pike's Utilities,\n          http://www.math.mun.ca/~dapike/FF23utils/trio-discord.php\n        \"\"\"\n        self._remap_snps_to_GRCh37([individual1, individual2, individual3])\n\n        df = individual1.snps\n\n        # remove nulls for reference individual\n        df = df.loc[df[\"genotype\"].notnull()]\n\n        # add SNPs shared with `individual2`\n        df = df.join(individual2.snps[\"genotype\"], rsuffix=\"2\")\n\n        genotype1 = \"genotype_\" + individual1.get_var_name()\n        genotype2 = \"genotype_\" + individual2.get_var_name()\n\n        if individual3 is None:\n            df = df.rename(columns={\"genotype\": genotype1, \"genotype2\": genotype2})\n\n            # find discordant SNPs between reference and comparison individuals\n            df = df.loc[\n                df[genotype2].notnull()\n                & (\n                    (df[genotype1].str.len() == 1)\n                    & (df[genotype2].str.len() == 1)\n                    & (df[genotype1] != df[genotype2])\n                )\n                | (\n                    (df[genotype1].str.len() == 2)\n                    & (df[genotype2].str.len() == 2)\n                    & (df[genotype1].str[0] != df[genotype2].str[0])\n                    & (df[genotype1].str[0] != df[genotype2].str[1])\n                    & (df[genotype1].str[1] != df[genotype2].str[0])\n                    & (df[genotype1].str[1] != df[genotype2].str[1])\n                )\n            ]\n            if save_output:\n                save_df_as_csv(\n                    df,\n                    self._output_dir,\n                    \"discordant_snps_\"\n                    + individual1.get_var_name()\n                    + \"_\"\n                    + individual2.get_var_name()\n                    + \"_GRCh37.csv\",\n                )\n        else:\n            # add SNPs shared with `individual3`\n            df = df.join(individual3.snps[\"genotype\"], rsuffix=\"3\")\n\n            genotype3 = \"genotype_\" + individual3.get_var_name()\n\n            df = df.rename(\n                columns={\n                    \"genotype\": genotype1,\n                    \"genotype2\": genotype2,\n                    \"genotype3\": genotype3,\n                }\n            )\n\n            # find discordant SNPs between child and two parents\n            df = df.loc[\n                (\n                    df[genotype2].notnull()\n                    & (\n                        (df[genotype1].str.len() == 1)\n                        & (df[genotype2].str.len() == 1)\n                        & (df[genotype1] != df[genotype2])\n                    )\n                    | (\n                        (df[genotype1].str.len() == 2)\n                        & (df[genotype2].str.len() == 2)\n                        & (df[genotype1].str[0] != df[genotype2].str[0])\n                        & (df[genotype1].str[0] != df[genotype2].str[1])\n                        & (df[genotype1].str[1] != df[genotype2].str[0])\n                        & (df[genotype1].str[1] != df[genotype2].str[1])\n                    )\n                )\n                | (\n                    df[genotype3].notnull()\n                    & (\n                        (df[genotype1].str.len() == 1)\n                        & (df[genotype3].str.len() == 1)\n                        & (df[genotype1] != df[genotype3])\n                    )\n                    | (\n                        (df[genotype1].str.len() == 2)\n                        & (df[genotype3].str.len() == 2)\n                        & (df[genotype1].str[0] != df[genotype3].str[0])\n                        & (df[genotype1].str[0] != df[genotype3].str[1])\n                        & (df[genotype1].str[1] != df[genotype3].str[0])\n                        & (df[genotype1].str[1] != df[genotype3].str[1])\n                    )\n                )\n                | (\n                    df[genotype2].notnull()\n                    & df[genotype3].notnull()\n                    & (df[genotype2].str.len() == 2)\n                    & (df[genotype2].str[0] == df[genotype2].str[1])\n                    & (df[genotype2] == df[genotype3])\n                    & (df[genotype1] != df[genotype2])\n                )\n            ]\n\n            if save_output:\n                save_df_as_csv(\n                    df,\n                    self._output_dir,\n                    \"discordant_snps_\"\n                    + individual1.get_var_name()\n                    + \"_\"\n                    + individual2.get_var_name()\n                    + \"_\"\n                    + individual3.get_var_name()\n                    + \"_GRCh37.csv\",\n                )\n\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the shared DNA between two individuals.", "response": "def find_shared_dna(\n        self,\n        individual1,\n        individual2,\n        cM_threshold=0.75,\n        snp_threshold=1100,\n        shared_genes=False,\n        save_output=True,\n    ):\n        \"\"\" Find the shared DNA between two individuals.\n\n        Computes the genetic distance in centiMorgans (cMs) between SNPs using the HapMap Phase II\n        GRCh37 genetic map. Applies thresholds to determine the shared DNA. Plots shared DNA.\n        Optionally determines shared genes (i.e., genes that are transcribed from the shared DNA).\n\n        All output is saved to the output directory as `CSV` or `PNG` files.\n\n        Parameters\n        ----------\n        individual1 : Individual\n        individual2 : Individual\n        cM_threshold : float\n            minimum centiMorgans for each shared DNA segment\n        snp_threshold : int\n            minimum SNPs for each shared DNA segment\n        shared_genes : bool\n            determine shared genes\n        save_output : bool\n            specifies whether to save output files in the output directory\n\n        Returns\n        -------\n        one_chrom_shared_dna : pandas.DataFrame\n            segments of shared DNA on one chromosome\n        two_chrom_shared_dna : pandas.DataFrame\n            segments of shared DNA on two chromosomes\n        one_chrom_shared_genes : pandas.DataFrame\n            shared genes on one chromosome\n        two_chrom_shared_genes : pandas.DataFrame\n            shared genes on two chromosomes\n        \"\"\"\n        one_chrom_shared_genes = pd.DataFrame()\n        two_chrom_shared_genes = pd.DataFrame()\n\n        self._remap_snps_to_GRCh37([individual1, individual2])\n\n        df = individual1.snps\n\n        df = df.join(individual2.snps[\"genotype\"], rsuffix=\"2\", how=\"inner\")\n\n        genotype1 = \"genotype_\" + individual1.get_var_name()\n        genotype2 = \"genotype_\" + individual2.get_var_name()\n\n        df = df.rename(columns={\"genotype\": genotype1, \"genotype2\": genotype2})\n\n        one_x_chrom = self._is_one_individual_male([individual1, individual2])\n\n        # determine the genetic distance between each SNP using the HapMap Phase II genetic map\n        genetic_map, df = self._compute_snp_distances(df)\n\n        # determine where individuals share an allele on one chromosome\n        df[\"one_chrom_match\"] = np.where(\n            df[genotype1].isnull()\n            | df[genotype2].isnull()\n            | (df[genotype1].str[0] == df[genotype2].str[0])\n            | (df[genotype1].str[0] == df[genotype2].str[1])\n            | (df[genotype1].str[1] == df[genotype2].str[0])\n            | (df[genotype1].str[1] == df[genotype2].str[1]),\n            True,\n            False,\n        )\n\n        # determine where individuals share alleles on both chromosomes\n        df[\"two_chrom_match\"] = np.where(\n            df[genotype1].isnull()\n            | df[genotype2].isnull()\n            | (\n                (df[genotype1].str.len() == 2)\n                & (df[genotype2].str.len() == 2)\n                & (\n                    (df[genotype1] == df[genotype2])\n                    | (\n                        (df[genotype1].str[0] == df[genotype2].str[1])\n                        & (df[genotype1].str[1] == df[genotype2].str[0])\n                    )\n                )\n            ),\n            True,\n            False,\n        )\n\n        # compute shared DNA between individuals\n        one_chrom_shared_dna = self._compute_shared_dna(\n            df, genetic_map, \"one_chrom_match\", cM_threshold, snp_threshold, one_x_chrom\n        )\n\n        two_chrom_shared_dna = self._compute_shared_dna(\n            df, genetic_map, \"two_chrom_match\", cM_threshold, snp_threshold, one_x_chrom\n        )\n\n        cytobands = self._resources.get_cytoBand_hg19()\n\n        # plot data\n        if save_output:\n            if create_dir(self._output_dir):\n                plot_chromosomes(\n                    one_chrom_shared_dna,\n                    two_chrom_shared_dna,\n                    cytobands,\n                    os.path.join(\n                        self._output_dir,\n                        \"shared_dna_\"\n                        + individual1.get_var_name()\n                        + \"_\"\n                        + individual2.get_var_name()\n                        + \".png\",\n                    ),\n                    individual1.name + \" / \" + individual2.name + \" shared DNA\",\n                    37,\n                )\n\n        one_chrom_shared_dna = self._convert_shared_dna_list_to_df(one_chrom_shared_dna)\n\n        if len(one_chrom_shared_dna) > 0:\n            if save_output:\n                self._save_shared_dna_csv_format(\n                    one_chrom_shared_dna,\n                    \"one\",\n                    individual1.get_var_name(),\n                    individual2.get_var_name(),\n                )\n            if shared_genes:\n                one_chrom_shared_genes = self._compute_shared_genes(\n                    one_chrom_shared_dna,\n                    \"one\",\n                    individual1.get_var_name(),\n                    individual2.get_var_name(),\n                    save_output,\n                )\n\n        two_chrom_shared_dna = self._convert_shared_dna_list_to_df(two_chrom_shared_dna)\n\n        if len(two_chrom_shared_dna) > 0:\n            if save_output:\n                self._save_shared_dna_csv_format(\n                    two_chrom_shared_dna,\n                    \"two\",\n                    individual1.get_var_name(),\n                    individual2.get_var_name(),\n                )\n            if shared_genes:\n                two_chrom_shared_genes = self._compute_shared_genes(\n                    two_chrom_shared_dna,\n                    \"two\",\n                    individual1.get_var_name(),\n                    individual2.get_var_name(),\n                    save_output,\n                )\n\n        return (\n            one_chrom_shared_dna,\n            two_chrom_shared_dna,\n            one_chrom_shared_genes,\n            two_chrom_shared_genes,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_genetic_map_HapMapII_GRCh37(self):\n        if self._genetic_map_HapMapII_GRCh37 is None:\n            self._genetic_map_HapMapII_GRCh37 = self._load_genetic_map(\n                self._get_path_genetic_map_HapMapII_GRCh37()\n            )\n\n        return self._genetic_map_HapMapII_GRCh37", "response": "Returns the International HapMap Consortium HapMap Phase II genetic map for Build 37."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cytoBand_hg19(self):\n        if self._cytoBand_hg19 is None:\n            self._cytoBand_hg19 = self._load_cytoBand(self._get_path_cytoBand_hg19())\n\n        return self._cytoBand_hg19", "response": "Get UCSC cytoBand table for Build 37."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_knownGene_hg19(self):\n        if self._knownGene_hg19 is None:\n            self._knownGene_hg19 = self._load_knownGene(self._get_path_knownGene_hg19())\n\n        return self._knownGene_hg19", "response": "Get UCSC knownGene table for Build 37."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets UCSC kgXref table for Build 37.", "response": "def get_kgXref_hg19(self):\n        \"\"\" Get UCSC kgXref table for Build 37.\n\n        Returns\n        -------\n        pandas.DataFrame\n            kgXref table if loading was successful, else None\n        \"\"\"\n        if self._kgXref_hg19 is None:\n            self._kgXref_hg19 = self._load_kgXref(self._get_path_kgXref_hg19())\n\n        return self._kgXref_hg19"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets assembly mapping data.", "response": "def get_assembly_mapping_data(self, source_assembly, target_assembly):\n        \"\"\" Get assembly mapping data.\n\n        Parameters\n        ----------\n        source_assembly : {'NCBI36', 'GRCh37', 'GRCh38'}\n            assembly to remap from\n        target_assembly : {'NCBI36', 'GRCh37', 'GRCh38'}\n            assembly to remap to\n\n        Returns\n        -------\n        dict\n            dict of json assembly mapping data if loading was successful, else None\n        \"\"\"\n        return self._load_assembly_mapping_data(\n            self._get_path_assembly_mapping_data(source_assembly, target_assembly)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_example_datasets(self):\n        paths = []\n        paths.append(\n            self._download_file(\n                \"https://opensnp.org/data/662.23andme.304\",\n                \"662.23andme.304.txt.gz\",\n                compress=True,\n            )\n        )\n        paths.append(\n            self._download_file(\n                \"https://opensnp.org/data/662.23andme.340\",\n                \"662.23andme.340.txt.gz\",\n                compress=True,\n            )\n        )\n        paths.append(\n            self._download_file(\n                \"https://opensnp.org/data/662.ftdna-illumina.341\",\n                \"662.ftdna-illumina.341.csv.gz\",\n                compress=True,\n            )\n        )\n        paths.append(\n            self._download_file(\n                \"https://opensnp.org/data/663.23andme.305\",\n                \"663.23andme.305.txt.gz\",\n                compress=True,\n            )\n        )\n\n        # these two files consist of concatenated gzip files and therefore need special handling\n        paths.append(\n            self._download_file(\n                \"https://opensnp.org/data/4583.ftdna-illumina.3482\",\n                \"4583.ftdna-illumina.3482.csv.gz\",\n            )\n        )\n        paths.append(\n            self._download_file(\n                \"https://opensnp.org/data/4584.ftdna-illumina.3483\",\n                \"4584.ftdna-illumina.3483.csv.gz\",\n            )\n        )\n\n        try:\n            for gzip_path in paths[-2:]:\n                # https://stackoverflow.com/q/4928560\n                # https://stackoverflow.com/a/37042747\n                with open(gzip_path, \"rb\") as f:\n                    decompressor = zlib.decompressobj(31)\n\n                    # decompress data from first concatenated gzip file\n                    data = decompressor.decompress(f.read())\n\n                    if len(decompressor.unused_data) > 0:\n                        # decompress data from second concatenated gzip file, if any\n                        additional_data = zlib.decompress(decompressor.unused_data, 31)\n                        data += additional_data[33:]  # skip over second header\n\n                # recompress data\n                with gzip.open(gzip_path, \"wb\") as f:\n                    f.write(data)\n        except Exception as err:\n            print(err)\n\n        return paths", "response": "Download example datasets from openSNP <https://opensnp. org > _."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_all_resources(self):\n        resources = {}\n        resources[\n            \"genetic_map_HapMapII_GRCh37\"\n        ] = self.get_genetic_map_HapMapII_GRCh37()\n        resources[\"cytoBand_hg19\"] = self.get_cytoBand_hg19()\n        resources[\"knownGene_hg19\"] = self.get_knownGene_hg19()\n        resources[\"kgXref_hg19\"] = self.get_kgXref_hg19()\n        for source, target in itertools.permutations([\"NCBI36\", \"GRCh37\", \"GRCh38\"], 2):\n            resources[source + \"_\" + target] = self.get_assembly_mapping_data(\n                source, target\n            )\n        return resources", "response": "Get / download all resources used throughout Lineage."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads genetic map from a file.", "response": "def _load_genetic_map(filename):\n        \"\"\" Load genetic map (e.g. HapMapII).\n\n        Parameters\n        ----------\n        filename : str\n            path to compressed archive with genetic map data\n\n        Returns\n        -------\n        genetic_map : dict\n            dict of pandas.DataFrame genetic maps if loading was successful, else None\n\n        Notes\n        -----\n        Keys of returned dict are chromosomes and values are the corresponding genetic map.\n        \"\"\"\n        try:\n            genetic_map = {}\n\n            with tarfile.open(filename, \"r\") as tar:\n                # http://stackoverflow.com/a/2018576\n                for member in tar.getmembers():\n                    if \"genetic_map\" in member.name:\n                        df = pd.read_csv(tar.extractfile(member), sep=\"\\t\")\n                        df = df.rename(\n                            columns={\n                                \"Position(bp)\": \"pos\",\n                                \"Rate(cM/Mb)\": \"rate\",\n                                \"Map(cM)\": \"map\",\n                            }\n                        )\n                        del df[\"Chromosome\"]\n                        start_pos = member.name.index(\"chr\") + 3\n                        end_pos = member.name.index(\".\")\n                        genetic_map[member.name[start_pos:end_pos]] = df\n\n            # X chrom consists of X PAR regions and X non-PAR region\n            genetic_map[\"X\"] = pd.concat(\n                [genetic_map[\"X_par1\"], genetic_map[\"X\"], genetic_map[\"X_par2\"]]\n            )\n            del genetic_map[\"X_par1\"]\n            del genetic_map[\"X_par2\"]\n\n            return genetic_map\n        except Exception as err:\n            print(err)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_assembly_mapping_data(filename):\n        try:\n            assembly_mapping_data = {}\n\n            with tarfile.open(filename, \"r\") as tar:\n                # http://stackoverflow.com/a/2018576\n                for member in tar.getmembers():\n                    if \".json\" in member.name:\n                        with tar.extractfile(member) as tar_file:\n                            tar_bytes = tar_file.read()\n                        # https://stackoverflow.com/a/42683509/4727627\n                        assembly_mapping_data[member.name.split(\".\")[0]] = json.loads(\n                            tar_bytes.decode(\"utf-8\")\n                        )\n\n            return assembly_mapping_data\n        except Exception as err:\n            print(err)\n            return None", "response": "Load assembly mapping data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_cytoBand(filename):\n        try:\n            # adapted from chromosome plotting code (see [1]_)\n            df = pd.read_table(\n                filename, names=[\"chrom\", \"start\", \"end\", \"name\", \"gie_stain\"]\n            )\n            df[\"chrom\"] = df[\"chrom\"].str[3:]\n            return df\n        except Exception as err:\n            print(err)\n            return None", "response": "Load UCSC cytoBand table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_knownGene(filename):\n        try:\n            df = pd.read_table(\n                filename,\n                names=[\n                    \"name\",\n                    \"chrom\",\n                    \"strand\",\n                    \"txStart\",\n                    \"txEnd\",\n                    \"cdsStart\",\n                    \"cdsEnd\",\n                    \"exonCount\",\n                    \"exonStarts\",\n                    \"exonEnds\",\n                    \"proteinID\",\n                    \"alignID\",\n                ],\n                index_col=0,\n            )\n            df[\"chrom\"] = df[\"chrom\"].str[3:]\n            return df\n        except Exception as err:\n            print(err)\n            return None", "response": "Load UCSC knownGene table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading UCSC kgXref table.", "response": "def _load_kgXref(filename):\n        \"\"\" Load UCSC kgXref table.\n\n        Parameters\n        ----------\n        filename : str\n            path to kgXref file\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            kgXref table if loading was successful, else None\n        \"\"\"\n        try:\n            df = pd.read_table(\n                filename,\n                names=[\n                    \"kgID\",\n                    \"mRNA\",\n                    \"spID\",\n                    \"spDisplayID\",\n                    \"geneSymbol\",\n                    \"refseq\",\n                    \"protAcc\",\n                    \"description\",\n                    \"rfamAcc\",\n                    \"tRnaName\",\n                ],\n                index_col=0,\n                dtype=object,\n            )\n            return df\n        except Exception as err:\n            print(err)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_path_assembly_mapping_data(\n        self, source_assembly, target_assembly, retries=10\n    ):\n        \"\"\" Get local path to assembly mapping data, downloading if necessary.\n\n        Parameters\n        ----------\n        source_assembly : {'NCBI36', 'GRCh37', 'GRCh38'}\n            assembly to remap from\n        target_assembly : {'NCBI36', 'GRCh37', 'GRCh38'}\n            assembly to remap to\n        retries : int\n            number of retries per chromosome to download assembly mapping data\n\n        Returns\n        -------\n        str\n            path to <source_assembly>_<target_assembly>.tar.gz\n\n        References\n        ----------\n        ..[1] Ensembl, Assembly Information Endpoint,\n          https://rest.ensembl.org/documentation/info/assembly_info\n        ..[2] Ensembl, Assembly Map Endpoint,\n          http://rest.ensembl.org/documentation/info/assembly_map\n\n        \"\"\"\n\n        if not lineage.create_dir(self._resources_dir):\n            return None\n\n        chroms = [\n            \"1\",\n            \"2\",\n            \"3\",\n            \"4\",\n            \"5\",\n            \"6\",\n            \"7\",\n            \"8\",\n            \"9\",\n            \"10\",\n            \"11\",\n            \"12\",\n            \"13\",\n            \"14\",\n            \"15\",\n            \"16\",\n            \"17\",\n            \"18\",\n            \"19\",\n            \"20\",\n            \"21\",\n            \"22\",\n            \"X\",\n            \"Y\",\n            \"MT\",\n        ]\n\n        assembly_mapping_data = source_assembly + \"_\" + target_assembly\n        destination = os.path.join(\n            self._resources_dir, assembly_mapping_data + \".tar.gz\"\n        )\n\n        if not os.path.exists(destination) or not self._all_chroms_in_tar(\n            chroms, destination\n        ):\n            print(\"Downloading {}\".format(os.path.relpath(destination)))\n\n            try:\n                with tarfile.open(destination, \"w:gz\") as out_tar:\n                    for chrom in chroms:\n                        file = chrom + \".json\"\n\n                        map_endpoint = (\n                            \"/map/human/\"\n                            + source_assembly\n                            + \"/\"\n                            + chrom\n                            + \"/\"\n                            + target_assembly\n                            + \"?\"\n                        )\n\n                        # get assembly mapping data\n                        response = None\n                        retry = 0\n                        while response is None and retry < retries:\n                            response = self._ensembl_rest_client.perform_rest_action(\n                                map_endpoint\n                            )\n                            retry += 1\n\n                        if response is not None:\n                            # open temp file, save json response to file, close temp file\n                            with tempfile.NamedTemporaryFile(\n                                delete=False, mode=\"w\"\n                            ) as f:\n                                json.dump(response, f)\n\n                            # add temp file to archive\n                            out_tar.add(f.name, arcname=file)\n\n                            # remove temp file\n                            os.remove(f.name)\n            except Exception as err:\n                print(err)\n                return None\n\n        return destination", "response": "Get local path to assembly mapping data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _download_file(self, url, filename, compress=False, timeout=30):\n        if not lineage.create_dir(self._resources_dir):\n            return None\n\n        if compress and filename[-3:] != \".gz\":\n            filename += \".gz\"\n\n        destination = os.path.join(self._resources_dir, filename)\n\n        if not os.path.exists(destination):\n            try:\n                if compress:\n                    open_func = gzip.open\n                else:\n                    open_func = open\n\n                # get file if it hasn't already been downloaded\n                # http://stackoverflow.com/a/7244263\n                with urllib.request.urlopen(\n                    url, timeout=timeout\n                ) as response, open_func(destination, \"wb\") as f:\n                    self._print_download_msg(destination)\n                    data = response.read()  # a `bytes` object\n                    f.write(data)\n            except urllib.error.URLError as err:\n                print(err)\n                destination = None\n                # try HTTP if an FTP error occurred\n                if \"ftp://\" in url:\n                    destination = self._download_file(\n                        url.replace(\"ftp://\", \"http://\"),\n                        filename,\n                        compress=compress,\n                        timeout=timeout,\n                    )\n            except Exception as err:\n                print(err)\n                return None\n\n        return destination", "response": "Download a file from url and save as filename."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef discrepant_snps(self):\n        df = self._discrepant_positions.append(self._discrepant_genotypes)\n        if len(df) > 1:\n            df = df.drop_duplicates()\n        return df", "response": "Returns a DataFrame containing the set of discrepant SNPs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_snps(\n        self,\n        raw_data,\n        discrepant_snp_positions_threshold=100,\n        discrepant_genotypes_threshold=500,\n        save_output=False,\n    ):\n        \"\"\" Load raw genotype data.\n\n        Parameters\n        ----------\n        raw_data : list or str\n            path(s) to file(s) with raw genotype data\n        discrepant_snp_positions_threshold : int\n            threshold for discrepant SNP positions between existing data and data to be loaded,\n            a large value could indicate mismatched genome assemblies\n        discrepant_genotypes_threshold : int\n            threshold for discrepant genotype data between existing data and data to be loaded,\n            a large value could indicated mismatched individuals\n        save_output : bool\n            specifies whether to save discrepant SNP output to CSV files in the output directory\n        \"\"\"\n        if type(raw_data) is list:\n            for file in raw_data:\n                self._load_snps_helper(\n                    file,\n                    discrepant_snp_positions_threshold,\n                    discrepant_genotypes_threshold,\n                    save_output,\n                )\n        elif type(raw_data) is str:\n            self._load_snps_helper(\n                raw_data,\n                discrepant_snp_positions_threshold,\n                discrepant_genotypes_threshold,\n                save_output,\n            )\n        else:\n            raise TypeError(\"invalid filetype\")", "response": "Loads SNPs from raw data into a single entry in the output directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_snps(self, filename=None):\n        comment = (\n            \"# Source(s): {}\\n\"\n            \"# Assembly: {}\\n\"\n            \"# SNPs: {}\\n\"\n            \"# Chromosomes: {}\\n\".format(\n                self.source, self.assembly, self.snp_count, self.chromosomes_summary\n            )\n        )\n\n        if filename is None:\n            filename = self.get_var_name() + \"_lineage_\" + self.assembly + \".csv\"\n\n        return lineage.save_df_as_csv(\n            self._snps,\n            self._output_dir,\n            filename,\n            comment=comment,\n            header=[\"chromosome\", \"position\", \"genotype\"],\n        )", "response": "Save the SNPs of the current object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remap_snps(self, target_assembly, complement_bases=True):\n        from lineage import Lineage\n\n        l = Lineage()\n        return l.remap_snps(self, target_assembly, complement_bases)", "response": "This method maps the SNP coordinates of this Individual to another assembly."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the _snps and _build properties of this object.", "response": "def _set_snps(self, snps, build=37):\n        \"\"\" Set `_snps` and `_build` properties of this ``Individual``.\n\n        Notes\n        -----\n        Intended to be used internally to `lineage`.\n\n        Parameters\n        ----------\n        snps : pandas.DataFrame\n            individual's genetic data normalized for use with `lineage`\n        build : int\n            build of this ``Individual``'s SNPs\n        \"\"\"\n        self._snps = snps\n        self._build = build"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds SNPs to this Individual.", "response": "def _add_snps(\n        self,\n        snps,\n        discrepant_snp_positions_threshold,\n        discrepant_genotypes_threshold,\n        save_output,\n    ):\n        \"\"\" Add SNPs to this Individual.\n\n        Parameters\n        ----------\n        snps : SNPs\n            SNPs to add\n        discrepant_snp_positions_threshold : int\n            see above\n        discrepant_genotypes_threshold : int\n            see above\n        save_output\n            see above\n\n        Returns\n        -------\n        discrepant_positions : pandas.DataFrame\n        discrepant_genotypes : pandas.DataFrame\n        \"\"\"\n        discrepant_positions = pd.DataFrame()\n        discrepant_genotypes = pd.DataFrame()\n\n        if snps.snps is None:\n            return discrepant_positions, discrepant_genotypes\n\n        build = snps.build\n        source = [s.strip() for s in snps.source.split(\",\")]\n\n        if not snps.build_detected:\n            print(\"build not detected, assuming build {}\".format(snps.build))\n\n        if self._build is None:\n            self._build = build\n        elif self._build != build:\n            print(\n                \"build / assembly mismatch between current build of SNPs and SNPs being loaded\"\n            )\n\n        # ensure there area always two X alleles\n        snps = self._double_single_alleles(snps.snps, \"X\")\n\n        if self._snps is None:\n            self._source.extend(source)\n            self._snps = snps\n        else:\n            common_snps = self._snps.join(snps, how=\"inner\", rsuffix=\"_added\")\n\n            discrepant_positions = common_snps.loc[\n                (common_snps[\"chrom\"] != common_snps[\"chrom_added\"])\n                | (common_snps[\"pos\"] != common_snps[\"pos_added\"])\n            ]\n\n            if 0 < len(discrepant_positions) < discrepant_snp_positions_threshold:\n                print(\n                    str(len(discrepant_positions)) + \" SNP positions were discrepant; \"\n                    \"keeping original positions\"\n                )\n\n                if save_output:\n                    self._discrepant_positions_file_count += 1\n                    lineage.save_df_as_csv(\n                        discrepant_positions,\n                        self._output_dir,\n                        self.get_var_name()\n                        + \"_discrepant_positions_\"\n                        + str(self._discrepant_positions_file_count)\n                        + \".csv\",\n                    )\n            elif len(discrepant_positions) >= discrepant_snp_positions_threshold:\n                print(\n                    \"too many SNPs differ in position; ensure same genome build is being used\"\n                )\n                return discrepant_positions, discrepant_genotypes\n\n            # remove null genotypes\n            common_snps = common_snps.loc[\n                ~common_snps[\"genotype\"].isnull()\n                & ~common_snps[\"genotype_added\"].isnull()\n            ]\n\n            # discrepant genotypes are where alleles are not equivalent (i.e., alleles are not the\n            # same and not swapped)\n            discrepant_genotypes = common_snps.loc[\n                (\n                    (common_snps[\"genotype\"].str.len() == 1)\n                    & (common_snps[\"genotype_added\"].str.len() == 1)\n                    & ~(\n                        common_snps[\"genotype\"].str[0]\n                        == common_snps[\"genotype_added\"].str[0]\n                    )\n                )\n                | (\n                    (common_snps[\"genotype\"].str.len() == 2)\n                    & (common_snps[\"genotype_added\"].str.len() == 2)\n                    & ~(\n                        (\n                            common_snps[\"genotype\"].str[0]\n                            == common_snps[\"genotype_added\"].str[0]\n                        )\n                        & (\n                            common_snps[\"genotype\"].str[1]\n                            == common_snps[\"genotype_added\"].str[1]\n                        )\n                    )\n                    & ~(\n                        (\n                            common_snps[\"genotype\"].str[0]\n                            == common_snps[\"genotype_added\"].str[1]\n                        )\n                        & (\n                            common_snps[\"genotype\"].str[1]\n                            == common_snps[\"genotype_added\"].str[0]\n                        )\n                    )\n                )\n            ]\n\n            if 0 < len(discrepant_genotypes) < discrepant_genotypes_threshold:\n                print(\n                    str(len(discrepant_genotypes)) + \" SNP genotypes were discrepant; \"\n                    \"marking those as null\"\n                )\n\n                if save_output:\n                    self._discrepant_genotypes_file_count += 1\n                    lineage.save_df_as_csv(\n                        discrepant_genotypes,\n                        self._output_dir,\n                        self.get_var_name()\n                        + \"_discrepant_genotypes_\"\n                        + str(self._discrepant_genotypes_file_count)\n                        + \".csv\",\n                    )\n            elif len(discrepant_genotypes) >= discrepant_genotypes_threshold:\n                print(\n                    \"too many SNPs differ in their genotype; ensure file is for same \"\n                    \"individual\"\n                )\n                return discrepant_positions, discrepant_genotypes\n\n            # add new SNPs\n            self._source.extend(source)\n            self._snps = self._snps.combine_first(snps)\n            self._snps.loc[discrepant_genotypes.index, \"genotype\"] = np.nan\n\n            # combine_first converts position to float64, so convert it back to int64\n            self._snps[\"pos\"] = self._snps[\"pos\"].astype(np.int64)\n\n        self._snps = sort_snps(self._snps)\n\n        return discrepant_positions, discrepant_genotypes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _double_single_alleles(df, chrom):\n        # find all single alleles of the specified chromosome\n        single_alleles = np.where(\n            (df[\"chrom\"] == chrom) & (df[\"genotype\"].str.len() == 1)\n        )[0]\n\n        # double those alleles\n        df.ix[single_alleles, \"genotype\"] = df.ix[single_alleles, \"genotype\"] * 2\n\n        return df", "response": "Double any single alleles in the specified chromosome."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef seperate_symbols(func):\n    params = []\n    vars = []\n    for symbol in func.free_symbols:\n        if not isidentifier(str(symbol)):\n            continue  # E.g. Indexed objects might print to A[i, j]\n        if isinstance(symbol, Parameter):\n            params.append(symbol)\n        elif isinstance(symbol, Idx):\n            # Idx objects are not seen as parameters or vars.\n            pass\n        elif isinstance(symbol, (MatrixExpr, Expr)):\n            vars.append(symbol)\n        else:\n            raise TypeError('model contains an unknown symbol type, {}'.format(type(symbol)))\n\n    for der in func.atoms(sympy.Derivative):\n        # Used by jacobians and hessians, where derivatives are treated as\n        # Variables. This way of writing it is purposefully discriminatory\n        # against derivatives wrt variables, since such derivatives should be\n        # performed explicitly in the case of jacs/hess, and are treated\n        # differently in the case of ODEModels.\n        if der.expr in vars and all(isinstance(s, Parameter) for s in der.variables):\n            vars.append(der)\n\n    params.sort(key=lambda symbol: symbol.name)\n    vars.sort(key=lambda symbol: symbol.name)\n    return vars, params", "response": "Seperate the symbols in symbolic function func. Return them in alphabetical order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a symbolic expression into a Python lambda function.", "response": "def sympy_to_py(func, args):\n    \"\"\"\n    Turn a symbolic expression into a Python lambda function,\n    which has the names of the variables and parameters as it's argument names.\n\n    :param func: sympy expression\n    :param args: variables and parameters in this model\n    :return: lambda function to be used for numerical evaluation of the model.\n    \"\"\"\n    # replace the derivatives with printable variables.\n    derivatives = {var: Variable(var.name) for var in args\n                   if isinstance(var, sympy.Derivative)}\n    func = func.xreplace(derivatives)\n    args = [derivatives[var] if isinstance(var, sympy.Derivative) else var\n            for var in args]\n    lambdafunc = lambdify(args, func, printer=SymfitNumPyPrinter,\n                          dummify=False)\n    # Check if the names of the lambda function are what we expect\n    signature = inspect_sig.signature(lambdafunc)\n    sig_parameters = OrderedDict(signature.parameters)\n    for arg, lambda_arg in zip(args, sig_parameters):\n        if arg.name != lambda_arg:\n            break\n    else:  # Lambdifying succesful!\n        return lambdafunc\n\n    # If we are here (very rare), then one of the lambda arg is still a Dummy.\n    # In this case we will manually handle the naming.\n    lambda_names = sig_parameters.keys()\n    arg_names = [arg.name for arg in args]\n    conversion = dict(zip(arg_names, lambda_names))\n\n    # Wrap the lambda such that arg names are translated into the correct dummy\n    # symbol names\n    @wraps(lambdafunc)\n    def wrapped_lambdafunc(*ordered_args, **kwargs):\n        converted_kwargs = {conversion[k]: v for k, v in kwargs.items()}\n        return lambdafunc(*ordered_args, **converted_kwargs)\n\n    # Update the signature of wrapped_lambdafunc to math our args\n    new_sig_parameters = OrderedDict()\n    for arg_name, dummy_name in conversion.items():\n        if arg_name == dummy_name:  # Already has the correct name\n            new_sig_parameters[arg_name] = sig_parameters[arg_name]\n        else:  # Change the dummy inspect.Parameter to the correct name\n            param = sig_parameters[dummy_name]\n            param = param.replace(name=arg_name)\n            new_sig_parameters[arg_name] = param\n\n    wrapped_lambdafunc.__signature__ = signature.replace(\n        parameters=new_sig_parameters.values()\n    )\n    return wrapped_lambdafunc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a symbolic expression to one scipy digs.", "response": "def sympy_to_scipy(func, vars, params):\n    \"\"\"\n    Convert a symbolic expression to one scipy digs. Not used by ``symfit`` any more.\n\n    :param func: sympy expression\n    :param vars: variables\n    :param params: parameters\n    :return: Scipy-style function to be used for numerical evaluation of the model.\n    \"\"\"\n    lambda_func = sympy_to_py(func, vars, params)\n    def f(x, p):\n        \"\"\"\n        Scipy style function.\n\n        :param x: list of arrays, NxM\n        :param p: tuple of parameter values.\n        \"\"\"\n        x = np.atleast_2d(x)\n        y = [x[i] for i in range(len(x))] if len(x[0]) else []\n        try:\n            ans = lambda_func(*(y + list(p)))\n        except TypeError:\n            # Possibly this is a constant function in which case it only has Parameters.\n            ans = lambda_func(*list(p))# * np.ones(x_shape)\n        return ans\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef variables(names, **kwargs):\n    return symbols(names, cls=Variable, seq=True, **kwargs)", "response": "Convenience function for creating multiple variables in a sequence. For more\n    control consider using symbols. seq = True."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef jacobian(expr, symbols):\n    jac = []\n    for symbol in symbols:\n        # Differentiate to every param\n        f = sympy.diff(expr, symbol)\n        jac.append(f)\n    return jac", "response": "Derive a symbolic expr w. r. t. each symbol in symbols. This returns a symbolic jacobian vector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef key2str(target):\n    return target.__class__((str(symbol), value) for symbol, value in target.items())", "response": "Converts key to string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef name(self):\n    base_str = 'd{}{}_'.format(self.derivative_count if\n                               self.derivative_count > 1 else '', self.expr)\n    for var, count in self.variable_count:\n        base_str += 'd{}{}'.format(var,  count if count > 1 else '')\n    return base_str", "response": "Return the name of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping for the function that returns the jacobian of a scalar function.", "response": "def resize_jac(self, func):\n        \"\"\"\n        Removes values with identical indices to fixed parameters from the\n        output of func. func has to return the jacobian of a scalar function.\n\n        :param func: Jacobian function to be wrapped. Is assumed to be the\n            jacobian of a scalar function.\n        :return: Jacobian corresponding to non-fixed parameters only.\n        \"\"\"\n        if func is None:\n            return None\n        @wraps(func)\n        def resized(*args, **kwargs):\n            out = func(*args, **kwargs)\n            # Make one dimensional, corresponding to a scalar function.\n            out = np.atleast_1d(np.squeeze(out))\n            mask = [p not in self._fixed_params for p in self.parameters]\n            return out[mask]\n        return resized"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resize_hess(self, func):\n        if func is None:\n            return None\n        @wraps(func)\n        def resized(*args, **kwargs):\n            out = func(*args, **kwargs)\n            # Make two dimensional, corresponding to a scalar function.\n            out = np.atleast_2d(np.squeeze(out))\n            mask = [p not in self._fixed_params for p in self.parameters]\n            return np.atleast_2d(out[mask, mask])\n        return resized", "response": "Resizes the Hessian of a scalar function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, **minimizer_kwargs):\n        bound_arguments = self.__signature__.bind(**minimizer_kwargs)\n        # Include default values in bound_argument object.\n        # Start from a new OrderedDict to guarantee ordering.\n        arguments = OrderedDict()\n        for param in self.__signature__.parameters.values():\n            if param.name in bound_arguments.arguments:\n                arguments[param.name] = bound_arguments.arguments[param.name]\n            else:\n                arguments[param.name] = param.default\n        bound_arguments.arguments = arguments\n\n        answers = []\n        next_guess = self.initial_guesses\n        for minimizer, kwargs in zip(self.minimizers, bound_arguments.arguments.values()):\n            minimizer.initial_guesses = next_guess\n            ans = minimizer.execute(**kwargs)\n            next_guess = list(ans.params.values())\n            answers.append(ans)\n        final = answers[-1]\n        # TODO: Compile all previous results in one, instead of just the\n        # number of function evaluations. But there's some code down the\n        # line that expects scalars.\n        final.infodict['nfev'] = sum(ans.infodict['nfev'] for ans in answers)\n        return final", "response": "Execute the chained - minimization."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a signature for execute based on the minimizers this ArcGIS requires.", "response": "def _make_signature(self):\n        \"\"\"\n        Create a signature for `execute` based on the minimizers this\n        `ChainedMinimizer` was initiated with. For the format, see the docstring\n        of :meth:`ChainedMinimizer.execute`.\n\n        :return: :class:`inspect.Signature` instance.\n        \"\"\"\n        # Create KEYWORD_ONLY arguments with the names of the minimizers.\n        name = lambda x: x.__class__.__name__\n        count = Counter(\n            [name(minimizer) for minimizer in self.minimizers]\n        ) # Count the number of each minimizer, they don't have to be unique\n\n        # Note that these are inspect_sig.Parameter's, not symfit parameters!\n        parameters = []\n        for minimizer in reversed(self.minimizers):\n            if count[name(minimizer)] == 1:\n                # No ambiguity, so use the name directly.\n                param_name = name(minimizer)\n            else:\n                # Ambiguity, so append the number of remaining minimizers\n                param_name = '{}_{}'.format(name(minimizer), count[name(minimizer)])\n            count[name(minimizer)] -= 1\n\n            parameters.append(\n                inspect_sig.Parameter(\n                    param_name,\n                    kind=inspect_sig.Parameter.KEYWORD_ONLY,\n                    default={}\n                )\n            )\n        return inspect_sig.Signature(parameters=reversed(parameters))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls the wrapped algorithm.", "response": "def execute(self, bounds=None, jacobian=None, hessian=None, constraints=None, **minimize_options):\n        \"\"\"\n        Calls the wrapped algorithm.\n\n        :param bounds: The bounds for the parameters. Usually filled by\n            :class:`~symfit.core.minimizers.BoundedMinimizer`.\n        :param jacobian: The Jacobian. Usually filled by\n            :class:`~symfit.core.minimizers.ScipyGradientMinimize`.\n        :param \\*\\*minimize_options: Further keywords to pass to\n            :func:`scipy.optimize.minimize`. Note that your `method` will\n            usually be filled by a specific subclass.\n        \"\"\"\n        ans = minimize(\n            self.objective,\n            self.initial_guesses,\n            method=self.method_name(),\n            bounds=bounds,\n            constraints=constraints,\n            jac=jacobian,\n            hess=hessian,\n            **minimize_options\n        )\n        return self._pack_output(ans)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pack_output(self, ans):\n        # Build infodic\n        infodic = {\n            'nfev': ans.nfev,\n        }\n\n        best_vals = []\n        found = iter(np.atleast_1d(ans.x))\n        for param in self.parameters:\n            if param.fixed:\n                best_vals.append(param.value)\n            else:\n                best_vals.append(next(found))\n\n        fit_results = dict(\n            model=DummyModel(params=self.parameters),\n            popt=best_vals,\n            covariance_matrix=None,\n            infodic=infodic,\n            mesg=ans.message,\n            ier=ans.nit if hasattr(ans, 'nit') else None,\n            objective_value=ans.fun,\n        )\n\n        if 'hess_inv' in ans:\n            try:\n                fit_results['hessian_inv'] = ans.hess_inv.todense()\n            except AttributeError:\n                fit_results['hessian_inv'] = ans.hess_inv\n        return FitResults(**fit_results)", "response": "Packs the output of a minimization in a\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all constraints in a scipy compatible format.", "response": "def scipy_constraints(self, constraints):\n        \"\"\"\n        Returns all constraints in a scipy compatible format.\n\n        :param constraints: List of either MinimizeModel instances (this is what\n          is provided by :class:`~symfit.core.fit.Fit`),\n          :class:`~symfit.core.fit.BaseModel`, or\n          :class:`sympy.core.relational.Relational`.\n        :return: dict of scipy compatible statements.\n        \"\"\"\n        cons = []\n        types = {  # scipy only distinguishes two types of constraint.\n            sympy.Eq: 'eq', sympy.Ge: 'ineq',\n        }\n\n        for constraint in constraints:\n            if isinstance(constraint, MinimizeModel):\n                # Typically the case when called by `Fit\n                constraint_type = constraint.model.constraint_type\n            elif hasattr(constraint, 'constraint_type'):\n                # Model object, not provided by `Fit`. Do the best we can.\n                if self.parameters != constraint.params:\n                    raise AssertionError('The constraint should accept the same'\n                                         ' parameters as used for the fit.')\n                constraint_type = constraint.constraint_type\n                constraint = MinimizeModel(constraint, data=self.objective.data)\n            elif isinstance(constraint, sympy.Rel):\n                constraint_type = constraint.__class__\n                constraint = self.objective.model.__class__.as_constraint(\n                    constraint, self.objective.model\n                )\n                constraint = MinimizeModel(constraint, data=self.objective.data)\n            else:\n                raise TypeError('Unknown type for a constraint.')\n            con = {\n                'type': types[constraint_type],\n                'fun': constraint,\n                }\n            cons.append(con)\n        cons = tuple(cons)\n        return cons"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the jacobian and hessian method for the a class.", "response": "def _get_jacobian_hessian_strategy(self):\n        \"\"\"\n        Figure out how to calculate the jacobian and hessian. Will return a\n        tuple describing how best to calculate the jacobian and hessian,\n        repectively. If None, it should be calculated using the available\n        analytical method.\n\n        :return: tuple of jacobian_method, hessian_method\n        \"\"\"\n        if self.jacobian is not None and self.hessian is None:\n            jacobian = None\n            hessian = 'cs'\n        elif self.jacobian is None and self.hessian is None:\n            jacobian = 'cs'\n            hessian = soBFGS(exception_strategy='damp_update')\n        else:\n            jacobian = None\n            hessian = None\n        return jacobian, hessian"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self, **minimize_options):\n        if 'minimizer_kwargs' not in minimize_options:\n            minimize_options['minimizer_kwargs'] = {}\n\n        if 'method' not in minimize_options['minimizer_kwargs']:\n            # If no minimizer was set by the user upon execute, use local_minimizer\n            minimize_options['minimizer_kwargs']['method'] = self.local_minimizer.method_name()\n        if 'jac' not in minimize_options['minimizer_kwargs'] and isinstance(self.local_minimizer, GradientMinimizer):\n            # Assign the jacobian\n            minimize_options['minimizer_kwargs']['jac'] = self.local_minimizer.wrapped_jacobian\n        if 'constraints' not in minimize_options['minimizer_kwargs'] and isinstance(self.local_minimizer, ConstrainedMinimizer):\n            # Assign constraints\n            minimize_options['minimizer_kwargs']['constraints'] = self.local_minimizer.wrapped_constraints\n        if 'bounds' not in minimize_options['minimizer_kwargs'] and isinstance(self.local_minimizer, BoundedMinimizer):\n            # Assign bounds\n            minimize_options['minimizer_kwargs']['bounds'] = self.local_minimizer.bounds\n\n        ans = basinhopping(\n            self.objective,\n            self.initial_guesses,\n            **minimize_options\n        )\n        return self._pack_output(ans)", "response": "Execute the basin - hopping minimization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute the fit on the current object.", "response": "def execute(self, **minpack_options):\n        \"\"\"\n        :param \\*\\*minpack_options: Any named arguments to be passed to leastsqbound\n        \"\"\"\n        popt, pcov, infodic, mesg, ier = leastsqbound(\n            self.objective,\n            # Dfun=self.jacobian,\n            x0=self.initial_guesses,\n            bounds=self.bounds,\n            full_output=True,\n            **minpack_options\n        )\n\n        fit_results = dict(\n            model=DummyModel(params=self.params),\n            popt=popt,\n            covariance_matrix=None,\n            infodic=infodic,\n            mesg=mesg,\n            ier=ier,\n            chi_squared=np.sum(infodic['fvec']**2),\n        )\n\n        return FitResults(**fit_results)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a DiracDelta expression.", "response": "def _print_DiracDelta(self, expr):\n        \"\"\"\n        Replaces a DiracDelta(x) by np.inf if x == 0, and 0 otherwise. This is\n        wrong, but the only thing we can do by the time we are printing. To\n        prevent mistakes, integrate before printing.\n        \"\"\"\n        return \"{0}({1}, [{1} == 0 , {1} != 0], [{2}, 0])\".format(\n                                        self._module_format('numpy.piecewise'),\n                                        self._print(expr.args[0]),\n                                        self._module_format('numpy.inf'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes the interactive guessing procedure.", "response": "def execute(self, **kwargs):\n        \"\"\"\n        Execute the interactive guessing procedure.\n\n        :param show: Whether or not to show the figure. Useful for testing.\n        :type show: bool\n        :param block: Blocking call to matplotlib\n        :type show: bool\n\n        Any additional keyword arguments are passed to\n        matplotlib.pyplot.show().\n        \"\"\"\n        show = kwargs.pop('show')\n        if show:\n            # self.fig.show()  # Apparently this does something else,\n            # see https://github.com/matplotlib/matplotlib/issues/6138\n            plt.show(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset up the figure and the axes for the current version of the object.", "response": "def _set_up_figure(self, x_mins, x_maxs, y_mins, y_maxs):\n        \"\"\"\n        Prepare the matplotlib figure: make all the subplots; adjust their\n        x and y range; plot the data; and plot an putative function.\n        \"\"\"\n        self.fig = plt.figure()\n\n        # Make room for the sliders:\n        bot = 0.1 + 0.05*len(self.model.params)\n        self.fig.subplots_adjust(bottom=bot)\n\n        # If these are not ints, matplotlib will crash and burn with an utterly\n        # vague error.\n        nrows = int(np.ceil(len(self._projections)**0.5))\n        ncols = int(np.ceil(len(self._projections)/nrows))\n\n        # Make all the subplots: set the x and y limits, scatter the data, and\n        # plot the putative function.\n        self._plots = {}\n\n        for plotnr, proj in enumerate(self._projections, 1):\n            x, y = proj\n            if Derivative(y, x) in self.model:\n                title_format = '$\\\\frac{{\\\\partial {dependant}}}{{\\\\partial {independant}}} = {expression}$'\n            else:\n                title_format = '${dependant}({independant}) = {expression}$'\n            plotlabel = title_format.format(\n                dependant=latex(y, mode='plain'),\n                independant=latex(x, mode='plain'),\n                expression=latex(self.model[y], mode='plain'))\n            ax = self.fig.add_subplot(ncols, nrows, plotnr,\n                                      label=plotlabel)\n            ax.set_title(ax.get_label())\n            ax.set_ylim(y_mins[y], y_maxs[y])\n            ax.set_xlim(x_mins[x], x_maxs[x])\n            ax.set_xlabel('${}$'.format(x))\n            ax.set_ylabel('${}$'.format(y))\n            self._plot_data(proj, ax)\n            plot = self._plot_model(proj, ax)\n            self._plots[proj] = plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a slider for every parameter.", "response": "def _set_up_sliders(self):\n        \"\"\"\n        Creates an slider for every parameter.\n        \"\"\"\n        i = 0.05\n        self._sliders = {}\n        for param in self.model.params:\n            if not param.fixed:\n                axbg = 'lightgoldenrodyellow'\n            else:\n                axbg = 'red'\n            # start-x, start-y, width, height\n            ax = self.fig.add_axes((0.162, i, 0.68, 0.03),\n                                   facecolor=axbg, label=param)\n            val = param.value\n            if not hasattr(param, 'min') or param.min is None:\n                minimum = 0\n            else:\n                minimum = param.min\n            if not hasattr(param, 'max') or param.max is None:\n                maximum = 2 * val\n            else:\n                maximum = param.max\n\n            slid = plt.Slider(ax, param, minimum, maximum,\n                              valinit=val, valfmt='% 5.4g')\n            self._sliders[param] = slid\n            slid.on_changed(self._update_plot)\n            i += 0.05"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _eval_model(self):\n        arguments = self._x_grid.copy()\n        arguments.update({param: param.value for param in self.model.params})\n        return self.model(**key2str(arguments))", "response": "Convenience method for evaluating the model with the current parameters\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_data(self, proj, ax):\n        x, y = proj\n        ax.scatter(self.ig.independent_data[x],\n                   self.ig.dependent_data[y], c='b')", "response": "Creates and plots a scatter plot of the original data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the model proposed for the projection proj on ax.", "response": "def plot_model(self, proj, ax):\n        \"\"\"\n        Plots the model proposed for the projection proj on ax.\n        \"\"\"\n        x, y = proj\n        y_vals = getattr(self.ig._eval_model(), y.name)\n        x_vals = self.ig._x_points[x]\n        plot, = ax.plot(x_vals, y_vals, c='red')\n        return plot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the plot of the proposed model.", "response": "def update_plot(self, indep_var, dep_var):\n        \"\"\"\n        Updates the plot of the proposed model.\n        \"\"\"\n        evaluated_model = self.ig._eval_model()\n        plot = self.ig._plots[(indep_var, dep_var)]\n        y_vals = getattr(evaluated_model, dep_var.name)\n        x_vals = self.ig._x_points[indep_var]\n        plot.set_data(x_vals, y_vals)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the model proposed for the projection proj on ax.", "response": "def plot_model(self, proj, ax):\n        \"\"\"\n        Plots the model proposed for the projection proj on ax.\n        \"\"\"\n        x, y = proj\n        evaluated_model = self.ig._eval_model()\n        y_vals = getattr(evaluated_model, y.name)\n        x_vals = self.ig._x_grid[x]\n        plot = ax.errorbar(x_vals, y_vals, xerr=0, yerr=0, c='red')\n        return plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_plot(self, indep_var, dep_var):\n        evaluated_model = self.ig._eval_model()\n        y_vals = getattr(evaluated_model, dep_var.name)\n        x_vals = self.ig._x_grid[indep_var]\n\n        x_plot_data = []\n        y_plot_data = []\n        y_plot_error = []\n        # TODO: Numpy magic\n        # We need the error interval for every plotted point, so find all\n        # the points plotted at x=x_i, and do some statistics on those.\n        # Since all the points are on a grid made by meshgrid, the error\n        # in x will alwys be 0.\n        for x_val in self.ig._x_points[indep_var]:\n            # We get away with this instead of digitize because x_vals is\n            # on a grid made with meshgrid\n            idx_mask = x_vals == x_val\n            xs = x_vals[idx_mask]\n            x_plot_data.append(xs[0])\n            ys = y_vals[idx_mask]\n            y_plot_data.append(np.mean(ys))\n            y_error = np.percentile(ys, self.ig.percentile)\n            y_plot_error.append(y_error)\n\n        x_plot_data = np.array(x_plot_data)\n        y_plot_data = np.array(y_plot_data)\n        y_plot_error = np.array(y_plot_error)\n\n        xs = np.column_stack((x_plot_data, x_plot_data))\n        yerr = y_plot_error + y_plot_data[:, np.newaxis]\n        y_segments = np.dstack((xs, yerr))\n        plot_line, caps, error_lines = self.ig._plots[(indep_var, dep_var)]\n        plot_line.set_data(x_plot_data, y_plot_data)\n        error_lines[1].set_segments(y_segments)", "response": "Updates the plot of the proposed model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Gaussian pdf.", "response": "def Gaussian(x, mu, sig):\n    \"\"\"\n    Gaussian pdf.\n    :param x: free variable.\n    :param mu: mean of the distribution.\n    :param sig: standard deviation of the distribution.\n    :return: sympy.Expr for a Gaussian pdf.\n    \"\"\"\n    return sympy.exp(-(x - mu)**2/(2*sig**2))/sympy.sqrt(2*sympy.pi*sig**2)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef BivariateGaussian(x, y, mu_x, mu_y, sig_x, sig_y, rho):\n    exponent = - 1 / (2 * (1 - rho**2))\n    exponent *= (x - mu_x)**2 / sig_x**2 + (y - mu_y)**2 / sig_y**2 \\\n                - 2 * rho * (x - mu_x) * (y - mu_y) / (sig_x * sig_y)\n    return sympy.exp(exponent) / (2 * sympy.pi * sig_x * sig_y * sympy.sqrt(1 - rho**2))", "response": "Bivariate Gaussian pdf.\n\n    :param x: :class:`symfit.core.argument.Variable`\n    :param y: :class:`symfit.core.argument.Variable`\n    :param mu_x: :class:`symfit.core.argument.Parameter` for the mean of `x`\n    :param mu_y: :class:`symfit.core.argument.Parameter` for the mean of `y`\n    :param sig_x: :class:`symfit.core.argument.Parameter` for the standard\n        deviation of `x`\n    :param sig_y: :class:`symfit.core.argument.Parameter` for the standard\n        deviation of `y`\n    :param rho: :class:`symfit.core.argument.Parameter` for the correlation\n        between `x` and `y`.\n    :return: sympy expression for a Bivariate Gaussian pdf."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a namedtuple with the given names and values as keys.", "response": "def variabletuple(typename, variables, *args, **kwargs):\n    \"\"\"\n    Create a :func:`~collections.namedtuple` using :class:`~symfit.core.argument.Variable`'s\n    whoses names will be used as `field_names`.\n\n    The main reason for using this object is the `_asdict()` method: whereas a\n    ``namedtuple`` initiates such an :class:`collections.OrderedDict` with the\n    ``field_names`` as keys, this object returns a\n    :class:`collections.OrderedDict` which immediately has the ``Variable``\n    objects as keys.\n\n    Example::\n\n        >>> x = Variable('x')\n        >>> Result = variabletuple('Result', [x])\n        >>> res = Result(5.0)\n        >>> res._asdict()\n        OrderedDict((x, 5.0))\n\n    :param typename: Name of the `variabletuple`.\n    :param variables: List of :class:`~symfit.core.argument.Variable`, to be used\n        as `field_names`\n    :param args: See :func:`~collections.namedtuple`\n    :param kwargs: See :func:`~collections.namedtuple`\n    :return: Type ``typename``\n    \"\"\"\n    def _asdict(self):\n        return OrderedDict(zip(variables, self))\n\n    field_names = [var.name for var in variables]\n    named = namedtuple(typename, field_names, *args, **kwargs)\n    named._asdict = _asdict\n    return named"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef r_squared(model, fit_result, data):\n    # First filter out the dependent vars\n    y_is = [data[var] for var in model if var in data]\n    x_is = [value for var, value in data.items() if var.name in model.__signature__.parameters]\n    y_bars = [np.mean(y_i) if y_i is not None else None for y_i in y_is]\n    f_is = model(*x_is, **fit_result.params)\n    SS_res = np.sum([np.sum((y_i - f_i)**2) for y_i, f_i in zip(y_is, f_is) if y_i is not None])\n    SS_tot = np.sum([np.sum((y_i - y_bar)**2) for y_i, y_bar in zip(y_is, y_bars) if y_i is not None])\n    return 1 - SS_res/SS_tot", "response": "Calculates the coefficient of determination R^2 for the fit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a partial derivation of a variable.", "response": "def _partial_diff(var, *params):\n    \"\"\"\n    Sympy does not handle repeated partial derivation correctly, e.g.\n    D(D(y, a), a) = D(y, a, a) but D(D(y, a), b) = 0.\n    Use this function instead to prevent evaluation to zero.\n    \"\"\"\n    if isinstance(var, sympy.Derivative):\n        return sympy.Derivative(var.expr, *(var.variables + params))\n    else:\n        return D(var, *params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a Jacobian of a symbolical model.", "response": "def jacobian_from_model(model, as_functions=False):\n    \"\"\"\n    Build a :class:`~symfit.core.fit.CallableModel` representing the Jacobian of\n    ``model``.\n\n    This function make sure the chain rule is correctly applied for\n    interdependent variables.\n\n    :param model: Any symbolical model-type.\n    :param as_functions: If `True`, the result is returned using\n        :class:`sympy.core.function.Function` where needed, e.g.\n        ``{y(x, a): a * x}`` instead of ``{y: a * x}``.\n    :return: :class:`~symfit.core.fit.CallableModel` representing the Jacobian\n        of ``model``.\n    \"\"\"\n    # Inverse dict so we can turn functions back into vars in the end\n    functions_as_vars = dict((v, k) for k, v in model.vars_as_functions.items())\n    # Create the jacobian components. The `vars` here in the model_dict are\n    # always of the type D(y, a), but the righthand-side might still contain\n    # functions instead of vars depending on the value of `as_functions`.\n    jac = {}\n    for func, expr in model.function_dict.items():\n        for param in model.params:\n            target = D(func, param)\n            dfdp = expr.diff(param)\n            if as_functions:\n                jac[_partial_subs(target, functions_as_vars)] = dfdp\n            else:\n                # Turn Function objects back into Variables.\n                dfdp = dfdp.subs(functions_as_vars, evaluate=False)\n                jac[_partial_subs(target, functions_as_vars)] = dfdp\n    # Next lines are needed for the Hessian, where the components of model still\n    # contain functions instead of vars.\n    if as_functions:\n        jac.update(model)\n    else:\n        jac.update({y: expr.subs(functions_as_vars, evaluate=False)\n                    for y, expr in model.items()})\n    jacobian_model = CallableModel(jac)\n    return jacobian_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new instance of the class with the specified constraint and model.", "response": "def as_constraint(cls, constraint, model, constraint_type=None, **init_kwargs):\n        \"\"\"\n        Initiate a Model which should serve as a constraint. Such a\n        constraint-model should be initiated with knowledge of another\n        ``BaseModel``, from which it will take its parameters::\n\n            model = Model({y: a * x + b})\n            constraint = Model.as_constraint(Eq(a, 1), model)\n\n        ``constraint.params`` will be ``[a, b]`` instead of ``[a]``.\n\n        :param constraint: An ``Expr``, a mapping or iterable of ``Expr``, or a\n            ``Relational``.\n        :param model: An instance of (a subclass of)\n            :class:`~symfit.core.fit.BaseModel`.\n        :param constraint_type: When ``constraint`` is not\n            a :class:`~sympy.core.relational.Relational`, a\n            :class:`~sympy.core.relational.Relational` has to be provided\n            explicitly.\n        :param kwargs: Any additional keyword arguments which will be passed on\n            to the init method.\n        \"\"\"\n        allowed_types = [sympy.Eq, sympy.Ge, sympy.Le]\n\n        if isinstance(constraint, Relational):\n            constraint_type = constraint.__class__\n            constraint = constraint.lhs - constraint.rhs\n\n        # Initiate the constraint model, in such a way that we take care\n        # of any dependencies\n        instance = cls.with_dependencies(constraint,\n                                         dependency_model=model,\n                                         **init_kwargs)\n\n        # Check if the constraint_type is allowed, and flip the sign if needed\n        if constraint_type not in allowed_types:\n            raise ModelError(\n                'Only constraints of the type {} are allowed. A constraint'\n                ' of type {} was provided.'.format(allowed_types,\n                                                   constraint_type)\n            )\n        elif constraint_type is sympy.Le:\n            # We change this to a Ge and flip the sign\n            instance = - instance\n            constraint_type = sympy.Ge\n\n        instance.constraint_type = constraint_type\n\n        if len(instance.dependent_vars) != 1:\n            raise ModelError('Only scalar models can be used as constraints.')\n\n        # self.params has to be a subset of model.params\n        if set(instance.params) <= set(model.params):\n            instance.params = model.params\n        else:\n            raise ModelError('The parameters of ``constraint`` have to be a '\n                             'subset of those of ``model``.')\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new instance of the class with the same parameters as the given model.", "response": "def with_dependencies(cls, model_expr, dependency_model, **init_kwargs):\n        \"\"\"\n        Initiate a model whose components depend on another model. For example::\n\n            >>> x, y, z = variables('x, y, z')\n            >>> dependency_model = Model({y: x**2})\n            >>> model_dict = {z: y**2}\n            >>> model = Model.with_dependencies(model_dict, dependency_model)\n            >>> print(model)\n            [y(x; ) = x**2,\n             z(y; ) = y**2]\n\n        :param model_expr: The ``Expr`` or mapping/iterable of ``Expr`` to be\n            turned into a model.\n        :param dependency_model: An instance of (a subclass of)\n            :class:`~symfit.core.fit.BaseModel`, which contains components on\n            which the argument ``model_expr`` depends.\n        :param init_kwargs: Any kwargs to be passed on to the standard\n            init method of this class.\n        :return: A stand-alone :class:`~symfit.core.fit.BaseModel` subclass.\n        \"\"\"\n        model = cls(model_expr, **init_kwargs)  # Initiate model instance.\n        if any(var in dependency_model for var in model.independent_vars):\n            # This model depends on the output of the dependency_model,\n            # so we need to work those components into the model_dict.\n            model_dict = model.model_dict.copy()\n            # This is the case for BaseNumericalModel's\n            connectivity_mapping = init_kwargs.get('connectivity_mapping',\n                                                   model.connectivity_mapping)\n            for var in model.independent_vars:\n                if var in dependency_model:\n                    # Add this var and all its dependencies.\n                    # Walk over all possible dependencies of this\n                    # variable until we no longer have dependencies.\n                    for symbol in dependency_model.ordered_symbols:\n                        # Not everything in ordered_symbols is a key of\n                        # model, think e.g. parameters\n                        if symbol in dependency_model:\n                            if symbol not in model_dict:\n                                model_dict[symbol] = dependency_model[symbol]\n                                connectivity_mapping[symbol] = dependency_model.connectivity_mapping[symbol]\n                        if symbol == var:\n                            break\n            # connectivity_mapping in init_kwargs has been updated if it was\n            # present, since python is pass by reference. If it wasn't present,\n            # we are dealing with a type of model that will build its own\n            # connectivity_mapping upon init.\n            model = cls(model_dict, **init_kwargs)\n        return model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the internal state of the object from a dictionary.", "response": "def _init_from_dict(self, model_dict):\n        \"\"\"\n        Initiate self from a model_dict to make sure attributes such as vars, params are available.\n\n        Creates lists of alphabetically sorted independent vars, dependent vars, sigma vars, and parameters.\n        Finally it creates a signature for this model so it can be called nicely. This signature only contains\n        independent vars and params, as one would expect.\n\n        :param model_dict: dict of (dependent_var, expression) pairs.\n        \"\"\"\n        sort_func = lambda symbol: symbol.name\n        self.model_dict = OrderedDict(sorted(model_dict.items(),\n                                             key=lambda i: sort_func(i[0])))\n        # Everything at the bottom of the toposort is independent, at the top\n        # dependent, and the rest interdependent.\n        ordered = list(toposort(self.connectivity_mapping))\n        independent = sorted(ordered.pop(0), key=sort_func)\n        self.dependent_vars = sorted(ordered.pop(-1), key=sort_func)\n        self.interdependent_vars = sorted(\n            [item for items in ordered for item in items],\n            key=sort_func\n        )\n        # `independent` contains both params and vars, needs to be separated\n        self.independent_vars = [s for s in independent if\n                                 not isinstance(s, Parameter) and not s in self]\n        self.params = [s for s in independent if isinstance(s, Parameter)]\n\n        try:\n            assert not any(isinstance(var, Parameter)\n                           for var in self.dependent_vars)\n            assert not any(isinstance(var, Parameter)\n                           for var in self.interdependent_vars)\n        except AssertionError:\n            raise ModelError('`Parameter`\\'s can not feature in the role '\n                             'of `Variable`')\n        # Make Variable object corresponding to each depedent var.\n        self.sigmas = {var: Variable(name='sigma_{}'.format(var.name))\n                       for var in self.dependent_vars}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vars_as_functions(self):\n        vars2functions = {}\n        key = lambda arg: [isinstance(arg, Parameter), str(arg)]\n        # Iterate over all symbols in this model in topological order, turning\n        # each one into a function object recursively.\n        for symbol in self.ordered_symbols:\n            if symbol in self.connectivity_mapping:\n                dependencies = self.connectivity_mapping[symbol]\n                # Replace the dependency by it's function if possible\n                dependencies = [vars2functions.get(dependency, dependency)\n                               for dependency in dependencies]\n                # sort by vars first, then params, and alphabetically within\n                # each group\n                dependencies = sorted(dependencies, key=key)\n                vars2functions[symbol] = sympy.Function(symbol.name)(*dependencies)\n        return vars2functions", "response": "Turn the keys of this model into functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of all functions replaced by the current model variables.", "response": "def function_dict(self):\n        \"\"\"\n        Equivalent to ``self.model_dict``, but with all variables replaced by\n        functions if applicable. Sorted by the evaluation order according to\n        ``self.ordered_symbols``, not alphabetical like ``self.model_dict``!\n        \"\"\"\n        func_dict = OrderedDict()\n        for var, func in self.vars_as_functions.items():\n            expr = self.model_dict[var].xreplace(self.vars_as_functions)\n            func_dict[func] = expr\n        return func_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connectivity_mapping(self):\n        connectivity = {}\n        for var, expr in self.items():\n            vars, params = seperate_symbols(expr)\n            connectivity[var] = set(vars + params)\n        return connectivity", "response": "This property returns a mapping between the variables in the a\n            graph and the parameters in the a\n            graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ordered_symbols(self):\n        key_func = lambda s: [isinstance(s, sympy.Derivative),\n                           isinstance(s, sympy.Derivative) and s.derivative_count]\n        symbols = []\n        for symbol in toposort(self.connectivity_mapping):\n            symbols.extend(sorted(symbol, key=key_func))\n\n        return symbols", "response": "Returns a list of all symbols in this model topologically sorted so that they can be evaluated in the correct order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vars(self):\n        return self.independent_vars + self.dependent_vars + [self.sigmas[var] for var in self.dependent_vars]", "response": "Returns a list of dependent independent and sigma variables in that order."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of tuples of all bounds on parameters.", "response": "def bounds(self):\n        \"\"\"\n        :return: List of tuples of all bounds on parameters.\n        \"\"\"\n        bounds = []\n        for p in self.params:\n            if p.fixed:\n                if p.value >= 0.0:\n                    bounds.append([np.nextafter(p.value, 0), p.value])\n                else:\n                    bounds.append([p.value, np.nextafter(p.value, 0)])\n            else:\n                bounds.append([p.min, p.max])\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shared_parameters(self):\n        if len(self) == 1:  # Not a vector\n            return False\n        else:\n            params_thusfar = []\n            for component in self.values():\n                vars, params = seperate_symbols(component)\n                if set(params).intersection(set(params_thusfar)):\n                    return True\n                else:\n                    params_thusfar += params\n            else:\n                return False", "response": "Returns True if parameters are shared between the vector\n            components of this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nevaluates the lambda functions of each of the components in model_dict.", "response": "def eval_components(self, *args, **kwargs):\n        \"\"\"\n        :return: evaluated lambda functions of each of the components in\n            model_dict, to be used in numerical calculation.\n        \"\"\"\n        bound_arguments = self.__signature__.bind(*args, **kwargs)\n        kwargs = bound_arguments.arguments  # Only work with kwargs\n        components = dict(zip(self, self.numerical_components))\n        # Evaluate the variables in topological order.\n        for symbol in self.ordered_symbols:\n            if symbol.name not in kwargs:\n                dependencies = self.connectivity_mapping[symbol]\n                dependencies_kwargs = {d.name: kwargs[d.name]\n                                       for d in dependencies}\n                kwargs[symbol.name] = components[symbol](**dependencies_kwargs)\n\n        return [np.atleast_1d(kwargs[var.name]) for var in self]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the finite difference of the model using the sixth order central method.", "response": "def finite_difference(self, *args, **kwargs):\n        \"\"\"\n        Calculates a numerical approximation of the Jacobian of the model using\n        the sixth order central finite difference method. Accepts a `dx`\n        keyword to tune the relative stepsize used.\n        Makes 6*n_params calls to the model.\n\n        :return: A numerical approximation of the Jacobian of the model as a\n                 list with length n_components containing numpy arrays of shape\n                 (n_params, n_datapoints)\n        \"\"\"\n        # See also: scipy.misc.derivative. It might be convinced to work, but\n        # it will make way too many function evaluations\n        dx = kwargs.pop('dx')\n        bound_arguments = self.__signature__.bind(*args, **kwargs)\n        var_vals = [bound_arguments.arguments[var.name] for var in self.independent_vars]\n        param_vals = [bound_arguments.arguments[param.name] for param in self.params]\n        param_vals = np.array(param_vals, dtype=float)\n        f = partial(self, *var_vals)\n        # See also: scipy.misc.central_diff_weights\n        factors = np.array((3/2., -3/5., 1/10.))\n        orders = np.arange(1, len(factors) + 1)\n        out = []\n        # TODO: Dark numpy magic. Needs an extra dimension in out, and a sum\n        #       over the right axis at the end.\n\n        # We can't make the output arrays yet, since we don't know the size of\n        # the components. So put a sentinel value.\n        out = None\n\n        for param_idx, param_val in enumerate(param_vals):\n            for order, factor in zip(orders, factors):\n                h = np.zeros(len(self.params))\n                # Note: stepsize (h) depends on the parameter values...\n                h[param_idx] = dx * order\n                if abs(param_val) >= 1e-7:\n                    # ...but it'd better not be (too close to) 0.\n                    h[param_idx] *= param_val\n                up = f(*(param_vals + h))\n                down = f(*(param_vals - h))\n                if out is None:\n                    # Initialize output arrays. Now that we evaluated f, we\n                    # know the size of our data.\n                    out = []\n                    # out is a list  of length Ncomponents with numpy arrays of\n                    # shape (Nparams, Ndata). Part of our misery comes from the\n                    # fact that the length of the data may be different for all\n                    # the components. Numpy doesn't like ragged arrays, so make\n                    # a list of arrays.\n                    for comp_idx in range(len(self)):\n                        try:\n                            len(up[comp_idx])\n                        except TypeError:  # output[comp_idx] is a number\n                            data_shape = (1,)\n                        else:\n                            data_shape = up[comp_idx].shape\n                        # Initialize at 0 so we can += all the contributions\n                        param_grad = np.zeros([len(self.params)] + list(data_shape), dtype=float)\n                        out.append(param_grad)\n                for comp_idx in range(len(self)):\n                    diff = up[comp_idx] - down[comp_idx]\n                    out[comp_idx][param_idx, :] += factor * diff / (2 * h[param_idx])\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eval_jacobian(self, *args, **kwargs):\n        Ans = variabletuple('Ans', self)\n        return Ans(*self.finite_difference(*args, **kwargs))", "response": "Evaluate the jacobian matrix of the function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef numerical_components(self):\n        Ans = variabletuple('Ans', self.keys())\n        # All components must feature the independent vars and params, that's\n        # the API convention. But for those components which also contain\n        # interdependence, we add those vars\n        components = []\n        for var, expr in self.items():\n            dependencies = self.connectivity_mapping[var]\n            # vars first, then params, and alphabetically within each group\n            key = lambda arg: [isinstance(arg, Parameter), str(arg)]\n            ordered = sorted(dependencies, key=key)\n            components.append(sympy_to_py(expr, ordered))\n        return Ans(*components)", "response": "Returns a lambda function that returns each of the analytical components in the model_dict to be used in numerical calculation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Jacobian of the model for all the the partial derivatives.", "response": "def jacobian(self):\n        \"\"\"\n        :return: Jacobian filled with the symbolic expressions for all the\n            partial derivatives. Partial derivatives are of the components of\n            the function with respect to the Parameter's, not the independent\n            Variable's. The return shape is a list over the models components,\n            filled with tha symbolical jacobian for that component, as a list.\n        \"\"\"\n        jac = []\n        for var, expr in self.items():\n            jac_row = []\n            for param in self.params:\n                partial_dv = D(var, param)\n                jac_row.append(self.jacobian_model[partial_dv])\n            jac.append(jac_row)\n        return jac"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate the Jacobian at the specified point.", "response": "def eval_jacobian(self, *args, **kwargs):\n        \"\"\"\n        :return: Jacobian evaluated at the specified point.\n        \"\"\"\n        eval_jac_dict = self.jacobian_model(*args, **kwargs)._asdict()\n        # Take zero for component which are not present, happens for Constraints\n        jac = [[np.broadcast_to(eval_jac_dict.get(D(var, param), 0),\n                                eval_jac_dict[var].shape)\n                for param in self.params]\n            for var in self\n        ]\n\n        # Use numpy to broadcast these arrays together and then stack them along\n        # the parameter dimension. We do not include the component direction in\n        # this, because the components can have independent shapes.\n        for idx, comp in enumerate(jac):\n            jac[idx] = np.stack(np.broadcast_arrays(*comp))\n\n        Ans = variabletuple('Ans', self.keys())\n        return Ans(*jac)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hessian(self):\n        return [[[sympy.diff(partial_dv, param) for param in self.params]\n                 for partial_dv in comp] for comp in self.jacobian]", "response": "Returns the Hessian filled with the symbolic expressions for all the\n            second order partial derivatives."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate the Hessian model and return the result as a namedtuple.", "response": "def eval_hessian(self, *args, **kwargs):\n        \"\"\"\n        :return: Hessian evaluated at the specified point.\n        \"\"\"\n        # Evaluate the hessian model and use the resulting Ans namedtuple as a\n        # dict. From this, take the relevant components.\n        eval_hess_dict = self.hessian_model(*args, **kwargs)._asdict()\n        hess = [[[np.broadcast_to(eval_hess_dict.get(D(var, p1, p2), 0),\n                                  eval_hess_dict[var].shape)\n                    for p2 in self.params]\n                for p1 in self.params]\n            for var in self\n        ]\n        # Use numpy to broadcast these arrays together and then stack them along\n        # the parameter dimension. We do not include the component direction in\n        # this, because the components can have independent shapes.\n        for idx, comp in enumerate(hess):\n            hess[idx] = np.stack(np.broadcast_arrays(*comp))\n\n        Ans = variabletuple('Ans', self.keys())\n        return Ans(*hess)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _model_sanity(model):\n        if not isinstance(model, ODEModel) and not isinstance(model, BaseNumericalModel):\n            # Such a model should probably not contain derivatives\n            for var, expr in model.items():\n                if isinstance(var, sympy.Derivative) or expr.has(sympy.Derivative):\n                    warnings.warn(RuntimeWarning(\n                        'The model contains derivatives in its definition. '\n                        'Are you sure you don\\'t mean to use `symfit.ODEModel`?'\n                    ))", "response": "Perform some basic sanity checking on the model instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread - only Property", "response": "def dependent_data(self):\n        \"\"\"\n        Read-only Property\n\n        :return: Data belonging to each dependent variable as a dict with\n                 variable names as key, data as value.\n        :rtype: collections.OrderedDict\n        \"\"\"\n        return OrderedDict((var, self.data[var])\n                           for var in self.model.dependent_vars)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading - only Property", "response": "def independent_data(self):\n        \"\"\"\n        Read-only Property\n\n        :return: Data belonging to each independent variable as a dict with\n                 variable names as key, data as value.\n        :rtype: collections.OrderedDict\n        \"\"\"\n        return OrderedDict((var, self.data[var]) for var in self.model.independent_vars)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data_shapes(self):\n        independent_shapes = []\n        for var, data in self.independent_data.items():\n            if data is not None:\n                independent_shapes.append(data.shape)\n\n        dependent_shapes = []\n        for var, data in self.dependent_data.items():\n            if data is not None:\n                dependent_shapes.append(data.shape)\n\n        return list(set(independent_shapes)), list(set(dependent_shapes))", "response": "Returns the shape of the data for the in - memory in - memory entry set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initial_guesses(self):\n        return np.array([param.value for param in self.model.params])", "response": "Returns a numpy array of initial guesses for every parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives best fit parameters, this function finds the covariance matrix. This matrix gives the (co)variance in the parameters. :param best_fit_params: ``dict`` of best fit parameters as given by .best_fit_params() :return: covariance matrix.", "response": "def covariance_matrix(self, best_fit_params):\n        \"\"\"\n        Given best fit parameters, this function finds the covariance matrix.\n        This matrix gives the (co)variance in the parameters.\n\n        :param best_fit_params: ``dict`` of best fit parameters as given by .best_fit_params()\n        :return: covariance matrix.\n        \"\"\"\n        cov_matrix = self._covariance_matrix(best_fit_params,\n                                             objective=self.objective)\n        if cov_matrix is None:\n            # If the covariance matrix could not be computed we try again by\n            # approximating the hessian with the jacobian.\n\n            # VectorLeastSquares should be turned into a LeastSquares for\n            # cov matrix calculation\n            if self.objective.__class__ is VectorLeastSquares:\n                base = LeastSquares\n            else:\n                base = self.objective.__class__\n\n            class HessApproximation(base, HessianObjectiveJacApprox):\n                \"\"\"\n                Class which impersonates ``base``, but which returns zeros\n                for the models Hessian. This will effectively result in the\n                calculation of the approximate Hessian by calculating\n                outer(J.T, J) when calling ``base.eval_hessian``.\n                \"\"\"\n\n            objective = HessApproximation(self.objective.model,\n                                          self.objective.data)\n            cov_matrix = self._covariance_matrix(best_fit_params,\n                                                 objective=objective)\n\n        return cov_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _determine_minimizer(self):\n        if self.constraints:\n            return SLSQP\n        elif any([bound is not None for pair in self.model.bounds for bound in pair]):\n            # If any bound is set\n            return LBFGSB\n        else:\n            return BFGS", "response": "Determine the most suitable minimizer by the presence of constraints or the presence of bounds or the constraints."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _init_minimizer(self, minimizer, **minimizer_options):\n\n        if isinstance(minimizer, BaseMinimizer):\n            return minimizer\n        if issubclass(minimizer, BasinHopping):\n            minimizer_options['local_minimizer'] = self._init_minimizer(\n                self._determine_minimizer()\n            )\n        if issubclass(minimizer, GradientMinimizer):\n            # If an analytical version of the Jacobian exists we should use\n            # that, otherwise we let the minimizer estimate it itself.\n            # Hence the check of jacobian_model, as this is the\n            # py function version of the analytical jacobian.\n            if hasattr(self.model, 'jacobian_model') and hasattr(self.objective, 'eval_jacobian'):\n                minimizer_options['jacobian'] = self.objective.eval_jacobian\n        if issubclass(minimizer, HessianMinimizer):\n            # If an analytical version of the Hessian exists we should use\n            # that, otherwise we let the minimizer estimate it itself.\n            # Hence the check of hessian_model, as this is the\n            # py function version of the analytical hessian.\n            if hasattr(self.model, 'hessian_model') and hasattr(self.objective, 'eval_hessian'):\n                minimizer_options['hessian'] = self.objective.eval_hessian\n\n        if issubclass(minimizer, ConstrainedMinimizer):\n            # set the constraints as MinimizeModel. The dependent vars of the\n            # constraint are set to None since their value is irrelevant.\n            constraint_objectives = []\n            for constraint in self.constraints:\n                data = self.data  # No copy, share state\n                data.update({var: None for var in constraint.dependent_vars})\n                constraint_objectives.append(MinimizeModel(constraint, data))\n            minimizer_options['constraints'] = constraint_objectives\n        return minimizer(self.objective, self.model.params, **minimizer_options)", "response": "Initializes a new minimalizer instance for the given base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_constraints(self, constraints, model):\n        con_models = []\n        for constraint in constraints:\n            if hasattr(constraint, 'constraint_type'):\n                con_models.append(constraint)\n            else:\n                if isinstance(model, BaseNumericalModel):\n                    # Numerical models need to be provided with a connectivity\n                    # mapping, so we cannot use the type of model. Instead,\n                    # use the bare minimum for an analytical model for the\n                    # constraint. ToDo: once GradientNumericalModel etc are\n                    # introduced, pick the corresponding analytical model for\n                    # the constraint.\n                    con_models.append(\n                        CallableModel.as_constraint(constraint, model)\n                    )\n                else:\n                    con_models.append(\n                        model.__class__.as_constraint(constraint, model)\n                    )\n        return con_models", "response": "Takes the user provided constraints and converts them to a list of constraints that are extended to also have the\n         parameters of model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes the fit. :param minimize_options: keyword arguments to be passed to the specified minimizer. :return: FitResults instance", "response": "def execute(self, **minimize_options):\n        \"\"\"\n        Execute the fit.\n\n        :param minimize_options: keyword arguments to be passed to the specified\n            minimizer.\n        :return: FitResults instance\n        \"\"\"\n        minimizer_ans = self.minimizer.execute(**minimize_options)\n        try: # to build covariance matrix\n            cov_matrix = minimizer_ans.covariance_matrix\n        except AttributeError:\n            cov_matrix = self.covariance_matrix(dict(zip(self.model.params, minimizer_ans._popt)))\n        else:\n            if cov_matrix is None:\n                cov_matrix = self.covariance_matrix(dict(zip(self.model.params, minimizer_ans._popt)))\n        finally:\n            minimizer_ans.covariance_matrix = cov_matrix\n        # Overwrite the DummyModel with the current model\n        minimizer_ans.model = self.model\n        minimizer_ans.gof_qualifiers['r_squared'] = r_squared(self.model, minimizer_ans, self.data)\n        return minimizer_ans"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the number of components that can be integrated by this ODEModel.", "response": "def _ncomponents(self):\n        \"\"\"\n        :return: The `numerical_components` for an ODEModel. This differs from\n            the traditional `numerical_components`, in that these component can\n            also contain dependent variables, not just the independent ones.\n\n            Each of these components does not correspond to e.g. `y(t) = ...`,\n            but to `D(y, t) = ...`. The system spanned by these component\n            therefore still needs to be integrated.\n        \"\"\"\n        return [sympy_to_py(expr, self.independent_vars + self.dependent_vars + self.params)\n                for expr in self.values()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _njacobian(self):\n        return [\n            [sympy_to_py(\n                    sympy.diff(expr, var), self.independent_vars + self.dependent_vars + self.params\n                ) for var in self.dependent_vars\n            ] for _, expr in self.items()\n        ]", "response": "Returns the numerical jacobian of the components of the ODEModel with\n            regards to the dependent variables and the fit parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate a function at a given point.", "response": "def call(self, *values, **named_values):\n    \"\"\"\n    Call an expression to evaluate it at the given point.\n\n    Future improvements: I would like if func and signature could be buffered after the\n    first call so they don't have to be recalculated for every call. However, nothing\n    can be stored on self as sympy uses __slots__ for efficiency. This means there is no\n    instance dict to put stuff in! And I'm pretty sure it's ill advised to hack into the\n    __slots__ of Expr.\n\n    However, for the moment I don't really notice a performance penalty in running tests.\n\n    p.s. In the current setup signature is not even needed since no introspection is possible\n    on the Expr before calling it anyway, which makes calculating the signature absolutely useless.\n    However, I hope that someday some monkey patching expert in shining armour comes by and finds\n    a way to store it in __signature__ upon __init__ of any ``symfit`` expr such that calling\n    inspect_sig.signature on a symbolic expression will tell you which arguments to provide.\n\n    :param self: Any subclass of sympy.Expr\n    :param values: Values for the Parameters and Variables of the Expr.\n    :param named_values: Values for the vars and params by name. ``named_values`` is\n        allowed to contain too many values, as this sometimes happens when using\n        \\*\\*fit_result.params on a submodel. The irrelevant params are simply ignored.\n    :return: The function evaluated at ``values``. The type depends entirely on the input.\n        Typically an array or a float but nothing is enforced.\n    \"\"\"\n    independent_vars, params = seperate_symbols(self)\n    # Convert to a pythonic function\n    func = sympy_to_py(self, independent_vars + params)\n\n    # Handle args and kwargs according to the allowed names.\n    parameters = [  # Note that these are inspect_sig.Parameter's, not symfit parameters!\n        inspect_sig.Parameter(arg.name, inspect_sig.Parameter.POSITIONAL_OR_KEYWORD)\n            for arg in independent_vars + params\n    ]\n\n    arg_names = [arg.name for arg in independent_vars + params]\n    relevant_named_values = {\n        name: value for name, value in named_values.items() if name in arg_names\n    }\n\n    signature = inspect_sig.Signature(parameters=parameters)\n    bound_arguments = signature.bind(*values, **relevant_named_values)\n\n    return func(**bound_arguments.arguments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef variance(self, param):\n        param_number = self.model.params.index(param)\n        try:\n            return self.covariance_matrix[param_number, param_number]\n        except TypeError:\n            # covariance_matrix can be None\n            return None", "response": "Return the variance of a given parameter instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef covariance(self, param_1, param_2):\n        param_1_number = self.model.params.index(param_1)\n        param_2_number = self.model.params.index(param_2)\n        return self.covariance_matrix[param_1_number, param_2_number]", "response": "Return the covariance between two parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _array_safe_dict_eq(one_dict, other_dict):\n        for key in one_dict:\n            try:\n                assert one_dict[key] == other_dict[key]\n            except ValueError as err:\n                # When dealing with arrays, we need to use numpy for comparison\n                if isinstance(one_dict[key], dict):\n                    assert FitResults._array_safe_dict_eq(one_dict[key], other_dict[key])\n                else:\n                    assert np.allclose(one_dict[key], other_dict[key])\n            except AssertionError:\n                return False\n        else: return True", "response": "Checks if two dicts contain arrays and returns True if they are equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _import_submodules(package_name):\n    package = sys.modules[package_name]\n    out = {}\n    for loader, name, is_pkg in pkgutil.walk_packages(package.__path__):\n        try:\n            #module = importlib.import_module('{}.{}'.format(package_name, name))\n            module = importlib.import_module('.{}'.format(name), package=package_name)\n            out[name] = module\n        except:\n            continue\n    return out", "response": "Imports all submodules of a module recursively"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a symbolic fourier series of order n.", "response": "def fourier_series(x, f, n=0):\n    \"\"\"\n    Returns a symbolic fourier series of order `n`.\n\n    :param n: Order of the fourier series.\n    :param x: Independent variable\n    :param f: Frequency of the fourier series\n    \"\"\"\n    # Make the parameter objects for all the terms\n    a0, *cos_a = parameters(','.join(['a{}'.format(i) for i in range(0, n + 1)]))\n    sin_b = parameters(','.join(['b{}'.format(i) for i in range(1, n + 1)]))\n    # Construct the series\n    series = a0 + sum(ai * cos(i * f * x) + bi * sin(i * f * x)\n                     for i, (ai, bi) in enumerate(zip(cos_a, sin_b), start=1))\n    return series"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the internal to external gradients.", "response": "def _internal2external_grad(xi, bounds):\n    \"\"\"\n    Calculate the internal (unconstrained) to external (constained)\n    parameter gradiants.\n    \"\"\"\n    grad = empty_like(xi)\n    for i, (v, bound) in enumerate(zip(xi, bounds)):\n        lower, upper = bound\n        if lower is None and upper is None:  # No constraints\n            grad[i] = 1.0\n        elif upper is None:     # only lower bound\n            grad[i] = v / sqrt(v * v + 1.)\n        elif lower is None:     # only upper bound\n            grad[i] = -v / sqrt(v * v + 1.)\n        else:   # lower and upper bounds\n            grad[i] = (upper - lower) * cos(v) / 2.\n    return grad"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _internal2external_func(bounds):\n    ls = [_internal2external_lambda(b) for b in bounds]\n\n    def convert_i2e(xi):\n        xe = empty_like(xi)\n        xe[:] = [l(p) for l, p in zip(ls, xi)]\n        return xe\n\n    return convert_i2e", "response": "Make a function which converts between internal and external parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _internal2external_lambda(bound):\n    lower, upper = bound\n\n    if lower is None and upper is None:  # no constraints\n        return lambda x: x\n    elif upper is None:     # only lower bound\n        return lambda x: lower - 1. + sqrt(x * x + 1.)\n    elif lower is None:     # only upper bound\n        return lambda x: upper + 1. - sqrt(x * x + 1.)\n    else:\n        return lambda x: lower + ((upper - lower) / 2.) * (sin(x) + 1.)", "response": "Make a lambda function which converts a single internal ( uncontrained )\n    parameter to a external ( constrained ) parameter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a function which converts between external and internal parameters.", "response": "def _external2internal_func(bounds):\n    \"\"\"\n    Make a function which converts between external (constrained) and\n    internal (unconstrained) parameters.\n    \"\"\"\n    ls = [_external2internal_lambda(b) for b in bounds]\n\n    def convert_e2i(xe):\n        xi = empty_like(xe)\n        xi[:] = [l(p) for l, p in zip(ls, xe)]\n        return xi\n\n    return convert_e2i"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eval_jacobian(self, ordered_parameters=[], **parameters):\n        evaluated_func = super(LeastSquares, self).__call__(\n            ordered_parameters, **parameters\n        )\n        evaluated_jac = super(LeastSquares, self).eval_jacobian(\n            ordered_parameters, **parameters\n        )\n\n        result = 0\n        for var, f, jac_comp in zip(self.model.dependent_vars, evaluated_func,\n                                    evaluated_jac):\n            y = self.dependent_data[var]\n            sigma_var = self.model.sigmas[var]\n            if y is not None:\n                sigma = self.sigma_data[sigma_var]\n                pre_sum = jac_comp * ((y - f) / sigma**2)[np.newaxis, ...]\n                axes = tuple(range(1, len(pre_sum.shape)))\n                result -= np.sum(pre_sum, axis=axes, keepdims=False)\n        return np.atleast_1d(np.squeeze(np.array(result)))", "response": "Evaluate the Jacobian of the object at the specified order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef eval_hessian(self, ordered_parameters=[], **parameters):\n        evaluated_func = super(LeastSquares, self).__call__(\n            ordered_parameters, **parameters\n        )\n        evaluated_jac = super(LeastSquares, self).eval_jacobian(\n            ordered_parameters, **parameters\n        )\n        evaluated_hess = super(LeastSquares, self).eval_hessian(\n            ordered_parameters, **parameters\n        )\n\n        result = 0\n        for var, f, jac_comp, hess_comp in zip(self.model.dependent_vars,\n                                               evaluated_func, evaluated_jac,\n                                               evaluated_hess):\n            y = self.dependent_data[var]\n            sigma_var = self.model.sigmas[var]\n            if y is not None:\n                sigma = self.sigma_data[sigma_var]\n                p1 = hess_comp * ((y - f) / sigma**2)[np.newaxis, np.newaxis, ...]\n                # Outer product\n                p2 = np.einsum('i...,j...->ij...', jac_comp, jac_comp)\n                p2 = p2 / sigma[np.newaxis, np.newaxis, ...]**2\n                # We sum away everything except the matrices in the axes 0 & 1.\n                axes = tuple(range(2, len(p2.shape)))\n                result += np.sum(p2 - p1, axis=axes, keepdims=False)\n        return np.atleast_2d(np.squeeze(np.array(result)))", "response": "Evaluate the Hessian of the least squares model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates the Hessian of the objective jacApprox model.", "response": "def eval_hessian(self, ordered_parameters=[], **parameters):\n        \"\"\"\n        :return: Zeros with the shape of the Hessian of the model.\n        \"\"\"\n        result = super(HessianObjectiveJacApprox, self).__call__(\n            ordered_parameters, **parameters\n        )\n        num_params = len(self.model.params)\n        return [np.broadcast_to(\n                    np.zeros_like(comp),\n                    (num_params, num_params) + comp.shape\n                ) for comp in result]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eval_jacobian(self, ordered_parameters=[], **parameters):\n        apply_func = parameters.pop('apply_func')\n        evaluated_func = super(LogLikelihood, self).__call__(\n            ordered_parameters, **parameters\n        )\n        evaluated_jac = super(LogLikelihood, self).eval_jacobian(\n            ordered_parameters, **parameters\n        )\n\n        result = []\n        for jac_comp in evaluated_jac:\n            for df in jac_comp:\n                result.append(\n                    - apply_func(\n                        df.flatten() / evaluated_func\n                    )\n                )\n        else:\n            return np.atleast_1d(np.squeeze(np.array(result)))", "response": "Evaluate the Jacobian for log - likelihood."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eval_hessian(self, ordered_parameters=[], **parameters):\n        evaluated_func = super(LogLikelihood, self).__call__(\n            ordered_parameters, **parameters\n        )\n        evaluated_jac = super(LogLikelihood, self).eval_jacobian(\n            ordered_parameters, **parameters\n        )\n        evaluated_hess = super(LogLikelihood, self).eval_hessian(\n            ordered_parameters, **parameters\n        )\n\n        result = 0\n        for f, jac_comp, hess_comp in zip(evaluated_func, evaluated_jac, evaluated_hess):\n            # Outer product\n            jac_outer_jac = np.einsum('i...,j...->ij...', jac_comp, jac_comp)\n            dd_logf = - hess_comp / f[np.newaxis, np.newaxis, ...] + \\\n                      (1 / f**2)[np.newaxis, np.newaxis, ...] * jac_outer_jac\n            # We sum away everything except the matrices in the axes 0 & 1.\n            axes = tuple(range(2, len(dd_logf.shape)))\n            result += np.sum(dd_logf, axis=axes, keepdims=False)\n        else:\n            return np.atleast_2d(np.squeeze(np.array(result)))", "response": "Evaluate the log - likelihood function and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef points_with_surrounding_gaps(points):\n    points_with_gaps = []\n    last_point = -1\n    for point in points:\n        if last_point + 1 == point:\n            pass\n        elif last_point + 2 == point:\n            points_with_gaps.append(last_point + 1)\n        else:\n            points_with_gaps.append(last_point + 1)\n            points_with_gaps.append(point - 1)\n        points_with_gaps.append(point)\n        last_point = point\n    return points_with_gaps", "response": "This function returns a list of points that are surrounded by gaps."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_fieldsets(self, request, obj=None):\n        main_fields = ('description', 'start_date', 'end_date', 'state')\n\n        if obj:\n            main_fields += ('default_alternative',)\n        else:\n            main_fields = ('name',) + main_fields\n\n        return (\n            (None, {\n                'fields': main_fields,\n            }),\n            ('Relevant Goals', {\n                'classes': ('collapse', 'hidden-relevant-goals'),\n                'fields': ('relevant_chi2_goals', 'relevant_mwu_goals'),\n            })\n        )", "response": "Returns a list of fieldsets that can be used to display the add and change."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the default alternative dropdown with appropriate choices", "response": "def get_form(self, request, obj=None, **kwargs):\n        \"\"\"\n        Add the default alternative dropdown with appropriate choices\n        \"\"\"\n        if obj:\n            if obj.alternatives:\n                choices = [(alternative, alternative) for alternative in obj.alternatives.keys()]\n            else:\n                choices = [(conf.CONTROL_GROUP, conf.CONTROL_GROUP)]\n\n            class ExperimentModelForm(forms.ModelForm):\n                default_alternative = forms.ChoiceField(choices=choices,\n                                                        initial=obj.default_alternative,\n                                                        required=False)\n            kwargs['form'] = ExperimentModelForm\n        return super(ExperimentAdmin, self).get_form(request, obj=obj, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nallow the admin user to change their assigned alternative", "response": "def set_alternative_view(self, request):\n        \"\"\"\n        Allows the admin user to change their assigned alternative\n        \"\"\"\n        if not request.user.has_perm('experiments.change_experiment'):\n            return HttpResponseForbidden()\n\n        experiment_name = request.POST.get(\"experiment\")\n        alternative_name = request.POST.get(\"alternative\")\n        if not (experiment_name and alternative_name):\n            return HttpResponseBadRequest()\n\n        participant(request).set_alternative(experiment_name, alternative_name)\n        return JsonResponse({\n            'success': True,\n            'alternative': participant(request).get_alternative(experiment_name)\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_state_view(self, request):\n        if not request.user.has_perm('experiments.change_experiment'):\n            return HttpResponseForbidden()\n\n        try:\n            state = int(request.POST.get(\"state\", \"\"))\n        except ValueError:\n            return HttpResponseBadRequest()\n\n        try:\n            experiment = Experiment.objects.get(name=request.POST.get(\"experiment\"))\n        except Experiment.DoesNotExist:\n            return HttpResponseBadRequest()\n\n        experiment.state = state\n\n        if state == 0:\n            experiment.end_date = timezone.now()\n        else:\n            experiment.end_date = None\n\n        experiment.save()\n\n        return HttpResponse()", "response": "Changes the state of the current experiment"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a node list for the given experiment.", "response": "def experiment(parser, token):\n    \"\"\"\n    Split Testing experiment tag has the following syntax :\n    \n    {% experiment <experiment_name> <alternative>  %}\n    experiment content goes here\n    {% endexperiment %}\n    \n    If the alternative name is neither 'test' nor 'control' an exception is raised\n    during rendering.\n    \"\"\"\n    try:\n        token_contents = token.split_contents()\n        experiment_name, alternative, weight, user_variable = _parse_token_contents(token_contents)\n\n        node_list = parser.parse(('endexperiment', ))\n        parser.delete_first_token()\n    except ValueError:\n        raise template.TemplateSyntaxError(\"Syntax should be like :\"\n                \"{% experiment experiment_name alternative [weight=val] [user=val] %}\")\n\n    return ExperimentNode(node_list, experiment_name, alternative, weight, user_variable)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mann_whitney(a_distribution, b_distribution, use_continuity=True):\n    MINIMUM_VALUES = 20\n\n    all_values = sorted(set(a_distribution) | set(b_distribution))\n\n    count_so_far = 0\n    a_rank_sum = 0\n    b_rank_sum = 0\n    a_count = 0\n    b_count = 0\n\n    variance_adjustment = 0\n\n    for v in all_values:\n        a_for_value = a_distribution.get(v, 0)\n        b_for_value = b_distribution.get(v, 0)\n        total_for_value = a_for_value + b_for_value\n        average_rank = count_so_far + (1 + total_for_value) / 2.0\n\n        a_rank_sum += average_rank * a_for_value\n        b_rank_sum += average_rank * b_for_value\n        a_count += a_for_value\n        b_count += b_for_value\n        count_so_far += total_for_value\n\n        variance_adjustment += total_for_value ** 3 - total_for_value\n\n    if a_count < MINIMUM_VALUES or b_count < MINIMUM_VALUES:\n        return 0, None\n\n    a_u = a_rank_sum - a_count * (a_count + 1) / 2.0\n    b_u = b_rank_sum - b_count * (b_count + 1) / 2.0\n\n    small_u = min(a_u, b_u)\n    big_u = max(a_u, b_u)\n\n    # These need adjusting for the huge number of ties we will have\n    total_count = float(a_count + b_count)\n    u_distribution_mean = a_count * b_count / 2.0\n    u_distribution_sd = (\n        (a_count * b_count / (total_count * (total_count - 1))) ** 0.5 *\n        ((total_count ** 3 - total_count - variance_adjustment) / 12.0) ** 0.5)\n\n    if u_distribution_sd == 0:\n        return small_u, None\n\n    if use_continuity:\n        # normal approximation for prob calc with continuity correction\n        z_score = abs((big_u - 0.5 - u_distribution_mean) / u_distribution_sd)\n    else:\n        # normal approximation for prob calc\n        z_score = abs((big_u - u_distribution_mean) / u_distribution_sd)\n\n    return small_u, 1 - zprob(z_score)", "response": "Mann - Whitney algorithm for the given set of sources."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the chi - squared - p value of a given matrix of child arrays.", "response": "def chi_square_p_value(matrix):\n    \"\"\"\n    Accepts a matrix (an array of arrays, where each child array represents a row)\n\n    Example from http://math.hws.edu/javamath/ryan/ChiSquare.html:\n\n    Suppose you conducted a drug trial on a group of animals and you\n    hypothesized that the animals receiving the drug would survive better than\n    those that did not receive the drug. You conduct the study and collect the\n    following data:\n\n    Ho: The survival of the animals is independent of drug treatment.\n\n    Ha: The survival of the animals is associated with drug treatment.\n\n    In that case, your matrix should be:\n    [\n     [ Survivors in Test, Dead in Test ],\n     [ Survivors in Control, Dead in Control ]\n    ]\n\n    Code adapted from http://codecomments.wordpress.com/2008/02/13/computing-chi-squared-p-value-from-contingency-table-in-python/\n    \"\"\"\n    try:\n        num_rows = len(matrix)\n        num_columns = len(matrix[0])\n    except TypeError:\n        return None\n\n    if num_rows != num_columns:\n        return None\n\n    # Sanity checking\n    if num_rows == 0:\n        return None\n    for row in matrix:\n        if len(row) != num_columns:\n            return None\n\n    row_sums = []\n    # for each row\n    for row in matrix:\n        # add up all the values in the row\n        row_sums.append(sum(row))\n\n    column_sums = []\n    # for each column i\n    for i in range(num_columns):\n        column_sum = 0.0\n        # get the i'th value from each row\n        for row in matrix:\n            column_sum += row[i]\n        column_sums.append(column_sum)\n\n    # the total sum could be calculated from either the rows or the columns\n    # coerce to float to make subsequent division generate float results\n    grand_total = float(sum(row_sums))\n\n    if grand_total <= 0:\n        return None, None\n\n    observed_test_statistic = 0.0\n    for i in range(num_rows):\n        for j in range(num_columns):\n            expected_value = (row_sums[i] / grand_total) * (column_sums[j] / grand_total) * grand_total\n            if expected_value <= 0:\n                return None, None\n            observed_value = matrix[i][j]\n            observed_test_statistic += ((observed_value - expected_value) ** 2) / expected_value\n            # See https://bitbucket.org/akoha/django-lean/issue/16/g_test-formula-is-incorrect\n            #observed_test_statistic += 2 * (observed_value*log(observed_value/expected_value))\n\n    degrees_freedom = (num_columns - 1) * (num_rows - 1)\n\n    p_value = chisqprob(observed_test_statistic, degrees_freedom)\n\n    return observed_test_statistic, p_value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenroll this user in the given experiment. Returns the user s current user s current user s current alternative if they are not already part of the experiment.", "response": "def enroll(self, experiment_name, alternatives, force_alternative=None):\n        \"\"\"\n        Enroll this user in the experiment if they are not already part of it. Returns the selected alternative\n\n        force_alternative: Optionally force a user in an alternative at enrollment time\n        \"\"\"\n        chosen_alternative = conf.CONTROL_GROUP\n\n        experiment = experiment_manager.get_experiment(experiment_name)\n\n        if experiment:\n            if experiment.is_displaying_alternatives():\n                if isinstance(alternatives, collections.Mapping):\n                    if conf.CONTROL_GROUP not in alternatives:\n                        experiment.ensure_alternative_exists(conf.CONTROL_GROUP, 1)\n                    for alternative, weight in alternatives.items():\n                        experiment.ensure_alternative_exists(alternative, weight)\n                else:\n                    alternatives_including_control = alternatives + [conf.CONTROL_GROUP]\n                    for alternative in alternatives_including_control:\n                        experiment.ensure_alternative_exists(alternative)\n\n                assigned_alternative = self._get_enrollment(experiment)\n                if assigned_alternative:\n                    chosen_alternative = assigned_alternative\n                elif experiment.is_accepting_new_users():\n                    if force_alternative:\n                        chosen_alternative = force_alternative\n                    else:\n                        chosen_alternative = experiment.random_alternative()\n                    self._set_enrollment(experiment, chosen_alternative)\n            else:\n                chosen_alternative = experiment.default_alternative\n\n        return chosen_alternative"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the alternative this user is enrolled in.", "response": "def get_alternative(self, experiment_name):\n        \"\"\"\n        Get the alternative this user is enrolled in.\n        \"\"\"\n        experiment = None\n        try:\n            # catching the KeyError instead of using .get so that the experiment is auto created if desired\n            experiment = experiment_manager[experiment_name]\n        except KeyError:\n            pass\n        if experiment:\n            if experiment.is_displaying_alternatives():\n                alternative = self._get_enrollment(experiment)\n                if alternative is not None:\n                    return alternative\n            else:\n                return experiment.default_alternative\n        return conf.CONTROL_GROUP"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef goal(self, goal_name, count=1):\n        for enrollment in self._get_all_enrollments():\n            if enrollment.experiment.is_displaying_alternatives():\n                self._experiment_goal(enrollment.experiment, enrollment.alternative, goal_name, count)", "response": "Record that this user has performed a particular goal. This will update the goal stats for all experiments that are enrolled in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nincorporate all enrollments and goals performed by the other user.", "response": "def incorporate(self, other_user):\n        \"\"\"Incorporate all enrollments and goals performed by the other user\n\n        If this user is not enrolled in a given experiment, the results for the\n        other user are incorporated. For experiments this user is already\n        enrolled in the results of the other user are discarded.\n\n        This takes a relatively large amount of time for each experiment the other\n        user is enrolled in.\"\"\"\n        for enrollment in other_user._get_all_enrollments():\n            if not self._get_enrollment(enrollment.experiment):\n                self._set_enrollment(enrollment.experiment, enrollment.alternative, enrollment.enrollment_date, enrollment.last_seen)\n                goals = self.experiment_counter.participant_goal_frequencies(enrollment.experiment, enrollment.alternative, other_user._participant_identifier())\n                for goal_name, count in goals:\n                    self.experiment_counter.increment_goal_count(enrollment.experiment, enrollment.alternative, goal_name, self._participant_identifier(), count)\n            other_user._cancel_enrollment(enrollment.experiment)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit(self):\n        for enrollment in self._get_all_enrollments():\n            if enrollment.experiment.is_displaying_alternatives():\n                # We have two different goals, VISIT_NOT_PRESENT_COUNT_GOAL and VISIT_PRESENT_COUNT_GOAL.\n                # VISIT_PRESENT_COUNT_GOAL will avoid firing on the first time we set last_seen as it is assumed that the user is\n                # on the page and therefore it would automatically trigger and be valueless.\n                # This should be used for experiments when we enroll the user as part of the pageview,\n                # alternatively we can use the NOT_PRESENT GOAL which will increment on the first pageview,\n                # this is mainly useful for notification actions when the users isn't initially present.\n\n                if not enrollment.last_seen:\n                    self._experiment_goal(enrollment.experiment, enrollment.alternative, conf.VISIT_NOT_PRESENT_COUNT_GOAL, 1)\n                    self._set_last_seen(enrollment.experiment, now())\n                elif now() - enrollment.last_seen >= timedelta(hours=conf.SESSION_LENGTH):\n                    self._experiment_goal(enrollment.experiment, enrollment.alternative, conf.VISIT_NOT_PRESENT_COUNT_GOAL, 1)\n                    self._experiment_goal(enrollment.experiment, enrollment.alternative, conf.VISIT_PRESENT_COUNT_GOAL, 1)\n                    self._set_last_seen(enrollment.experiment, now())", "response": "Record that the user has visited the site for the purposes of retention tracking"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the user is enrolled in the given experiment. Returns the selected alternative.", "response": "def is_enrolled(self, experiment_name, alternative):\n        \"\"\"Enroll this user in the experiment if they are not already part of it. Returns the selected alternative\"\"\"\n        \"\"\"Test if the user is enrolled in the supplied alternative for the given experiment.\n\n        The supplied alternative will be added to the list of possible alternatives for the\n        experiment if it is not already there. If the user is not yet enrolled in the supplied\n        experiment they will be enrolled, and an alternative chosen at random.\"\"\"\n        chosen_alternative = self.enroll(experiment_name, [alternative])\n        return alternative == chosen_alternative"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert(self):\n        lines = []\n\n        pages = self.flatten_pages(self.config['pages'])\n\n        f_exclude = mkdocs_pandoc.filters.exclude.ExcludeFilter(\n                exclude=self.exclude)\n\n        f_include = mkdocs_pandoc.filters.include.IncludeFilter(\n                base_path=self.config['docs_dir'],\n                encoding=self.encoding)\n\n        # First, do the processing that must be done on a per-file basis:\n        # Adjust header levels, insert chapter headings and adjust image paths.\n\n        f_headlevel = mkdocs_pandoc.filters.headlevels.HeadlevelFilter(pages)\n\n        for page in pages:\n            fname = os.path.join(self.config['docs_dir'], page['file'])\n            try:\n                p = codecs.open(fname, 'r', self.encoding)\n            except IOError as e:\n                raise FatalError(\"Couldn't open %s for reading: %s\" % (fname,\n                    e.strerror), 1)\n            f_chapterhead = mkdocs_pandoc.filters.chapterhead.ChapterheadFilter(\n                    headlevel=page['level'],\n                    title=page['title']\n                    )\n\n            f_image = mkdocs_pandoc.filters.images.ImageFilter(\n                    filename=page['file'],\n                    image_path=self.config['site_dir'],\n                    image_ext=self.image_ext)\n\n            lines_tmp = []\n\n            for line in p.readlines():\n                lines_tmp.append(line.rstrip())\n\n            if self.exclude:\n                lines_tmp = f_exclude.run(lines_tmp)\n\n            if self.filter_include:\n                lines_tmp = f_include.run(lines_tmp)\n\n            lines_tmp = f_headlevel.run(lines_tmp)\n            lines_tmp = f_chapterhead.run(lines_tmp)\n            lines_tmp = f_image.run(lines_tmp)\n            lines.extend(lines_tmp)\n            # Add an empty line between pages to prevent text from a previous\n            # file from butting up against headers in a subsequent file.\n            lines.append('')\n\n        # Strip anchor tags\n        if self.strip_anchors:\n            lines = mkdocs_pandoc.filters.anchors.AnchorFilter().run(lines)\n\n        # Fix cross references\n        if self.filter_xrefs:\n            lines = mkdocs_pandoc.filters.xref.XrefFilter().run(lines)\n\n        if self.filter_toc:\n            lines = mkdocs_pandoc.filters.toc.TocFilter().run(lines)\n\n        if self.filter_tables:\n            lines = mkdocs_pandoc.filters.tables.TableFilter().run(lines)\n\n        return(lines)", "response": "User - facing conversion method. Returns a list of pandoc document lines."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blocks(self, lines):\n        state = markdown.blockparser.State()\n        blocks = []\n\n        # We use three states: start, ``` and '\\n'\n        state.set('start')\n\n        # index of current block\n        currblock = 0\n\n        for line in lines:\n            line += '\\n'\n            if state.isstate('start'):\n                if line[:3] == '```':\n                    state.set('```')\n                else:\n                    state.set('\\n')\n                blocks.append('')\n                currblock = len(blocks) - 1\n            else:\n                marker = line[:3]  # Will capture either '\\n' or '```'\n                if state.isstate(marker):\n                    state.reset()\n            blocks[currblock] += line\n\n        return blocks", "response": "Groups lines into markdown blocks"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a table to grid table format", "response": "def convert_table(self, block):\n        \"\"\"\"Converts a table to grid table format\"\"\"\n        lines_orig = block.split('\\n')\n        lines_orig.pop() # Remove extra newline at end of block\n        widest_cell = [] # Will hold the width of the widest cell for each column\n        widest_word = [] # Will hold the width of the widest word for each column\n        widths = []      # Will hold the computed widths of grid table columns\n\n        rows = []   # Will hold table cells during processing\n        lines = []  # Will hold the finished table\n\n        has_border = False  # Will be set to True if this is a bordered table\n\n        width_unit = 0.0 # This number is used to divide up self.width according\n                         # to the following formula:\n                         #\n                         # self.width = width_unit * maxwidth\n                         #\n                         # Where maxwidth is the sum over all elements of\n                         # widest_cell.\n\n        # Only process tables, leave everything else untouched\n\n        if not self.test(None, block):\n            return lines_orig\n\n        if lines_orig[0].startswith('|'):\n            has_border = True\n\n        # Initialize width arrays\n\n        for i in range(0, len(self._split_row(lines_orig[0], has_border))):\n            widest_cell.append(0)\n            widest_word.append(0)\n            widths.append(0)\n\n        # Parse lines into array of cells and record width of widest cell/word\n\n        for line in lines_orig:\n            row = self._split_row(line, has_border)\n            # pad widest_cell to account for under length first row\n            for i in range(0, len(row) - len(widest_cell)):\n                widest_cell.append(0)\n                widest_word.append(0)\n                widths.append(0)\n            for i in range(0, len(row)):\n                # Record cell width\n                if len(row[i]) > widest_cell[i]:\n                    widest_cell[i] = len(row[i])\n                # Record longest word\n                words = row[i].split()\n                for word in words:\n                    # Keep URLs from throwing the word length count off too badly.\n                    match = re.match(r'\\[(.*?)\\]\\(.*?\\)', word)\n                    if match:\n                       word = match.group(1)\n\n                    if len(word) > widest_word[i]:\n                        widest_word[i] = len(word)\n            rows.append(row)\n\n        # Remove table header divider line from rows\n        rows.pop(1)\n\n        # Compute first approximation of column widths based on maximum cell width\n\n        for width in widest_cell:\n            width_unit += float(width)\n\n        width_unit = self.width / width_unit\n\n        for i in range(0, len(widest_cell)):\n            widths[i] = int(widest_cell[i] * width_unit)\n\n        # Add rounding errors to narrowest column\n        if sum(widths) < self.width:\n            widths[widths.index(min(widths))] += self.width - sum(widths)\n\n        # Attempt to correct first approximation of column widths based on\n        # words that fail to fit their cell's width (if this fails textwrap\n        # will break up long words but since it does not add hyphens this\n        # should be avoided)\n\n        for i in range(0, len(widths)):\n            if widths[i] < widest_word[i]:\n                offset = widest_word[i] - widths[i]\n                for j in range(0, len(widths)):\n                    if widths[j] - widest_word[j] >= offset:\n                        widths[j] -= offset\n                        widths[i] += offset\n                        offset = 0\n\n        lines.append(self.ruler_line(widths, linetype='-'))\n\n        # Only add header row if it contains more than just whitespace\n        if ''.join(rows[0]).strip() != '':\n                lines.extend(self.wrap_row(widths, rows[0]))\n                lines.append(self.ruler_line(widths, linetype='='))\n\n        for row in rows[1:]:\n            # Skip empty rows\n            if ''.join(row).strip() == '':\n                continue\n            lines.extend(self.wrap_row(widths, row))\n            lines.append(self.ruler_line(widths, linetype='-'))\n\n        # Append empty line after table\n        lines.append('')\n\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, lines):\n        ret = []\n\n        for block in self.blocks(lines):\n            ret.extend(self.convert_table(block))\n\n        return ret", "response": "Passes all blocks through convert_table and returns a list of lines."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a ruler line for separating rows from each other", "response": "def ruler_line(self, widths, linetype='-'):\n        \"\"\"Generates a ruler line for separating rows from each other\"\"\"\n        cells = []\n        for w in widths:\n            cells.append(linetype * (w+2))\n        return '+' + '+'.join(cells) + '+'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap a single line table row into a fixed width multi - line table.", "response": "def wrap_row(self, widths, row, width_default=None):\n        \"\"\"Wraps a single line table row into a fixed width, multi-line table.\"\"\"\n        lines = []\n        longest = 0 # longest wrapped column in row\n\n        if not width_default:\n            width_default = self.width_default\n\n        # Wrap column contents\n        for i in range(0, len(row)):\n            w=width_default # column width\n\n            # Only set column width dynamicaly for non-rogue rows\n            if i < len(widths):\n              w = widths[i]\n\n            tw = textwrap.TextWrapper(width=w, break_on_hyphens=False)\n            # Wrap and left-justify\n            row[i] = tw.wrap(textwrap.dedent(row[i]))\n            # Pad with spaces up to to fixed column width\n            for l in range(0, len(row[i])):\n                    row[i][l] += (w - len(row[i][l])) * ' '\n            if len(row[i]) > longest:\n                longest = len(row[i])\n\n        # Pad all columns to have the same number of lines\n        for i in range(0, len(row)):\n            w=width_default # column width\n\n            # Only set column width dynamicaly for non-rogue rows\n            if i < len(widths):\n                w = widths[i]\n\n            if len(row[i]) < longest:\n                for j in range(len(row[i]), longest):\n                    row[i].append(w * ' ')\n\n        for l in range(0,longest):\n            line = []\n            for c in range(len(row)):\n                line.append(row[c][l])\n            line = '| ' + ' | '.join(line) + ' |'\n            lines.append(line)\n\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, lines):\n\n        head = [('#' * self.headlevel) + ' ' + self.title, '']\n\n        head.extend(lines)\n\n        return head", "response": "Filter method for getting the head of the log entries"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, lines):\n        # Nothing to do in this case\n        if (not self.adjust_path) and (not self.image_ext):\n            return lines\n\n        ret = []\n\n        for line in lines:\n            processed = {}\n            while True:\n                alt = ''\n                img_name = ''\n\n                match = re.search(r'!\\[(.*?)\\]\\((.*?)\\)', line)\n\n                # Make sure there is in fact an image file name\n                if match:\n                    # Skip images we already processed\n                    if match.group(0) in processed:\n                        break\n                    # Skip URLs\n                    if re.match('\\w+://', match.group(2)):\n                        break\n                    alt = match.group(1)\n                    img_name = match.group(2)\n                else:\n                    break\n\n                if self.image_ext:\n                    img_name = re.sub(r'\\.\\w+$', '.' + self.image_ext, img_name)\n\n                if self.adjust_path and (self.image_path or self.filename):\n                    # explicitely specified image path takes precedence over\n                    # path relative to chapter\n                    if self.image_path and self.filename:\n                        img_name = os.path.join(\n                                os.path.abspath(self.image_path),\n                                os.path.dirname(self.filename),\n                                img_name)\n\n                    # generate image path relative to file name\n                    if self.filename and (not self.image_path):\n                        img_name = os.path.join(\n                                os.path.abspath(\n                                    os.path.dirname(self.filename)),\n                                img_name)\n                    \n                # handle Windows '\\', although this adds a small amount of unnecessary work on Unix systems\n                img_name = img_name.replace(os.path.sep, '/')\n                             \n                line = re.sub(r'!\\[(.*?)\\]\\((.*?)\\)',\n                        '![%s](%s)' % (alt, img_name), line)\n\n                # Mark this image as processed\n                processed[match.group(0)] = True\n\n            ret.append(line)\n\n        return ret", "response": "Filter method for the filter method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the export view to urls.", "response": "def get_urls(self):\n        \"\"\"\n        Add the export view to urls.\n        \"\"\"\n        urls = super(FormSubmissionAdmin, self).get_urls()\n        from django.conf.urls import url\n\n        def wrap(view):\n            def wrapper(*args, **kwargs):\n                return self.admin_site.admin_view(view)(*args, **kwargs)\n            return update_wrapper(wrapper, view)\n\n        info = self.model._meta.app_label, self.model._meta.model_name\n\n        extra_urls = [\n            url(r'^export/$', wrap(self.export_view), name='%s_%s_export' % info),\n        ]\n        return extra_urls + urls"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_view(self, request, form_url=''):\n\n        info = self.opts.app_label, self.opts.model_name\n\n        if not self.has_export_permission(request):\n            raise PermissionDenied\n\n        form = SubmissionExportForm(data=request.POST if request.method == 'POST' else None)\n\n        if form.is_valid():\n            data = form.cleaned_data\n            queryset = self.get_queryset(request) \\\n                .filter(plugin_id=data.get('form')) \\\n                .select_related('created_by', 'plugin', )\n\n            from_date, to_date = data.get('from_date'), data.get('to_date')\n            headers = data.get('headers', [])\n\n            if from_date:\n                queryset = queryset.filter(creation_date__gte=from_date)\n            if to_date:\n                queryset = queryset.filter(creation_date__lt=to_date + datetime.timedelta(days=1))\n\n            if not queryset.exists():\n                message = _('No matching %s found for the given criteria. '\n                            'Please try again.') % self.opts.verbose_name_plural\n                self.message_user(request, message, level=messages.WARNING)\n                if request.is_ajax():\n                    data = {\n                        'reloadBrowser': True,\n                        'submissionCount': 0,\n                    }\n                    return JsonResponse(data)\n                return redirect('admin:%s_%s_export' % info)\n\n            latest_submission = queryset[:1].get()\n            dataset = Dataset(title=Truncator(latest_submission.plugin.name).chars(31))\n\n            if not headers:\n                headers = [field['label'].strip() for field in latest_submission.form_data]\n                for submission in queryset:\n                    for field in submission.form_data:\n                        label = field['label'].strip()\n                        if label not in headers:\n                            headers.append(label)\n\n                if request.is_ajax():\n                    data = {\n                        'reloadBrowser': False,\n                        'submissionCount': queryset.count(),\n                        'availableHeaders': headers,\n                    }\n                    return JsonResponse(data)\n\n            headers.extend(['Submitted By', 'Submitted on', 'Sender IP', 'Referrer URL'])\n            dataset.headers = headers\n\n            def humanize(field):\n                value = field['value']\n                field_type = field['type']\n\n                if value in (None, '', [], (), {}):\n                    return None\n\n                if field_type == 'checkbox':\n                    value = yesno(bool(value), u'{0},{1}'.format(_('Yes'), _('No')))\n                if field_type == 'checkbox_multiple':\n                    value = ', '.join(list(value))\n                return value\n\n            for submission in queryset:\n                row = [None] * len(headers)\n                for field in submission.form_data:\n                    label = field['label'].strip()\n                    if label in headers:\n                        row[headers.index(label)] = humanize(field)\n\n                    row[-4] = force_text(submission.created_by or _('Unknown')) \n                    row[-3] = submission.creation_date.strftime(\n                        settings.DJANGOCMS_FORMS_DATETIME_FORMAT)\n                    row[-2] = submission.ip\n                    row[-1] = submission.referrer\n                dataset.append(row)\n\n            mimetype = {\n                'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n                'csv': 'text/csv',\n                'html': 'text/html',\n                'yaml': 'text/yaml',\n                'json': 'application/json',\n            }\n\n            file_type = data.get('file_type', 'xlsx')\n            filename = settings.DJANGOCMS_FORMS_EXPORT_FILENAME.format(\n                form_name=slugify(latest_submission.plugin.name))\n            filename = timezone.now().strftime(filename)\n            filename = '%s.%s' % (filename, file_type)\n\n            response = HttpResponse(\n                getattr(dataset, file_type), {\n                    'content_type': mimetype.get(file_type, 'application/octet-stream')\n                })\n\n            response['Content-Disposition'] = 'attachment; filename=%s' % filename\n            return response\n\n        # Wrap in all admin layout\n        fieldsets = ((None, {'fields': form.fields.keys()}),)\n        adminform = AdminForm(form, fieldsets, {}, model_admin=self)\n        media = self.media + adminform.media\n\n        context = {\n            'title': _('Export %s') % force_text(self.opts.verbose_name_plural),\n            'adminform': adminform,\n            'is_popup': (IS_POPUP_VAR in request.POST or IS_POPUP_VAR in request.GET),\n            'media': mark_safe(media),\n            'errors': AdminErrorList(form, ()),\n            'app_label': self.opts.app_label,\n        }\n        return self.render_export_form(request, context, form_url)", "response": "The export admin view for this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_export_form(self, request, context, form_url=''):\n        context.update({\n            'has_change_permission': self.has_change_permission(request),\n            'form_url': mark_safe(form_url),\n            'opts': self.opts,\n            'add': True,\n            'save_on_top': self.save_on_top,\n        })\n\n        return TemplateResponse(request, self.export_form_template, context)", "response": "Render the from submission export form."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_form_template(self):\n        form_template = self.cleaned_data.get('form_template', '')\n        if form_template:\n            try:\n                get_template(form_template)\n            except TemplateDoesNotExist:\n                msg = _('Selected Form Template does not exist.')\n                raise forms.ValidationError(msg)\n        return form_template", "response": "Check if template exists and if not raise a validation error"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntiming - delay embedding.", "response": "def _embed(x, order=3, delay=1):\n    \"\"\"Time-delay embedding.\n\n    Parameters\n    ----------\n    x : 1d-array, shape (n_times)\n        Time series\n    order : int\n        Embedding dimension (order)\n    delay : int\n        Delay.\n\n    Returns\n    -------\n    embedded : ndarray, shape (n_times - (order - 1) * delay, order)\n        Embedded time-series.\n    \"\"\"\n    N = len(x)\n    Y = np.empty((order, N - (order - 1) * delay))\n    for i in range(order):\n        Y[i] = x[i * delay:i * delay + Y.shape[1]]\n    return Y.T"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a set of vectors with given lag and dimension", "response": "def util_pattern_space(time_series, lag, dim):\n    \"\"\"Create a set of sequences with given lag and dimension\n\n    Args:\n       time_series: Vector or string of the sample data\n       lag: Lag between beginning of sequences\n       dim: Dimension (number of patterns)\n\n    Returns:\n        2D array of vectors\n    \"\"\"\n    n = len(time_series)\n\n    if lag * dim > n:\n        raise Exception('Result matrix exceeded size limit, try to change lag or dim.')\n    elif lag < 1:\n        raise Exception('Lag should be greater or equal to 1.')\n\n    pattern_space = np.empty((n - lag * (dim - 1), dim))\n    for i in range(n - lag * (dim - 1)):\n        for j in range(dim):\n            pattern_space[i][j] = time_series[i + j * lag]\n\n    return pattern_space"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting coarse - grained time series with given scale", "response": "def util_granulate_time_series(time_series, scale):\n    \"\"\"Extract coarse-grained time series\n\n    Args:\n        time_series: Time series\n        scale: Scale factor\n\n    Returns:\n        Vector of coarse-grained time series with given scale factor\n    \"\"\"\n    n = len(time_series)\n    b = int(np.fix(n / scale))\n    temp = np.reshape(time_series[0:b*scale], (b, scale))\n    cts = np.mean(temp, axis = 1)\n    return cts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shannon_entropy(time_series):\n\n    # Check if string\n    if not isinstance(time_series, str):\n        time_series = list(time_series)\n\n    # Create a frequency data\n    data_set = list(set(time_series))\n    freq_list = []\n    for entry in data_set:\n        counter = 0.\n        for i in time_series:\n            if i == entry:\n                counter += 1\n        freq_list.append(float(counter) / len(time_series))\n\n    # Shannon entropy\n    ent = 0.0\n    for freq in freq_list:\n        ent += freq * np.log2(freq)\n    ent = -ent\n    return ent", "response": "Return the Shannon Entropy of the sample data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_entropy(time_series, sample_length, tolerance = None):\n    #The code below follows the sample length convention of Ref [1] so:\n    M = sample_length - 1;\n\n    time_series = np.array(time_series)\n    if tolerance is None:\n        tolerance = 0.1*np.std(time_series)\n\n    n = len(time_series)\n\n    #Ntemp is a vector that holds the number of matches. N[k] holds matches templates of length k\n    Ntemp = np.zeros(M + 2)\n    #Templates of length 0 matches by definition:\n    Ntemp[0] = n*(n - 1) / 2\n\n\n    for i in range(n - M - 1):\n        template = time_series[i:(i+M+1)];#We have 'M+1' elements in the template\n        rem_time_series = time_series[i+1:]\n\n        searchlist = np.nonzero(np.abs(rem_time_series - template[0]) < tolerance)[0]\n\n        go = len(searchlist) > 0;\n\n        length = 1;\n\n        Ntemp[length] += len(searchlist)\n\n        while go:\n            length += 1\n            nextindxlist = searchlist + 1;\n            nextindxlist = nextindxlist[nextindxlist < n - 1 - i]#Remove candidates too close to the end\n            nextcandidates = rem_time_series[nextindxlist]\n            hitlist = np.abs(nextcandidates - template[length-1]) < tolerance\n            searchlist = nextindxlist[hitlist]\n\n            Ntemp[length] += np.sum(hitlist)\n\n            go = any(hitlist) and length < M + 1\n\n\n    sampen =  - np.log(Ntemp[1:] / Ntemp[:-1])\n    return sampen", "response": "Calculates the sample entropy of degree m of a time_series."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multiscale_entropy(time_series, sample_length, tolerance = None, maxscale = None):\n\n    if tolerance is None:\n        #we need to fix the tolerance at this level. If it remains 'None' it will be changed in call to sample_entropy()\n        tolerance = 0.1*np.std(time_series)\n    if maxscale is None:\n        maxscale = len(time_series)\n\n    mse = np.zeros(maxscale)\n\n    for i in range(maxscale):\n        temp = util_granulate_time_series(time_series, i+1)\n        mse[i] = sample_entropy(temp, sample_length, tolerance)[-1]\n    return mse", "response": "Calculate the Multiscale Entropy of the given time series considering different time - scales of the time series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the Multiscale Permutation Entropy for a set of time series.", "response": "def multiscale_permutation_entropy(time_series, m, delay, scale):\n    \"\"\"Calculate the Multiscale Permutation Entropy\n\n    Args:\n        time_series: Time series for analysis\n        m: Order of permutation entropy\n        delay: Time delay\n        scale: Scale factor\n\n    Returns:\n        Vector containing Multiscale Permutation Entropy\n\n    Reference:\n        [1] Francesco Carlo Morabito et al. Multivariate Multi-Scale Permutation Entropy for\n            Complexity Analysis of Alzheimer\u2019s Disease EEG. www.mdpi.com/1099-4300/14/7/1186\n        [2] http://www.mathworks.com/matlabcentral/fileexchange/37288-multiscale-permutation-entropy-mpe/content/MPerm.m\n    \"\"\"\n    mspe = []\n    for i in range(scale):\n        coarse_time_series = util_granulate_time_series(time_series, i + 1)\n        pe = permutation_entropy(coarse_time_series, order=m, delay=delay)\n        mspe.append(pe)\n    return mspe"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the Composite Multiscale Entropy of the given time series.", "response": "def composite_multiscale_entropy(time_series, sample_length, scale, tolerance=None):\n    \"\"\"Calculate the Composite Multiscale Entropy of the given time series.\n\n    Args:\n        time_series: Time series for analysis\n        sample_length: Number of sequential points of the time series\n        scale: Scale factor\n        tolerance: Tolerance (default = 0.1...0.2 * std(time_series))\n\n    Returns:\n        Vector containing Composite Multiscale Entropy\n\n    Reference:\n        [1] Wu, Shuen-De, et al. \"Time series analysis using\n            composite multiscale entropy.\" Entropy 15.3 (2013): 1069-1084.\n    \"\"\"\n    cmse = np.zeros((1, scale))\n\n    for i in range(scale):\n        for j in range(i):\n            tmp = util_granulate_time_series(time_series[j:], i + 1)\n            cmse[i] += sample_entropy(tmp, sample_length, tolerance) / (i + 1)\n    return cmse"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ones_comp_sum16(num1: int, num2: int) -> int:\n\n    carry = 1 << 16\n    result = num1 + num2\n    return result if result < carry else result + 1 - carry", "response": "Calculates the 1 s complement sum for 16 - bit numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checksum(source: bytes) -> int:\n    if len(source) % 2:  # if the total length is odd, padding with one octet of zeros for computing the checksum\n        source += b'\\x00'\n    sum = 0\n    for i in range(0, len(source), 2):\n        sum = ones_comp_sum16(sum, (source[i + 1] << 8) + source[i])\n    return ~sum & 0xffff", "response": "Calculates the checksum of the input bytes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_one_ping(sock: socket, dest_addr: str, icmp_id: int, seq: int, size: int):\n    try:\n        dest_addr = socket.gethostbyname(dest_addr)  # Domain name will translated into IP address, and IP address leaves unchanged.\n    except socket.gaierror as e:\n        print(\"Cannot resolve {}: Unknown host\".format(dest_addr))\n        raise errors.HostUnknown(dest_addr) from e\n    pseudo_checksum = 0  # Pseudo checksum is used to calculate the real checksum.\n    icmp_header = struct.pack(ICMP_HEADER_FORMAT, IcmpType.ECHO_REQUEST, ICMP_DEFAULT_CODE, pseudo_checksum, icmp_id, seq)\n    padding = (size - struct.calcsize(ICMP_TIME_FORMAT) - struct.calcsize(ICMP_HEADER_FORMAT)) * \"Q\"  # Using double to store current time.\n    icmp_payload = struct.pack(ICMP_TIME_FORMAT, default_timer()) + padding.encode()\n    real_checksum = checksum(icmp_header + icmp_payload)  # Calculates the checksum on the dummy header and the icmp_payload.\n    # Don't know why I need socket.htons() on real_checksum since ICMP_HEADER_FORMAT already in Network Bytes Order (big-endian)\n    icmp_header = struct.pack(ICMP_HEADER_FORMAT, IcmpType.ECHO_REQUEST, ICMP_DEFAULT_CODE, socket.htons(real_checksum), icmp_id, seq)  # Put real checksum into ICMP header.\n    packet = icmp_header + icmp_payload\n    sock.sendto(packet, (dest_addr, 0))", "response": "Sends one ping to the given destination."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef receive_one_ping(sock: socket, icmp_id: int, seq: int, timeout: int) -> float or None:\n    ip_header_slice = slice(0, struct.calcsize(IP_HEADER_FORMAT))  # [0:20]\n    icmp_header_slice = slice(ip_header_slice.stop, ip_header_slice.stop + struct.calcsize(ICMP_HEADER_FORMAT))  # [20:28]\n    ip_header_keys = ('version', 'tos', 'len', 'id', 'flags', 'ttl', 'protocol', 'checksum', 'src_addr', 'dest_addr')\n    icmp_header_keys = ('type', 'code', 'checksum', 'id', 'seq')\n    while True:\n        selected = select.select([sock], [], [], timeout)\n        if selected[0] == []:  # Timeout\n            raise errors.Timeout(timeout)\n        time_recv = default_timer()\n        recv_data, addr = sock.recvfrom(1024)\n        ip_header_raw, icmp_header_raw, icmp_payload_raw = recv_data[ip_header_slice], recv_data[icmp_header_slice], recv_data[icmp_header_slice.stop:]\n        ip_header = dict(zip(ip_header_keys, struct.unpack(IP_HEADER_FORMAT, ip_header_raw)))\n        _debug(\"IP HEADER:\", ip_header)\n        icmp_header = dict(zip(icmp_header_keys, struct.unpack(ICMP_HEADER_FORMAT, icmp_header_raw)))\n        _debug(\"ICMP HEADER:\", icmp_header)\n        if icmp_header['type'] == IcmpType.TIME_EXCEEDED:  # TIME_EXCEEDED has no icmp_id and icmp_seq. Usually they are 0.\n            if icmp_header['code'] == IcmpTimeExceededCode.TTL_EXPIRED:\n                raise errors.TimeToLiveExpired()  # Some router does not report TTL expired and then timeout shows.\n            raise errors.TimeExceeded()\n        if icmp_header['id'] == icmp_id and icmp_header['seq'] == seq:  # ECHO_REPLY should match the\n            if icmp_header['type'] == IcmpType.ECHO_REQUEST:  # filters out the ECHO_REQUEST itself.\n                _debug(\"ECHO_REQUEST filtered out.\")\n                continue\n            if icmp_header['type'] == IcmpType.ECHO_REPLY:\n                time_sent = struct.unpack(ICMP_TIME_FORMAT, icmp_payload_raw[0:struct.calcsize(ICMP_TIME_FORMAT)])[0]\n                return time_recv - time_sent", "response": "Receives a single ping from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending one ping to destination address with the given timeout.", "response": "def ping(dest_addr: str, timeout: int = 4, unit: str = \"s\", src_addr: str = None, ttl: int = 64, seq: int = 0, size: int = 56) -> float or None:\n    \"\"\"\n    Send one ping to destination address with the given timeout.\n\n    Args:\n        dest_addr: The destination address, can be an IP address or a domain name. Ex. \"192.168.1.1\"/\"example.com\"\n        timeout: Timeout in seconds. Default is 4s, same as Windows CMD. (default 4)\n        unit: The unit of returned value. \"s\" for seconds, \"ms\" for milliseconds. (default \"s\")\n        src_addr: The IP address to ping from. This is for multi-interface clients. Ex. \"192.168.1.20\". (default None)\n        ttl: The Time-To-Live of the outgoing packet. Default is 64, same as in Linux and macOS. (default 64)\n        seq: ICMP packet sequence, usually increases from 0 in the same process. (default 0)\n        size: The ICMP packet payload size in bytes. Default is 56, same as in macOS. (default 56)\n\n    Returns:\n        The delay in seconds/milliseconds or None on timeout.\n\n    Raises:\n        PingError: Any PingError will raise again if `ping3.EXCEPTIONS` is True.\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP) as sock:\n        sock.setsockopt(socket.SOL_IP, socket.IP_TTL, ttl)\n        if src_addr:\n            sock.bind((src_addr, 0))\n        icmp_id = threading.current_thread().ident % 0xFFFF\n        try:\n            send_one_ping(sock=sock, dest_addr=dest_addr, icmp_id=icmp_id, seq=seq, size=size)\n            delay = receive_one_ping(sock=sock, icmp_id=icmp_id, seq=seq, timeout=timeout)  # in seconds\n        except errors.PingError as e:\n            _debug(e)\n            if EXCEPTIONS:\n                raise e\n            return None\n        if delay is None:\n            return None\n        if unit == \"ms\":\n            delay *= 1000  # in milliseconds\n    return delay"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends pings to destination address with the given timeout and display the result.", "response": "def verbose_ping(dest_addr: str, count: int = 4, *args, **kwargs):\n    \"\"\"\n    Send pings to destination address with the given timeout and display the result.\n\n    Args:\n        dest_addr: The destination address. Ex. \"192.168.1.1\"/\"example.com\"\n        count: How many pings should be sent. Default is 4, same as Windows CMD. (default 4)\n        *args and **kwargs: And all the other arguments available in ping() except `seq`.\n\n    Returns:\n        Formatted ping results printed.\n    \"\"\"\n    timeout = kwargs.get(\"timeout\")\n    src = kwargs.get(\"src\")\n    unit = kwargs.setdefault(\"unit\", \"ms\")\n    for i in range(count):\n        output_text = \"ping '{}'\".format(dest_addr)\n        output_text += \" from '{}'\".format(src) if src else \"\"\n        output_text += \" ... \"\n        print(output_text, end=\"\")\n        delay = ping(dest_addr, seq=i, *args, **kwargs)\n        if delay is None:\n            print(\"Timeout > {}s\".format(timeout) if timeout else \"Timeout\")\n        else:\n            print(\"{value}{unit}\".format(value=int(delay), unit=unit))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the distance parameter", "response": "def distance(self, val):\n        \"\"\" set the distance parameter \"\"\"\n        tmp = 2\n        try:\n            int(val)\n            if val > 0 and val <= 2:\n                tmp = val\n        except (ValueError, TypeError):\n            pass\n        self._distance = tmp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export(self, filepath, encoding=\"utf-8\", gzipped=True):\n        data = json.dumps(self.word_frequency.dictionary, sort_keys=True)\n        write_file(filepath, encoding, gzipped, data)", "response": "Exports the word frequency list for import in the future\n\n            "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the probability of the word being the desired correct", "response": "def word_probability(self, word, total_words=None):\n        \"\"\" Calculate the probability of the `word` being the desired, correct\n            word\n\n            Args:\n                word (str): The word for which the word probability is \\\n                calculated\n                total_words (int): The total number of words to use in the \\\n                calculation; use the default for using the whole word \\\n                frequency\n            Returns:\n                float: The probability that the word is the correct word \"\"\"\n        if total_words is None:\n            total_words = self._word_frequency.total_words\n        return self._word_frequency.dictionary[word] / total_words"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef correction(self, word):\n        return max(self.candidates(word), key=self.word_probability)", "response": "Returns the most probable correct spelling for the word\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating possible spelling corrections for the provided word up to an edit distance of two if and only when needed is not found.", "response": "def candidates(self, word):\n        \"\"\" Generate possible spelling corrections for the provided word up to\n            an edit distance of two, if and only when needed\n\n            Args:\n                word (str): The word for which to calculate candidate spellings\n            Returns:\n                set: The set of words that are possible candidates \"\"\"\n        if self.known([word]):  # short-cut if word is correct already\n            return {word}\n        # get edit distance 1...\n        res = [x for x in self.edit_distance_1(word)]\n        tmp = self.known(res)\n        if tmp:\n            return tmp\n        # if still not found, use the edit distance 1 to calc edit distance 2\n        if self._distance == 2:\n            tmp = self.known([x for x in self.__edit_distance_alt(res)])\n            if tmp:\n                return tmp\n        return {word}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef known(self, words):\n        tmp = [w.lower() for w in words]\n        return set(\n            w\n            for w in tmp\n            if w in self._word_frequency.dictionary\n            or not self._check_if_should_check(w)\n        )", "response": "Returns a set of words that appear in the dictionary of words\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef edit_distance_1(self, word):\n        word = word.lower()\n        if self._check_if_should_check(word) is False:\n            return {word}\n        letters = self._word_frequency.letters\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)", "response": "Compute all strings that are one edit away from word using only\n            the letters in the corpus are used to calculate the edit distance from the word."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing all strings that are two edits away from word using only the letters in the corpus are used to calculate the edit distance between the two strings.", "response": "def edit_distance_2(self, word):\n        \"\"\" Compute all strings that are two edits away from `word` using only\n            the letters in the corpus\n\n            Args:\n                word (str): The word for which to calculate the edit distance\n            Returns:\n                set: The set of strings that are edit distance two from the \\\n                provided word \"\"\"\n        word = word.lower()\n        return [\n            e2 for e1 in self.edit_distance_1(word) for e2 in self.edit_distance_1(e1)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __edit_distance_alt(self, words):\n        words = [x.lower() for x in words]\n        return [e2 for e1 in words for e2 in self.edit_distance_1(e1)]", "response": "Compute all strings that are 1 edits away from all the words using\n            only the letters in the corpus\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the key and return the associated value or default if the key is not present", "response": "def pop(self, key, default=None):\n        \"\"\" Remove the key and return the associated value or default if not\n            found\n\n            Args:\n                key (str): The key to remove\n                default (obj): The value to return if key is not present \"\"\"\n        return self._dictionary.pop(key.lower(), default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over the words in the dictionary and yield the next word and the number of instances in the dictionary", "response": "def items(self):\n        \"\"\" Iterator over the words in the dictionary\n\n            Yields:\n                str: The next word in the dictionary\n                int: The number of instances in the dictionary\n            Note:\n                This is the same as `dict.items()` \"\"\"\n        for word in self._dictionary.keys():\n            yield word, self._dictionary[word]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload in a pre - built word frequency list from a json file.", "response": "def load_dictionary(self, filename, encoding=\"utf-8\"):\n        \"\"\" Load in a pre-built word frequency list\n\n            Args:\n                filename (str): The filepath to the json (optionally gzipped) \\\n                file to be loaded\n                encoding (str): The encoding of the dictionary \"\"\"\n        with load_file(filename, encoding) as data:\n            self._dictionary.update(json.loads(data.lower(), encoding=encoding))\n            self._update_dictionary()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload in a text file from which to generate a word frequency list", "response": "def load_text_file(self, filename, encoding=\"utf-8\", tokenizer=None):\n        \"\"\" Load in a text file from which to generate a word frequency list\n\n            Args:\n                filename (str): The filepath to the text file to be loaded\n                encoding (str): The encoding of the text file\n                tokenizer (function): The function to use to tokenize a string\n        \"\"\"\n        with load_file(filename, encoding=encoding) as data:\n            self.load_text(data, tokenizer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a text from which to generate a word frequency list", "response": "def load_text(self, text, tokenizer=None):\n        \"\"\" Load text from which to generate a word frequency list\n\n            Args:\n                text (str): The text to be loaded\n                tokenizer (function): The function to use to tokenize a string\n        \"\"\"\n        if tokenizer:\n            words = [x.lower() for x in tokenizer(text)]\n        else:\n            words = self.tokenize(text)\n\n        self._dictionary.update(words)\n        self._update_dictionary()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a list of words from which to generate a word frequency list", "response": "def load_words(self, words):\n        \"\"\" Load a list of words from which to generate a word frequency list\n\n            Args:\n                words (list): The list of words to be loaded \"\"\"\n        self._dictionary.update([word.lower() for word in words])\n        self._update_dictionary()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_words(self, words):\n        for word in words:\n            self._dictionary.pop(word.lower())\n        self._update_dictionary()", "response": "Removes a list of words from the word frequency list\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a word from the frequency list", "response": "def remove(self, word):\n        \"\"\" Remove a word from the word frequency list\n\n            Args:\n                word (str): The word to remove \"\"\"\n        self._dictionary.pop(word.lower())\n        self._update_dictionary()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_by_threshold(self, threshold=5):\n        keys = [x for x in self._dictionary.keys()]\n        for key in keys:\n            if self._dictionary[key] <= threshold:\n                self._dictionary.pop(key)\n        self._update_dictionary()", "response": "Removes all words at or below the provided threshold."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_dictionary(self):\n        self._total_words = sum(self._dictionary.values())\n        self._unique_words = len(self._dictionary.keys())\n        self._letters = set()\n        for key in self._dictionary:\n            self._letters.update(key)", "response": "Update the word frequency object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_file(filepath, encoding, gzipped, data):\n    if gzipped:\n        with gzip.open(filepath, \"wt\") as fobj:\n            fobj.write(data)\n    else:\n        with OPEN(filepath, \"w\", encoding=encoding) as fobj:\n            if sys.version_info < (3, 0):\n                data = data.decode(encoding)\n            fobj.write(data)", "response": "Writes the data to the file specified by filepath."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef preprocess(self, raw_inputs):\n        image_arrays = []\n        for raw_im in raw_inputs:\n            im = raw_im.convert('L')\n            im = im.resize(MNIST_DIM, Image.ANTIALIAS)\n            arr = np.array(im)\n            image_arrays.append(arr)\n\n        inputs = np.array(image_arrays)\n        return inputs.reshape(len(inputs),\n                              MNIST_DIM[0],\n                              MNIST_DIM[1], 1).astype('float32') / 255", "response": "Convert images into the format required by our model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize_new_session():\n    if 'image_uid_counter' in session and 'image_list' in session:\n        logger.debug('images are already being tracked')\n    else:\n        # reset image list counter for the session\n        session['image_uid_counter'] = 0\n        session['image_list'] = []\n    if 'img_input_dir' in session and 'img_output_dir' in session:\n        logger.debug('temporary image directories already exist')\n    else:\n        # make image upload directory\n        session['img_input_dir'] = mkdtemp()\n        session['img_output_dir'] = mkdtemp()", "response": "Check if necessary\n    and initialize if necessary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading images via REST interface", "response": "def images():\n    \"\"\"Upload images via REST interface\n\n    Check if file upload was successful and sanatize user input.\n\n    TODO: return file URL instead of filename\n\n    \"\"\"\n    if request.method == 'POST':\n        file_upload = request.files['file']\n        if file_upload:\n            image = dict()\n            image['filename'] = secure_filename(file_upload.filename)\n            full_path = os.path.join(session['img_input_dir'],\n                                     image['filename'])\n            file_upload.save(full_path)\n            image['uid'] = session['image_uid_counter']\n            session['image_uid_counter'] += 1\n            current_app.logger.debug('File %d is saved as %s',\n                                     image['uid'],\n                                     image['filename'])\n            session['image_list'].append(image)\n            return jsonify(ok=\"true\", file=image['filename'], uid=image['uid'])\n        return jsonify(ok=\"false\")\n    if request.method == 'GET':\n        return jsonify(images=session['image_list'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of available visualizers Responses with a JSON list of available visualizers", "response": "def visualizers():\n    \"\"\"Get a list of available visualizers\n\n    Responses with a JSON list of available visualizers\n\n    \"\"\"\n    list_of_visualizers = []\n    for visualizer in get_visualizations():\n        list_of_visualizers.append({'name': visualizer})\n    return jsonify(visualizers=list_of_visualizers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntriggers a visualization via the REST API returning the result of the visualization.", "response": "def visualize():\n    \"\"\"Trigger a visualization via the REST API\n\n    Takes a single image and generates the visualization data, returning the\n    output exactly as given by the target visualization.\n\n    \"\"\"\n\n    session['settings'] = {}\n    image_uid = request.args.get('image')\n    vis_name = request.args.get('visualizer')\n    vis = get_visualizations()[vis_name]\n    if vis.ALLOWED_SETTINGS:\n        for key in vis.ALLOWED_SETTINGS.keys():\n            if request.args.get(key) is not None:\n                session['settings'][key] = request.args.get(key)\n            else:\n                session['settings'][key] = vis.ALLOWED_SETTINGS[key][0]\n    else:\n        logger.debug('Selected Visualizer {0} has no settings.'.format(vis_name))\n    inputs = []\n    for image in session['image_list']:\n        if image['uid'] == int(image_uid):\n            full_path = os.path.join(session['img_input_dir'],\n                                     image['filename'])\n            entry = dict()\n            entry['filename'] = image['filename']\n            entry['data'] = Image.open(full_path)\n            inputs.append(entry)\n\n    vis.update_settings(session['settings'])\n    output = vis.make_visualization(\n        inputs, output_dir=session['img_output_dir'])\n    return jsonify(output[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset():\n    shutil.rmtree(session['img_input_dir'])\n    shutil.rmtree(session['img_output_dir'])\n    session.clear()\n    return jsonify(ok='true')", "response": "Delete the session and clear temporary directories\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_settings(self, settings):\n\n        def error_string(setting, setting_val):\n            return ('{val} is not an acceptable value for '\n                    'parameter {param} for visualization'\n                    '{vis}.').format(val=setting_val,\n                                     param=setting,\n                                     vis=self.__class__.__name__)\n\n        for setting in settings:\n            if settings[setting] in self.ALLOWED_SETTINGS[setting]:\n                # if the setting is allowed, set the attribute but remove\n                # invalid variable characters\n                #\n                # see:\n                #\n                # https://stackoverflow.com/questions/3303312/how-do-i-convert-a-string-to-a-valid-variable-name-in-python\n                setattr(self, '_' + re.sub('\\W|^(?=\\d)', '_', setting).lower(),\n                        settings[setting])\n            else:\n                raise ValueError(error_string(settings[setting], setting))", "response": "Update the settings of the child class with the values from the settings dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the Keras model and weights from the given data directory.", "response": "def load(self, data_dir):\n        \"\"\"Load graph and weight data.\n\n        Args:\n            data_dir (:obj:`str`): location of Keras checkpoint (`.hdf5`) files\n                and model (in `.json`) structure.  The default behavior\n                is to take the latest of each, by OS timestamp.\n        \"\"\"\n        # for tensorflow compatibility\n        K.set_learning_phase(0)\n\n        # find newest ckpt and graph files\n        try:\n            latest_ckpt = max(glob.iglob(\n                os.path.join(data_dir, '*.h*5')), key=os.path.getctime)\n            latest_ckpt_name = os.path.basename(latest_ckpt)\n            latest_ckpt_time = str(\n                datetime.fromtimestamp(os.path.getmtime(latest_ckpt)))\n        except ValueError:\n            raise FileNotFoundError('No checkpoint (.hdf5 or .h5) files '\n                                    'available at {}'.format(data_dir))\n        try:\n            latest_json = max(glob.iglob(os.path.join(data_dir, '*.json')),\n                              key=os.path.getctime)\n            with open(latest_json, 'r') as f:\n                model_json = json.loads(f.read())\n                self._model = model_from_json(model_json)\n\n            self._model.load_weights(latest_ckpt)\n        except ValueError:\n            try:\n                self._model = load_model(latest_ckpt)\n            except ValueError:\n                raise FileNotFoundError('The (.hdf5 or .h5) files available at'\n                                        '{} don\\'t have the model'\n                                        ' architecture.'\n                                        .format(latest_ckpt))\n\n        self._sess = K.get_session()\n        self._tf_predict_var = self._model.outputs[0]\n        self._tf_input_var = self._model.inputs[0]\n        self._model_name = type(self).__name__\n        self._latest_ckpt_name = latest_ckpt_name\n        self._latest_ckpt_time = latest_ckpt_time"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, data_dir, tf_input_var=None, tf_predict_var=None):\n        # find newest ckpt and meta files\n        try:\n            latest_ckpt_fn = max(\n                filter(\n                    # exclude index and meta files which may have earlier\n                    # timestamps\n                    lambda x: os.path.splitext(x)[-1].startswith('.meta') or\n                    os.path.splitext(x)[-1].startswith('.index'),\n                    glob.glob(os.path.join(data_dir, '*.ckpt*'))),\n                key=os.path.getctime)\n            latest_ckpt_time = str(\n                datetime.fromtimestamp(os.path.getmtime(latest_ckpt_fn)))\n            # remove any step info that's been appended to the extension\n            fileext_div = latest_ckpt_fn.rfind('.ckpt')\n            additional_ext = latest_ckpt_fn.rfind('.', fileext_div + 1)\n            if additional_ext < 0:\n                latest_ckpt = latest_ckpt_fn\n            else:\n                latest_ckpt = latest_ckpt_fn[:additional_ext]\n        except ValueError:\n            raise FileNotFoundError('No checkpoint (.ckpt) files '\n                                    'available at {}'.format(data_dir))\n\n        try:\n            latest_meta = max(glob.iglob(os.path.join(data_dir, '*.meta')),\n                              key=os.path.getctime)\n        except ValueError:\n            raise FileNotFoundError('No graph (.meta) files '\n                                    'available at {}'.format(data_dir))\n\n        self._sess = tf.Session()\n        self._sess.as_default()\n\n        self._saver = tf.train.import_meta_graph(latest_meta)\n        self._saver.restore(self._sess, latest_ckpt)\n\n        self._tf_input_var = self._sess.graph.get_tensor_by_name(tf_input_var)\n        self._tf_predict_var = self._sess.graph.get_tensor_by_name(\n            tf_predict_var)\n        self._model_name = type(self).__name__\n        self._latest_ckpt_name = latest_ckpt_fn\n        self._latest_ckpt_time = latest_ckpt_time", "response": "Loads the graph and weight data from the checkpoint data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_model(model_cls_path, model_cls_name, model_load_args):\n    spec = importlib.util.spec_from_file_location('active_model',\n                                                  model_cls_path)\n    model_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(model_module)\n    model_cls = getattr(model_module, model_cls_name)\n    model = model_cls()\n    if not isinstance(model, BaseModel):\n        warnings.warn(\"Loaded model '%s' at '%s' is not an instance of %r\"\n                      % (model_cls_name, model_cls_path, BaseModel))\n    model.load(**model_load_args)\n    return model", "response": "Loads the described model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive predicted class probabilites for a set of examples annotate each logit with a class name.", "response": "def decode_prob(self, class_probabilities):\n        \"\"\"Given predicted class probabilites for a set of examples, annotate\n        each logit with a class name.\n\n        By default, we name each class using its index in the logits array.\n\n        Args:\n            class_probabilities (array): Class probabilities as output by\n                `self.predict`, i.e., a numpy array of shape (num_examples,\n                num_classes).\n\n        Returns:\n            Annotated class probabilities for each input example, as a list of\n            dicts where each dict is formatted as:\n                {\n                    'index': class_index,\n                    'name': class_name,\n                    'prob': class_probability\n                }\n\n        \"\"\"\n        results = []\n        for row in class_probabilities:\n            entries = []\n            for i, prob in enumerate(row):\n                entries.append({'index': i,\n                                'name': str(i),\n                                'prob': prob})\n\n            entries = sorted(entries,\n                             key=itemgetter('prob'),\n                             reverse=True)[:self.top_probs]\n\n            for entry in entries:\n                entry['prob'] = '{:.3f}'.format(entry['prob'])\n            results.append(entries)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_visualization_classes():\n    visualization_attr = vars(import_module('picasso.visualizations'))\n    visualization_submodules = [\n        visualization_attr[x]\n        for x in visualization_attr\n        if isinstance(visualization_attr[x], ModuleType)]\n    visualization_classes = []\n    for submodule in visualization_submodules:\n        attrs = vars(submodule)\n        for attr_name in attrs:\n            attr = attrs[attr_name]\n            if (inspect.isclass(attr)\n                and issubclass(attr, BaseVisualization)\n                    and attr is not BaseVisualization):\n                visualization_classes.append(attr)\n    return visualization_classes", "response": "Import visualizations classes dynamically\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the NN model that s being analyzed from the request context.", "response": "def get_model():\n    \"\"\"Get the NN model that's being analyzed from the request context.  Put\n    the model in the request context if it is not yet there.\n\n    Returns:\n        instance of :class:`.models.model.Model` or derived\n        class\n    \"\"\"\n    if not hasattr(g, 'model'):\n        g.model = load_model(current_app.config['MODEL_CLS_PATH'],\n                             current_app.config['MODEL_CLS_NAME'],\n                             current_app.config['MODEL_LOAD_ARGS'])\n    return g.model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_visualizations():\n    if not hasattr(g, 'visualizations'):\n        g.visualizations = {}\n        for VisClass in _get_visualization_classes():\n            vis = VisClass(get_model())\n            g.visualizations[vis.__class__.__name__] = vis\n    return g.visualizations", "response": "Get the available visualizations in the request context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_app_state():\n    if not hasattr(g, 'app_state'):\n        model = get_model()\n        g.app_state = {\n            'app_title': APP_TITLE,\n            'model_name': type(model).__name__,\n            'latest_ckpt_name': model.latest_ckpt_name,\n            'latest_ckpt_time': model.latest_ckpt_time\n        }\n    return g.app_state", "response": "Get current status of application in context\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def login(\n        username: str, password: str, brand: str,\n        websession: ClientSession = None) -> API:\n    \"\"\"Log in to the API.\"\"\"\n    api = API(brand, websession)\n    await api.authenticate(username, password)\n    return api", "response": "Log in to the API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_websession(self):\n        from socket import AF_INET\n        from aiohttp import ClientTimeout, TCPConnector\n\n        _LOGGER.debug('Creating web session')\n        conn = TCPConnector(\n            family=AF_INET,\n            limit_per_host=5,\n            enable_cleanup_closed=True,\n        )\n\n        # Create session object.\n        session_timeout = ClientTimeout(connect=10)\n        self._websession = ClientSession(connector=conn,\n                                         timeout=session_timeout)\n        self._supplied_websession = False", "response": "Create a web session object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose the web session if not already closed and created by us.", "response": "async def close_websession(self):\n        \"\"\"Close web session if not already closed and created by us.\"\"\"\n        # We do not close the web session if it was provided.\n        if self._supplied_websession or self._websession is None:\n            return\n\n        _LOGGER.debug('Closing connections')\n        # Need to set _websession to none first to prevent any other task\n        # from closing it as well.\n        temp_websession = self._websession\n        self._websession = None\n        await temp_websession.close()\n        await asyncio.sleep(0)\n        _LOGGER.debug('Connections closed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def authenticate(self, username: str, password: str) -> None:\n        self._credentials = {\n            'username': username,\n            'password': password,\n        }\n\n        await self._get_security_token()", "response": "Authenticate against the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _get_security_token(self) -> None:\n        _LOGGER.debug('Requesting security token.')\n        if self._credentials is None:\n            return\n\n        # Make sure only 1 request can be sent at a time.\n        async with self._security_token_lock:\n            # Confirm there is still no security token.\n            if self._security_token is None:\n                login_resp = await self._request(\n                    'post',\n                    LOGIN_ENDPOINT,\n                    json=self._credentials,\n                    login_request=True,\n                )\n\n                return_code = int(login_resp.get('ReturnCode', 1))\n                if return_code != 0:\n                    if return_code == 203:\n                        # Invalid username or password.\n                        _LOGGER.debug('Invalid username or password')\n                        self._credentials = None\n                    raise MyQError(login_resp['ErrorMessage'])\n\n                self._security_token = login_resp['SecurityToken']", "response": "Request a security token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of all devices associated with the account.", "response": "async def get_devices(self, covers_only: bool = True) -> list:\n        \"\"\"Get a list of all devices associated with the account.\"\"\"\n        from .device import MyQDevice\n\n        _LOGGER.debug('Retrieving list of devices')\n        devices_resp = await self._request('get', DEVICE_LIST_ENDPOINT)\n        # print(json.dumps(devices_resp, indent=4))\n\n        device_list = []\n        if devices_resp is None:\n            return device_list\n\n        for device in devices_resp['Devices']:\n            if not covers_only or \\\n               device['MyQDeviceTypeName'] in SUPPORTED_DEVICE_TYPE_NAMES:\n\n                self._devices.append({\n                    'device_id': device['MyQDeviceId'],\n                    'device_info': device\n                })\n                myq_device = MyQDevice(\n                    self._devices[-1], self._brand, self)\n                device_list.append(myq_device)\n\n        # Store current device states.\n        self._store_device_states(devices_resp.get('Devices', []))\n\n        _LOGGER.debug('List of devices retrieved')\n        return device_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the aiohttp session and run the example.", "response": "async def main() -> None:\n    \"\"\"Create the aiohttp session and run the example.\"\"\"\n\n    loglevels = dict((logging.getLevelName(level), level)\n                     for level in [10, 20, 30, 40, 50])\n\n    logging.basicConfig(\n        level=loglevels[LOGLEVEL],\n        format='%(asctime)s:%(levelname)s:\\t%(name)s\\t%(message)s')\n\n    async with ClientSession() as websession:\n        try:\n            myq = await pymyq.login(\n                MYQ_ACCOUNT_EMAIL, MYQ_ACCOUNT_PASSWORD, MYQ_BRAND, websession)\n\n            devices = await myq.get_devices()\n            for idx, device in enumerate(devices):\n                print('Device #{0}: {1}'.format(idx + 1, device.name))\n                print('--------')\n                print('Brand: {0}'.format(device.brand))\n                print('Type: {0}'.format(device.type))\n                print('Serial: {0}'.format(device.serial))\n                print('Device ID: {0}'.format(device.device_id))\n                print('Parent ID: {0}'.format(device.parent_id))\n                print('Online: {0}'.format(device.available))\n                print('Unattended Open: {0}'.format(device.open_allowed))\n                print('Unattended Close: {0}'.format(device.close_allowed))\n                print()\n                print('Current State: {0}'.format(device.state))\n                if JSON_DUMP:\n                    print(json.dumps(device._device, indent=4))\n                else:\n                    if device.state != STATE_OPEN:\n                        print('Opening the device...')\n                        await device.open()\n                        print('    0 Current State: {0}'.format(device.state))\n                        for waited in range(1, 30):\n                            if device.state == STATE_OPEN:\n                                break\n                            await asyncio.sleep(1)\n                            await device.update()\n                            print('    {} Current State: {}'.format(\n                                waited, device.state))\n\n                        await asyncio.sleep(10)\n                        await device.update()\n                        print()\n                        print('Current State: {0}'.format(device.state))\n\n                    if device.state != STATE_CLOSED:\n                        print('Closing the device...')\n                        await device.close()\n                        print('    0 Current State: {0}'.format(device.state))\n                        for waited in range(1, 30):\n                            if device.state == STATE_CLOSED:\n                                break\n                            await asyncio.sleep(1)\n                            await device.update()\n                            print('    {} Current State: {}'.format(\n                                waited, device.state))\n\n                        await asyncio.sleep(10)\n                        await device.update()\n                        print()\n                        print('Current State: {0}'.format(device.state))\n        except MyQError as err:\n            print(err)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the device name.", "response": "def name(self) -> str:\n        \"\"\"Return the device name.\"\"\"\n        return next(\n            attr['Value'] for attr in self._device_json.get('Attributes', [])\n            if attr.get('AttributeDisplayName') == 'desc')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns if device is online or not.", "response": "def available(self) -> bool:\n        \"\"\"Return if device is online or not.\"\"\"\n        # Both ability to retrieve state from MyQ cloud AND device itself has\n        # to be online.\n        is_available = self.api.online and \\\n            next(\n                attr['Value'] for attr in\n                self._device_json.get('Attributes', [])\n                if attr.get('AttributeDisplayName') == 'online') == \"True\"\n\n        return is_available"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the door can be opened unattended.", "response": "def open_allowed(self) -> bool:\n        \"\"\"Door can be opened unattended.\"\"\"\n        return next(\n            attr['Value'] for attr in self._device_json.get('Attributes', [])\n            if attr.get('AttributeDisplayName') == 'isunattendedopenallowed')\\\n            == \"1\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close_allowed(self) -> bool:\n        return next(\n            attr['Value'] for attr in self._device_json.get('Attributes', [])\n            if attr.get('AttributeDisplayName') == 'isunattendedcloseallowed')\\\n            == \"1\"", "response": "Check if the door can be closed unattended."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef state(self) -> str:\n        return self._coerce_state_from_string(\n            next(\n                attr['Value'] for attr in self._device_json.get(\n                    'Attributes', [])\n                if attr.get('AttributeDisplayName') == 'doorstate'))", "response": "Return the current state of the door."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_state(self, value: str) -> None:\n        attribute = next(attr for attr in self._device['device_info'].get(\n            'Attributes', []) if attr.get(\n                'AttributeDisplayName') == 'doorstate')\n        if attribute is not None:\n            attribute['Value'] = value", "response": "Update state temporary during open or close."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _coerce_state_from_string(value: Union[int, str]) -> str:\n        try:\n            return STATE_MAP[int(value)]\n        except KeyError:\n            _LOGGER.error('Unknown state: %s', value)\n            return STATE_UNKNOWN", "response": "Return a proper state from a string input."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _set_state(self, state: int) -> bool:\n        try:\n            set_state_resp = await self.api._request(\n                'put',\n                DEVICE_SET_ENDPOINT,\n                json={\n                    'attributeName': 'desireddoorstate',\n                    'myQDeviceId': self.device_id,\n                    'AttributeValue': state,\n                })\n        except RequestError as err:\n            _LOGGER.error('%s: Setting state failed (and halting): %s',\n                          self.name, err)\n            return False\n\n        if set_state_resp is None:\n            return False\n\n        if int(set_state_resp.get('ReturnCode', 1)) != 0:\n            _LOGGER.error(\n                '%s: Error setting the device state: %s', self.name,\n                set_state_resp.get('ErrorMessage', 'Unknown Error'))\n            return False\n\n        return True", "response": "Set the state of the device."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def update(self) -> None:\n        if self.next_allowed_update is not None and \\\n                datetime.utcnow() < self.next_allowed_update:\n            return\n\n        self.next_allowed_update = None\n        await self.api._update_device_state()\n        self._device_json = self._device['device_info']", "response": "Retrieve updated device state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_transaction(cls, txid):\n\n        for api_call in cls.GET_TX_MAIN:\n            try:\n                return api_call(txid)\n            except cls.IGNORED_ERRORS:\n                pass\n\n        raise ConnectionError('All APIs are unreachable.')", "response": "Gets the full transaction details."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tx_amount(cls, txid, txindex):\n\n        for api_call in cls.GET_TX_AMOUNT_MAIN:\n            try:\n                return api_call(txid, txindex)\n            except cls.IGNORED_ERRORS:\n                pass\n\n        raise ConnectionError('All APIs are unreachable.')", "response": "Get the amount of a given transaction output."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fee(speed=FEE_SPEED_MEDIUM):\n    if speed == FEE_SPEED_FAST:\n        return DEFAULT_FEE_FAST\n    elif speed == FEE_SPEED_MEDIUM:\n        return DEFAULT_FEE_MEDIUM\n    elif speed == FEE_SPEED_SLOW:\n        return DEFAULT_FEE_SLOW\n    else:\n        raise ValueError('Invalid speed argument.')", "response": "Gets the recommended satoshi per byte fee."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches all available unspent transaction outputs.", "response": "def get_unspents(self):\n        \"\"\"Fetches all available unspent transaction outputs.\n\n        :rtype: ``list`` of :class:`~bitcash.network.meta.Unspent`\n        \"\"\"\n        self.unspents[:] = NetworkAPI.get_unspent(self.address)\n        self.balance = sum(unspent.amount for unspent in self.unspents)\n        return self.unspents"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a signed P2PKH transaction.", "response": "def create_transaction(self, outputs, fee=None, leftover=None, combine=True,\n                           message=None, unspents=None, custom_pushdata=False):  # pragma: no cover\n        \"\"\"Creates a signed P2PKH transaction.\n\n        :param outputs: A sequence of outputs you wish to send in the form\n                        ``(destination, amount, currency)``. The amount can\n                        be either an int, float, or string as long as it is\n                        a valid input to ``decimal.Decimal``. The currency\n                        must be :ref:`supported <supported currencies>`.\n        :type outputs: ``list`` of ``tuple``\n        :param fee: The number of satoshi per byte to pay to miners. By default\n                    Bitcash will poll `<https://bitcoincashfees.earn.com>`_ and use a fee\n                    that will allow your transaction to be confirmed as soon as\n                    possible.\n        :type fee: ``int``\n        :param leftover: The destination that will receive any change from the\n                         transaction. By default Bitcash will send any change to\n                         the same address you sent from.\n        :type leftover: ``str``\n        :param combine: Whether or not Bitcash should use all available UTXOs to\n                        make future transactions smaller and therefore reduce\n                        fees. By default Bitcash will consolidate UTXOs.\n        :type combine: ``bool``\n        :param message: A message to include in the transaction. This will be\n                        stored in the blockchain forever. Due to size limits,\n                        each message will be stored in chunks of 220 bytes.\n        :type message: ``str``\n        :param unspents: The UTXOs to use as the inputs. By default Bitcash will\n                         communicate with the blockchain itself.\n        :type unspents: ``list`` of :class:`~bitcash.network.meta.Unspent`\n        :returns: The signed transaction as hex.\n        :rtype: ``str``\n        \"\"\"\n\n        unspents, outputs = sanitize_tx_data(\n            unspents or self.unspents,\n            outputs,\n            fee or get_fee(),\n            leftover or self.address,\n            combine=combine,\n            message=message,\n            compressed=self.is_compressed(),\n            custom_pushdata=custom_pushdata\n        )\n\n        return create_p2pkh_transaction(self, unspents, outputs, custom_pushdata=custom_pushdata)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a P2PKH transaction using previously prepared transaction data.", "response": "def sign_transaction(self, tx_data):  # pragma: no cover\n        \"\"\"Creates a signed P2PKH transaction using previously prepared\n        transaction data.\n\n        :param tx_data: Output of :func:`~bitcash.PrivateKey.prepare_transaction`.\n        :type tx_data: ``str``\n        :returns: The signed transaction as hex.\n        :rtype: ``str``\n        \"\"\"\n        data = json.loads(tx_data)\n\n        unspents = [Unspent.from_dict(unspent) for unspent in data['unspents']]\n        outputs = data['outputs']\n\n        return create_p2pkh_transaction(self, unspents, outputs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sanitize_tx_data(unspents, outputs, fee, leftover, combine=True, message=None, compressed=True, custom_pushdata=False):\n\n    outputs = outputs.copy()\n\n    for i, output in enumerate(outputs):\n        dest, amount, currency = output\n        # LEGACYADDRESSDEPRECATION\n        # FIXME: Will be removed in an upcoming release, breaking compatibility with legacy addresses.\n        dest = cashaddress.to_cash_address(dest)\n        outputs[i] = (dest, currency_to_satoshi_cached(amount, currency))\n\n    if not unspents:\n        raise ValueError('Transactions must have at least one unspent.')\n\n    # Temporary storage so all outputs precede messages.\n    messages = []\n    total_op_return_size = 0\n\n    if message and (custom_pushdata is False):\n        try:\n            message = message.encode('utf-8')\n        except AttributeError:\n            pass # assume message is already a bytes-like object\n\n        message_chunks = chunk_data(message, MESSAGE_LIMIT)\n\n        for message in message_chunks:\n            messages.append((message, 0))\n            total_op_return_size += get_op_return_size(message, custom_pushdata=False)\n\n    elif message and (custom_pushdata is True):\n        if (len(message) >= 220):\n            # FIXME add capability for >220 bytes for custom pushdata elements\n            raise ValueError(\"Currently cannot exceed 220 bytes with custom_pushdata.\")\n        else:\n            messages.append((message, 0))\n            total_op_return_size += get_op_return_size(message, custom_pushdata=True)\n\n\n    # Include return address in fee estimate.\n    total_in = 0\n    num_outputs = len(outputs) + 1\n    sum_outputs = sum(out[1] for out in outputs)\n\n    if combine:\n        # calculated_fee is in total satoshis.\n        calculated_fee = estimate_tx_fee(len(unspents), num_outputs, fee, compressed, total_op_return_size)\n        total_out = sum_outputs + calculated_fee\n        unspents = unspents.copy()\n        total_in += sum(unspent.amount for unspent in unspents)\n\n    else:\n        unspents = sorted(unspents, key=lambda x: x.amount)\n\n        index = 0\n\n        for index, unspent in enumerate(unspents):\n            total_in += unspent.amount\n            calculated_fee = estimate_tx_fee(len(unspents[:index + 1]), num_outputs, fee, compressed, total_op_return_size)\n            total_out = sum_outputs + calculated_fee\n\n            if total_in >= total_out:\n                break\n\n        unspents[:] = unspents[:index + 1]\n\n    remaining = total_in - total_out\n\n    if remaining > 0:\n        outputs.append((leftover, remaining))\n    elif remaining < 0:\n        raise InsufficientFunds('Balance {} is less than {} (including '\n                                'fee).'.format(total_in, total_out))\n\n    outputs.extend(messages)\n\n    return unspents, outputs", "response": "Sanitize the data for a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_source_files( self ):\n        from pybfd.gen_supported_disasm import get_supported_architectures, \\\n                                               get_supported_machines, \\\n                                               generate_supported_architectures_source, \\\n                                               generate_supported_disassembler_header, \\\n                                               gen_supported_archs\n\n        #\n        # Step 1 . Get the patch to libopcodes and nm utility for further\n        # usage.\n        #\n        libs_dirs = [os.path.dirname(lib) for lib in self.libs]\n\n        libopcodes = [lib for lib in self.libs if os.path.basename(lib).startswith(\"libopcodes\")][0]\n        print \"[+] Detecting libbfd/libopcodes compiled architectures\"\n\n        if self.with_static_binutils: # use the nm from the binutils distro\n\n            nms = [\n                os.path.join( libs_dir, \"..\", \"bin\", \"nm\" ), # default name of nm\n                os.path.join( libs_dir, \"..\", \"bin\", \"gnm\" ) # in OSX brew install binutils's nm as gnm.\n            ]\n            path_to_nm = None\n            for nm_fullpath in nms:\n                if os.path.isfile( nm_fullpath ):\n                    path_to_nm = nm_fullpath\n                    break\n            if path_to_nm == None:\n                raise Exception(\"no suitable 'nm' found.\")\n        else:\n            path_to_nm = \"nm\"  # Use the nm in the $PATH (TODO: its assume that nm exists)\n        #\n        # Step 2 .\n        #\n        # Prepare the libs to be used as option of the compiler.\n\n        path_to_bfd_header = os.path.join( self.includes, \"bfd.h\")\n        supported_machines = get_supported_machines(path_to_bfd_header)\n\n        supported_archs = get_supported_architectures(\n            path_to_nm,\n            libopcodes,\n            supported_machines,\n            self.with_static_binutils == None)\n\n        source_bfd_archs_c = generate_supported_architectures_source(supported_archs, supported_machines)\n        print \"[+] Generating .C files...\"\n        gen_file = os.path.join(PACKAGE_DIR, \"gen_bfd_archs.c\")\n        with open(gen_file, \"w+\") as fd:\n            fd.write(source_bfd_archs_c)\n        print \"[+]   %s\" % gen_file\n\n        if self.with_static_binutils:\n           link_to_libs = [] # ...\n        else:\n           link_to_libs = [self.prepare_libs_for_cc(os.path.basename(lib)) for lib in self.libs]\n\n        c_compiler = new_compiler()\n        objects = c_compiler.compile(\n            [os.path.join(PACKAGE_DIR, \"gen_bfd_archs.c\"), ],\n            include_dirs = [self.includes,]\n            )\n        program = c_compiler.link_executable(\n            objects,\n            libraries = link_to_libs,\n            library_dirs = libs_dirs,\n            output_progname = \"gen_bfd_archs\",\n            output_dir = PACKAGE_DIR\n        )\n        gen_tool = os.path.join(PACKAGE_DIR, \"gen_bfd_archs\")\n        gen_file = os.path.join(self.build_lib, PACKAGE_DIR, \"bfd_archs.py\")\n        cmd = \"%s > %s\" % (\n                    gen_tool,\n                    gen_file  )\n\n        print \"[+] Generating .py files...\"\n        # generate C dependent definitions\n        os.system( cmd )\n        # generate python specific data\n        with open(gen_file, \"a\") as f:\n            f.write( gen_supported_archs(supported_archs) )\n\n        # Remove unused files.\n        for obj in objects:\n            os.unlink(obj)\n        os.unlink(gen_tool)\n\n        print \"[+]   %s\" % gen_file\n\n        #\n        # Step 3 . Generate header file to be used by the PyBFD extension\n        #           modules bfd.c and opcodes.c.\n        #\n        gen_source = generate_supported_disassembler_header(supported_archs)\n\n        if len(supported_archs) == 0:\n            raise Exception(\"Unable to determine libopcodes' supported \" \\\n                \"platforms from '%s'\" % libopcodes)\n\n        print \"[+] Generating .h files...\"\n        gen_file = os.path.join(PACKAGE_DIR, \"supported_disasm.h\")\n        with open(gen_file, \"w+\") as fd:\n            fd.write(gen_source)\n        print \"[+]   %s\" % gen_file\n\n        return supported_archs", "response": "Generate source files for the current version of the extension module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd Mac OS X support.", "response": "def _darwin_current_arch(self):\n        \"\"\"Add Mac OS X support.\"\"\"\n        if sys.platform == \"darwin\":\n            if sys.maxsize > 2 ** 32: # 64bits.\n                return platform.mac_ver()[2] # Both Darwin and Python are 64bits.\n            else: # Python 32 bits\n                return platform.processor()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompiles the python extension module for further installation.", "response": "def build_extensions(self):\n        \"\"\"Compile the python extension module for further installation.\"\"\"\n        global final_supported_archs\n\n        ext_extra_objects = []\n        ext_libs = []\n        ext_libs_dir = []\n        ext_includes = []\n\n        self.platform = CustomBuildExtension.PLATFORMS.get( sys.platform, None )\n        if self.platform == None:\n            raise Exception(\"unsupported platform: %s\" % sys.platform)\n\n        if self.with_static_binutils: # the user has specified a custom binutils distro.\n            print \"[+] Using specific binutils static distribution\"\n            print \"[+]   %s\" % self.with_static_binutils\n            self.platform[\"libs\"] = [os.path.join( self.with_static_binutils, \"lib\"),]\n            self.platform[\"includes\"] = [os.path.join( self.with_static_binutils, \"include\"),]\n            self.platform[\"possible-lib-ext\"] = [\".a\",] # for all unix platforms.\n\n        # check for known includes\n        for inc in self.platform[\"includes\"]:\n            if self.check_includes(inc):\n                self.includes = inc # found a valid include dir with bintuils\n                break\n        if self.includes == None:\n            raise Exception(\"unable to determine correct include path for bfd.h / dis-asm.h\")\n\n        print \"[+] Using binutils headers at:\"\n        print \"[+]   %s\" % self.includes\n\n        # we'll use this include path for building.\n        ext_includes = [self.includes, ]\n\n        # Try to guess libopcodes / libbfd libs.\n        libs_dirs = self.platform[\"libs\"]\n        print \"[+] Searching binutils libraries...\"\n        for libdir in libs_dirs:\n            for possible_lib_ext in self.platform[\"possible-lib-ext\"]:\n                libs = self.find_binutils_libs(libdir, possible_lib_ext)\n                if libs:\n                    if self.libs:\n                        self.libs = self.libs + libs\n                    else:\n                        self.libs = libs\n                    break\n\n        if self.libs == None:\n            raise Exception(\"unable to find binutils libraries.\")\n\n        for lib in self.libs:\n            print \"[+]   %s\" % lib\n        #\n        # check for libopcodes / libbfd\n        #\n        libnames = [os.path.basename(lib) for lib in self.libs]\n        libraries_paths = [os.path.dirname(lib) for lib in self.libs]\n        libraries_paths = list(set(libraries_paths))  # removing duplicates\n        if not all( [lib.startswith(\"libopcodes\") or lib.startswith(\"libbfd\") for lib in libnames] ):\n            raise Exception(\"missing expected library (libopcodes / libbfd) in %s.\" % \"\\n\".join(libraries_paths))\n\n        ext_libs_dir += libraries_paths\n\n        if self.with_static_binutils:\n            # use libs as extra objects...\n            ext_extra_objects.extend( self.libs )\n        else:\n            ext_libs = [self.prepare_libs_for_cc(os.path.basename(lib)) for lib in self.libs]\n\n        # add dependecy to libiberty\n        if self.with_static_binutils or sys.platform == \"darwin\": # in OSX we always needs a static lib-iverty.\n\n            lib_liberty_partialpath = libraries_paths\n            if sys.platform == \"darwin\": # in osx the lib-iberty is prefixe by \"machine\" ppc/i386/x86_64\n                lib_liberty_partialpath.append( self._darwin_current_arch() )\n            lib_liberty_partialpath.append( \"libiberty.a\" )\n\n            lib_liberty_fullpath = os.path.join(*lib_liberty_partialpath ) # merge the prefix and the path\n            if not os.path.isfile(lib_liberty_fullpath):\n                raise Exception(\"missing expected library (libiberty) in %s.\" % \"\\n\".join(libraries_paths))\n            ext_extra_objects.append(lib_liberty_fullpath)\n\n        # generate .py / .h files that depends of libopcodes / libbfd currently selected\n        final_supported_archs = self.generate_source_files()\n\n        # final hacks for OSX\n        if sys.platform == \"darwin\":\n            # fix arch value.\n            os.environ[\"ARCHFLAGS\"] = \"-arch %s\" % self._darwin_current_arch()\n            # In OSX we've to link against libintl.\n            ext_libs.append(\"intl\")\n\n            # TODO: we have to improve the detection of gettext/libintl in OSX.. this is a quick fix.\n            dirs = [\n                \"/usr/local/opt/gettext/lib\", # homebrew\n                \"/opt/local/lib\" # macports\n            ]\n            for d in dirs:\n                if os.path.exists(d):\n                    ext_libs_dir.append(d)\n\n        # fix extensions.\n        for extension in self.extensions:\n            extension.include_dirs.extend( ext_includes )\n            extension.extra_objects.extend( ext_extra_objects )\n            extension.libraries.extend( ext_libs )\n            extension.library_dirs.extend( ext_libs_dir )\n\n        return build_ext.build_extensions(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndump the specified buffer in hex + ASCII format.", "response": "def dump(self, src, length=16, start=0, preffix=\"\"):\n        \"\"\"Dump the specified buffer in hex + ASCII format.\"\"\"\n        FILTER = \\\n            \"\".join([(len(repr(chr(x)))==3) and chr(x) or '.' \\\n                for x in xrange(256)])\n\n        result = list()\n\n        for i in xrange(0, len(src), length):\n            s           = src[i : i + length]\n            hexa        = \" \".join([\"%02X\" % ord(x) for x in s])\n            printable   = s.translate(FILTER)\n\n            result.append(\"%s%08X   %-*s   %s\\n\" % \\\n                (preffix, start + i, length * 3, hexa, printable))\n\n        return ''.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the entire section content.", "response": "def content(self):\n        \"\"\"Return the entire section content.\"\"\"\n        return _bfd.section_get_content(self.bfd, self._ptr, 0, self.size)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_content(self, offset, size):\n        return _bfd.section_get_content(self.bfd, self._ptr, offset, size)", "response": "Return the specified number of bytes from the current section."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest case for simple opcode disassembly.", "response": "def main():\n    \"\"\"Test case for simple opcode disassembly.\"\"\"\n    test_targets = (    \n        [ARCH_I386, MACH_I386_I386_INTEL_SYNTAX, ENDIAN_MONO, \"\\x55\\x89\\xe5\\xE8\\xB8\\xFF\\xFF\\xFF\", 0x1000],\n        [ARCH_I386, MACH_X86_64_INTEL_SYNTAX, ENDIAN_MONO, \"\\x55\\x48\\x89\\xe5\\xE8\\xA3\\xFF\\xFF\\xFF\", 0x1000],\n        [ARCH_ARM, MACH_ARM_2, ENDIAN_LITTLE, \"\\x04\\xe0\\x2d\\xe5\\xED\\xFF\\xFF\\xEB\", 0x1000],\n        [ARCH_MIPS, MACH_MIPSISA32, ENDIAN_BIG, \"\\x0C\\x10\\x00\\x97\\x00\\x00\\x00\\x00\", 0x1000],\n        [ARCH_POWERPC, MACH_PPC, ENDIAN_BIG, \"\\x94\\x21\\xFF\\xE8\\x7C\\x08\\x02\\xA6\", 0x1000],\n        #[ARCH_XTENSA, MACH_XTENSA, ENDIAN_BIG, \"\\x6C\\x10\\x06\\xD7\\x10\", 0x1000],\n        )\n\n    for target_arch, target_mach, target_endian, binary, address in test_targets:\n        #\n        # Initialize libopcodes with the current architecture.\n        #\n        opcodes = Opcodes(target_arch, target_mach, target_endian)\n\n        # Print some architecture-specific information.\n        print \"\\n[+] Architecture %s - Machine %d\" % \\\n            (opcodes.architecture_name, opcodes.machine)\n\n        print \"[+] Disassembly:\"\n\n        # Print all the disassembled instructions.\n        for vma, size, disasm in opcodes.disassemble(binary, address):\n            print \"0x%X (size=%d)\\t %s\" % (vma, size, disasm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_bfd(self, abfd):\n        self._ptr = _opcodes.initialize_bfd(abfd._ptr)              \n\n        # Already done inside opcodes.c\n        #self.architecture = abfd.architecture\n        #self.machine = abfd.machine\n        #self.endian = abfd.endian\n\n        # force intel syntax\n        if self.architecture == ARCH_I386:\n            if abfd.arch_size == 32:\n                self.machine = MACH_I386_I386_INTEL_SYNTAX\n                #abfd.machine = MACH_I386_I386_INTEL_SYNTAX\n            elif abfd.arch_size == 64:\n                self.machine = MACH_X86_64_INTEL_SYNTAX", "response": "Initialize underlying libOpcodes library using BFD."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize underlying libOpcodes library not using BFD.", "response": "def initialize_non_bfd(self, architecture=None, machine=None,\n        endian=ENDIAN_UNKNOWN):\n        \"\"\"Initialize underlying libOpcodes library not using BFD.\"\"\"\n\n        if None in [architecture, machine, endian]:\n            return\n\n        self.architecture = architecture\n        self.machine = machine\n        self.endian = endian"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the binary buffer to disassemble with other related information .", "response": "def initialize_smart_disassemble(self, data, start_address=0):\n        \"\"\"\n        Set the binary buffer to disassemble with other related information\n        ready for an instruction by instruction disassembly session.\n        \n        \"\"\"\n        _opcodes.initialize_smart_disassemble(\n            self._ptr, data, start_address)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvoking the callback function for every instruction disassembled at the given offset from the base address.", "response": "def start_smart_disassemble(self, start_address, callback):\n        \"\"\"\n        Invok the callback function for every instruction disassembled at the\n        previously initialized smart session with the given offset from the\n        start address supplied (also in the initialization).\n\n        \"\"\"\n        _opcodes.start_smart_disassemble(self._ptr, start_address, callback)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_single_instruction_callback(self, address, size, branch_delay_insn,\n        insn_type, target, target2, disassembly):\n        \"\"\"\n        Callack on each disassembled instruction to print its information.\n        \n        \"\"\"\n        print \"0x%X SZ=%d BD=%d IT=%d\\t%s\" % \\\n            (address, size, branch_delay_insn, insn_type, disassembly)\n\n        return PYBFD_DISASM_CONTINUE", "response": "Print the information of a single instruction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disassemble(self, data, start_address=0):\n        return _opcodes.disassemble(self._ptr, data, start_address)", "response": "Disassemble the given binary buffer into a list of virtual memory address instruction length and disassembly code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_supported_disassembler_header(supported_archs):\n    arch_entries = []\n\n    for arch, little, big, comment in supported_archs:\n        arch_entries.append( header_arch_entry % (arch, little, big) )\n\n    return supported_archs_header % ( \",\\n\".join(arch_entries) )", "response": "Extract export symbols using binutils s nm utility from Binutils and\n    generate a current header for PyBFD."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting export symbols using binutils s nm utility from Binutils and generate a current header for PyBFD.", "response": "def generate_supported_architectures_source(supported_archs, supported_machines):\n    \"\"\"Extract export symbols using binutils's nm utility from Binutils and\n    generate a current header for PyBFD.\n\n    \"\"\"\n    arch_entries = []\n    mach_entries = []\n\n    for arch, little, big, comment in supported_archs:\n        arch_entries.append( pybfd_arch_entry % (arch[4 : ].upper(), arch) )\n\n    for mach, value in supported_machines:\n        mach_entries.append( pybfd_mach_entry % (mach[4 : ].upper(), value) )\n\n    return gen_bfd_archs_code % (\n        \"\\n\".join(arch_entries),\n        \"\\n\".join(mach_entries))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open(self, _file, target=DEFAULT_TARGET):\n        # Close any existing BFD structure instance. \n        self.close()\n\n        #\n        # STEP 1. Open the BFD pointer.\n        #\n        # Determine if the user passed a file-descriptor or a _file and\n        # proceed accordingly.\n        if type(_file) is FileType:\n            # The user specified a file descriptor.\n            filename = _file.name\n\n            if islink(filename):\n                raise BfdException(\"Symlinks file-descriptors are not valid\")\n                    \n            try:\n                self._ptr = _bfd.fdopenr(filename, target, dup(_file.fileno()))\n            except Exception, err:\n                raise BfdException(\n                    \"Unable to open file-descriptor %s : %s\" % (filename, err))\n\n        elif type(_file) is StringType:\n            # The user spcified a filaname so first check if file exists.\n            filename = _file\n            try:\n                with open(_file): pass\n            except IOError:\n                raise BfdException(\"File %s does not exist.\" % filename)\n\n            #\n            # Proceed to open the specified file and create a new BFD.\n            #\n            try:\n                self._ptr = _bfd.openr(filename, target)\n            except (TypeError, IOError), err:\n                raise BfdException(\n                    \"Unable to open file %s : %s\" % (filename, err))\n\n        elif type(_file) is IntType:\n            # The user specified an already-open BFD pointer so we avoid any\n            # further open operation and move on to file format recognition.\n            self._ptr = _file\n\n        else:\n            raise BfdException(\n                \"Invalid file type specified for open operation (%r)\" % _file)\n\n        #\n        # STEP 2. Determine file format of the BFD.\n        #\n        # Now that the BFD is open we'll proceed to determine its file format.\n        # We'll use the objdump logic to determine it and raise an error in\n        # case we were unable to get it right.\n        #\n        try:\n            # Type opening it as an archieve and if it success then check\n            # subfiles.\n            if _bfd.check_format(self._ptr, BfdFormat.ARCHIVE):\n                # Set current format and store the inner file list.\n                self.file_format = BfdFormat.ARCHIVE\n\n                self.__populate_archive_files()\n            else:\n                # DO NOT USE bfd_check_format_matches() becuase its not tested.\n                # An implementation example if on objdump.c at function\n                # display_bfd().\n                if _bfd.check_format(self._ptr, BfdFormat.OBJECT):\n                    self.file_format = BfdFormat.OBJECT\n\n                elif _bfd.check_format(self._ptr, BfdFormat.CORE):\n                    self.file_format = BfdFormat.CORE\n\n                else:\n                    pass\n                    raise BfdException(_bfd.get_last_error_message())\n\n        except TypeError, err:\n            raise BfdException(\n                \"Unable to initialize file format : %s\" % err)\n\n        #\n        # STEP 3. Extract inner sections and symbolic information.\n        #\n        if self._ptr is not None:\n            # If the file is a valid BFD file format but not an archive then\n            # get its sections and symbolic information (if any).\n            if self.file_format in [BfdFormat.OBJECT, BfdFormat.CORE]:\n                self.__populate_sections()\n                self.__populate_symbols()", "response": "Open the existing file for reading."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstore the list of files inside an archive file.", "response": "def __populate_archive_files(self):\n        \"\"\"Store the list of files inside an archive file.\"\"\"\n        self.archive_files = []\n        for _ptr in _bfd.archive_list_files(self._ptr):\n            try:\n                self.archive_files.append(Bfd(_ptr))\n            except BfdException, err:\n                #print \"Error populating archive file list : %s\" % err\n                #print_exc()\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the list of files inside an archive file.", "response": "def archive_filenames(self):\n        \"\"\"Return the list of files inside an archive file.\"\"\"\n        try:\n            return _bfd.archive_list_filenames(self._ptr)\n        except TypeError, err:\n            raise BfdException(err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef file_format_name(self):\n        try:\n            return BfdFormatNamesLong[self.file_format]\n        except IndexError, err:\n            raise BfdException(\"Invalid format specified (%d)\" % self.file_format)", "response": "Return the current format name of the open bdf."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of the section present in the BFD to populate our internal list.", "response": "def __populate_sections(self):\n        \"\"\"Get a list of the section present in the bfd to populate our\n        internal list.\n\n        \"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        for section in _bfd.get_sections_list(self._ptr):\n            try:\n                bfd_section = BfdSection(self._ptr, section)\n                self._sections[bfd_section.name] = bfd_section\n            except BfdSectionException, err:\n                #print \"Exception during section pasing : %s\" % err\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __populate_symbols(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        try:\n            symbols = _bfd.get_symbols(self._ptr)\n\n            # Temporary dictionary ordered by section index. This is necessary\n            # because the symbolic information return the section index it belongs\n            # to.\n            sections = {}\n            for section in self.sections:\n                sections[self.sections[section].index] = self.sections[section]\n\n            for symbol in symbols:\n                # Extract each field for further processing.\n                symbol_section_index = symbol[0]\n                symbol_name = symbol[1]\n                symbol_value = symbol[2]\n                symbol_flags = symbol[3]\n\n                # Get the effective address of the current symbol.\n                symbol_flags = tuple(\n                    [f for f in SYMBOL_FLAGS_LIST if symbol_flags & f == f] )\n\n                # Create a new symbol instance to hold symbolic information.\n                new_symbol = Symbol(\n                    sections.get(symbol_section_index, None),\n                    symbol_name,\n                    symbol_value,\n                    symbol_flags)\n\n                if new_symbol.section is None:\n                    continue\n                symbol_address = new_symbol.section.vma + new_symbol.value\n                #if new_symbol.flags in \\\n                #    [SymbolFlags.LOCAL , SymbolFlags.GLOBAL , SymbolFlags.EXPORT]:\n                #    symbol_address = new_symbol.section.vma + new_symbol.value\n                #else:\n                #    # TODO: Enhance this!\n                #    # Discard any other symbol information.\n                #    continue\n\n                self._symbols[symbol_address] = new_symbol\n\n            del sections\n        except BfdSectionException, err:\n            raise BfdException(\"Exception on symbolic ifnormation parsing.\")", "response": "Get a list of the symbols present in the BFD to populate our internal list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if self._ptr:\n            #try:\n            #    # Release inner BFD files in case we're an archive BFD.\n            #    if self.is_archive:\n            #        [inner_bfd.close() for inner_bfd in self.archive_files]\n            #except TypeError, err:\n            #    pass\n\n            try:\n                _bfd.close(self._ptr)\n            except TypeError, err:\n                raise BfdException(\"Unable to close bfd (%s)\" % err)\n            finally:\n                self._ptr = None", "response": "Close any existing BFD structure before open a new one."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the filename of the BFD file being processed.", "response": "def filename(self):\n        \"\"\"Return the filename of the BFD file being processed.\"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.FILENAME)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the cacheable attribute of the BFD file being processed.", "response": "def cacheable(self):\n        \"\"\"Return the cacheable attribute of the BFD file being processed.\"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.CACHEABLE)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.FORMAT)", "response": "Return the format attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef target(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.TARGET)", "response": "Return the target of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef machine(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.FLAVOUR)", "response": "Return the flavour attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the family_coff attribute of the BFD file being processed.", "response": "def family_coff(self):\n        \"\"\"Return the family_coff attribute of the BFD file being processed.\"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.FAMILY_COFF)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the big endian attribute of the BFD file being processed.", "response": "def big_endian(self):\n        \"\"\"Return the big endian attribute of the BFD file being processed.\"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.IS_BIG_ENDIAN)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef little_endian(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.IS_LITTLE_ENDIAN)", "response": "Return the little_endian attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef header_big_endian(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.HEADER_BIG_ENDIAN)", "response": "Return the header_big_endian attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef header_little_endian(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.HEADER_LITTLE_ENDIAN)", "response": "Return the header_little_endian attribute of the BFD file being\n        processed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the file flags attribute of the BFD file being processed.", "response": "def file_flags(self):\n        \"\"\"Return the file flags attribute of the BFD file being processed.\"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.FILE_FLAGS)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the new file flags attribute of the BFD file being processed.", "response": "def file_flags(self, _file_flags):\n        \"\"\"Set the new file flags attribute of the BFD file being processed.\"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.set_file_flags(self._ptr, _file_flags)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the applicable file flags attribute of the BFD file being processed.", "response": "def applicable_file_flags(self):\n        \"\"\"\n        Return the applicable file flags attribute of the BFD file being\n        processed.\n\n        \"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.APPLICABLE_FILE_FLAGS)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef my_archieve(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.MY_ARCHIEVE)", "response": "Return the my archieve attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_map(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.HAS_MAP)", "response": "Return the has map attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_thin_archieve(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.IS_THIN_ARCHIEVE)", "response": "Return the is thin archieve attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_gap_in_elf_shndx(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.HAS_GAP_IN_ELF_SHNDX)", "response": "Return the has gap in elf shndx attribute of the BFD file being\n        processed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef valid_reloction_types(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.VALID_RELOC_TYPES)", "response": "Return the valid_reloc_types attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_data(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.USRDATA)", "response": "Return the usrdata attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_address(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.START_ADDRESS)", "response": "Return the start address attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstores the new start address attribute of the BFD file being processed.", "response": "def start_address(self, _start_address):\n        \"\"\"Store the new start address attribute of the BFD file being\n        processed.\n        \n        \"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.set_start_address(self._ptr, _start_address)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gp_size(self, _gp_size):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.set_gp_size(self._ptr, _gp_size)", "response": "Store the new start address attribute of the BFD file being\n        processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef symbols_count(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.SYMCOUNT)", "response": "Return the symcount attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef out_symbols(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.OUTSYMBOLS)", "response": "Return the out symbols attribute of the BFD file being processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the sections_count attribute of the BFD file being processed.", "response": "def sections_count(self):\n        \"\"\"Return the sections_count attribute of the BFD file being processed.\"\"\"\n        # This should match the 'sections' attribute length so instead should\n        # use :\n        #\n        #   len(bfd.sections)\n        #\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.COUNT_SECTIONS)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dynamic_symbols_count(self):\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.DYNAMIC_SYMCOUNT)", "response": "Return the dynamic symbols count attribute of the BFD file being\n        processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the symbol leading char attribute of the BFD file being processed.", "response": "def symbol_leading_char(self):\n        \"\"\"Return the symbol leading char attribute of the BFD file being\n        processed.\n        \n        \"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.SYMBOL_LEADING_CHAR)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the architecure size in bits.", "response": "def arch_size(self):\n        \"\"\"Return the architecure size in bits.\"\"\"\n        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        try:\n            return _bfd.get_arch_size(self._ptr)\n        except Exception, err:\n            raise BfdException(\"Unable to determine architeure size.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay a matrix on Nuimo s LED matrix display.", "response": "def display_matrix(self, matrix, interval=2.0, brightness=1.0, fading=False, ignore_duplicates=False):\n        \"\"\"\n        Displays an LED matrix on Nuimo's LED matrix display.\n\n        :param matrix: the matrix to display\n        :param interval: interval in seconds until the matrix disappears again\n        :param brightness: led brightness between 0..1\n        :param fading: if True, the previous matrix fades into the new matrix\n        :param ignore_duplicates: if True, the matrix is not sent again if already being displayed\n        \"\"\"\n        self._matrix_writer.write(\n            matrix=matrix,\n            interval=interval,\n            brightness=brightness,\n            fading=fading,\n            ignore_duplicates=ignore_duplicates\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping for ASN lookups.", "response": "def lookup_asn(self, *args, **kwargs):\n        \"\"\"\n        Temporary wrapper for IP ASN lookups (moved to\n        asn.IPASN.lookup()). This will be removed in a future\n        release.\n        \"\"\"\n\n        from warnings import warn\n        warn('Net.lookup_asn() has been deprecated and will be removed. '\n             'You should now use asn.IPASN.lookup() for IP ASN lookups.')\n        from .asn import IPASN\n        response = None\n        ipasn = IPASN(self)\n        return ipasn.lookup(*args, **kwargs), response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_asn_dns(self):\n\n        try:\n\n            log.debug('ASN query for {0}'.format(self.dns_zone))\n            data = self.dns_resolver.query(self.dns_zone, 'TXT')\n            return list(data)\n\n        except (dns.resolver.NXDOMAIN, dns.resolver.NoNameservers,\n                dns.resolver.NoAnswer, dns.exception.Timeout) as e:\n\n            raise ASNLookupError(\n                'ASN lookup failed (DNS {0}) for {1}.'.format(\n                    e.__class__.__name__, self.address_str)\n            )\n\n        except:  # pragma: no cover\n\n            raise ASNLookupError(\n                'ASN lookup failed for {0}.'.format(self.address_str)\n            )", "response": "This function is used to retrieve the ASN information for an IP address from a Cymru DNS zone."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_asn_origin_whois(self, asn_registry='radb', asn=None,\n                             retry_count=3, server=None, port=43):\n        \"\"\"\n        The function for retrieving CIDR info for an ASN via whois.\n\n        Args:\n            asn_registry (:obj:`str`): The source to run the query against\n                (asn.ASN_ORIGIN_WHOIS).\n            asn (:obj:`str`): The AS number (required).\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            server (:obj:`str`): An optional server to connect to.\n            port (:obj:`int`): The network port to connect on. Defaults to 43.\n\n        Returns:\n            str: The raw ASN origin whois data.\n\n        Raises:\n            WhoisLookupError: The ASN origin whois lookup failed.\n            WhoisRateLimitError: The ASN origin Whois request rate limited and\n                retries were exhausted.\n        \"\"\"\n\n        try:\n\n            if server is None:\n                server = ASN_ORIGIN_WHOIS[asn_registry]['server']\n\n            # Create the connection for the whois query.\n            conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            conn.settimeout(self.timeout)\n            log.debug('ASN origin WHOIS query for {0} at {1}:{2}'.format(\n                asn, server, port))\n            conn.connect((server, port))\n\n            # Prep the query.\n            query = ' -i origin {0}{1}'.format(asn, '\\r\\n')\n\n            # Query the whois server, and store the results.\n            conn.send(query.encode())\n\n            response = ''\n            while True:\n\n                d = conn.recv(4096).decode()\n\n                response += d\n\n                if not d:\n\n                    break\n\n            conn.close()\n\n            # TODO: this was taken from get_whois(). Need to test rate limiting\n            if 'Query rate limit exceeded' in response:  # pragma: no cover\n\n                if retry_count > 0:\n\n                    log.debug('ASN origin WHOIS query rate limit exceeded. '\n                              'Waiting...')\n                    sleep(1)\n                    return self.get_asn_origin_whois(\n                        asn_registry=asn_registry, asn=asn,\n                        retry_count=retry_count-1,\n                        server=server, port=port\n                    )\n\n                else:\n\n                    raise WhoisRateLimitError(\n                        'ASN origin Whois lookup failed for {0}. Rate limit '\n                        'exceeded, wait and try again (possibly a '\n                        'temporary block).'.format(asn))\n\n            elif ('error 501' in response or 'error 230' in response\n                  ):  # pragma: no cover\n\n                log.debug('ASN origin WHOIS query error: {0}'.format(response))\n                raise ValueError\n\n            return str(response)\n\n        except (socket.timeout, socket.error) as e:\n\n            log.debug('ASN origin WHOIS query socket error: {0}'.format(e))\n            if retry_count > 0:\n\n                log.debug('ASN origin WHOIS query retrying (count: {0})'\n                          ''.format(str(retry_count)))\n                return self.get_asn_origin_whois(\n                    asn_registry=asn_registry, asn=asn,\n                    retry_count=retry_count-1, server=server, port=port\n                )\n\n            else:\n\n                raise WhoisLookupError(\n                    'ASN origin WHOIS lookup failed for {0}.'.format(asn)\n                )\n\n        except WhoisRateLimitError:  # pragma: no cover\n\n            raise\n\n        except:  # pragma: no cover\n\n            raise WhoisLookupError(\n                'ASN origin WHOIS lookup failed for {0}.'.format(asn)\n            )", "response": "This function returns the raw ASN origin whois data for an ASN."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_whois(self, asn_registry='arin', retry_count=3, server=None,\n                  port=43, extra_blacklist=None):\n        \"\"\"\n        The function for retrieving whois or rwhois information for an IP\n        address via any port. Defaults to port 43/tcp (WHOIS).\n\n        Args:\n            asn_registry (:obj:`str`): The NIC to run the query against.\n                Defaults to 'arin'.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            server (:obj:`str`): An optional server to connect to. If\n                provided, asn_registry will be ignored.\n            port (:obj:`int`): The network port to connect on. Defaults to 43.\n            extra_blacklist (:obj:`list` of :obj:`str`): Blacklisted whois\n                servers in addition to the global BLACKLIST. Defaults to None.\n\n        Returns:\n            str: The raw whois data.\n\n        Raises:\n            BlacklistError: Raised if the whois server provided is in the\n                global BLACKLIST or extra_blacklist.\n            WhoisLookupError: The whois lookup failed.\n            WhoisRateLimitError: The Whois request rate limited and retries\n                were exhausted.\n        \"\"\"\n\n        try:\n\n            extra_bl = extra_blacklist if extra_blacklist else []\n\n            if any(server in srv for srv in (BLACKLIST, extra_bl)):\n                raise BlacklistError(\n                    'The server {0} is blacklisted.'.format(server)\n                )\n\n            if server is None:\n                server = RIR_WHOIS[asn_registry]['server']\n\n            # Create the connection for the whois query.\n            conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            conn.settimeout(self.timeout)\n            log.debug('WHOIS query for {0} at {1}:{2}'.format(\n                self.address_str, server, port))\n            conn.connect((server, port))\n\n            # Prep the query.\n            query = self.address_str + '\\r\\n'\n            if asn_registry == 'arin':\n\n                query = 'n + {0}'.format(query)\n\n            # Query the whois server, and store the results.\n            conn.send(query.encode())\n\n            response = ''\n            while True:\n\n                d = conn.recv(4096).decode('ascii', 'ignore')\n\n                response += d\n\n                if not d:\n\n                    break\n\n            conn.close()\n\n            if 'Query rate limit exceeded' in response:  # pragma: no cover\n\n                if retry_count > 0:\n\n                    log.debug('WHOIS query rate limit exceeded. Waiting...')\n                    sleep(1)\n                    return self.get_whois(\n                        asn_registry=asn_registry, retry_count=retry_count-1,\n                        server=server, port=port,\n                        extra_blacklist=extra_blacklist\n                    )\n\n                else:\n\n                    raise WhoisRateLimitError(\n                        'Whois lookup failed for {0}. Rate limit '\n                        'exceeded, wait and try again (possibly a '\n                        'temporary block).'.format(self.address_str))\n\n            elif ('error 501' in response or 'error 230' in response\n                  ):  # pragma: no cover\n\n                log.debug('WHOIS query error: {0}'.format(response))\n                raise ValueError\n\n            return str(response)\n\n        except (socket.timeout, socket.error) as e:\n\n            log.debug('WHOIS query socket error: {0}'.format(e))\n            if retry_count > 0:\n\n                log.debug('WHOIS query retrying (count: {0})'.format(\n                    str(retry_count)))\n                return self.get_whois(\n                    asn_registry=asn_registry, retry_count=retry_count-1,\n                    server=server, port=port, extra_blacklist=extra_blacklist\n                )\n\n            else:\n\n                raise WhoisLookupError(\n                    'WHOIS lookup failed for {0}.'.format(self.address_str)\n                )\n\n        except WhoisRateLimitError:  # pragma: no cover\n\n            raise\n\n        except BlacklistError:\n\n            raise\n\n        except:  # pragma: no cover\n\n            raise WhoisLookupError(\n                'WHOIS lookup failed for {0}.'.format(self.address_str)\n            )", "response": "This function returns the raw whois data for a given NIC."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_host(self, retry_count=3):\n\n        try:\n\n            default_timeout_set = False\n            if not socket.getdefaulttimeout():\n\n                socket.setdefaulttimeout(self.timeout)\n                default_timeout_set = True\n\n            log.debug('Host query for {0}'.format(self.address_str))\n            ret = socket.gethostbyaddr(self.address_str)\n\n            if default_timeout_set:  # pragma: no cover\n\n                socket.setdefaulttimeout(None)\n\n            results = namedtuple('get_host_results', 'hostname, aliaslist, '\n                                                     'ipaddrlist')\n            return results(ret)\n\n        except (socket.timeout, socket.error) as e:\n\n            log.debug('Host query socket error: {0}'.format(e))\n            if retry_count > 0:\n\n                log.debug('Host query retrying (count: {0})'.format(\n                    str(retry_count)))\n\n                return self.get_host(retry_count - 1)\n\n            else:\n\n                raise HostLookupError(\n                    'Host lookup failed for {0}.'.format(self.address_str)\n                )\n\n        except:  # pragma: no cover\n\n            raise HostLookupError(\n                'Host lookup failed for {0}.'.format(self.address_str)\n            )", "response": "This function returns the hostname and aliaslist for the given IP address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_http_raw(self, url=None, retry_count=3, headers=None,\n                     request_type='GET', form_data=None):\n        \"\"\"\n        The function for retrieving a raw HTML result via HTTP.\n\n        Args:\n            url (:obj:`str`): The URL to retrieve (required).\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            headers (:obj:`dict`): The HTTP headers. The Accept header\n                defaults to 'text/html'.\n            request_type (:obj:`str`): Request type 'GET' or 'POST'. Defaults\n                to 'GET'.\n            form_data (:obj:`dict`): Optional form POST data.\n\n        Returns:\n            str: The raw data.\n\n        Raises:\n            HTTPLookupError: The HTTP lookup failed.\n        \"\"\"\n\n        if headers is None:\n            headers = {'Accept': 'text/html'}\n\n        enc_form_data = None\n        if form_data:\n            enc_form_data = urlencode(form_data)\n            try:\n                # Py 2 inspection will alert on the encoding arg, no harm done.\n                enc_form_data = bytes(enc_form_data, encoding='ascii')\n            except TypeError:  # pragma: no cover\n                pass\n\n        try:\n\n            # Create the connection for the HTTP query.\n            log.debug('HTTP query for {0} at {1}'.format(\n                self.address_str, url))\n            try:\n                # Py 2 inspection alert bypassed by using kwargs dict.\n                conn = Request(url=url, data=enc_form_data, headers=headers,\n                               **{'method': request_type})\n            except TypeError:  # pragma: no cover\n                conn = Request(url=url, data=enc_form_data, headers=headers)\n            data = self.opener.open(conn, timeout=self.timeout)\n\n            try:\n                d = data.readall().decode('ascii', 'ignore')\n            except AttributeError:  # pragma: no cover\n                d = data.read().decode('ascii', 'ignore')\n\n            return str(d)\n\n        except (URLError, socket.timeout, socket.error) as e:\n\n            log.debug('HTTP query socket error: {0}'.format(e))\n            if retry_count > 0:\n\n                log.debug('HTTP query retrying (count: {0})'.format(\n                    str(retry_count)))\n\n                return self.get_http_raw(\n                    url=url, retry_count=retry_count - 1, headers=headers,\n                    request_type=request_type, form_data=form_data\n                )\n\n            else:\n\n                raise HTTPLookupError('HTTP lookup failed for {0}.'.format(\n                    url))\n\n        except HTTPLookupError as e:  # pragma: no cover\n\n            raise e\n\n        except Exception:  # pragma: no cover\n\n            raise HTTPLookupError('HTTP lookup failed for {0}.'.format(url))", "response": "This function is used to retrieve a raw HTML result from the URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction for formatting CLI output results.", "response": "def generate_output(line='0', short=None, name=None, value=None,\n                    is_parent=False, colorize=True):\n    \"\"\"\n    The function for formatting CLI output results.\n\n    Args:\n        line (:obj:`str`): The line number (0-4). Determines indentation.\n            Defaults to '0'.\n        short (:obj:`str`): The optional abbreviated name for a field.\n            See hr.py for values.\n        name (:obj:`str`): The optional name for a field. See hr.py for values.\n        value (:obj:`str`): The field data (required).\n        is_parent (:obj:`bool`): Set to True if the field value has sub-items\n            (dicts/lists). Defaults to False.\n        colorize (:obj:`bool`): Colorize the console output with ANSI colors.\n            Defaults to True.\n\n    Returns:\n        str: The generated output.\n    \"\"\"\n\n    # TODO: so ugly\n    output = '{0}{1}{2}{3}{4}{5}{6}{7}\\n'.format(\n        LINES['{0}{1}'.format(line, 'C' if colorize else '')] if (\n            line in LINES.keys()) else '',\n        COLOR_DEPTH[line] if (colorize and line in COLOR_DEPTH) else '',\n        ANSI['b'],\n        short if short is not None else (\n            name if (name is not None) else ''\n        ),\n        '' if (name is None or short is None) else ' ({0})'.format(\n            name),\n        '' if (name is None and short is None) else ': ',\n        ANSI['end'] if colorize else '',\n        '' if is_parent else value\n    )\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_output_header(self, query_type='RDAP'):\n\n        output = '\\n{0}{1}{2} query for {3}:{4}\\n\\n'.format(\n            ANSI['ul'],\n            ANSI['b'],\n            query_type,\n            self.obj.address_str,\n            ANSI['end']\n        )\n\n        return output", "response": "This function is used to generate the CLI output header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_output_entities(self, json_data=None, hr=True,\n                                 show_name=False, colorize=True):\n        \"\"\"\n        The function for generating CLI output RDAP entity results.\n\n        Args:\n            json_data (:obj:`dict`): The data to process. Defaults to None.\n            hr (:obj:`bool`): Enable human readable key translations. Defaults\n                to True.\n            show_name (:obj:`bool`): Show human readable name (default is to\n                only show short). Defaults to False.\n            colorize (:obj:`bool`): Colorize the console output with ANSI\n                colors. Defaults to True.\n\n        Returns:\n            str: The generated output.\n        \"\"\"\n\n        output = ''\n        short = HR_RDAP['entities']['_short'] if hr else 'entities'\n        name = HR_RDAP['entities']['_name'] if (hr and show_name) else None\n\n        output += generate_output(\n            line='0',\n            short=short,\n            name=name,\n            is_parent=False if (json_data is None or\n                                json_data['entities'] is None) else True,\n            value='None' if (json_data is None or\n                             json_data['entities'] is None) else None,\n            colorize=colorize\n        )\n\n        if json_data is not None:\n\n            for ent in json_data['entities']:\n\n                output += generate_output(\n                    line='1',\n                    value=ent,\n                    colorize=colorize\n                )\n\n        return output", "response": "This function is used to generate CLI output for the RDAP entity results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_output_events(self, source, key, val, line='2', hr=True,\n                               show_name=False, colorize=True):\n        \"\"\"\n        The function for generating CLI output RDAP events results.\n\n        Args:\n            source (:obj:`str`): The parent key 'network' or 'objects'\n                (required).\n            key (:obj:`str`): The event key 'events' or 'events_actor'\n                (required).\n            val (:obj:`dict`): The event dictionary (required).\n            line (:obj:`str`): The line number (0-4). Determines indentation.\n                Defaults to '0'.\n            hr (:obj:`bool`): Enable human readable key translations. Defaults\n                to True.\n            show_name (:obj:`bool`): Show human readable name (default is to\n                only show short). Defaults to False.\n            colorize (:obj:`bool`): Colorize the console output with ANSI\n                colors. Defaults to True.\n\n        Returns:\n            str: The generated output.\n        \"\"\"\n\n        output = generate_output(\n            line=line,\n            short=HR_RDAP[source][key]['_short'] if hr else key,\n            name=HR_RDAP[source][key]['_name'] if (hr and show_name) else None,\n            is_parent=False if (val is None or\n                                len(val) == 0) else True,\n            value='None' if (val is None or\n                             len(val) == 0) else None,\n            colorize=colorize\n        )\n\n        if val is not None:\n\n            count = 0\n            for item in val:\n\n                try:\n                    action = item['action']\n                except KeyError:\n                    action = None\n\n                try:\n                    timestamp = item['timestamp']\n                except KeyError:\n                    timestamp = None\n\n                try:\n                    actor = item['actor']\n                except KeyError:\n                    actor = None\n\n                if count > 0:\n                    output += generate_output(\n                        line=str(int(line)+1),\n                        is_parent=True,\n                        colorize=colorize\n                    )\n\n                output += generate_output(\n                    line=str(int(line)+1),\n                    short=HR_RDAP_COMMON[key]['action'][\n                        '_short'] if hr else 'action',\n                    name=HR_RDAP_COMMON[key]['action'][\n                        '_name'] if (hr and show_name) else None,\n                    value=action,\n                    colorize=colorize\n                )\n\n                output += generate_output(\n                    line=str(int(line)+1),\n                    short=HR_RDAP_COMMON[key]['timestamp'][\n                        '_short'] if hr else 'timestamp',\n                    name=HR_RDAP_COMMON[key]['timestamp'][\n                        '_name'] if (hr and show_name) else None,\n                    value=timestamp,\n                    colorize=colorize\n                )\n\n                output += generate_output(\n                    line=str(int(line)+1),\n                    short=HR_RDAP_COMMON[key]['actor'][\n                        '_short'] if hr else 'actor',\n                    name=HR_RDAP_COMMON[key]['actor'][\n                        '_name'] if (hr and show_name) else None,\n                    value=actor,\n                    colorize=colorize\n                )\n\n                count += 1\n\n        return output", "response": "This function is used to generate CLI output events for RDAP objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_output_list(self, source, key, val, line='2', hr=True,\n                             show_name=False, colorize=True):\n        \"\"\"\n        The function for generating CLI output RDAP list results.\n\n        Args:\n            source (:obj:`str`): The parent key 'network' or 'objects'\n                (required).\n            key (:obj:`str`): The event key 'events' or 'events_actor'\n                (required).\n            val (:obj:`dict`): The event dictionary (required).\n            line (:obj:`str`): The line number (0-4). Determines indentation.\n                Defaults to '0'.\n            hr (:obj:`bool`): Enable human readable key translations. Defaults\n                to True.\n            show_name (:obj:`bool`): Show human readable name (default is to\n                only show short). Defaults to False.\n            colorize (:obj:`bool`): Colorize the console output with ANSI\n                colors. Defaults to True.\n\n        Returns:\n            str: The generated output.\n        \"\"\"\n\n        output = generate_output(\n            line=line,\n            short=HR_RDAP[source][key]['_short'] if hr else key,\n            name=HR_RDAP[source][key]['_name'] if (hr and show_name) else None,\n            is_parent=False if (val is None or\n                                len(val) == 0) else True,\n            value='None' if (val is None or\n                             len(val) == 0) else None,\n            colorize=colorize\n        )\n\n        if val is not None:\n            for item in val:\n                output += generate_output(\n                    line=str(int(line)+1),\n                    value=item,\n                    colorize=colorize\n                )\n\n        return output", "response": "This function is used to generate CLI output list results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_output_notices(self, source, key, val, line='1', hr=True,\n                                show_name=False, colorize=True):\n        \"\"\"\n        The function for generating CLI output RDAP notices results.\n\n        Args:\n            source (:obj:`str`): The parent key 'network' or 'objects'\n                (required).\n            key (:obj:`str`): The event key 'events' or 'events_actor'\n                (required).\n            val (:obj:`dict`): The event dictionary (required).\n            line (:obj:`str`): The line number (0-4). Determines indentation.\n                Defaults to '0'.\n            hr (:obj:`bool`): Enable human readable key translations. Defaults\n                to True.\n            show_name (:obj:`bool`): Show human readable name (default is to\n                only show short). Defaults to False.\n            colorize (:obj:`bool`): Colorize the console output with ANSI\n                colors. Defaults to True.\n\n        Returns:\n            str: The generated output.\n        \"\"\"\n\n        output = generate_output(\n            line=line,\n            short=HR_RDAP[source][key]['_short'] if hr else key,\n            name=HR_RDAP[source][key]['_name'] if (hr and show_name) else None,\n            is_parent=False if (val is None or\n                                len(val) == 0) else True,\n            value='None' if (val is None or\n                             len(val) == 0) else None,\n            colorize=colorize\n        )\n\n        if val is not None:\n\n            count = 0\n            for item in val:\n\n                title = item['title']\n                description = item['description']\n                links = item['links']\n\n                if count > 0:\n                    output += generate_output(\n                        line=str(int(line)+1),\n                        is_parent=True,\n                        colorize=colorize\n                    )\n\n                output += generate_output(\n                    line=str(int(line)+1),\n                    short=HR_RDAP_COMMON[key]['title']['_short'] if hr else (\n                        'title'),\n                    name=HR_RDAP_COMMON[key]['title']['_name'] if (\n                        hr and show_name) else None,\n                    value=title,\n                    colorize=colorize\n                )\n\n                output += generate_output(\n                    line=str(int(line)+1),\n                    short=HR_RDAP_COMMON[key]['description'][\n                        '_short'] if hr else 'description',\n                    name=HR_RDAP_COMMON[key]['description'][\n                        '_name'] if (hr and show_name) else None,\n                    value=description.replace(\n                        '\\n',\n                        '\\n{0}'.format(generate_output(line='3'))\n                    ),\n                    colorize=colorize\n                )\n                output += self.generate_output_list(\n                    source=source,\n                    key='links',\n                    val=links,\n                    line=str(int(line)+1),\n                    hr=hr,\n                    show_name=show_name,\n                    colorize=colorize\n                )\n\n                count += 1\n\n        return output", "response": "This function is used to generate CLI output RDAP notices results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_output_objects(self, json_data=None, hr=True, show_name=False,\n                                colorize=True):\n        \"\"\"\n        The function for generating CLI output RDAP object results.\n\n        Args:\n            json_data (:obj:`dict`): The data to process. Defaults to None.\n            hr (:obj:`bool`): Enable human readable key translations. Defaults\n                to True.\n            show_name (:obj:`bool`): Show human readable name (default is to\n                only show short). Defaults to False.\n            colorize (:obj:`bool`): Colorize the console output with ANSI\n                colors. Defaults to True.\n\n        Returns:\n            str: The generated output.\n        \"\"\"\n\n        if json_data is None:\n            json_data = {}\n\n        output = generate_output(\n            line='0',\n            short=HR_RDAP['objects']['_short'] if hr else 'objects',\n            name=HR_RDAP['objects']['_name'] if (hr and show_name) else None,\n            is_parent=True,\n            colorize=colorize\n        )\n\n        count = 0\n        for obj_name, obj in json_data['objects'].items():\n            if count > 0:\n                output += self.generate_output_newline(\n                    line='1',\n                    colorize=colorize\n                )\n            count += 1\n\n            output += generate_output(\n                line='1',\n                short=obj_name,\n                is_parent=True,\n                colorize=colorize\n            )\n\n            for key, val in obj.items():\n\n                if key in ['links', 'entities', 'roles', 'status']:\n\n                    output += self.generate_output_list(\n                        source='objects',\n                        key=key,\n                        val=val,\n                        line='2',\n                        hr=hr,\n                        show_name=show_name,\n                        colorize=colorize\n                    )\n\n                elif key in ['notices', 'remarks']:\n\n                    output += self.generate_output_notices(\n                        source='objects',\n                        key=key,\n                        val=val,\n                        line='2',\n                        hr=hr,\n                        show_name=show_name,\n                        colorize=colorize\n                    )\n\n                elif key == 'events':\n\n                    output += self.generate_output_events(\n                        source='objects',\n                        key=key,\n                        val=val,\n                        line='2',\n                        hr=hr,\n                        show_name=show_name,\n                        colorize=colorize\n                    )\n\n                elif key == 'contact':\n\n                    output += generate_output(\n                        line='2',\n                        short=HR_RDAP['objects']['contact'][\n                            '_short'] if hr else 'contact',\n                        name=HR_RDAP['objects']['contact']['_name'] if (\n                            hr and show_name) else None,\n                        is_parent=False if (val is None or\n                                            len(val) == 0) else True,\n                        value='None' if (val is None or\n                                         len(val) == 0) else None,\n                        colorize=colorize\n                    )\n\n                    if val is not None:\n\n                        for k, v in val.items():\n\n                            if k in ['phone', 'address', 'email']:\n\n                                output += generate_output(\n                                    line='3',\n                                    short=HR_RDAP['objects']['contact'][k][\n                                        '_short'] if hr else k,\n                                    name=HR_RDAP['objects']['contact'][k][\n                                        '_name'] if (\n                                        hr and show_name) else None,\n                                    is_parent=False if (\n                                        val is None or\n                                        len(val) == 0\n                                    ) else True,\n                                    value='None' if (val is None or\n                                                     len(val) == 0) else None,\n                                    colorize=colorize\n                                )\n\n                                if v is not None:\n                                    for item in v:\n                                        i_type = ', '.join(item['type']) if (\n                                            isinstance(item['type'], list)\n                                        ) else item['type']\n\n                                        i_type = i_type if (\n                                            i_type is not None and\n                                            len(i_type) > 0) else ''\n\n                                        i_value = item['value'].replace(\n                                            '\\n',\n                                            '\\n{0}'.format(\n                                                generate_output(\n                                                    line='4',\n                                                    is_parent=True,\n                                                    colorize=colorize\n                                                ).replace('\\n', ''))\n                                        )\n\n                                        tmp_out = '{0}{1}{2}'.format(\n                                            i_type,\n                                            ': ' if i_type != '' else '',\n                                            i_value\n                                        )\n\n                                        output += generate_output(\n                                            line='4',\n                                            value=tmp_out,\n                                            colorize=colorize\n                                        )\n\n                            else:\n\n                                output += generate_output(\n                                    line='3',\n                                    short=HR_RDAP['objects']['contact'][k][\n                                        '_short'] if hr else k,\n                                    name=HR_RDAP['objects']['contact'][k][\n                                        '_name'] if (\n                                        hr and show_name) else None,\n                                    value=v,\n                                    colorize=colorize\n                                )\n\n                elif key not in ['raw']:\n\n                    output += generate_output(\n                        line='2',\n                        short=HR_RDAP['objects'][key]['_short'] if hr else key,\n                        name=HR_RDAP['objects'][key]['_name'] if (\n                            hr and show_name) else None,\n                        value=val,\n                        colorize=colorize\n                    )\n\n        return output", "response": "This function is used to generate CLI output RDAP object results."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lookup_rdap(self, hr=True, show_name=False, colorize=True, **kwargs):\n\n        # Perform the RDAP lookup\n        ret = self.obj.lookup_rdap(**kwargs)\n\n        if script_args.json:\n\n            output = json.dumps(ret)\n\n        else:\n\n            # Header\n            output = self.generate_output_header(query_type='RDAP')\n\n            # ASN\n            output += self.generate_output_asn(\n                json_data=ret, hr=hr, show_name=show_name, colorize=colorize\n            )\n            output += self.generate_output_newline(colorize=colorize)\n\n            # Entities\n            output += self.generate_output_entities(\n                json_data=ret, hr=hr, show_name=show_name, colorize=colorize\n            )\n            output += self.generate_output_newline(colorize=colorize)\n\n            # Network\n            output += self.generate_output_network(\n                json_data=ret, hr=hr, show_name=show_name, colorize=colorize\n            )\n            output += self.generate_output_newline(colorize=colorize)\n\n            # Objects\n            output += self.generate_output_objects(\n                json_data=ret, hr=hr, show_name=show_name, colorize=colorize\n            )\n            output += self.generate_output_newline(colorize=colorize)\n\n            if 'nir' in ret:\n\n                # NIR\n                output += self.generate_output_nir(\n                    json_data=ret, hr=hr, show_name=show_name,\n                    colorize=colorize\n                )\n                output += self.generate_output_newline(colorize=colorize)\n\n        return output", "response": "This function is wrapping IPWhois. lookup_rdap and generating the formatted CLI output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_output_whois_nets(self, json_data=None, hr=True,\n                                   show_name=False, colorize=True):\n        \"\"\"\n        The function for generating CLI output Legacy Whois networks results.\n\n        Args:\n            json_data (:obj:`dict`): The data to process. Defaults to None.\n            hr (:obj:`bool`): Enable human readable key translations. Defaults\n                to True.\n            show_name (:obj:`bool`): Show human readable name (default is to\n                only show short). Defaults to False.\n            colorize (:obj:`bool`): Colorize the console output with ANSI\n                colors. Defaults to True.\n\n        Returns:\n            str: The generated output.\n        \"\"\"\n\n        if json_data is None:\n            json_data = {}\n\n        output = generate_output(\n            line='0',\n            short=HR_WHOIS['nets']['_short'] if hr else 'nets',\n            name=HR_WHOIS['nets']['_name'] if (hr and show_name) else None,\n            is_parent=True,\n            colorize=colorize\n        )\n\n        count = 0\n        for net in json_data['nets']:\n            if count > 0:\n                output += self.generate_output_newline(\n                    line='1',\n                    colorize=colorize\n                )\n            count += 1\n\n            output += generate_output(\n                line='1',\n                short=net['handle'],\n                is_parent=True,\n                colorize=colorize\n            )\n\n            for key, val in net.items():\n\n                if val and '\\n' in val:\n\n                    output += generate_output(\n                        line='2',\n                        short=HR_WHOIS['nets'][key]['_short'] if hr else key,\n                        name=HR_WHOIS['nets'][key]['_name'] if (\n                            hr and show_name) else None,\n                        is_parent=False if (val is None or\n                                            len(val) == 0) else True,\n                        value='None' if (val is None or\n                                         len(val) == 0) else None,\n                        colorize=colorize\n                    )\n\n                    for v in val.split('\\n'):\n                        output += generate_output(\n                            line='3',\n                            value=v,\n                            colorize=colorize\n                        )\n\n                else:\n\n                    output += generate_output(\n                        line='2',\n                        short=HR_WHOIS['nets'][key]['_short'] if hr else key,\n                        name=HR_WHOIS['nets'][key]['_name'] if (\n                            hr and show_name) else None,\n                        value=val,\n                        colorize=colorize\n                    )\n\n        return output", "response": "This function is used to generate CLI output for Legacy Whois networks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup_whois(self, hr=True, show_name=False, colorize=True, **kwargs):\n\n        # Perform the RDAP lookup\n        ret = self.obj.lookup_whois(**kwargs)\n\n        if script_args.json:\n\n            output = json.dumps(ret)\n\n        else:\n\n            # Header\n            output = self.generate_output_header(query_type='Legacy Whois')\n\n            # ASN\n            output += self.generate_output_asn(\n                json_data=ret, hr=hr, show_name=show_name, colorize=colorize\n            )\n            output += self.generate_output_newline(colorize=colorize)\n\n            # Network\n            output += self.generate_output_whois_nets(\n                json_data=ret, hr=hr, show_name=show_name, colorize=colorize\n            )\n            output += self.generate_output_newline(colorize=colorize)\n\n            # Referral\n            output += self.generate_output_whois_referral(\n                json_data=ret, hr=hr, show_name=show_name, colorize=colorize\n            )\n            output += self.generate_output_newline(colorize=colorize)\n\n            if 'nir' in ret:\n\n                # NIR\n                output += self.generate_output_nir(\n                    json_data=ret, hr=hr, show_name=show_name,\n                    colorize=colorize\n                )\n                output += self.generate_output_newline(colorize=colorize)\n\n        return output", "response": "This function is wrapping the IPWhois. lookup_whois method and generating the formatted CLI output."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_fields_dns(self, *args, **kwargs):\n\n        from warnings import warn\n        warn('IPASN._parse_fields_dns() has been deprecated and will be '\n             'removed. You should now use IPASN.parse_fields_dns().')\n        return self.parse_fields_dns(*args, **kwargs)", "response": "Deprecated. Use parse_fields_dns instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_fields_whois(self, response):\n\n        try:\n\n            temp = response.split('|')\n\n            # Parse out the ASN information.\n            ret = {'asn_registry': temp[4].strip(' \\n')}\n\n            if ret['asn_registry'] not in self.rir_whois.keys():\n\n                raise ASNRegistryError(\n                    'ASN registry {0} is not known.'.format(\n                        ret['asn_registry'])\n                )\n\n            ret['asn'] = temp[0].strip(' \\n')\n            ret['asn_cidr'] = temp[2].strip(' \\n')\n            ret['asn_country_code'] = temp[3].strip(' \\n').upper()\n            ret['asn_date'] = temp[5].strip(' \\n')\n            ret['asn_description'] = temp[6].strip(' \\n')\n\n        except ASNRegistryError:\n\n            raise\n\n        except Exception as e:\n\n            raise ASNParseError('Parsing failed for \"{0}\" with exception: {1}.'\n                                ''.format(response, e)[:100])\n\n        return ret", "response": "This function is used to parse the ASN fields from a whois response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_fields_whois(self, *args, **kwargs):\n\n        from warnings import warn\n        warn('IPASN._parse_fields_whois() has been deprecated and will be '\n             'removed. You should now use IPASN.parse_fields_whois().')\n        return self.parse_fields_whois(*args, **kwargs)", "response": "Deprecated. Use parse_fields_whois instead."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_fields_http(self, response, extra_org_map=None):\n\n        # Set the org_map. Map the orgRef handle to an RIR.\n        org_map = self.org_map.copy()\n        try:\n\n            org_map.update(extra_org_map)\n\n        except (TypeError, ValueError, IndexError, KeyError):\n\n            pass\n\n        try:\n\n            asn_data = {\n                'asn_registry': None,\n                'asn': None,\n                'asn_cidr': None,\n                'asn_country_code': None,\n                'asn_date': None,\n                'asn_description': None\n            }\n\n            try:\n\n                net_list = response['nets']['net']\n\n                if not isinstance(net_list, list):\n                    net_list = [net_list]\n\n            except (KeyError, TypeError):\n\n                log.debug('No networks found')\n                net_list = []\n\n            for n in reversed(net_list):\n\n                try:\n\n                    asn_data['asn_registry'] = (\n                        org_map[n['orgRef']['@handle'].upper()]\n                    )\n\n                except KeyError as e:\n\n                    log.debug('Could not parse ASN registry via HTTP: '\n                              '{0}'.format(str(e)))\n                    continue\n\n                break\n\n            if not asn_data['asn_registry']:\n\n                log.debug('Could not parse ASN registry via HTTP')\n                raise ASNRegistryError('ASN registry lookup failed.')\n\n        except ASNRegistryError:\n\n            raise\n\n        except Exception as e:  # pragma: no cover\n\n            raise ASNParseError('Parsing failed for \"{0}\" with exception: {1}.'\n                                ''.format(response, e)[:100])\n\n        return asn_data", "response": "This function is used to parse the ASN fields from an HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup(self, inc_raw=False, retry_count=3, asn_alts=None,\n               extra_org_map=None, asn_methods=None,\n               get_asn_description=True):\n        \"\"\"\n        The wrapper function for retrieving and parsing ASN information for an\n        IP address.\n\n        Args:\n            inc_raw (:obj:`bool`): Whether to include the raw results in the\n                returned dictionary. Defaults to False.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            asn_alts (:obj:`list`): Additional lookup types to attempt if the\n                ASN dns lookup fails. Allow permutations must be enabled.\n                Defaults to all ['whois', 'http']. *WARNING* deprecated in\n                favor of new argument asn_methods. Defaults to None.\n            extra_org_map (:obj:`dict`): Mapping org handles to RIRs. This is\n                for limited cases where ARIN REST (ASN fallback HTTP lookup)\n                does not show an RIR as the org handle e.g., DNIC (which is\n                now the built in ORG_MAP) e.g., {'DNIC': 'arin'}. Valid RIR\n                values are (note the case-sensitive - this is meant to match\n                the REST result): 'ARIN', 'RIPE', 'apnic', 'lacnic', 'afrinic'\n                Defaults to None.\n            asn_methods (:obj:`list`): ASN lookup types to attempt, in order.\n                If None, defaults to all: ['dns', 'whois', 'http'].\n            get_asn_description (:obj:`bool`): Whether to run an additional\n                query when pulling ASN information via dns, in order to get\n                the ASN description. Defaults to True.\n\n        Returns:\n            dict: The ASN lookup results\n\n            ::\n\n                {\n                    'asn' (str) - The Autonomous System Number\n                    'asn_date' (str) - The ASN Allocation date\n                    'asn_registry' (str) - The assigned ASN registry\n                    'asn_cidr' (str) - The assigned ASN CIDR\n                    'asn_country_code' (str) - The assigned ASN country code\n                    'asn_description' (str) - The ASN description\n                    'raw' (str) - Raw ASN results if the inc_raw parameter is\n                        True.\n                }\n\n        Raises:\n            ValueError: methods argument requires one of dns, whois, http.\n            ASNRegistryError: ASN registry does not match.\n        \"\"\"\n\n        if asn_methods is None:\n\n            if asn_alts is None:\n\n                lookups = ['dns', 'whois', 'http']\n\n            else:\n\n                from warnings import warn\n                warn('IPASN.lookup() asn_alts argument has been deprecated '\n                     'and will be removed. You should now use the asn_methods '\n                     'argument.')\n                lookups = ['dns'] + asn_alts\n\n        else:\n\n            if {'dns', 'whois', 'http'}.isdisjoint(asn_methods):\n\n                raise ValueError('methods argument requires at least one of '\n                                 'dns, whois, http.')\n\n            lookups = asn_methods\n\n        response = None\n        asn_data = None\n        dns_success = False\n        for index, lookup_method in enumerate(lookups):\n\n            if index > 0 and not asn_methods and not (\n                    self._net.allow_permutations):\n\n                raise ASNRegistryError('ASN registry lookup failed. '\n                                       'Permutations not allowed.')\n\n            if lookup_method == 'dns':\n\n                try:\n\n                    self._net.dns_resolver.lifetime = (\n                        self._net.dns_resolver.timeout * (\n                            retry_count and retry_count or 1\n                        )\n                    )\n                    response = self._net.get_asn_dns()\n                    asn_data_list = []\n                    for asn_entry in response:\n\n                        asn_data_list.append(self.parse_fields_dns(\n                            str(asn_entry)))\n\n                    # Iterate through the parsed ASN results to find the\n                    # smallest CIDR\n                    asn_data = asn_data_list.pop(0)\n                    try:\n\n                        prefix_len = ip_network(asn_data['asn_cidr']).prefixlen\n                        for asn_parsed in asn_data_list:\n                            prefix_len_comp = ip_network(\n                                asn_parsed['asn_cidr']).prefixlen\n                            if prefix_len_comp > prefix_len:\n                                asn_data = asn_parsed\n                                prefix_len = prefix_len_comp\n\n                    except (KeyError, ValueError):  # pragma: no cover\n\n                        pass\n\n                    dns_success = True\n                    break\n\n                except (ASNLookupError, ASNRegistryError) as e:\n\n                    log.debug('ASN DNS lookup failed: {0}'.format(e))\n                    pass\n\n            elif lookup_method == 'whois':\n\n                try:\n\n                    response = self._net.get_asn_whois(retry_count)\n                    asn_data = self.parse_fields_whois(\n                        response)  # pragma: no cover\n                    break\n\n                except (ASNLookupError, ASNRegistryError) as e:\n\n                    log.debug('ASN WHOIS lookup failed: {0}'.format(e))\n                    pass\n\n            elif lookup_method == 'http':\n\n                try:\n\n                    response = self._net.get_asn_http(\n                        retry_count=retry_count\n                    )\n                    asn_data = self.parse_fields_http(response,\n                                                       extra_org_map)\n                    break\n\n                except (ASNLookupError, ASNRegistryError) as e:\n\n                    log.debug('ASN HTTP lookup failed: {0}'.format(e))\n                    pass\n\n        if asn_data is None:\n\n            raise ASNRegistryError('ASN lookup failed with no more methods to '\n                                   'try.')\n\n        if get_asn_description and dns_success:\n\n            try:\n\n                response = self._net.get_asn_verbose_dns('AS{0}'.format(\n                    asn_data['asn']))\n                asn_verbose_data = self.parse_fields_verbose_dns(response)\n                asn_data['asn_description'] = asn_verbose_data[\n                    'asn_description']\n\n            except (ASNLookupError, ASNRegistryError) as e:  # pragma: no cover\n\n                log.debug('ASN DNS verbose lookup failed: {0}'.format(e))\n                pass\n\n        if inc_raw:\n\n            asn_data['raw'] = response\n\n        return asn_data", "response": "This function is used to retrieve and parse the ASN information for an animal system and return the result as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_nets_radb(self, *args, **kwargs):\n\n        from warnings import warn\n        warn('ASNOrigin._get_nets_radb() has been deprecated and will be '\n             'removed. You should now use ASNOrigin.get_nets_radb().')\n        return self.get_nets_radb(*args, **kwargs)", "response": "Deprecated. Use ASNOrigin. get_nets_radb."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lookup(self, asn=None, inc_raw=False, retry_count=3, response=None,\n               field_list=None, asn_alts=None, asn_methods=None):\n        \"\"\"\n        The function for retrieving and parsing ASN origin whois information\n        via port 43/tcp (WHOIS).\n\n        Args:\n            asn (:obj:`str`): The ASN (required).\n            inc_raw (:obj:`bool`): Whether to include the raw results in the\n                returned dictionary. Defaults to False.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            response (:obj:`str`): Optional response object, this bypasses the\n                Whois lookup. Defaults to None.\n            field_list (:obj:`list`): If provided, fields to parse:\n                ['description', 'maintainer', 'updated', 'source']\n                If None, defaults to all.\n            asn_alts (:obj:`list`): Additional lookup types to attempt if the\n                ASN whois lookup fails. If None, defaults to all ['http'].\n                *WARNING* deprecated in favor of new argument asn_methods.\n            asn_methods (:obj:`list`): ASN lookup types to attempt, in order.\n                If None, defaults to all ['whois', 'http'].\n\n        Returns:\n            dict: The ASN origin lookup results\n\n            ::\n\n                {\n                    'query' (str) - The Autonomous System Number\n                    'nets' (list) - Dictionaries containing network\n                        information which consists of the fields listed in the\n                        ASN_ORIGIN_WHOIS dictionary.\n                    'raw' (str) - Raw ASN origin whois results if the inc_raw\n                        parameter is True.\n                }\n\n        Raises:\n            ValueError: methods argument requires one of whois, http.\n            ASNOriginLookupError: ASN origin lookup failed.\n        \"\"\"\n\n        if asn[0:2] != 'AS':\n\n            asn = 'AS{0}'.format(asn)\n\n        if asn_methods is None:\n\n            if asn_alts is None:\n\n                lookups = ['whois', 'http']\n\n            else:\n\n                from warnings import warn\n                warn('ASNOrigin.lookup() asn_alts argument has been deprecated'\n                     ' and will be removed. You should now use the asn_methods'\n                     ' argument.')\n                lookups = ['whois'] + asn_alts\n\n        else:\n\n            if {'whois', 'http'}.isdisjoint(asn_methods):\n\n                raise ValueError('methods argument requires at least one of '\n                                 'whois, http.')\n\n            lookups = asn_methods\n\n        # Create the return dictionary.\n        results = {\n            'query': asn,\n            'nets': [],\n            'raw': None\n        }\n\n        is_http = False\n\n        # Only fetch the response if we haven't already.\n        if response is None:\n\n            for index, lookup_method in enumerate(lookups):\n\n                if lookup_method == 'whois':\n\n                    try:\n\n                        log.debug('Response not given, perform ASN origin '\n                                  'WHOIS lookup for {0}'.format(asn))\n\n                        # Retrieve the whois data.\n                        response = self._net.get_asn_origin_whois(\n                            asn=asn, retry_count=retry_count\n                        )\n\n                    except (WhoisLookupError, WhoisRateLimitError) as e:\n\n                        log.debug('ASN origin WHOIS lookup failed: {0}'\n                                  ''.format(e))\n                        pass\n\n                elif lookup_method == 'http':\n\n                    try:\n\n                        log.debug('Response not given, perform ASN origin '\n                                  'HTTP lookup for: {0}'.format(asn))\n\n                        tmp = ASN_ORIGIN_HTTP['radb']['form_data']\n                        tmp[str(ASN_ORIGIN_HTTP['radb']['form_data_asn_field']\n                                )] = asn\n                        response = self._net.get_http_raw(\n                            url=ASN_ORIGIN_HTTP['radb']['url'],\n                            retry_count=retry_count,\n                            request_type='POST',\n                            form_data=tmp\n                        )\n                        is_http = True   # pragma: no cover\n\n                    except HTTPLookupError as e:\n\n                        log.debug('ASN origin HTTP lookup failed: {0}'\n                                  ''.format(e))\n                        pass\n\n            if response is None:\n\n                raise ASNOriginLookupError('ASN origin lookup failed with no '\n                                           'more methods to try.')\n\n        # If inc_raw parameter is True, add the response to return dictionary.\n        if inc_raw:\n\n            results['raw'] = response\n\n        nets = []\n        nets_response = self.get_nets_radb(response, is_http)\n\n        nets.extend(nets_response)\n\n        if is_http:   # pragma: no cover\n            fields = ASN_ORIGIN_HTTP\n        else:\n            fields = ASN_ORIGIN_WHOIS\n\n        # Iterate through all of the network sections and parse out the\n        # appropriate fields for each.\n        log.debug('Parsing ASN origin data')\n\n        for index, net in enumerate(nets):\n\n            section_end = None\n            if index + 1 < len(nets):\n\n                section_end = nets[index + 1]['start']\n\n            temp_net = self.parse_fields(\n                response,\n                fields['radb']['fields'],\n                section_end,\n                net['end'],\n                field_list\n            )\n\n            # Merge the net dictionaries.\n            net.update(temp_net)\n\n            # The start and end values are no longer needed.\n            del net['start'], net['end']\n\n        # Add the networks to the return dictionary.\n        results['nets'] = nets\n\n        return results", "response": "This function returns the results of a specific ASN origin whois information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_fields(self, response, fields_dict, net_start=None,\n                     net_end=None, dt_format=None, field_list=None,\n                     hourdelta=0, is_contact=False):\n        \"\"\"\n        The function for parsing whois fields from a data input.\n\n        Args:\n            response (:obj:`str`): The response from the whois/rwhois server.\n            fields_dict (:obj:`dict`): The mapping of fields to regex search\n                values (required).\n            net_start (:obj:`int`): The starting point of the network (if\n                parsing multiple networks). Defaults to None.\n            net_end (:obj:`int`): The ending point of the network (if parsing\n                multiple networks). Defaults to None.\n            dt_format (:obj:`str`): The format of datetime fields if known.\n                Defaults to None.\n            field_list (:obj:`list` of :obj:`str`): If provided, fields to\n                parse. Defaults to :obj:`ipwhois.nir.BASE_NET` if is_contact\n                is False. Otherwise, defaults to\n                :obj:`ipwhois.nir.BASE_CONTACT`.\n            hourdelta (:obj:`int`): The timezone delta for created/updated\n                fields. Defaults to 0.\n            is_contact (:obj:`bool`): If True, uses contact information\n                field parsing. Defaults to False.\n\n        Returns:\n            dict: A dictionary of fields provided in fields_dict, mapping to\n                the results of the regex searches.\n        \"\"\"\n\n        response = '{0}\\n'.format(response)\n        if is_contact:\n\n            ret = {}\n\n            if not field_list:\n\n                field_list = list(BASE_CONTACT.keys())\n\n        else:\n\n            ret = {\n                'contacts': {'admin': None, 'tech': None},\n                'contact_admin': {},\n                'contact_tech': {}\n            }\n\n            if not field_list:\n\n                field_list = list(BASE_NET.keys())\n                field_list.remove('contacts')\n                field_list.append('contact_admin')\n                field_list.append('contact_tech')\n\n        generate = ((field, pattern) for (field, pattern) in\n                    fields_dict.items() if field in field_list)\n\n        for field, pattern in generate:\n\n            pattern = re.compile(\n                str(pattern),\n                re.DOTALL\n            )\n\n            if net_start is not None:\n\n                match = pattern.finditer(response, net_end, net_start)\n\n            elif net_end is not None:\n\n                match = pattern.finditer(response, net_end)\n\n            else:\n\n                match = pattern.finditer(response)\n\n            values = []\n            for m in match:\n\n                try:\n\n                    values.append(m.group('val').strip())\n\n                except IndexError:\n\n                    pass\n\n            if len(values) > 0:\n\n                value = None\n                try:\n\n                    if field in ['created', 'updated'] and dt_format:\n\n                        value = (\n                            datetime.strptime(\n                                values[0],\n                                str(dt_format)\n                            ) - timedelta(hours=hourdelta)\n                        ).isoformat('T')\n\n                    elif field in ['nameservers']:\n\n                        value = list(unique_everseen(values))\n\n                    else:\n\n                        values = unique_everseen(values)\n                        value = '\\n'.join(values)\n\n                except ValueError as e:\n\n                    log.debug('NIR whois field parsing failed for {0}: {1}'\n                              ''.format(field, e))\n                    pass\n\n                ret[field] = value\n\n        return ret", "response": "This function parses whois fields from a response from the whois server and returns a dictionary of fields that are parsed into a dictionary of fields that are returned by the regex searches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_nets_jpnic(self, *args, **kwargs):\n\n        from warnings import warn\n        warn('NIRWhois._get_nets_jpnic() has been deprecated and will be '\n             'removed. You should now use NIRWhois.get_nets_jpnic().')\n        return self.get_nets_jpnic(*args, **kwargs)", "response": "Deprecated. Use get_nets_jpnic instead."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_contact(self, response=None, nir=None, handle=None,\n                    retry_count=3, dt_format=None):\n        \"\"\"\n        The function for retrieving and parsing NIR whois data based on\n        NIR_WHOIS contact_fields.\n\n        Args:\n            response (:obj:`str`): Optional response object, this bypasses the\n                lookup.\n            nir (:obj:`str`): The NIR to query ('jpnic' or 'krnic'). Required\n                if response is None.\n            handle (:obj:`str`): For NIRs that have separate contact queries\n                (JPNIC), this is the contact handle to use in the query.\n                Defaults to None.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            dt_format (:obj:`str`): The format of datetime fields if known.\n                Defaults to None.\n\n        Returns:\n            dict: Mapping of the fields provided in contact_fields, to their\n                parsed results.\n        \"\"\"\n\n        if response or nir == 'krnic':\n\n            contact_response = response\n\n        else:\n\n            # Retrieve the whois data.\n            contact_response = self._net.get_http_raw(\n                url=str(NIR_WHOIS[nir]['url']).format(handle),\n                retry_count=retry_count,\n                headers=NIR_WHOIS[nir]['request_headers'],\n                request_type=NIR_WHOIS[nir]['request_type']\n            )\n\n        return self.parse_fields(\n            response=contact_response,\n            fields_dict=NIR_WHOIS[nir]['contact_fields'],\n            dt_format=dt_format,\n            hourdelta=int(NIR_WHOIS[nir]['dt_hourdelta']),\n            is_contact=True\n        )", "response": "This function returns the NIR whois data from the NIR whois server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lookup(self, nir=None, inc_raw=False, retry_count=3, response=None,\n               field_list=None, is_offline=False):\n        \"\"\"\n        The function for retrieving and parsing NIR whois information for an IP\n        address via HTTP (HTML scraping).\n\n        Args:\n            nir (:obj:`str`): The NIR to query ('jpnic' or 'krnic'). Required\n                if response is None.\n            inc_raw (:obj:`bool`, optional): Whether to include the raw\n                results in the returned dictionary. Defaults to False.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            response (:obj:`str`): Optional response object, this bypasses the\n                NIR lookup. Required when is_offline=True.\n            field_list (:obj:`list` of :obj:`str`): If provided, fields to\n                parse. Defaults to :obj:`ipwhois.nir.BASE_NET`.\n            is_offline (:obj:`bool`): Whether to perform lookups offline. If\n                True, response and asn_data must be provided. Primarily used\n                for testing.\n\n        Returns:\n            dict: The NIR whois results:\n\n            ::\n\n                {\n                    'query' (str) - The IP address.\n                    'nets' (list of dict) - Network information which consists\n                        of the fields listed in the ipwhois.nir.NIR_WHOIS\n                        dictionary.\n                    'raw' (str) - Raw NIR whois results if the inc_raw\n                        parameter is True.\n                }\n        \"\"\"\n\n        if nir not in NIR_WHOIS.keys():\n\n            raise KeyError('Invalid arg for nir (National Internet Registry')\n\n        # Create the return dictionary.\n        results = {\n            'query': self._net.address_str,\n            'raw': None\n        }\n\n        # Only fetch the response if we haven't already.\n        if response is None:\n\n            if is_offline:\n\n                raise KeyError('response argument required when '\n                               'is_offline=True')\n\n            log.debug('Response not given, perform WHOIS lookup for {0}'\n                      .format(self._net.address_str))\n\n            form_data = None\n            if NIR_WHOIS[nir]['form_data_ip_field']:\n                form_data = {NIR_WHOIS[nir]['form_data_ip_field']:\n                             self._net.address_str}\n\n            # Retrieve the whois data.\n            response = self._net.get_http_raw(\n                url=str(NIR_WHOIS[nir]['url']).format(self._net.address_str),\n                retry_count=retry_count,\n                headers=NIR_WHOIS[nir]['request_headers'],\n                request_type=NIR_WHOIS[nir]['request_type'],\n                form_data=form_data\n            )\n\n        # If inc_raw parameter is True, add the response to return dictionary.\n        if inc_raw:\n\n            results['raw'] = response\n\n        nets = []\n        nets_response = None\n        if nir == 'jpnic':\n\n            nets_response = self.get_nets_jpnic(response)\n\n        elif nir == 'krnic':\n\n            nets_response = self.get_nets_krnic(response)\n\n        nets.extend(nets_response)\n\n        global_contacts = {}\n\n        # Iterate through all of the network sections and parse out the\n        # appropriate fields for each.\n        log.debug('Parsing NIR WHOIS data')\n        for index, net in enumerate(nets):\n\n            section_end = None\n            if index + 1 < len(nets):\n                section_end = nets[index + 1]['start']\n\n            try:\n\n                dt_format = NIR_WHOIS[nir]['dt_format']\n\n            except KeyError:  # pragma: no cover\n\n                dt_format = None\n\n            temp_net = self.parse_fields(\n                response=response,\n                fields_dict=NIR_WHOIS[nir]['fields'],\n                net_start=section_end,\n                net_end=net['end'],\n                dt_format=dt_format,\n                field_list=field_list,\n                hourdelta=int(NIR_WHOIS[nir]['dt_hourdelta'])\n            )\n            temp_net['country'] = NIR_WHOIS[nir]['country_code']\n            contacts = {\n                'admin': temp_net['contact_admin'],\n                'tech': temp_net['contact_tech']\n            }\n\n            del (\n                temp_net['contact_admin'],\n                temp_net['contact_tech']\n            )\n\n            if not is_offline:\n\n                for key, val in contacts.items():\n\n                    if len(val) > 0:\n\n                        if isinstance(val, str):\n\n                            val = val.splitlines()\n\n                        for contact in val:\n\n                            if contact in global_contacts.keys():\n\n                                temp_net['contacts'][key] = (\n                                    global_contacts[contact]\n                                )\n\n                            else:\n\n                                if nir == 'krnic':\n\n                                    tmp_response = contact\n                                    tmp_handle = None\n\n                                else:\n\n                                    tmp_response = None\n                                    tmp_handle = contact\n\n                                temp_net['contacts'][key] = self.get_contact(\n                                    response=tmp_response,\n                                    handle=tmp_handle,\n                                    nir=nir,\n                                    retry_count=retry_count,\n                                    dt_format=dt_format\n                                )\n                                global_contacts[contact] = (\n                                    temp_net['contacts'][key]\n                                )\n\n            # Merge the net dictionaries.\n            net.update(temp_net)\n\n            # The start and end values are no longer needed.\n            del net['start'], net['end']\n\n        # Add the networks to the return dictionary.\n        results['nets'] = nets\n\n        return results", "response": "This function returns the results of a NIR whois lookup request for an IP."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bulk_lookup_rdap(addresses=None, inc_raw=False, retry_count=3, depth=0,\n                     excluded_entities=None, rate_limit_timeout=60,\n                     socket_timeout=10, asn_timeout=240, proxy_openers=None):\n    \"\"\"\n    The function for bulk retrieving and parsing whois information for a list\n    of IP addresses via HTTP (RDAP). This bulk lookup method uses bulk\n    ASN Whois lookups first to retrieve the ASN for each IP. It then optimizes\n    RDAP queries to achieve the fastest overall time, accounting for\n    rate-limiting RIRs.\n\n    Args:\n        addresses (:obj:`list` of :obj:`str`): IP addresses to lookup.\n        inc_raw (:obj:`bool`, optional): Whether to include the raw whois\n            results in the returned dictionary. Defaults to False.\n        retry_count (:obj:`int`): The number of times to retry in case socket\n            errors, timeouts, connection resets, etc. are encountered.\n            Defaults to 3.\n        depth (:obj:`int`): How many levels deep to run queries when additional\n            referenced objects are found. Defaults to 0.\n        excluded_entities (:obj:`list` of :obj:`str`): Entity handles to not\n            perform lookups. Defaults to None.\n        rate_limit_timeout (:obj:`int`): The number of seconds to wait before\n            retrying when a rate limit notice is returned via rdap+json.\n            Defaults to 60.\n        socket_timeout (:obj:`int`): The default timeout for socket\n            connections in seconds. Defaults to 10.\n        asn_timeout (:obj:`int`): The default timeout for bulk ASN lookups in\n            seconds. Defaults to 240.\n        proxy_openers (:obj:`list` of :obj:`OpenerDirector`): Proxy openers\n            for single/rotating proxy support. Defaults to None.\n\n    Returns:\n        namedtuple:\n\n        :results (dict): IP address keys with the values as dictionaries\n            returned by IPWhois.lookup_rdap().\n        :stats (dict): Stats for the lookups:\n\n        ::\n\n            {\n                'ip_input_total' (int) - The total number of addresses\n                    originally provided for lookup via the addresses argument.\n                'ip_unique_total' (int) - The total number of unique addresses\n                    found in the addresses argument.\n                'ip_lookup_total' (int) - The total number of addresses that\n                    lookups were attempted for, excluding any that failed ASN\n                    registry checks.\n                'lacnic' (dict) -\n                {\n                    'failed' (list) - The addresses that failed to lookup.\n                        Excludes any that failed initially, but succeeded after\n                        futher retries.\n                    'rate_limited' (list) - The addresses that encountered\n                        rate-limiting. Unless an address is also in 'failed',\n                        it eventually succeeded.\n                    'total' (int) - The total number of addresses belonging to\n                        this RIR that lookups were attempted for.\n                }\n                'ripencc' (dict) - Same as 'lacnic' above.\n                'apnic' (dict) - Same as 'lacnic' above.\n                'afrinic' (dict) - Same as 'lacnic' above.\n                'arin' (dict) - Same as 'lacnic' above.\n                'unallocated_addresses' (list) - The addresses that are\n                    unallocated/failed ASN lookups. These can be addresses that\n                    are not listed for one of the 5 RIRs (other). No attempt\n                    was made to perform an RDAP lookup for these.\n            }\n\n    Raises:\n        ASNLookupError: The ASN bulk lookup failed, cannot proceed with bulk\n            RDAP lookup.\n    \"\"\"\n\n    if not isinstance(addresses, list):\n\n        raise ValueError('addresses must be a list of IP address strings')\n\n    # Initialize the dicts/lists\n    results = {}\n    failed_lookups_dict = {}\n    rated_lookups = []\n    stats = {\n        'ip_input_total': len(addresses),\n        'ip_unique_total': 0,\n        'ip_lookup_total': 0,\n        'lacnic': {'failed': [], 'rate_limited': [], 'total': 0},\n        'ripencc': {'failed': [], 'rate_limited': [], 'total': 0},\n        'apnic': {'failed': [], 'rate_limited': [], 'total': 0},\n        'afrinic': {'failed': [], 'rate_limited': [], 'total': 0},\n        'arin': {'failed': [], 'rate_limited': [], 'total': 0},\n        'unallocated_addresses': []\n    }\n    asn_parsed_results = {}\n\n    if proxy_openers is None:\n\n        proxy_openers = [None]\n\n    proxy_openers_copy = iter(proxy_openers)\n\n    # Make sure addresses is unique\n    unique_ip_list = list(unique_everseen(addresses))\n\n    # Get the unique count to return\n    stats['ip_unique_total'] = len(unique_ip_list)\n\n    # This is needed for iteration order\n    rir_keys_ordered = ['lacnic', 'ripencc', 'apnic', 'afrinic', 'arin']\n\n    # First query the ASN data for all IPs, can raise ASNLookupError, no catch\n    bulk_asn = get_bulk_asn_whois(unique_ip_list, timeout=asn_timeout)\n\n    # ASN results are returned as string, parse lines to list and remove first\n    asn_result_list = bulk_asn.split('\\n')\n    del asn_result_list[0]\n\n    # We need to instantiate IPASN, which currently needs a Net object,\n    # IP doesn't matter here\n    net = Net('1.2.3.4')\n    ipasn = IPASN(net)\n\n    # Iterate each IP ASN result, and add valid RIR results to\n    # asn_parsed_results for RDAP lookups\n    for asn_result in asn_result_list:\n\n        temp = asn_result.split('|')\n\n        # Not a valid entry, move on to next\n        if len(temp) == 1:\n\n            continue\n\n        ip = temp[1].strip()\n\n        # We need this since ASN bulk lookup is returning duplicates\n        # This is an issue on the Cymru end\n        if ip in asn_parsed_results.keys():  # pragma: no cover\n\n            continue\n\n        try:\n\n            results = ipasn.parse_fields_whois(asn_result)\n\n        except ASNRegistryError:  # pragma: no cover\n\n            continue\n\n        # Add valid IP ASN result to asn_parsed_results for RDAP lookup\n        asn_parsed_results[ip] = results\n        stats[results['asn_registry']]['total'] += 1\n\n    # Set the list of IPs that are not allocated/failed ASN lookup\n    stats['unallocated_addresses'] = list(k for k in addresses if k not in\n                                          asn_parsed_results)\n\n    # Set the total lookup count after unique IP and ASN result filtering\n    stats['ip_lookup_total'] = len(asn_parsed_results)\n\n    # Track the total number of LACNIC queries left. This is tracked in order\n    # to ensure the 9 priority LACNIC queries/min don't go into infinite loop\n    lacnic_total_left = stats['lacnic']['total']\n\n    # Set the start time, this value is updated when the rate limit is reset\n    old_time = time.time()\n\n    # Rate limit tracking dict for all RIRs\n    rate_tracker = {\n        'lacnic': {'time': old_time, 'count': 0},\n        'ripencc': {'time': old_time, 'count': 0},\n        'apnic': {'time': old_time, 'count': 0},\n        'afrinic': {'time': old_time, 'count': 0},\n        'arin': {'time': old_time, 'count': 0}\n    }\n\n    # Iterate all of the IPs to perform RDAP lookups until none are left\n    while len(asn_parsed_results) > 0:\n\n        # Sequentially run through each RIR to minimize lookups in a row to\n        # the same RIR.\n        for rir in rir_keys_ordered:\n\n            # If there are still LACNIC IPs left to lookup and the rate limit\n            # hasn't been reached, skip to find a LACNIC IP to lookup\n            if (\n                rir != 'lacnic' and lacnic_total_left > 0 and\n                (rate_tracker['lacnic']['count'] != 9 or\n                    (time.time() - rate_tracker['lacnic']['time']\n                     ) >= rate_limit_timeout\n                 )\n               ):  # pragma: no cover\n\n                continue\n\n            # If the RIR rate limit has been reached and hasn't expired,\n            # move on to the next RIR\n            if (\n                rate_tracker[rir]['count'] == 9 and (\n                    (time.time() - rate_tracker[rir]['time']\n                     ) < rate_limit_timeout)\n               ):  # pragma: no cover\n\n                continue\n\n            # If the RIR rate limit has expired, reset the count/timer\n            # and perform the lookup\n            elif ((time.time() - rate_tracker[rir]['time']\n                   ) >= rate_limit_timeout):  # pragma: no cover\n\n                rate_tracker[rir]['count'] = 0\n                rate_tracker[rir]['time'] = time.time()\n\n            # Create a copy of the lookup IP dict so we can modify on\n            # successful/failed queries. Loop each IP until it matches the\n            # correct RIR in the parent loop, and attempt lookup\n            tmp_dict = asn_parsed_results.copy()\n\n            for ip, asn_data in tmp_dict.items():\n\n                # Check to see if IP matches parent loop RIR for lookup\n                if asn_data['asn_registry'] == rir:\n\n                    log.debug('Starting lookup for IP: {0} '\n                              'RIR: {1}'.format(ip, rir))\n\n                    # Add to count for rate-limit tracking only for LACNIC,\n                    # since we have not seen aggressive rate-limiting from the\n                    # other RIRs yet\n                    if rir == 'lacnic':\n\n                        rate_tracker[rir]['count'] += 1\n\n                    # Get the next proxy opener to use, or None\n                    try:\n\n                        opener = next(proxy_openers_copy)\n\n                    # Start at the beginning if all have been used\n                    except StopIteration:\n\n                        proxy_openers_copy = iter(proxy_openers)\n                        opener = next(proxy_openers_copy)\n\n                    # Instantiate the objects needed for the RDAP lookup\n                    net = Net(ip, timeout=socket_timeout, proxy_opener=opener)\n                    rdap = RDAP(net)\n\n                    try:\n\n                        # Perform the RDAP lookup. retry_count is set to 0\n                        # here since we handle that in this function\n                        results = rdap.lookup(\n                            inc_raw=inc_raw, retry_count=0, asn_data=asn_data,\n                            depth=depth, excluded_entities=excluded_entities\n                        )\n\n                        log.debug('Successful lookup for IP: {0} '\n                                  'RIR: {1}'.format(ip, rir))\n\n                        # Lookup was successful, add to result. Set the nir\n                        # key to None as this is not supported\n                        # (yet - requires more queries)\n                        results[ip] = results\n                        results[ip]['nir'] = None\n\n                        # Remove the IP from the lookup queue\n                        del asn_parsed_results[ip]\n\n                        # If this was LACNIC IP, reduce the total left count\n                        if rir == 'lacnic':\n\n                            lacnic_total_left -= 1\n\n                        log.debug(\n                            '{0} total lookups left, {1} LACNIC lookups left'\n                            ''.format(str(len(asn_parsed_results)),\n                                      str(lacnic_total_left))\n                        )\n\n                        # If this IP failed previously, remove it from the\n                        # failed return dict\n                        if (\n                            ip in failed_lookups_dict.keys()\n                        ):  # pragma: no cover\n\n                            del failed_lookups_dict[ip]\n\n                        # Break out of the IP list loop, we need to change to\n                        # the next RIR\n                        break\n\n                    except HTTPLookupError:  # pragma: no cover\n\n                        log.debug('Failed lookup for IP: {0} '\n                                  'RIR: {1}'.format(ip, rir))\n\n                        # Add the IP to the failed lookups dict if not there\n                        if ip not in failed_lookups_dict.keys():\n\n                            failed_lookups_dict[ip] = 1\n\n                        # This IP has already failed at least once, increment\n                        # the failure count until retry_count reached, then\n                        # stop trying\n                        else:\n\n                            failed_lookups_dict[ip] += 1\n\n                            if failed_lookups_dict[ip] == retry_count:\n\n                                del asn_parsed_results[ip]\n                                stats[rir]['failed'].append(ip)\n\n                                if rir == 'lacnic':\n\n                                    lacnic_total_left -= 1\n\n                        # Since this IP failed, we don't break to move to next\n                        # RIR, we check the next IP for this RIR\n                        continue\n\n                    except HTTPRateLimitError:  # pragma: no cover\n\n                        # Add the IP to the rate-limited lookups dict if not\n                        # there\n                        if ip not in rated_lookups:\n\n                            rated_lookups.append(ip)\n                            stats[rir]['rate_limited'].append(ip)\n\n                        log.debug('Rate limiting triggered for IP: {0} '\n                                  'RIR: {1}'.format(ip, rir))\n\n                        # Since rate-limit was reached, reset the timer and\n                        # max out the count\n                        rate_tracker[rir]['time'] = time.time()\n                        rate_tracker[rir]['count'] = 9\n\n                        # Break out of the IP list loop, we need to change to\n                        # the next RIR\n                        break\n\n    return_tuple = namedtuple('return_tuple', ['results', 'stats'])\n    return return_tuple(results, stats)", "response": "This function is used to perform bulk lookups on a list of IP addresses via HTTP. It uses the bulk lookup method to retrieve the ASN for each IP and parse the results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_address(self, val):\n\n        ret = {\n            'type': None,\n            'value': None\n        }\n\n        try:\n\n            ret['type'] = val[1]['type']\n\n        except (KeyError, ValueError, TypeError):\n\n                pass\n\n        try:\n\n            ret['value'] = val[1]['label']\n\n        except (KeyError, ValueError, TypeError):\n\n            ret['value'] = '\\n'.join(val[3]).strip()\n\n        try:\n\n            self.vars['address'].append(ret)\n\n        except AttributeError:\n\n            self.vars['address'] = []\n            self.vars['address'].append(ret)", "response": "This function is used to parse the vcard address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_email(self, val):\n\n        ret = {\n            'type': None,\n            'value': None\n        }\n\n        try:\n\n            ret['type'] = val[1]['type']\n\n        except (KeyError, ValueError, TypeError):\n\n                pass\n\n        ret['value'] = val[3].strip()\n\n        try:\n\n            self.vars['email'].append(ret)\n\n        except AttributeError:\n\n            self.vars['email'] = []\n            self.vars['email'].append(ret)", "response": "This function is used to parse the vcard email addresses."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self):\n\n        keys = {\n            'fn': self._parse_name,\n            'kind': self._parse_kind,\n            'adr': self._parse_address,\n            'tel': self._parse_phone,\n            'email': self._parse_email,\n            'role': self._parse_role,\n            'title': self._parse_title\n        }\n\n        for val in self.vcard:\n\n            try:\n\n                parser = keys.get(val[0])\n                parser(val)\n\n            except (KeyError, ValueError, TypeError):\n\n                pass", "response": "Parses the vcard into a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef summarize_events(self, events_json):\n\n        ret = []\n\n        for event in events_json:\n\n            event_dict = {\n                'action': event['eventAction'],\n                'timestamp': event['eventDate'],\n                'actor': None\n            }\n\n            try:\n\n                event_dict['actor'] = event['eventActor']\n\n            except (KeyError, ValueError, TypeError):\n\n                pass\n\n            ret.append(event_dict)\n\n        return ret", "response": "This function is used to summarize RDAP events in to a list of dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup(self, inc_raw=False, retry_count=3, asn_data=None, depth=0,\n               excluded_entities=None, response=None, bootstrap=False,\n               rate_limit_timeout=120):\n        \"\"\"\n        The function for retrieving and parsing information for an IP\n        address via RDAP (HTTP).\n\n        Args:\n            inc_raw (:obj:`bool`, optional): Whether to include the raw\n                results in the returned dictionary. Defaults to False.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            asn_data (:obj:`dict`): Result from\n                :obj:`ipwhois.asn.IPASN.lookup`. Optional if the bootstrap\n                parameter is True.\n            depth (:obj:`int`): How many levels deep to run queries when\n                additional referenced objects are found. Defaults to 0.\n            excluded_entities (:obj:`list`): Entity handles to not perform\n                lookups. Defaults to None.\n            response (:obj:`str`): Optional response object, this bypasses the\n                RDAP lookup.\n            bootstrap (:obj:`bool`): If True, performs lookups via ARIN\n                bootstrap rather than lookups based on ASN data. Defaults to\n                False.\n            rate_limit_timeout (:obj:`int`): The number of seconds to wait\n                before retrying when a rate limit notice is returned via\n                rdap+json. Defaults to 120.\n\n        Returns:\n            dict: The IP RDAP lookup results\n\n            ::\n\n                {\n                    'query' (str) - The IP address\n                    'entities' (list) - Entity handles referred by the top\n                        level query.\n                    'network' (dict) - Network information which consists of\n                        the fields listed in the ipwhois.rdap._RDAPNetwork\n                        dict.\n                    'objects' (dict) - Mapping of entity handle->entity dict\n                        which consists of the fields listed in the\n                        ipwhois.rdap._RDAPEntity dict. The raw result is\n                        included for each object if the inc_raw parameter\n                        is True.\n                }\n        \"\"\"\n\n        if not excluded_entities:\n\n            excluded_entities = []\n\n        # Create the return dictionary.\n        results = {\n            'query': self._net.address_str,\n            'network': None,\n            'entities': None,\n            'objects': None,\n            'raw': None\n        }\n\n        if bootstrap:\n\n            ip_url = '{0}/ip/{1}'.format(BOOTSTRAP_URL, self._net.address_str)\n\n        else:\n\n            ip_url = str(RIR_RDAP[asn_data['asn_registry']]['ip_url']).format(\n                self._net.address_str)\n\n        # Only fetch the response if we haven't already.\n        if response is None:\n\n            log.debug('Response not given, perform RDAP lookup for {0}'.format(\n                ip_url))\n\n            # Retrieve the whois data.\n            response = self._net.get_http_json(\n                url=ip_url, retry_count=retry_count,\n                rate_limit_timeout=rate_limit_timeout\n            )\n\n        if inc_raw:\n\n            results['raw'] = response\n\n        log.debug('Parsing RDAP network object')\n        result_net = _RDAPNetwork(response)\n        result_net.parse()\n        results['network'] = result_net.vars\n        results['entities'] = []\n        results['objects'] = {}\n        roles = {}\n\n        # Iterate through and parse the root level entities.\n        log.debug('Parsing RDAP root level entities')\n        try:\n\n            for ent in response['entities']:\n\n                if ent['handle'] not in [results['entities'],\n                                         excluded_entities]:\n\n                    result_ent = _RDAPEntity(ent)\n                    result_ent.parse()\n\n                    results['objects'][ent['handle']] = result_ent.vars\n\n                    results['entities'].append(ent['handle'])\n\n                    try:\n\n                        for tmp in ent['entities']:\n\n                            roles[tmp['handle']] = tmp['roles']\n\n                    except KeyError:\n\n                        pass\n\n        except KeyError:\n\n            pass\n\n        # Iterate through to the defined depth, retrieving and parsing all\n        # unique entities.\n        temp_objects = results['objects']\n\n        if depth > 0 and len(temp_objects) > 0:\n\n            log.debug('Parsing RDAP sub-entities to depth: {0}'.format(str(\n                depth)))\n\n        while depth > 0 and len(temp_objects) > 0:\n\n            new_objects = {}\n            for obj in temp_objects.values():\n\n                try:\n\n                    for ent in obj['entities']:\n\n                        if ent not in (list(results['objects'].keys()) +\n                                       list(new_objects.keys()) +\n                                       excluded_entities):\n\n                            if bootstrap:\n                                entity_url = '{0}/entity/{1}'.format(\n                                    BOOTSTRAP_URL, ent)\n                            else:\n                                tmp_reg = asn_data['asn_registry']\n                                entity_url = RIR_RDAP[tmp_reg]['entity_url']\n                                entity_url = str(entity_url).format(ent)\n\n                            try:\n\n                                # RDAP entity query\n                                response = self._net.get_http_json(\n                                    url=entity_url, retry_count=retry_count,\n                                    rate_limit_timeout=rate_limit_timeout\n                                )\n\n                                # Parse the entity\n                                result_ent = _RDAPEntity(response)\n                                result_ent.parse()\n                                new_objects[ent] = result_ent.vars\n\n                                new_objects[ent]['roles'] = None\n                                try:\n\n                                    new_objects[ent]['roles'] = roles[ent]\n\n                                except KeyError:  # pragma: no cover\n\n                                    pass\n\n                                try:\n\n                                    for tmp in response['entities']:\n\n                                        if tmp['handle'] not in roles:\n\n                                            roles[tmp['handle']] = tmp['roles']\n\n                                except (IndexError, KeyError):\n\n                                    pass\n\n                                if inc_raw:\n\n                                    new_objects[ent]['raw'] = response\n\n                            except (HTTPLookupError, InvalidEntityObject):\n\n                                pass\n\n                except TypeError:\n\n                    pass\n\n            # Update the result objects, and set the new temp object list to\n            # iterate for the next depth of entities.\n            results['objects'].update(new_objects)\n            temp_objects = new_objects\n            depth -= 1\n\n        return results", "response": "This function returns the information for an IP - related object in the base - level RDAP network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ipv4_lstrip_zeros(address):\n\n    # Split  the octets.\n    obj = address.strip().split('.')\n\n    for x, y in enumerate(obj):\n\n        # Strip leading zeros. Split / here in case CIDR is attached.\n        obj[x] = y.split('/')[0].lstrip('0')\n        if obj[x] in ['', None]:\n            obj[x] = '0'\n\n    return '.'.join(obj)", "response": "This function is used to strip leading zeros in each octet of an IPv4 address."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a named tuple that represents whether or not the given IPv4 address is defined.", "response": "def ipv4_is_defined(address):\n    \"\"\"\n    The function for checking if an IPv4 address is defined (does not need to\n    be resolved).\n\n    Args:\n        address (:obj:`str`): An IPv4 address.\n\n    Returns:\n        namedtuple:\n\n        :is_defined (bool): True if given address is defined, otherwise\n            False\n        :ietf_name (str): IETF assignment name if given address is\n            defined, otherwise ''\n        :ietf_rfc (str): IETF assignment RFC if given address is defined,\n            otherwise ''\n    \"\"\"\n\n    # Initialize the IP address object.\n    query_ip = IPv4Address(str(address))\n\n    # Initialize the results named tuple\n    results = namedtuple('ipv4_is_defined_results', 'is_defined, ietf_name, '\n                                                    'ietf_rfc')\n\n    # This Network\n    if query_ip in IPv4Network('0.0.0.0/8'):\n\n        return results(True, 'This Network', 'RFC 1122, Section 3.2.1.3')\n\n    # Loopback\n    elif query_ip.is_loopback:\n\n        return results(True, 'Loopback', 'RFC 1122, Section 3.2.1.3')\n\n    # Link Local\n    elif query_ip.is_link_local:\n\n        return results(True, 'Link Local', 'RFC 3927')\n\n    # IETF Protocol Assignments\n    elif query_ip in IPv4Network('192.0.0.0/24'):\n\n        return results(True, 'IETF Protocol Assignments', 'RFC 5736')\n\n    # TEST-NET-1\n    elif query_ip in IPv4Network('192.0.2.0/24'):\n\n        return results(True, 'TEST-NET-1', 'RFC 5737')\n\n    # 6to4 Relay Anycast\n    elif query_ip in IPv4Network('192.88.99.0/24'):\n\n        return results(True, '6to4 Relay Anycast', 'RFC 3068')\n\n    # Network Interconnect Device Benchmark Testing\n    elif query_ip in IPv4Network('198.18.0.0/15'):\n\n        return (results(True,\n                'Network Interconnect Device Benchmark Testing',\n                        'RFC 2544'))\n\n    # TEST-NET-2\n    elif query_ip in IPv4Network('198.51.100.0/24'):\n\n        return results(True, 'TEST-NET-2', 'RFC 5737')\n\n    # TEST-NET-3\n    elif query_ip in IPv4Network('203.0.113.0/24'):\n\n        return results(True, 'TEST-NET-3', 'RFC 5737')\n\n    # Multicast\n    elif query_ip.is_multicast:\n\n        return results(True, 'Multicast', 'RFC 3171')\n\n    # Limited Broadcast\n    elif query_ip in IPv4Network('255.255.255.255/32'):\n\n        return results(True, 'Limited Broadcast', 'RFC 919, Section 7')\n\n    # Private-Use Networks\n    elif query_ip.is_private:\n\n        return results(True, 'Private-Use Networks', 'RFC 1918')\n\n    # New IANA Reserved\n    # TODO: Someone needs to find the RFC for this\n    elif query_ip in IPv4Network('198.97.38.0/24'):\n\n        return results(True, 'IANA Reserved', '')\n\n    return results(False, '', '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ipv6_is_defined(address):\n\n    # Initialize the IP address object.\n    query_ip = IPv6Address(str(address))\n\n    # Initialize the results named tuple\n    results = namedtuple('ipv6_is_defined_results', 'is_defined, ietf_name, '\n                                                    'ietf_rfc')\n    # Multicast\n    if query_ip.is_multicast:\n\n        return results(True, 'Multicast', 'RFC 4291, Section 2.7')\n\n    # Unspecified\n    elif query_ip.is_unspecified:\n\n        return results(True, 'Unspecified', 'RFC 4291, Section 2.5.2')\n\n    # Loopback.\n    elif query_ip.is_loopback:\n\n        return results(True, 'Loopback', 'RFC 4291, Section 2.5.3')\n\n    # Reserved\n    elif query_ip.is_reserved:\n\n        return results(True, 'Reserved', 'RFC 4291')\n\n    # Link-Local\n    elif query_ip.is_link_local:\n\n        return results(True, 'Link-Local', 'RFC 4291, Section 2.5.6')\n\n    # Site-Local\n    elif query_ip.is_site_local:\n\n        return results(True, 'Site-Local', 'RFC 4291, Section 2.5.7')\n\n    # Unique Local Unicast\n    elif query_ip.is_private:\n\n        return results(True, 'Unique Local Unicast', 'RFC 4193')\n\n    return results(False, '', '')", "response": "Returns a function for checking if an IPv6 address is defined."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unique_everseen(iterable, key=None):\n\n    seen = set()\n    seen_add = seen.add\n\n    if key is None:\n\n        for element in filterfalse(seen.__contains__, iterable):\n\n            seen_add(element)\n            yield element\n\n    else:\n\n        for element in iterable:\n\n            k = key(element)\n\n            if k not in seen:\n\n                seen_add(k)\n                yield element", "response": "Returns a generator that yields unique elements preserving the order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ipv4_generate_random(total=100):\n\n    count = 0\n    yielded = set()\n    while count < total:\n\n        address = str(IPv4Address(random.randint(0, 2**32-1)))\n\n        if not ipv4_is_defined(address)[0] and address not in yielded:\n\n            count += 1\n            yielded.add(address)\n            yield address", "response": "Returns a generator to produce random IPv4 addresses that are not defined in the system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ipv6_generate_random(total=100):\n\n    count = 0\n    yielded = set()\n    while count < total:\n\n        address = str(IPv6Address(random.randint(0, 2**128-1)))\n\n        if not ipv6_is_defined(address)[0] and address not in yielded:\n\n            count += 1\n            yielded.add(address)\n            yield address", "response": "Returns a generator to produce random IPv6 addresses that are not there."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_fields(self, *args, **kwargs):\n\n        from warnings import warn\n        warn('Whois._parse_fields() has been deprecated and will be '\n             'removed. You should now use Whois.parse_fields().')\n        return self.parse_fields(*args, **kwargs)", "response": "Deprecated. Use Whois. parse_fields instead."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_nets_arin(self, *args, **kwargs):\n\n        from warnings import warn\n        warn('Whois._get_nets_arin() has been deprecated and will be '\n             'removed. You should now use Whois.get_nets_arin().')\n        return self.get_nets_arin(*args, **kwargs)", "response": "Deprecated. Use Whois. get_nets_arin instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nets_other(self, response):\n\n        nets = []\n\n        # Iterate through all of the networks found, storing the CIDR value\n        # and the start and end positions.\n        for match in re.finditer(\n            r'^(inetnum|inet6num|route):[^\\S\\n]+((.+?)[^\\S\\n]-[^\\S\\n](.+)|'\n                '.+)$',\n            response,\n            re.MULTILINE\n        ):\n\n            try:\n\n                net = copy.deepcopy(BASE_NET)\n                net_range = match.group(2).strip()\n\n                try:\n\n                    net['range'] = net['range'] = '{0} - {1}'.format(\n                        ip_network(net_range)[0].__str__(),\n                        ip_network(net_range)[-1].__str__()\n                    ) if '/' in net_range else net_range\n\n                except ValueError:  # pragma: no cover\n\n                    net['range'] = net_range\n\n                if match.group(3) and match.group(4):\n\n                    addrs = []\n                    addrs.extend(summarize_address_range(\n                        ip_address(match.group(3).strip()),\n                        ip_address(match.group(4).strip())))\n\n                    cidr = ', '.join(\n                        [i.__str__() for i in collapse_addresses(addrs)]\n                    )\n\n                else:\n\n                    cidr = ip_network(net_range).__str__()\n\n                net['cidr'] = cidr\n                net['start'] = match.start()\n                net['end'] = match.end()\n                nets.append(net)\n\n            except (ValueError, TypeError):\n\n                pass\n\n        return nets", "response": "This function parses the whois response and returns a list of dicts with the network routing block and the start and end positions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup_whois(self, inc_raw=False, retry_count=3, get_referral=False,\n                     extra_blacklist=None, ignore_referral_errors=False,\n                     field_list=None, asn_alts=None, extra_org_map=None,\n                     inc_nir=True, nir_field_list=None, asn_methods=None,\n                     get_asn_description=True):\n        \"\"\"\n        The function for retrieving and parsing whois information for an IP\n        address via port 43 (WHOIS).\n\n        Args:\n            inc_raw (:obj:`bool`): Whether to include the raw whois results in\n                the returned dictionary. Defaults to False.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            get_referral (:obj:`bool`): Whether to retrieve referral whois\n                information, if available. Defaults to False.\n            extra_blacklist (:obj:`list`): Blacklisted whois servers in\n                addition to the global BLACKLIST. Defaults to None.\n            ignore_referral_errors (:obj:`bool`): Whether to ignore and\n                continue when an exception is encountered on referral whois\n                lookups. Defaults to False.\n            field_list (:obj:`list`): If provided, a list of fields to parse:\n                ['name', 'handle', 'description', 'country', 'state', 'city',\n                'address', 'postal_code', 'emails', 'created', 'updated']\n                If None, defaults to all.\n            asn_alts (:obj:`list`): Additional lookup types to attempt if the\n                ASN dns lookup fails. Allow permutations must be enabled.\n                If None, defaults to all ['whois', 'http']. *WARNING*\n                deprecated in favor of new argument asn_methods.\n            extra_org_map (:obj:`dict`): Dictionary mapping org handles to\n                RIRs. This is for limited cases where ARIN REST (ASN fallback\n                HTTP lookup) does not show an RIR as the org handle e.g., DNIC\n                (which is now the built in ORG_MAP) e.g., {'DNIC': 'arin'}.\n                Valid RIR values are (note the case-sensitive - this is meant\n                to match the REST result):\n                'ARIN', 'RIPE', 'apnic', 'lacnic', 'afrinic'\n                Defaults to None.\n            inc_nir (:obj:`bool`): Whether to retrieve NIR (National Internet\n                Registry) information, if registry is JPNIC (Japan) or KRNIC\n                (Korea). If True, extra network requests will be required.\n                If False, the information returned for JP or KR IPs is\n                severely restricted. Defaults to True.\n            nir_field_list (:obj:`list`): If provided and inc_nir, a list of\n                fields to parse:\n                ['name', 'handle', 'country', 'address', 'postal_code',\n                'nameservers', 'created', 'updated', 'contacts']\n                If None, defaults to all.\n            asn_methods (:obj:`list`): ASN lookup types to attempt, in order.\n                If None, defaults to all ['dns', 'whois', 'http'].\n            get_asn_description (:obj:`bool`): Whether to run an additional\n                query when pulling ASN information via dns, in order to get\n                the ASN description. Defaults to True.\n\n        Returns:\n            dict: The IP whois lookup results\n\n            ::\n\n                {\n                    'query' (str) - The IP address\n                    'asn' (str) - The Autonomous System Number\n                    'asn_date' (str) - The ASN Allocation date\n                    'asn_registry' (str) - The assigned ASN registry\n                    'asn_cidr' (str) - The assigned ASN CIDR\n                    'asn_country_code' (str) - The assigned ASN country code\n                    'asn_description' (str) - The ASN description\n                    'nets' (list) - Dictionaries containing network\n                        information which consists of the fields listed in the\n                        ipwhois.whois.RIR_WHOIS dictionary.\n                    'raw' (str) - Raw whois results if the inc_raw parameter\n                        is True.\n                    'referral' (dict) - Referral whois information if\n                        get_referral is True and the server is not blacklisted.\n                        Consists of fields listed in the ipwhois.whois.RWHOIS\n                        dictionary.\n                    'raw_referral' (str) - Raw referral whois results if the\n                        inc_raw parameter is True.\n                    'nir' (dict) - ipwhois.nir.NIRWhois() results if inc_nir\n                        is True.\n                }\n        \"\"\"\n\n        from .whois import Whois\n\n        # Create the return dictionary.\n        results = {'nir': None}\n\n        # Retrieve the ASN information.\n        log.debug('ASN lookup for {0}'.format(self.address_str))\n\n        asn_data = self.ipasn.lookup(\n            inc_raw=inc_raw, retry_count=retry_count, asn_alts=asn_alts,\n            extra_org_map=extra_org_map, asn_methods=asn_methods,\n            get_asn_description=get_asn_description\n        )\n\n        # Add the ASN information to the return dictionary.\n        results.update(asn_data)\n\n        # Retrieve the whois data and parse.\n        whois = Whois(self.net)\n        log.debug('WHOIS lookup for {0}'.format(self.address_str))\n        whois_data = whois.lookup(\n            inc_raw=inc_raw, retry_count=retry_count, response=None,\n            get_referral=get_referral, extra_blacklist=extra_blacklist,\n            ignore_referral_errors=ignore_referral_errors, asn_data=asn_data,\n            field_list=field_list\n        )\n\n        # Add the WHOIS information to the return dictionary.\n        results.update(whois_data)\n\n        if inc_nir:\n\n            nir = None\n            if 'JP' == asn_data['asn_country_code']:\n                nir = 'jpnic'\n            elif 'KR' == asn_data['asn_country_code']:\n                nir = 'krnic'\n\n            if nir:\n\n                nir_whois = NIRWhois(self.net)\n                nir_data = nir_whois.lookup(\n                    nir=nir, inc_raw=inc_raw, retry_count=retry_count,\n                    response=None,\n                    field_list=nir_field_list, is_offline=False\n                )\n\n                # Add the NIR information to the return dictionary.\n                results['nir'] = nir_data\n\n        return results", "response": "This function returns the raw whois information for an IP\n            from the WHOIS server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lookup_rdap(self, inc_raw=False, retry_count=3, depth=0,\n                    excluded_entities=None, bootstrap=False,\n                    rate_limit_timeout=120, asn_alts=None, extra_org_map=None,\n                    inc_nir=True, nir_field_list=None, asn_methods=None,\n                    get_asn_description=True):\n        \"\"\"\n        The function for retrieving and parsing whois information for an IP\n        address via HTTP (RDAP).\n\n        **This is now the recommended method, as RDAP contains much better\n        information to parse.**\n\n        Args:\n            inc_raw (:obj:`bool`): Whether to include the raw whois results in\n                the returned dictionary. Defaults to False.\n            retry_count (:obj:`int`): The number of times to retry in case\n                socket errors, timeouts, connection resets, etc. are\n                encountered. Defaults to 3.\n            depth (:obj:`int`): How many levels deep to run queries when\n                additional referenced objects are found. Defaults to 0.\n            excluded_entities (:obj:`list`): Entity handles to not perform\n                lookups. Defaults to None.\n            bootstrap (:obj:`bool`): If True, performs lookups via ARIN\n                bootstrap rather than lookups based on ASN data. ASN lookups\n                are not performed and no output for any of the asn* fields is\n                provided. Defaults to False.\n            rate_limit_timeout (:obj:`int`): The number of seconds to wait\n                before retrying when a rate limit notice is returned via\n                rdap+json. Defaults to 120.\n            asn_alts (:obj:`list`): Additional lookup types to attempt if the\n                ASN dns lookup fails. Allow permutations must be enabled.\n                If None, defaults to all ['whois', 'http']. *WARNING*\n                deprecated in favor of new argument asn_methods.\n            extra_org_map (:obj:`dict`): Dictionary mapping org handles to\n                RIRs. This is for limited cases where ARIN REST (ASN fallback\n                HTTP lookup) does not show an RIR as the org handle e.g., DNIC\n                (which is now the built in ORG_MAP) e.g., {'DNIC': 'arin'}.\n                Valid RIR values are (note the case-sensitive - this is meant\n                to match the REST result):\n                'ARIN', 'RIPE', 'apnic', 'lacnic', 'afrinic'\n                Defaults to None.\n            inc_nir (:obj:`bool`): Whether to retrieve NIR (National Internet\n                Registry) information, if registry is JPNIC (Japan) or KRNIC\n                (Korea). If True, extra network requests will be required.\n                If False, the information returned for JP or KR IPs is\n                severely restricted. Defaults to True.\n            nir_field_list (:obj:`list`): If provided and inc_nir, a list of\n                fields to parse:\n                ['name', 'handle', 'country', 'address', 'postal_code',\n                'nameservers', 'created', 'updated', 'contacts']\n                If None, defaults to all.\n            asn_methods (:obj:`list`): ASN lookup types to attempt, in order.\n                If None, defaults to all ['dns', 'whois', 'http'].\n            get_asn_description (:obj:`bool`): Whether to run an additional\n                query when pulling ASN information via dns, in order to get\n                the ASN description. Defaults to True.\n\n        Returns:\n            dict: The IP RDAP lookup results\n\n            ::\n\n                {\n                    'query' (str) - The IP address\n                    'asn' (str) - The Autonomous System Number\n                    'asn_date' (str) - The ASN Allocation date\n                    'asn_registry' (str) - The assigned ASN registry\n                    'asn_cidr' (str) - The assigned ASN CIDR\n                    'asn_country_code' (str) - The assigned ASN country code\n                    'asn_description' (str) - The ASN description\n                    'entities' (list) - Entity handles referred by the top\n                        level query.\n                    'network' (dict) - Network information which consists of\n                        the fields listed in the ipwhois.rdap._RDAPNetwork\n                        dict.\n                    'objects' (dict) - Mapping of entity handle->entity dict\n                        which consists of the fields listed in the\n                        ipwhois.rdap._RDAPEntity dict. The raw result is\n                        included for each object if the inc_raw parameter\n                        is True.\n                    'raw' (dict) - Whois results in json format if the inc_raw\n                        parameter is True.\n                    'nir' (dict) - ipwhois.nir.NIRWhois results if inc_nir is\n                        True.\n                }\n        \"\"\"\n\n        from .rdap import RDAP\n\n        # Create the return dictionary.\n        results = {'nir': None}\n\n        asn_data = None\n        response = None\n        if not bootstrap:\n\n            # Retrieve the ASN information.\n            log.debug('ASN lookup for {0}'.format(self.address_str))\n            asn_data = self.ipasn.lookup(\n                inc_raw=inc_raw, retry_count=retry_count, asn_alts=asn_alts,\n                extra_org_map=extra_org_map, asn_methods=asn_methods,\n                get_asn_description=get_asn_description\n            )\n\n            # Add the ASN information to the return dictionary.\n            results.update(asn_data)\n\n        # Retrieve the RDAP data and parse.\n        rdap = RDAP(self.net)\n        log.debug('RDAP lookup for {0}'.format(self.address_str))\n        rdap_data = rdap.lookup(\n            inc_raw=inc_raw, retry_count=retry_count, asn_data=asn_data,\n            depth=depth, excluded_entities=excluded_entities,\n            response=response, bootstrap=bootstrap,\n            rate_limit_timeout=rate_limit_timeout\n        )\n\n        # Add the RDAP information to the return dictionary.\n        results.update(rdap_data)\n\n        if inc_nir:\n\n            nir = None\n            if 'JP' == asn_data['asn_country_code']:\n                nir = 'jpnic'\n            elif 'KR' == asn_data['asn_country_code']:\n                nir = 'krnic'\n\n            if nir:\n                nir_whois = NIRWhois(self.net)\n                nir_data = nir_whois.lookup(\n                    nir=nir, inc_raw=inc_raw, retry_count=retry_count,\n                    response=None,\n                    field_list=nir_field_list, is_offline=False\n                )\n\n                # Add the NIR information to the return dictionary.\n                results['nir'] = nir_data\n\n        return results", "response": "This function is used to retrieve and parse the IP\n            information from an RDAP. It is used to retrieve and parse the IP\n            information from the RDAP."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n\n        return int(datetime_to_timestamp(value))", "response": "Serialize given datetime to timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserializes given datetime to timestamp.", "response": "def _serialize(self, value, *args, **kwargs):\n        \"\"\"Serialize given datetime to timestamp.\"\"\"\n        if value is not None:\n            value = super(MSTimestamp, self)._serialize(value, *args) * 1e3\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a default value to a raw field.", "response": "def convert_default(self, field, **params):\n        \"\"\"Return raw field.\"\"\"\n        for klass, ma_field in self.TYPE_MAPPING:\n            if isinstance(field, klass):\n                return ma_field(**params)\n        return fields.Raw(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_instance(self, data):\n        if not self.opts.model:\n            return data\n\n        if self.instance is not None:\n            for key, value in data.items():\n                setattr(self.instance, key, value)\n            return self.instance\n\n        return self.opts.model(**data)", "response": "Build object from data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract(self, stim):\n        ''' Returns all words. '''\n        props = [(e.text, e.onset, e.duration) for e in stim.elements]\n        vals, onsets, durations = map(list, zip(*props))\n        return ExtractorResult(vals, stim, self, ['word'], onsets, durations)", "response": "Extracts all words from a stimulus."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload one or more stimuli from a file.", "response": "def load_stims(source, dtype=None, fail_silently=False):\n    \"\"\" Load one or more stimuli directly from file, inferring/extracting\n    metadata as needed.\n\n    Args:\n        source (str or list): The location to load the stim(s) from. Can be\n            the path to a directory, to a single file, or a list of filenames.\n        dtype (str): The type of stim to load. If dtype is None, relies on the\n            filename extension for guidance. If dtype is provided, must be\n            one of 'video', 'image', 'audio', or 'text'.\n        fail_silently (bool): If True do not raise error when trying to load a\n            missing stimulus from a list of sources.\n\n    Returns: A list of Stims.\n    \"\"\"\n    from .video import VideoStim, ImageStim\n    from .audio import AudioStim\n    from .text import TextStim\n\n    if isinstance(source, string_types):\n        return_list = False\n        source = [source]\n    else:\n        return_list = True\n\n    stims = []\n\n    stim_map = {\n        'image': ImageStim,\n        'video': VideoStim,\n        'text': TextStim,\n        'audio': AudioStim\n    }\n\n    def load_file(source):\n        source = realpath(source)\n        import magic  # requires libmagic, so import here\n        mime = magic.from_file(source, mime=True)\n        if not isinstance(mime, string_types):\n            mime = mime.decode('utf-8')\n        mime = mime.split('/')[0]\n        if mime in stim_map.keys():\n            s = stim_map[mime](source)\n            stims.append(s)\n\n    def load_url(source):\n        try:\n            main_type = urlopen(source).info().get_content_maintype()  # Py3\n        except:\n            main_type = urlopen(source).info().getmaintype()  # Py2\n        if main_type in stim_map.keys():\n            s = stim_map[main_type](url=source)\n            stims.append(s)\n\n    for s in source:\n        if bool(urlparse(s).scheme):\n            load_url(s)\n        elif isdir(s):\n            for f in glob(join(s, '*')):\n                if isfile(f):\n                    load_file(f)\n        elif isfile(s):\n            load_file(s)\n        else:\n            if not (return_list and fail_silently):\n                raise IOError(\"File not found\")\n\n    if return_list:\n        return stims\n\n    return stims[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_filename(self):\n        ''' Return the source filename of the current Stim. '''\n        if self.filename is None or not os.path.exists(self.filename):\n            tf = tempfile.mktemp() + self._default_file_extension\n            self.save(tf)\n            yield tf\n            os.remove(tf)\n        else:\n            yield self.filename", "response": "Return the source filename of the current Stim."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_results(results, format='wide', timing=True, metadata=True,\n                  extractor_names=True, object_id=True, aggfunc=None,\n                  invalid_results='ignore', **to_df_kwargs):\n    ''' Merges a list of ExtractorResults instances and returns a pandas DF.\n\n    Args:\n        results (list, tuple): A list of ExtractorResult instances to merge.\n        format (str): Format to return the data in. Can be either 'wide' or\n            'long'. In the wide case, every extracted feature is a column,\n            and every Stim is a row. In the long case, every row contains a\n            single Stim/Extractor/feature combination.\n        timing (bool, str): Whether or not to include columns for onset,\n            order, and duration.\n        metadata (bool): if True, includes Stim metadata columns in the\n            returned DataFrame. These columns include 'stim_name', 'class',\n            'filename', 'history', and 'source_file'. Note that these values\n            are often long strings, so the returned DF will be considerably\n            larger.\n        extractor_names (str, bool): How to handle extractor names when\n            returning results. The specific behavior depends on whether format\n            is 'long' or 'wide'. Valid values include:\n\n                - 'prepend' or True: In both 'long' and 'wide' formats,\n                  feature names will be prepended with the Extractor name\n                  (e.g., \"FaceExtractor#face_likelihood\").\n                - 'drop' or False: In both 'long' and 'wide' formats, extractor\n                  names will be omitted entirely from the result. Note that\n                  this can create feature name conflicts when merging results\n                  from multiple Extractors, so is generally discouraged.\n                - 'column': In 'long' format, extractor name will be included\n                  as a separate column. Not valid for 'wide' format (and will\n                  raise an error).\n                - 'multi': In 'wide' format, a MultiIndex will be used for the\n                  columns, with the first level of the index containing the\n                  Extractor name and the second level containing the feature\n                  name. This value is invalid if format='long' (and will raise\n                  and error).\n\n        object_id (bool): If True, attempts to intelligently add an\n            'object_id' column that differentiates between multiple objects in\n            the results that may share onsets/orders/durations (and would\n            otherwise be impossible to distinguish). This frequently occurs for\n            ImageExtractors that identify multiple target objects (e.g., faces)\n            within a single ImageStim. Default is 'auto', which includes the\n            'object_id' column if and only if it has a non-constant value.\n        aggfunc (str, Callable): If format='wide' and extractor_names='drop',\n            it's possible for name clashes between features to occur. In such\n            cases, the aggfunc argument is passed onto pandas' pivot_table\n            function, and specifies how to aggregate multiple values for the\n            same index. Can be a callable or any string value recognized by\n            pandas. By default (None), 'mean' will be used for numeric columns\n            and 'first' will be used for object/categorical columns.\n        invalid_results (str): Specifies desired action for treating elements\n            of the passed in results argument that are not ExtractorResult\n            objects. Valid values include:\n                - 'ignore' will ignore them and merge the valid\n                    ExtractorResults.\n                - 'fail' will raise an exception on any invalid input\n\n\n    Returns: a pandas DataFrame. For format details, see 'format' argument.\n    '''\n\n    results = flatten(results)\n\n    _timing = True if timing == 'auto' else timing\n    _object_id = True if object_id == 'auto' else object_id\n\n    if extractor_names is True:\n        extractor_names = 'prepend'\n    elif extractor_names is False:\n        extractor_names = 'drop'\n\n    dfs = []\n    for r in results:\n        if isinstance(r, ExtractorResult):\n            dfs.append(r.to_df(timing=_timing, metadata=metadata,\n                               format='long', extractor_name=True,\n                               object_id=_object_id, **to_df_kwargs))\n        elif invalid_results == 'fail':\n            raise ValueError(\"At least one of the provided results was not an\"\n                             \"ExtractorResult. Set the invalid_results\"\n                             \"parameter to 'ignore' if you wish to ignore\"\n                             \"this.\")\n\n    if len(dfs) == 0:\n        return pd.DataFrame()\n\n    data = pd.concat(dfs, axis=0).reset_index(drop=True)\n\n    if object_id == 'auto' and data['object_id'].nunique() == 1:\n        data = data.drop('object_id', axis=1)\n\n    if extractor_names in ['prepend', 'multi']:\n        data['feature'] = data['extractor'] + '#' + data['feature'].astype(str)\n\n    if extractor_names != 'column':\n        data = data.drop('extractor', axis=1)\n\n    if format == 'wide':\n        ind_cols = {'stim_name', 'onset', 'order', 'duration', 'object_id',\n                    'class', 'filename', 'history', 'source_file'}\n        ind_cols = list(ind_cols & set(data.columns))\n        # pandas groupby/index operations can't handle NaNs in index, (see\n        # issue at https://github.com/pandas-dev/pandas/issues/3729), so we\n        # replace NaNs with a placeholder and then re-substitute after\n        # pivoting.\n        dtypes = data[ind_cols].dtypes\n        data[ind_cols] = data[ind_cols].fillna('PlAcEholdER')\n\n        # Set default aggfunc based on column type, otherwise bad things happen\n        if aggfunc is None:\n            aggfunc = 'mean' if is_numeric_dtype(data['value']) else 'first'\n\n        data = data.pivot_table(index=ind_cols, columns='feature',\n                                values='value', aggfunc=aggfunc).reset_index()\n        data.columns.name = None  # vestigial--is set to 'feature'\n        data[ind_cols] = data[ind_cols].replace('PlAcEholdER', np.nan)\n        data[ind_cols] = data[ind_cols].astype(dict(zip(ind_cols, dtypes)))\n\n    if timing == 'auto' and 'onset' in data.columns:\n        if data['onset'].isnull().all():\n            data = data.drop(['onset', 'order', 'duration'], axis=1)\n\n    if 'onset' in data.columns:\n        key = [('onset', ''), ('order', ''), ('duration', '')] \\\n            if isinstance(data.columns, pd.MultiIndex) \\\n            else ['onset', 'order', 'duration']\n        data = data.sort_values(key).reset_index(drop=True)\n\n    if extractor_names == 'multi':\n        if format == 'long':\n            raise ValueError(\"Invalid extractor_names value 'multi'. When \"\n                             \"format is 'long', extractor_names must be \"\n                             \"one of 'drop', 'prepend', or 'column'.\")\n        data.columns = pd.MultiIndex.from_tuples(\n            [c.split('#') for c in data.columns])\n    return data", "response": "Merge a list of ExtractorResults instances into a single pandas DF."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_df(self, timing=True, metadata=False, format='wide',\n              extractor_name=False, object_id=True, **to_df_kwargs):\n        ''' Convert current instance to a pandas DatasFrame.\n\n        Args:\n            timing (bool): If True, adds columns for event onset and duration.\n                Note that these columns will be added even if there are no\n                valid values in the current object (NaNs will be inserted).\n                If 'auto', timing columns are only inserted if there's at least\n                one valid (i.e., non-NaN) onset/order/duration.\n            metadata (bool): If True, adds columns for key metadata (including\n                the name, filename, class, history, and source file of the\n                Stim).\n            format (str): Format to return the data in. Can be either 'wide' or\n                'long'. In the wide case, every extracted feature is a column,\n                and every result object is in a row. In the long case, every\n                row contains a single record/feature combination.\n            extractor_name (bool): If True, includes the Extractor name as\n                a column (in 'long' format) or index level (in 'wide' format).\n            object_id (bool): If True, attempts to intelligently add an\n                'object_id' column that differentiates between multiple objects\n                in the results that may share onsets and durations (and would\n                otherwise be impossible to distinguish). This frequently occurs\n                for ImageExtractors that identify multiple target objects\n                (e.g., faces) within a single ImageStim. In addition to boolean\n                values, the special value 'auto' can be passed, in which case\n                the object_id column will only be inserted if the resulting\n                constant would be non-constant.\n\n        Returns:\n            A pandas DataFrame.\n        '''\n\n        # Ideally, Extractors should implement their own _to_df() class method\n        # that produces a DataFrame in standardized format. Failing that, we\n        # assume self._data is already array-like and can be wrapped in a DF.\n        if hasattr(self.extractor, '_to_df'):\n            df = self.extractor._to_df(self, **to_df_kwargs)\n        else:\n            features = self.features\n            data = np.array(self._data)\n            if features is None:\n                features = ['feature_%d' % (i + 1)\n                            for i in range(data.shape[1])]\n            df = pd.DataFrame(data, columns=features)\n\n        if hasattr(self, '_onsets'):\n            onsets = np.array(self._onsets)\n            onsets += 0.0 if self.onset is None else self.onset\n        else:\n            onsets = np.nan if self.onset is None else self.onset\n        durations = getattr(self, '_durations', self.duration)\n        durations = np.nan if durations is None else durations\n        orders = getattr(self, '_orders', self.order)\n        orders = np.nan if orders is None else orders\n\n        # If any features clash with protected keys, append underscore\n        protected = ['onset', 'order', 'duration', 'extractor', 'stim_name', \\\n                     'class', 'filename', 'history', 'source_file']\n        df = df.rename(columns={k: k + '_' for k in protected})\n\n        index_cols = []\n\n        # Generally we leave it to Extractors to properly track the number of\n        # objects returned in the result DF, using the 'object_id' column.\n        # But in cases where the Extractor punt on this and_object_id=True, we\n        # take our best guess. The logic is that we increment the object\n        # counter for any row in the DF that cannot be uniquely distinguished\n        # from other rows by onset and duration.\n        if object_id and 'object_id' not in df.columns:\n            index = pd.Series(onsets).astype(str) + '_' + \\\n                pd.Series(durations).astype(str)\n            if object_id is True or (object_id == 'auto' and\n                                     len(set(index)) < len(df)):\n                ids = np.arange(len(df)) if len(index) == 1 \\\n                    else df.groupby(index).cumcount()\n                df.insert(0, 'object_id', ids)\n                index_cols = ['object_id']\n\n        if timing is True or (timing == 'auto' and\n                              (np.isfinite(durations).any() or\n                               np.isfinite(orders).any())):\n            df.insert(0, 'onset', onsets)\n            df.insert(0, 'duration', durations)\n            df.insert(0, 'order', orders)\n            df = df.sort_values('onset').reset_index(drop=True)\n            index_cols.extend(['onset', 'order', 'duration'])\n\n        if format == 'long':\n            df = df.melt(index_cols, var_name='feature')\n            df = df.dropna(subset=['value'])\n\n        if extractor_name:\n            name = self.extractor.name\n            if format == 'long':\n                df['extractor'] = name\n            else:\n                df.columns = pd.MultiIndex.from_product([[name], df.columns])\n\n        if metadata:\n            df['stim_name'] = self.stim.name\n            df['class'] = self.stim.__class__.__name__\n            df['filename'] = self.stim.filename\n            hist = '' if self.stim.history is None else str(self.stim.history)\n            df['history'] = hist\n            df['source_file'] = self.history.to_df().iloc[0].source_file\n        return df", "response": "Convert the current instance to a pandas DatasFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the component elements of the specified type.", "response": "def get_stim(self, type_, return_all=False):\n        ''' Returns component elements of the specified type.\n\n        Args:\n            type_ (str or Stim class): the desired Stim subclass to return.\n            return_all (bool): when True, returns all elements that matched the\n                specified type as a list. When False (default), returns only\n                the first matching Stim.\n\n        Returns:\n            If return_all is True, a list of matching elements (or an empty\n            list if no elements match). If return_all is False, returns the\n            first matching Stim, or None if no elements match.\n        '''\n        if isinstance(type_, string_types):\n            type_ = _get_stim_class(type_)\n        matches = []\n        for s in self.elements:\n            if isinstance(s, type_):\n                if not return_all:\n                    return s\n                matches.append(s)\n        if not matches:\n            return [] if return_all else None\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_types(self, types, all_=True):\n        ''' Check whether the current component list matches all Stim types\n        in the types argument.\n\n        Args:\n            types (Stim, list): a Stim class or iterable of Stim classes.\n            all_ (bool): if True, all input types must match; if False, at\n                least one input type must match.\n\n        Returns:\n            True if all passed types match at least one Stim in the component\n            list, otherwise False.\n        '''\n        func = all if all_ else any\n        return func([self.get_stim(t) for t in listify(types)])", "response": "Check whether the current component list matches all Stim types in the types argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an animal of Transformer class that matches the given name.", "response": "def get_transformer(name, base=None, *args, **kwargs):\n    ''' Scans list of currently available Transformer classes and returns an\n    instantiation of the first one whose name perfectly matches\n    (case-insensitive).\n\n    Args:\n        name (str): The name of the transformer to retrieve. Case-insensitive;\n            e.g., 'stftextractor' or 'CornerDetectionExtractor'.\n        base (str, list): Optional name of transformer modules to search.\n            Valid values are 'converters', 'extractors', and 'filters'.\n        args, kwargs: Optional positional or keyword arguments to pass onto\n            the Transformer.\n    '''\n\n    name = name.lower()\n\n    # Default to searching all kinds of Transformers\n    if base is None:\n        base = ['extractors', 'converters', 'filters']\n\n    base = listify(base)\n\n    for b in base:\n        importlib.import_module('pliers.%s' % b)\n        mod = getattr(pliers, b)\n        classes = getattr(mod, '__all__')\n        for cls_name in classes:\n            if cls_name.lower() == name.lower():\n                cls = getattr(mod, cls_name)\n                return cls(*args, **kwargs)\n\n    raise KeyError(\"No transformer named '%s' found.\" % name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform(self, stims, validation='strict', *args, **kwargs):\n        ''' Executes the transformation on the passed stim(s).\n\n        Args:\n            stims (str, Stim, list): One or more stimuli to process. Must be\n                one of:\n\n                    - A string giving the path to a file that can be read in\n                      as a Stim (e.g., a .txt file, .jpg image, etc.)\n                    - A Stim instance of any type.\n                    - An iterable of stims, where each element is either a\n                      string or a Stim.\n\n            validation (str): String specifying how validation errors should\n                be handled. Must be one of:\n\n                    - 'strict': Raise an exception on any validation error\n                    - 'warn': Issue a warning for all validation errors\n                    - 'loose': Silently ignore all validation errors\n\n            args: Optional positional arguments to pass onto the internal\n                _transform call.\n            kwargs: Optional positional arguments to pass onto the internal\n                _transform call.\n        '''\n\n        if isinstance(stims, string_types):\n            stims = load_stims(stims)\n\n        # If stims is a CompoundStim and the Transformer is expecting a single\n        # input type, extract all matching stims\n        if isinstance(stims, CompoundStim) and not isinstance(self._input_type, tuple):\n            stims = stims.get_stim(self._input_type, return_all=True)\n            if not stims:\n                raise ValueError(\"No stims of class %s found in the provided\"\n                                 \"CompoundStim instance.\" % self._input_type)\n\n        # If stims is an iterable, naively loop over elements, removing\n        # invalid results if needed\n        if isiterable(stims):\n            iters = self._iterate(stims, validation=validation, *args,\n                                  **kwargs)\n            if config.get_option('drop_bad_extractor_results'):\n                iters = (i for i in iters if i is not None)\n            iters = progress_bar_wrapper(iters, desc='Stim')\n            return set_iterable_type(iters)\n\n        # Validate stim, and then either pass it directly to the Transformer\n        # or, if a conversion occurred, recurse.\n        else:\n            try:\n                validated_stim = self._validate(stims)\n            except TypeError as err:\n                if validation == 'strict':\n                    raise err\n                elif validation == 'warn':\n                    logging.warn(str(err))\n                    return\n                elif validation == 'loose':\n                    return\n            # If a conversion occurred during validation, we recurse\n            if stims is not validated_stim:\n                return self.transform(validated_stim, *args, **kwargs)\n            else:\n                result = self._transform(validated_stim, *args, **kwargs)\n                result = _log_transformation(validated_stim, result, self)\n                if isgenerator(result):\n                    result = list(result)\n                self._propagate_context(validated_stim, result)\n                return result", "response": "Executes the transformation on the passed stimuli."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sampling_rate(filename):\n        ''' Use moviepy/FFMPEG to get the sampling rate '''\n        infos = ffmpeg_parse_infos(filename)\n        fps = infos.get('audio_fps', 44100)\n        if fps == 'unknown':\n            fps = 44100\n        return fps", "response": "Use moviepy to get the sampling rate"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, path):\n        ''' Save clip data to file.\n\n        Args:\n            path (str): Filename to save audio data to.\n        '''\n        self.clip.write_audiofile(path, fps=self.sampling_rate)", "response": "Save the audio data to file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_from_standard_locations():\n    ''' Check standard locations for config files and update settings if found.\n    Order is user's home dir, environment variable ($PLIERS_CONFIG), and then\n    current directory--with later files taking precedence over earlier ones.\n    '''\n    locs = [\n        join(expanduser('~'), _config_name),\n        join('.', _config_name)\n    ]\n    if 'PLIERS_CONFIG' in os.environ:\n        locs.insert(1, os.environ['PLIERS_CONFIG'])\n\n    from_file(locs, False)", "response": "Check standard locations for config files and update settings if found."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_converter(in_type, out_type, *args, **kwargs):\n    ''' Scans the list of available Converters and returns an instantiation\n    of the first one whose input and output types match those passed in.\n\n    Args:\n        in_type (type): The type of input the converter must have.\n        out_type (type): The type of output the converter must have.\n        args, kwargs: Optional positional and keyword arguments to pass onto\n            matching Converter's initializer.\n    '''\n    convs = pliers.converters.__all__\n\n    # If config includes default converters for this combination, try them\n    # first\n    out_type = listify(out_type)[::-1]\n    default_convs = config.get_option('default_converters')\n\n    for ot in out_type:\n        conv_str = '%s->%s' % (in_type.__name__, ot.__name__)\n        if conv_str in default_convs:\n            convs = list(default_convs[conv_str]) + convs\n\n    for name in convs:\n        cls = getattr(pliers.converters, name)\n        if not issubclass(cls, Converter):\n            continue\n\n        available = cls.available if issubclass(\n            cls, EnvironmentKeyMixin) else True\n        if cls._input_type == in_type and cls._output_type in out_type \\\n                and available:\n            conv = cls(*args, **kwargs)\n            return conv\n\n    return None", "response": "Returns a converter that can be used to convert the input and output types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_graph():\n  # Creates graph from saved graph_def.pb.\n  with tf.gfile.FastGFile(os.path.join(\n      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    _ = tf.import_graph_def(graph_def, name='')", "response": "Creates a graph from saved graph_def file and returns a saver."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun inference on an image.", "response": "def run_inference_on_image(image):\n  \"\"\"Runs inference on an image.\n\n  Args:\n    image: Image file name.\n\n  Returns:\n    Nothing\n  \"\"\"\n  if not tf.gfile.Exists(image):\n    tf.logging.fatal('File does not exist %s', image)\n  image_data = tf.gfile.FastGFile(image, 'rb').read()\n\n  # Creates graph from saved GraphDef.\n  create_graph()\n\n  with tf.Session() as sess:\n    # Some useful tensors:\n    # 'softmax:0': A tensor containing the normalized prediction across\n    #   1000 labels.\n    # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\n    #   float description of the image.\n    # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\n    #   encoding of the image.\n    # Runs the softmax tensor by feeding the image_data as input to the graph.\n    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n    predictions = sess.run(softmax_tensor,\n                           {'DecodeJpeg/contents:0': image_data})\n    predictions = np.squeeze(predictions)\n\n    # Creates node ID --> English string lookup.\n    node_lookup = NodeLookup()\n\n    top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\n    for node_id in top_k:\n      human_string = node_lookup.id_to_string(node_id)\n      score = predictions[node_id]\n      print('%s (score = %.5f)' % (human_string, score))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, label_lookup_path, uid_lookup_path):\n    if not tf.gfile.Exists(uid_lookup_path):\n      tf.logging.fatal('File does not exist %s', uid_lookup_path)\n    if not tf.gfile.Exists(label_lookup_path):\n      tf.logging.fatal('File does not exist %s', label_lookup_path)\n\n    # Loads mapping from string UID to human-readable string\n    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n    uid_to_human = {}\n    p = re.compile(r'[n\\d]*[ \\S,]*')\n    for line in proto_as_ascii_lines:\n      parsed_items = p.findall(line)\n      uid = parsed_items[0]\n      human_string = parsed_items[2]\n      uid_to_human[uid] = human_string\n\n    # Loads mapping from string UID to integer node ID.\n    node_id_to_uid = {}\n    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n    for line in proto_as_ascii:\n      if line.startswith('  target_class:'):\n        target_class = int(line.split(': ')[1])\n      if line.startswith('  target_class_string:'):\n        target_class_string = line.split(': ')[1]\n        node_id_to_uid[target_class] = target_class_string[1:-2]\n\n    # Loads the final mapping of integer node ID to human-readable string\n    node_id_to_name = {}\n    for key, val in node_id_to_uid.items():\n      if val not in uid_to_human:\n        tf.logging.fatal('Failed to locate: %s', val)\n      name = uid_to_human[val]\n      node_id_to_name[key] = name\n\n    return node_id_to_name", "response": "Loads a human readable English name for each softmax node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch_dictionary(name, url=None, format=None, index=0, rename=None,\n                     save=True, force_retrieve=False):\n    ''' Retrieve a dictionary of text norms from the web or local storage.\n\n    Args:\n        name (str): The name of the dictionary. If no url is passed, this must\n            match either one of the keys in the predefined dictionary file (see\n            dictionaries.json), or the name assigned to a previous dictionary\n            retrieved from a specific URL.\n        url (str): The URL of dictionary file to retrieve. Optional if name\n            matches an existing dictionary.\n        format (str): One of 'csv', 'tsv', 'xls', or None. Used to read data\n            appropriately. Note that most forms of compression will be detected\n            and handled automatically, so the format string refers only to the\n            format of the decompressed file. When format is None, the format\n            will be inferred from the filename.\n        index (str, int): The name or numeric index of the column to used as\n            the dictionary index. Passed directly to pd.ix.\n        rename (dict): An optional dictionary passed to pd.rename(); can be\n            used to rename columns in the loaded dictionary. Note that the\n            locally-saved dictionary will retain the renamed columns.\n        save (bool): Whether or not to save the dictionary locally the first\n            time it is retrieved.\n        force_retrieve (bool): If True, remote dictionary will always be\n            downloaded, even if a local copy exists (and the local copy will\n            be overwritten).\n\n    Returns: A pandas DataFrame indexed by strings (typically words).\n\n    '''\n    file_path = os.path.join(_get_dictionary_path(), name + '.csv')\n    if not force_retrieve and os.path.exists(file_path):\n        df = pd.read_csv(file_path)\n        index = datasets[name].get('index', df.columns[index])\n        return df.set_index(index)\n\n    if name in datasets:\n        url = datasets[name]['url']\n        format = datasets[name].get('format', format)\n        index = datasets[name].get('index', index)\n        rename = datasets.get('rename', rename)\n\n    if url is None:\n        raise ValueError(\"Dataset '%s' not found in local storage or presets, \"\n                         \"and no download URL provided.\" % name)\n    data = _download_dictionary(url, format=format, rename=rename)\n\n    if isinstance(index, int):\n        index = data.columns[index]\n    data = data.set_index(index)\n\n    if save:\n        file_path = os.path.join(_get_dictionary_path(), name + '.csv')\n        data.to_csv(file_path, encoding='utf-8')\n    return data", "response": "Retrieves a dictionary of text norms from the web or local storage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a Google API Face JSON response into a Pandas Dataframe.", "response": "def _to_df(self, result, handle_annotations=None):\n        '''\n        Converts a Google API Face JSON response into a Pandas Dataframe.\n\n        Args:\n            result (ExtractorResult): Result object from which to parse out a\n                Dataframe.\n            handle_annotations (str): How returned face annotations should be\n                handled in cases where there are multiple faces.\n                'first' indicates to only use the first face JSON object, all\n                other values will default to including every face.\n        '''\n        annotations = result._data\n        if handle_annotations == 'first':\n            annotations = [annotations[0]]\n\n        face_results = []\n        for i, annotation in enumerate(annotations):\n            data_dict = {}\n            for field, val in annotation.items():\n                if 'Confidence' in field:\n                    data_dict['face_' + field] = val\n                elif 'oundingPoly' in field:\n                    for j, vertex in enumerate(val['vertices']):\n                        for dim in ['x', 'y']:\n                            name = '%s_vertex%d_%s' % (field, j+1, dim)\n                            val = vertex[dim] if dim in vertex else np.nan\n                            data_dict[name] = val\n                elif field == 'landmarks':\n                    for lm in val:\n                        name = 'landmark_' + lm['type'] + '_%s'\n                        lm_pos = {name %\n                                  k: v for (k, v) in lm['position'].items()}\n                        data_dict.update(lm_pos)\n                else:\n                    data_dict[field] = val\n\n            face_results.append(data_dict)\n\n        return pd.DataFrame(face_results)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef correlation_matrix(df):\n    '''\n    Returns a pandas DataFrame with the pair-wise correlations of the columns.\n\n    Args:\n        df: pandas DataFrame with columns to run diagnostics on\n    '''\n    columns = df.columns.tolist()\n    corr = pd.DataFrame(\n        np.corrcoef(df, rowvar=0), columns=columns, index=columns)\n    return corr", "response": "Returns a pandas DataFrame with the pair - wise correlations of the columns."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a pandas Series with eigenvalues of the correlation matrix.", "response": "def eigenvalues(df):\n    '''\n    Returns a pandas Series with eigenvalues of the correlation matrix.\n\n    Args:\n        df: pandas DataFrame with columns to run diagnostics on\n    '''\n    corr = np.corrcoef(df, rowvar=0)\n    eigvals = np.linalg.eigvals(corr)\n    return pd.Series(eigvals, df.columns, name='Eigenvalue')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a pandas Series with condition indices of the df columns.", "response": "def condition_indices(df):\n    '''\n    Returns a pandas Series with condition indices of the df columns.\n\n    Args:\n        df: pandas DataFrame with columns to run diagnostics on\n    '''\n    eigvals = eigenvalues(df)\n    cond_idx = np.sqrt(eigvals.max() / eigvals)\n    return pd.Series(cond_idx, df.columns, name='Condition index')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the variance inflation factor for each column in the df.", "response": "def variance_inflation_factors(df):\n    '''\n    Computes the variance inflation factor (VIF) for each column in the df.\n    Returns a pandas Series of VIFs\n\n    Args:\n        df: pandas DataFrame with columns to run diagnostics on\n    '''\n    corr = np.corrcoef(df, rowvar=0)\n    corr_inv = np.linalg.inv(corr)\n    vifs = np.diagonal(corr_inv)\n    return pd.Series(vifs, df.columns, name='VIF')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a pandas Series with Mahalanobis distances for each sample on the specified axis.", "response": "def mahalanobis_distances(df, axis=0):\n    '''\n    Returns a pandas Series with Mahalanobis distances for each sample on the\n    axis.\n\n    Note: does not work well when # of observations < # of dimensions\n    Will either return NaN in answer\n    or (in the extreme case) fail with a Singular Matrix LinAlgError\n\n    Args:\n        df: pandas DataFrame with columns to run diagnostics on\n        axis: 0 to find outlier rows, 1 to find outlier columns\n    '''\n    df = df.transpose() if axis == 1 else df\n    means = df.mean()\n    try:\n        inv_cov = np.linalg.inv(df.cov())\n    except LinAlgError:\n        return pd.Series([np.NAN] * len(df.index), df.index,\n                         name='Mahalanobis')\n    dists = []\n    for i, sample in df.iterrows():\n        dists.append(mahalanobis(sample, means, inv_cov))\n\n    return pd.Series(dists, df.index, name='Mahalanobis')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef summary(self, stdout=True, plot=False):\n        '''\n        Displays diagnostics to the user\n\n        Args:\n            stdout (bool): print results to the console\n            plot (bool): use Seaborn to plot results\n        '''\n        if stdout:\n            print('Collinearity summary:')\n            print(pd.concat([self.results['Eigenvalues'],\n                             self.results['ConditionIndices'],\n                             self.results['VIFs'],\n                             self.results['CorrelationMatrix']],\n                            axis=1))\n\n            print('Outlier summary:')\n            print(self.results['RowMahalanobisDistances'])\n            print(self.results['ColumnMahalanobisDistances'])\n\n            print('Validity summary:')\n            print(self.results['Variances'])\n\n        if plot:\n            verify_dependencies('seaborn')\n            for key, result in self.results.items():\n                if key == 'CorrelationMatrix':\n                    ax = plt.axes()\n                    sns.heatmap(result, cmap='Blues', ax=ax)\n                    ax.set_title(key)\n                    sns.plt.show()\n                else:\n                    result.plot(kind='bar', title=key)\n                    plt.show()", "response": "Prints the summary of the user s results to the console or plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flag(self, diagnostic, thresh=None):\n        '''\n        Returns indices of diagnostic that satisfy (return True from) the\n        threshold predicate. Will use class-level default threshold if\n        None provided.\n\n        Args:\n            diagnostic (str): name of the diagnostic\n            thresh (func): threshold function (boolean predicate) to apply to\n            each element\n        '''\n        if thresh is None:\n            thresh = self.defaults[diagnostic]\n\n        result = self.results[diagnostic]\n        if isinstance(result, pd.DataFrame):\n            if diagnostic == 'CorrelationMatrix':\n                result = result.copy()\n                np.fill_diagonal(result.values, 0)\n            return result.applymap(thresh).sum().nonzero()[0]\n        else:\n            return result.apply(thresh).nonzero()[0]", "response": "Returns indices of diagnostic that satisfy ( return True from ) the\n            threshold predicate. Will use class - level default threshold if None provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flag_all(self, thresh_dict=None, include=None, exclude=None):\n        '''\n        Returns indices of (rows, columns) that satisfy flag() on any\n        diagnostic. Uses user-provided thresholds in thresh_dict/\n\n        Args:\n            thresh_dict (dict): dictionary of diagnostic->threshold functions\n            include (list): optional sublist of diagnostics to flag\n            exclude (list): optional sublist of diagnostics to not flag\n        '''\n        if thresh_dict is None:\n            thresh_dict = {}\n        row_idx = set()\n        col_idx = set()\n        include = self.results if include is None else include\n        include = list(\n            set(include) - set(exclude)) if exclude is not None else include\n        for diagnostic in include:\n            if diagnostic in thresh_dict:\n                flagged = self.flag(diagnostic, thresh_dict[diagnostic])\n            else:\n                flagged = self.flag(diagnostic)\n\n            if diagnostic == 'RowMahalanobisDistances':\n                row_idx = row_idx.union(flagged)\n            else:\n                col_idx = col_idx.union(flagged)\n\n        return sorted(list(row_idx)), sorted(list(col_idx))", "response": "Returns indices of rows columns that satisfy flag on any\n            diagnostic. Uses user - provided thresholds in thresh_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_nodes(self, nodes, parent=None, mode='horizontal'):\n        ''' Adds one or more nodes to the current graph.\n\n        Args:\n            nodes (list): A list of nodes to add. Each element must be one of\n                the following:\n\n                * A dict containing keyword args to pass onto to the Node init.\n                * An iterable containing 1 - 3 elements. The first element is\n                  mandatory, and specifies the Transformer at that node. The\n                  second element (optional) is an iterable of child nodes\n                  (specified in the same format). The third element\n                  (optional) is a string giving the (unique) name of the\n                  node.\n                * A Node instance.\n                * A Transformer instance.\n\n            parent (Node): Optional parent node (i.e., the node containing the\n                pliers Transformer from which the to-be-created nodes receive\n                their inputs).\n            mode (str): Indicates the direction with which to add the new nodes\n                * horizontal: the nodes should each be added as a child of the\n                  'parent' argument (or a Graph root by default).\n                * vertical: the nodes should each be added in sequence with\n                  the first node being the child of the 'parnet' argument\n                  (a Graph root by default) and each subsequent node being\n                  the child of the previous node in the list.\n        '''\n        for n in nodes:\n            node_args = self._parse_node_args(n)\n            if mode == 'horizontal':\n                self.add_node(parent=parent, **node_args)\n            elif mode == 'vertical':\n                parent = self.add_node(parent=parent, return_node=True,\n                                       **node_args)\n            else:\n                raise ValueError(\"Invalid mode for adding nodes to a graph:\"\n                                 \"%s\" % mode)", "response": "Adds one or more nodes to the current graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a node to the current graph.", "response": "def add_node(self, transformer, name=None, children=None, parent=None,\n                 parameters={}, return_node=False):\n        ''' Adds a node to the current graph.\n\n        Args:\n            transformer (str, Transformer): The pliers Transformer to use at\n                the to-be-added node. Either a case-insensitive string giving\n                the name of a Transformer class, or an initialized Transformer\n                instance.\n            name (str): Optional name to give this Node.\n            children (list): Optional list of child nodes (i.e., nodes to pass\n                the to-be-added node's Transformer output to).\n            parent (Node): Optional node from which the to-be-added Node\n                receives its input.\n            parameters (dict): Optional keyword arguments to pass onto the\n                Transformer initialized at this Node if a string is passed to\n                the 'transformer' argument. Ignored if an already-initialized\n                Transformer is passed.\n            return_node (bool): If True, returns the initialized Node instance.\n\n        Returns:\n            The initialized Node instance if return_node is True,\n                None otherwise.\n        '''\n\n        node = Node(transformer, name, **parameters)\n        self.nodes[node.id] = node\n\n        if parent is None:\n            self.roots.append(node)\n        else:\n            parent = self.nodes[parent.id]\n            parent.add_child(node)\n\n        if children is not None:\n            self.add_nodes(children, parent=node)\n\n        if return_node:\n            return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, stim, merge=True, **merge_kwargs):\n        ''' Executes the graph by calling all Transformers in sequence.\n\n        Args:\n            stim (str, Stim, list): One or more valid inputs to any\n                Transformer's 'transform' call.\n            merge (bool): If True, all results are merged into a single pandas\n                DataFrame before being returned. If False, a list of\n                ExtractorResult objects is returned (one per Extractor/Stim\n                combination).\n            merge_kwargs: Optional keyword arguments to pass onto the\n                merge_results() call.\n        '''\n        results = list(chain(*[self.run_node(n, stim) for n in self.roots]))\n        results = list(flatten(results))\n        self._results = results  # For use in plotting\n        return merge_results(results, **merge_kwargs) if merge else results", "response": "Executes the graph by calling all Transformers in sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes the Transformer at a specific node. Returns a list of results.", "response": "def run_node(self, node, stim):\n        ''' Executes the Transformer at a specific node.\n\n        Args:\n            node (str, Node): If a string, the name of the Node in the current\n                Graph. Otherwise the Node instance to execute.\n            stim (str, stim, list): Any valid input to the Transformer stored\n                at the target node.\n        '''\n        if isinstance(node, string_types):\n            node = self.nodes[node]\n\n        result = node.transformer.transform(stim)\n        if node.is_leaf():\n            return listify(result)\n\n        stim = result\n        # If result is a generator, the first child will destroy the\n        # iterable, so cache via list conversion\n        if len(node.children) > 1 and isgenerator(stim):\n            stim = list(stim)\n        return list(chain(*[self.run_node(c, stim) for c in node.children]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering a graph of the current object.", "response": "def draw(self, filename, color=True):\n        ''' Render a plot of the graph via pygraphviz.\n\n        Args:\n            filename (str): Path to save the generated image to.\n            color (bool): If True, will color graph nodes based on their type,\n                otherwise will draw a black-and-white graph.\n        '''\n        verify_dependencies(['pgv'])\n        if not hasattr(self, '_results'):\n            raise RuntimeError(\"Graph cannot be drawn before it is executed. \"\n                               \"Try calling run() first.\")\n\n        g = pgv.AGraph(directed=True)\n        g.node_attr['colorscheme'] = 'set312'\n\n        for elem in self._results:\n            if not hasattr(elem, 'history'):\n                continue\n            log = elem.history\n\n            while log:\n                # Configure nodes\n                source_from = log.parent[6] if log.parent else ''\n                s_node = hash((source_from, log[2]))\n                s_color = stim_list.index(log[2])\n                s_color = s_color % 12 + 1\n\n                t_node = hash((log[6], log[7]))\n                t_style = 'filled,' if color else ''\n                t_style += 'dotted' if log.implicit else ''\n                if log[6].endswith('Extractor'):\n                    t_color = '#0082c8'\n                elif log[6].endswith('Filter'):\n                    t_color = '#e6194b'\n                else:\n                    t_color = '#3cb44b'\n\n                r_node = hash((log[6], log[5]))\n                r_color = stim_list.index(log[5])\n                r_color = r_color % 12 + 1\n\n                # Add nodes\n                if color:\n                    g.add_node(s_node, label=log[2], shape='ellipse',\n                               style='filled', fillcolor=s_color)\n                    g.add_node(t_node, label=log[6], shape='box',\n                               style=t_style, fillcolor=t_color)\n                    g.add_node(r_node, label=log[5], shape='ellipse',\n                               style='filled', fillcolor=r_color)\n                else:\n                    g.add_node(s_node, label=log[2], shape='ellipse')\n                    g.add_node(t_node, label=log[6], shape='box',\n                               style=t_style)\n                    g.add_node(r_node, label=log[5], shape='ellipse')\n\n                # Add edges\n                g.add_edge(s_node, t_node, style=t_style)\n                g.add_edge(t_node, r_node, style=t_style)\n                log = log.parent\n\n        g.draw(filename, prog='dot')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the JSON representation of this graph.", "response": "def to_json(self):\n        ''' Returns the JSON representation of this graph. '''\n        roots = []\n        for r in self.roots:\n            roots.append(r.to_json())\n        return {'roots': roots}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the JSON representation of this graph to the provided filename.", "response": "def save(self, filename):\n        ''' Writes the JSON representation of this graph to the provided\n        filename, such that the graph can be easily reconstructed using\n        Graph(spec=filename).\n\n        Args:\n            filename (str): Path at which to write out the json file.\n        '''\n        with open(filename, 'w') as outfile:\n            json.dump(self.to_json(), outfile)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef batch_iterable(l, n):\n    ''' Chunks iterable into n sized batches\n    Solution from: http://stackoverflow.com/questions/1915170/split-a-generator-iterable-every-n-items-in-python-splitevery'''\n    i = iter(l)\n    piece = list(islice(i, n))\n    while piece:\n        yield piece\n        piece = list(islice(i, n))", "response": "Yields n sized batches from a list iterable into n sized batches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_iterable_type(obj):\n    ''' Returns either a generator or a list depending on config-level\n    settings. Should be used to wrap almost every internal iterable return.\n    Also inspects elements recursively in the case of list returns, to\n    ensure that there are no nested generators. '''\n    if not isiterable(obj):\n        return obj\n\n    if config.get_option('use_generators'):\n        return obj if isgenerator(obj) else (i for i in obj)\n    else:\n        return [set_iterable_type(i) for i in obj]", "response": "Returns either a generator or a list depending on config - level\n    settings. Should be used to wrap almost every internal iterable return."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isgenerator(obj):\n    ''' Returns True if object is a generator, or a generator wrapped by a\n    tqdm object. '''\n    return isinstance(obj, GeneratorType) or (hasattr(obj, 'iterable') and\n           isinstance(getattr(obj, 'iterable'), GeneratorType))", "response": "Returns True if object is a generator or a\n    tqdm object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap that applies tqdm progress bar conditional on config settings.", "response": "def progress_bar_wrapper(iterable, **kwargs):\n    ''' Wrapper that applies tqdm progress bar conditional on config settings.\n    '''\n    return tqdm(iterable, **kwargs) if (config.get_option('progress_bar')\n        and not isinstance(iterable, tqdm)) else iterable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the video frame at the specified index.", "response": "def get_frame(self, index):\n        ''' Get video frame at the specified index.\n\n        Args:\n            index (int): Positional index of the desired frame.\n        '''\n\n        frame_num = self.frame_index[index]\n        onset = float(frame_num) / self.fps\n\n        if index < self.n_frames - 1:\n            next_frame_num = self.frame_index[index + 1]\n            end = float(next_frame_num) / self.fps\n        else:\n            end = float(self.duration)\n\n        duration = end - onset if end > onset else 0.0\n\n        return VideoFrameStim(self, frame_num,\n                              data=self.clip.get_frame(onset),\n                              duration=duration)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, path):\n        ''' Save source video to file.\n\n        Args:\n            path (str): Filename to save to.\n\n        Notes: Saves entire source video to file, not just currently selected\n            frames.\n        '''\n        # IMPORTANT WARNING: saves entire source video\n        self.clip.write_videofile(path, audio_fps=self.clip.audio.fps)", "response": "Save source video to file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the next available frame in the sequence of frames.", "response": "def get_frame(self, index=None, onset=None):\n        ''' Overrides the default behavior by giving access to the onset\n        argument.\n\n        Args:\n            index (int): Positional index of the desired frame.\n            onset (float): Onset (in seconds) of the desired frame.\n        '''\n        if onset:\n            index = int(onset * self.fps)\n\n        return super(VideoStim, self).get_frame(index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhashes list of data strings or data", "response": "def hash_data(data, blocksize=65536):\n    \"\"\"\" Hashes list of data, strings or data \"\"\"\n    data = pickle.dumps(data)\n\n    hasher = hashlib.sha1()\n    hasher.update(data)\n\n    return hasher.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_updates(transformers, datastore=None, stimuli=None):\n    # Find datastore file\n    datastore = datastore or expanduser('~/.pliers_updates')\n    prior_data = pd.read_csv(datastore) if exists(datastore) else None\n\n    # Load stimuli\n    stimuli = stimuli or glob.glob(\n        join(dirname(realpath(__file__)), '../tests/data/image/CC0/*'))\n    stimuli = load_stims(stimuli)\n\n    # Get transformers\n    loaded_transformers = {get_transformer(name, **params): (name, params)\n                           for name, params in transformers}\n\n    # Transform stimuli\n    results = pd.DataFrame({'time_extracted': [datetime.datetime.now()]})\n    for trans in loaded_transformers.keys():\n        for stim in stimuli:\n            if trans._stim_matches_input_types(stim):\n                res = trans.transform(stim)\n\n                try: # Add iterable\n                    res = [getattr(res, '_data', res.data) for r in res]\n                except TypeError:\n                    res = getattr(res, '_data', res.data)\n\n                res = hash_data(res)\n\n                results[\"{}.{}\".format(trans.__hash__(), stim.name)] = [res]\n\n    # Check for mismatches\n    mismatches = []\n    if prior_data is not None:\n        last = prior_data[\n            prior_data.time_extracted == prior_data.time_extracted.max()]. \\\n            iloc[0].drop('time_extracted')\n\n        for label, value in results.iteritems():\n            old = last.get(label)\n            new = value.values[0]\n\n            if old is not None:\n                if isinstance(new, str):\n                    if new != old:\n                        mismatches.append(label)\n                elif not np.isclose(old, new):\n                    mismatches.append(label)\n\n        results = prior_data.append(results)\n\n    results.to_csv(datastore, index=False)\n\n    # Get corresponding transformer name and parameters\n    def get_trans(hash_tr):\n        for obj, attr in loaded_transformers.items():\n            if str(obj.__hash__()) == hash_tr:\n                return attr\n\n    delta_t = set([m.split('.')[0] for m in mismatches])\n    delta_t = [get_trans(dt) for dt in delta_t]\n\n    return {'transformers': delta_t, 'mismatches': mismatches}", "response": "Checks if output has changed in a battery of stimuli."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks for valid mac adresses.", "response": "def valid_miflora_mac(mac, pat=re.compile(r\"C4:7C:8D:[0-9A-F]{2}:[0-9A-F]{2}:[0-9A-F]{2}\")):\n    \"\"\"Check for valid mac adresses.\"\"\"\n    if not pat.match(mac.upper()):\n        raise argparse.ArgumentTypeError('The MAC address \"{}\" seems to be in the wrong format'.format(mac))\n    return mac"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npolls data from the sensor.", "response": "def poll(args):\n    \"\"\"Poll data from the sensor.\"\"\"\n    backend = _get_backend(args)\n    poller = MiFloraPoller(args.mac, backend)\n    print(\"Getting data from Mi Flora\")\n    print(\"FW: {}\".format(poller.firmware_version()))\n    print(\"Name: {}\".format(poller.name()))\n    print(\"Temperature: {}\".format(poller.parameter_value(MI_TEMPERATURE)))\n    print(\"Moisture: {}\".format(poller.parameter_value(MI_MOISTURE)))\n    print(\"Light: {}\".format(poller.parameter_value(MI_LIGHT)))\n    print(\"Conductivity: {}\".format(poller.parameter_value(MI_CONDUCTIVITY)))\n    print(\"Battery: {}\".format(poller.parameter_value(MI_BATTERY)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_backend(args):\n    if args.backend == 'gatttool':\n        backend = GatttoolBackend\n    elif args.backend == 'bluepy':\n        backend = BluepyBackend\n    elif args.backend == 'pygatt':\n        backend = PygattBackend\n    else:\n        raise Exception('unknown backend: {}'.format(args.backend))\n    return backend", "response": "Extract the backend class from the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all available backends.", "response": "def list_backends(_):\n    \"\"\"List all available backends.\"\"\"\n    backends = [b.__name__ for b in available_backends()]\n    print('\\n'.join(backends))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--backend', choices=['gatttool', 'bluepy', 'pygatt'], default='gatttool')\n    parser.add_argument('-v', '--verbose', action='store_const', const=True)\n    subparsers = parser.add_subparsers(help='sub-command help', )\n\n    parser_poll = subparsers.add_parser('poll', help='poll data from a sensor')\n    parser_poll.add_argument('mac', type=valid_miflora_mac)\n    parser_poll.set_defaults(func=poll)\n\n    parser_scan = subparsers.add_parser('scan', help='scan for devices')\n    parser_scan.set_defaults(func=scan)\n\n    parser_scan = subparsers.add_parser('backends', help='list the available backends')\n    parser_scan.set_defaults(func=list_backends)\n\n    args = parser.parse_args()\n\n    if args.verbose:\n        logging.basicConfig(level=logging.DEBUG)\n\n    if not hasattr(args, \"func\"):\n        parser.print_help()\n        sys.exit(0)\n\n    args.func(args)", "response": "Main function.\n\n    Mostly parsing the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scan(backend, timeout=10):\n    result = []\n    for (mac, name) in backend.scan_for_devices(timeout):\n        if (name is not None and name.lower() in VALID_DEVICE_NAMES) or \\\n                mac is not None and mac.upper().startswith(DEVICE_PREFIX):\n            result.append(mac.upper())\n    return result", "response": "Scan for miflora devices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling the cache with new data from the sensor.", "response": "def fill_cache(self):\n        \"\"\"Fill the cache with new data from the sensor.\"\"\"\n        _LOGGER.debug('Filling cache with new sensor data.')\n        try:\n            firmware_version = self.firmware_version()\n        except BluetoothBackendException:\n            # If a sensor doesn't work, wait 5 minutes before retrying\n            self._last_read = datetime.now() - self._cache_timeout + \\\n                timedelta(seconds=300)\n            raise\n\n        with self._bt_interface.connect(self._mac) as connection:\n            if firmware_version >= \"2.6.6\":\n                # for the newer models a magic number must be written before we can read the current data\n                try:\n                    connection.write_handle(_HANDLE_WRITE_MODE_CHANGE, _DATA_MODE_CHANGE)   # pylint: disable=no-member\n                    # If a sensor doesn't work, wait 5 minutes before retrying\n                except BluetoothBackendException:\n                    self._last_read = datetime.now() - self._cache_timeout + \\\n                        timedelta(seconds=300)\n                    return\n            self._cache = connection.read_handle(_HANDLE_READ_SENSOR_DATA)  # pylint: disable=no-member\n            _LOGGER.debug('Received result for handle %s: %s',\n                          _HANDLE_READ_SENSOR_DATA, self._format_bytes(self._cache))\n            self._check_data()\n            if self.cache_available():\n                self._last_read = datetime.now()\n            else:\n                # If a sensor doesn't work, wait 5 minutes before retrying\n                self._last_read = datetime.now() - self._cache_timeout + \\\n                    timedelta(seconds=300)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the firmware version.", "response": "def firmware_version(self):\n        \"\"\"Return the firmware version.\"\"\"\n        if (self._firmware_version is None) or \\\n                (datetime.now() - timedelta(hours=24) > self._fw_last_read):\n            self._fw_last_read = datetime.now()\n            with self._bt_interface.connect(self._mac) as connection:\n                res = connection.read_handle(_HANDLE_READ_VERSION_BATTERY)  # pylint: disable=no-member\n                _LOGGER.debug('Received result for handle %s: %s',\n                              _HANDLE_READ_VERSION_BATTERY, self._format_bytes(res))\n            if res is None:\n                self.battery = 0\n                self._firmware_version = None\n            else:\n                self.battery = res[0]\n                self._firmware_version = \"\".join(map(chr, res[2:]))\n        return self._firmware_version"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of a monitored parameter.", "response": "def parameter_value(self, parameter, read_cached=True):\n        \"\"\"Return a value of one of the monitored paramaters.\n\n        This method will try to retrieve the data from cache and only\n        request it by bluetooth if no cached value is stored or the cache is\n        expired.\n        This behaviour can be overwritten by the \"read_cached\" parameter.\n        \"\"\"\n        # Special handling for battery attribute\n        if parameter == MI_BATTERY:\n            return self.battery_level()\n\n        # Use the lock to make sure the cache isn't updated multiple times\n        with self.lock:\n            if (read_cached is False) or \\\n                    (self._last_read is None) or \\\n                    (datetime.now() - self._cache_timeout > self._last_read):\n                self.fill_cache()\n            else:\n                _LOGGER.debug(\"Using cache (%s < %s)\",\n                              datetime.now() - self._last_read,\n                              self._cache_timeout)\n\n        if self.cache_available() and (len(self._cache) == 16):\n            return self._parse_data()[parameter]\n        else:\n            raise BluetoothBackendException(\"Could not read data from Mi Flora sensor %s\" % self._mac)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that the data in the cache is valid.", "response": "def _check_data(self):\n        \"\"\"Ensure that the data in the cache is valid.\n\n        If it's invalid, the cache is wiped.\n        \"\"\"\n        if not self.cache_available():\n            return\n        if self._cache[7] > 100:  # moisture over 100 procent\n            self.clear_cache()\n            return\n        if self._firmware_version >= \"2.6.6\":\n            if sum(self._cache[10:]) == 0:\n                self.clear_cache()\n                return\n        if sum(self._cache) == 0:\n            self.clear_cache()\n            return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the sensor data and returns a dictionary of the values.", "response": "def _parse_data(self):\n        \"\"\"Parses the byte array returned by the sensor.\n\n        The sensor returns 16 bytes in total. It's unclear what the meaning of these bytes\n        is beyond what is decoded in this method.\n\n        semantics of the data (in little endian encoding):\n        bytes   0-1: temperature in 0.1 \u00b0C\n        byte      2: unknown\n        bytes   3-6: brightness in Lux\n        byte      7: moisture in %\n        byted   8-9: conductivity in \u00b5S/cm\n        bytes 10-15: unknown\n        \"\"\"\n        data = self._cache\n        res = dict()\n        temp, res[MI_LIGHT], res[MI_MOISTURE], res[MI_CONDUCTIVITY] = \\\n            unpack('<hxIBhxxxxxx', data)\n        res[MI_TEMPERATURE] = temp/10.0\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_version_string(version_string):\n    string_parts =  version_string.split(\".\")\n    version_parts = [\n        int(re.match(\"([0-9]*)\", string_parts[0]).group(0)),\n        int(re.match(\"([0-9]*)\", string_parts[1]).group(0)),\n        int(re.match(\"([0-9]*)\", string_parts[2]).group(0))\n    ]\n    return version_parts", "response": "Parses a semver version string stripping off rc stuff if present."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bigger_version(version_string_a, version_string_b):\n    major_a, minor_a, patch_a = parse_version_string(version_string_a)\n    major_b, minor_b, patch_b = parse_version_string(version_string_b)\n\n    if major_a > major_b:\n        return version_string_a\n    elif major_a == major_b and minor_a > minor_b:\n        return version_string_a\n    elif major_a == major_b and minor_a == minor_b and patch_a > patch_b:\n        return version_string_a\n    return version_string_b", "response": "Returns the bigger version of two version strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nversions check decorator. Currently only checks Bigger Than.", "response": "def api_version(created_ver, last_changed_ver, return_value_ver):\n    \"\"\"Version check decorator. Currently only checks Bigger Than.\"\"\"\n    def api_min_version_decorator(function):      \n        def wrapper(function, self, *args, **kwargs):\n            if not self.version_check_mode == \"none\":\n                if self.version_check_mode == \"created\":\n                    version = created_ver\n                else:\n                    version = bigger_version(last_changed_ver, return_value_ver)\n                major, minor, patch = parse_version_string(version)\n                if major > self.mastodon_major:\n                    raise MastodonVersionError(\"Version check failed (Need version \" + version + \")\")\n                elif major == self.mastodon_major and minor > self.mastodon_minor:\n                    print(self.mastodon_minor)\n                    raise MastodonVersionError(\"Version check failed (Need version \" + version + \")\")\n                elif major == self.mastodon_major and minor == self.mastodon_minor and patch > self.mastodon_patch:\n                    raise MastodonVersionError(\"Version check failed (Need version \" + version + \", patch is \" + str(self.mastodon_patch) + \")\")\n            return function(self, *args, **kwargs)\n        function.__doc__ = function.__doc__ + \"\\n\\n        *Added: Mastodon v\" + created_ver + \", last changed: Mastodon v\" + last_changed_ver + \"*\"\n        return decorate(function, wrapper)\n    return api_min_version_decorator"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new app with given client_name and scopes.", "response": "def create_app(client_name, scopes=__DEFAULT_SCOPES, redirect_uris=None, website=None, to_file=None,\n                   api_base_url=__DEFAULT_BASE_URL, request_timeout=__DEFAULT_TIMEOUT, session=None):\n        \"\"\"\n        Create a new app with given `client_name` and `scopes` (The basic scropse are \"read\", \"write\", \"follow\" and \"push\" \n        - more granular scopes are available, please refere to Mastodon documentation for which).\n\n        Specify `redirect_uris` if you want users to be redirected to a certain page after authenticating in an oauth flow.\n        You can specify multiple URLs by passing a list. Note that if you wish to use OAuth authentication with redirects,\n        the redirect URI must be one of the URLs specified here.\n        \n        Specify `to_file` to persist your apps info to a file so you can use them in the constructor.\n        Specify `api_base_url` if you want to register an app on an instance different from the flagship one.        \n        Specify `website` to give a website for your app.\n\n        Specify `session` with a requests.Session for it to be used instead of the deafult.\n        \n        Presently, app registration is open by default, but this is not guaranteed to be the case for all\n        future mastodon instances or even the flagship instance in the future.\n        \n\n        Returns `client_id` and `client_secret`, both as strings.\n        \"\"\"\n        api_base_url = Mastodon.__protocolize(api_base_url)\n\n        request_data = {\n            'client_name': client_name,\n            'scopes': \" \".join(scopes)\n        }\n\n        try:\n            if redirect_uris is not None:\n                if isinstance(redirect_uris, (list, tuple)):\n                    redirect_uris = \"\\n\".join(list(redirect_uris))\n                request_data['redirect_uris'] = redirect_uris\n            else:\n                request_data['redirect_uris'] = 'urn:ietf:wg:oauth:2.0:oob'\n            if website is not None:\n                request_data['website'] = website\n            if session:\n                ret = session.post(api_base_url + '/api/v1/apps', data=request_data, timeout=request_timeout)\n                response = ret.json()\n            else:\n                response = requests.post(api_base_url + '/api/v1/apps', data=request_data, timeout=request_timeout)\n                response = response.json()\n        except Exception as e:\n            raise MastodonNetworkError(\"Could not complete request: %s\" % e)\n\n        if to_file is not None:\n            with open(to_file, 'w') as secret_file:\n                secret_file.write(response['client_id'] + '\\n')\n                secret_file.write(response['client_secret'] + '\\n')\n\n        return (response['client_id'], response['client_secret'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retrieve_mastodon_version(self):\n        try:\n            version_str = self.__instance()[\"version\"]\n        except:\n            # instance() was added in 1.1.0, so our best guess is 1.0.0.\n            version_str = \"1.0.0\"\n            \n        self.mastodon_major, self.mastodon_minor, self.mastodon_patch = parse_version_string(version_str)\n        return version_str", "response": "Retrieves the installed mastodon version and sets major minor and patch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_minimum_version(self, version_str):\n        self.retrieve_mastodon_version()\n        major, minor, patch = parse_version_string(version_str)\n        if major > self.mastodon_major:\n            return False\n        elif major == self.mastodon_major and minor > self.mastodon_minor:\n            return False\n        elif major == self.mastodon_major and minor == self.mastodon_minor and patch > self.mastodon_patch:\n            return False\n        return True", "response": "Verify that the minimum version of the user is present."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auth_request_url(self, client_id=None, redirect_uris=\"urn:ietf:wg:oauth:2.0:oob\",\n                         scopes=__DEFAULT_SCOPES, force_login=False):\n        \"\"\"\n        Returns the url that a client needs to request an oauth grant from the server.\n        \n        To log in with oauth, send your user to this URL. The user will then log in and\n        get a code which you can pass to log_in.\n        \n        scopes are as in `log_in()`_, redirect_uris is where the user should be redirected to\n        after authentication. Note that redirect_uris must be one of the URLs given during\n        app registration. When using urn:ietf:wg:oauth:2.0:oob, the code is simply displayed,\n        otherwise it is added to the given URL as the \"code\" request parameter.\n        \n        Pass force_login if you want the user to always log in even when already logged\n        into web mastodon (i.e. when registering multiple different accounts in an app).\n        \"\"\"\n        if client_id is None:\n            client_id = self.client_id\n        else:\n            if os.path.isfile(client_id):\n                with open(client_id, 'r') as secret_file:\n                    client_id = secret_file.readline().rstrip()\n\n        params = dict()\n        params['client_id'] = client_id\n        params['response_type'] = \"code\"\n        params['redirect_uri'] = redirect_uris\n        params['scope'] = \" \".join(scopes)\n        params['force_login'] = force_login\n        formatted_params = urlencode(params)\n        return \"\".join([self.api_base_url, \"/oauth/authorize?\", formatted_params])", "response": "Returns the url that a client needs to request an oauth grant from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog in a user into the Mastodon.", "response": "def log_in(self, username=None, password=None,\n               code=None, redirect_uri=\"urn:ietf:wg:oauth:2.0:oob\", refresh_token=None,\n               scopes=__DEFAULT_SCOPES, to_file=None):\n        \"\"\"\n        Get the access token for a user.\n        \n        The username is the e-mail used to log in into mastodon.\n\n        Can persist access token to file `to_file`, to be used in the constructor.\n\n        Handles password and OAuth-based authorization.\n        \n        Will throw a `MastodonIllegalArgumentError` if the OAuth or the\n        username / password credentials given are incorrect, and\n        `MastodonAPIError` if all of the requested scopes were not granted.\n\n        For OAuth2, obtain a code via having your user go to the url returned by \n        `auth_request_url()`_ and pass it as the code parameter. In this case,\n        make sure to also pass the same redirect_uri parameter as you used when\n        generating the auth request URL.\n\n        Returns the access token as a string.\n        \"\"\"\n        if username is not None and password is not None:\n            params = self.__generate_params(locals(), ['scopes', 'to_file', 'code', 'refresh_token'])\n            params['grant_type'] = 'password'\n        elif code is not None:\n            params = self.__generate_params(locals(), ['scopes', 'to_file', 'username', 'password', 'refresh_token'])\n            params['grant_type'] = 'authorization_code'\n        elif refresh_token is not None:\n            params = self.__generate_params(locals(), ['scopes', 'to_file', 'username', 'password', 'code'])\n            params['grant_type'] = 'refresh_token'\n        else:\n            raise MastodonIllegalArgumentError('Invalid arguments given. username and password or code are required.')\n\n        params['client_id'] = self.client_id\n        params['client_secret'] = self.client_secret\n        params['scope'] = \" \".join(scopes)\n\n        try:\n            response = self.__api_request('POST', '/oauth/token', params, do_ratelimiting=False)\n            self.access_token = response['access_token']\n            self.__set_refresh_token(response.get('refresh_token'))\n            self.__set_token_expired(int(response.get('expires_in', 0)))\n        except Exception as e:\n            if username is not None or password is not None:\n                raise MastodonIllegalArgumentError('Invalid user name, password, or redirect_uris: %s' % e)\n            elif code is not None:\n                raise MastodonIllegalArgumentError('Invalid access token or redirect_uris: %s' % e)\n            else:\n                raise MastodonIllegalArgumentError('Invalid request: %s' % e)\n\n        received_scopes = response[\"scope\"].split(\" \")\n        for scope_set in self.__SCOPE_SETS.keys():\n            if scope_set in received_scopes:\n                received_scopes += self.__SCOPE_SETS[scope_set]\n        \n        if not set(scopes) <= set(received_scopes):\n            raise MastodonAPIError(\n                'Granted scopes \"' + \" \".join(received_scopes) + '\" do not contain all of the requested scopes \"' + \" \".join(scopes) + '\".')\n\n        if to_file is not None:\n            with open(to_file, 'w') as token_file:\n                token_file.write(response['access_token'] + '\\n')\n        \n        self.__logged_in_id = None\n        \n        return response['access_token']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_account(self, username, password, email, agreement=False, locale=\"en\", scopes=__DEFAULT_SCOPES, to_file=None):\n        params = self.__generate_params(locals(), ['to_file', 'scopes'])\n        params['client_id'] = self.client_id\n        params['client_secret'] = self.client_secret\n        \n        if agreement == False:\n            del params_initial['agreement']\n        \n        # Step 1: Get a user-free token via oauth\n        try:\n            oauth_params = {}\n            oauth_params['scope'] = \" \".join(scopes)\n            oauth_params['client_id'] = self.client_id\n            oauth_params['client_secret'] = self.client_secret\n            oauth_params['grant_type'] = 'client_credentials'\n            \n            response = self.__api_request('POST', '/oauth/token', oauth_params, do_ratelimiting=False)\n            temp_access_token = response['access_token']\n        except Exception as e:\n            raise MastodonIllegalArgumentError('Invalid request during oauth phase: %s' % e)\n        \n        # Step 2: Use that to create a user\n        try:\n            response = self.__api_request('POST', '/api/v1/accounts', params, do_ratelimiting=False, \n                                          access_token_override = temp_access_token)\n            self.access_token = response['access_token']\n            self.__set_refresh_token(response.get('refresh_token'))\n            self.__set_token_expired(int(response.get('expires_in', 0)))\n        except Exception as e:\n            raise MastodonIllegalArgumentError('Invalid request: %s' % e)\n        \n        # Step 3: Check scopes, persist, et cetera\n        received_scopes = response[\"scope\"].split(\" \")\n        for scope_set in self.__SCOPE_SETS.keys():\n            if scope_set in received_scopes:\n                received_scopes += self.__SCOPE_SETS[scope_set]\n        \n        if not set(scopes) <= set(received_scopes):\n            raise MastodonAPIError(\n                'Granted scopes \"' + \" \".join(received_scopes) + '\" do not contain all of the requested scopes \"' + \" \".join(scopes) + '\".')\n        \n        if to_file is not None:\n            with open(to_file, 'w') as token_file:\n                token_file.write(response['access_token'] + '\\n')\n        \n        self.__logged_in_id = None\n        \n        return response['access_token']", "response": "Create a new user account with the given username password and email."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches statuses most recent ones first.", "response": "def timeline(self, timeline=\"home\", max_id=None, min_id=None, since_id=None, limit=None):\n        \"\"\"\n        Fetch statuses, most recent ones first. `timeline` can be 'home', 'local', 'public',\n        'tag/hashtag' or 'list/id'. See the following functions documentation for what those do.\n        Local hashtag timelines are supported via the `timeline_hashtag()`_ function.\n        \n        The default timeline is the \"home\" timeline.\n\n        Media only queries are supported via the `timeline_public()`_ and `timeline_hashtag()`_ functions.\n\n        Returns a list of `toot dicts`_.\n        \"\"\"\n        if max_id != None:\n            max_id = self.__unpack_id(max_id)\n        \n        if min_id != None:\n            min_id = self.__unpack_id(min_id)\n            \n        if since_id != None:\n            since_id = self.__unpack_id(since_id)\n        \n        params_initial = locals()\n\n        if timeline == \"local\":\n            timeline = \"public\"\n            params_initial['local'] = True\n\n        params = self.__generate_params(params_initial, ['timeline'])\n        url = '/api/v1/timelines/{0}'.format(timeline)\n        return self.__api_request('GET', url, params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch the logged - in users home timeline.", "response": "def timeline_home(self, max_id=None, min_id=None, since_id=None, limit=None):\n        \"\"\"\n        Fetch the logged-in users home timeline (i.e. followed users and self).\n\n        Returns a list of `toot dicts`_.\n        \"\"\"\n        return self.timeline('home', max_id=max_id, min_id=min_id, \n                             since_id=since_id, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching the local timeline without replies.", "response": "def timeline_local(self, max_id=None, min_id=None, since_id=None, limit=None):\n        \"\"\"\n        Fetches the local / instance-wide timeline, not including replies.\n\n        Returns a list of `toot dicts`_.\n        \"\"\"\n        return self.timeline('local', max_id=max_id, min_id=min_id,\n                             since_id=since_id, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timeline_public(self, max_id=None, min_id=None, since_id=None, limit=None, only_media=False):\n        if max_id != None:\n            max_id = self.__unpack_id(max_id)\n            \n        if min_id != None:\n            min_id = self.__unpack_id(min_id)\n        \n        if since_id != None:\n            since_id = self.__unpack_id(since_id)\n        \n        params_initial = locals()        \n        \n        if only_media == False:\n            del params_initial['only_media']\n            \n        url = '/api/v1/timelines/public'        \n        params = self.__generate_params(params_initial)\n        \n        return self.__api_request('GET', url, params)", "response": "Fetch the public timeline."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching a list of toot dicts with a given hashtag.", "response": "def timeline_hashtag(self, hashtag, local=False, max_id=None, min_id=None, since_id=None, limit=None, only_media=False):\n        \"\"\"\n        Fetch a timeline of toots with a given hashtag. The hashtag parameter\n        should not contain the leading #.\n\n        Set `local` to True to retrieve only instance-local tagged posts.\n        Set `only_media` to True to retrieve only statuses with media attachments.\n        \n        Returns a list of `toot dicts`_.\n        \"\"\"\n        if hashtag.startswith(\"#\"):\n            raise MastodonIllegalArgumentError(\"Hashtag parameter should omit leading #\")\n            \n        if max_id != None:\n            max_id = self.__unpack_id(max_id)\n        \n        if min_id != None:\n            min_id = self.__unpack_id(min_id)        \n        \n        if since_id != None:\n            since_id = self.__unpack_id(since_id)\n            \n        params_initial = locals()        \n        \n        if local == False:\n            del params_initial['local']\n        \n        if only_media == False:\n            del params_initial['only_media']\n            \n        url = '/api/v1/timelines/tag/{0}'.format(hashtag)        \n        params = self.__generate_params(params_initial, ['hashtag'])\n        \n        return self.__api_request('GET', url, params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef timeline_list(self, id, max_id=None, min_id=None, since_id=None, limit=None):\n        id = self.__unpack_id(id)\n        return self.timeline('list/{0}'.format(id), max_id=max_id,\n                             min_id=min_id, since_id=since_id, limit=limit)", "response": "Fetch a list of toots by users in a given list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conversations(self, max_id=None, min_id=None, since_id=None, limit=None):\n        if max_id != None:\n            max_id = self.__unpack_id(max_id)\n        \n        if min_id != None:\n            min_id = self.__unpack_id(min_id)\n            \n        if since_id != None:\n            since_id = self.__unpack_id(since_id)            \n        \n        params = self.__generate_params(locals())\n        return self.__api_request('GET', \"/api/v1/conversations/\", params)", "response": "Fetches a users conversations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching information about a single toot.", "response": "def status(self, id):\n        \"\"\"\n        Fetch information about a single toot.\n\n        Does not require authentication for publicly visible statuses.\n\n        Returns a `toot dict`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}'.format(str(id))\n        return self.__api_request('GET', url)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch a card associated with a status.", "response": "def status_card(self, id):\n        \"\"\"\n        Fetch a card associated with a status. A card describes an object (such as an\n        external video or link) embedded into a status.\n\n        Does not require authentication for publicly visible statuses.\n\n        Returns a `card dict`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/card'.format(str(id))\n        return self.__api_request('GET', url)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch information about ancestors and descendants of a toot.", "response": "def status_context(self, id):\n        \"\"\"\n        Fetch information about ancestors and descendants of a toot.\n\n        Does not require authentication for publicly visible statuses.\n\n        Returns a `context dict`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/context'.format(str(id))\n        return self.__api_request('GET', url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef status_reblogged_by(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/reblogged_by'.format(str(id))\n        return self.__api_request('GET', url)", "response": "Fetch a list of users that have reblogged a status."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef status_favourited_by(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/favourited_by'.format(str(id))\n        return self.__api_request('GET', url)", "response": "Fetch a list of users that have favourited a status."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scheduled_status(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/scheduled_statuses/{0}'.format(str(id))\n        return self.__api_request('GET', url)", "response": "Fetch information about the scheduled status with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef poll(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/polls/{0}'.format(str(id))\n        return self.__api_request('GET', url)", "response": "Fetch information about the poll with the given id. Returns a dict."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching user information by user id.", "response": "def account(self, id):\n        \"\"\"\n        Fetch account information by user `id`.\n        \n        Does not require authentication.\n        \n        Returns a `user dict`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/accounts/{0}'.format(str(id))\n        return self.__api_request('GET', url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching all the statuses of a user.", "response": "def account_statuses(self, id, only_media=False, pinned=False, exclude_replies=False, max_id=None, min_id=None, since_id=None, limit=None):\n        \"\"\"\n        Fetch statuses by user `id`. Same options as `timeline()`_ are permitted.\n        Returned toots are from the perspective of the logged-in user, i.e.\n        all statuses visible to the logged-in user (including DMs) are\n        included.\n\n        If `only_media` is set, return only statuses with media attachments.\n        If `pinned` is set, return only statuses that have been pinned. Note that \n        as of Mastodon 2.1.0, this only works properly for instance-local users.\n        If `exclude_replies` is set, filter out all statuses that are replies.\n\n        Does not require authentication.\n\n        Returns a list of `toot dicts`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        if max_id != None:\n            max_id = self.__unpack_id(max_id)\n        \n        if min_id != None:\n            min_id = self.__unpack_id(min_id)\n        \n        if since_id != None:\n            since_id = self.__unpack_id(since_id)\n        \n        params = self.__generate_params(locals(), ['id'])\n        if pinned == False:\n            del params[\"pinned\"]\n        if only_media == False:\n            del params[\"only_media\"]\n        if exclude_replies == False:\n            del params[\"exclude_replies\"]\n        \n        url = '/api/v1/accounts/{0}/statuses'.format(str(id))\n        return self.__api_request('GET', url, params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef account_following(self, id, max_id=None, min_id=None, since_id=None, limit=None):\n        id = self.__unpack_id(id)\n        if max_id != None:\n            max_id = self.__unpack_id(max_id)\n        \n        if min_id != None:\n            min_id = self.__unpack_id(min_id)\n        \n        if since_id != None:\n            since_id = self.__unpack_id(since_id)\n            \n        params = self.__generate_params(locals(), ['id'])\n        url = '/api/v1/accounts/{0}/following'.format(str(id))\n        return self.__api_request('GET', url, params)", "response": "Fetch users the given user is following."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch relationship between a logged in user and a given account.", "response": "def account_relationships(self, id):\n        \"\"\"\n        Fetch relationship (following, followed_by, blocking, follow requested) of \n        the logged in user to a given account. `id` can be a list.\n\n        Returns a list of `relationship dicts`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals())\n        return self.__api_request('GET', '/api/v1/accounts/relationships',\n                                  params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef account_search(self, q, limit=None, following=False):\n        params = self.__generate_params(locals())\n        \n        if params[\"following\"] == False:\n            del params[\"following\"]\n            \n        return self.__api_request('GET', '/api/v1/accounts/search', params)", "response": "Search for matching accounts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all of the logged - in users lists which the specified user is a member of. Returns a list of dicts.", "response": "def account_lists(self, id):\n        \"\"\"\n        Get all of the logged-in users lists which the specified user is\n        a member of.\n        \n        Returns a list of `list dicts`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals(), ['id'])\n        url = '/api/v1/accounts/{0}/lists'.format(str(id))\n        return self.__api_request('GET', url, params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching information about the specified filter with the specified id.", "response": "def filter(self, id):\n        \"\"\"\n        Fetches information about the filter with the specified `id`.\n        \n        Returns a `filter dict`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/filters/{0}'.format(str(id))\n        return self.__api_request('GET', url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search(self, q, resolve=True, result_type=None, account_id=None, offset=None, min_id=None, max_id=None):\n        return self.search_v2(q, resolve=resolve, result_type=result_type, account_id=account_id,\n                              offset=offset, min_id=min_id, max_id=max_id)", "response": "Search for hashtags accounts and statuses."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch for hashtags in the current user s account.", "response": "def search_v1(self, q, resolve=False):\n        \"\"\"\n        Identical to `search_v2()`, except in that it does not return\n        tags as `hashtag dicts`_.\n\n        Returns a `search result dict`_.\n        \"\"\"\n        params = self.__generate_params(locals())\n        if resolve == False:\n            del params['resolve']\n        return self.__api_request('GET', '/api/v1/search', params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for hashtags in a user s account.", "response": "def search_v2(self, q, resolve=True, result_type=None, account_id=None, offset=None, min_id=None, max_id=None):\n        \"\"\"\n        Identical to `search_v1()`, except in that it returns tags as\n        `hashtag dicts`_, has more parameters, and resolves by default.\n\n        Returns a `search result dict`_.\n        \"\"\"\n        params = self.__generate_params(locals())\n        \n        if resolve == False:\n            del params['resolve']\n        \n        if \"result_type\" in params:\n            params[\"type\"] = params[\"result_type\"]\n            del params[\"result_type\"]\n        \n        return self.__api_request('GET', '/api/v2/search', params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch info about a specific list.", "response": "def list(self, id):\n        \"\"\"\n        Fetch info about a specific list.\n        \n        Returns a `list dict`_.\n        \"\"\"\n        id = self.__unpack_id(id)        \n        return self.__api_request('GET', '/api/v1/lists/{0}'.format(id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the accounts that are on the given list.", "response": "def list_accounts(self, id, max_id=None, min_id=None, since_id=None, limit=None):\n        \"\"\"\n        Get the accounts that are on the given list. A `limit` of 0 can\n        be specified to get all accounts without pagination.\n        \n        Returns a list of `user dicts`_.\n        \"\"\"\n        id = self.__unpack_id(id)\n        \n        if max_id != None:\n            max_id = self.__unpack_id(max_id)\n        \n        if min_id != None:\n            min_id = self.__unpack_id(min_id)\n        \n        if since_id != None:\n            since_id = self.__unpack_id(since_id)\n        \n        params = self.__generate_params(locals(), ['id']) \n        return self.__api_request('GET', '/api/v1/lists/{0}/accounts'.format(id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef status_post(self, status, in_reply_to_id=None, media_ids=None,\n                    sensitive=False, visibility=None, spoiler_text=None,\n                    language=None, idempotency_key=None, content_type=None,\n                    scheduled_at=None, poll=None):\n        \"\"\"\n        Post a status. Can optionally be in reply to another status and contain\n        media.\n        \n        `media_ids` should be a list. (If it's not, the function will turn it\n        into one.) It can contain up to four pieces of media (uploaded via \n        `media_post()`_). `media_ids` can also be the `media dicts`_ returned \n        by `media_post()`_ - they are unpacked automatically.\n\n        The `sensitive` boolean decides whether or not media attached to the post\n        should be marked as sensitive, which hides it by default on the Mastodon\n        web front-end.\n\n        The visibility parameter is a string value and accepts any of:\n        'direct' - post will be visible only to mentioned users\n        'private' - post will be visible only to followers\n        'unlisted' - post will be public but not appear on the public timeline\n        'public' - post will be public\n\n        If not passed in, visibility defaults to match the current account's\n        default-privacy setting (starting with Mastodon version 1.6) or its\n        locked setting - private if the account is locked, public otherwise\n        (for Mastodon versions lower than 1.6).\n\n        The `spoiler_text` parameter is a string to be shown as a warning before\n        the text of the status.  If no text is passed in, no warning will be\n        displayed.\n\n        Specify `language` to override automatic language detection. The parameter\n        accepts all valid ISO 639-2 language codes.\n\n        You can set `idempotency_key` to a value to uniquely identify an attempt\n        at posting a status. Even if you call this function more than once,\n        if you call it with the same `idempotency_key`, only one status will\n        be created.\n\n        Pass a datetime as `scheduled_at` to schedule the toot for a specific time\n        (the time must be at least 5 minutes into the future). If this is passed,\n        status_post returns a `scheduled toot dict`_ instead.\n\n        Pass `poll` to attach a poll to the status. An appropriate object can be\n        constructed using `make_poll()`_\n\n        Specify `content_type` to set the content type of your post on Pleroma.\n        It accepts 'text/plain' (default), 'text/markdown', and 'text/html'.\n        This parameter is not supported on Mastodon servers, but will be\n        safely ignored if set.\n\n        Returns a `toot dict`_ with the new status.\n        \"\"\"\n        if in_reply_to_id != None:\n            in_reply_to_id = self.__unpack_id(in_reply_to_id)\n        \n        if scheduled_at != None:\n            scheduled_at = self.__consistent_isoformat_utc(scheduled_at)\n        \n        params_initial = locals()\n\n        # Validate visibility parameter\n        valid_visibilities = ['private', 'public', 'unlisted', 'direct']\n        if params_initial['visibility'] == None:\n            del params_initial['visibility']\n        else:\n            params_initial['visibility'] = params_initial['visibility'].lower()\n            if params_initial['visibility'] not in valid_visibilities:\n                raise ValueError('Invalid visibility value! Acceptable '\n                                'values are %s' % valid_visibilities)\n\n        if params_initial['language'] == None:\n            del params_initial['language']\n\n        if params_initial['sensitive'] is False:\n            del [params_initial['sensitive']]\n\n        headers = {}\n        if idempotency_key != None:\n            headers['Idempotency-Key'] = idempotency_key\n            \n        if media_ids is not None:\n            try:\n                media_ids_proper = []\n                if not isinstance(media_ids, (list, tuple)):\n                    media_ids = [media_ids]\n                for media_id in media_ids:\n                    if isinstance(media_id, dict):\n                        media_ids_proper.append(media_id[\"id\"])\n                    else:\n                        media_ids_proper.append(media_id)\n            except Exception as e:\n                raise MastodonIllegalArgumentError(\"Invalid media \"\n                                                   \"dict: %s\" % e)\n\n            params_initial[\"media_ids\"] = media_ids_proper\n\n        if params_initial['content_type'] == None:\n            del params_initial['content_type']\n\n        use_json = False\n        if not poll is None:\n            use_json = True\n\n        params = self.__generate_params(params_initial, ['idempotency_key'])\n        return self.__api_request('POST', '/api/v1/statuses', params, headers = headers, use_json = use_json)", "response": "Create a status post for a status."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status_reply(self, to_status, status, media_ids=None, sensitive=False, visibility=None, \n                     spoiler_text=None, language=None, idempotency_key=None, content_type=None,\n                     scheduled_at=None, poll=None, untag=False):\n        \"\"\"\n        Helper function - acts like status_post, but prepends the name of all\n        the users that are being replied to to the status text and retains\n        CW and visibility if not explicitly overridden.\n        \n        Set `untag` to True if you want the reply to only go to the user you\n        are replying to, removing every other mentioned user from the\n        conversation.\n        \"\"\"\n        user_id = self.__get_logged_in_id()\n        \n        # Determine users to mention\n        mentioned_accounts = collections.OrderedDict()\n        mentioned_accounts[to_status.account.id] = to_status.account.acct\n        \n        if not untag:\n            for account in to_status.mentions:\n                if account.id != user_id and not account.id in mentioned_accounts.keys():\n                    mentioned_accounts[account.id] = account.acct\n                \n        # Join into one piece of text. The space is added inside because of self-replies.\n        status = \"\".join(map(lambda x: \"@\" + x + \" \", mentioned_accounts.values())) + status\n            \n        # Retain visibility / cw\n        if visibility == None and 'visibility' in to_status:\n            visibility = to_status.visibility\n        if spoiler_text == None and 'spoiler_text' in to_status:\n            spoiler_text = to_status.spoiler_text\n            \n        return self.status_post(status, in_reply_to_id = to_status.id, media_ids = media_ids, sensitive = sensitive, \n                         visibility = visibility, spoiler_text = spoiler_text, language = language,\n                         idempotency_key = idempotency_key, content_type = content_type, \n                         scheduled_at = scheduled_at, poll = poll)", "response": "This function is used to reply to a status message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_poll(self, options, expires_in, multiple=False, hide_totals=False):\n        poll_params = locals()\n        del poll_params[\"self\"]\n        return poll_params", "response": "Generates a poll object that can be passed as the poll option when posting a status."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a status from the cache", "response": "def status_delete(self, id):\n        \"\"\"\n        Delete a status\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}'.format(str(id))\n        self.__api_request('DELETE', url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef status_reblog(self, id, visibility=None):\n        params = self.__generate_params(locals(), ['id'])\n        valid_visibilities = ['private', 'public', 'unlisted', 'direct']\n        if 'visibility' in params:\n            params['visibility'] = params['visibility'].lower()\n            if params['visibility'] not in valid_visibilities:\n                raise ValueError('Invalid visibility value! Acceptable '\n                                'values are %s' % valid_visibilities)\n        \n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/reblog'.format(str(id))\n        return self.__api_request('POST', url, params)", "response": "Reblog / boost a status.\n        \n        The visibility parameter functions the same as in `status_post()`_ and\n        allows you to reduce the visibility of a reblogged status.\n\n        Returns a `toot dict`_ with a new status that wraps around the reblogged one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef status_unreblog(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/unreblog'.format(str(id))\n        return self.__api_request('POST', url)", "response": "Un - reblog a status."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef status_unfavourite(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/unfavourite'.format(str(id))\n        return self.__api_request('POST', url)", "response": "Un - favourite a status."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status_mute(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/mute'.format(str(id))\n        return self.__api_request('POST', url)", "response": "Mute notifications for a status."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npins a status for the logged - in user.", "response": "def status_pin(self, id):\n        \"\"\"\n        Pin a status for the logged-in user.\n\n        Returns a `toot dict`_ with the now pinned status\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/pin'.format(str(id))\n        return self.__api_request('POST', url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nunpinning a pinned status for the logged - in user.", "response": "def status_unpin(self, id):\n        \"\"\"\n        Unpin a pinned status for the logged-in user.\n\n        Returns a `toot dict`_ with the status that used to be pinned.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/unpin'.format(str(id))\n        return self.__api_request('POST', url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the scheduled time of a scheduled status.", "response": "def scheduled_status_update(self, id, scheduled_at):\n        \"\"\"\n        Update the scheduled time of a scheduled status.\n        \n        New time must be at least 5 minutes into the future.\n        \n        Returns a `scheduled toot dict`_\n        \"\"\"\n        scheduled_at = self.__consistent_isoformat_utc(scheduled_at)\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals(), ['id'])\n        url = '/api/v1/scheduled_statuses/{0}'.format(str(id))\n        return self.__api_request('PUT', url, params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a scheduled status.", "response": "def scheduled_status_delete(self, id):\n        \"\"\"\n        Deletes a scheduled status.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/scheduled_statuses/{0}'.format(str(id))\n        self.__api_request('DELETE', url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvoting in the given poll.", "response": "def poll_vote(self, id, choices):\n        \"\"\"\n        Vote in the given poll.\n        \n        `choices` is the index of the choice you wish to register a vote for \n        (i.e. its index in the corresponding polls `options` field. In case \n        of a poll that allows selection of more than one option, a list of\n        indices can be passed.\n        \n        You can only submit choices for any given poll once in case of\n        single-option polls, or only once per option in case of multi-option\n        polls.\n        \n        Returns the updated `poll dict`_\n        \"\"\"\n        id = self.__unpack_id(id)\n        if not isinstance(choices, list):\n            choices = [choices]\n        params = self.__generate_params(locals(), ['id'])\n        \n        url = '/api/v1/polls/{0}/votes'.format(id)\n        self.__api_request('POST', url, params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a single notification from the cache.", "response": "def notifications_dismiss(self, id):\n        \"\"\"\n        Deletes a single notification\n        \"\"\"\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals())\n        self.__api_request('POST', '/api/v1/notifications/dismiss', params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conversations_read(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/conversations/{0}/read'.format(str(id))\n        return self.__api_request('POST', url)", "response": "Mark a single conversation as read. Returns the updated dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef account_follow(self, id, reblogs=True):\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals())\n        \n        if params[\"reblogs\"] == None:\n            del params[\"reblogs\"]\n            \n        url = '/api/v1/accounts/{0}/follow'.format(str(id))\n        return self.__api_request('POST', url, params)", "response": "Follow a user.\n\n        Set `reblogs` to False to hide boosts by the followed user.\n\n        Returns a `relationship dict`_ containing the updated relationship to the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef follows(self, uri):\n        params = self.__generate_params(locals())\n        return self.__api_request('POST', '/api/v1/follows', params)", "response": "Follow a remote user by uri. Returns a dict of user attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nblock a user. Returns a `relationship dict`_ containing the updated relationship to the user.", "response": "def account_block(self, id):\n        \"\"\"\n        Block a user.\n\n        Returns a `relationship dict`_ containing the updated relationship to the user.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/accounts/{0}/block'.format(str(id))\n        return self.__api_request('POST', url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef account_unblock(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/accounts/{0}/unblock'.format(str(id))\n        return self.__api_request('POST', url)", "response": "Unblock a user.\n\n        Returns a `relationship dict`_ containing the updated relationship to the user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmuting a user s local time - store.", "response": "def account_mute(self, id, notifications=True):\n        \"\"\"\n        Mute a user.\n\n        Set `notifications` to False to receive notifications even though the user is\n        muted from timelines.\n\n        Returns a `relationship dict`_ containing the updated relationship to the user.\n        \"\"\"\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals(), ['id'])\n        url = '/api/v1/accounts/{0}/mute'.format(str(id))\n        return self.__api_request('POST', url, params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef account_unmute(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/accounts/{0}/unmute'.format(str(id))\n        return self.__api_request('POST', url)", "response": "Unmute a user.\n\n        Returns a `relationship dict`_ containing the updated relationship to the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef account_update_credentials(self, display_name=None, note=None,\n                                   avatar=None, avatar_mime_type=None,\n                                   header=None, header_mime_type=None, \n                                   locked=None, fields=None):\n        \"\"\"\n        Update the profile for the currently logged-in user.\n\n        'note' is the user's bio.\n\n        'avatar' and 'header' are images. As with media uploads, it is possible to either\n        pass image data and a mime type, or a filename of an image file, for either.\n        \n        'locked' specifies whether the user needs to manually approve follow requests.\n        \n        'fields' can be a list of up to four name-value pairs (specified as tuples) to \n        appear as semi-structured information in the users profile.\n        \n        Returns the updated `user dict` of the logged-in user.\n        \"\"\"\n        params_initial = collections.OrderedDict(locals())\n        \n        # Load avatar, if specified\n        if not avatar is None:\n            if avatar_mime_type is None and (isinstance(avatar, str) and os.path.isfile(avatar)):\n                avatar_mime_type = guess_type(avatar)\n                avatar = open(avatar, 'rb')\n\n            if avatar_mime_type is None:\n                raise MastodonIllegalArgumentError('Could not determine mime type or data passed directly without mime type.')\n        \n        # Load header, if specified\n        if not header is None:\n            if header_mime_type is None and (isinstance(avatar, str) and os.path.isfile(header)):\n                header_mime_type = guess_type(header)\n                header = open(header, 'rb')\n\n            if header_mime_type is None:\n                raise MastodonIllegalArgumentError('Could not determine mime type or data passed directly without mime type.')\n        \n        # Convert fields\n        if fields != None:\n            if len(fields) > 4:\n                raise MastodonIllegalArgumentError('A maximum of four fields are allowed.')\n            \n            fields_attributes = []\n            for idx, (field_name, field_value) in enumerate(fields):\n                params_initial['fields_attributes[' + str(idx) + '][name]'] = field_name\n                params_initial['fields_attributes[' + str(idx) + '][value]'] = field_value\n            \n        # Clean up params\n        for param in [\"avatar\", \"avatar_mime_type\", \"header\", \"header_mime_type\", \"fields\"]:\n            if param in params_initial:\n                del params_initial[param]\n        \n        # Create file info\n        files = {}\n        if not avatar is None:\n            avatar_file_name = \"mastodonpyupload_\" + mimetypes.guess_extension(avatar_mime_type)\n            files[\"avatar\"] = (avatar_file_name, avatar, avatar_mime_type)\n        if not header is None:\n            header_file_name = \"mastodonpyupload_\" + mimetypes.guess_extension(header_mime_type)\n            files[\"header\"] = (header_file_name, header, header_mime_type)\n        \n        params = self.__generate_params(params_initial)\n        return self.__api_request('PATCH', '/api/v1/accounts/update_credentials', params, files=files)", "response": "Update the credentials of the currently logged - in user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npin a user s endorsement cache.", "response": "def account_pin(self, id):\n        \"\"\"\n        Pin / endorse a user.\n        \n        Returns a `relationship dict`_ containing the updated relationship to the user.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/accounts/{0}/pin'.format(str(id))\n        return self.__api_request('POST', url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nunpinning / un-endorse a user. Returns a `relationship dict`_ containing the updated relationship to the user.", "response": "def account_unpin(self, id):\n        \"\"\"\n        Unpin / un-endorse a user.\n\n        Returns a `relationship dict`_ containing the updated relationship to the user.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/accounts/{0}/unpin'.format(str(id))\n        return self.__api_request('POST', url)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_create(self, phrase, context, irreversible = False, whole_word = True, expires_in = None):\n        params = self.__generate_params(locals())\n        \n        for context_val in context:\n            if not context_val in ['home', 'notifications', 'public', 'thread']:\n                raise MastodonIllegalArgumentError('Invalid filter context.')\n        \n        return self.__api_request('POST', '/api/v1/filters', params)", "response": "This method creates a new keyword filter. The filter is created for the given phrase and context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_update(self, id, phrase = None, context = None, irreversible = None, whole_word = None, expires_in = None):\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals(), ['id'])\n        url = '/api/v1/filters/{0}'.format(str(id))\n        return self.__api_request('PUT', url, params)", "response": "Update the filter with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_delete(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/filters/{0}'.format(str(id))\n        self.__api_request('DELETE', url)", "response": "Deletes the filter with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef suggestion_delete(self, account_id):\n        account_id = self.__unpack_id(account_id)\n        url = '/api/v1/suggestions/{0}'.format(str(account_id))\n        self.__api_request('DELETE', url)", "response": "Remove the user with the given account_id from the follow suggestions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_create(self, title):\n        params = self.__generate_params(locals())\n        return self.__api_request('POST', '/api/v1/lists', params)", "response": "Create a new list with the given title."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate info about a list.", "response": "def list_update(self, id, title):\n        \"\"\"\n        Update info about a list, where \"info\" is really the lists `title`.\n        \n        Returns the `list dict`_ of the modified list.\n        \"\"\"\n        id = self.__unpack_id(id)\n        params = self.__generate_params(locals(), ['id'])\n        return self.__api_request('PUT', '/api/v1/lists/{0}'.format(id), params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_accounts_add(self, id, account_ids):\n        id = self.__unpack_id(id)\n        \n        if not isinstance(account_ids, list):\n            account_ids = [account_ids]\n        account_ids = list(map(lambda x: self.__unpack_id(x), account_ids))\n        \n        params = self.__generate_params(locals(), ['id'])        \n        self.__api_request('POST', '/api/v1/lists/{0}/accounts'.format(id), params)", "response": "Add the account(s) given in account_ids to the list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreporting statuses to the users administrators.", "response": "def report(self, account_id, status_ids = None, comment = None, forward = False):\n        \"\"\"\n        Report statuses to the instances administrators.\n\n        Accepts a list of toot IDs associated with the report, and a comment.\n        \n        Set forward to True to forward a report of a remote user to that users\n        instance as well as sending it to the instance local administrators.\n\n        Returns a `report dict`_.\n        \"\"\"\n        account_id = self.__unpack_id(account_id)\n        \n        if not status_ids is None:\n            if not isinstance(status_ids, list):\n                status_ids = [status_ids]\n        status_ids = list(map(lambda x: self.__unpack_id(x), status_ids))\n        \n        params_initial = locals()        \n        if forward == False:\n            del params_initial['forward']\n        \n        params = self.__generate_params(params_initial)\n        return self.__api_request('POST', '/api/v1/reports/', params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef follow_request_authorize(self, id):\n        id = self.__unpack_id(id)\n        url = '/api/v1/follow_requests/{0}/authorize'.format(str(id))\n        self.__api_request('POST', url)", "response": "Accept an incoming follow request."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrejects an incoming follow request.", "response": "def follow_request_reject(self, id):\n        \"\"\"\n        Reject an incoming follow request.\n        \"\"\"\n        id = self.__unpack_id(id)\n        url = '/api/v1/follow_requests/{0}/reject'.format(str(id))\n        self.__api_request('POST', url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nposts an image. `media_file` can either be image data or a file name. If image data is passed directly, the mime type has to be specified manually, otherwise, it is determined from the file name. `focus` should be a tuple of floats between -1 and 1, giving the x and y coordinates of the images focus point for cropping (with the origin being the images center). Throws a `MastodonIllegalArgumentError` if the mime type of the passed data or file can not be determined properly. Returns a `media dict`_. This contains the id that can be used in status_post to attach the media file to a toot.", "response": "def media_post(self, media_file, mime_type=None, description=None, focus=None):\n        \"\"\"\n        Post an image. `media_file` can either be image data or\n        a file name. If image data is passed directly, the mime\n        type has to be specified manually, otherwise, it is\n        determined from the file name. `focus` should be a tuple\n        of floats between -1 and 1, giving the x and y coordinates\n        of the images focus point for cropping (with the origin being the images\n        center).\n\n        Throws a `MastodonIllegalArgumentError` if the mime type of the\n        passed data or file can not be determined properly.\n\n        Returns a `media dict`_. This contains the id that can be used in\n        status_post to attach the media file to a toot.\n        \"\"\"\n        if mime_type is None and (isinstance(media_file, str) and os.path.isfile(media_file)):\n            mime_type = guess_type(media_file)\n            media_file = open(media_file, 'rb')\n        elif isinstance(media_file, str) and os.path.isfile(media_file):\n            media_file = open(media_file, 'rb')\n\n        if mime_type is None:\n            raise MastodonIllegalArgumentError('Could not determine mime type'\n                                               ' or data passed directly '\n                                               'without mime type.')\n\n        random_suffix = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10))\n        file_name = \"mastodonpyupload_\" + str(time.time()) + \"_\" + str(random_suffix) + mimetypes.guess_extension(\n            mime_type)\n\n        if focus != None:\n            focus = str(focus[0]) + \",\" + str(focus[1])\n            \n        media_file_description = (file_name, media_file, mime_type)\n        return self.__api_request('POST', '/api/v1/media',\n                                  files={'file': media_file_description},\n                                  params={'description': description, 'focus': focus})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef media_update(self, id, description=None, focus=None):\n        id = self.__unpack_id(id)\n\n        if focus != None:\n            focus = str(focus[0]) + \",\" + str(focus[1])\n            \n        params = self.__generate_params(locals(), ['id'])  \n        return self.__api_request('PUT', '/api/v1/media/{0}'.format(str(id)), params)", "response": "Update the metadata of the media with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a block for all statuses originating from the specified domain.", "response": "def domain_block(self, domain=None):\n        \"\"\"\n        Add a block for all statuses originating from the specified domain for the logged-in user.\n        \"\"\"\n        params = self.__generate_params(locals())\n        self.__api_request('POST', '/api/v1/domain_blocks', params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a domain block for the logged - in user.", "response": "def domain_unblock(self, domain=None):\n        \"\"\"\n        Remove a domain block for the logged-in user.\n        \"\"\"\n        params = self.__generate_params(locals())\n        self.__api_request('DELETE', '/api/v1/domain_blocks', params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef push_subscription_set(self, endpoint, encrypt_params, follow_events=None, \n                              favourite_events=None, reblog_events=None, \n                              mention_events=None):\n        \"\"\"\n        Sets up or modifies the push subscription the logged-in user has for this app.\n        \n        `endpoint` is the endpoint URL mastodon should call for pushes. Note that mastodon\n        requires https for this URL. `encrypt_params` is a dict with key parameters that allow\n        the server to encrypt data for you: A public key `pubkey` and a shared secret `auth`.\n        You can generate this as well as the corresponding private key using the \n        `push_subscription_generate_keys()`_ function.\n        \n        The rest of the parameters controls what kind of events you wish to subscribe to.\n        \n        Returns a `push subscription dict`_.\n        \"\"\"\n        endpoint = Mastodon.__protocolize(endpoint)\n        \n        push_pubkey_b64 = base64.b64encode(encrypt_params['pubkey'])\n        push_auth_b64 = base64.b64encode(encrypt_params['auth'])\n        \n        params = {\n            'subscription[endpoint]': endpoint,\n            'subscription[keys][p256dh]': push_pubkey_b64,\n            'subscription[keys][auth]': push_auth_b64\n        }\n        \n        if follow_events != None:\n            params['data[alerts][follow]'] = follow_events\n        \n        if favourite_events != None:\n            params['data[alerts][favourite]'] = favourite_events\n            \n        if reblog_events != None:\n            params['data[alerts][reblog]'] = reblog_events\n            \n        if mention_events != None:\n            params['data[alerts][mention]'] = mention_events\n            \n        return self.__api_request('POST', '/api/v1/push/subscription', params)", "response": "This method creates or updates the push subscription set for the logged - in user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push_subscription_update(self, follow_events=None, \n                              favourite_events=None, reblog_events=None, \n                              mention_events=None):\n        \"\"\"\n        Modifies what kind of events the app wishes to subscribe to.\n        \n        Returns the updated `push subscription dict`_.\n        \"\"\"\n        params = {}\n        \n        if follow_events != None:\n            params['data[alerts][follow]'] = follow_events\n        \n        if favourite_events != None:\n            params['data[alerts][favourite]'] = favourite_events\n            \n        if reblog_events != None:\n            params['data[alerts][reblog]'] = reblog_events\n            \n        if mention_events != None:\n            params['data[alerts][mention]'] = mention_events\n            \n        return self.__api_request('PUT', '/api/v1/push/subscription', params)", "response": "Update the push subscription dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a private key public key and shared secret for use in webpush subscriptions.", "response": "def push_subscription_generate_keys(self):\n        \"\"\"\n        Generates a private key, public key and shared secret for use in webpush subscriptions.\n        \n        Returns two dicts: One with the private key and shared secret and another with the \n        public key and shared secret.\n        \"\"\"\n        push_key_pair = ec.generate_private_key(ec.SECP256R1(), default_backend())\n        push_key_priv = push_key_pair.private_numbers().private_value\n        push_key_pub = push_key_pair.public_key().public_numbers().encode_point() \n        push_shared_secret = os.urandom(16)\n        \n        priv_dict = {\n            'privkey': push_key_priv,\n            'auth': push_shared_secret\n        }\n        \n        pub_dict = {\n            'pubkey': push_key_pub,\n            'auth': push_shared_secret\n        }\n        \n        return priv_dict, pub_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef push_subscription_decrypt_push(self, data, decrypt_params, encryption_header, crypto_key_header):\n        salt = self.__decode_webpush_b64(encryption_header.split(\"salt=\")[1].strip())\n        dhparams = self.__decode_webpush_b64(crypto_key_header.split(\"dh=\")[1].split(\";\")[0].strip())\n        p256ecdsa = self.__decode_webpush_b64(crypto_key_header.split(\"p256ecdsa=\")[1].strip())\n        dec_key = ec.derive_private_key(decrypt_params['privkey'], ec.SECP256R1(), default_backend())\n        decrypted = http_ece.decrypt(\n            data,\n            salt = salt,\n            key = p256ecdsa,\n            private_key = dec_key, \n            dh = dhparams, \n            auth_secret=decrypt_params['auth'],\n            keylabel = \"P-256\",\n            version = \"aesgcm\"\n        )\n        \n        return json.loads(decrypted.decode('utf-8'), object_hook = Mastodon.__json_hooks)", "response": "Decrypts data received in a webpush request. Requires the private key dict \n        from push_subscription_generate_keys_ ( decrypt_params ) and the server Crypto - Key headers from the received webpush reply Returns the decoded webpush as a push notification dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_next(self, previous_page):\n        if isinstance(previous_page, list) and len(previous_page) != 0:\n            if hasattr(previous_page[-1], '_pagination_next'):\n                params = copy.deepcopy(previous_page[-1]._pagination_next)\n            else:\n                return None\n        else:\n            params = copy.deepcopy(previous_page)\n\n        method = params['_pagination_method']\n        del params['_pagination_method']\n\n        endpoint = params['_pagination_endpoint']\n        del params['_pagination_endpoint']\n\n        return self.__api_request(method, endpoint, params)", "response": "Fetch the next page of results of a paginated request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_previous(self, next_page):\n        if isinstance(next_page, list) and len(next_page) != 0:\n            if hasattr(next_page[0], '_pagination_prev'):\n                params = copy.deepcopy(next_page[0]._pagination_prev)\n            else:\n                return None\n        else:\n            params = copy.deepcopy(next_page)\n\n        method = params['_pagination_method']\n        del params['_pagination_method']\n\n        endpoint = params['_pagination_endpoint']\n        del params['_pagination_endpoint']\n\n        return self.__api_request(method, endpoint, params)", "response": "Fetch the previous page of results of a paginated request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_remaining(self, first_page):\n        first_page = copy.deepcopy(first_page)\n\n        all_pages = []\n        current_page = first_page\n        while current_page is not None and len(current_page) > 0:\n            all_pages.extend(current_page)\n            current_page = self.fetch_next(current_page)\n\n        return all_pages", "response": "Fetches all the remaining pages of a paginated request starting from first_page and returns the entire set of results including the first page\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstreaming events that are relevant to the authorized user.", "response": "def stream_user(self, listener, run_async=False, timeout=__DEFAULT_STREAM_TIMEOUT, reconnect_async=False, reconnect_async_wait_sec=__DEFAULT_STREAM_RECONNECT_WAIT_SEC):\n        \"\"\"\n        Streams events that are relevant to the authorized user, i.e. home\n        timeline and notifications.\n        \"\"\"\n        return self.__stream('/api/v1/streaming/user', listener, run_async=run_async, timeout=timeout, reconnect_async=reconnect_async, reconnect_async_wait_sec=reconnect_async_wait_sec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stream_hashtag(self, tag, listener, run_async=False, timeout=__DEFAULT_STREAM_TIMEOUT, reconnect_async=False, reconnect_async_wait_sec=__DEFAULT_STREAM_RECONNECT_WAIT_SEC):\n        if tag.startswith(\"#\"):\n            raise MastodonIllegalArgumentError(\"Tag parameter should omit leading #\")\n        return self.__stream(\"/api/v1/streaming/hashtag?tag={}\".format(tag), listener, run_async=run_async, timeout=timeout, reconnect_async=reconnect_async, reconnect_async_wait_sec=reconnect_async_wait_sec)", "response": "Stream all public statuses for a hashtag."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stream_list(self, id, listener, run_async=False, timeout=__DEFAULT_STREAM_TIMEOUT, reconnect_async=False, reconnect_async_wait_sec=__DEFAULT_STREAM_RECONNECT_WAIT_SEC):\n        id =  self.__unpack_id(id)\n        return self.__stream(\"/api/v1/streaming/list?list={}\".format(id), listener, run_async=run_async, timeout=timeout, reconnect_async=reconnect_async, reconnect_async_wait_sec=reconnect_async_wait_sec)", "response": "Stream events for the given list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __datetime_to_epoch(self, date_time):\n        date_time_utc = None\n        if date_time.tzinfo is None:\n            date_time_utc = date_time.replace(tzinfo=pytz.utc)\n        else:\n            date_time_utc = date_time.astimezone(pytz.utc)\n\n        epoch_utc = datetime.datetime.utcfromtimestamp(0).replace(tzinfo=pytz.utc)\n\n        return (date_time_utc - epoch_utc).total_seconds()", "response": "Converts a python datetime to unix epoch"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __get_logged_in_id(self):\n        if self.__logged_in_id == None:\n            self.__logged_in_id = self.account_verify_credentials().id\n        return self.__logged_in_id", "response": "Fetch the logged in users ID with caching."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __json_date_parse(json_object):\n        known_date_fields = [\"created_at\", \"week\", \"day\", \"expires_at\", \"scheduled_at\"]\n        for k, v in json_object.items():\n            if k in known_date_fields:\n                if v != None:\n                    try:\n                        if isinstance(v, int):\n                            json_object[k] = datetime.datetime.fromtimestamp(v, pytz.utc)\n                        else:\n                            json_object[k] = dateutil.parser.parse(v)\n                    except:\n                        raise MastodonAPIError('Encountered invalid date.')\n        return json_object", "response": "Parse dates in certain known json fields if possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing True / False strings in certain known fields.", "response": "def __json_truefalse_parse(json_object):\n        \"\"\"\n        Parse 'True' / 'False' strings in certain known fields\n        \"\"\"\n        for key in ('follow', 'favourite', 'reblog', 'mention'):\n            if (key in json_object and isinstance(json_object[key], six.text_type)):\n                if json_object[key] == 'True':\n                    json_object[key] = True\n                if json_object[key] == 'False':\n                    json_object[key] = False\n        return json_object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __json_strnum_to_bignum(json_object):\n        for key in ('id', 'week', 'in_reply_to_id', 'in_reply_to_account_id', 'logins', 'registrations', 'statuses'):\n            if (key in json_object and isinstance(json_object[key], six.text_type)):\n                try:\n                    json_object[key] = int(json_object[key])\n                except ValueError:\n                    pass\n\n        return json_object", "response": "Converts json string numerals to native python bignums."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the json object.", "response": "def __json_hooks(json_object):\n        \"\"\"\n        All the json hooks. Used in request parsing.\n        \"\"\"\n        json_object = Mastodon.__json_strnum_to_bignum(json_object)        \n        json_object = Mastodon.__json_date_parse(json_object)\n        json_object = Mastodon.__json_truefalse_parse(json_object)\n        json_object = Mastodon.__json_allow_dict_attrs(json_object)\n        return json_object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction that does what isoformat does but it actually does the same every time instead of randomly doing different things on some systems.", "response": "def __consistent_isoformat_utc(datetime_val):\n        \"\"\"\n        Function that does what isoformat does but it actually does the same\n        every time instead of randomly doing different things on some systems\n        and also it represents that time as the equivalent UTC time.\n        \"\"\"\n        isotime = datetime_val.astimezone(pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n        if isotime[-2] != \":\":\n            isotime = isotime[:-2] + \":\" + isotime[-2:]\n        return isotime"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __stream(self, endpoint, listener, params={}, run_async=False, timeout=__DEFAULT_STREAM_TIMEOUT, reconnect_async=False, reconnect_async_wait_sec=__DEFAULT_STREAM_RECONNECT_WAIT_SEC):\n\n        # Check if we have to redirect\n        instance = self.instance()\n        if \"streaming_api\" in instance[\"urls\"] and instance[\"urls\"][\"streaming_api\"] != self.api_base_url:\n            # This is probably a websockets URL, which is really for the browser, but requests can't handle it\n            # So we do this below to turn it into an HTTPS or HTTP URL\n            parse = urlparse(instance[\"urls\"][\"streaming_api\"])\n            if parse.scheme == 'wss':\n                url = \"https://\" + parse.netloc\n            elif parse.scheme == 'ws':\n                url = \"http://\" + parse.netloc\n            else:\n                raise MastodonAPIError(\n                        \"Could not parse streaming api location returned from server: {}.\".format(\n                            instance[\"urls\"][\"streaming_api\"]))\n        else:\n            url = self.api_base_url\n\n        # The streaming server can't handle two slashes in a path, so remove trailing slashes\n        if url[-1] == '/':\n            url = url[:-1]\n        \n        # Connect function (called and then potentially passed to async handler)\n        def connect_func():\n            headers = {\"Authorization\": \"Bearer \" + self.access_token}\n            connection = self.session.get(url + endpoint, headers = headers, data = params, stream = True,\n                                  timeout=(self.request_timeout, timeout))\n\n            if connection.status_code != 200:\n                raise MastodonNetworkError(\"Could not connect to streaming server: %s\" % connection.reason)\n            return connection\n        connection = None\n        \n        # Async stream handler\n        class __stream_handle():\n            def __init__(self, connection, connect_func, reconnect_async, reconnect_async_wait_sec):\n                self.closed = False\n                self.running = True\n                self.connection = connection\n                self.connect_func = connect_func\n                self.reconnect_async = reconnect_async\n                self.reconnect_async_wait_sec = reconnect_async_wait_sec\n                self.reconnecting = False\n                \n            def close(self):\n                self.closed = True\n                self.connection.close()\n\n            def is_alive(self):\n                return self._thread.is_alive()\n\n            def is_receiving(self):\n                if self.closed or not self.running or self.reconnecting or not self.is_alive():\n                    return False\n                else:\n                    return True\n\n            def _threadproc(self):\n                self._thread = threading.current_thread()\n                \n                # Run until closed or until error if not autoreconnecting\n                while self.running:\n                    if not self.connection is None:\n                        with closing(self.connection) as r:\n                            try:\n                                listener.handle_stream(r)\n                            except (AttributeError, MastodonMalformedEventError, MastodonNetworkError) as e:\n                                if not (self.closed or self.reconnect_async):\n                                    raise e\n                                else:\n                                    if self.closed:\n                                        self.running = False\n\n                    # Reconnect loop. Try immediately once, then with delays on error.\n                    if (self.reconnect_async and not self.closed) or self.connection is None:\n                        self.reconnecting = True\n                        connect_success = False\n                        while not connect_success:\n                            connect_success = True\n                            try:\n                                self.connection = self.connect_func()\n                                if self.connection.status_code != 200:\n                                    time.sleep(self.reconnect_async_wait_sec)\n                                    connect_success = False\n                                    exception = MastodonNetworkError(\"Could not connect to server.\")\n                                    listener.on_abort(exception)\n                            except:\n                                time.sleep(self.reconnect_async_wait_sec)\n                                connect_success = False\n                        self.reconnecting = False\n                    else:\n                        self.running = False\n                return 0\n\n        if run_async:\n            handle = __stream_handle(connection, connect_func, reconnect_async, reconnect_async_wait_sec)\n            t = threading.Thread(args=(), target=handle._threadproc)\n            t.daemon = True\n            t.start()\n            return handle\n        else:\n            # Blocking, never returns (can only leave via exception)\n            connection = connect_func()            \n            with closing(connection) as r:\n                listener.handle_stream(r)", "response": "Stream the current state of the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __generate_params(self, params, exclude=[]):\n        params = collections.OrderedDict(params)\n\n        del params['self']\n        param_keys = list(params.keys())\n        for key in param_keys:\n            if isinstance(params[key], bool) and params[key] == False:\n                params[key] = '0'\n            if isinstance(params[key], bool) and params[key] == True:\n                params[key] = '1'\n                \n        for key in param_keys:\n            if params[key] is None or key in exclude:\n                del params[key]\n\n        param_keys = list(params.keys())\n        for key in param_keys:\n            if isinstance(params[key], list):\n                params[key + \"[]\"] = params[key]\n                del params[key]\n            \n        return params", "response": "Internal function to generate named - parameters - to - dict helper."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __protocolize(base_url):\n        if not base_url.startswith(\"http://\") and not base_url.startswith(\"https://\"):\n            base_url = \"https://\" + base_url\n\n        # Some API endpoints can't handle extra /'s in path requests\n        base_url = base_url.rstrip(\"/\")\n        return base_url", "response": "Internal add - protocol - to - url helper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_stream(self, response):\n        event = {}\n        line_buffer = bytearray()\n        try:\n            for chunk in response.iter_content(chunk_size = 1):\n                if chunk:\n                    for chunk_part in chunk:\n                        chunk_part = bytearray([chunk_part])\n                        if chunk_part == b'\\n':\n                            try:\n                                line = line_buffer.decode('utf-8')\n                            except UnicodeDecodeError as err:\n                                exception = MastodonMalformedEventError(\"Malformed UTF-8\")\n                                self.on_abort(exception)\n                                six.raise_from(\n                                    exception,\n                                    err\n                                )\n                            if line == '':\n                                self._dispatch(event)\n                                event = {}\n                            else:\n                                event = self._parse_line(line, event)\n                            line_buffer = bytearray()\n                        else:\n                            line_buffer.extend(chunk_part)\n        except ChunkedEncodingError as err:\n            exception = MastodonNetworkError(\"Server ceased communication.\")\n            self.on_abort(exception)\n            six.raise_from(\n                exception,\n                err\n            )\n        except MastodonReadTimeout as err:\n            exception = MastodonReadTimeout(\"Timed out while reading from server.\"),\n            self.on_abort(exception)\n            six.raise_from(\n                exception,\n                err\n            )", "response": "Handles a stream of events from the Mastodon server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the status of this SMS.", "response": "def status(self):\n        \"\"\" Status of this SMS. Can be ENROUTE, DELIVERED or FAILED\n        \n        The actual status report object may be accessed via the 'report' attribute\n        if status is 'DELIVERED' or 'FAILED'\n        \"\"\"\n        if self.report == None:\n            return SentSms.ENROUTE\n        else:\n            return SentSms.DELIVERED if self.report.deliveryStatus == StatusReport.DELIVERED else SentSms.FAILED"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to the modem and initializes the SIM card with the specified PIN code.", "response": "def connect(self, pin=None):\n        \"\"\" Opens the port and initializes the modem and SIM card\n         \n        :param pin: The SIM card PIN code, if any\n        :type pin: str\n        \n        :raise PinRequiredError: if the SIM card requires a PIN but none was provided\n        :raise IncorrectPinError: if the specified PIN is incorrect\n        \"\"\"\n        self.log.info('Connecting to modem on port %s at %dbps', self.port, self.baudrate)        \n        super(GsmModem, self).connect()\n        # Send some initialization commands to the modem\n        try:        \n            self.write('ATZ') # reset configuration\n        except CommandError:\n            # Some modems require a SIM PIN at this stage already; unlock it now\n            # Attempt to enable detailed error messages (to catch incorrect PIN error)\n            # but ignore if it fails\n            self.write('AT+CMEE=1', parseError=False)            \n            self._unlockSim(pin)\n            pinCheckComplete = True\n            self.write('ATZ') # reset configuration        \n        else:\n            pinCheckComplete = False\n        self.write('ATE0') # echo off\n        try:\n            cfun = int(lineStartingWith('+CFUN:', self.write('AT+CFUN?'))[7:]) # example response: +CFUN: 1\n            if cfun != 1:\n                self.write('AT+CFUN=1')\n        except CommandError:\n            pass # just ignore if the +CFUN command isn't supported\n                \n        self.write('AT+CMEE=1') # enable detailed error messages (even if it has already been set - ATZ may reset this)\n        if not pinCheckComplete:\n            self._unlockSim(pin)\n\n        # Get list of supported commands from modem\n        commands = self.supportedCommands\n\n        # Device-specific settings\n        callUpdateTableHint = 0 # unknown modem\n        enableWind = False\n        if commands != None:\n            if '^CVOICE' in commands:\n                self.write('AT^CVOICE=0', parseError=False) # Enable voice calls\n            if '+VTS' in commands: # Check for DTMF sending support\n                Call.dtmfSupport = True\n            elif '^DTMF' in commands:\n                # Huawei modems use ^DTMF to send DTMF tones\n                callUpdateTableHint = 1 # Huawei\n            if '^USSDMODE' in commands:\n                # Enable Huawei text-mode USSD\n                self.write('AT^USSDMODE=0', parseError=False)\n            if '+WIND' in commands:\n                callUpdateTableHint = 2 # Wavecom\n                enableWind = True\n            elif '+ZPAS' in commands:\n                callUpdateTableHint = 3 # ZTE\n        else:\n            # Try to enable general notifications on Wavecom-like device\n            enableWind = True\n\n        if enableWind:\n            try:\n                wind = lineStartingWith('+WIND:', self.write('AT+WIND?')) # Check current WIND value; example response: +WIND: 63\n            except CommandError:\n                # Modem does not support +WIND notifications. See if we can detect other known call update notifications\n                pass\n            else:\n                # Enable notifications for call setup, hangup, etc\n                if int(wind[7:]) != 50:\n                    self.write('AT+WIND=50')\n                callUpdateTableHint = 2 # Wavecom\n\n        # Attempt to identify modem type directly (if not already) - for outgoing call status updates\n        if callUpdateTableHint == 0:\n            if self.manufacturer.lower() == 'huawei':\n                callUpdateTableHint = 1 # huawei\n            else:\n                # See if this is a ZTE modem that has not yet been identified based on supported commands\n                try:\n                    self.write('AT+ZPAS?')\n                except CommandError:\n                    pass # Not a ZTE modem\n                else:\n                    callUpdateTableHint = 3 # ZTE\n        # Load outgoing call status updates based on identified modem features\n        if callUpdateTableHint == 1:\n            # Use Hauwei's ^NOTIFICATIONs\n            self.log.info('Loading Huawei call state update table')\n            self._callStatusUpdates = ((re.compile(r'^\\^ORIG:(\\d),(\\d)$'), self._handleCallInitiated),\n                                       (re.compile(r'^\\^CONN:(\\d),(\\d)$'), self._handleCallAnswered),\n                                       (re.compile(r'^\\^CEND:(\\d),(\\d),(\\d)+,(\\d)+$'), self._handleCallEnded))\n            self._mustPollCallStatus = False\n            # Huawei modems use ^DTMF to send DTMF tones; use that instead\n            Call.DTMF_COMMAND_BASE = '^DTMF={cid},'\n            Call.dtmfSupport = True\n        elif callUpdateTableHint == 2:\n            # Wavecom modem: +WIND notifications supported\n            self.log.info('Loading Wavecom call state update table')\n            self._callStatusUpdates = ((re.compile(r'^\\+WIND: 5,(\\d)$'), self._handleCallInitiated),\n                                      (re.compile(r'^OK$'), self._handleCallAnswered),\n                                      (re.compile(r'^\\+WIND: 6,(\\d)$'), self._handleCallEnded))\n            self._waitForAtdResponse = False # Wavecom modems return OK only when the call is answered\n            self._mustPollCallStatus = False\n            if commands == None: # older modem, assume it has standard DTMF support\n                Call.dtmfSupport = True\n        elif callUpdateTableHint == 3: # ZTE\n            # Use ZTE notifications (\"CONNECT\"/\"HANGUP\", but no \"call initiated\" notification)\n            self.log.info('Loading ZTE call state update table')\n            self._callStatusUpdates = ((re.compile(r'^CONNECT$'), self._handleCallAnswered),\n                                       (re.compile(r'^HANGUP:\\s*(\\d+)$'), self._handleCallEnded),\n                                       (re.compile(r'^OK$'), self._handleCallRejected))\n            self._waitForAtdResponse = False # ZTE modems do not return an immediate  OK only when the call is answered\n            self._mustPollCallStatus = False\n            self._waitForCallInitUpdate = False # ZTE modems do not provide \"call initiated\" updates\n            if commands == None: # ZTE uses standard +VTS for DTMF\n                Call.dtmfSupport = True\n        else:\n            # Unknown modem - we do not know what its call updates look like. Use polling instead\n            self.log.info('Unknown/generic modem type - will use polling for call state updates')\n            self._mustPollCallStatus = True\n            self._pollCallStatusRegex = re.compile('^\\+CLCC:\\s+(\\d+),(\\d),(\\d),(\\d),([^,]),\"([^,]*)\",(\\d+)$')\n            self._waitForAtdResponse = True # Most modems return OK immediately after issuing ATD\n\n        # General meta-information setup\n        self.write('AT+COPS=3,0', parseError=False) # Use long alphanumeric name format\n                \n        # SMS setup\n        self.write('AT+CMGF={0}'.format(1 if self._smsTextMode else 0)) # Switch to text or PDU mode for SMS messages\n        self._compileSmsRegexes()\n        if self._smscNumber != None:\n            self.write('AT+CSCA=\"{0}\"'.format(self._smscNumber)) # Set default SMSC number\n            currentSmscNumber = self._smscNumber\n        else:\n            currentSmscNumber = self.smsc\n        # Some modems delete the SMSC number when setting text-mode SMS parameters; preserve it if needed\n        if currentSmscNumber != None:\n            self._smscNumber = None # clear cache\n        self.write('AT+CSMP=49,167,0,0', parseError=False) # Enable delivery reports\n        # ...check SMSC again to ensure it did not change\n        if currentSmscNumber != None and self.smsc != currentSmscNumber:\n            self.smsc = currentSmscNumber\n\n        # Set message storage, but first check what the modem supports - example response: +CPMS: ((\"SM\",\"BM\",\"SR\"),(\"SM\"))\n        try:\n            cpmsLine = lineStartingWith('+CPMS', self.write('AT+CPMS=?'))\n        except CommandError:\n            # Modem does not support AT+CPMS; SMS reading unavailable\n            self._smsReadSupported = False\n            self.log.warning('SMS preferred message storage query not supported by modem. SMS reading unavailable.')\n        else:\n            cpmsSupport = cpmsLine.split(' ', 1)[1].split('),(')\n            # Do a sanity check on the memory types returned - Nokia S60 devices return empty strings, for example\n            for memItem in cpmsSupport:\n                if len(memItem) == 0:\n                    # No support for reading stored SMS via AT commands - probably a Nokia S60\n                    self._smsReadSupported = False\n                    self.log.warning('Invalid SMS message storage support returned by modem. SMS reading unavailable. Response was: \"%s\"', cpmsLine)\n                    break\n            else:\n                # Suppported memory types look fine, continue\n                preferredMemoryTypes = ('\"ME\"', '\"SM\"', '\"SR\"')\n                cpmsItems = [''] * len(cpmsSupport)\n                for i in xrange(len(cpmsSupport)):\n                    for memType in preferredMemoryTypes:\n                        if memType in cpmsSupport[i]:\n                            if i == 0:\n                                self._smsMemReadDelete = memType\n                            cpmsItems[i] = memType\n                            break\n                self.write('AT+CPMS={0}'.format(','.join(cpmsItems))) # Set message storage\n            del cpmsSupport\n            del cpmsLine\n        \n        if self._smsReadSupported:\n            try:\n                self.write('AT+CNMI=2,1,0,2') # Set message notifications\n            except CommandError:\n                # Message notifications not supported\n                self._smsReadSupported = False\n                self.log.warning('Incoming SMS notifications not supported by modem. SMS receiving unavailable.')\n        \n        # Incoming call notification setup\n        try:\n            self.write('AT+CLIP=1') # Enable calling line identification presentation\n        except CommandError as clipError:\n            self._callingLineIdentification = False\n            self.log.warning('Incoming call calling line identification (caller ID) not supported by modem. Error: {0}'.format(clipError))\n        else:\n            self._callingLineIdentification = True\n            try:\n                self.write('AT+CRC=1') # Enable extended format of incoming indication (optional)\n            except CommandError as crcError:\n                self._extendedIncomingCallIndication = False\n                self.log.warning('Extended format incoming call indication not supported by modem. Error: {0}'.format(crcError))\n            else:\n                self._extendedIncomingCallIndication = True        \n\n        # Call control setup\n        self.write('AT+CVHU=0', parseError=False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _unlockSim(self, pin):\n        # Unlock the SIM card if needed\n        try:\n            cpinResponse = lineStartingWith('+CPIN', self.write('AT+CPIN?', timeout=0.25))\n        except TimeoutException as timeout:\n            # Wavecom modems do not end +CPIN responses with \"OK\" (github issue #19) - see if just the +CPIN response was returned\n            if timeout.data != None:\n                cpinResponse = lineStartingWith('+CPIN', timeout.data)\n                if cpinResponse == None:\n                    # No useful response read\n                    raise timeout\n            else:\n                # Nothing read (real timeout)\n                raise timeout\n        if cpinResponse != '+CPIN: READY':\n            if pin != None:\n                self.write('AT+CPIN=\"{0}\"'.format(pin))\n            else:\n                raise PinRequiredError('AT+CPIN')", "response": "Unlocks the SIM card using the specified PIN."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting data to the modem.", "response": "def write(self, data, waitForResponse=True, timeout=5, parseError=True, writeTerm='\\r', expectedResponseTermSeq=None):\n        \"\"\" Write data to the modem.\n\n        This method adds the ``\\\\r\\\\n`` end-of-line sequence to the data parameter, and\n        writes it to the modem.\n\n        :param data: Command/data to be written to the modem\n        :type data: str\n        :param waitForResponse: Whether this method should block and return the response from the modem or not\n        :type waitForResponse: bool\n        :param timeout: Maximum amount of time in seconds to wait for a response from the modem\n        :type timeout: int\n        :param parseError: If True, a CommandError is raised if the modem responds with an error (otherwise the response is returned as-is)\n        :type parseError: bool\n        :param writeTerm: The terminating sequence to append to the written data\n        :type writeTerm: str\n        :param expectedResponseTermSeq: The expected terminating sequence that marks the end of the modem's response (defaults to ``\\\\r\\\\n``)\n        :type expectedResponseTermSeq: str\n\n        :raise CommandError: if the command returns an error (only if parseError parameter is True)\n        :raise TimeoutException: if no response to the command was received from the modem\n\n        :return: A list containing the response lines from the modem, or None if waitForResponse is False\n        :rtype: list\n        \"\"\"\n        self.log.debug('write: %s', data)\n        responseLines = super(GsmModem, self).write(data + writeTerm, waitForResponse=waitForResponse, timeout=timeout, expectedResponseTermSeq=expectedResponseTermSeq)\n        if self._writeWait > 0: # Sleep a bit if required (some older modems suffer under load)            \n            time.sleep(self._writeWait)\n        if waitForResponse:\n            cmdStatusLine = responseLines[-1]\n            if parseError:\n                if 'ERROR' in cmdStatusLine:\n                    cmErrorMatch = self.CM_ERROR_REGEX.match(cmdStatusLine)\n                    if cmErrorMatch:\n                        errorType = cmErrorMatch.group(1)\n                        errorCode = int(cmErrorMatch.group(2))\n                        if errorCode == 515 or errorCode == 14:\n                            # 515 means: \"Please wait, init or command processing in progress.\"\n                            # 14 means \"SIM busy\"\n                            self._writeWait += 0.2 # Increase waiting period temporarily\n                            # Retry the command after waiting a bit\n                            self.log.debug('Device/SIM busy error detected; self._writeWait adjusted to %fs', self._writeWait)\n                            time.sleep(self._writeWait)\n                            result = self.write(data, waitForResponse, timeout, parseError, writeTerm, expectedResponseTermSeq)\n                            self.log.debug('self_writeWait set to 0.1 because of recovering from device busy (515) error')\n                            if errorCode == 515:\n                                self._writeWait = 0.1 # Set this to something sane for further commands (slow modem)\n                            else:\n                                self._writeWait = 0 # The modem was just waiting for the SIM card\n                            return result\n                        if errorType == 'CME':\n                            raise CmeError(data, int(errorCode))\n                        else: # CMS error\n                            raise CmsError(data, int(errorCode))\n                    else:\n                        raise CommandError(data)\n                elif cmdStatusLine == 'COMMAND NOT SUPPORT': # Some Huawei modems respond with this for unknown commands\n                    raise CommandError(data + '({0})'.format(cmdStatusLine))\n            return responseLines"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef signalStrength(self):\n        csq = self.CSQ_REGEX.match(self.write('AT+CSQ')[0])\n        if csq:\n            ss = int(csq.group(1))\n            return ss if ss != 99 else -1\n        else:\n            raise CommandError()", "response": "Checks the modem s cellular network signal strength"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the name of the GSM Network Operator to which the modem is connected", "response": "def networkName(self):\n        \"\"\" :return: the name of the GSM Network Operator to which the modem is connected \"\"\"\n        copsMatch = lineMatching(r'^\\+COPS: (\\d),(\\d),\"(.+)\",{0,1}\\d*$', self.write('AT+COPS?')) # response format: +COPS: mode,format,\"operator_name\",x\n        if copsMatch:\n            return copsMatch.group(3)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef supportedCommands(self):\n        try:\n            # AT+CLAC responses differ between modems. Most respond with +CLAC: and then a comma-separated list of commands\n            # while others simply return each command on a new line, with no +CLAC: prefix\n            response = self.write('AT+CLAC')\n            if len(response) == 2: # Single-line response, comma separated\n                commands = response[0]\n                if commands.startswith('+CLAC'):\n                    commands = commands[6:] # remove the +CLAC: prefix before splitting\n                return commands.split(',')\n            elif len(response) > 2: # Multi-line response\n                return [cmd.strip() for cmd in response[:-1]]\n            else:\n                self.log.debug('Unhandled +CLAC response: {0}'.format(response))\n                return None\n        except CommandError:\n            return None", "response": "Returns a list of AT commands supported by this modem."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef smsTextMode(self, textMode):\n        if textMode != self._smsTextMode:\n            if self.alive:\n                self.write('AT+CMGF={0}'.format(1 if textMode else 0))\n            self._smsTextMode = textMode\n            self._compileSmsRegexes()", "response": "Set to True for the modem to use text mode for SMS."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the current SMS memory to use for read delete and write operations.", "response": "def _setSmsMemory(self, readDelete=None, write=None):\n        \"\"\" Set the current SMS memory to use for read/delete/write operations \"\"\"\n        # Switch to the correct memory type if required\n        if write != None and write != self._smsMemWrite:\n            self.write()\n            readDel = readDelete or self._smsMemReadDelete\n            self.write('AT+CPMS=\"{0}\",\"{1}\"'.format(readDel, write))\n            self._smsMemReadDelete = readDel\n            self._smsMemWrite = write\n        elif readDelete != None and readDelete != self._smsMemReadDelete:\n            self.write('AT+CPMS=\"{0}\"'.format(readDelete))\n            self._smsMemReadDelete = readDelete"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compileSmsRegexes(self):\n        if self._smsTextMode:\n            if self.CMGR_SM_DELIVER_REGEX_TEXT == None:\n                self.CMGR_SM_DELIVER_REGEX_TEXT = re.compile(r'^\\+CMGR: \"([^\"]+)\",\"([^\"]+)\",[^,]*,\"([^\"]+)\"$')\n                self.CMGR_SM_REPORT_REGEXT_TEXT = re.compile(r'^\\+CMGR: ([^,]*),\\d+,(\\d+),\"{0,1}([^\"]*)\"{0,1},\\d*,\"([^\"]+)\",\"([^\"]+)\",(\\d+)$')\n        elif self.CMGR_REGEX_PDU == None:\n            self.CMGR_REGEX_PDU = re.compile(r'^\\+CMGR: (\\d*),\"{0,1}([^\"]*)\"{0,1},(\\d+)$')", "response": "Compiles regular expression used for parsing SMS messages based on current mode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the default SMSC number stored on the SIM card", "response": "def smsc(self):\n        \"\"\" :return: The default SMSC number stored on the SIM card \"\"\"\n        if self._smscNumber == None:\n            try:\n                readSmsc = self.write('AT+CSCA?')\n            except SmscNumberUnknownError:\n                pass # Some modems return a CMS 330 error if the value isn't set\n            else:\n                cscaMatch = lineMatching(r'\\+CSCA:\\s*\"([^,]+)\",(\\d+)$', readSmsc)\n                if cscaMatch:\n                    self._smscNumber = cscaMatch.group(1)\n        return self._smscNumber"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef smsc(self, smscNumber):\n        if smscNumber != self._smscNumber:\n            if self.alive:\n                self.write('AT+CSCA=\"{0}\"'.format(smscNumber))\n            self._smscNumber = smscNumber", "response": "Set the default SMSC number to use when sending SMS messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef waitForNetworkCoverage(self, timeout=None):\n        block = [True]\n        if timeout != None:\n            # Set up a timeout mechanism\n            def _cancelBlock():                \n                block[0] = False                \n            t = threading.Timer(timeout, _cancelBlock)\n            t.start()\n        ss = -1\n        checkCreg = True\n        while block[0]:\n            if checkCreg:\n                cregResult = lineMatching(r'^\\+CREG:\\s*(\\d),(\\d)$', self.write('AT+CREG?', parseError=False)) # example result: +CREG: 0,1\n                if cregResult:\n                    status = int(cregResult.group(2))\n                    if status in (1, 5):\n                        # 1: registered, home network, 5: registered, roaming\n                        # Now simply check and return network signal strength\n                        checkCreg = False\n                    elif status == 3:\n                        raise InvalidStateException('Network registration denied')\n                    elif status == 0:\n                        raise InvalidStateException('Device not searching for network operator')\n                else:\n                    # Disable network registration check; only use signal strength\n                    self.log.info('+CREG check disabled due to invalid response or unsupported command')\n                    checkCreg = False\n            else:\n                # Check signal strength\n                ss = self.signalStrength\n                if ss > 0:\n                    return ss\n            time.sleep(1)\n        else:\n            # If this is reached, the timer task has triggered\n            raise TimeoutException()", "response": "Block until the modem has GSM network coverage."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sendSms(self, destination, text, waitForDeliveryReport=False, deliveryTimeout=15, sendFlash=False):\n        if self._smsTextMode:\n            self.write('AT+CMGS=\"{0}\"'.format(destination), timeout=3, expectedResponseTermSeq='> ')\n            result = lineStartingWith('+CMGS:', self.write(text, timeout=15, writeTerm=chr(26)))\n        else:\n            pdus = encodeSmsSubmitPdu(destination, text, reference=self._smsRef, sendFlash=sendFlash)\n            for pdu in pdus:\n                self.write('AT+CMGS={0}'.format(pdu.tpduLength), timeout=3, expectedResponseTermSeq='> ')\n                result = lineStartingWith('+CMGS:', self.write(str(pdu), timeout=15, writeTerm=chr(26))) # example: +CMGS: xx\n        if result == None:\n            raise CommandError('Modem did not respond with +CMGS response')\n        reference = int(result[7:])\n        self._smsRef = reference + 1\n        if self._smsRef > 255:\n            self._smsRef = 0\n        sms = SentSms(destination, text, reference)\n        # Add a weak-referenced entry for this SMS (allows us to update the SMS state if a status report is received)\n        self.sentSms[reference] = sms\n        if waitForDeliveryReport:\n            self._smsStatusReportEvent = threading.Event()\n            if self._smsStatusReportEvent.wait(deliveryTimeout):\n                self._smsStatusReportEvent = None\n            else: # Response timed out\n                self._smsStatusReportEvent = None\n                raise TimeoutException()\n        return sms", "response": "Send an SMS text message to the specified destination."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a USSD string to the specified USSD access number.", "response": "def sendUssd(self, ussdString, responseTimeout=15):\n        \"\"\" Starts a USSD session by dialing the the specified USSD string, or \\\n        sends the specified string in the existing USSD session (if any)\n                \n        :param ussdString: The USSD access number to dial\n        :param responseTimeout: Maximum time to wait a response, in seconds\n        \n        :raise TimeoutException: if no response is received in time\n        \n        :return: The USSD response message/session (as a Ussd object)\n        :rtype: gsmmodem.modem.Ussd\n        \"\"\"\n        self._ussdSessionEvent = threading.Event()\n        try:\n            cusdResponse = self.write('AT+CUSD=1,\"{0}\",15'.format(ussdString), timeout=responseTimeout) # Should respond with \"OK\"\n        except Exception:\n            self._ussdSessionEvent = None # Cancel the thread sync lock\n            raise\n\n        # Some modems issue the +CUSD response before the acknowledgment \"OK\" - check for that\n        if len(cusdResponse) > 1:\n            cusdResponseFound = lineStartingWith('+CUSD', cusdResponse) != None\n            if cusdResponseFound:\n                self._ussdSessionEvent = None # Cancel thread sync lock\n                return self._parseCusdResponse(cusdResponse)\n        # Wait for the +CUSD notification message\n        if self._ussdSessionEvent.wait(responseTimeout):\n            self._ussdSessionEvent = None\n            return self._ussdResponse\n        else: # Response timed out\n            self._ussdSessionEvent = None            \n            raise TimeoutException()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a call to the specified phone number using a voice phone number.", "response": "def dial(self, number, timeout=5, callStatusUpdateCallbackFunc=None):\n        \"\"\" Calls the specified phone number using a voice phone call\n\n        :param number: The phone number to dial\n        :param timeout: Maximum time to wait for the call to be established\n        :param callStatusUpdateCallbackFunc: Callback function that is executed if the call's status changes due to\n               remote events (i.e. when it is answered, the call is ended by the remote party)\n\n        :return: The outgoing call\n        :rtype: gsmmodem.modem.Call\n        \"\"\"\n        if self._waitForCallInitUpdate:\n            # Wait for the \"call originated\" notification message\n            self._dialEvent = threading.Event()\n            try:\n                self.write('ATD{0};'.format(number), timeout=timeout, waitForResponse=self._waitForAtdResponse)\n            except Exception:\n                self._dialEvent = None # Cancel the thread sync lock\n                raise\n        else:\n            # Don't wait for a call init update - base the call ID on the number of active calls\n            self.write('ATD{0};'.format(number), timeout=timeout, waitForResponse=self._waitForAtdResponse)\n            self.log.debug(\"Not waiting for outgoing call init update message\")\n            callId = len(self.activeCalls) + 1\n            callType = 0 # Assume voice\n            call = Call(self, callId, callType, number, callStatusUpdateCallbackFunc)\n            self.activeCalls[callId] = call\n            return call\n\n        if self._mustPollCallStatus:\n            # Fake a call notification by polling call status until the status indicates that the call is being dialed\n            threading.Thread(target=self._pollCallStatus, kwargs={'expectedState': 0, 'timeout': timeout}).start()\n\n        if self._dialEvent.wait(timeout):\n            self._dialEvent = None\n            callId, callType = self._dialResponse\n            call = Call(self, callId, callType, number, callStatusUpdateCallbackFunc)\n            self.activeCalls[callId] = call\n            return call\n        else: # Call establishing timed out\n            self._dialEvent = None\n            raise TimeoutException()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses all stored SMS messages currently stored on the device and SIM card.", "response": "def processStoredSms(self, unreadOnly=False):\n        \"\"\" Process all SMS messages currently stored on the device/SIM card.\n        \n        Reads all (or just unread) received SMS messages currently stored on the \n        device/SIM card, initiates \"SMS received\" events for them, and removes \n        them from the SIM card.\n        This is useful if SMS messages were received during a period that\n        python-gsmmodem was not running but the modem was powered on.\n        \n        :param unreadOnly: If True, only process unread SMS messages\n        :type unreadOnly: boolean\n        \"\"\"\n        states = [Sms.STATUS_RECEIVED_UNREAD]\n        if not unreadOnly:\n            states.insert(0, Sms.STATUS_RECEIVED_READ)\n        for msgStatus in states:\n            messages = self.listStoredSms(status=msgStatus, delete=True)\n            for sms in messages:\n                self.smsReceivedCallback(sms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listStoredSms(self, status=Sms.STATUS_ALL, memory=None, delete=False):\n        self._setSmsMemory(readDelete=memory)\n        messages = []\n        delMessages = set()\n        if self._smsTextMode:\n            cmglRegex= re.compile(r'^\\+CMGL: (\\d+),\"([^\"]+)\",\"([^\"]+)\",[^,]*,\"([^\"]+)\"$')\n            for key, val in dictItemsIter(Sms.TEXT_MODE_STATUS_MAP):\n                if status == val:\n                    statusStr = key\n                    break\n            else:\n                raise ValueError('Invalid status value: {0}'.format(status))\n            result = self.write('AT+CMGL=\"{0}\"'.format(statusStr))\n            msgLines = []\n            msgIndex = msgStatus = number = msgTime = None\n            for line in result:\n                cmglMatch = cmglRegex.match(line)                \n                if cmglMatch:\n                    # New message; save old one if applicable\n                    if msgIndex != None and len(msgLines) > 0:\n                        msgText = '\\n'.join(msgLines)\n                        msgLines = []\n                        messages.append(ReceivedSms(self, Sms.TEXT_MODE_STATUS_MAP[msgStatus], number, parseTextModeTimeStr(msgTime), msgText))\n                        delMessages.add(int(msgIndex))\n                    msgIndex, msgStatus, number, msgTime = cmglMatch.groups()\n                    msgLines = []\n                else:\n                    if line != 'OK':\n                        msgLines.append(line)\n            if msgIndex != None and len(msgLines) > 0:\n                msgText = '\\n'.join(msgLines)\n                msgLines = []\n                messages.append(ReceivedSms(self, Sms.TEXT_MODE_STATUS_MAP[msgStatus], number, parseTextModeTimeStr(msgTime), msgText))\n                delMessages.add(int(msgIndex))\n        else:\n            cmglRegex = re.compile(r'^\\+CMGL:\\s*(\\d+),\\s*(\\d+),.*$')\n            readPdu = False\n            result = self.write('AT+CMGL={0}'.format(status))\n            for line in result:\n                if not readPdu:\n                    cmglMatch = cmglRegex.match(line)\n                    if cmglMatch:\n                        msgIndex = int(cmglMatch.group(1))\n                        msgStat = int(cmglMatch.group(2))\n                        readPdu = True\n                else:\n                    try:\n                        smsDict = decodeSmsPdu(line)\n                    except EncodingError:\n                        self.log.debug('Discarding line from +CMGL response: %s', line)\n                    else:\n                        if smsDict['type'] == 'SMS-DELIVER':\n                            sms = ReceivedSms(self, int(msgStat), smsDict['number'], smsDict['time'], smsDict['text'], smsDict['smsc'])\n                        elif smsDict['type'] == 'SMS-STATUS-REPORT':\n                            sms = StatusReport(self, int(msgStat), smsDict['reference'], smsDict['number'], smsDict['time'], smsDict['discharge'], smsDict['status'])\n                        else:\n                            raise CommandError('Invalid PDU type for readStoredSms(): {0}'.format(smsDict['type']))\n                        messages.append(sms)\n                        delMessages.add(msgIndex)\n                        readPdu = False\n        if delete:\n            if status == Sms.STATUS_ALL:\n                # Delete all messages\n                self.deleteMultipleStoredSms()\n            else:\n                for msgIndex in delMessages:\n                    self.deleteStoredSms(msgIndex)\n        return messages", "response": "Returns a list of all stored SMS messages currently stored on the device or SIM card."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handleCallAnswered(self, regexMatch, callId=None):\n        if regexMatch:\n            groups = regexMatch.groups()\n            if len(groups) > 1:\n                callId = int(groups[0])\n                self.activeCalls[callId].answered = True\n            else:\n                # Call ID not available for this notificition - check for the first outgoing call that has not been answered\n                for call in dictValuesIter(self.activeCalls):\n                    if call.answered == False and type(call) == Call:\n                        call.answered = True\n                        return\n        else:\n            # Use supplied values\n            self.activeCalls[callId].answered = True", "response": "Handler for outgoing call answered event notification line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handleSmsStatusReport(self, notificationLine):\n        self.log.debug('SMS status report received')\n        cdsiMatch = self.CDSI_REGEX.match(notificationLine)\n        if cdsiMatch:\n            msgMemory = cdsiMatch.group(1)\n            msgIndex = cdsiMatch.group(2)\n            report = self.readStoredSms(msgIndex, msgMemory)\n            self.deleteStoredSms(msgIndex)\n            # Update sent SMS status if possible            \n            if report.reference in self.sentSms:                \n                self.sentSms[report.reference].report = report\n            if self._smsStatusReportEvent:                \n                # A sendSms() call is waiting for this response - notify waiting thread\n                self._smsStatusReportEvent.set()\n            else:\n                # Nothing is waiting for this report directly - use callback\n                self.smsStatusReportCallback(report)", "response": "Handle SMS status reports"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef readStoredSms(self, index, memory=None):\n        # Switch to the correct memory type if required\n        self._setSmsMemory(readDelete=memory)\n        msgData = self.write('AT+CMGR={0}'.format(index))\n        # Parse meta information\n        if self._smsTextMode:\n            cmgrMatch = self.CMGR_SM_DELIVER_REGEX_TEXT.match(msgData[0])\n            if cmgrMatch:\n                msgStatus, number, msgTime = cmgrMatch.groups()\n                msgText = '\\n'.join(msgData[1:-1])\n                return ReceivedSms(self, Sms.TEXT_MODE_STATUS_MAP[msgStatus], number, parseTextModeTimeStr(msgTime), msgText)\n            else:\n                # Try parsing status report\n                cmgrMatch = self.CMGR_SM_REPORT_REGEXT_TEXT.match(msgData[0])\n                if cmgrMatch:\n                    msgStatus, reference, number, sentTime, deliverTime, deliverStatus = cmgrMatch.groups()\n                    if msgStatus.startswith('\"'):\n                        msgStatus = msgStatus[1:-1]                    \n                    if len(msgStatus) == 0:\n                        msgStatus = \"REC UNREAD\"\n                    return StatusReport(self, Sms.TEXT_MODE_STATUS_MAP[msgStatus], int(reference), number, parseTextModeTimeStr(sentTime), parseTextModeTimeStr(deliverTime), int(deliverStatus))\n                else:\n                    raise CommandError('Failed to parse text-mode SMS message +CMGR response: {0}'.format(msgData))\n        else:\n            cmgrMatch = self.CMGR_REGEX_PDU.match(msgData[0])\n            if not cmgrMatch:\n                raise CommandError('Failed to parse PDU-mode SMS message +CMGR response: {0}'.format(msgData))\n            stat, alpha, length = cmgrMatch.groups()\n            try:\n                stat = int(stat)\n            except Exception:\n                # Some modems (ZTE) do not always read return status - default to RECEIVED UNREAD\n                stat = Sms.STATUS_RECEIVED_UNREAD\n            pdu = msgData[1]\n            smsDict = decodeSmsPdu(pdu)\n            if smsDict['type'] == 'SMS-DELIVER':\n                return ReceivedSms(self, int(stat), smsDict['number'], smsDict['time'], smsDict['text'], smsDict['smsc'])\n            elif smsDict['type'] == 'SMS-STATUS-REPORT':\n                return StatusReport(self, int(stat), smsDict['reference'], smsDict['number'], smsDict['time'], smsDict['discharge'], smsDict['status'])\n            else:\n                raise CommandError('Invalid PDU type for readStoredSms(): {0}'.format(smsDict['type']))", "response": "Reads and returns the stored SMS message at the specified index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deleteStoredSms(self, index, memory=None):\n        self._setSmsMemory(readDelete=memory)\n        self.write('AT+CMGD={0},0'.format(index))", "response": "Delete the stored SMS message at the specified index in modem / SIM card memory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deleteMultipleStoredSms(self, delFlag=4, memory=None):\n        if 0 < delFlag <= 4:\n            self._setSmsMemory(readDelete=memory)\n            self.write('AT+CMGD=1,{0}'.format(delFlag))\n        else:\n            raise ValueError('\"delFlag\" must be in range [1,4]')", "response": "Delete multiple stored SMS messages from the device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling USSD event notification line", "response": "def _handleUssd(self, lines):\n        \"\"\" Handler for USSD event notification line(s) \"\"\"\n        if self._ussdSessionEvent:\n            # A sendUssd() call is waiting for this response - parse it\n            self._ussdResponse = self._parseCusdResponse(lines)\n            # Notify waiting thread\n            self._ussdSessionEvent.set()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse one or more + CUSD response lines and returns a Ussd object.", "response": "def _parseCusdResponse(self, lines):\n        \"\"\" Parses one or more +CUSD notification lines (for USSD)\n        :return: USSD response object\n        :rtype: gsmmodem.modem.Ussd\n        \"\"\"\n        if len(lines) > 1:\n            # Issue #20: Some modem/network combinations use \\r\\n as in-message EOL indicators;\n            # - join lines to compensate for that (thanks to davidjb for the fix)\n            # Also, look for more than one +CUSD response because of certain modems' strange behaviour\n            cusdMatches = list(self.CUSD_REGEX.finditer('\\r\\n'.join(lines)))\n        else:\n            # Single standard +CUSD response\n            cusdMatches = [self.CUSD_REGEX.match(lines[0])]\n        message = None\n        sessionActive = True\n        if len(cusdMatches) > 1:\n            self.log.debug('Multiple +CUSD responses received; filtering...')\n            # Some modems issue a non-standard \"extra\" +CUSD notification for releasing the session\n            for cusdMatch in cusdMatches:\n                if cusdMatch.group(1) == '2':\n                    # Set the session to inactive, but ignore the message\n                    self.log.debug('Ignoring \"session release\" message: %s', cusdMatch.group(2))\n                    sessionActive = False\n                else:\n                    # Not a \"session release\" message\n                    message = cusdMatch.group(2)\n                    if sessionActive and cusdMatch.group(1) != '1':\n                        sessionActive = False\n        else:\n            sessionActive = cusdMatches[0].group(1) == '1'\n            message = cusdMatches[0].group(2)\n        return Ussd(self, sessionActive, message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npoll the status of outgoing calls.", "response": "def _pollCallStatus(self, expectedState, callId=None, timeout=None):\n        \"\"\" Poll the status of outgoing calls.    \n        This is used for modems that do not have a known set of call status update notifications.\n        \n        :param expectedState: The internal state we are waiting for. 0 == initiated, 1 == answered, 2 = hangup\n        :type expectedState: int\n        \n        :raise TimeoutException: If a timeout was specified, and has occurred\n        \"\"\"\n        callDone = False\n        timeLeft = timeout or 999999\n        while self.alive and not callDone and timeLeft > 0:\n            time.sleep(0.5)\n            if expectedState == 0: # Only call initializing can timeout\n                timeLeft -= 0.5\n            try:\n                clcc = self._pollCallStatusRegex.match(self.write('AT+CLCC')[0])\n            except TimeoutException as timeout:\n                # Can happend if the call was ended during our time.sleep() call\n                clcc = None\n            if clcc:\n                direction = int(clcc.group(2))\n                if direction == 0: # Outgoing call\n                    # Determine call state\n                    stat = int(clcc.group(3))\n                    if expectedState == 0: # waiting for call initiated\n                        if stat == 2 or stat == 3: # Dialing or ringing (\"alerting\")                            \n                            callId = int(clcc.group(1))\n                            callType = int(clcc.group(4))\n                            self._handleCallInitiated(None, callId, callType) # if self_dialEvent is None, this does nothing\n                            expectedState = 1 # Now wait for call answer\n                    elif expectedState == 1: # waiting for call to be answered\n                        if stat == 0: # Call active\n                            callId = int(clcc.group(1))\n                            self._handleCallAnswered(None, callId)\n                            expectedState = 2 # Now wait for call hangup                            \n            elif expectedState == 2 : # waiting for remote hangup\n                # Since there was no +CLCC response, the call is no longer active\n                callDone = True\n                self._handleCallEnded(None, callId=callId)\n            elif expectedState == 1: # waiting for call to be answered\n                # Call was rejected\n                callDone = True\n                self._handleCallRejected(None, callId=callId)\n        if timeLeft <= 0:\n            raise TimeoutException()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sendDtmfTone(self, tones):\n        if self.answered:\n            dtmfCommandBase = self.DTMF_COMMAND_BASE.format(cid=self.id)\n            toneLen = len(tones)\n            if len(tones) > 1:\n                cmd = ('AT{0}{1};{0}' + ';{0}'.join(tones[1:])).format(dtmfCommandBase, tones[0])                \n            else:\n                cmd = 'AT{0}{1}'.format(dtmfCommandBase, tones)\n            try:\n                self._gsmModem.write(cmd, timeout=(5 + toneLen))\n            except CmeError as e:\n                if e.code == 30:\n                    # No network service - can happen if call is ended during DTMF transmission (but also if DTMF is sent immediately after call is answered)\n                    raise InterruptedException('No network service', e)\n                elif e.code == 3:\n                    # Operation not allowed - can happen if call is ended during DTMF transmission\n                    raise InterruptedException('Operation not allowed', e)\n                else:\n                    raise e\n        else:\n            raise InvalidStateException('Call is not active (it has not yet been answered, or it has ended).')", "response": "Send one or more DTMF tones to the remote party."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nends the phone call.", "response": "def hangup(self):\n        \"\"\" End the phone call.\n        \n        Does nothing if the call is already inactive.\n        \"\"\"\n        if self.active:\n            self._gsmModem.write('ATH')\n            self.answered = False\n            self.active = False\n        if self.id in self._gsmModem.activeCalls:\n            del self._gsmModem.activeCalls[self.id]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nanswer the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone call. i. e. the phone number of the phone.", "response": "def answer(self):\n        \"\"\" Answer the phone call.        \n        :return: self (for chaining method calls)\n        \"\"\"\n        if self.ringing:\n            self._gsmModem.write('ATA')\n            self.ringing = False\n            self.answered = True\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reply(self, message):\n        if self.sessionActive:\n            return self._gsmModem.sendUssd(message)\n        else:\n            raise InvalidStateException('USSD session is inactive')", "response": "Sends a reply to this USSD message in the same USSD session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nloop and copy console -> serial until EXIT_CHARCTER character is found.", "response": "def _inputLoop(self):\n        \"\"\" Loop and copy console->serial until EXIT_CHARCTER character is found. \"\"\"\n        try:\n            while self.alive:\n                try:\n                    c = console.getkey()                    \n                except KeyboardInterrupt:\n                    print('kbint')\n                    c = serial.to_bytes([3])\n                if c == self.EXIT_CHARACTER: \n                    self.stop()\n                elif c == '\\n':\n                    # Convert newline input into \\r\n                    self.serial.write(self.WRITE_TERM)\n                    if self.echo:\n                        # Locally just echo the real newline\n                        sys.stdout.write(c)\n                        sys.stdout.flush()\n                else:\n                    #print('writing: ', c)\n                    self.serial.write(c)\n                    if self.echo:\n                        sys.stdout.write(c)\n                        sys.stdout.flush()\n        except:\n            self.alive = False\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _color(self, color, msg):\n        if self.useColor:\n            return '{0}{1}{2}'.format(color, msg, self.RESET_SEQ)\n        else:\n            return msg", "response": "Converts a message to be printed to the user s terminal in red"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooping and copy console -> serial until EXIT_CHARCTER character is found.", "response": "def _inputLoop(self):\n        \"\"\" Loop and copy console->serial until EXIT_CHARCTER character is found. \"\"\"\n        \n        # Switch statement for handling \"special\" characters\n        actionChars = {self.EXIT_CHARACTER: self._exit,\n                       self.EXIT_CHARACTER_2: self._exit,\n                        \n                       console.CURSOR_LEFT: self._cursorLeft,\n                       console.CURSOR_RIGHT: self._cursorRight,\n                       console.CURSOR_UP: self._cursorUp,\n                       console.CURSOR_DOWN: self._cursorDown,\n                        \n                       '\\n': self._doConfirmInput,\n                       '\\t': self._doCommandCompletion,\n                       \n                       self.CTRL_Z_CHARACTER: self._handleCtrlZ,\n                       self.ESC_CHARACTER: self._handleEsc,\n                       \n                       self.BACKSPACE_CHARACTER: self._handleBackspace,\n                       console.DELETE: self._handleDelete,\n                       console.HOME: self._handleHome,\n                       console.END: self._handleEnd}\n\n        try:\n            while self.alive:\n                try:\n                    c = console.getkey()\n                except KeyboardInterrupt:\n                    c = serial.to_bytes([3])\n                if c in actionChars:\n                    # Handle character directly                    \n                    actionChars[c]()\n                elif len(c) == 1 and self._isPrintable(c):\n                    self.inputBuffer.insert(self.cursorPos, c)\n                    self.cursorPos += 1\n                    self._refreshInputPrompt()\n                #else:\n                #    for a in c:\n                #        print('GOT:',a,'(',ord(a),')')\n        except:\n            self.alive = False\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handleCtrlZ(self):\n        if self._typingSms:\n            self.serial.write(''.join(self.inputBuffer))\n            self.serial.write(self.CTRL_Z_CHARACTER)\n            self._typingSms = False\n            self.inputBuffer = []\n            self.cursorPos = 0\n            sys.stdout.write('\\n')\n            self._refreshInputPrompt()", "response": "Handler for CTRL + Z keypresses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _exit(self):\n        self._removeInputPrompt()\n        print(self._color(self.COLOR_YELLOW, 'CLOSING TERMINAL...')) \n        self.stop()", "response": "Closes the terminal and app."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling cursor left events", "response": "def _cursorLeft(self):\n        \"\"\" Handles \"cursor left\" events \"\"\"\n        if self.cursorPos > 0:\n            self.cursorPos -= 1\n            sys.stdout.write(console.CURSOR_LEFT)\n            sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cursorRight(self):\n        if self.cursorPos < len(self.inputBuffer):\n            self.cursorPos += 1\n            sys.stdout.write(console.CURSOR_RIGHT)\n            sys.stdout.flush()", "response": "Handles cursor right events"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cursorUp(self):\n        if self.historyPos > 0:\n            self.historyPos -= 1\n            clearLen = len(self.inputBuffer)\n            self.inputBuffer = list(self.history[self.historyPos])\n            self.cursorPos = len(self.inputBuffer)\n            self._refreshInputPrompt(clearLen)", "response": "Handles cursor up events"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handleEnd(self):\n        self.cursorPos = len(self.inputBuffer)\n        self._refreshInputPrompt(len(self.inputBuffer))", "response": "Handles the end character."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _doCommandCompletion(self):\n        prefix =  ''.join(self.inputBuffer).strip().upper()\n        matches = self.completion.keys(prefix)\n        matchLen = len(matches)        \n        if matchLen == 0 and prefix[-1] == '=':\n            try:                \n                command = prefix[:-1]\n            except KeyError:\n                pass                        \n            else:\n                self.__printCommandSyntax(command)\n        elif matchLen > 0:                        \n            if matchLen == 1:\n                if matches[0] == prefix:\n                    # User has already entered command - show command syntax\n                    self.__printCommandSyntax(prefix)\n                else:\n                    # Complete only possible command\n                    self.inputBuffer = list(matches[0])\n                    self.cursorPos = len(self.inputBuffer)\n                    self._refreshInputPrompt(len(self.inputBuffer))\n                return\n            else:\n                commonPrefix = self.completion.longestCommonPrefix(''.join(self.inputBuffer))\n                self.inputBuffer = list(commonPrefix)\n                self.cursorPos = len(self.inputBuffer)\n                if matchLen > 20:\n                    matches = matches[:20]\n                    matches.append('... ({0} more)'.format(matchLen - 20))\n            sys.stdout.write('\\n')\n            for match in matches:\n                sys.stdout.write(' {0} '.format(match))\n            sys.stdout.write('\\n')\n            sys.stdout.flush()\n            self._refreshInputPrompt(len(self.inputBuffer))", "response": "Command - completion method"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __printCommandSyntax(self, command):\n        commandHelp = self.completion[command]\n        if commandHelp != None and len(commandHelp) > 2:\n            commandValues = commandHelp[2]\n            #commandDefault = commandHelp[3]\n            displayHelp = [self._color(self.COLOR_WHITE, command)]\n            if commandValues != None:\n                valuesIsEnum = len(commandHelp) >= 6\n                if '+' in command or command.upper() in ['ATS0']:\n                    displayHelp.append(self._color(self.COLOR_WHITE, '='))\n                displayHelp.append(('|' if valuesIsEnum else ',').join([value[0] for value in commandValues]))\n            sys.stdout.write('\\r Syntax: {0}\\n'.format(self._color(self.COLOR_WHITE, ''.join(displayHelp))))\n            sys.stdout.flush()\n            self._refreshInputPrompt(len(self.inputBuffer))", "response": "Command-completion helper method: print command syntax"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect to the device and starts the read thread", "response": "def connect(self):\n        \"\"\" Connects to the device and starts the read thread \"\"\"                \n        self.serial = serial.Serial(port=self.port, baudrate=self.baudrate, timeout=self.timeout)\n        # Start read thread\n        self.alive = True \n        self.rxThread = threading.Thread(target=self._readLoop)\n        self.rxThread.daemon = True\n        self.rxThread.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self):\n        self.alive = False\n        self.rxThread.join()\n        self.serial.close()", "response": "Stops the read thread waits for it to exit cleanly closes the underlying serial port"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the lines from the connected device and handle them", "response": "def _readLoop(self):\n        \"\"\" Read thread main loop\n        \n        Reads lines from the connected device\n        \"\"\"\n        try:\n            readTermSeq = list(self.RX_EOL_SEQ)\n            readTermLen = len(readTermSeq)\n            rxBuffer = []\n            while self.alive:\n                data = self.serial.read(1)\n                if data != '': # check for timeout\n                    #print >> sys.stderr, ' RX:', data,'({0})'.format(ord(data))\n                    rxBuffer.append(data)\n                    if rxBuffer[-readTermLen:] == readTermSeq:                        \n                        # A line (or other logical segment) has been read\n                        line = ''.join(rxBuffer[:-readTermLen])\n                        rxBuffer = []\n                        if len(line) > 0:                          \n                            #print 'calling handler'                      \n                            self._handleLineRead(line)\n                    elif self._expectResponseTermSeq:\n                        if rxBuffer[-len(self._expectResponseTermSeq):] == self._expectResponseTermSeq:\n                            line = ''.join(rxBuffer) \n                            rxBuffer = []\n                            self._handleLineRead(line, checkForResponseTerm=False)                                                \n            #else:\n                #' <RX timeout>'\n        except serial.SerialException as e:\n            self.alive = False\n            try:\n                self.serial.close()\n            except Exception: #pragma: no cover\n                pass\n            # Notify the fatal error handler\n            self.fatalErrorCallback(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _allKeys(self, prefix):\n        global dictItemsIter\n        result = [prefix + self.key] if self.key != None else []\n        for key, trie in dictItemsIter(self.slots):        \n            result.extend(trie._allKeys(prefix + key))        \n        return result", "response": "Private implementation method. Use keys() instead."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding an SMS - SUBMIT PDU to send a message to the specified mobile number.", "response": "def encodeSmsSubmitPdu(number, text, reference=0, validity=None, smsc=None, requestStatusReport=True, rejectDuplicates=False, sendFlash=False):\n    \"\"\" Creates an SMS-SUBMIT PDU for sending a message with the specified text to the specified number\n    \n    :param number: the destination mobile number\n    :type number: str\n    :param text: the message text\n    :type text: str\n    :param reference: message reference number (see also: rejectDuplicates parameter)\n    :type reference: int\n    :param validity: message validity period (absolute or relative)\n    :type validity: datetime.timedelta (relative) or datetime.datetime (absolute)\n    :param smsc: SMSC number to use (leave None to use default)\n    :type smsc: str\n    :param rejectDuplicates: Flag that controls the TP-RD parameter (messages with same destination and reference may be rejected if True)\n    :type rejectDuplicates: bool\n            \n    :return: A list of one or more tuples containing the SMS PDU (as a bytearray, and the length of the TPDU part\n    :rtype: list of tuples\n    \"\"\"     \n    tpduFirstOctet = 0x01 # SMS-SUBMIT PDU\n    if validity != None:\n        # Validity period format (TP-VPF) is stored in bits 4,3 of the first TPDU octet\n        if type(validity) == timedelta:\n            # Relative (TP-VP is integer)\n            tpduFirstOctet |= 0x10 # bit4 == 1, bit3 == 0\n            validityPeriod = [_encodeRelativeValidityPeriod(validity)]\n        elif type(validity) == datetime:\n            # Absolute (TP-VP is semi-octet encoded date)\n            tpduFirstOctet |= 0x18 # bit4 == 1, bit3 == 1\n            validityPeriod = _encodeTimestamp(validity) \n        else:\n            raise TypeError('\"validity\" must be of type datetime.timedelta (for relative value) or datetime.datetime (for absolute value)')        \n    else:\n        validityPeriod = None\n    if rejectDuplicates:\n        tpduFirstOctet |= 0x04 # bit2 == 1\n    if requestStatusReport:\n        tpduFirstOctet |= 0x20 # bit5 == 1\n    \n    # Encode message text and set data coding scheme based on text contents\n    try:\n        encodedText = encodeGsm7(text)\n    except ValueError:\n        # Cannot encode text using GSM-7; use UCS2 instead\n        alphabet = 0x08 # UCS2\n    else:\n        alphabet = 0x00 # GSM-7    \n        \n    # Check if message should be concatenated\n    if len(text) > MAX_MESSAGE_LENGTH[alphabet]:\n        # Text too long for single PDU - add \"concatenation\" User Data Header\n        concatHeaderPrototype = Concatenation()\n        concatHeaderPrototype.reference = reference\n        pduCount = int(len(text) / MAX_MESSAGE_LENGTH[alphabet]) + 1\n        concatHeaderPrototype.parts  = pduCount\n        tpduFirstOctet |= 0x40\n    else:\n        concatHeaderPrototype = None\n        pduCount = 1\n    \n    # Construct required PDU(s)\n    pdus = []    \n    for i in xrange(pduCount):\n        pdu = bytearray()\n        if smsc:\n            pdu.extend(_encodeAddressField(smsc, smscField=True))\n        else:\n            pdu.append(0x00) # Don't supply an SMSC number - use the one configured in the device \n    \n        udh = bytearray()\n        if concatHeaderPrototype != None:\n            concatHeader = copy(concatHeaderPrototype)\n            concatHeader.number = i + 1\n            if alphabet == 0x00:\n                pduText = text[i*153:(i+1) * 153]\n            elif alphabet == 0x08:\n                pduText = text[i * 67 : (i + 1) * 67]\n            udh.extend(concatHeader.encode())\n        else:\n            pduText = text\n        \n        udhLen = len(udh)        \n        \n        pdu.append(tpduFirstOctet)\n        pdu.append(reference) # message reference\n        # Add destination number    \n        pdu.extend(_encodeAddressField(number))\n        pdu.append(0x00) # Protocol identifier - no higher-level protocol\n    \n        pdu.append(alphabet if not sendFlash else (0x10 if alphabet == 0x00 else 0x18))\n        if validityPeriod:\n            pdu.extend(validityPeriod)\n        \n        if alphabet == 0x00: # GSM-7\n            encodedText = encodeGsm7(pduText)\n            userDataLength = len(encodedText) # Payload size in septets/characters\n            if udhLen > 0:\n                shift = ((udhLen + 1) * 8) % 7 # \"fill bits\" needed to make the UDH end on a septet boundary\n                userData = packSeptets(encodedText, padBits=shift)\n                if shift > 0:\n                    userDataLength += 1 # take padding bits into account\n            else:\n                userData = packSeptets(encodedText)\n        elif alphabet == 0x08: # UCS2\n            userData = encodeUcs2(pduText)\n            userDataLength = len(userData)\n          \n        if udhLen > 0:            \n            userDataLength += udhLen + 1 # +1 for the UDH length indicator byte\n            pdu.append(userDataLength)\n            pdu.append(udhLen)\n            pdu.extend(udh) # UDH\n        else:\n            pdu.append(userDataLength)\n        pdu.extend(userData) # User Data (message payload)\n        tpdu_length = len(pdu) - 1\n        pdus.append(Pdu(pdu, tpdu_length))\n    return pdus"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decodeSmsPdu(pdu):\n    try:\n        pdu = toByteArray(pdu)\n    except Exception as e:\n        # Python 2 raises TypeError, Python 3 raises binascii.Error\n        raise EncodingError(e)\n    result = {}\n    pduIter = iter(pdu)\n \n    smscNumber, smscBytesRead = _decodeAddressField(pduIter, smscField=True)\n    result['smsc'] = smscNumber\n    result['tpdu_length'] = len(pdu) - smscBytesRead\n    \n    tpduFirstOctet = next(pduIter) \n    \n    pduType = tpduFirstOctet & 0x03 # bits 1-0\n    if pduType == 0x00: # SMS-DELIVER or SMS-DELIVER REPORT\n        result['type'] = 'SMS-DELIVER'\n        result['number'] = _decodeAddressField(pduIter)[0]\n        result['protocol_id'] = next(pduIter)\n        dataCoding = _decodeDataCoding(next(pduIter))\n        result['time'] = _decodeTimestamp(pduIter)\n        userDataLen = next(pduIter)\n        udhPresent = (tpduFirstOctet & 0x40) != 0\n        ud = _decodeUserData(pduIter, userDataLen, dataCoding, udhPresent)\n        result.update(ud)\n    elif pduType == 0x01: # SMS-SUBMIT or SMS-SUBMIT-REPORT\n        result['type'] = 'SMS-SUBMIT'\n        result['reference'] = next(pduIter) # message reference - we don't really use this\n        result['number'] = _decodeAddressField(pduIter)[0]\n        result['protocol_id'] = next(pduIter)\n        dataCoding = _decodeDataCoding(next(pduIter))\n        validityPeriodFormat = (tpduFirstOctet & 0x18) >> 3 # bits 4,3\n        if validityPeriodFormat == 0x02: # TP-VP field present and integer represented (relative)\n            result['validity'] = _decodeRelativeValidityPeriod(next(pduIter))\n        elif validityPeriodFormat == 0x03: # TP-VP field present and semi-octet represented (absolute)            \n            result['validity'] = _decodeTimestamp(pduIter)\n        userDataLen = next(pduIter)\n        udhPresent = (tpduFirstOctet & 0x40) != 0\n        ud = _decodeUserData(pduIter, userDataLen, dataCoding, udhPresent)\n        result.update(ud)\n    elif pduType == 0x02: # SMS-STATUS-REPORT or SMS-COMMAND\n        result['type'] = 'SMS-STATUS-REPORT'\n        result['reference'] = next(pduIter)\n        result['number'] = _decodeAddressField(pduIter)[0]\n        result['time'] = _decodeTimestamp(pduIter)\n        result['discharge'] = _decodeTimestamp(pduIter)\n        result['status'] = next(pduIter)        \n    else:\n        raise EncodingError('Unknown SMS message type: {0}. First TPDU octet was: {1}'.format(pduType, tpduFirstOctet))\n    \n    return result", "response": "Decodes an SMS PDU into a dictionary containing the number and text fields."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _decodeUserData(byteIter, userDataLen, dataCoding, udhPresent):\n    result = {}\n    if udhPresent:\n        # User Data Header is present\n        result['udh'] = []\n        udhLen = next(byteIter)\n        ieLenRead = 0\n        # Parse and store UDH fields\n        while ieLenRead < udhLen:\n            ie = InformationElement.decode(byteIter)\n            ieLenRead += len(ie)\n            result['udh'].append(ie)\n        del ieLenRead\n        if dataCoding == 0x00: # GSM-7\n            # Since we are using 7-bit data, \"fill bits\" may have been added to make the UDH end on a septet boundary\n            shift = ((udhLen + 1) * 8) % 7 # \"fill bits\" needed to make the UDH end on a septet boundary\n            # Simulate another \"shift\" in the unpackSeptets algorithm in order to ignore the fill bits\n            prevOctet = next(byteIter)\n            shift += 1\n\n    if dataCoding == 0x00: # GSM-7\n        if udhPresent:\n            userDataSeptets = unpackSeptets(byteIter, userDataLen, prevOctet, shift)\n        else:\n            userDataSeptets = unpackSeptets(byteIter, userDataLen)\n        result['text'] = decodeGsm7(userDataSeptets)\n    elif dataCoding == 0x02: # UCS2\n        result['text'] = decodeUcs2(byteIter, userDataLen)\n    else: # 8-bit (data)\n        userData = []\n        for b in byteIter:\n            userData.append(unichr(b))\n        result['text'] = ''.join(userData)\n    return result", "response": "Decodes a PDU user data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _decodeRelativeValidityPeriod(tpVp):\n    if tpVp <= 143:\n        return timedelta(minutes=((tpVp + 1) * 5))\n    elif 144 <= tpVp <= 167:\n        return timedelta(hours=12, minutes=((tpVp - 143) * 30))\n    elif 168 <= tpVp <= 196:\n        return timedelta(days=(tpVp - 166))\n    elif 197 <= tpVp <= 255:\n        return timedelta(weeks=(tpVp - 192))\n    else:\n        raise ValueError('tpVp must be in range [0, 255]')", "response": "Calculates the relative validity period for a given TOTP version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode the given relative validity period timedelta into an integer for use in an SMS PDU.", "response": "def _encodeRelativeValidityPeriod(validityPeriod):\n    \"\"\" Encodes the specified relative validity period timedelta into an integer for use in an SMS PDU\n    (based on the table in section 9.2.3.12 of GSM 03.40)\n    \n    :param validityPeriod: The validity period to encode\n    :type validityPeriod: datetime.timedelta\n    :rtype: int\n    \"\"\"\n    # Python 2.6 does not have timedelta.total_seconds(), so compute it manually\n    #seconds = validityPeriod.total_seconds()\n    seconds = validityPeriod.seconds + (validityPeriod.days * 24 * 3600)\n    if seconds <= 43200: # 12 hours\n        tpVp = int(seconds / 300) - 1 # divide by 5 minutes, subtract 1\n    elif seconds <= 86400: # 24 hours\n        tpVp = int((seconds - 43200) / 1800) + 143 # subtract 12 hours, divide by 30 minutes. add 143\n    elif validityPeriod.days <= 30: # 30 days\n        tpVp = validityPeriod.days + 166 # amount of days + 166\n    elif validityPeriod.days <= 441: # max value of tpVp is 255\n        tpVp = int(validityPeriod.days / 7) + 192 # amount of weeks + 192\n    else:\n        raise ValueError('Validity period too long; tpVp limited to 1 octet (max value: 255)')\n    return tpVp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding a 7 - octet timestamp.", "response": "def _decodeTimestamp(byteIter):\n    \"\"\" Decodes a 7-octet timestamp \"\"\"\n    dateStr = decodeSemiOctets(byteIter, 7)\n    timeZoneStr = dateStr[-2:]        \n    return datetime.strptime(dateStr[:-2], '%y%m%d%H%M%S').replace(tzinfo=SmsPduTzInfo(timeZoneStr))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode a date object into a 7 - octet bytearray.", "response": "def _encodeTimestamp(timestamp):\n    \"\"\" Encodes a 7-octet timestamp from the specified date\n    \n    Note: the specified timestamp must have a UTC offset set; you can use gsmmodem.util.SimpleOffsetTzInfo for simple cases\n    \n    :param timestamp: The timestamp to encode\n    :type timestamp: datetime.datetime\n    \n    :return: The encoded timestamp\n    :rtype: bytearray\n    \"\"\"\n    if timestamp.tzinfo == None:\n        raise ValueError('Please specify time zone information for the timestamp (e.g. by using gsmmodem.util.SimpleOffsetTzInfo)')\n\n    # See if the timezone difference is positive/negative\n    tzDelta = timestamp.utcoffset()\n    if tzDelta.days >= 0:\n        tzValStr = '{0:0>2}'.format(int(tzDelta.seconds / 60 / 15))\n    else: # negative\n        tzVal = int((tzDelta.days * -3600 * 24 - tzDelta.seconds) / 60 / 15) # calculate offset in 0.25 hours\n        # Cast as literal hex value and set MSB of first semi-octet of timezone to 1 to indicate negative value\n        tzVal = int('{0:0>2}'.format(tzVal), 16) | 0x80\n        tzValStr = '{0:0>2X}'.format(tzVal)\n\n    dateStr = timestamp.strftime('%y%m%d%H%M%S') + tzValStr\n\n    return encodeSemiOctets(dateStr)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decodeAddressField(byteIter, smscField=False, log=False):\n    addressLen = next(byteIter)\n    if addressLen > 0:\n        toa = next(byteIter)\n        ton = (toa & 0x70) # bits 6,5,4 of type-of-address == type-of-number\n        if ton == 0x50: \n            # Alphanumberic number            \n            addressLen = int(math.ceil(addressLen / 2.0))\n            septets = unpackSeptets(byteIter, addressLen)\n            addressValue = decodeGsm7(septets)\n            return (addressValue, (addressLen + 2))\n        else:\n            # ton == 0x00: Unknown (might be international, local, etc) - leave as is            \n            # ton == 0x20: National number\n            if smscField:\n                addressValue = decodeSemiOctets(byteIter, addressLen-1)\n            else:\n                if addressLen % 2:\n                    addressLen = int(addressLen / 2) + 1\n                else:\n                    addressLen = int(addressLen / 2)                \n                addressValue = decodeSemiOctets(byteIter, addressLen)\n                addressLen += 1 # for the return value, add the toa byte\n            if ton == 0x10: # International number\n                addressValue = '+' + addressValue\n            return (addressValue, (addressLen + 1))\n    else:\n        return (None, 1)", "response": "Decodes the address field from the byte stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _encodeAddressField(address, smscField=False):\n    # First, see if this is a number or an alphanumeric string\n    toa = 0x80 | 0x00 | 0x01 # Type-of-address start | Unknown type-of-number | ISDN/tel numbering plan\n    alphaNumeric = False    \n    if address.isalnum():\n        # Might just be a local number\n        if address.isdigit():\n            # Local number\n            toa |= 0x20\n        else:\n            # Alphanumeric address\n            toa |= 0x50\n            toa &= 0xFE # switch to \"unknown\" numbering plan\n            alphaNumeric = True\n    else:\n        if address[0] == '+' and address[1:].isdigit():\n            # International number\n            toa |= 0x10\n            # Remove the '+' prefix\n            address = address[1:]\n        else:\n            # Alphanumeric address\n            toa |= 0x50\n            toa &= 0xFE # switch to \"unknown\" numbering plan\n            alphaNumeric = True\n    if  alphaNumeric:\n        addressValue = packSeptets(encodeGsm7(address, False))\n        addressLen = len(addressValue) * 2        \n    else:\n        addressValue = encodeSemiOctets(address)\n        if smscField:            \n            addressLen = len(addressValue) + 1\n        else:\n            addressLen = len(address)\n    result = bytearray()\n    result.append(addressLen)\n    result.append(toa)\n    result.extend(addressValue)\n    return result", "response": "Encodes the given address into an address field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encodeSemiOctets(number):\n    if len(number) % 2 == 1:\n        number = number + 'F' # append the \"end\" indicator\n    octets = [int(number[i+1] + number[i], 16) for i in xrange(0, len(number), 2)]\n    return bytearray(octets)", "response": "Encode a number in a sequence of octet values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decodeSemiOctets(encodedNumber, numberOfOctets=None):\n    number = []\n    if type(encodedNumber) in (str, bytes):\n        encodedNumber = bytearray(codecs.decode(encodedNumber, 'hex_codec'))\n    i = 0\n    for octet in encodedNumber:        \n        hexVal = hex(octet)[2:].zfill(2)   \n        number.append(hexVal[1])\n        if hexVal[0] != 'f':\n            number.append(hexVal[0])\n        else:\n            break\n        if numberOfOctets != None:\n            i += 1\n            if i == numberOfOctets:\n                break\n    return ''.join(number)", "response": "This function decodes a number from a semi - octet encoded string or hex string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npack the specified octets into a single GSM - 7 septet.", "response": "def packSeptets(octets, padBits=0):\n    \"\"\" Packs the specified octets into septets\n    \n    Typically the output of encodeGsm7 would be used as input to this function. The resulting\n    bytearray contains the original GSM-7 characters packed into septets ready for transmission.\n    \n    :rtype: bytearray\n    \"\"\"\n    result = bytearray()    \n    if type(octets) == str:\n        octets = iter(rawStrToByteArray(octets))\n    elif type(octets) == bytearray:\n        octets = iter(octets)\n    shift = padBits\n    if padBits == 0:\n        prevSeptet = next(octets)\n    else:\n        prevSeptet = 0x00\n    for octet in octets:\n        septet = octet & 0x7f;\n        if shift == 7:\n            # prevSeptet has already been fully added to result\n            shift = 0        \n            prevSeptet = septet\n            continue            \n        b = ((septet << (7 - shift)) & 0xFF) | (prevSeptet >> shift)\n        prevSeptet = septet\n        shift += 1\n        result.append(b)    \n    if shift != 7:\n        # There is a bit \"left over\" from prevSeptet\n        result.append(prevSeptet >> shift)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nunpacks the specified septets into a list of octets.", "response": "def unpackSeptets(septets, numberOfSeptets=None, prevOctet=None, shift=7):\n    \"\"\" Unpacks the specified septets into octets \n    \n    :param septets: Iterator or iterable containing the septets packed into octets\n    :type septets: iter(bytearray), bytearray or str\n    :param numberOfSeptets: The amount of septets to unpack (or None for all remaining in \"septets\")\n    :type numberOfSeptets: int or None\n    \n    :return: The septets unpacked into octets\n    :rtype: bytearray\n    \"\"\"    \n    result = bytearray()    \n    if type(septets) == str:\n        septets = iter(rawStrToByteArray(septets))\n    elif type(septets) == bytearray:\n        septets = iter(septets)    \n    if numberOfSeptets == None:        \n        numberOfSeptets = MAX_INT # Loop until StopIteration\n    i = 0\n    for octet in septets:\n        i += 1\n        if shift == 7:\n            shift = 1\n            if prevOctet != None:                \n                result.append(prevOctet >> 1)            \n            if i <= numberOfSeptets:\n                result.append(octet & 0x7F)\n                prevOctet = octet                \n            if i == numberOfSeptets:\n                break\n            else:\n                continue\n        b = ((octet << shift) & 0x7F) | (prevOctet >> (8 - shift))\n        \n        prevOctet = octet        \n        result.append(b)\n        shift += 1\n        \n        if i == numberOfSeptets:\n            break\n    if shift == 7:\n        b = prevOctet >> (8 - shift)\n        if b:\n            # The final septet value still needs to be unpacked\n            result.append(b)        \n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecodes UCS2 - encoded text from the specified byte iterator up to a maximum of numBytes.", "response": "def decodeUcs2(byteIter, numBytes):\n    \"\"\" Decodes UCS2-encoded text from the specified byte iterator, up to a maximum of numBytes \"\"\"\n    userData = []\n    i = 0\n    try:\n        while i < numBytes:\n            userData.append(unichr((next(byteIter) << 8) | next(byteIter)))\n            i += 2\n    except StopIteration:\n        # Not enough bytes in iterator to reach numBytes; return what we have\n        pass\n    return ''.join(userData)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encodeUcs2(text):\n    result = bytearray()\n    for b in map(ord, text):\n        result.append(b >> 8)\n        result.append(b & 0xFF)\n    return result", "response": "This function encodes the specified text string into a UCS2 - encoded bytearray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode(cls, byteIter):\n        iei = next(byteIter)\n        ieLen = next(byteIter)\n        ieData = []\n        for i in xrange(ieLen):\n            ieData.append(next(byteIter))\n        return InformationElement(iei, ieLen, ieData)", "response": "Decodes a single InformationElement at the current position in the specified\n        byte iterator \nAuthorizedException\n        returns None if the current position is out of range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode(self):\n        result = bytearray()\n        result.append(self.id)\n        result.append(self.dataLength)\n        result.extend(self.data)\n        return result", "response": "Encodes this IE and returns the resulting bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the specified time string into a datetime object representing the specified time string.", "response": "def parseTextModeTimeStr(timeStr):\n    \"\"\" Parses the specified SMS text mode time string\n    \n    The time stamp format is \"yy/MM/dd,hh:mm:ss\u00b1zz\"\n    (yy = year, MM = month, dd = day, hh = hour, mm = minute, ss = second, zz = time zone\n    [Note: the unit of time zone is a quarter of an hour])\n    \n    :param timeStr: The time string to parse\n    :type timeStr: str\n    \n    :return: datetime object representing the specified time string\n    :rtype: datetime.datetime\n    \"\"\"\n    msgTime = timeStr[:-3]\n    tzOffsetHours = int(int(timeStr[-3:]) * 0.25)\n    return datetime.strptime(msgTime, '%y/%m/%d,%H:%M:%S').replace(tzinfo=SimpleOffsetTzInfo(tzOffsetHours))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lineMatching(regexStr, lines):\n    regex = re.compile(regexStr)\n    for line in lines:\n        m = regex.match(line)\n        if m:\n            return m\n    else:\n        return None", "response": "Searches through the specified list of strings and returns the regular expression \n    that matches the specified regex string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lineMatchingPattern(pattern, lines):\n    for line in lines:\n        m = pattern.match(line)\n        if m:\n            return m\n    else:\n        return None", "response": "Searches through the specified list of strings and returns the regular expression \n    that matches the specified regular expression pattern."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef allLinesMatchingPattern(pattern, lines):\n    result = []\n    for line in lines:\n        m = pattern.match(line)\n        if m:\n            result.append(m)\n    return result", "response": "Like lineMatchingPattern but returns all lines that match the specified regular expression pattern."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget settings for getting Chrome and Chromium cookies on OSX.", "response": "def get_osx_config(browser: str) -> dict:\n    \"\"\"Get settings for getting Chrome/Chromium cookies on OSX.\n\n    Args:\n        browser: Either \"Chrome\" or \"Chromium\"\n    Returns:\n        Config dictionary for Chrome/Chromium cookie decryption\n\n    \"\"\"\n    # Verify supported browser, fail early otherwise\n    if browser.lower() == 'chrome':\n        cookie_file = ('~/Library/Application Support/Google/Chrome/Default/'\n                       'Cookies')\n    elif browser.lower() == \"chromium\":\n        cookie_file = '~/Library/Application Support/Chromium/Default/Cookies'\n    else:\n        raise ValueError(\"Browser must be either Chrome or Chromium.\")\n\n    config = {\n        'my_pass': keyring.get_password(\n            '{} Safe Storage'.format(browser), browser),\n        'iterations': 1003,\n        'cookie_file': cookie_file,\n        }\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the settings for Chrome or Chromium cookies on Linux.", "response": "def get_linux_config(browser: str) -> dict:\n    \"\"\"Get the settings for Chrome/Chromium cookies on Linux.\n\n    Args:\n        browser: Either \"Chrome\" or \"Chromium\"\n    Returns:\n        Config dictionary for Chrome/Chromium cookie decryption\n\n    \"\"\"\n    # Verify supported browser, fail early otherwise\n    if browser.lower() == 'chrome':\n        cookie_file = '~/.config/google-chrome/Default/Cookies'\n    elif browser.lower() == \"chromium\":\n        cookie_file = '~/.config/chromium/Default/Cookies'\n    else:\n        raise ValueError(\"Browser must be either Chrome or Chromium.\")\n\n    # Set the default linux password\n    config = {\n        'my_pass': 'peanuts',\n        'iterations': 1,\n        'cookie_file': cookie_file,\n    }\n\n    # Try to get pass from Gnome / libsecret if it seems available\n    # https://github.com/n8henrie/pycookiecheat/issues/12\n    try:\n        import gi\n        gi.require_version('Secret', '1')\n        from gi.repository import Secret\n    except ImportError:\n        pass\n    else:\n        flags = Secret.ServiceFlags.LOAD_COLLECTIONS\n        service = Secret.Service.get_sync(flags)\n\n        gnome_keyring = service.get_collections()\n        unlocked_keyrings = service.unlock_sync(gnome_keyring).unlocked\n\n        keyring_name = \"{} Safe Storage\".format(browser.capitalize())\n\n        for unlocked_keyring in unlocked_keyrings:\n            for item in unlocked_keyring.get_items():\n                if item.get_label() == keyring_name:\n                    item.load_secret_sync()\n                    config['my_pass'] = item.get_secret().get_text()\n                    break\n            else:\n                # Inner loop didn't `break`, keep looking\n                continue\n\n            # Inner loop did `break`, so `break` outer loop\n            break\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve cookies from Chrome or Chromium on OSX or Linux.", "response": "def chrome_cookies(\n        url: str,\n        cookie_file: str = None,\n        browser: str = \"Chrome\",\n        curl_cookie_file: str = None,\n        ) -> dict:\n    \"\"\"Retrieve cookies from Chrome/Chromium on OSX or Linux.\n\n    Args:\n        url: Domain from which to retrieve cookies, starting with http(s)\n        cookie_file: Path to alternate file to search for cookies\n        browser: Name of the browser's cookies to read ('Chrome' or 'Chromium')\n        curl_cookie_file: Path to save the cookie file to be used with cURL\n    Returns:\n        Dictionary of cookie values for URL\n\n    \"\"\"\n    # If running Chrome on OSX\n    if sys.platform == 'darwin':\n        config = get_osx_config(browser)\n    elif sys.platform.startswith('linux'):\n        config = get_linux_config(browser)\n    else:\n        raise OSError(\"This script only works on OSX or Linux.\")\n\n    config.update({\n        'init_vector': b' ' * 16,\n        'length': 16,\n        'salt': b'saltysalt',\n    })\n\n    if cookie_file:\n        cookie_file = str(pathlib.Path(cookie_file).expanduser())\n    else:\n        cookie_file = str(pathlib.Path(config['cookie_file']).expanduser())\n\n    enc_key = pbkdf2_hmac(hash_name='sha1',\n                          password=config['my_pass'].encode('utf8'),\n                          salt=config['salt'],\n                          iterations=config['iterations'],\n                          dklen=config['length'])\n\n    parsed_url = urllib.parse.urlparse(url)\n    if parsed_url.scheme:\n        domain = parsed_url.netloc\n    else:\n        raise urllib.error.URLError(\"You must include a scheme with your URL.\")\n\n    try:\n        conn = sqlite3.connect(cookie_file)\n    except sqlite3.OperationalError:\n        print(\"Unable to connect to cookie_file at: {}\\n\".format(cookie_file))\n        raise\n\n    # Check whether the column name is `secure` or `is_secure`\n    secure_column_name = 'is_secure'\n    for sl_no, column_name, data_type, is_null, default_val, pk \\\n            in conn.execute('PRAGMA table_info(cookies)'):\n        if column_name == 'secure':\n            secure_column_name = 'secure'\n            break\n\n    sql = ('select host_key, path, ' + secure_column_name +\n           ', expires_utc, name, value, encrypted_value '\n           'from cookies where host_key like ?')\n\n    cookies = dict()\n    curl_cookies = []\n\n    for host_key in generate_host_keys(domain):\n        for hk, path, is_secure, expires_utc, cookie_key, val, enc_val \\\n                in conn.execute(sql, (host_key,)):\n            # if there is a not encrypted value or if the encrypted value\n            # doesn't start with the 'v1[01]' prefix, return v\n            if val or (enc_val[:3] not in (b'v10', b'v11')):\n                pass\n            else:\n                val = chrome_decrypt(enc_val, key=enc_key,\n                                     init_vector=config['init_vector'])\n            cookies[cookie_key] = val\n            if curl_cookie_file:\n                # http://www.cookiecentral.com/faq/#3.5\n                curl_cookies.append('\\t'.join(\n                    [hk, 'TRUE', path, 'TRUE' if is_secure else 'FALSE',\n                     str(expires_utc), cookie_key, val]\n                ))\n\n    conn.rollback()\n\n    # Save the file to destination\n    if curl_cookie_file:\n        with open(curl_cookie_file, \"w\") as text_file:\n            text_file.write('\\n'.join(curl_cookies) + '\\n')\n\n    return cookies"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields the host keys for a given hostname.", "response": "def generate_host_keys(hostname: str) -> Iterator[str]:\n    \"\"\"Yield Chrome/Chromium keys for `hostname`, from least to most specific.\n\n    Given a hostname like foo.example.com, this yields the key sequence:\n\n    example.com\n    .example.com\n    foo.example.com\n    .foo.example.com\n\n    \"\"\"\n    labels = hostname.split('.')\n    for i in range(2, len(labels) + 1):\n        domain = '.'.join(labels[-i:])\n        yield domain\n        yield '.' + domain"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that the given setpoint value is valid.", "response": "def _checkSetpointValue( setpointvalue, maxvalue ):   \n    \"\"\"Check that the given setpointvalue is valid.\n    \n    Args:\n        * setpointvalue (numerical): The setpoint value to be checked. Must be positive.\n        * maxvalue (numerical): Upper limit for setpoint value. Must be positive.\n        \n    Raises:\n        TypeError, ValueError\n    \n    \"\"\"\n    if maxvalue is None:\n        raise TypeError('The maxvalue (for the setpoint) must not be None!')\n    minimalmodbus._checkNumerical(setpointvalue, minvalue=0, maxvalue=maxvalue, description='setpoint value')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that the given time value is valid.", "response": "def _checkTimeValue( timevalue, maxvalue ):   \n    \"\"\"Check that the given timevalue is valid.\n    \n    Args:\n        * timevalue (numerical): The time value to be checked. Must be positive.\n        * maxvalue (numerical): Upper limit for time value. Must be positive.\n        \n    Raises:\n        TypeError, ValueError\n    \n    \"\"\"\n    if maxvalue is None:\n        raise TypeError('The maxvalue (for the time value) must not be None!')\n    minimalmodbus._checkNumerical(timevalue, minvalue=0, maxvalue=maxvalue, description='time value')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the register address for a given pattern related parameter.", "response": "def _calculateRegisterAddress( registertype, patternnumber, stepnumber = None):\n    \"\"\"Calculate the register address for pattern related parameters.\n    \n    Args:\n        * registertype (string): The type of parameter, for example 'cycles'. \n          Allowed are the keys from :data:`REGISTER_START`.\n        * patternnumber (int): The pattern number.\n        * stepnumber (int): The step number. Use None if it not should affect the calculation.\n    \n    Returns:\n        The register address (int).\n    \n    Raises:\n        TypeError, ValueError\n    \n    \"\"\"\n    if stepnumber is None:\n        stepnumber = 0\n    \n    # Argument checking\n    _checkPatternNumber( patternnumber )\n    _checkStepNumber( stepnumber )\n    \n    if not registertype in list(REGISTER_START.keys()): # To comply with both Python2 and Python3\n        raise ValueError('Wrong register type: {0}. Allowed values: {1}'.format( \n            repr(registertype), repr( list(REGISTER_START.keys()) )))\n\n    # Calculate register address\n    address = REGISTER_START[registertype] + \\\n                patternnumber * REGISTER_OFFSET_PER_PATTERN[registertype] + \\\n                stepnumber * REGISTER_OFFSET_PER_STEP[registertype]\n        \n    return address"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the setpoint value.", "response": "def set_setpoint(self, setpointvalue):\n        \"\"\"Set the setpoint.\n        \n        Args:\n            setpointvalue (float): Setpoint [most often in degrees]\n            \n        \"\"\"\n        _checkSetpointValue( setpointvalue, self.setpoint_max )\n        self.write_register( 4097, setpointvalue, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_control_mode(self):\n        mode_value = self.read_register(4101)\n        \n        try:\n            return CONTROL_MODES[mode_value]\n        except KeyError:\n            raise ValueError( 'Could not parse the control mode value: {0}. Allowed values are: {1}'.format(\n                repr(mode_value), repr(list(CONTROL_MODES.keys()) ) ))", "response": "Get the name of the current operation mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_control_mode(self, modevalue):\n        minimalmodbus._checkInt(modevalue, minvalue=0, maxvalue=3, description='control mode') \n        self.write_register(4101, modevalue)", "response": "Set the control mode of the related resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the setpoint value for a pattern step.", "response": "def set_pattern_step_setpoint(self, patternnumber, stepnumber, setpointvalue):\n        \"\"\"Set the setpoint value for a step.\n\n        Args:\n            * patternnumber (integer): 0-7\n            * stepnumber (integer): 0-7\n            * setpointvalue (float): Setpoint value\n        \"\"\"\n        _checkPatternNumber(patternnumber)\n        _checkStepNumber(stepnumber)\n        _checkSetpointValue(setpointvalue, self.setpoint_max)\n        \n        address = _calculateRegisterAddress('setpoint', patternnumber, stepnumber)\n        self.write_register(address, setpointvalue, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_pattern_step_time(self, patternnumber, stepnumber):\n        _checkPatternNumber(patternnumber)\n        _checkStepNumber(stepnumber)\n        \n        address = _calculateRegisterAddress('time', patternnumber, stepnumber)\n        return self.read_register(address, 0)", "response": "Get the step time for a pattern."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the step time.", "response": "def set_pattern_step_time(self, patternnumber, stepnumber, timevalue):\n        \"\"\"Set the step time.\n\n        Args:\n            * patternnumber (integer): 0-7\n            * stepnumber (integer): 0-7\n            * timevalue (integer??): 0-900\n        \"\"\"\n        _checkPatternNumber(patternnumber)\n        _checkStepNumber(stepnumber)\n        _checkTimeValue(timevalue, self.time_max)\n        \n        address = _calculateRegisterAddress('time', patternnumber, stepnumber)\n        self.write_register(address, timevalue, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pattern_actual_step(self, patternnumber):\n        _checkPatternNumber(patternnumber)\n        \n        address = _calculateRegisterAddress('actualstep', patternnumber)\n        return self.read_register(address, 0)", "response": "Get the actual step parameter for a given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the actual step parameter for a given pattern.", "response": "def set_pattern_actual_step(self, patternnumber, value):\n        \"\"\"Set the 'actual step' parameter for a given pattern.\n\n        Args:\n            * patternnumber (integer): 0-7\n            * value (integer): 0-7\n        \"\"\"\n        _checkPatternNumber(patternnumber)\n        _checkStepNumber(value)\n        \n        address = _calculateRegisterAddress('actualstep', patternnumber)\n        self.write_register(address, value, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the number of additional cycles for a given pattern.", "response": "def get_pattern_additional_cycles(self, patternnumber):\n        \"\"\"Get the number of additional cycles for a given pattern.\n\n        Args:\n            patternnumber (integer): 0-7\n            \n        Returns:\n            The number of additional cycles (int).            \n            \n        \"\"\"\n        _checkPatternNumber(patternnumber)\n\n        address = _calculateRegisterAddress('cycles', patternnumber)\n        return self.read_register(address)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_pattern_additional_cycles(self, patternnumber, value):\n        _checkPatternNumber(patternnumber)\n        minimalmodbus._checkInt(value, minvalue=0, maxvalue=99, description='number of additional cycles') \n            \n        address = _calculateRegisterAddress('cycles', patternnumber)\n        self.write_register(address, value, 0)", "response": "Set the number of additional cycles for a given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_pattern_link_topattern(self, patternnumber):\n        _checkPatternNumber(patternnumber)\n\n        address = _calculateRegisterAddress('linkpattern', patternnumber)\n        return self.read_register(address)", "response": "Get the linked pattern value for a given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_pattern_variables(self, patternnumber):\n        _checkPatternNumber(patternnumber)\n        \n        outputstring = ''\n        for stepnumber in range(8):\n            outputstring += 'SP{0}: {1}  Time{0}: {2}\\n'.format(stepnumber, \\\n                self.get_pattern_step_setpoint( patternnumber, stepnumber), \\\n                self.get_pattern_step_time(     patternnumber, stepnumber)   )\n        \n        outputstring += 'Actual step:        {0}\\n'.format(self.get_pattern_actual_step(        patternnumber) )\n        outputstring += 'Additional cycles:  {0}\\n'.format(self.get_pattern_additional_cycles(  patternnumber) )\n        outputstring += 'Linked pattern:     {0}\\n'.format(self.get_pattern_link_topattern(     patternnumber) ) \n            \n        return outputstring", "response": "Get all variables for a given pattern at one time."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets all variables for a given pattern at one time.", "response": "def set_all_pattern_variables(self, patternnumber, \\\n        sp0, ti0, sp1, ti1, sp2, ti2, sp3, ti3, sp4, ti4, sp5, ti5, sp6, ti6, sp7, ti7, \\\n        actual_step, additional_cycles, link_pattern):\n        \"\"\"Set all variables for a given pattern at one time.\n\n        Args:\n            * patternnumber (integer): 0-7\n            * sp[*n*] (float): setpoint value for step *n*\n            * ti[*n*] (integer??): step time for step *n*, 0-900\n            * actual_step (int): ?\n            * additional_cycles(int): ?\n            * link_pattern(int): ?\n                       \n        \"\"\"\n        _checkPatternNumber(patternnumber)\n        \n        self.set_pattern_step_setpoint(patternnumber, 0, sp0)\n        self.set_pattern_step_setpoint(patternnumber, 1, sp1)\n        self.set_pattern_step_setpoint(patternnumber, 2, sp2)\n        self.set_pattern_step_setpoint(patternnumber, 3, sp3)\n        self.set_pattern_step_setpoint(patternnumber, 4, sp4)\n        self.set_pattern_step_setpoint(patternnumber, 5, sp5)\n        self.set_pattern_step_setpoint(patternnumber, 6, sp6)\n        self.set_pattern_step_setpoint(patternnumber, 7, sp7)\n        self.set_pattern_step_time(    patternnumber, 0, ti0)\n        self.set_pattern_step_time(    patternnumber, 1, ti1)\n        self.set_pattern_step_time(    patternnumber, 2, ti2)\n        self.set_pattern_step_time(    patternnumber, 3, ti3)\n        self.set_pattern_step_time(    patternnumber, 4, ti4)\n        self.set_pattern_step_time(    patternnumber, 5, ti5)\n        self.set_pattern_step_time(    patternnumber, 6, ti6)\n        self.set_pattern_step_time(    patternnumber, 7, ti7)\n        self.set_pattern_additional_cycles(patternnumber, additional_cycles)\n        self.set_pattern_link_topattern(   patternnumber, link_pattern)\n        self.set_pattern_actual_step(      patternnumber, actual_step)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a dummy_serial port on dummy_serial.", "response": "def open(self):\n        \"\"\"Open a (previously initialized) port on dummy_serial.\"\"\"\n        if VERBOSE:\n            _print_out('\\nDummy_serial: Opening port\\n')\n\n        if self._isOpen:\n            raise IOError('Dummy_serial: The port is already open')\n            \n        self._isOpen = True\n        self.port = self.initial_port_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose a port on dummy_serial.", "response": "def close(self):\n        \"\"\"Close a port on dummy_serial.\"\"\"\n        if VERBOSE:\n            _print_out('\\nDummy_serial: Closing port\\n')\n\n        if not self._isOpen:\n            raise IOError('Dummy_serial: The port is already closed')\n            \n        self._isOpen = False\n        self.port = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, inputdata):\n        if VERBOSE:\n            _print_out('\\nDummy_serial: Writing to port. Given:' + repr(inputdata) + '\\n')\n            \n        if sys.version_info[0] > 2:\n            if not type(inputdata) == bytes:\n                raise TypeError('The input must be type bytes. Given:' + repr(inputdata))\n            inputstring = str(inputdata, encoding='latin1')\n        else:\n            inputstring = inputdata\n\n        if not self._isOpen:\n            raise IOError('Dummy_serial: Trying to write, but the port is not open. Given:' + repr(inputdata))\n\n        # Look up which data that should be waiting for subsequent read commands\n        try:\n            response = RESPONSES[inputstring]\n        except:\n            response = DEFAULT_RESPONSE\n        self._waiting_data = response", "response": "Write to a dummy_serial."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self, numberOfBytes):\n        if VERBOSE:\n            _print_out('\\nDummy_serial: Reading from port (max length {!r} bytes)'.format(numberOfBytes))\n        \n        if numberOfBytes < 0:\n            raise IOError('Dummy_serial: The numberOfBytes to read must not be negative. Given: {!r}'.format(numberOfBytes))\n        \n        if not self._isOpen:\n            raise IOError('Dummy_serial: Trying to read, but the port is not open.')\n\n        # Do the actual reading from the waiting data, and simulate the influence of numberOfBytes\n\n        if self._waiting_data == DEFAULT_RESPONSE:\n            returnstring = self._waiting_data \n        elif numberOfBytes == len(self._waiting_data):\n            returnstring = self._waiting_data \n            self._waiting_data = NO_DATA_PRESENT\n        elif numberOfBytes < len(self._waiting_data):\n            if VERBOSE:\n                _print_out('Dummy_serial: The numberOfBytes to read is smaller than the available data. ' + \\\n                    'Some bytes will be kept for later. Available data: {!r} (length = {}), numberOfBytes: {}'.format( \\\n                    self._waiting_data, len(self._waiting_data), numberOfBytes))\n            returnstring = self._waiting_data[:numberOfBytes]\n            self._waiting_data = self._waiting_data[numberOfBytes:]\n        else: # Wait for timeout, as we have asked for more data than available\n            if VERBOSE:\n                _print_out('Dummy_serial: The numberOfBytes to read is larger than the available data. ' + \\\n                    'Will sleep until timeout. Available  data: {!r} (length = {}), numberOfBytes: {}'.format( \\\n                    self._waiting_data, len(self._waiting_data), numberOfBytes))\n            time.sleep(self.timeout)\n            returnstring = self._waiting_data \n            self._waiting_data = NO_DATA_PRESENT\n        \n        # TODO Adapt the behavior to better mimic the Windows behavior\n        \n        if VERBOSE:\n            _print_out('Dummy_serial read return data: {!r} (has length {})\\n'.format(returnstring, len(returnstring)))\n\n        if sys.version_info[0] > 2: # Convert types to make it python3 compatible\n            return bytes(returnstring, encoding='latin1')\n        else:\n            return returnstring", "response": "Read from a dummy_serial port."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a request string from the slaveaddress the function code and the payloaddata.", "response": "def _embedPayload(slaveaddress, mode, functioncode, payloaddata):\n    \"\"\"Build a request from the slaveaddress, the function code and the payload data.\n\n    Args:\n        * slaveaddress (int): The address of the slave.\n        * mode (str): The modbus protcol mode (MODE_RTU or MODE_ASCII)\n        * functioncode (int): The function code for the command to be performed. Can for example be 16 (Write register).\n        * payloaddata (str): The byte string to be sent to the slave.\n\n    Returns:\n        The built (raw) request string for sending to the slave (including CRC etc).\n\n    Raises:\n        ValueError, TypeError.\n\n    The resulting request has the format:\n     * RTU Mode: slaveaddress byte + functioncode byte + payloaddata + CRC (which is two bytes).\n     * ASCII Mode: header (:) + slaveaddress (2 characters) + functioncode (2 characters) + payloaddata + LRC (which is two characters) + footer (CRLF)\n\n    The LRC or CRC is calculated from the byte string made up of slaveaddress + functioncode + payloaddata.\n    The header, LRC/CRC, and footer are excluded from the calculation.\n\n    \"\"\"\n    _checkSlaveaddress(slaveaddress)\n    _checkMode(mode)\n    _checkFunctioncode(functioncode, None)\n    _checkString(payloaddata, description='payload')\n\n    firstPart = _numToOneByteString(slaveaddress) + _numToOneByteString(functioncode) + payloaddata\n\n    if mode == MODE_ASCII:\n        request = _ASCII_HEADER + \\\n                _hexencode(firstPart) + \\\n                _hexencode(_calculateLrcString(firstPart)) + \\\n                _ASCII_FOOTER\n    else:\n        request = firstPart + _calculateCrcString(firstPart)\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extractPayload(response, slaveaddress, mode, functioncode):\n    BYTEPOSITION_FOR_ASCII_HEADER          = 0  # Relative to plain response\n\n    BYTEPOSITION_FOR_SLAVEADDRESS          = 0  # Relative to (stripped) response\n    BYTEPOSITION_FOR_FUNCTIONCODE          = 1\n\n    NUMBER_OF_RESPONSE_STARTBYTES          = 2  # Number of bytes before the response payload (in stripped response)\n    NUMBER_OF_CRC_BYTES                    = 2\n    NUMBER_OF_LRC_BYTES                    = 1\n    BITNUMBER_FUNCTIONCODE_ERRORINDICATION = 7\n\n    MINIMAL_RESPONSE_LENGTH_RTU            = NUMBER_OF_RESPONSE_STARTBYTES + NUMBER_OF_CRC_BYTES\n    MINIMAL_RESPONSE_LENGTH_ASCII          = 9\n\n    # Argument validity testing\n    _checkString(response, description='response')\n    _checkSlaveaddress(slaveaddress)\n    _checkMode(mode)\n    _checkFunctioncode(functioncode, None)\n\n    plainresponse = response\n\n    # Validate response length\n    if mode == MODE_ASCII:\n        if len(response) < MINIMAL_RESPONSE_LENGTH_ASCII:\n            raise ValueError('Too short Modbus ASCII response (minimum length {} bytes). Response: {!r}'.format( \\\n                MINIMAL_RESPONSE_LENGTH_ASCII,\n                response))\n    elif len(response) < MINIMAL_RESPONSE_LENGTH_RTU:\n            raise ValueError('Too short Modbus RTU response (minimum length {} bytes). Response: {!r}'.format( \\\n                MINIMAL_RESPONSE_LENGTH_RTU,\n                response))\n\n    # Validate the ASCII header and footer.\n    if mode == MODE_ASCII:\n        if response[BYTEPOSITION_FOR_ASCII_HEADER] != _ASCII_HEADER:\n            raise ValueError('Did not find header ({!r}) as start of ASCII response. The plain response is: {!r}'.format( \\\n                _ASCII_HEADER,\n                response))\n        elif response[-len(_ASCII_FOOTER):] != _ASCII_FOOTER:\n            raise ValueError('Did not find footer ({!r}) as end of ASCII response. The plain response is: {!r}'.format( \\\n                _ASCII_FOOTER,\n                response))\n\n        # Strip ASCII header and footer\n        response = response[1:-2]\n\n        if len(response) % 2 != 0:\n            template = 'Stripped ASCII frames should have an even number of bytes, but is {} bytes. ' + \\\n                    'The stripped response is: {!r} (plain response: {!r})'\n            raise ValueError(template.format(len(response), response, plainresponse))\n\n        # Convert the ASCII (stripped) response string to RTU-like response string\n        response = _hexdecode(response)\n\n    # Validate response checksum\n    if mode == MODE_ASCII:\n        calculateChecksum = _calculateLrcString\n        numberOfChecksumBytes = NUMBER_OF_LRC_BYTES\n    else:\n        calculateChecksum = _calculateCrcString\n        numberOfChecksumBytes = NUMBER_OF_CRC_BYTES\n\n    receivedChecksum = response[-numberOfChecksumBytes:]\n    responseWithoutChecksum = response[0 : len(response) - numberOfChecksumBytes]\n    calculatedChecksum = calculateChecksum(responseWithoutChecksum)\n\n    if receivedChecksum != calculatedChecksum:\n        template = 'Checksum error in {} mode: {!r} instead of {!r} . The response is: {!r} (plain response: {!r})'\n        text = template.format(\n                mode,\n                receivedChecksum,\n                calculatedChecksum,\n                response, plainresponse)\n        raise ValueError(text)\n\n    # Check slave address\n    responseaddress = ord(response[BYTEPOSITION_FOR_SLAVEADDRESS])\n\n    if responseaddress != slaveaddress:\n        raise ValueError('Wrong return slave address: {} instead of {}. The response is: {!r}'.format( \\\n            responseaddress, slaveaddress, response))\n\n    # Check function code\n    receivedFunctioncode = ord(response[BYTEPOSITION_FOR_FUNCTIONCODE])\n\n    if receivedFunctioncode == _setBitOn(functioncode, BITNUMBER_FUNCTIONCODE_ERRORINDICATION):\n        raise ValueError('The slave is indicating an error. The response is: {!r}'.format(response))\n\n    elif receivedFunctioncode != functioncode:\n        raise ValueError('Wrong functioncode: {} instead of {}. The response is: {!r}'.format( \\\n            receivedFunctioncode, functioncode, response))\n\n    # Read data payload\n    firstDatabyteNumber = NUMBER_OF_RESPONSE_STARTBYTES\n\n    if mode == MODE_ASCII:\n        lastDatabyteNumber = len(response) - NUMBER_OF_LRC_BYTES\n    else:\n        lastDatabyteNumber = len(response) - NUMBER_OF_CRC_BYTES\n\n    payload = response[firstDatabyteNumber:lastDatabyteNumber]\n    return payload", "response": "Extracts the payload part from the slave s response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _predictResponseSize(mode, functioncode, payloadToSlave):\n    MIN_PAYLOAD_LENGTH = 4  # For implemented functioncodes here\n    BYTERANGE_FOR_GIVEN_SIZE = slice(2, 4)  # Within the payload\n\n    NUMBER_OF_PAYLOAD_BYTES_IN_WRITE_CONFIRMATION = 4\n    NUMBER_OF_PAYLOAD_BYTES_FOR_BYTECOUNTFIELD = 1\n\n    RTU_TO_ASCII_PAYLOAD_FACTOR = 2\n\n    NUMBER_OF_RTU_RESPONSE_STARTBYTES   = 2\n    NUMBER_OF_RTU_RESPONSE_ENDBYTES     = 2\n    NUMBER_OF_ASCII_RESPONSE_STARTBYTES = 5\n    NUMBER_OF_ASCII_RESPONSE_ENDBYTES   = 4\n\n    # Argument validity testing\n    _checkMode(mode)\n    _checkFunctioncode(functioncode, None)\n    _checkString(payloadToSlave, description='payload', minlength=MIN_PAYLOAD_LENGTH)\n\n    # Calculate payload size\n    if functioncode in [5, 6, 15, 16]:\n        response_payload_size = NUMBER_OF_PAYLOAD_BYTES_IN_WRITE_CONFIRMATION\n\n    elif functioncode in [1, 2, 3, 4]:\n        given_size = _twoByteStringToNum(payloadToSlave[BYTERANGE_FOR_GIVEN_SIZE])\n        if functioncode == 1 or functioncode == 2:\n            # Algorithm from MODBUS APPLICATION PROTOCOL SPECIFICATION V1.1b\n            number_of_inputs = given_size\n            response_payload_size = NUMBER_OF_PAYLOAD_BYTES_FOR_BYTECOUNTFIELD + \\\n                                    number_of_inputs // 8 + (1 if number_of_inputs % 8 else 0)\n\n        elif functioncode == 3 or functioncode == 4:\n            number_of_registers = given_size\n            response_payload_size = NUMBER_OF_PAYLOAD_BYTES_FOR_BYTECOUNTFIELD + \\\n                                    number_of_registers * _NUMBER_OF_BYTES_PER_REGISTER\n\n    else:\n        raise ValueError('Wrong functioncode: {}. The payload is: {!r}'.format( \\\n            functioncode, payloadToSlave))\n\n    # Calculate number of bytes to read\n    if mode == MODE_ASCII:\n        return NUMBER_OF_ASCII_RESPONSE_STARTBYTES + \\\n            response_payload_size * RTU_TO_ASCII_PAYLOAD_FACTOR + \\\n            NUMBER_OF_ASCII_RESPONSE_ENDBYTES\n    else:\n        return NUMBER_OF_RTU_RESPONSE_STARTBYTES + \\\n            response_payload_size + \\\n            NUMBER_OF_RTU_RESPONSE_ENDBYTES", "response": "Predicts the number of bytes that should be sent to the slave."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the minimum silent period length to comply with the 3. 5 character silence between messages.", "response": "def _calculate_minimum_silent_period(baudrate):\n    \"\"\"Calculate the silent period length to comply with the 3.5 character silence between messages.\n\n    Args:\n        baudrate (numerical): The baudrate for the serial port\n\n    Returns:\n        The number of seconds (float) that should pass between each message on the bus.\n\n    Raises:\n        ValueError, TypeError.\n\n    \"\"\"\n    _checkNumerical(baudrate, minvalue=1, description='baudrate')  # Avoid division by zero\n\n    BITTIMES_PER_CHARACTERTIME = 11\n    MINIMUM_SILENT_CHARACTERTIMES = 3.5\n\n    bittime = 1 / float(baudrate)\n    return bittime * BITTIMES_PER_CHARACTERTIME * MINIMUM_SILENT_CHARACTERTIMES"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _numToTwoByteString(value, numberOfDecimals=0, LsbFirst=False, signed=False):\n    _checkNumerical(value, description='inputvalue')\n    _checkInt(numberOfDecimals, minvalue=0, description='number of decimals')\n    _checkBool(LsbFirst, description='LsbFirst')\n    _checkBool(signed, description='signed parameter')\n\n    multiplier = 10 ** numberOfDecimals\n    integer = int(float(value) * multiplier)\n\n    if LsbFirst:\n        formatcode = '<'  # Little-endian\n    else:\n        formatcode = '>'  # Big-endian\n    if signed:\n        formatcode += 'h'  # (Signed) short (2 bytes)\n    else:\n        formatcode += 'H'  # Unsigned short (2 bytes)\n\n    outstring = _pack(formatcode, integer)\n    assert len(outstring) == 2\n    return outstring", "response": "Convert a numerical value to a two - byte string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _twoByteStringToNum(bytestring, numberOfDecimals=0, signed=False):\n    _checkString(bytestring, minlength=2, maxlength=2, description='bytestring')\n    _checkInt(numberOfDecimals, minvalue=0, description='number of decimals')\n    _checkBool(signed, description='signed parameter')\n\n    formatcode = '>'  # Big-endian\n    if signed:\n        formatcode += 'h'  # (Signed) short (2 bytes)\n    else:\n        formatcode += 'H'  # Unsigned short (2 bytes)\n\n    fullregister = _unpack(formatcode, bytestring)\n\n    if numberOfDecimals == 0:\n        return fullregister\n    divisor = 10 ** numberOfDecimals\n    return fullregister / float(divisor)", "response": "Convert a two - byte string to a numerical value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pack(formatstring, value):\n    _checkString(formatstring, description='formatstring', minlength=1)\n\n    try:\n        result = struct.pack(formatstring, value)\n    except:\n        errortext = 'The value to send is probably out of range, as the num-to-bytestring conversion failed.'\n        errortext += ' Value: {0!r} Struct format code is: {1}'\n        raise ValueError(errortext.format(value, formatstring))\n\n    if sys.version_info[0] > 2:\n        return str(result, encoding='latin1')  # Convert types to make it Python3 compatible\n    return result", "response": "Pack a value into a bytestring."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _unpack(formatstring, packed):\n    _checkString(formatstring, description='formatstring', minlength=1)\n    _checkString(packed, description='packed string', minlength=1)\n\n    if sys.version_info[0] > 2:\n        packed = bytes(packed, encoding='latin1')  # Convert types to make it Python3 compatible\n\n    try:\n        value = struct.unpack(formatstring, packed)[0]\n    except:\n        errortext = 'The received bytestring is probably wrong, as the bytestring-to-num conversion failed.'\n        errortext += ' Bytestring: {0!r} Struct format code is: {1}'\n        raise ValueError(errortext.format(packed, formatstring))\n\n    return value", "response": "Unpack a bytestring into a value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _hexencode(bytestring, insert_spaces = False):\n    _checkString(bytestring, description='byte string')\n\n    separator = '' if not insert_spaces else ' '\n    \n    # Use plain string formatting instead of binhex.hexlify,\n    # in order to have it Python 2.x and 3.x compatible\n\n    byte_representions = []\n    for c in bytestring:\n        byte_representions.append( '{0:02X}'.format(ord(c)) )\n    return separator.join(byte_representions).strip()", "response": "Convert a byte string to a hex encoded string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a hex encoded string to a byte string.", "response": "def _hexdecode(hexstring):\n    \"\"\"Convert a hex encoded string to a byte string.\n\n    For example '4A' will return 'J', and '04' will return ``'\\\\x04'`` (which has length 1).\n\n    Args:\n        hexstring (str): Can be for example 'A3' or 'A3B4'. Must be of even length.\n        Allowed characters are '0' to '9', 'a' to 'f' and 'A' to 'F' (not space).\n\n    Returns:\n        A string of half the length, with characters corresponding to all 0-255 values for each byte.\n\n    Raises:\n        TypeError, ValueError\n\n    \"\"\"\n    # Note: For Python3 the appropriate would be: raise TypeError(new_error_message) from err\n    # but the Python2 interpreter will indicate SyntaxError.\n    # Thus we need to live with this warning in Python3:\n    # 'During handling of the above exception, another exception occurred'\n\n    _checkString(hexstring, description='hexstring')\n\n    if len(hexstring) % 2 != 0:\n        raise ValueError('The input hexstring must be of even length. Given: {!r}'.format(hexstring))\n\n    if sys.version_info[0] > 2:\n        by = bytes(hexstring, 'latin1')\n        try:\n            return str(binascii.unhexlify(by), encoding='latin1')\n        except binascii.Error as err:\n            new_error_message = 'Hexdecode reported an error: {!s}. Input hexstring: {}'.format(err.args[0], hexstring)\n            raise TypeError(new_error_message)\n\n    else:\n        try:\n            return hexstring.decode('hex')\n        except TypeError as err:\n            raise TypeError('Hexdecode reported an error: {}. Input hexstring: {}'.format(err.message, hexstring))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a response string to a numerical value.", "response": "def _bitResponseToValue(bytestring):\n    \"\"\"Convert a response string to a numerical value.\n\n    Args:\n        bytestring (str): A string of length 1. Can be for example ``\\\\x01``.\n\n    Returns:\n        The converted value (int).\n\n    Raises:\n        TypeError, ValueError\n\n    \"\"\"\n    _checkString(bytestring, description='bytestring', minlength=1, maxlength=1)\n\n    RESPONSE_ON  = '\\x01'\n    RESPONSE_OFF = '\\x00'\n\n    if bytestring == RESPONSE_ON:\n        return 1\n    elif bytestring == RESPONSE_OFF:\n        return 0\n    else:\n        raise ValueError('Could not convert bit response to a value. Input: {0!r}'.format(bytestring))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the bit pattern that is used for writing single bits.", "response": "def _createBitpattern(functioncode, value):\n    \"\"\"Create the bit pattern that is used for writing single bits.\n\n    This is basically a storage of numerical constants.\n\n    Args:\n        * functioncode (int): can be 5 or 15\n        * value (int): can be 0 or 1\n\n    Returns:\n        The bit pattern (string).\n\n    Raises:\n        TypeError, ValueError\n\n    \"\"\"\n    _checkFunctioncode(functioncode, [5, 15])\n    _checkInt(value, minvalue=0, maxvalue=1, description='inputvalue')\n\n    if functioncode == 5:\n        if value == 0:\n            return '\\x00\\x00'\n        else:\n            return '\\xff\\x00'\n\n    elif functioncode == 15:\n        if value == 0:\n            return '\\x00'\n        else:\n            return '\\x01'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _twosComplement(x, bits=16):\n    _checkInt(bits, minvalue=0, description='number of bits')\n    _checkInt(x, description='input')\n    upperlimit = 2 ** (bits - 1) - 1\n    lowerlimit = -2 ** (bits - 1)\n    if x > upperlimit or x < lowerlimit:\n        raise ValueError('The input value is out of range. Given value is {0}, but allowed range is {1} to {2} when using {3} bits.' \\\n            .format(x, lowerlimit, upperlimit, bits))\n\n    # Calculate two'2 complement\n    if x >= 0:\n        return x\n    return x + 2 ** bits", "response": "Calculate the two s complement of an integer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an integer that represents the inverse of a two s complement of an integer.", "response": "def _fromTwosComplement(x, bits=16):\n    \"\"\"Calculate the inverse(?) of a two's complement of an integer.\n\n    Args:\n        * x (int): input integer.\n        * bits (int): number of bits, must be > 0.\n\n    Returns:\n        An int, that represents the inverse(?) of two's complement of the input.\n\n    Example for bits=8:\n\n    === =======\n    x   returns\n    === =======\n    0   0\n    1   1\n    127 127\n    128 -128\n    129 -127\n    255 -1\n    === =======\n\n    \"\"\"\n    _checkInt(bits, minvalue=0, description='number of bits')\n\n    _checkInt(x, description='input')\n    upperlimit = 2 ** (bits) - 1\n    lowerlimit = 0\n    if x > upperlimit or x < lowerlimit:\n        raise ValueError('The input value is out of range. Given value is {0}, but allowed range is {1} to {2} when using {3} bits.' \\\n            .format(x, lowerlimit, upperlimit, bits))\n\n    # Calculate inverse(?) of two'2 complement\n    limit = 2 ** (bits - 1) - 1\n    if x <= limit:\n        return x\n    return x - 2 ** bits"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the bit bitNum to True.", "response": "def _setBitOn(x, bitNum):\n    \"\"\"Set bit 'bitNum' to True.\n\n    Args:\n        * x (int): The value before.\n        * bitNum (int): The bit number that should be set to True.\n\n    Returns:\n        The value after setting the bit. This is an integer.\n\n    For example:\n        For x = 4 (dec) = 0100 (bin), setting bit number 0 results in 0101 (bin) = 5 (dec).\n\n    \"\"\"\n    _checkInt(x, minvalue=0, description='input value')\n    _checkInt(bitNum, minvalue=0, description='bitnumber')\n\n    return x | (1 << bitNum)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the CRC - 16 for Modbus.", "response": "def _calculateCrcString(inputstring):\n    \"\"\"Calculate CRC-16 for Modbus.\n\n    Args:\n        inputstring (str): An arbitrary-length message (without the CRC).\n\n    Returns:\n        A two-byte CRC string, where the least significant byte is first.\n\n    \"\"\"\n    _checkString(inputstring, description='input CRC string')\n \n    # Preload a 16-bit register with ones\n    register = 0xFFFF\n\n    for char in inputstring:\n        register = (register >> 8) ^ _CRC16TABLE[(register ^ ord(char)) & 0xFF]\n \n    return _numToTwoByteString(register, LsbFirst=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the LRC for Modbus.", "response": "def _calculateLrcString(inputstring):\n    \"\"\"Calculate LRC for Modbus.\n\n    Args:\n        inputstring (str): An arbitrary-length message (without the beginning\n        colon and terminating CRLF). It should already be decoded from hex-string.\n\n    Returns:\n        A one-byte LRC bytestring (not encoded to hex-string)\n\n    Algorithm from the document 'MODBUS over serial line specification and implementation guide V1.02'.\n\n    The LRC is calculated as 8 bits (one byte).\n\n    For example a LRC 0110 0001 (bin) = 61 (hex) = 97 (dec) = 'a'. This function will\n    then return 'a'.\n\n    In Modbus ASCII mode, this should be transmitted using two characters. This\n    example should be transmitted '61', which is a string of length two. This function\n    does not handle that conversion for transmission.\n    \"\"\"\n    _checkString(inputstring, description='input LRC string')\n\n    register = 0\n    for character in inputstring:\n        register += ord(character)\n\n    lrc = ((register ^ 0xFF) + 1) & 0xFF\n\n    lrcString = _numToOneByteString(lrc)\n    return lrcString"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the Modbus mode is valie.", "response": "def _checkMode(mode):\n    \"\"\"Check that the Modbus mode is valie.\n\n    Args:\n        mode (string): The Modbus mode (MODE_RTU or MODE_ASCII)\n\n    Raises:\n        TypeError, ValueError\n\n    \"\"\"\n\n    if not isinstance(mode, str):\n        raise TypeError('The {0} should be a string. Given: {1!r}'.format(\"mode\", mode))\n\n    if mode not in [MODE_RTU, MODE_ASCII]:\n        raise ValueError(\"Unreconized Modbus mode given. Must be 'rtu' or 'ascii' but {0!r} was given.\".format(mode))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the given functioncode is in the listOfAllowedValues list.", "response": "def _checkFunctioncode(functioncode, listOfAllowedValues=[]):\n    \"\"\"Check that the given functioncode is in the listOfAllowedValues.\n\n    Also verifies that 1 <= function code <= 127.\n\n    Args:\n        * functioncode (int): The function code\n        * listOfAllowedValues (list of int): Allowed values. Use *None* to bypass this part of the checking.\n\n    Raises:\n        TypeError, ValueError\n\n    \"\"\"\n    FUNCTIONCODE_MIN = 1\n    FUNCTIONCODE_MAX = 127\n\n    _checkInt(functioncode, FUNCTIONCODE_MIN, FUNCTIONCODE_MAX, description='functioncode')\n\n    if listOfAllowedValues is None:\n        return\n\n    if not isinstance(listOfAllowedValues, list):\n        raise TypeError('The listOfAllowedValues should be a list. Given: {0!r}'.format(listOfAllowedValues))\n\n    for value in listOfAllowedValues:\n        _checkInt(value, FUNCTIONCODE_MIN, FUNCTIONCODE_MAX, description='functioncode inside listOfAllowedValues')\n\n    if functioncode not in listOfAllowedValues:\n        raise ValueError('Wrong function code: {0}, allowed values are {1!r}'.format(functioncode, listOfAllowedValues))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _checkResponseByteCount(payload):\n    POSITION_FOR_GIVEN_NUMBER = 0\n    NUMBER_OF_BYTES_TO_SKIP = 1\n\n    _checkString(payload, minlength=1, description='payload')\n\n    givenNumberOfDatabytes = ord(payload[POSITION_FOR_GIVEN_NUMBER])\n    countedNumberOfDatabytes = len(payload) - NUMBER_OF_BYTES_TO_SKIP\n\n    if givenNumberOfDatabytes != countedNumberOfDatabytes:\n        errortemplate = 'Wrong given number of bytes in the response: {0}, but counted is {1} as data payload length is {2}.' + \\\n            ' The data payload is: {3!r}'\n        errortext = errortemplate.format(givenNumberOfDatabytes, countedNumberOfDatabytes, len(payload), payload)\n        raise ValueError(errortext)", "response": "Checks that the number of bytes in the response is correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that the start adress as given in the response is correct.", "response": "def _checkResponseRegisterAddress(payload, registeraddress):\n    \"\"\"Check that the start adress as given in the response is correct.\n\n    The first two bytes in the payload holds the address value.\n\n    Args:\n        * payload (string): The payload\n        * registeraddress (int): The register address (use decimal numbers, not hex).\n\n    Raises:\n        TypeError, ValueError\n\n    \"\"\"\n    _checkString(payload, minlength=2, description='payload')\n    _checkRegisteraddress(registeraddress)\n\n    BYTERANGE_FOR_STARTADDRESS = slice(0, 2)\n\n    bytesForStartAddress = payload[BYTERANGE_FOR_STARTADDRESS]\n    receivedStartAddress = _twoByteStringToNum(bytesForStartAddress)\n\n    if receivedStartAddress != registeraddress:\n        raise ValueError('Wrong given write start adress: {0}, but commanded is {1}. The data payload is: {2!r}'.format( \\\n            receivedStartAddress, registeraddress, payload))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _checkResponseNumberOfRegisters(payload, numberOfRegisters):\n    _checkString(payload, minlength=4, description='payload')\n    _checkInt(numberOfRegisters, minvalue=1, maxvalue=0xFFFF, description='numberOfRegisters')\n\n    BYTERANGE_FOR_NUMBER_OF_REGISTERS = slice(2, 4)\n\n    bytesForNumberOfRegisters = payload[BYTERANGE_FOR_NUMBER_OF_REGISTERS]\n    receivedNumberOfWrittenReisters = _twoByteStringToNum(bytesForNumberOfRegisters)\n\n    if receivedNumberOfWrittenReisters != numberOfRegisters:\n        raise ValueError('Wrong number of registers to write in the response: {0}, but commanded is {1}. The data payload is: {2!r}'.format( \\\n            receivedNumberOfWrittenReisters, numberOfRegisters, payload))", "response": "Check that the number of registers in the response is correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _checkResponseWriteData(payload, writedata):\n    _checkString(payload, minlength=4, description='payload')\n    _checkString(writedata, minlength=2, maxlength=2, description='writedata')\n\n    BYTERANGE_FOR_WRITEDATA = slice(2, 4)\n\n    receivedWritedata = payload[BYTERANGE_FOR_WRITEDATA]\n\n    if receivedWritedata != writedata:\n        raise ValueError('Wrong write data in the response: {0!r}, but commanded is {1!r}. The data payload is: {2!r}'.format( \\\n            receivedWritedata, writedata, payload))", "response": "Check that the write data as given in the response is correct."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the given string is valid.", "response": "def _checkString(inputstring, description, minlength=0, maxlength=None):\n    \"\"\"Check that the given string is valid.\n\n    Args:\n        * inputstring (string): The string to be checked\n        * description (string): Used in error messages for the checked inputstring\n        * minlength (int): Minimum length of the string\n        * maxlength (int or None): Maximum length of the string\n\n    Raises:\n        TypeError, ValueError\n\n    Uses the function :func:`_checkInt` internally.\n\n    \"\"\"\n    # Type checking\n    if not isinstance(description, str):\n        raise TypeError('The description should be a string. Given: {0!r}'.format(description))\n\n    if not isinstance(inputstring, str):\n        raise TypeError('The {0} should be a string. Given: {1!r}'.format(description, inputstring))\n\n    if not isinstance(maxlength, (int, type(None))):\n        raise TypeError('The maxlength must be an integer or None. Given: {0!r}'.format(maxlength))\n\n    # Check values\n    _checkInt(minlength, minvalue=0, maxvalue=None, description='minlength')\n\n    if len(inputstring) < minlength:\n        raise ValueError('The {0} is too short: {1}, but minimum value is {2}. Given: {3!r}'.format( \\\n            description, len(inputstring), minlength, inputstring))\n\n    if not maxlength is None:\n        if maxlength < 0:\n            raise ValueError('The maxlength must be positive. Given: {0}'.format(maxlength))\n\n        if maxlength < minlength:\n            raise ValueError('The maxlength must not be smaller than minlength. Given: {0} and {1}'.format( \\\n                maxlength, minlength))\n\n        if len(inputstring) > maxlength:\n            raise ValueError('The {0} is too long: {1}, but maximum value is {2}. Given: {3!r}'.format( \\\n                description, len(inputstring), maxlength, inputstring))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the given integer is valid.", "response": "def _checkInt(inputvalue, minvalue=None, maxvalue=None, description='inputvalue'):\n    \"\"\"Check that the given integer is valid.\n\n    Args:\n        * inputvalue (int or long): The integer to be checked\n        * minvalue (int or long, or None): Minimum value of the integer\n        * maxvalue (int or long, or None): Maximum value of the integer\n        * description (string): Used in error messages for the checked inputvalue\n\n    Raises:\n        TypeError, ValueError\n\n    Note: Can not use the function :func:`_checkString`, as that function uses this function internally.\n\n    \"\"\"\n    if not isinstance(description, str):\n        raise TypeError('The description should be a string. Given: {0!r}'.format(description))\n\n    if not isinstance(inputvalue, (int, long)):\n        raise TypeError('The {0} must be an integer. Given: {1!r}'.format(description, inputvalue))\n\n    if not isinstance(minvalue, (int, long, type(None))):\n        raise TypeError('The minvalue must be an integer or None. Given: {0!r}'.format(minvalue))\n\n    if not isinstance(maxvalue, (int, long, type(None))):\n        raise TypeError('The maxvalue must be an integer or None. Given: {0!r}'.format(maxvalue))\n\n    _checkNumerical(inputvalue, minvalue, maxvalue, description)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _checkNumerical(inputvalue, minvalue=None, maxvalue=None, description='inputvalue'):\n    # Type checking\n    if not isinstance(description, str):\n        raise TypeError('The description should be a string. Given: {0!r}'.format(description))\n\n    if not isinstance(inputvalue, (int, long, float)):\n        raise TypeError('The {0} must be numerical. Given: {1!r}'.format(description, inputvalue))\n\n    if not isinstance(minvalue, (int, float, long, type(None))):\n        raise TypeError('The minvalue must be numeric or None. Given: {0!r}'.format(minvalue))\n\n    if not isinstance(maxvalue, (int, float, long, type(None))):\n        raise TypeError('The maxvalue must be numeric or None. Given: {0!r}'.format(maxvalue))\n\n    # Consistency checking\n    if (not minvalue is None) and (not maxvalue is None):\n        if maxvalue < minvalue:\n            raise ValueError('The maxvalue must not be smaller than minvalue. Given: {0} and {1}, respectively.'.format( \\\n                maxvalue, minvalue))\n\n    # Value checking\n    if not minvalue is None:\n        if inputvalue < minvalue:\n            raise ValueError('The {0} is too small: {1}, but minimum value is {2}.'.format( \\\n                description, inputvalue, minvalue))\n\n    if not maxvalue is None:\n        if inputvalue > maxvalue:\n            raise ValueError('The {0} is too large: {1}, but maximum value is {2}.'.format( \\\n                description, inputvalue, maxvalue))", "response": "Checks that the given numerical value is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the given inputvalue is a boolean.", "response": "def _checkBool(inputvalue, description='inputvalue'):\n    \"\"\"Check that the given inputvalue is a boolean.\n\n    Args:\n        * inputvalue (boolean): The value to be checked.\n        * description (string): Used in error messages for the checked inputvalue.\n\n    Raises:\n        TypeError, ValueError\n\n    \"\"\"\n    _checkString(description, minlength=1, description='description string')\n    if not isinstance(inputvalue, bool):\n        raise TypeError('The {0} must be boolean. Given: {1!r}'.format(description, inputvalue))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _interpretRawMessage(inputstr):\n    raise NotImplementedError()\n    output = ''\n    output += 'Modbus bytestring decoder\\n'\n    output += 'Input string (length {} characters): {!r} \\n'.format(len(inputstr), inputstr)\n\n    # Detect modbus type\n    if inputstr.startswith(_ASCII_HEADER) and inputstr.endswith(_ASCII_FOOTER):\n        mode = MODE_ASCII\n    else:\n        mode = MODE_RTU\n    output += 'Probably Modbus {} mode.\\n'.format(mode.upper())\n\n    # Extract slave address and function code\n    try:\n        if mode == MODE_ASCII:\n            slaveaddress = int(inputstr[1:3])\n            functioncode = int(inputstr[3:5])\n        else:\n            slaveaddress = ord(inputstr[0])\n            functioncode = ord(inputstr[1])\n        output += 'Slave address: {} (dec). Function code: {} (dec).\\n'.format(slaveaddress, functioncode)\n    except:\n        output += '\\nCould not extract slave address and function code. \\n\\n'\n\n    # Check message validity\n    try:\n        extractedpayload = _extractPayload(inputstr, slaveaddress, mode, functioncode)\n        output += 'Valid message. Extracted payload: {!r}\\n'.format(extractedpayload)\n    except (ValueError, TypeError) as err:\n        output += '\\nThe message does not seem to be valid Modbus {}. Error message: \\n{}. \\n\\n'.format(mode.upper(), err.message)\n    except NameError as err:\n        output += '\\nNo message validity checking. \\n\\n' # Slave address or function code not available\n\n    # Generate table describing the message\n    if mode == MODE_RTU:\n        output += '\\nPos   Character Hex  Dec  Probable interpretation \\n'\n        output += '------------------------------------------------- \\n'\n        for i, character in enumerate(inputstr):\n            if i==0:\n                description = 'Slave address'\n            elif i==1:\n                description = 'Function code'\n            elif i==len(inputstr)-2:\n                description = 'Checksum, CRC LSB'\n            elif i==len(inputstr)-1:\n                description = 'Checksum, CRC MSB'\n            else:\n                description = 'Payload'\n            output += '{0:3.0f}:  {1!r:<8}  {2:02X}  {2: 4.0f}  {3:<10} \\n'.format(i, character, ord(character), description)\n        \n    elif mode == MODE_ASCII:\n        output += '\\nPos   Character(s) Converted  Hex  Dec  Probable interpretation \\n'\n        output += '--------------------------------------------------------------- \\n'\n        \n        i = 0\n        while i < len(inputstr):\n            \n            if inputstr[i] in [':', '\\r', '\\n']:\n                if inputstr[i] == ':': \n                    description = 'Start character'\n                else:\n                    description = 'Stop character'\n                    \n                output += '{0:3.0f}:  {1!r:<8}                          {2} \\n'.format(i, inputstr[i], description)\n                i += 1\n                \n            else:\n                if i == 1:\n                    description = 'Slave address'\n                elif i == 3:\n                    description = 'Function code'\n                elif i == len(inputstr)-4:\n                    description = 'Checksum (LRC)'\n                else:\n                    description = 'Payload'\n                \n                try:\n                    hexvalue = _hexdecode(inputstr[i:i+2])\n                    output +=  '{0:3.0f}:  {1!r:<8}     {2!r}     {3:02X}  {3: 4.0f}  {4} \\n'.format(i, inputstr[i:i+2], hexvalue, ord(hexvalue), description)\n                except:\n                    output +=  '{0:3.0f}:  {1!r:<8}     ?           ?     ?  {2} \\n'.format(i, inputstr[i:i+2], description)\n                i += 2\n        \n    # Generate description for the payload\n    output += '\\n\\n'\n    try:\n        output += _interpretPayload(functioncode, extractedpayload)\n    except:\n        output += '\\nCould not interpret the payload. \\n\\n' # Payload or function code not available\n    \n    return output", "response": "r Returns a human readable string of a Modbus bytestring."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _interpretPayload(functioncode, payload):\n    raise NotImplementedError()\n    output = ''\n    output += 'Modbus payload decoder\\n'\n    output += 'Input payload (length {} characters): {!r} \\n'.format(len(payload), payload)\n    output += 'Function code: {} (dec).\\n'.format(functioncode)\n    \n    if len(payload) == 4:\n        FourbyteMessageFirstHalfValue = _twoByteStringToNum(payload[0:2])\n        FourbyteMessageSecondHalfValue = _twoByteStringToNum(payload[2:4])\n\n\n    return output", "response": "r Returns a human readable string of a Modbus payload."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getDiagnosticString():\n    text = '\\n## Diagnostic output from minimalmodbus ## \\n\\n'\n    text += 'Minimalmodbus version: ' + __version__ + '\\n'\n    text += 'Minimalmodbus status: ' + __status__ + '\\n'\n    text += 'File name (with relative path): ' + __file__ + '\\n'\n    text += 'Full file path: ' + os.path.abspath(__file__) + '\\n\\n'\n    text += 'pySerial version: ' + serial.VERSION + '\\n'\n    text += 'pySerial full file path: ' + os.path.abspath(serial.__file__) + '\\n\\n'\n    text += 'Platform: ' + sys.platform + '\\n'\n    text += 'Filesystem encoding: ' + repr(sys.getfilesystemencoding()) + '\\n'\n    text += 'Byteorder: ' + sys.byteorder + '\\n'\n    text += 'Python version: ' + sys.version + '\\n'\n    text += 'Python version info: ' + repr(sys.version_info) + '\\n'\n    text += 'Python flags: ' + repr(sys.flags) + '\\n'\n    text += 'Python argv: ' + repr(sys.argv) + '\\n'\n    text += 'Python prefix: ' + repr(sys.prefix) + '\\n'\n    text += 'Python exec prefix: ' + repr(sys.exec_prefix) + '\\n'\n    text += 'Python executable: ' + repr(sys.executable) + '\\n'\n    try:\n        text += 'Long info: ' + repr(sys.long_info) + '\\n'\n    except:\n        text += 'Long info: (none)\\n'  # For Python3 compatibility\n    try:\n        text += 'Float repr style: ' + repr(sys.float_repr_style) + '\\n\\n'\n    except:\n        text += 'Float repr style: (none) \\n\\n'  # For Python 2.6 compatibility\n    text += 'Variable __name__: ' + __name__ + '\\n'\n    text += 'Current directory: ' + os.getcwd() + '\\n\\n'\n    text += 'Python path: \\n'\n    text += '\\n'.join(sys.path) + '\\n'\n    text += '\\n## End of diagnostic output ## \\n'\n    return text", "response": "Generate a diagnostic string from the minimalmodbus module version the platform current directory etc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_bit(self, registeraddress, functioncode=2):\n        _checkFunctioncode(functioncode, [1, 2])\n        return self._genericCommand(functioncode, registeraddress)", "response": "Read one bit from the slave."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_bit(self, registeraddress, value, functioncode=5):\n        _checkFunctioncode(functioncode, [5, 15])\n        _checkInt(value, minvalue=0, maxvalue=1, description='input value')\n        self._genericCommand(functioncode, registeraddress, value)", "response": "Write one bit to the slave."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_register(self, registeraddress, numberOfDecimals=0, functioncode=3, signed=False):\n        _checkFunctioncode(functioncode, [3, 4])\n        _checkInt(numberOfDecimals, minvalue=0, maxvalue=10, description='number of decimals')\n        _checkBool(signed, description='signed')\n        return self._genericCommand(functioncode, registeraddress, numberOfDecimals=numberOfDecimals, signed=signed)", "response": "Read an integer from one 16 - bit register in the slave."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting an integer to one 16 - bit register in the slave.", "response": "def write_register(self, registeraddress, value, numberOfDecimals=0, functioncode=16, signed=False):\n        \"\"\"Write an integer to one 16-bit register in the slave, possibly scaling it.\n\n        The slave register can hold integer values in the range 0 to 65535 (\"Unsigned INT16\").\n\n        Args:\n            * registeraddress (int): The slave register address  (use decimal numbers, not hex).\n            * value (int or float): The value to store in the slave register (might be scaled before sending).\n            * numberOfDecimals (int): The number of decimals for content conversion.\n            * functioncode (int): Modbus function code. Can be 6 or 16.\n            * signed (bool): Whether the data should be interpreted as unsigned or signed.\n\n        To store for example ``value=77.0``, use ``numberOfDecimals=1`` if the slave register will hold it as 770 internally.\n        This will multiply ``value`` by 10 before sending it to the slave register.\n\n        Similarly ``numberOfDecimals=2`` will multiply ``value`` by 100 before sending it to the slave register.\n\n        For discussion on negative values, the range and on alternative names, see :meth:`.read_register`.\n\n        Use the parameter ``signed=True`` if writing to a register that can hold\n        negative values. Then negative input will be automatically converted into\n        upper range data (two's complement).\n\n        Returns:\n            None\n\n        Raises:\n            ValueError, TypeError, IOError\n\n        \"\"\"\n        _checkFunctioncode(functioncode, [6, 16])\n        _checkInt(numberOfDecimals, minvalue=0, maxvalue=10, description='number of decimals')\n        _checkBool(signed, description='signed')\n        _checkNumerical(value, description='input value')\n\n        self._genericCommand(functioncode, registeraddress, value, numberOfDecimals, signed=signed)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a long integer from the slave.", "response": "def read_long(self, registeraddress, functioncode=3, signed=False):\n        \"\"\"Read a long integer (32 bits) from the slave.\n\n        Long integers (32 bits = 4 bytes) are stored in two consecutive 16-bit registers in the slave.\n\n        Args:\n            * registeraddress (int): The slave register start address (use decimal numbers, not hex).\n            * functioncode (int): Modbus function code. Can be 3 or 4.\n            * signed (bool): Whether the data should be interpreted as unsigned or signed.\n\n        ============== ================== ================ ==========================\n        ``signed``     Data type in slave Alternative name Range\n        ============== ================== ================ ==========================\n        :const:`False` Unsigned INT32     Unsigned long    0 to 4294967295\n        :const:`True`  INT32              Long             -2147483648 to 2147483647\n        ============== ================== ================ ==========================\n\n        Returns:\n            The numerical value (int).\n\n        Raises:\n            ValueError, TypeError, IOError\n\n        \"\"\"\n        _checkFunctioncode(functioncode, [3, 4])\n        _checkBool(signed, description='signed')\n        return self._genericCommand(functioncode, registeraddress, numberOfRegisters=2, signed=signed, payloadformat='long')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_long(self, registeraddress, value, signed=False):\n        MAX_VALUE_LONG =  4294967295  # Unsigned INT32\n        MIN_VALUE_LONG = -2147483648  # INT32\n\n        _checkInt(value, minvalue=MIN_VALUE_LONG, maxvalue=MAX_VALUE_LONG, description='input value')\n        _checkBool(signed, description='signed')\n        self._genericCommand(16, registeraddress, value, numberOfRegisters=2, signed=signed, payloadformat='long')", "response": "Write a 32 - bit long integer to the slave."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a float from the Modbus master.", "response": "def read_float(self, registeraddress, functioncode=3, numberOfRegisters=2):\n        \"\"\"Read a floating point number from the slave.\n\n        Floats are stored in two or more consecutive 16-bit registers in the slave. The\n        encoding is according to the standard IEEE 754.\n\n        There are differences in the byte order used by different manufacturers. A floating\n        point value of 1.0 is encoded (in single precision) as 3f800000 (hex). In this\n        implementation the data will be sent as ``'\\\\x3f\\\\x80'`` and ``'\\\\x00\\\\x00'``\n        to two consecutetive registers . Make sure to test that it makes sense for your instrument.\n        It is pretty straight-forward to change this code if some other byte order is\n        required by anyone (see support section).\n\n        Args:\n            * registeraddress (int): The slave register start address (use decimal numbers, not hex).\n            * functioncode (int): Modbus function code. Can be 3 or 4.\n            * numberOfRegisters (int): The number of registers allocated for the float. Can be 2 or 4.\n\n        ====================================== ================= =========== =================\n        Type of floating point number in slave Size              Registers   Range\n        ====================================== ================= =========== =================\n        Single precision (binary32)            32 bits (4 bytes) 2 registers 1.4E-45 to 3.4E38\n        Double precision (binary64)            64 bits (8 bytes) 4 registers 5E-324 to 1.8E308\n        ====================================== ================= =========== =================\n\n        Returns:\n            The numerical value (float).\n\n        Raises:\n            ValueError, TypeError, IOError\n\n        \"\"\"\n        _checkFunctioncode(functioncode, [3, 4])\n        _checkInt(numberOfRegisters, minvalue=2, maxvalue=4, description='number of registers')\n        return self._genericCommand(functioncode, registeraddress, numberOfRegisters=numberOfRegisters, payloadformat='float')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a floating point number to the slave.", "response": "def write_float(self, registeraddress, value, numberOfRegisters=2):\n        \"\"\"Write a floating point number to the slave.\n\n        Floats are stored in two or more consecutive 16-bit registers in the slave.\n\n        Uses Modbus function code 16.\n\n        For discussion on precision, number of registers and on byte order, see :meth:`.read_float`.\n\n        Args:\n            * registeraddress (int): The slave register start address (use decimal numbers, not hex).\n            * value (float or int): The value to store in the slave\n            * numberOfRegisters (int): The number of registers allocated for the float. Can be 2 or 4.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError, TypeError, IOError\n\n        \"\"\"\n        _checkNumerical(value, description='input value')\n        _checkInt(numberOfRegisters, minvalue=2, maxvalue=4, description='number of registers')\n        self._genericCommand(16, registeraddress, value, \\\n            numberOfRegisters=numberOfRegisters, payloadformat='float')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_string(self, registeraddress, numberOfRegisters=16, functioncode=3):\n        _checkFunctioncode(functioncode, [3, 4])\n        _checkInt(numberOfRegisters, minvalue=1, description='number of registers for read string')\n        return self._genericCommand(functioncode, registeraddress, \\\n            numberOfRegisters=numberOfRegisters, payloadformat='string')", "response": "Read a string from the slave."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_string(self, registeraddress, textstring, numberOfRegisters=16):\n        _checkInt(numberOfRegisters, minvalue=1, description='number of registers for write string')\n        _checkString(textstring, 'input string', minlength=1, maxlength=2 * numberOfRegisters)\n        self._genericCommand(16, registeraddress, textstring, \\\n            numberOfRegisters=numberOfRegisters, payloadformat='string')", "response": "Write a string to the slave."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite integers to 16 - bit registers in the slave.", "response": "def write_registers(self, registeraddress, values):\n        \"\"\"Write integers to 16-bit registers in the slave.\n\n        The slave register can hold integer values in the range 0 to 65535 (\"Unsigned INT16\").\n\n        Uses Modbus function code 16.\n\n        The number of registers that will be written is defined by the length of the ``values`` list.\n\n        Args:\n            * registeraddress (int): The slave register start address (use decimal numbers, not hex).\n            * values (list of int): The values to store in the slave registers.\n\n        Any scaling of the register data, or converting it to negative number (two's complement)\n        must be done manually.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError, TypeError, IOError\n\n        \"\"\"\n        if not isinstance(values, list):\n            raise TypeError('The \"values parameter\" must be a list. Given: {0!r}'.format(values))\n        _checkInt(len(values), minvalue=1, description='length of input list')\n        # Note: The content of the list is checked at content conversion.\n\n        self._genericCommand(16, registeraddress, values, numberOfRegisters=len(values), payloadformat='registers')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _performCommand(self, functioncode, payloadToSlave):\n        DEFAULT_NUMBER_OF_BYTES_TO_READ = 1000\n\n        _checkFunctioncode(functioncode, None)\n        _checkString(payloadToSlave, description='payload')\n\n        # Build request\n        request = _embedPayload(self.address, self.mode, functioncode, payloadToSlave)\n\n        # Calculate number of bytes to read\n        number_of_bytes_to_read = DEFAULT_NUMBER_OF_BYTES_TO_READ\n        if self.precalculate_read_size:\n            try:\n                number_of_bytes_to_read = _predictResponseSize(self.mode, functioncode, payloadToSlave)\n            except:\n                if self.debug:\n                    template = 'MinimalModbus debug mode. Could not precalculate response size for Modbus {} mode. ' + \\\n                        'Will read {} bytes. request: {!r}'\n                    _print_out(template.format(self.mode, number_of_bytes_to_read, request))\n\n        # Communicate\n        response = self._communicate(request, number_of_bytes_to_read)\n\n        # Extract payload\n        payloadFromSlave = _extractPayload(response, self.address, self.mode, functioncode)\n        return payloadFromSlave", "response": "Performs the command having the given functioncode."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntalks to the slave via a serial port.", "response": "def _communicate(self, request, number_of_bytes_to_read):\n        \"\"\"Talk to the slave via a serial port.\n\n        Args:\n            request (str): The raw request that is to be sent to the slave.\n            number_of_bytes_to_read (int): number of bytes to read\n\n        Returns:\n            The raw data (string) returned from the slave.\n\n        Raises:\n            TypeError, ValueError, IOError\n\n        Note that the answer might have strange ASCII control signs, which\n        makes it difficult to print it in the promt (messes up a bit).\n        Use repr() to make the string printable (shows ASCII values for control signs.)\n\n        Will block until reaching *number_of_bytes_to_read* or timeout.\n\n        If the attribute :attr:`Instrument.debug` is :const:`True`, the communication details are printed.\n\n        If the attribute :attr:`Instrument.close_port_after_each_call` is :const:`True` the\n        serial port is closed after each call.\n\n        Timing::\n\n                                                  Request from master (Master is writing)\n                                                  |\n                                                  |       Response from slave (Master is reading)\n                                                  |       |\n            ----W----R----------------------------W-------R----------------------------------------\n                     |                            |       |\n                     |<----- Silent period ------>|       |\n                                                  |       |\n                             Roundtrip time  ---->|-------|<--\n\n        The resolution for Python's time.time() is lower on Windows than on Linux.\n        It is about 16 ms on Windows according to\n        http://stackoverflow.com/questions/157359/accurate-timestamping-in-python\n\n        For Python3, the information sent to and from pySerial should be of the type bytes.\n        This is taken care of automatically by MinimalModbus.\n        \n        \n\n        \"\"\"\n\n        _checkString(request, minlength=1, description='request')\n        _checkInt(number_of_bytes_to_read)\n\n        if self.debug:\n            _print_out('\\nMinimalModbus debug mode. Writing to instrument (expecting {} bytes back): {!r} ({})'. \\\n                format(number_of_bytes_to_read, request, _hexlify(request)))\n\n        if self.close_port_after_each_call:\n            self.serial.open()\n\n        #self.serial.flushInput() TODO\n\n        if sys.version_info[0] > 2:\n            request = bytes(request, encoding='latin1')  # Convert types to make it Python3 compatible\n\n        # Sleep to make sure 3.5 character times have passed\n        minimum_silent_period   = _calculate_minimum_silent_period(self.serial.baudrate)\n        time_since_read         = time.time() - _LATEST_READ_TIMES.get(self.serial.port, 0)\n\n        if time_since_read < minimum_silent_period:\n            sleep_time = minimum_silent_period - time_since_read\n\n            if self.debug:\n                template = 'MinimalModbus debug mode. Sleeping for {:.1f} ms. ' + \\\n                        'Minimum silent period: {:.1f} ms, time since read: {:.1f} ms.'\n                text = template.format(\n                    sleep_time * _SECONDS_TO_MILLISECONDS,\n                    minimum_silent_period * _SECONDS_TO_MILLISECONDS,\n                    time_since_read * _SECONDS_TO_MILLISECONDS)\n                _print_out(text)\n\n            time.sleep(sleep_time)\n\n        elif self.debug:\n            template = 'MinimalModbus debug mode. No sleep required before write. ' + \\\n                'Time since previous read: {:.1f} ms, minimum silent period: {:.2f} ms.'\n            text = template.format(\n                time_since_read * _SECONDS_TO_MILLISECONDS,\n                minimum_silent_period * _SECONDS_TO_MILLISECONDS)\n            _print_out(text)\n\n        # Write request\n        latest_write_time = time.time()\n        \n        self.serial.write(request)\n\n        # Read and discard local echo\n        if self.handle_local_echo:\n            localEchoToDiscard = self.serial.read(len(request))\n            if self.debug:\n                template = 'MinimalModbus debug mode. Discarding this local echo: {!r} ({} bytes).' \n                text = template.format(localEchoToDiscard, len(localEchoToDiscard))\n                _print_out(text)\n            if localEchoToDiscard != request:\n                template = 'Local echo handling is enabled, but the local echo does not match the sent request. ' + \\\n                    'Request: {!r} ({} bytes), local echo: {!r} ({} bytes).' \n                text = template.format(request, len(request), localEchoToDiscard, len(localEchoToDiscard))\n                raise IOError(text)\n\n        # Read response\n        answer = self.serial.read(number_of_bytes_to_read)\n        _LATEST_READ_TIMES[self.serial.port] = time.time()\n\n        if self.close_port_after_each_call:\n            self.serial.close()\n\n        if sys.version_info[0] > 2:\n            answer = str(answer, encoding='latin1')  # Convert types to make it Python3 compatible\n\n        if self.debug:\n            template = 'MinimalModbus debug mode. Response from instrument: {!r} ({}) ({} bytes), ' + \\\n                'roundtrip time: {:.1f} ms. Timeout setting: {:.1f} ms.\\n'\n            text = template.format(\n                answer,\n                _hexlify(answer),\n                len(answer),\n                (_LATEST_READ_TIMES.get(self.serial.port, 0) - latest_write_time) * _SECONDS_TO_MILLISECONDS,\n                self.serial.timeout * _SECONDS_TO_MILLISECONDS)\n            _print_out(text)\n\n        if len(answer) == 0:\n            raise IOError('No communication with the instrument (no answer)')\n\n        return answer"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _playsoundWin(sound, block = True):\n    '''\n    Utilizes windll.winmm. Tested and known to work with MP3 and WAVE on\n    Windows 7 with Python 2.7. Probably works with more file formats.\n    Probably works on Windows XP thru Windows 10. Probably works with all\n    versions of Python.\n\n    Inspired by (but not copied from) Michael Gundlach <gundlach@gmail.com>'s mp3play:\n    https://github.com/michaelgundlach/mp3play\n\n    I never would have tried using windll.winmm without seeing his code.\n    '''\n    from ctypes import c_buffer, windll\n    from random import random\n    from time   import sleep\n    from sys    import getfilesystemencoding\n\n    def winCommand(*command):\n        buf = c_buffer(255)\n        command = ' '.join(command).encode(getfilesystemencoding())\n        errorCode = int(windll.winmm.mciSendStringA(command, buf, 254, 0))\n        if errorCode:\n            errorBuffer = c_buffer(255)\n            windll.winmm.mciGetErrorStringA(errorCode, errorBuffer, 254)\n            exceptionMessage = ('\\n    Error ' + str(errorCode) + ' for command:'\n                                '\\n        ' + command.decode() +\n                                '\\n    ' + errorBuffer.value.decode())\n            raise PlaysoundException(exceptionMessage)\n        return buf.value\n\n    alias = 'playsound_' + str(random())\n    winCommand('open \"' + sound + '\" alias', alias)\n    winCommand('set', alias, 'time format milliseconds')\n    durationInMS = winCommand('status', alias, 'length')\n    winCommand('play', alias, 'from 0 to', durationInMS.decode())\n\n    if block:\n        sleep(float(durationInMS) / 1000.0)", "response": "Plays a sound on the current process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplay a sound on the AppKit. NSSound.", "response": "def _playsoundOSX(sound, block = True):\n    '''\n    Utilizes AppKit.NSSound. Tested and known to work with MP3 and WAVE on\n    OS X 10.11 with Python 2.7. Probably works with anything QuickTime supports.\n    Probably works on OS X 10.5 and newer. Probably works with all versions of\n    Python.\n\n    Inspired by (but not copied from) Aaron's Stack Overflow answer here:\n    http://stackoverflow.com/a/34568298/901641\n\n    I never would have tried using AppKit.NSSound without seeing his code.\n    '''\n    from AppKit     import NSSound\n    from Foundation import NSURL\n    from time       import sleep\n\n    if '://' not in sound:\n        if not sound.startswith('/'):\n            from os import getcwd\n            sound = getcwd() + '/' + sound\n        sound = 'file://' + sound\n    url   = NSURL.URLWithString_(sound)\n    nssound = NSSound.alloc().initWithContentsOfURL_byReference_(url, True)\n    if not nssound:\n        raise IOError('Unable to load sound named: ' + sound)\n    nssound.play()\n\n    if block:\n        sleep(nssound.duration())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _playsoundNix(sound, block=True):\n    if not block:\n        raise NotImplementedError(\n            \"block=False cannot be used on this platform yet\")\n\n    # pathname2url escapes non-URL-safe characters\n    import os\n    try:\n        from urllib.request import pathname2url\n    except ImportError:\n        # python 2\n        from urllib import pathname2url\n\n    import gi\n    gi.require_version('Gst', '1.0')\n    from gi.repository import Gst\n\n    Gst.init(None)\n\n    playbin = Gst.ElementFactory.make('playbin', 'playbin')\n    if sound.startswith(('http://', 'https://')):\n        playbin.props.uri = sound\n    else:\n        playbin.props.uri = 'file://' + pathname2url(os.path.abspath(sound))\n\n    set_result = playbin.set_state(Gst.State.PLAYING)\n    if set_result != Gst.StateChangeReturn.ASYNC:\n        raise PlaysoundException(\n            \"playbin.set_state returned \" + repr(set_result))\n\n    # FIXME: use some other bus method than poll() with block=False\n    # https://lazka.github.io/pgi-docs/#Gst-1.0/classes/Bus.html\n    bus = playbin.get_bus()\n    bus.poll(Gst.MessageType.EOS, Gst.CLOCK_TIME_NONE)\n    playbin.set_state(Gst.State.NULL)", "response": "Play a sound using NIX."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Pandas DataFrame with rows where column values match are removed.", "response": "def remove_rows_matching(df, column, match):\n    \"\"\"\n    Return a ``DataFrame`` with rows where `column` values match `match` are removed.\n\n    The selected `column` series of values from the supplied Pandas ``DataFrame`` is compared\n    to `match`, and those rows that match are removed from the DataFrame.\n\n    :param df: Pandas ``DataFrame``\n    :param column: Column indexer\n    :param match: ``str`` match target\n    :return: Pandas ``DataFrame`` filtered\n    \"\"\"\n    df = df.copy()\n    mask = df[column].values != match\n    return df.iloc[mask, :]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Pandas DataFrame with rows where column values containing match are removed from the DataFrame.", "response": "def remove_rows_containing(df, column, match):\n    \"\"\"\n    Return a ``DataFrame`` with rows where `column` values containing `match` are removed.\n\n    The selected `column` series of values from the supplied Pandas ``DataFrame`` is compared\n    to `match`, and those rows that contain it are removed from the DataFrame.\n\n    :param df: Pandas ``DataFrame``\n    :param column: Column indexer\n    :param match: ``str`` match target\n    :return: Pandas ``DataFrame`` filtered\n    \"\"\"\n    df = df.copy()\n    mask = [match not in str(v) for v in df[column].values]\n    return df.iloc[mask, :]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_localization_probability(df, threshold=0.75):\n    df = df.copy()\n    localization_probability_mask = df['Localization prob'].values >= threshold\n    return df.iloc[localization_probability_mask, :]", "response": "Filter data to remove rows with a localization probability below threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef minimum_valid_values_in_any_group(df, levels=None, n=1, invalid=np.nan):\n    df = df.copy()\n    \n    if levels is None:\n        if 'Group' in df.columns.names:\n            levels = [df.columns.names.index('Group')]\n\n    # Filter by at least 7 (values in class:timepoint) at least in at least one group\n    if invalid is np.nan:\n        dfx = ~np.isnan(df)\n    else:\n        dfx = df != invalid\n    \n    dfc = dfx.astype(int).sum(axis=1, level=levels)\n    \n    dfm = dfc.max(axis=1) >= n\n    \n    mask = dfm.values\n    \n    return df.iloc[mask, :]", "response": "Filter a DataFrame by at least n valid values in any group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(df, match, columns=['Proteins','Protein names','Gene names']):\n    df = df.copy()\n    dft = df.reset_index()\n    \n    mask = np.zeros((dft.shape[0],), dtype=bool)\n    idx = ['Proteins','Protein names','Gene names']\n    for i in idx:\n        if i in dft.columns:\n            mask = mask | np.array([match in str(l) for l in dft[i].values])\n\n    return df.iloc[mask]", "response": "Search for a given string in a set of columns in a processed DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_exclude(df, s):\n    keep = ~np.array( [s in c for c in df.columns.values] )\n    return df.iloc[:, keep]", "response": "Filter dataframe to exclude matching columns based on search for s"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters dataframe to include specified columns retaining any Intensity columns.", "response": "def filter_select_columns_intensity(df, prefix, columns):\n    \"\"\"\n    Filter dataframe to include specified columns, retaining any Intensity columns.\n    \"\"\"\n    # Note: I use %s.+ (not %s.*) so it forces a match with the prefix string, ONLY if it is followed by something.\n    return df.filter(regex='^(%s.+|%s)$' % (prefix, '|'.join(columns)) )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering to include only the Intensity values with optional specified label excluding other Intensity measurements but retaining all other columns.", "response": "def filter_intensity(df, label=\"\", with_multiplicity=False):\n    \"\"\"\n    Filter to include only the Intensity values with optional specified label, excluding other \n    Intensity measurements, but retaining all other columns.\n    \"\"\"\n    label += \".*__\\d\" if with_multiplicity else \"\"\n\n    dft = df.filter(regex=\"^(?!Intensity).*$\")\n    dfi = df.filter(regex='^(.*Intensity.*%s.*__\\d)$' % label)\n\n    return pd.concat([dft,dfi], axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering to include only the Ratio values with optional specified label excluding other Intensity measurements but retaining all other columns.", "response": "def filter_ratio(df, label=\"\", with_multiplicity=False):\n    \"\"\"\n    Filter to include only the Ratio values with optional specified label, excluding other\n    Intensity measurements, but retaining all other columns.\n    \"\"\"\n    label += \".*__\\d\" if with_multiplicity else \"\"\n\n    dft = df.filter(regex=\"^(?!Ratio).*$\")\n    dfr = df.filter(regex='^(.*Ratio.*%s)$' % label)\n\n    return pd.concat([dft,dfr], axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_maxquant(f, header=0, index_col='id', **kwargs):\n    df = pd.read_csv(f, delimiter='\\t', header=header, index_col=index_col, **kwargs)\n\n    return df", "response": "Read the quantified table output from MaxQuant run."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a Perseus processed data table", "response": "def read_perseus(f):\n    \"\"\"\n    Load a Perseus processed data table\n\n    :param f: Source file\n    :return: Pandas dataframe of imported data\n    \"\"\"\n    df = pd.read_csv(f, delimiter='\\t', header=[0,1,2,3], low_memory=False)\n    df.columns = pd.MultiIndex.from_tuples([(x,) for x in df.columns.get_level_values(0)])\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_perseus(f, df):\n\n    ### Generate the Perseus like type index\n\n    FIELD_TYPE_MAP = {\n        'Amino acid':'C',\n        'Charge':'C',\n        'Reverse':'C',\n        'Potential contaminant':'C',\n        'Multiplicity':'C',\n        'Localization prob':'N',\n        'PEP':'N',\n        'Score':'N',\n        'Delta score':'N',\n        'Score for localization':'N',\n        'Mass error [ppm]':'N',\n        'Intensity':'N',\n        'Position':'N',\n        'Proteins':'T',\n        'Positions within proteins':'T',\n        'Leading proteins':'T',\n        'Protein names':'T',\n        'Gene names':'T',\n        'Sequence window':'T',\n        'Unique identifier':'T',\n    }\n\n    def map_field_type(n, c):\n        try:\n            t = FIELD_TYPE_MAP[c]\n        except:\n            t = \"E\"\n\n        # In the first element, add type indicator\n        if n == 0:\n            t = \"#!{Type}%s\" % t\n\n        return t\n\n    df = df.copy()\n    df.columns = pd.MultiIndex.from_tuples([(k, map_field_type(n, k)) for n, k in enumerate(df.columns)], names=[\"Label\",\"Type\"])\n    df = df.transpose().reset_index().transpose()\n    df.to_csv(f, index=False, header=False)", "response": "Export a dataframe to Perseus format"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites out the data frame of phosphosites in the following format.", "response": "def write_phosphopath(df, f, extra_columns=None):\n    \"\"\"\n    Write out the data frame of phosphosites in the following format::\n\n        protein, protein-Rsite, Rsite, multiplicity\n        Q13619\tQ13619-S10\tS10\t1\n        Q9H3Z4\tQ9H3Z4-S10\tS10\t1\n        Q6GQQ9\tQ6GQQ9-S100\tS100\t1\n        Q86YP4\tQ86YP4-S100\tS100\t1\n        Q9H307\tQ9H307-S100\tS100\t1\n        Q8NEY1\tQ8NEY1-S1000\tS1000\t1\n\n    The file is written as a comma-separated (CSV) file to file ``f``.\n\n    :param df:\n    :param f:\n    :return:\n    \"\"\"\n\n    proteins = [_protein_id(k) for k in df.index.get_level_values('Proteins')]\n    amino_acids = df.index.get_level_values('Amino acid')\n    positions = _get_positions(df)\n    multiplicity = [k[-1] for k in df.index.get_level_values('Multiplicity')]\n\n    apos = [\"%s%s\" % x for x in zip(amino_acids, positions)]\n    prar = [\"%s-%s\" % x for x in zip(proteins, apos)]\n\n    phdf = pd.DataFrame(np.array(list(zip(proteins, prar, apos, multiplicity))))\n    if extra_columns:\n        for c in extra_columns:\n            phdf[c] = df[c].values\n\n    phdf.to_csv(f, sep='\\t', index=None, header=None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites out the data frame ratio between two groups of protein - Rsite multiplicity - timepoint and control selector.", "response": "def write_phosphopath_ratio(df, f, a, *args, **kwargs):\n    \"\"\"\n    Write out the data frame ratio between two groups\n    protein-Rsite-multiplicity-timepoint\n    ID\tRatio\n    Q13619-S10-1-1\t0.5\n    Q9H3Z4-S10-1-1\t0.502\n    Q6GQQ9-S100-1-1\t0.504\n    Q86YP4-S100-1-1\t0.506\n    Q9H307-S100-1-1\t0.508\n    Q8NEY1-S1000-1-1\t0.51\n    Q13541-S101-1-1\t0.512\n    O95785-S1012-2-1\t0.514\n    O95785-S1017-2-1\t0.516\n    Q9Y4G8-S1022-1-1\t0.518\n    P35658-S1023-1-1\t0.52\n\n    Provide a dataframe, filename for output and a control selector. A series of\n     selectors following this will be compared (ratio mean) to the first. If you\n     provide a kwargs timepoint_idx the timepoint information from your selection will\n     be added from the selector index, e.g. timepoint_idx=1 will use the first level\n     of the selector as timepoint information, so (\"Control\", 30) would give timepoint 30.\n\n    :param df:\n    :param a:\n    :param *args\n    :param **kwargs: use timepoint= to define column index for timepoint information, extracted from args.\n    :return:\n    \"\"\"\n    timepoint_idx = kwargs.get('timepoint_idx', None)\n\n    proteins = [get_protein_id(k) for k in df.index.get_level_values('Proteins')]\n    amino_acids = df.index.get_level_values('Amino acid')\n    positions = _get_positions(df)\n    multiplicity = [int(k[-1]) for k in df.index.get_level_values('Multiplicity')]\n\n    apos = [\"%s%s\" % x for x in zip(amino_acids, positions)]\n\n    phdfs = []\n\n    # Convert timepoints to 1-based ordinal.\n    tp_map = set()\n    for c in args:\n        tp_map.add(c[timepoint_idx])\n    tp_map = sorted(tp_map)\n\n    for c in args:\n        v = df[a].mean(axis=1).values / df[c].mean(axis=1).values\n        tp = [1 + tp_map.index(c[timepoint_idx])]\n        tps = tp * len(proteins) if timepoint_idx else [1] * len(proteins)\n\n        prar = [\"%s-%s-%d-%d\" % x for x in zip(proteins, apos, multiplicity, tps)]\n        phdf = pd.DataFrame(np.array(list(zip(prar, v))))\n        phdf.columns = [\"ID\", \"Ratio\"]\n        phdfs.append(phdf)\n\n    pd.concat(phdfs).to_csv(f, sep='\\t', index=None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_r(df, f, sep=\",\", index_join=\"@\", columns_join=\".\"):\n\n    df = df.copy()\n    df.index = [\"@\".join([str(s) for s in v]) for v in df.index.values]\n    df.columns = [\".\".join([str(s) for s in v]) for v in df.index.values]\n    df.to_csv(f, sep=sep)", "response": "Export dataframe in a format easily importable to R\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing a gaussian distribution of missing values.", "response": "def gaussian(df, width=0.3, downshift=-1.8, prefix=None):\n    \"\"\"\n    Impute missing values by drawing from a normal distribution\n\n    :param df:\n    :param width: Scale factor for the imputed distribution relative to the standard deviation of measured values. Can be a single number or list of one per column.\n    :param downshift: Shift the imputed values down, in units of std. dev. Can be a single number or list of one per column\n    :param prefix: The column prefix for imputed columns\n    :return:\n    \"\"\"\n\n    df = df.copy()\n\n    imputed = df.isnull()  # Keep track of what's real\n\n    if prefix:\n        mask = np.array([l.startswith(prefix) for l in df.columns.values])\n        mycols = np.arange(0, df.shape[1])[mask]\n    else:\n        mycols = np.arange(0, df.shape[1])\n\n\n    if type(width) is not list:\n        width = [width] * len(mycols)\n\n    elif len(mycols) != len(width):\n        raise ValueError(\"Length of iterable 'width' does not match # of columns\")\n\n    if type(downshift) is not list:\n        downshift = [downshift] * len(mycols)\n\n    elif len(mycols) != len(downshift):\n        raise ValueError(\"Length of iterable 'downshift' does not match # of columns\")\n\n    for i in mycols:\n        data = df.iloc[:, i]\n        mask = data.isnull().values\n        mean = data.mean(axis=0)\n        stddev = data.std(axis=0)\n\n        m = mean + downshift[i]*stddev\n        s = stddev*width[i]\n\n        # Generate a list of random numbers for filling in\n        values = np.random.normal(loc=m, scale=s, size=df.shape[0])\n\n        # Now fill them in\n        df.iloc[mask, i] = values[mask]\n\n    return df, imputed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_cov_ellipse(cov, pos, nstd=2, **kwargs):\n\n    def eigsorted(cov):\n        vals, vecs = np.linalg.eigh(cov)\n        order = vals.argsort()[::-1]\n        return vals[order], vecs[:, order]\n\n    vals, vecs = eigsorted(cov)\n    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n\n    # Width and height are \"full\" widths, not radius\n    width, height = 2 * nstd * np.sqrt(vals)\n    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, fill=False, **kwargs)\n\n    return ellip", "response": "Plots an nstd sigma error ellipse based on the specified covariance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _pca_scores(\n        scores,\n        pc1=0,\n        pc2=1,\n        fcol=None,\n        ecol=None,\n        marker='o',\n        markersize=30,\n        label_scores=None,\n        show_covariance_ellipse=True,\n        optimize_label_iter=OPTIMIZE_LABEL_ITER_DEFAULT,\n        **kwargs\n    ):\n    \"\"\"\n    Plot a scores plot for two principal components as AxB scatter plot.\n\n    Returns the plotted axis.\n\n    :param scores: DataFrame containing scores\n    :param pc1: Column indexer into scores for PC1\n    :param pc2: Column indexer into scores for PC2\n    :param fcol: Face (fill) color definition\n    :param ecol: Edge color definition\n    :param marker: Marker style (matplotlib; default 'o')\n    :param markersize: int Size of the marker\n    :param label_scores: Index level to label markers with\n    :param show_covariance_ellipse: Plot covariance (2*std) ellipse around each grouping\n    :param optimize_label_iter: Number of iterations to run label adjustment algorithm\n    :return: Generated axes\n    \"\"\"\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(1,1,1)\n    levels = [0,1]    \n\n    for c in set(scores.columns.values):\n\n        try:\n            data = scores[c].values.reshape(2,-1)\n        except:\n            continue\n\n        fc = hierarchical_match(fcol, c, 'k')\n        ec = hierarchical_match(ecol, c)\n        \n        if ec is None:\n            ec = fc\n\n        if type(markersize) == str:\n            # Use as a key vs. index value in this levels\n            idx = scores.columns.names.index(markersize)\n            s = c[idx]\n        elif callable(markersize):\n            s = markersize(c)\n        else:\n            s = markersize\n\n        ax.scatter(data[pc1,:], data[pc2,:], s=s, marker=marker, edgecolors=ec, c=fc)\n\n        if show_covariance_ellipse and data.shape[1] > 2:\n            cov = data[[pc1, pc2], :].T\n            ellip = plot_point_cov(cov, nstd=2, linestyle='dashed', linewidth=0.5, edgecolor=ec or fc,\n                                   alpha=0.8)  #**kwargs for ellipse styling\n            ax.add_artist(ellip)\n\n    if label_scores:\n        scores_f = scores.iloc[ [pc1, pc2] ]\n        idxs = get_index_list( scores_f.columns.names, label_scores )\n        texts = []\n\n        for n, (x, y) in enumerate(scores_f.T.values):\n            t = ax.text(x, y, build_combined_label( scores_f.columns.values[n], idxs, ', '), bbox=dict(boxstyle='round,pad=0.3', fc='#ffffff', ec='none', alpha=0.6))\n            texts.append(t)\n\n        if texts and optimize_label_iter:\n            adjust_text(\n                texts,\n                lim=optimize_label_iter\n            )\n\n    ax.set_xlabel(scores.index[pc1], fontsize=16)\n    ax.set_ylabel(scores.index[pc2], fontsize=16)\n    fig.tight_layout()\n    return ax", "response": "Plots a scores plot for two principal components."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the weighted version of the base class.", "response": "def _pca_weights(\n        weights,\n        pc,\n        threshold=None,\n        label_threshold=None,\n        label_weights=None,\n        optimize_label_iter=OPTIMIZE_LABEL_ITER_DEFAULT,\n        **kwargs\n    ):\n    \"\"\"\n    :param weights:\n    :param pc:\n    :param threshold:\n    :param label_threshold:\n    :param label_weights:\n    :param kwargs:\n    :return:\n    \"\"\"\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(weights.iloc[:, pc].values)\n    ylim = np.max( np.abs( weights.values ) ) * 1.1\n    ax.set_ylim( -ylim, +ylim  )\n    ax.set_xlim(0, weights.shape[0])\n    ax.set_aspect(1./ax.get_data_ratio())\n    \n    wts = weights.iloc[:, pc]\n    texts = []\n\n    if threshold:\n        if label_threshold is None:\n            label_threshold = threshold\n\n        if label_weights:\n\n            FILTER_UP = wts.values >= label_threshold\n            FILTER_DOWN = wts.values <= -label_threshold\n            FILTER = FILTER_UP | FILTER_DOWN\n\n            wti = np.arange(0, weights.shape[0])\n            wti = wti[FILTER]\n\n            idxs = get_index_list( wts.index.names, label_weights )\n            for x in wti:\n                y = wts.iloc[x]\n                t = ax.text(x, y, build_combined_label( wts.index.values[x], idxs), bbox=dict(boxstyle='round,pad=0.3', fc='#ffffff', ec='none', alpha=0.4))\n                texts.append(t)\n\n        if texts and optimize_label_iter:\n            adjust_text(\n                texts,\n                lim=optimize_label_iter,\n                arrowprops=dict(arrowstyle='->', color='red')\n            )\n\n        ax.axhline(threshold, 0, 1)\n        ax.axhline(-threshold, 0, 1)\n\n    ax.set_ylabel(\"Weights on Principal Component %d\" % (pc+1), fontsize=16)\n    fig.tight_layout()\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pca(df, n_components=2, mean_center=False, fcol=None, ecol=None, marker='o', markersize=40, threshold=None, label_threshold=None, label_weights=None, label_scores=None, return_df=False, show_covariance_ellipse=False, *args, **kwargs):\n\n    scores, weights = analysis.pca(df, n_components=n_components, *args, **kwargs)\n\n    scores_ax = _pca_scores(scores, fcol=fcol, ecol=ecol, marker=marker, markersize=markersize, label_scores=label_scores, show_covariance_ellipse=show_covariance_ellipse)\n    weights_ax = []\n    \n    for pc in range(0, weights.shape[1]):\n        weights_ax.append( _pca_weights(weights, pc, threshold=threshold, label_threshold=label_threshold, label_weights=label_weights) )\n    \n    if return_df:\n        return scores, weights\n    else:\n        return scores_ax, weights_ax", "response": "This function performs the principal component analysis from a Pandas Data Frame and generates scores and weights plots."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plsda(df, a, b, n_components=2, mean_center=False, scale=True, fcol=None, ecol=None, marker='o', markersize=40, threshold=None, label_threshold=None, label_weights=None, label_scores=None, return_df=False, show_covariance_ellipse=False, *args, **kwargs):\n\n    scores, weights, loadings = analysis.plsda(df, a, b, n_components=n_components, scale=scale, *args, **kwargs)\n\n    scores_ax = _pca_scores(scores, fcol=fcol, ecol=ecol, marker=marker, markersize=markersize, label_scores=label_scores, show_covariance_ellipse=show_covariance_ellipse)\n    weights_ax = []\n\n    for pc in range(0, weights.shape[1]):\n        weights_ax.append( _pca_weights(weights, pc, threshold=threshold, label_threshold=label_threshold, label_weights=label_weights) )\n\n    if return_df:\n        return scores, weights\n    else:\n        return scores_ax, weights_ax", "response": "Partial Least Squares Regression Analysis"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enrichment(dfenr, level=None):\n    df = dfenr.sort_index(level=(\"Group\", \"Replicate\",\"Timepoint\",\"Technical\" ), axis=1)\n\n    if level:\n        axes = df.mul(100).T.boxplot(by=level)\n    else:\n        colors = np.array((1 - df.T.values[:, 0], df.T.values[:, 0], [0] * len(df.T.values))).T.tolist()\n        colors = [[tuple(i) for i in colors]]\n        df = df.mul(100)\n        axes = df.T.plot(kind=\"bar\", figsize=(len(df.T) * 0.2, 10), color=colors)\n        axes.legend_.remove()\n\n    axes.set_ylim(0, 100)\n    axes.set_ylabel(\"% enrichment\")\n    axes.set_title(\"\")\n\n    return axes", "response": "Generates an enrichment pie chart series from a calculate enrichment table"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate volcano plot of two sample groups.", "response": "def volcano(df, a, b=None,\n            fdr=0.05,\n            figsize=(8,10),\n            show_numbers=True,\n            threshold=2,\n            minimum_sample_n=0,\n            estimate_qvalues=False,\n            labels_from=None,\n            labels_for=None,\n            title=None,\n            label_format=None,\n            markersize=64,\n            s0=0.00001,\n            draw_fdr=True,\n            is_log2=False,\n            fillna=None,\n            label_sig_only=True,\n            ax=None,\n            xlim=None,\n            ylim=None,\n            fc='grey',\n            fc_sig='blue',\n            fc_sigr='red',\n            optimize_label_iter=OPTIMIZE_LABEL_ITER_DEFAULT\n            ):\n    \"\"\"\n    Volcano plot of two sample groups showing t-test p value vs. log2(fc).\n\n    Generates a volcano plot for two sample groups, selected from `df` using `a` and `b` indexers. The mean of\n    each group is calculated along the y-axis (per protein) and used to generate a log2 ratio. If a log2-transformed\n    dataset is supplied set `islog2=True` (a warning will be given when negative values are present).\n\n    If `a` and `b` are provided a two-sample independent t-test is performed between each group. If `minimum_sample_n` is supplied, any values (proteins)\n    without this number of samples will be dropped from the analysis.\n\n    If only `a` is provided, a one-sample t-test is performend.\n\n    Individual data points can be labelled in the resulting plot by passing `labels_from` with a index name, and `labels_for`\n    with a list of matching values for which to plot labels.\n\n\n    :param df: Pandas `dataframe`\n    :param a: `tuple` or `str` indexer for group A\n    :param b: `tuple` or `str` indexer for group B\n    :param fdr: `float` false discovery rate cut-off\n    :param threshold: `float` log2(fc) ratio cut -off\n    :param minimum_sample_n: `int` minimum sample for t-test\n    :param estimate_qvalues: `bool` estimate Q values (adjusted P)\n    :param labels_from: `str` or `int` index level to get labels from\n    :param labels_for: `list` of `str` matching labels to show\n    :param title: `str` title for plot\n    :param markersize: `int` size of markers\n    :param s0: `float` smoothing factor between fdr/fc cutoff\n    :param draw_fdr: `bool` draw the fdr/fc curve\n    :param is_log2: `bool` is the data log2 transformed already?\n    :param fillna: `float` fill NaN values with value (default: 0)\n    :param label_sig_only: `bool` only label significant values\n    :param ax: matplotlib `axis` on which to draw\n    :param fc: `str` hex or matplotlib color code, default color of points\n    :param optimize_label_iter: `50` (default) number of iterations to attempt to optimize label positions.\n                                Increase for clearer positions (longer processing time).\n    :return:\n    \"\"\"\n    df = df.copy()\n    \n    if np.any(df.values[~pd.isnull(df.values)] < 0) and not is_log2:\n        warnings.warn(\"Input data has negative values. If data is log2 transformed, set is_log2=True.\")\n\n    if fillna is not None:\n        df = df.fillna(fillna)\n        \n    if labels_from is None:\n        labels_from = list(df.index.names)\n\n    if b is not None:\n        # Calculate ratio between two groups\n        g1, g2 = df[a].values, df[b].values\n\n        if is_log2:\n            dr = np.nanmean(g2, axis=1) - np.nanmean(g1, axis=1)\n        else:\n            dr = np.log2(np.nanmean(g2, axis=1) / np.nanmean(g1, axis=1))\n\n        ginv = ( (~np.isnan(g1)).sum(axis=1) < minimum_sample_n ) | ((~np.isnan(g2)).sum(axis=1) < minimum_sample_n)\n\n        g1 = np.ma.masked_where(np.isnan(g1), g1)\n        g2 = np.ma.masked_where(np.isnan(g2), g2)\n\n        # Calculate the p value between two groups (t-test)\n        t, p = sp.stats.mstats.ttest_ind(g1.T, g2.T)\n\n    else:\n        g1 = df[a].values\n        dr = np.nanmean(g1, axis=1)\n\n        ginv = (~np.isnan(g1)).sum(axis=1) < minimum_sample_n\n\n        # Calculate the p value one sample t\n        g1 = np.ma.masked_where(np.isnan(g1), g1)\n        t, p = sp.stats.mstats.ttest_1samp(g1.T, popmean=0)\n\n    p = np.array(p) # Unmask\n\n    # Set p values to nan where not >= minimum_sample_n values\n    p[ ginv ] = np.nan\n\n    if ax is None:\n        fig = plt.figure(figsize=figsize)\n\n        ax = fig.add_subplot(1,1,1)\n\n    if estimate_qvalues:\n        p[~np.isnan(p)] = qvalues(p[~np.isnan(p)])\n        ax.set_ylabel('-log$_{10}$(Q)')\n    else:\n        ax.set_ylabel('-log$_{10}$(p)')\n\n\n    # There are values below the fdr\n    s0x, s0y, s0fn = calculate_s0_curve(s0, fdr, min(fdr/2, np.nanmin(p)), np.log2(threshold), np.nanmax(np.abs(dr)), curve_interval=0.001)\n\n    if draw_fdr is True:\n        ax.plot(s0x, -np.log10(s0y), fc_sigr, lw=1 )\n        ax.plot(-s0x, -np.log10(s0y), fc_sigr, lw=1 )\n\n    # Select data based on s0 curve\n    _FILTER_IN = []\n\n    for x, y in zip(dr, p):\n        x = np.abs(x)\n        spy = s0fn(x)\n\n        if len(s0x) == 0 or x < np.min(s0x):\n            _FILTER_IN.append(False)\n            continue\n\n        if y <= spy:\n            _FILTER_IN.append(True)\n        else:\n            _FILTER_IN.append(False)\n\n    _FILTER_IN = np.array(_FILTER_IN)\n    _FILTER_OUT = ~ _FILTER_IN\n    \n\n    def scatter(ax, f, c, alpha=0.5):\n        # lw = [float(l[0][-1])/5 for l in df[ f].index.values]\n        \n        if type(markersize) == str:\n            # Use as a key vs. index value in this levels\n            s = df.index.get_level_values(markersize)\n        elif callable(markersize):\n            s = np.array([markersize(c) for c in df.index.values])\n        else:\n            s = np.ones((df.shape[0],))*markersize\n        \n        \n        ax.scatter(dr[f], -np.log10(p[f]), c=c, s=s[f], linewidths=0, alpha=0.5)\n    \n    _FILTER_OUT1 = _FILTER_OUT & ~np.isnan(p) & (np.array(p) > fdr)\n    scatter(ax, _FILTER_OUT1, fc, alpha=0.3)\n\n    _FILTER_OUT2 = _FILTER_OUT & (np.array(p) <= fdr)\n    scatter(ax, _FILTER_OUT2, fc_sig, alpha=0.3)\n\n    if labels_for:\n        texts = []\n        idxs = get_index_list( df.index.names, labels_from )\n        for shown, label, x, y in zip( _FILTER_IN , df.index.values, dr, -np.log10(p)):\n            \n            if shown or not label_sig_only:\n                label = build_combined_label( label, idxs, label_format=label_format)\n                \n                if labels_for == True or any([l in label for l in labels_for]):\n                    t = ax.text(x, y, label , ha='center', va='center', rotation_mode='anchor', bbox=dict(boxstyle='round,pad=0.3', fc='#ffffff', ec='none', alpha=0.4))\n                    texts.append(t)\n\n        # Adjust spacing/positioning of labels if required.\n        if texts and optimize_label_iter:\n            adjust_text(\n                texts,\n                lim=optimize_label_iter,\n                arrowprops=dict(arrowstyle='->', color='red')\n            )\n\n    scatter(ax, _FILTER_IN, fc_sigr)\n    \n\n    ax.set_xlabel('log$_2$ ratio')\n\n    # Centre the plot horizontally\n    if xlim is None:\n        xmin, xmax = ax.get_xlim()\n        xlim = np.max(np.abs([xmin, xmax]))\n    ax.set_xlim((-xlim, xlim))\n\n    if ylim is None:\n        _, ylim = ax.get_ylim()\n    ax.set_ylim((0, ylim))\n\n    if title:\n        ax.set_title(title)\n\n    if show_numbers:\n        # Annotate axes with the numbers of points in each category (up & down):\n        # filtered (red), Sig-filtered (blue), nonsig (grey)\n        dr_up, dr_down = dr > 0, dr < 0\n        grey_up, blue_up, red_up = np.sum(_FILTER_OUT1 & dr_up), np.sum(_FILTER_OUT2 & dr_up), np.sum(_FILTER_IN & dr_up)\n        grey_do, blue_do, red_do = np.sum(_FILTER_OUT1 & dr_down), np.sum(_FILTER_OUT2 & dr_down), np.sum(_FILTER_IN & dr_down)\n\n        ax.text(0.95, 0.95, '%d' % red_up, horizontalalignment='right', transform=ax.transAxes, color=fc_sigr, fontsize=14)\n        ax.text(0.95, 0.90, '%d' % blue_up, horizontalalignment='right', transform=ax.transAxes, color=fc_sig, fontsize=14)\n        ax.text(0.95, 0.05, '%d' % grey_up, horizontalalignment='right', transform=ax.transAxes, color=fc, fontsize=14)\n\n        ax.text(0.05, 0.95, '%d' % red_do, horizontalalignment='left', transform=ax.transAxes, color=fc_sigr, fontsize=14)\n        ax.text(0.05, 0.90, '%d' % blue_do, horizontalalignment='left', transform=ax.transAxes, color=fc_sig, fontsize=14)\n        ax.text(0.05, 0.05, '%d' % grey_do, horizontalalignment='left', transform=ax.transAxes, color=fc, fontsize=14)\n\n    return ax, p, dr, _FILTER_IN"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a plot of relative numbers of modified amino acids in source DataFrame.", "response": "def modifiedaminoacids(df, kind='pie'):\n    \"\"\"\n    Generate a plot of relative numbers of modified amino acids in source DataFrame.\n\n    Plot a pie or bar chart showing the number and percentage of modified amino\n    acids in the supplied data frame. The amino acids displayed will be\n    determined from the supplied data/modification type.\n\n    :param df: processed DataFrame\n    :param kind: `str` type of plot; either \"pie\" or \"bar\"\n    :return: matplotlib ax\n    \"\"\"\n\n    colors =   ['#6baed6','#c6dbef','#bdbdbd']\n    total_aas, quants = analysis.modifiedaminoacids(df)\n    \n    df = pd.DataFrame()\n    for a, n in quants.items():\n        df[a] = [n]\n\n    df.sort_index(axis=1, inplace=True)\n    \n    if kind == 'bar' or kind == 'both':\n        ax1 = df.plot(kind='bar', figsize=(7,7), color=colors)\n        ax1.set_ylabel('Number of phosphorylated amino acids')\n        ax1.set_xlabel('Amino acid')\n        ax1.set_xticks([])\n        ylim = np.max(df.values)+1000\n        ax1.set_ylim(0, ylim )\n        _bartoplabel(ax1, 100*df.values[0], total_aas, ylim )\n\n        ax1.set_xlim((-0.3, 0.3))\n        return ax\n    \n    if kind == 'pie' or kind == 'both':\n\n        dfp =df.T\n        residues = dfp.index.values\n        \n        dfp.index = [\"%.2f%% (%d)\" % (100*df[i].values[0]/total_aas, df[i].values[0]) for i in dfp.index.values ]\n        ax2 = dfp.plot(kind='pie', y=0, colors=colors)\n        ax2.legend(residues, loc='upper left', bbox_to_anchor=(1.0, 1.0))\n        ax2.set_ylabel('')\n        ax2.set_xlabel('')\n        ax2.figure.set_size_inches(6,6)\n\n        for t in ax2.texts:\n            t.set_fontsize(15)\n\n        return ax2\n\n    return ax1, ax2"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the % of Class I II and III localised peptides according to standard thresholds.", "response": "def modificationlocalization(df):\n    \"\"\"\n    Plot the % of Class I, II and III localised peptides according to standard thresholds.\n\n    Generates a pie chart showing the % of peptides that fall within the Class I, II and III\n    classifications based on localisation probability. These definitions are::\n\n        Class I     0.75 < x\n        Class II    0.50 > x <= 0.75\n        Class III   0.25 > x <= 0.50\n\n    Any peptides with a localisation score of <= 0.25 are excluded.\n\n    :param df:\n    :return: matplotlib axis\n    \"\"\"\n\n\n    colors =  [\"#78c679\", \"#d9f0a3\", \"#ffffe5\"]\n\n    lp = df['Localization prob'].values\n    class_i = np.sum(lp > 0.75)\n    class_ii = np.sum( (lp > 0.5) & (lp <= 0.75 ) )\n    class_iii = np.sum( (lp > 0.25) & (lp <= 0.5 ) )\n\n    cl = [class_i, class_ii, class_iii]\n    total_v = class_i + class_ii + class_iii\n    \n    df = pd.DataFrame(cl)\n\n    df.index = [\"%.2f%% (%d)\" % (100*v/total_v, v) for n, v in enumerate(cl) ]\n    \n    ax = df.plot(kind='pie', y=0, colors=colors)\n\n    ax.legend(['Class I (> 0.75)', 'Class II (> 0.5 \u2264 0.75)', 'Class III (> 0.25, \u2264 0.5)'],\n              loc='upper left', bbox_to_anchor=(1.0, 1.0))\n    ax.set_ylabel('')\n    ax.set_xlabel('')\n\n    for t in ax.texts:\n        t.set_fontsize(15)\n\n    ax.figure.set_size_inches(6,6)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a box plot from pandas DataFrame with sample grouping.", "response": "def box(df, s=None, title_from=None, subplots=False, figsize=(18,6), groups=None, fcol=None, ecol=None, hatch=None, ylabel=\"\", xlabel=\"\"):\n    \"\"\"\n    Generate a box plot from pandas DataFrame with sample grouping.\n\n    Plot group mean, median and deviations for specific values (proteins) in the dataset. Plotting is controlled via\n    the `s` param, which is used as a search string along the y-axis. All matching values will be returned and plotted.\n    Multiple search values can be provided as a `list` of `str` and these will be searched as an `and` query.\n\n    Box fill and edge colors can be controlled on a full-index basis by passing a `dict` of indexer:color to\n    `fcol` and `ecol` respectively. Box hatching can be controlled by passing a `dict` of indexer:hatch to `hatch`.\n\n\n    :param df: Pandas `DataFrame`\n    :param s: `str` search y-axis for matching values (case-insensitive)\n    :param title_from: `list` of `str` of index levels to generate title from\n    :param subplots: `bool` use subplots to separate plot groups\n    :param figsize: `tuple` of `int` size of resulting figure\n    :param groups:\n    :param fcol: `dict` of `str` indexer:color where color is hex value or matplotlib color code\n    :param ecol: `dict` of `str` indexer:color where color is hex value or matplotlib color code\n    :param hatch: `dict` of `str` indexer:hatch where hatch is matplotlib hatch descriptor\n    :param ylabel: `str` ylabel for boxplot\n    :param xlabel: `str` xlabel for boxplot\n    :return: `list` of `Figure`\n    \"\"\"\n\n    df = df.copy()\n\n    if type(s) == str:\n        s = [s]\n\n    if title_from is None:\n        title_from = list(df.index.names)\n        \n    if groups is None:\n        groups = list( set( df.columns.get_level_values(0) ) )\n        \n    # Build the combined name/info string using label_from; replace the index\n    title_idxs = get_index_list( df.index.names, title_from )\n    df.index = [build_combined_label(r, title_idxs) for r in df.index.values]\n    \n    if s:\n        # Filter the table on the match string (s)\n        df = df.iloc[ [all([str(si).lower() in l.lower() for si in s]) for l in df.index.values] ]\n    \n    figures = []\n    # Iterate each matching row, building the correct structure dataframe\n    for ix in range(df.shape[0]):\n        \n        dfi = pd.DataFrame(df.iloc[ix]).T\n        label = dfi.index.values[0]\n        dfi = process.fold_columns_to_rows(dfi, levels_from=len(df.columns.names)-1)\n\n        if subplots:\n            gs = gridspec.GridSpec(1, len(subplots), width_ratios=[dfi[sp].shape[1] for sp in subplots])\n            subplotl = subplots\n        elif isinstance(dfi.columns, pd.MultiIndex) and len(dfi.columns.levels) > 1:\n            subplotl = dfi.columns.levels[0]\n            gs = gridspec.GridSpec(1, len(subplotl), width_ratios=[dfi[sp].shape[1] for sp in subplotl])\n        else:\n            # Subplots\n            subplotl = [None]\n            gs =  gridspec.GridSpec(1, 1) \n\n\n        first_ax = None\n\n        fig = plt.figure(figsize=figsize)\n\n        for n, sp in enumerate(subplotl):\n\n            if sp is None:\n                dfp = dfi\n            else:\n                dfp = dfi[sp]\n\n            ax = fig.add_subplot(gs[n], sharey=first_ax)\n            \n            #print(dfp.median(axis=1, level=0).reset_index())\n\n            medians = dfp.median(axis=1, level=0).reset_index()#.set_index('Replicate') #.dropna(axis=1)\n\n            if groups and all([g in medians.columns.get_level_values(0) for g in groups]):\n                medians = medians[ groups ]\n\n            ax, dic = medians.plot(\n                kind='box', \n                return_type = 'both',\n                patch_artist=True,\n                ax = ax,\n            )\n\n            ax.set_xlabel('')\n            for n, c in enumerate(medians.columns.values):\n                if sp is None:\n                    hier = []\n                else:\n                    hier = [sp]  \n                if type(c) == tuple:\n                    hier.extend(c)\n                else:\n                    hier.append(c)\n                    \n                if fcol:\n                    color = hierarchical_match(fcol, hier, None)\n                    if color:\n                        dic['boxes'][n].set_color( color )\n                if ecol:\n                    color = hierarchical_match(ecol, hier, None)\n                    if color:\n                        dic['boxes'][n].set_edgecolor( color )\n                if hatch:\n                    dic['boxes'][n].set_hatch( hierarchical_match(hatch, hier, '') )\n\n            ax.set_xlabel(xlabel)\n            ax.tick_params(axis='both', which='major', labelsize=12)\n\n            if first_ax is None:\n                first_ax = ax\n            else:\n                for yl in ax.get_yticklabels():\n                    yl.set_visible(False)\n\n        first_ax.set_ylabel(ylabel, fontsize=14)\n        fig.subplots_adjust(wspace=0.05)\n\n        fig.suptitle(label)\n        \n        figures.append(fig)\n        \n    return figures"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot a 2 or 3 - part venn diagram showing the overlap between two or three Pandas DataFrames.", "response": "def venn(df1, df2, df3=None, labels=None, ix1=None, ix2=None, ix3=None, return_intersection=False, fcols=None):\n    \"\"\"\n    Plot a 2 or 3-part venn diagram showing the overlap between 2 or 3 pandas DataFrames.\n\n    Provided with two or three Pandas DataFrames, this will return a venn diagram showing the overlap calculated between\n    the DataFrame indexes provided as ix1, ix2, ix3. Labels for each DataFrame can be provided as a list in the same order,\n    while `fcol` can be used to specify the colors of each section.\n\n    :param df1: Pandas DataFrame\n    :param df2: Pandas DataFrame\n    :param df3: Pandas DataFrame (optional)\n    :param labels: List of labels for the provided dataframes\n    :param ix1: Index level name of of Dataframe 1 to use for comparison\n    :param ix2: Index level name of of Dataframe 2 to use for comparison\n    :param ix3: Index level name of of Dataframe 3 to use for comparison\n    :param return_intersection: Return the intersection of the supplied indices\n    :param fcols: List of colors for the provided dataframes\n    :return: ax, or ax with intersection\n    \"\"\"\n    try:\n        import matplotlib_venn as mplv\n    except ImportError:\n        raise ImportError(\"To plot venn diagrams, install matplotlib-venn package: pip install matplotlib-venn\")\n\n    plt.gcf().clear()\n\n\n    if labels is None:\n        labels = [\"A\", \"B\", \"C\"]\n        \n    s1 = _process_ix(df1.index, ix1)\n    s2 = _process_ix(df2.index, ix2)\n    if df3 is not None:\n        s3 = _process_ix(df3.index, ix3)\n\n    kwargs = {}\n\n    if fcols:\n        kwargs['set_colors'] = [fcols[l] for l in labels]\n\n    if df3 is not None:\n        vn = mplv.venn3([s1,s2,s3], set_labels=labels, **kwargs)\n        intersection = s1 & s2 & s3\n    else:\n        vn = mplv.venn2([s1,s2], set_labels=labels, **kwargs)\n        intersection = s1 & s2\n\n\n\n    ax = plt.gca()\n\n    if return_intersection:\n        return ax, list(intersection)\n    else:\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the number of sites peptides and proteins in the dataset.", "response": "def sitespeptidesproteins(df, labels=None, colors=None, site_localization_probability=0.75):\n    \"\"\"\n    Plot the number of sites, peptides and proteins in the dataset.\n\n    Generates a plot with sites, peptides and proteins displayed hierarchically in chevrons.\n    The site count is limited to Class I (<=0.75 site localization probability) by default\n    but may be altered using the `site_localization_probability` parameter.\n\n    Labels and alternate colours may be supplied as a 3-entry iterable.\n\n    :param df: pandas DataFrame to calculate numbers from\n    :param labels: list/tuple of 3 strings containing labels\n    :param colors: list/tuple of 3 colours as hex codes or matplotlib color codes\n    :param site_localization_probability: the cut-off for site inclusion (default=0.75; Class I)\n    :return:\n    \"\"\"\n    fig = plt.figure(figsize=(4,6))\n    ax = fig.add_subplot(1,1,1)\n\n    shift = 0.5\n    values = analysis.sitespeptidesproteins(df, site_localization_probability)\n    if labels is None:\n        labels = ['Sites (Class I)', 'Peptides', 'Proteins']\n    if colors is None:\n        colors = ['#756bb1', '#bcbddc', '#dadaeb']\n\n    for n, (c, l, v) in enumerate(zip(colors, labels, values)):\n        ax.fill_between([0,1,2], np.array([shift,0,shift]) + n, np.array([1+shift,1,1+shift]) + n, color=c, alpha=0.5 )\n\n        ax.text(1, 0.5 + n, \"{}\\n{:,}\".format(l, v), ha='center', color='k', fontsize=16 )\n        \n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_axis_off()\n    \n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a ranking plot showing detected protein intensity vs. raw intensity value S curve.", "response": "def rankintensity(df, colors=None, labels_from='Protein names', number_of_annotations=3, show_go_enrichment=False, go_ids_from=None, go_enrichment='function', go_max_labels=8, go_fdr=None, progress_callback=None):\n    \"\"\"\n    Rank intensity plot, showing intensity order vs. raw intensity value S curve.\n\n    Generates a plot showing detected protein intensity plotted against protein intensity rank. A series of colors\n    can be provided to segment the S curve into regions. Gene ontology enrichments (as calculated via `analysis.go_enrichment`)\n    can be overlaid on the output. Note that since the ranking reflects simple abundance there is little meaning to enrichment\n    (FDR will remove most if not all items) and it is best considered an annotation of the 'types' of proteins in that region.\n\n    :param df: Pands DataFrame\n    :param colors: `list` of colors to segment the plot into\n    :param labels_from: Take labels from this column\n    :param number_of_annotations: Number of protein annotations at each tip\n    :param show_go_enrichment: Overlay plot with GO enrichment terms\n    :param go_ids_from: Get IDs for GO enrichment from this column\n    :param go_enrichment: Type of GO enrichment to show\n    :param go_max_labels: Maximum number of GO enrichment labels per segment\n    :param go_fdr: FDR cutoff to apply to the GO enrichment terms\n    :return: matplotlib Axes\n    \"\"\"\n\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(1,1,1)\n\n    labels = df[labels_from].values\n\n    if go_ids_from:\n        ids = df[go_ids_from]\n    else:\n        if 'Proteins' in df.columns.values:\n            ids = df['Proteins']\n        elif 'Protein IDs' in df.columns.values:\n            ids = df['Protein IDs']\n        else:\n            ids = None\n\n    y = np.log10(df['Intensity'].values)\n\n    filt = ~np.array( np.isnan(y) | np.isinf(y) )\n    y = y[filt]\n    labels = labels[filt]\n    if ids is not None:\n        ids = ids[filt]\n\n    sort = np.argsort(y)\n    y = y[sort]\n    labels = labels[sort]\n    if ids is not None:\n        ids = ids[sort]\n\n    x_max = y.shape[0]\n    y_min, y_max = np.min(y), np.max(y)\n    yrange = y_max-y_min\n\n    x = np.arange(x_max)\n\n    ax.set_xlim( (-x_max//3,  x_max+x_max//3) )\n    ax.set_ylim( (y_min-1,  y_max+1) )\n\n    # Set the dimensions so we can plot the labels correctly.\n    ax.scatter(x, y, alpha=0, zorder=-1)\n\n    ax.set_ylabel(\"Total intensity ($log_{10}$)\")\n    ax.set_xlabel(\"Ranked phosphoproteins\")\n\n    # Defines ranges over which text can be slotted in (avoid y overlapping)\n    slot_size = 0.03 # FIXME: We should calculate this\n    text_y_slots = np.arange(0.1, 0.95, slot_size)\n\n\n    text_y_cross = 0.55\n    # Build set of standard x offsets at defined y data points\n    # For each y slot, find the (nearest) data y, therefore the x\n    text_x_slots = []\n    inv =  ax.transLimits.inverted()\n    for ys in text_y_slots:\n        _, yd = inv.transform( (0, ys) )\n        text_x_slots.append( ax.transLimits.transform( (x[find_nearest_idx(y, yd)], 0 ) )[0] )\n    text_x_slots = np.array(text_x_slots)\n\n    text_x_slots[text_y_slots < text_y_cross] += 0.15\n    text_x_slots[text_y_slots > text_y_cross] -= 0.15\n\n    def annotate_obj(ax, n, labels, xs, ys, idx, yd, ha):\n        \"\"\"\n        \"\"\"\n\n        ni = 1\n        previous = {}\n        for l, xi, yi in zip(labels, xs, ys):\n            if type(l) == str:\n                l = get_shortstr(l)\n\n                if l in previous: # Use previous text slot for annotation; skip annotation part\n                    axf, ayf = previous[l]\n\n                elif text_y_slots[idx]:\n                    axf = text_x_slots[idx]\n                    ayf = text_y_slots[idx]\n                    text_x_slots[idx] = np.nan\n                    text_y_slots[idx] = np.nan\n\n                    ax.text(axf, ayf, l, transform=ax.transAxes,  ha=ha, va='center', color='k')\n\n                    idx += yd\n                    ni += 1\n\n                    previous[l] = (axf, ayf)\n\n                ax.annotate(\"\", xy=(xi, yi), xycoords='data',  xytext=(axf, ayf), textcoords='axes fraction',\n                        va='center', color='k',\n                        arrowprops=dict(arrowstyle='-', connectionstyle=\"arc3\", ec='k', lw=1), zorder=100)\n\n            if ni > n:\n                break\n\n    _n = np.min([labels.shape[0], 20])\n\n    annotate_obj(ax, number_of_annotations, labels[-1:-_n:-1], x[-1:-_n:-1], y[-1:-_n:-1], -1, -1, 'right')\n    annotate_obj(ax, number_of_annotations, labels[:_n], x[:_n], y[:_n], 0, +1, 'left')\n\n    if show_go_enrichment and ids is not None:\n\n        # Calculate orders of magnitude (range)\n        oomr = int(np.round(np.min(y))), int(np.round(np.max(y)))\n\n        # First -1, last +1\n        segments = [[x, x+1] for x in range(*oomr)]\n        segments[0][0] -= 3\n        segments[-1][1] += 3\n\n        if not isinstance(colors, list):\n            if not isinstance(colors, tuple):\n                colors = ('#1f77b4', '#d62728')\n            # Build a continuous scale from low to high\n            cmap = mpl.colors.LinearSegmentedColormap.from_list('custom', list(colors), len(segments))\n            colors = [mpl.colors.rgb2hex(cmap(n)) for n in range(len(segments))]\n\n        for n, (s, e) in enumerate(segments):\n            if progress_callback:\n                progress_callback(float(n)/len(segments))\n\n            mask = (y > s) & (y < e)\n            c = x[mask]\n\n            # Comparison relative to background\n            gids = list(set(ids[mask]) - set(ids[~mask]))\n            go = analysis.go_enrichment(gids, enrichment=go_enrichment, fdr=go_fdr)\n\n            if go is not None:\n                labels = [gi[1] for gi in go.index]\n\n                # Filter out less specific GO terms where specific terms exist (simple text matching)\n                labels_f = []\n                for l in labels:\n                    for ll in labels:\n                        if l in ll and l != ll:\n                            break\n                    else:\n                        labels_f.append(l)\n                labels = labels_f[:go_max_labels]\n\n                yr = ax.transLimits.transform( (0, y[c[0]]) )[1], ax.transLimits.transform( (0, y[c[-1]]) )[1]\n\n                # find axis label point for both start and end\n                if yr[0] < text_y_cross:\n                    yr = yr[0]-slot_size, yr[1]\n                else:\n                    yr = yr[0]+slot_size, yr[1]\n\n                yr = find_nearest_idx(text_y_slots, yr[0]), find_nearest_idx(text_y_slots, yr[1])\n\n                yrange = list(range(yr[0], yr[1]))\n\n                # Center ish\n                if len(yrange) > len(labels):\n                    crop = (len(yrange) - len(labels)) // 2\n                    if crop > 1:\n                        yrange = yrange[crop:-crop]\n\n                # display ranked top to bottom\n                for idx, l in zip(yrange, labels):\n                    axf = text_x_slots[idx]\n                    ayf = text_y_slots[idx]\n                    text_x_slots[idx] = np.nan\n                    text_y_slots[idx] = np.nan\n\n                    if ayf > text_y_cross:\n                        ha = 'right'\n                    else:\n                        ha = 'left'\n                    ax.text(axf, ayf, l, transform=ax.transAxes, ha=ha, color=colors[n])\n\n            # Calculate GO enrichment terms for each region?\n            ax.scatter(x[mask], y[mask], s=15, c=colors[n], lw=0, zorder=100)\n\n    else:\n            ax.scatter(x, y, s=15, c='k', lw=0, zorder=100)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a column - wise correlation plot from the supplied dataframes.", "response": "def correlation(df, cm=cm.PuOr_r, vmin=None, vmax=None, labels=None, show_scatter=False):\n    \"\"\"\n    Generate a column-wise correlation plot from the provided data.\n\n    The columns of the supplied dataframes will be correlated (using `analysis.correlation`) to\n    generate a Pearson correlation plot heatmap. Scatter plots of correlated samples can also be generated over\n    the redundant half of the plot to give a visual indication of the protein distribution.\n\n    :param df: `pandas.DataFrame`\n    :param cm: Matplotlib colormap (default cm.PuOr_r)\n    :param vmin: Minimum value for colormap normalization\n    :param vmax: Maximum value for colormap normalization\n    :param labels: Index column to retrieve labels from\n    :param show_scatter: Show overlaid scatter plots for each sample in lower-left half. Note that this is slow for large numbers of samples.\n    :return: `matplotlib.Figure` generated Figure.\n    \"\"\"\n\n\n    data = analysis.correlation(df)\n\n    if labels:\n        for axis in (0,1):\n            data.sort_index(level=labels, axis=axis, inplace=True)\n\n    data = data.values\n\n    # Plot the distributions\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1,1,1)\n\n    if vmin is None:\n        vmin = np.nanmin(data)\n\n    if vmax is None:\n        vmax = np.nanmax(data)\n\n    n_dims = data.shape[0]\n\n    # If showing scatter plots, set the inlay portion to np.nan\n    if show_scatter:\n        # Get the triangle, other values will be zeroed\n        idx = np.tril_indices(n_dims)\n        data[idx] = np.nan\n\n    cm.set_bad('w', 1.)\n    i = ax.imshow(data, cmap=cm, vmin=vmin, vmax=vmax, interpolation='none')\n    fig.colorbar(i)\n    fig.axes[0].grid('off')\n\n    if show_scatter:\n        figo = mpl.figure.Figure(figsize=(n_dims, n_dims), dpi=300)\n        # Create a dummy Agg canvas so we don't have to display/output this intermediate\n        canvas = FigureCanvasAgg(figo)\n\n        for x in range(0, n_dims):\n            for y in range(x, n_dims):\n\n                ax = figo.add_subplot(n_dims, n_dims, y*n_dims+x+1)\n\n                if x != y:\n                    xd = df.values[:, x]\n                    yd = df.values[:, y]\n                    ax.scatter(xd, yd, lw=0, s=5, c='k', alpha=0.2)\n\n                ax.grid('off')\n                ax.axis('off')\n\n\n        figo.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n        raw = BytesIO()\n        figo.savefig(raw, format='png', bbox_inches=0, transparent=True)\n        del figo\n\n        raw.seek(0)\n        img = mplimg.imread(raw)\n        ax2 = fig.add_axes(fig.axes[0].get_position(), label='image', zorder=1)\n        ax2.axis('off')\n        ax2.imshow(img)\n\n    if labels:\n        # Build labels from the supplied axis\n        labels = [\n            df.columns.get_level_values(l)\n            for l in labels\n        ]\n        labels = [\" \".join([str(s) for s in l]) for l in zip(*labels) ]\n\n        fig.axes[0].set_xticks(range(n_dims))\n        fig.axes[0].set_xticklabels(labels, rotation=45)\n\n        fig.axes[0].set_yticks(range(n_dims))\n        fig.axes[0].set_yticklabels(labels)\n\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _areadist(ax, v, xr, c, bins=100, by=None, alpha=1, label=None):\n    y, x = np.histogram(v[~np.isnan(v)], bins)\n    x = x[:-1]\n\n    if by is None:\n        by = np.zeros((bins,))\n\n    ax.fill_between(x, y, by, facecolor=c, alpha=alpha, label=label)\n    return y", "response": "Plot the histogram distribution but as an area plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compareimputed(df1, df2, bins=50):\n\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 10))\n\n    xr = np.nanmin( [np.nanmin(df1), np.nanmin(df2)] ), np.nanmax( [np.nanmax(df1), np.nanmax(df2)] )\n\n    ax1.set_title('Distributions of A and B')\n    _areadist(ax1, df2.values, xr, c='r', bins=bins)\n    _areadist(ax1, df1.values, xr, c='k', bins=bins, alpha=0.3)\n    ax1.set_xlabel('Value')\n    ax1.set_ylabel('Count')\n\n    ax2.set_title('Distributions of A and values unique to B')\n    # Calculate what is different isolate those values\n    _areadist(ax2, df2.values[ df2.values != df1.values ], xr, c='r', bins=bins)\n    _areadist(ax2, df1.values, xr, c='k', bins=bins, alpha=0.3)\n    ax2.set_xlabel('Value')\n    ax2.set_ylabel('Count')\n\n    # Calculate (at the lowest column index level) the difference between\n    # distribution of unique values, vs those in common\n    # Get indices of difference\n    dfc= df1.copy()\n    dfc[:] = (df2.values != df1.values).astype(int)\n    for i in dfc.columns.values:\n        dfc[i[:-1]] = np.max(dfc[i[:-1]].values, axis=1)\n\n    ax3.set_title('Distributions of associated values of A and substituted values in B')\n    _areadist(ax3, df2.values[ df2.values != df1.values ], xr, c='r')\n    _areadist(ax3, df1.values[ dfc.values == 1 ], xr, c='k', alpha=0.3)\n    ax3.set_xlabel('Value')\n    ax3.set_ylabel('Count')\n\n    return fig", "response": "Compare two DataFrames giving visualisations of the differences between two DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef comparedist(df, *args, **kwargs):\n\n    bins = kwargs.get('bins', 50)\n    xlabel = kwargs.get('xlabel', 'Value')\n    ylabel = kwargs.get('ylabel', 'Count')\n    base_fmt = kwargs.get('base_fmt')\n    arg_fmt = kwargs.get('arg_fmt')\n\n    # The base for comparisons is the first passed selector.\n    base_selector, selectors = args[0], args[1:]\n    df1 = df[base_selector]\n\n    fig, axes = plt.subplots(len(selectors), 1, figsize=(10, len(selectors) * 5))\n\n    if not isinstance(axes, np.ndarray):\n        axes = [axes]  # mpl returns a single object when only one.\n\n    for n, (ax1, selector) in enumerate(zip(axes, selectors)):\n\n        dfn = df[selector]\n\n        xr = np.nanmin( [np.nanmin(df1), np.nanmin(dfn)] ), np.nanmax( [np.nanmax(df1), np.nanmax(dfn)] )\n\n        ax1.set_title('Distributions of %s and %s' % (base_selector, selector))\n        _areadist(ax1, dfn.values, xr, c='r', bins=bins, label=format_label(base_selector, base_fmt))\n        _areadist(ax1, df1.values, xr, c='k', bins=bins, alpha=0.3, label=format_label(selector, arg_fmt))\n\n        ax1.set_xlabel(xlabel)\n        ax1.set_ylabel(ylabel)\n        _, ymax = ax1.get_ylim()\n        ax1.set_ylim(0, ymax)\n\n        ax1.legend(loc='upper right')\n\n    fig.tight_layout()\n    return fig", "response": "Compare the distributions of two DataFrames and return a Figure containing the differences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisualize data on a kegg pathway.", "response": "def kegg_pathway(df, pathway, a, b=None, ids_from=\"Proteins\", cmap=cm.PuOr_r, is_log2=False, fillna=None, z_score=1):\n    \"\"\"\n    Visualize data on a kegg pathway.\n\n\n    :param df:\n    :param pathway:\n    :param a:\n    :param b:\n    :param ids_from:\n    :param cmap:\n    :param is_log2:\n    :param fillna:\n    :param z_score:\n    :return:\n    \"\"\"\n\n    df = df.copy()\n\n    if np.any(df.values < 0) and not is_log2:\n        warnings.warn(\"Input data has negative values. If data is log2 transformed, set is_log2=True.\")\n\n    if fillna is not None:\n        df = df.fillna(fillna)\n\n    if z_score is None:\n        pass\n    elif z_score == 0:\n        df = (df - df.median(axis=0)) / df.std(axis=0)\n    elif z_score == 1:\n        df = ((df.T - df.median(axis=1).T) / df.std(axis=1).T).T\n\n\n    if b is not None:\n        # Calculate ratio between two groups\n        g1, g2 = df[a].values, df[b].values\n\n        if is_log2:\n            dr = np.nanmean(g2, axis=1) - np.nanmean(g1, axis=1)\n        else:\n            dr = np.log2(np.nanmean(g2, axis=1) / np.nanmean(g1, axis=1))\n\n    else:\n        g1 = df[a].values\n        dr = np.nanmean(g1, axis=1)\n\n\n    maxi = np.max(abs(dr))\n    norm = mpl.colors.Normalize(vmin=-maxi, vmax=maxi)\n    mapper = cm.ScalarMappable(norm=norm, cmap=cm.PuOr_r) # Orange up\n\n    node_colors = {}\n    for p, v in zip(df.index.get_level_values(ids_from), dr):\n        pid = str(p).split(\";\")[-1]\n\n        if \"_\" in pid:\n            pid = pid[:pid.index(\"_\")]\n\n        node_colors[pid] = mpl.colors.rgb2hex(mapper.to_rgba(v))\n\n\n    global uniprot_kegg_cache\n\n    # Only do this once\n    upids = list( node_colors.keys() )\n    upids = [p for p in upids if p not in uniprot_kegg_cache.keys()]\n\n    if upids:\n        new_pairs = get_uniprot_id_mapping_pairs('ACC+ID', 'KEGG_ID', upids)\n        uniprot_kegg_cache.update(new_pairs)\n\n        for p in upids:\n            if p not in uniprot_kegg_cache:\n                uniprot_kegg_cache[p] = None # Not found, don't look again\n\n\n    with StringIO() as f:\n        f.write('#hsa\\tData\\n')\n        for k, c in list(node_colors.items()):\n            if k in uniprot_kegg_cache and uniprot_kegg_cache[k] is not None:\n                kids = uniprot_kegg_cache[k]\n                for kegg_id in kids:\n                    f.write('%s\\t%s\\n' % (kegg_id.split(':')[-1], c ))\n\n        # Reset file\n        f.seek(0)\n\n        url = 'https://www.kegg.jp/kegg-bin/mcolor_pathway'\n        m = MultipartEncoder(\n            fields={\n                'map': pathway,\n                'mapping_list': ('filename', f),\n                'mode': 'color',\n                'submit': 'Exec',\n                'reference': 'white',\n                 }\n        )\n\n    r = requests.post(url, data=m, headers={'Content-Type': m.content_type})\n    if r.status_code == 200:\n        # src=\"/tmp/mark_pathway154353327948969/hsa04010.1.png\"\n        ms = re.finditer('src=\"(/tmp/mark_pathway[^\"]*.png)\"', r.text)\n        m = list(ms).pop()\n\n        # Download image data\n        image = Image.open(requests.get('http://www.kegg.jp%s' % m.group(1), stream=True).raw)\n        width, height = image.size   # Get dimensions\n        image = image.crop((1, 1, width-1, height-1)) # Crop black outline\n        print(\"Scale range: %.2f .. %.2f\" % (-maxi, maxi))\n\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply column - wise normalisation to expression columns.", "response": "def subtract_column_median(df, prefix='Intensity '):\n    \"\"\"\n    Apply column-wise normalisation to expression columns.\n\n    Default is median transform to expression columns beginning with Intensity\n\n\n    :param df:\n    :param prefix: The column prefix for expression columns\n    :return:\n    \"\"\"\n    df = df.copy()\n\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    mask = [l.startswith(prefix) for l in df.columns.values]\n    df.iloc[:, mask] = df.iloc[:, mask] - df.iloc[:, mask].median(axis=0)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopyrights (c) 2012, Nicolo Fusi, University of Sheffield All rights reserved. Estimates q-values from p-values Args ===== m: number of tests. If not specified m = pv.size verbose: print verbose messages? (default False) lowmem: use memory-efficient in-place algorithm pi0: if None, it's estimated as suggested in Storey and Tibshirani, 2003. For most GWAS this is not necessary, since pi0 is extremely likely to be 1 :param pv: :param m: :param verbose: :param lowmem: :param pi0: :return:", "response": "def qvalues(pv, m = None, verbose = False, lowmem = False, pi0 = None):\n    \"\"\"\n    Copyright (c) 2012, Nicolo Fusi, University of Sheffield\n    All rights reserved.\n\n    Estimates q-values from p-values\n\n    Args\n    =====\n\n    m: number of tests. If not specified m = pv.size\n    verbose: print verbose messages? (default False)\n    lowmem: use memory-efficient in-place algorithm\n    pi0: if None, it's estimated as suggested in Storey and Tibshirani, 2003.\n         For most GWAS this is not necessary, since pi0 is extremely likely to be\n         1\n\n    :param pv:\n    :param m:\n    :param verbose:\n    :param lowmem:\n    :param pi0:\n    :return:\n    \"\"\"\n\n    assert(pv.min() >= 0 and pv.max() <= 1), \"p-values should be between 0 and 1\"\n\n    original_shape = pv.shape\n    pv = pv.ravel() # flattens the array in place, more efficient than flatten() \n\n    if m == None:\n        m = float(len(pv))\n    else:\n        # the user has supplied an m\n        m *= 1.0\n\n    # if the number of hypotheses is small, just set pi0 to 1\n    if len(pv) < 100 and pi0 == None:\n        pi0 = 1.0\n    elif pi0 != None:\n        pi0 = pi0\n    else:\n        # evaluate pi0 for different lambdas\n        pi0 = []\n        lam = sp.arange(0, 0.90, 0.01)\n        counts = sp.array([(pv > i).sum() for i in sp.arange(0, 0.9, 0.01)])\n        \n        for l in range(len(lam)):\n            pi0.append(counts[l]/(m*(1-lam[l])))\n\n        pi0 = sp.array(pi0)\n\n        # fit natural cubic spline\n        tck = sp.interpolate.splrep(lam, pi0, k = 3)\n        pi0 = sp.interpolate.splev(lam[-1], tck)\n        \n        if pi0 > 1:\n            if verbose:\n                print(\"got pi0 > 1 (%.3f) while estimating qvalues, setting it to 1\" % pi0)\n            \n            pi0 = 1.0\n\n    assert(pi0 >= 0 and pi0 <= 1), \"pi0 is not between 0 and 1: %f\" % pi0\n\n\n    if lowmem:\n        # low memory version, only uses 1 pv and 1 qv matrices\n        qv = sp.zeros((len(pv),))\n        last_pv = pv.argmax()\n        qv[last_pv] = (pi0*pv[last_pv]*m)/float(m)\n        pv[last_pv] = -sp.inf\n        prev_qv = last_pv\n        for i in range(int(len(pv))-2, -1, -1):\n            cur_max = pv.argmax()\n            qv_i = (pi0*m*pv[cur_max]/float(i+1))\n            pv[cur_max] = -sp.inf\n            qv_i1 = prev_qv\n            qv[cur_max] = min(qv_i, qv_i1)\n            prev_qv = qv[cur_max]\n\n    else:\n        p_ordered = sp.argsort(pv)    \n        pv = pv[p_ordered]\n        qv = pi0 * m/len(pv) * pv\n        qv[-1] = min(qv[-1],1.0)\n\n        for i in range(len(pv)-2, -1, -1):\n            qv[i] = min(pi0*m*pv[i]/(i+1.0), qv[i+1])\n        \n        # reorder qvalues\n        qv_temp = qv.copy()\n        qv = sp.zeros_like(qv)\n        qv[p_ordered] = qv_temp\n\n        # reshape qvalues\n        qv = qv.reshape(original_shape)\n        \n    return qv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a complete list of shortform IDs from a DataFrame df", "response": "def get_protein_id_list(df, level=0):\n    \"\"\"\n    Return a complete list of shortform IDs from a DataFrame\n\n    Extract all protein IDs from a dataframe from multiple rows containing\n    protein IDs in MaxQuant output format: e.g. P07830;P63267;Q54A44;P63268\n\n    Long names (containing species information) are eliminated (split on ' ') and\n    isoforms are removed (split on '_').\n\n    :param df: DataFrame\n    :type df: pandas.DataFrame\n    :param level: Level of DataFrame index to extract IDs from\n    :type level: int or str\n    :return: list of string ids\n    \"\"\"\n    protein_list = []\n    for s in df.index.get_level_values(level):\n        protein_list.extend( get_protein_ids(s) )\n\n    return list(set(protein_list))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncombining a list of strings to a single str joined by sep.", "response": "def format_label(sl, fmt=None):\n    \"\"\"\n    Combine a list of strings to a single str, joined by sep.\n    Passes through single strings.\n    :param sl:\n    :return:\n    \"\"\"\n    if isinstance(sl, str):\n        # Already is a string.\n        return sl\n\n    if fmt:\n        return fmt.format(*sl)\n\n    return ' '.join(str(s) for s in sl)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a combined label from a list of indexes into sl by joining them with sep ( str.", "response": "def build_combined_label(sl, idxs, sep=' ', label_format=None):\n    \"\"\"\n    Generate a combined label from a list of indexes\n    into sl, by joining them with `sep` (str).\n\n    :param sl: Strings to combine\n    :type sl: dict of str\n    :param idxs: Indexes into sl\n    :type idxs: list of sl keys\n    :param sep:\n\n    :return: `str` of combined label\n    \"\"\"\n\n    if label_format:\n        return label_format % tuple([get_shortstr(str(sl[n])) for n in idxs])\n    else:\n        return sep.join([get_shortstr(str(sl[n])) for n in idxs])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmatch a key against a dict", "response": "def hierarchical_match(d, k, default=None):\n    \"\"\"\n    Match a key against a dict, simplifying element at a time\n\n\n    :param df: DataFrame\n    :type df: pandas.DataFrame\n    :param level: Level of DataFrame index to extract IDs from\n    :type level: int or str\n    :return: hiearchically matched value or default\n    \"\"\"\n    if d is None:\n        return default\n\n    if type(k) != list and type(k) != tuple:\n        k = [k]\n\n    for n, _ in enumerate(k):\n        key = tuple(k[0:len(k)-n])\n        if len(key) == 1:\n            key = key[0]\n\n        try:\n            d[key]\n        except:\n            pass\n        else:\n            return d[key]\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nseparate seq into num series of as - near - as possible equal length values.", "response": "def chunks(seq, num):\n    \"\"\"\n    Separate `seq` (`np.array`) into `num` series of as-near-as possible equal\n    length values.\n\n    :param seq: Sequence to split\n    :type seq: np.array\n    :param num: Number of parts to split sequence into\n    :type num: int\n    :return: np.array of split parts\n    \"\"\"\n\n    avg = len(seq) / float(num)\n    out = []\n    last = 0.0\n\n    while last < len(seq):\n        out.append(seq[int(last):int(last + avg)])\n        last += avg\n\n    return np.array(out)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_s0_curve(s0, minpval, maxpval, minratio, maxratio, curve_interval=0.1):\n\n    mminpval = -np.log10(minpval)\n    mmaxpval = -np.log10(maxpval)\n    maxpval_adjust = mmaxpval - mminpval\n\n    ax0 = (s0 + maxpval_adjust * minratio) / maxpval_adjust\n    edge_offset = (maxratio-ax0) % curve_interval\n    max_x = maxratio-edge_offset\n\n\n    if (max_x > ax0):\n        x = np.arange(ax0, max_x, curve_interval)\n    else:\n        x = np.arange(max_x, ax0, curve_interval)\n\n    fn = lambda x: 10 ** (-s0/(x-minratio) - mminpval)\n    y = fn(x)\n\n    return x, y, fn", "response": "Calculate the s0 curve for volcano plot."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef correlation(df, rowvar=False):\n\n    # Create a correlation matrix for all correlations\n    # of the columns (filled with na for all values)\n    df = df.copy()\n    maskv = np.ma.masked_where(np.isnan(df.values), df.values)\n    cdf = np.ma.corrcoef(maskv, rowvar=False)\n    cdf = pd.DataFrame(np.array(cdf))\n    cdf.columns = df.columns\n    cdf.index = df.columns\n    cdf = cdf.sort_index(level=0, axis=1)\n    cdf = cdf.sort_index(level=0)\n    return cdf", "response": "Calculates column - wise Pearson correlations using numpy. ma. corrcoef"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pca(df, n_components=2, mean_center=False, **kwargs):\n\n    if not sklearn:\n        assert('This library depends on scikit-learn (sklearn) to perform PCA analysis')\n        \n    from sklearn.decomposition import PCA\n\n    df = df.copy()\n    \n    # We have to zero fill, nan errors in PCA\n    df[ np.isnan(df) ] = 0\n\n    if mean_center:\n        mean = np.mean(df.values, axis=0)\n        df = df - mean\n\n    pca = PCA(n_components=n_components, **kwargs)\n    pca.fit(df.values.T)\n\n    scores = pd.DataFrame(pca.transform(df.values.T)).T\n    scores.index = ['Principal Component %d (%.2f%%)' % ( (n+1), pca.explained_variance_ratio_[n]*100 ) for n in range(0, scores.shape[0])]\n    scores.columns = df.columns\n\n    weights = pd.DataFrame(pca.components_).T\n    weights.index = df.index\n    weights.columns = ['Weights on Principal Component %d' % (n+1) for n in range(0, weights.shape[1])]\n       \n    return scores, weights", "response": "Principal Component Analysis based on sklearn. decomposition. PCA"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plsda(df, a, b, n_components=2, mean_center=False, scale=True, **kwargs):\n\n    if not sklearn:\n        assert('This library depends on scikit-learn (sklearn) to perform PLS-DA')\n\n    from sklearn.cross_decomposition import PLSRegression\n\n    df = df.copy()\n\n    # We have to zero fill, nan errors in PLSRegression\n    df[ np.isnan(df) ] = 0\n\n    if mean_center:\n        mean = np.mean(df.values, axis=0)\n        df = df - mean\n\n    sxa, _ = df.columns.get_loc_level(a)\n    sxb, _ = df.columns.get_loc_level(b)\n\n    dfa = df.iloc[:, sxa]\n    dfb = df.iloc[:, sxb]\n\n    dff = pd.concat([dfa, dfb], axis=1)\n    y = np.ones(dff.shape[1])\n    y[np.arange(dfa.shape[1])] = 0\n\n    plsr = PLSRegression(n_components=n_components, scale=scale, **kwargs)\n    plsr.fit(dff.values.T, y)\n\n    # Apply the generated model to the original data\n    x_scores = plsr.transform(df.values.T)\n\n    scores = pd.DataFrame(x_scores.T)\n    scores.index = ['Latent Variable %d' % (n+1) for n in range(0, scores.shape[0])]\n    scores.columns = df.columns\n\n    weights = pd.DataFrame(plsr.x_weights_)\n    weights.index = df.index\n    weights.columns = ['Weights on Latent Variable %d' % (n+1) for n in range(0, weights.shape[1])]\n\n    loadings = pd.DataFrame(plsr.x_loadings_)\n    loadings.index = df.index\n    loadings.columns = ['Loadings on Latent Variable %d' % (n+1) for n in range(0, loadings.shape[1])]\n\n    return scores, weights, loadings", "response": "Binary group partial least squares discriminant analysis on the supplied dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plsr(df, v, n_components=2, mean_center=False, scale=True, **kwargs):\n\n    if not sklearn:\n        assert('This library depends on scikit-learn (sklearn) to perform PLS-DA')\n\n    from sklearn.cross_decomposition import PLSRegression\n\n    df = df.copy()\n\n    # We have to zero fill, nan errors in PLSRegression\n    df[ np.isnan(df) ] = 0\n\n    if mean_center:\n        mean = np.mean(df.values, axis=0)\n        df = df - mean\n\n    #TODO: Extract values if v is DataFrame?\n\n    plsr = PLSRegression(n_components=n_components, scale=scale, **kwargs)\n    plsr.fit(df.values.T, v)\n\n    scores = pd.DataFrame(plsr.x_scores_.T)\n    scores.index = ['Latent Variable %d' % (n+1) for n in range(0, scores.shape[0])]\n    scores.columns = df.columns\n\n    weights = pd.DataFrame(plsr.x_weights_)\n    weights.index = df.index\n    weights.columns = ['Weights on Latent Variable %d' % (n+1) for n in range(0, weights.shape[1])]\n\n    loadings = pd.DataFrame(plsr.x_loadings_)\n    loadings.index = df.index\n    loadings.columns = ['Loadings on Latent Variable %d' % (n+1) for n in range(0, loadings.shape[1])]\n\n    #r = plsr.score(df.values.T, v)\n    predicted = plsr.predict(df.values.T)\n\n    return scores, weights, loadings, predicted", "response": "Partial Least Squares Regression Analysis based on sklearn. cross_decomposition. PLSRegression."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enrichment_from_evidence(dfe, modification=\"Phospho (STY)\"):\n\n    dfe = dfe.reset_index().set_index('Experiment')\n\n    dfe['Modifications'] = np.array([modification in m for m in dfe['Modifications']])\n    dfe = dfe.set_index('Modifications', append=True)\n\n    dfes = dfe.sum(axis=0, level=[0,1]).T\n\n    columns = dfes.sum(axis=1, level=0).columns\n\n    total = dfes.sum(axis=1, level=0).values.flatten() # Total values\n    modified = dfes.iloc[0, dfes.columns.get_level_values('Modifications').values ].values # Modified\n    enrichment = modified / total\n\n    return pd.DataFrame([enrichment], columns=columns, index=['% Enrichment'])", "response": "Calculate relative enrichment of peptide modifications from an evidence. txt file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enrichment_from_msp(dfmsp, modification=\"Phospho (STY)\"):\n\n    dfmsp['Modifications'] = np.array([modification in m for m in dfmsp['Modifications']])\n    dfmsp = dfmsp.set_index(['Modifications'])\n    dfmsp = dfmsp.filter(regex='Intensity ')\n\n    dfmsp[ dfmsp == 0] = np.nan\n    df_r = dfmsp.sum(axis=0, level=0)\n\n    modified = df_r.loc[True].values\n    total = df_r.sum(axis=0).values\n    enrichment = modified / total\n\n    return pd.DataFrame([enrichment], columns=dfmsp.columns, index=['% Enrichment'])", "response": "Calculate relative enrichment of peptide modifications from modificationSpecificPeptides. txt."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sitespeptidesproteins(df, site_localization_probability=0.75):\n\n    sites = filters.filter_localization_probability(df, site_localization_probability)['Sequence window']\n    peptides = set(df['Sequence window'])\n    proteins = set([str(p).split(';')[0] for p in df['Proteins']])\n    return len(sites), len(peptides), len(proteins)", "response": "Generates summary count of modified sites peptides and proteins in a processed dataset DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef modifiedaminoacids(df):\n\n    amino_acids = list(df['Amino acid'].values)\n    aas = set(amino_acids)\n    quants = {}\n\n    for aa in aas:\n        quants[aa] = amino_acids.count(aa)\n        \n    total_aas = len(amino_acids)\n\n    return total_aas, quants", "response": "Calculate the number of modified amino acids in supplied DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef go_enrichment(df, enrichment='function', organism='Homo sapiens', summary=True, fdr=0.05, ids_from=['Proteins','Protein IDs']):\n\n    if isinstance(df, pd.DataFrame) or isinstance(df, pd.Series):\n        l = list(set(ids_from) & set(df.index.names))[0]\n        data = \"\\n\".join([get_protein_id(s) for s in df.index.get_level_values(l)])\n    else:\n        data = \"\\n\".join([get_protein_id(s) for s in df])\n\n    r = requests.post(\"http://www.pantherdb.org/webservices/garuda/tools/enrichment/VER_2/enrichment.jsp\", data={\n            'organism': organism,\n            'type': 'enrichment',\n            'enrichmentType': enrichment},\n            files = {\n            'geneList': ('genelist.txt', StringIO(data) ),\n\n            }\n        )\n\n    try:\n        go = pd.read_csv(StringIO(r.text), sep='\\t', skiprows=5, lineterminator='\\n', header=None)\n    except ValueError:\n        return None\n\n    go.columns = [\"GO\", \"Name\", \"Gene ID\", \"P\", \"FDR\"]\n    go = go.set_index([\"GO\", \"Name\"])\n    if summary:\n        go = go.drop(\"Gene ID\", axis=1).mean(axis=0, level=[\"GO\",\"Name\"])\n\n    if fdr:\n        go = go[ go[\"P\"] < fdr ]\n\n    return go.sort_values(by=\"P\", ascending=True)", "response": "Calculate the GO enrichment for a specified set of indices using the PantherDB GO enrichment service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform an anova on a dataframe containing the selected groups and PT and sig values for the analysis of Variation.", "response": "def anova_1way(df, *args, **kwargs):\n    \"\"\"\n    Perform Analysis of Variation (ANOVA) on provided dataframe\n    and for specified groups. Groups for analysis can be specified\n    as individual arguments, e.g.\n    \n    anova(df, \"Group A\", \"Group B\")\n    anova(df, (\"Group A\", 5), (\"Group B\", 5))\n    \n    At least 2 groups must be provided.\n    \n    :return: Dataframe containing selected groups and P/T/sig value for the comparisons.\n    \n    \"\"\"\n    if len(args) < 2:\n        raise Exception(\"Not enough arguments. Provide at least two group/indexes for comparison.\")\n    \n    # Select columns for test\n    df = df[list(args)]\n    fdr = kwargs.get('fdr', 0.05)\n    \n    pv = []\n    tv = []\n    \n    for n in range(df.shape[0]):\n        \n        data = []    \n        \n        for idx in args:\n        \n            dv = df.iloc[n][idx].values\n            dv = np.ma.masked_where(np.isnan(dv), dv)\n        \n            data.append(dv)\n        \n        # Calculate the p value between two groups (t-test)\n        t, p = sp.stats.mstats.f_oneway(*data)\n        \n        pv.append(p)\n        tv.append(t)\n    \n    df['ANOVA p'] =  pv\n    df['ANOVA t'] =  tv\n    df['ANOVA sig'] =  np.array(pv) <= fdr\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a MultiIndex from a design table.", "response": "def build_index_from_design(df, design, remove_prefix=None, types=None, axis=1, auto_convert_numeric=True, unmatched_columns='index'):\n    \"\"\"\n    Build a MultiIndex from a design table.\n\n    Supply with a table with column headings for the new multiindex\n    and a index containing the labels to search for in the data.\n\n    :param df:\n    :param design:\n    :param remove:\n    :param types:\n    :param axis:\n    :param auto_convert_numeric:\n    :return:\n    \"\"\"\n\n    df = df.copy()\n    if 'Label' not in design.index.names:\n        design = design.set_index('Label')\n\n    if remove_prefix is None:\n        remove_prefix = []\n    if type(remove_prefix) is str:\n        remove_prefix=[remove_prefix]\n\n    unmatched_for_index = []\n\n    names = design.columns.values\n    idx_levels = len(names)\n    indexes = []\n    \n    # Convert numeric only columns_to_combine; except index\n    if auto_convert_numeric:\n        design = design.apply(pd.to_numeric, errors=\"ignore\")\n        # The match columns are always strings, so the index must also be\n        design.index = design.index.astype(str)\n    \n    # Apply type settings\n    if types:\n        for n, t in types.items():\n            if n in design.columns.values:\n                design[n] = design[n].astype(t)\n    \n    # Build the index\n    for lo in df.columns.values:\n\n        l = copy(lo)\n        for s in remove_prefix:\n            l = l.replace(s, '')\n\n        # Remove trailing/forward spaces\n        l = l.strip()\n        # Convert to numeric if possible\n        l = numeric(l)\n        # Attempt to match to the labels\n\n        try:\n            # Index\n            idx = design.loc[str(l)]\n\n        except:\n            if unmatched_columns:\n                unmatched_for_index.append(lo)\n            else:\n                # No match, fill with None\n                idx = tuple([None] * idx_levels)\n                indexes.append(idx)\n\n        else:\n            # We have a matched row, store it\n            idx = tuple(idx.values)\n            indexes.append(idx)\n\n    if axis == 0:\n        df.index = pd.MultiIndex.from_tuples(indexes, names=names)\n    else:\n\n        # If using unmatched for index, append\n        if unmatched_columns == 'index':\n            df = df.set_index(unmatched_for_index, append=True)\n\n        elif unmatched_columns == 'drop':\n            df = df.drop(unmatched_for_index, axis=1)\n\n        df.columns = pd.MultiIndex.from_tuples(indexes, names=names)\n\n    df = df.sort_index(axis=1)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_index_from_labels(df, indices, remove_prefix=None, types=None, axis=1):\n\n    df = df.copy()\n\n    if remove_prefix is None:\n        remove_prefix = []\n\n    if types is None:\n        types = {}\n\n    idx = [df.index, df.columns][axis]\n\n    indexes = []\n\n    for l in idx.get_level_values(0):\n\n        for s in remove_prefix:\n            l = l.replace(s+\" \", '')\n\n        ixr = []\n        for n, m in indices:\n            m = re.search(m, l)\n            if m:\n                r = m.group(1)\n\n                if n in types:\n                    # Map this value to a new type\n                    r = types[n](r)\n            else:\n                r = None\n\n            ixr.append(r)\n        indexes.append( tuple(ixr) )\n\n    if axis == 0:\n        df.index = pd.MultiIndex.from_tuples(indexes, names=[n for n, _ in indices])\n    else:\n        df.columns = pd.MultiIndex.from_tuples(indexes, names=[n for n, _ in indices])\n\n    return df", "response": "Build a MultiIndex from a list of labels and matching regex to a sample label\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncombines expression columns into a single column.", "response": "def combine_expression_columns(df, columns_to_combine, remove_combined=True):\n\n    \"\"\"\n    Combine expression columns, calculating the mean for 2 columns\n\n\n    :param df: Pandas dataframe\n    :param columns_to_combine: A list of tuples containing the column names to combine\n    :return:\n    \"\"\"\n\n    df = df.copy()\n\n    for ca, cb in columns_to_combine:\n        df[\"%s_(x+y)/2_%s\" % (ca, cb)] = (df[ca] + df[cb]) / 2\n\n    if remove_combined:\n        for ca, cb in columns_to_combine:\n            df.drop([ca, cb], inplace=True, axis=1)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms equivalent of expand side table in Perseus by folding multiplicity columns down onto duplicate rows.", "response": "def expand_side_table(df):\n    \"\"\"\n    Perform equivalent of 'expand side table' in Perseus by folding\n    Multiplicity columns down onto duplicate rows\n\n    The id is remapped to UID___Multiplicity, which\n    is different to Perseus behaviour, but prevents accidental of\n    non-matching rows from occurring later in analysis.\n\n    :param df:\n    :return:\n    \"\"\"\n\n    df = df.copy()\n\n    idx = df.index.names\n    df.reset_index(inplace=True)\n\n    def strip_multiplicity(df):\n        df.columns = [c[:-4] for c in df.columns]\n        return df\n        \n    def strip_multiple(s):\n        for sr in ['___1','___2','___3']:\n            if s.endswith(sr):\n                s = s[:-4]\n        return s\n\n    base = df.filter(regex='.*(?<!___\\d)$')\n    \n    # Remove columns that will match ripped multiplicity columns\n    for c in df.columns.values:\n        if strip_multiple(c) != c and strip_multiple(c) in list(base.columns.values):\n            base.drop(strip_multiple(c), axis=1, inplace=True)\n\n    multi1 = df.filter(regex='^.*___1$')\n    multi1 = strip_multiplicity(multi1)\n    multi1['Multiplicity'] = '___1'\n    multi1 = pd.concat([multi1, base], axis=1)\n\n    multi2 = df.filter(regex='^.*___2$')\n    multi2 = strip_multiplicity(multi2)\n    multi2['Multiplicity'] = '___2'\n    multi2 = pd.concat([multi2, base], axis=1)\n\n    multi3 = df.filter(regex='^.*___3$')\n    multi3 = strip_multiplicity(multi3)\n    multi3['Multiplicity'] = '___3'\n    multi3 = pd.concat([multi3, base], axis=1)\n\n    df = pd.concat([multi1, multi2, multi3], axis=0)\n    df['id'] = [\"%s%s\" % (a, b) for a, b in zip(df['id'], df['Multiplicity'])]\n\n    if idx[0] is not None:\n        df.set_index(idx, inplace=True)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_experimental_design(df, f, prefix='Intensity '):\n\n    df = df.copy()\n\n    edt = pd.read_csv(f, sep='\\t', header=0)\n\n    edt.set_index('Experiment', inplace=True)\n\n    new_column_labels = []\n    for l in df.columns.values:\n        try:\n            l = edt.loc[l.replace(prefix, '')]['Name']\n        except (IndexError, KeyError):\n            pass\n\n        new_column_labels.append(l)\n\n    df.columns = new_column_labels\n    return df", "response": "Load the experimental design template from MaxQuant and use it to apply the label names to the data columns."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply transformation to expression columns.", "response": "def transform_expression_columns(df, fn=np.log2, prefix='Intensity '):\n    \"\"\"\n    Apply transformation to expression columns.\n\n    Default is log2 transform to expression columns beginning with Intensity\n\n\n    :param df:\n    :param prefix: The column prefix for expression columns\n    :return:\n    \"\"\"\n    df = df.copy()\n\n    mask = np.array([l.startswith(prefix) for l in df.columns.values])\n    df.iloc[:, mask] = fn(df.iloc[:, mask])\n\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    \n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fold_columns_to_rows(df, levels_from=2):\n    \n    df = df.copy()\n    df.reset_index(inplace=True, drop=True) # Wipe out the current index\n    df = df.T\n    \n    # Build all index combinations\n\n    a = [list( set( df.index.get_level_values(i) ) ) for i in range(0, levels_from)]\n    combinations = list(itertools.product(*a))\n    \n    names = df.index.names[:levels_from]\n    \n    concats = []\n    for c in combinations:\n        \n        try:\n            dfcc = df.loc[c]\n\n        except KeyError:\n            continue\n\n        else:\n            # Silly pandas\n            if len(dfcc.shape) == 1:\n                continue\n        \n            dfcc.columns = pd.MultiIndex.from_tuples([c]*dfcc.shape[1], names=names)\n            concats.append(dfcc)\n\n    # Concatenate\n    dfc = pd.concat(concats, axis=1)\n    dfc.sort_index(axis=1, inplace=True)\n\n    # Fix name if collapsed\n    if dfc.index.name is None:\n        dfc.index.name = df.index.names[-1]\n\n    return dfc", "response": "Folds columns from the columns and folds down into the row index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_argument(self, arg_name, arg_value):\n        '''Add an additional argument to be passed to the fitness function\n        via additional arguments dictionary; this argument/value is not tuned\n\n        Args:\n            arg_name (string): name/dictionary key of argument\n            arg_value (any): dictionary value of argument\n        '''\n\n        if len(self._employers) > 0:\n            self._logger.log(\n                'warn',\n                'Adding an argument after the employers have been created'\n            )\n        if self._args is None:\n            self._args = {}\n        self._args[arg_name] = arg_value", "response": "Add an additional argument to be passed to the fitness function\n            via additional arguments dictionary ; this argument is not tuned\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a tunable value to the ABC", "response": "def add_value(self, value_type, value_min, value_max):\n        '''Add a tunable value to the ABC (fitness function must be\n        configured to handle it)\n\n        Args:\n            value_type (string): type of the value, 'int' or 'float'\n            value_min (int or float): minimum bound for the value\n            value_max (int or float): maximum bound for the value\n\n        Returns:\n            None\n        '''\n\n        if len(self._employers) > 0:\n            self._logger.log(\n                'warn',\n                'Adding a value after employers have been created'\n            )\n        value = (value_type,  (value_min, value_max))\n        self._value_ranges.append(value)\n        self._limit = self._num_employers*len(self._value_ranges)\n        self._logger.log(\n            'debug',\n            'Limit set to {}'.format(self._limit)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef args(self, args):\n        '''Set additional arguments to be passed to the fitness function\n\n        Args:\n            args (dict): additional arguments\n        '''\n        self._args = args\n        self._logger.log('debug', 'Args set to {}'.format(args))", "response": "Set additional arguments to be passed to the fitness function\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minimize(self, minimize):\n        '''Configures the ABC to minimize fitness function return value or\n        derived score\n\n        Args:\n            minimize (bool): if True, minimizes fitness function return value;\n                if False, minimizes derived score\n        '''\n\n        self._minimize = minimize\n        self._logger.log('debug', 'Minimize set to {}'.format(minimize))", "response": "Configures the ABC to minimize fitness function return value or derived score\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef num_employers(self, num_employers):\n        '''Sets the number of employer bees; at least two are required\n\n        Args:\n            num_employers (int): number of employer bees\n        '''\n\n        if num_employers < 2:\n            self._logger.log(\n                'warn',\n                'Two employers are needed: setting to two'\n            )\n            num_employers = 2\n        self._num_employers = num_employers\n        self._logger.log('debug', 'Number of employers set to {}'.format(\n            num_employers\n        ))\n        self._limit = num_employers * len(self._value_ranges)\n        self._logger.log('debug', 'Limit set to {}'.format(self._limit))", "response": "Sets the number of employers in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef value_ranges(self, value_ranges):\n        '''Set the types, min/max values for tunable parameters\n\n        Args:\n            value_ranges (list): each element defines a tunable variable in\n                the form \"(type ('int' or 'float'), (min_val, max_val))\";\n                initial, random values for each bee will between \"min_val\" and\n                \"max_val\"\n        '''\n\n        self._value_ranges = value_ranges\n        self._logger.log('debug', 'Value ranges set to {}'.format(\n            value_ranges\n        ))", "response": "Set the types min max values for tunable parameters\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the number of concurrent processes the ABC will utilize for the fitness function evaluation ; if > 1 single process is used otherwise single process is used.", "response": "def processes(self, processes):\n        '''Set the number of concurrent processes the ABC will utilize for\n        fitness function evaluation; if <= 1, single process is used\n\n        Args:\n            processes (int): number of concurrent processes\n        '''\n\n        if self._processes > 1:\n            self._pool.close()\n            self._pool.join()\n            self._pool = multiprocessing.Pool(processes)\n        else:\n            self._pool = None\n        self._logger.log('debug', 'Number of processes set to {}'.format(\n            processes\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_process_count(self):\n        '''Infers the number of CPU cores in the current system, sets the\n        number of concurrent processes accordingly\n        '''\n\n        try:\n            self.processes = multiprocessing.cpu_count()\n        except NotImplementedError:\n            self._logger.log(\n                'error',\n                'Could infer CPU count, setting number of processes back to 4'\n            )\n            self.processes = 4", "response": "Infers the number of CPU cores in the current system sets the\n        number of concurrent processes accordingly"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_employers(self):\n        '''Generate employer bees. This should be called directly after the\n        ABC is initialized.\n        '''\n\n        self.__verify_ready(True)\n        employers = []\n        for i in range(self._num_employers):\n            employer = EmployerBee(self.__gen_random_values())\n            if self._processes <= 1:\n                employer.error = self._fitness_fxn(\n                    employer.values, **self._args\n                )\n                employer.score = employer.get_score()\n                if np.isnan(employer.score):\n                    self._logger.log('warn', 'NaN bee score: {}, {}'.format(\n                        employer.id, employer.score\n                    ))\n                self._logger.log('debug', 'Bee number {} created'.format(\n                    i + 1\n                ))\n                self.__update(employer.score, employer.values, employer.error)\n            else:\n                employer.error = self._pool.apply_async(\n                    self._fitness_fxn,\n                    [employer.values],\n                    self._args\n                )\n                employers.append(employer)\n            self._employers.append(employer)\n        for idx, employer in enumerate(employers):\n            try:\n                employer.error = employer.error.get()\n                employer.score = employer.get_score()\n                if np.isnan(employer.score):\n                    self._logger.log('warn', 'NaN bee score: {}, {}'.format(\n                        employer.id, employer.score\n                    ))\n                self._logger.log('debug', 'Bee number {} created'.format(\n                    i + 1\n                ))\n                self.__update(employer.score, employer.values, employer.error)\n            except Exception as e:\n                raise e\n        self._logger.log('debug', 'Employer creation complete')", "response": "Generate and create the list of Employer objects. This method is called by the process method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a single iteration of the ABC.", "response": "def run_iteration(self):\n        '''Runs a single iteration of the ABC; employer phase -> probability\n        calculation -> onlooker phase -> check positions\n        '''\n\n        self._employer_phase()\n        self._calc_probability()\n        self._onlooker_phase()\n        self._check_positions()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _employer_phase(self):\n        '''Iterates through the employer bees and merges each with another\n        random bee (one value is moved in accordance with the second bee's\n        value); if the mutation performs better, the bee is moved to the new\n        position\n        '''\n\n        self._logger.log('debug', 'Employer bee phase')\n        modified = []\n        for bee in self._employers:\n            if self._processes <= 1:\n                new_values = self._merge_bee(bee)\n                self._move_bee(bee, new_values)\n            else:\n                modified.append((\n                    bee,\n                    self._pool.apply_async(self._merge_bee, [bee])\n                ))\n        for pair in modified:\n            self._move_bee(pair[0], pair[1].get())", "response": "This method is called by the process when the employer is started."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calc_probability(self):\n        '''Determines the probability that each bee will be chosen during the\n        onlooker phase; also determines if a new best-performing bee is found\n        '''\n\n        self._logger.log('debug', 'Calculating bee probabilities')\n        self.__verify_ready()\n        self._total_score = 0\n        for employer in self._employers:\n            self._total_score += employer.score\n            if self.__update(employer.score, employer.values, employer.error):\n                self._logger.log(\n                    'info',\n                    'Update to best performer -'\n                    ' error: {} | score: {} | values: {}'.format(\n                        employer.error,\n                        employer.score,\n                        employer.values\n                    )\n                )\n        for employer in self._employers:\n            employer.calculate_probability(self._total_score)", "response": "Determines the probability that each bee will be chosen during the onlooker phase ; also determines if a new best - performing bee is found\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_positions(self):\n        '''Checks each bee to see if it abandons its current food source (has\n        not found a better one in self._limit iterations); if abandoning, it\n        becomes a scout and generates a new, random food source\n        '''\n\n        self.__verify_ready()\n        max_trials = 0\n        scout = None\n        for bee in self._employers:\n            if (bee.failed_trials >= max_trials):\n                max_trials = bee.failed_trials\n                scout = bee\n        if scout is not None and scout.failed_trials > self._limit:\n            self._logger.log(\n                'debug',\n                'Sending scout (error of {} with limit of {})'.format(\n                    scout.error, scout.failed_trials\n                )\n            )\n            scout.values = self.__gen_random_values()", "response": "Checks each bee to see if it abandons its current food source and generates a new random food source."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_bee(self, bee):\n        '''Shifts a random value for a supplied bee with in accordance with\n        another random bee's value\n\n        Args:\n            bee (EmployerBee): supplied bee to merge\n\n        Returns:\n            tuple: (score of new position, values of new position, fitness\n                function return value of new position)\n        '''\n\n        random_dimension = randint(0, len(self._value_ranges) - 1)\n        second_bee = randint(0, self._num_employers - 1)\n        while (bee.id == self._employers[second_bee].id):\n            second_bee = randint(0, self._num_employers - 1)\n        new_bee = deepcopy(bee)\n        new_bee.values[random_dimension] = self.__onlooker.calculate_positions(\n            new_bee.values[random_dimension],\n            self._employers[second_bee].values[random_dimension],\n            self._value_ranges[random_dimension]\n        )\n        fitness_score = new_bee.get_score(self._fitness_fxn(\n            new_bee.values,\n            **self._args\n        ))\n        return (fitness_score, new_bee.values, new_bee.error)", "response": "Shifts a random value for a supplied bee with in accordance with\n        another random bee s value\n            tuple Returns fitness score values fitness_fxn function return value of new position"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __gen_random_values(self):\n        '''Generate random values based on supplied value ranges\n\n        Returns:\n            list: random values, one per tunable variable\n        '''\n\n        values = []\n        if self._value_ranges is None:\n            self._logger.log(\n                'crit',\n                'Must set the type/range of possible values'\n            )\n            raise RuntimeError(\"Must set the type/range of possible values\")\n        else:\n            for t in self._value_ranges:\n                if t[0] == 'int':\n                    values.append(randint(t[1][0], t[1][1]))\n                elif t[0] == 'float':\n                    values.append(np.random.uniform(t[1][0], t[1][1]))\n                else:\n                    self._logger.log(\n                        'crit',\n                        'Value type must be either an `int` or a `float`'\n                    )\n                    raise RuntimeError(\n                        'Value type must be either an `int` or a `float`'\n                    )\n        return values", "response": "Generate random values based on supplied value ranges\nAttributeNames Returns a list of random values one per tunable variable\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __verify_ready(self, creating=False):\n        '''Some cleanup, ensures that everything is set up properly to avoid\n        random errors during execution\n\n        Args:\n            creating (bool): True if currently creating employer bees, False\n                for checking all other operations\n        '''\n\n        if len(self._value_ranges) == 0:\n            self._logger.log(\n                'crit',\n                'Attribute value_ranges must have at least one value'\n            )\n            raise RuntimeWarning(\n                'Attribute value_ranges must have at least one value'\n            )\n        if len(self._employers) == 0 and creating is False:\n            self._logger.log('crit', 'Need to create employers')\n            raise RuntimeWarning('Need to create employers')", "response": "Some cleanup ensures that everything is set up properly to avoid random errors during execution"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting settings from a JSON file.", "response": "def import_settings(self, filename):\n        '''Import settings from a JSON file\n\n        Args:\n            filename (string): name of the file to import from\n        '''\n\n        if not os.path.isfile(filename):\n            self._logger.log(\n                'error',\n                'File: {} not found, continuing with default settings'.format(\n                    filename\n                )\n            )\n        else:\n            with open(filename, 'r') as jsonFile:\n                data = json.load(jsonFile)\n                self._value_ranges = data['valueRanges']\n                self._best_values = data['best_values']\n                self._best_values = []\n                for index, value in enumerate(data['best_values']):\n                    if self._value_ranges[index] == 'int':\n                        self._best_values.append(int(value))\n                    else:\n                        self._best_values.append(float(value))\n                self.minimize = data['minimize']\n                self.num_employers = data['num_employers']\n                self._best_score = float(data['best_score'])\n                self.limit = data['limit']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the settings to a JSON file", "response": "def save_settings(self, filename):\n        '''Save settings to a JSON file\n\n        Arge:\n            filename (string): name of the file to save to\n        '''\n\n        data = dict()\n        data['valueRanges'] = self._value_ranges\n        data['best_values'] = [str(value) for value in self._best_values]\n        data['minimize'] = self._minimize\n        data['num_employers'] = self._num_employers\n        data['best_score'] = str(self._best_score)\n        data['limit'] = self._limit\n        data['best_error'] = self._best_error\n        with open(filename, 'w') as outfile:\n            json.dump(data, outfile, indent=4, sort_keys=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate bee s fitness score given a value returned by the fitness function .", "response": "def get_score(self, error=None):\n        '''Calculate bee's fitness score given a value returned by the fitness\n        function\n\n        Args:\n            error (float): value returned by the fitness function\n\n        Returns:\n            float: derived fitness score\n        '''\n\n        if error is not None:\n            self.error = error\n        if self.error >= 0:\n            return 1 / (self.error + 1)\n        else:\n            return 1 + abs(self.error)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_positions(self, first_bee_val, second_bee_val, value_range):\n        '''Calculate the new value/position for two given bee values\n\n        Args:\n            first_bee_val (int or float): value from the first bee\n            second_bee_val (int or float): value from the second bee\n            value_ranges (tuple): \"(value type, (min_val, max_val))\" for the\n                given value\n\n        Returns:\n            int or float: new value\n        '''\n\n        value = first_bee_val + np.random.uniform(-1, 1) \\\n            * (first_bee_val - second_bee_val)\n        if value_range[0] == 'int':\n            value = int(value)\n        if value > value_range[1][1]:\n            value = value_range[1][1]\n        if value < value_range[1][0]:\n            value = value_range[1][0]\n\n        return value", "response": "Calculate the new value and position for two given bee\n            values"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates DNS entries in route53.", "response": "def create_elb_dns(self, regionspecific=False):\n        \"\"\"Create dns entries in route53.\n\n        Args:\n            regionspecific (bool): The DNS entry should have region on it\n        Returns:\n            str: Auto-generated DNS name for the Elastic Load Balancer.\n\n        \"\"\"\n        if regionspecific:\n            dns_elb = self.generated.dns()['elb_region']\n        else:\n            dns_elb = self.generated.dns()['elb']\n\n        dns_elb_aws = find_elb(name=self.app_name, env=self.env, region=self.region)\n\n        zone_ids = get_dns_zone_ids(env=self.env, facing=self.elb_subnet)\n\n        self.log.info('Updating Application URL: %s', dns_elb)\n\n        dns_kwargs = {\n            'dns_name': dns_elb,\n            'dns_name_aws': dns_elb_aws,\n            'dns_ttl': self.dns_ttl,\n        }\n\n        for zone_id in zone_ids:\n            self.log.debug('zone_id: %s', zone_id)\n            update_dns_zone_record(self.env, zone_id, **dns_kwargs)\n\n        return dns_elb"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate DNS entries in route53 for multiregion failover setups.", "response": "def create_failover_dns(self, primary_region='us-east-1'):\n        \"\"\"Create dns entries in route53 for multiregion failover setups.\n\n        Args:\n            primary_region (str): primary AWS region for failover\n        Returns:\n            Auto-generated DNS name.\n        \"\"\"\n        dns_record = self.generated.dns()['global']\n        zone_ids = get_dns_zone_ids(env=self.env, facing=self.elb_subnet)\n\n        elb_dns_aws = find_elb(name=self.app_name, env=self.env, region=self.region)\n        elb_dns_zone_id = find_elb_dns_zone_id(name=self.app_name, env=self.env, region=self.region)\n\n        if primary_region in elb_dns_aws:\n            failover_state = 'PRIMARY'\n        else:\n            failover_state = 'SECONDARY'\n        self.log.info(\"%s set as %s record\", elb_dns_aws, failover_state)\n\n        self.log.info('Updating Application Failover URL: %s', dns_record)\n\n        dns_kwargs = {\n            'dns_name': dns_record,\n            'elb_dns_zone_id': elb_dns_zone_id,\n            'elb_aws_dns': elb_dns_aws,\n            'dns_ttl': self.dns_ttl,\n            'failover_state': failover_state,\n        }\n\n        for zone_id in zone_ids:\n            self.log.debug('zone_id: %s', zone_id)\n            update_failover_dns_record(self.env, zone_id, **dns_kwargs)\n\n        return dns_record"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_listeners(elb_settings=None, env='dev', region='us-east-1'):\n    LOG.debug('ELB settings:\\n%s', elb_settings)\n\n    credential = get_env_credential(env=env)\n    account = credential['accountId']\n\n    listeners = []\n\n    if 'ports' in elb_settings:\n        for listener in elb_settings['ports']:\n            cert_name = format_cert_name(\n                env=env, region=region, account=account, certificate=listener.get('certificate', None))\n\n            lb_proto, lb_port = listener['loadbalancer'].split(':')\n            i_proto, i_port = listener['instance'].split(':')\n            listener_policies = listener.get('policies', [])\n            listener_policies += listener.get('listener_policies', [])\n            backend_policies = listener.get('backend_policies', [])\n\n            elb_data = {\n                'externalPort': int(lb_port),\n                'externalProtocol': lb_proto.upper(),\n                'internalPort': int(i_port),\n                'internalProtocol': i_proto.upper(),\n                'sslCertificateId': cert_name,\n                'listenerPolicies': listener_policies,\n                'backendPolicies': backend_policies,\n            }\n\n            listeners.append(elb_data)\n    else:\n        listener_policies = elb_settings.get('policies', [])\n        listener_policies += elb_settings.get('listener_policies', [])\n        backend_policies = elb_settings.get('backend_policies', [])\n\n        listeners = [{\n            'externalPort': int(elb_settings['lb_port']),\n            'externalProtocol': elb_settings['lb_proto'],\n            'internalPort': int(elb_settings['i_port']),\n            'internalProtocol': elb_settings['i_proto'],\n            'sslCertificateId': elb_settings['certificate'],\n            'listenerPolicies': listener_policies,\n            'backendPolicies': backend_policies,\n        }]\n\n    for listener in listeners:\n        LOG.info('ELB Listener:\\n'\n                 'loadbalancer %(externalProtocol)s:%(externalPort)d\\n'\n                 'instance %(internalProtocol)s:%(internalPort)d\\n'\n                 'certificate: %(sslCertificateId)s\\n'\n                 'listener_policies: %(listenerPolicies)s\\n'\n                 'backend_policies: %(backendPolicies)s', listener)\n    return listeners", "response": "Format ELB Listeners into standard list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats the SSL certificate name into ARN for ELB.", "response": "def format_cert_name(env='', account='', region='', certificate=None):\n    \"\"\"Format the SSL certificate name into ARN for ELB.\n\n    Args:\n        env (str): Account environment name\n        account (str): Account number for ARN\n        region (str): AWS Region.\n        certificate (str): Name of SSL certificate\n\n    Returns:\n        str: Fully qualified ARN for SSL certificate\n        None: Certificate is not desired\n    \"\"\"\n    cert_name = None\n\n    if certificate:\n        if certificate.startswith('arn'):\n            LOG.info(\"Full ARN provided...skipping lookup.\")\n            cert_name = certificate\n        else:\n            generated_cert_name = generate_custom_cert_name(env, region, account, certificate)\n            if generated_cert_name:\n                LOG.info(\"Found generated certificate %s from template\", generated_cert_name)\n                cert_name = generated_cert_name\n            else:\n                LOG.info(\"Using default certificate name logic\")\n                cert_name = ('arn:aws:iam::{account}:server-certificate/{name}'.format(\n                    account=account, name=certificate))\n    LOG.debug('Certificate name: %s', cert_name)\n\n    return cert_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a custom TLS Cert name based on a template.", "response": "def generate_custom_cert_name(env='', region='', account='', certificate=None):\n    \"\"\"Generate a custom TLS Cert name based on a template.\n\n    Args:\n        env (str): Account environment name\n        region (str): AWS Region.\n        account (str): Account number for ARN.\n        certificate (str): Name of SSL certificate.\n\n    Returns:\n        str: Fully qualified ARN for SSL certificate.\n        None: Template doesn't exist.\n    \"\"\"\n    cert_name = None\n    template_kwargs = {'account': account, 'name': certificate}\n\n    # TODO: Investigate moving this to a remote API, then fallback to local file if unable to connect\n    try:\n        rendered_template = get_template(template_file='infrastructure/iam/tlscert_naming.json.j2', **template_kwargs)\n        tlscert_dict = json.loads(rendered_template)\n    except ForemastTemplateNotFound:\n        LOG.info('Unable to find TLS Cert Template...falling back to default logic...')\n        return cert_name\n\n    # TODO: Move to v1 method for check\n    try:\n        LOG.info(\"Attempting to find TLS Cert using TLS Cert Template v1 lookup...\")\n        cert_name = tlscert_dict[env][certificate]\n        LOG.info(\"Found TLS certificate named %s under %s using TLS Cert Template v1\", certificate, env)\n    except KeyError:\n        LOG.error(\"Unable to find TLS certificate named %s under %s using v1 TLS Cert Template.\", certificate, env)\n\n    # TODO: Move variable to consts\n    # TODO: move to v2 method for check\n    tls_services = ['iam', 'acm']\n    if cert_name is None and all(service in tlscert_dict for service in tls_services):\n        LOG.info(\"Attempting to find TLS Cert using TLS Cert Template v2 lookup...\")\n        if certificate in tlscert_dict['iam'][env]:\n            cert_name = tlscert_dict['iam'][env][certificate]\n            LOG.info(\"Found IAM TLS certificate named %s under %s using TLS Cert Template v2\", certificate, env)\n        elif certificate in tlscert_dict['acm'][region][env]:\n            cert_name = tlscert_dict['acm'][region][env][certificate]\n            LOG.info(\"Found ACM TLS certificate named %s under %s in %s using TLS Cert Template v2\", certificate, env,\n                     region)\n        else:\n            LOG.error(\n                \"Unable to find TLS certificate named %s under parent keys [ACM, IAM] %s in v2 TLS Cert Template.\",\n                certificate, env)\n\n    return cert_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    logging.basicConfig(format=LOGGING_FORMAT)\n    log = logging.getLogger(__name__)\n\n    parser = argparse.ArgumentParser()\n    add_debug(parser)\n    add_app(parser)\n    add_env(parser)\n    add_properties(parser)\n\n    args = parser.parse_args()\n\n    logging.getLogger(__package__.split(\".\")[0]).setLevel(args.debug)\n    log.debug('Parsed arguements: %s', args)\n\n    if \"prod\" not in args.env:\n        log.info('No slack message sent, not a production environment')\n    else:\n        log.info(\"Sending slack message, production environment\")\n        slacknotify = SlackNotification(app=args.app, env=args.env, prop_path=args.properties)\n        slacknotify.post_message()", "response": "Send Slack notification to a configured channel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattempt to fully destroy AWS Resources for a Spinnaker Application.", "response": "def main():  # noqa\n    \"\"\"Attempt to fully destroy AWS Resources for a Spinnaker Application.\"\"\"\n    logging.basicConfig(format=LOGGING_FORMAT)\n\n    parser = argparse.ArgumentParser(description=main.__doc__)\n    add_debug(parser)\n    add_app(parser)\n    args = parser.parse_args()\n\n    if args.debug == logging.DEBUG:\n        logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n    else:\n        LOG.setLevel(args.debug)\n\n    for env in ENVS:\n        for region in REGIONS:\n            LOG.info('DESTROY %s:%s', env, region)\n\n            try:\n                destroy_dns(app=args.app, env=env)\n            except botocore.exceptions.ClientError as error:\n                LOG.warning('DNS issue for %s in %s: %s', env, region, error)\n\n            try:\n                destroy_elb(app=args.app, env=env, region=region)\n            except SpinnakerError:\n                pass\n\n            try:\n                destroy_iam(app=args.app, env=env)\n            except botocore.exceptions.ClientError as error:\n                LOG.warning('IAM issue for %s in %s: %s', env, region, error)\n\n            try:\n                destroy_s3(app=args.app, env=env)\n            except botocore.exceptions.ClientError as error:\n                LOG.warning('S3 issue for %s in %s: %s', env, region, error)\n\n            try:\n                destroy_sg(app=args.app, env=env, region=region)\n            except SpinnakerError:\n                pass\n\n            LOG.info('Destroyed %s:%s', env, region)\n\n    LOG.info('Destruction complete.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_provider_healthcheck(settings, default_provider='Discovery'):\n    ProviderHealthCheck = collections.namedtuple('ProviderHealthCheck', ['providers', 'has_healthcheck'])\n\n    eureka_enabled = settings['app']['eureka_enabled']\n    providers = settings['asg']['provider_healthcheck']\n\n    LOG.debug('Template defined Health Check Providers: %s', providers)\n\n    health_check_providers = []\n    has_healthcheck = False\n\n    normalized_default_provider = default_provider.capitalize()\n\n    if eureka_enabled:\n        LOG.info('Eureka enabled, enabling default Provider Health Check: %s', normalized_default_provider)\n\n        for provider, active in providers.items():\n            if provider.lower() == normalized_default_provider.lower():\n                providers[provider] = True\n                LOG.debug('Override defined Provider Health Check: %s -> %s', active, providers[provider])\n                break\n        else:\n            LOG.debug('Adding default Provider Health Check: %s', normalized_default_provider)\n            providers[normalized_default_provider] = True\n\n    for provider, active in providers.items():\n        if active:\n            health_check_providers.append(provider.capitalize())\n    LOG.info('Provider healthchecks: %s', health_check_providers)\n\n    if health_check_providers:\n        has_healthcheck = True\n\n    return ProviderHealthCheck(providers=health_check_providers, has_healthcheck=has_healthcheck)", "response": "Create a new ProviderHealthCheck object with the specified Provider Health Check providers and has_healthcheck set to True."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the correct template name based on the environment and pipeline type", "response": "def get_template_name(env, pipeline_type):\n    \"\"\"Generates the correct template name based on pipeline type\n\n    Args:\n        env (str): environment to generate templates for\n        pipeline_type (str): Type of pipeline like ec2 or lambda\n\n    Returns:\n        str: Name of template\n    \"\"\"\n    pipeline_base = 'pipeline/pipeline'\n    template_name_format = '{pipeline_base}'\n    if env.startswith('prod'):\n        template_name_format = template_name_format + '_{env}'\n    else:\n        template_name_format = template_name_format + '_stages'\n\n    if pipeline_type != 'ec2':\n        template_name_format = template_name_format + '_{pipeline_type}'\n\n    template_name_format = template_name_format + '.json.j2'\n    template_name = template_name_format.format(pipeline_base=pipeline_base, env=env, pipeline_type=pipeline_type)\n\n    return template_name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef construct_pipeline_block(env='',\n                             generated=None,\n                             previous_env='',\n                             region='us-east-1',\n                             settings=None,\n                             pipeline_data=None,\n                             region_subnets=None,\n                             **kwargs):\n    \"\"\"Create the Pipeline JSON from template.\n\n    This handles the common repeatable patterns in a pipeline, such as\n    judgement, infrastructure, tagger and qe.\n\n    Note:\n       ASG Health Check type is overridden to `EC2` when deploying to **dev** or\n       using :ref:`eureka_enabled`.\n\n    Args:\n        env (str): Deploy environment name, e.g. dev, stage, prod.\n        generated (gogoutils.Generator): Gogo Application name generator.\n        kwargs (dict): Extra variables to pass to Pipeline Templates.\n        previous_env (str): The previous deploy environment to use as\n            Trigger.\n        region (str): AWS Region to deploy to.\n        settings (dict): Environment settings from configurations.\n        pipeline_data (dict): Pipeline settings from configurations\n        region_subnets (dict): Subnets for a Region, e.g.\n            {'us-west-2': ['us-west-2a', 'us-west-2b', 'us-west-2c']}.\n\n    Returns:\n        dict: Pipeline JSON template rendered with configurations.\n\n    \"\"\"\n    LOG.info('%s block for [%s].', env, region)\n    LOG.debug('%s info:\\n%s', env, pformat(settings))\n\n    pipeline_type = pipeline_data['type']\n\n    if pipeline_type in EC2_PIPELINE_TYPES:\n        data = ec2_pipeline_setup(\n            generated=generated,\n            settings=settings,\n            env=env,\n            region=region,\n            pipeline_type=pipeline_type,\n            project=generated.project,\n            region_subnets=region_subnets,\n        )\n    else:\n        data = copy.deepcopy(settings)\n\n    data['app'].update({\n        'appname': generated.app_name(),\n        'repo_name': generated.repo,\n        'group_name': generated.project,\n        'environment': env,\n        'region': region,\n        'previous_env': previous_env,\n        'promote_restrict': pipeline_data['promote_restrict'],\n        'owner_email': pipeline_data['owner_email'],\n        'pipeline': pipeline_data,\n    })\n\n    LOG.debug('Block data:\\n%s', pformat(data))\n\n    template_name = get_template_name(env, pipeline_type)\n    pipeline_json = get_template(template_file=template_name, data=data, formats=generated, **kwargs)\n    return pipeline_json", "response": "Constructs the JSON template for a single pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ec2_pipeline_setup(\n        generated=None,\n        project='',\n        settings=None,\n        env='',\n        pipeline_type='',\n        region='',\n        region_subnets=None,\n):\n    \"\"\"Handles ec2 pipeline data setup\n\n    Args:\n        generated (gogoutils.Generator): Generated naming formats.\n        project (str): Group name of application\n        settings (dict): Environment settings from configurations.\n        env (str): Deploy environment name, e.g. dev, stage, prod.\n        pipeline_type (str): Type of Foremast Pipeline to configure.\n        region (str): AWS Region to deploy to.\n        region_subnets (dict): Subnets for a Region, e.g.\n            {'us-west-2': ['us-west-2a', 'us-west-2b', 'us-west-2c']}.\n\n    Returns:\n        dict: Updated settings to pass to templates for EC2 info\n\n    \"\"\"\n    data = copy.deepcopy(settings)\n    user_data = generate_encoded_user_data(\n        env=env,\n        region=region,\n        generated=generated,\n        group_name=project,\n        pipeline_type=pipeline_type,\n    )\n\n    # Use different variable to keep template simple\n    instance_security_groups = sorted(DEFAULT_EC2_SECURITYGROUPS[env])\n    instance_security_groups.append(generated.security_group_app)\n    instance_security_groups.extend(settings['security_group']['instance_extras'])\n    instance_security_groups = remove_duplicate_sg(instance_security_groups)\n\n    LOG.info('Instance security groups to attach: %s', instance_security_groups)\n\n    # check if scaling policy exists\n    if settings['asg']['scaling_policy']:\n        scalingpolicy = True\n        LOG.info('Found scaling policy')\n    else:\n        scalingpolicy = False\n        LOG.info('No scaling policy found')\n\n    if settings['app']['eureka_enabled']:\n        elb = []\n    else:\n        elb = [generated.elb_app]\n    LOG.info('Attaching the following ELB: %s', elb)\n\n    health_checks = check_provider_healthcheck(settings)\n\n    # Use EC2 Health Check for DEV or Eureka enabled\n    if env == 'dev' or settings['app']['eureka_enabled']:\n        data['asg'].update({'hc_type': 'EC2'})\n        LOG.info('Switching health check type to: EC2')\n\n    # Aggregate the default grace period, plus the exposed app_grace_period\n    # to allow per repo extension of asg healthcheck grace period\n    hc_grace_period = data['asg'].get('hc_grace_period')\n    app_grace_period = data['asg'].get('app_grace_period')\n    grace_period = hc_grace_period + app_grace_period\n\n    # TODO: Migrate the naming logic to an external library to make it easier\n    #       to update in the future. Gogo-Utils looks like a good candidate\n    ssh_keypair = data['asg'].get('ssh_keypair', None)\n    if not ssh_keypair:\n        ssh_keypair = '{0}_{1}_default'.format(env, region)\n    LOG.info('SSH keypair (%s) used', ssh_keypair)\n\n    if settings['app']['canary']:\n        canary_user_data = generate_encoded_user_data(\n            env=env,\n            region=region,\n            generated=generated,\n            group_name=project,\n            canary=True,\n        )\n        data['app'].update({\n            'canary_encoded_user_data': canary_user_data,\n        })\n\n    data['asg'].update({\n        'hc_type': data['asg'].get('hc_type').upper(),\n        'hc_grace_period': grace_period,\n        'ssh_keypair': ssh_keypair,\n        'provider_healthcheck': json.dumps(health_checks.providers),\n        'enable_public_ips': json.dumps(settings['asg']['enable_public_ips']),\n        'has_provider_healthcheck': health_checks.has_healthcheck,\n        'asg_whitelist': ASG_WHITELIST,\n    })\n\n    data['app'].update({\n        'az_dict': json.dumps(region_subnets),\n        'encoded_user_data': user_data,\n        'instance_security_groups': json.dumps(instance_security_groups),\n        'elb': json.dumps(elb),\n        'scalingpolicy': scalingpolicy,\n    })\n\n    return data", "response": "Handles ec2 pipeline data setup"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_pipeline(self):\n        pipelines = self.settings['pipeline']['pipeline_files']\n        self.log.info('Uploading manual Pipelines: %s', pipelines)\n\n        lookup = FileLookup(git_short=self.generated.gitlab()['main'], runway_dir=self.runway_dir)\n\n        for json_file in pipelines:\n            json_dict = lookup.json(filename=json_file)\n\n            json_dict.setdefault('application', self.app_name)\n            json_dict.setdefault('name', normalize_pipeline_name(name=json_file))\n\n            json_dict.setdefault('id', get_pipeline_id(app=json_dict['application'], name=json_dict['name']))\n\n            self.post_pipeline(json_dict)\n\n        return True", "response": "Use JSON files to create Pipelines."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    logging.basicConfig(format=LOGGING_FORMAT)\n    log = logging.getLogger(__name__)\n\n    parser = argparse.ArgumentParser()\n    add_debug(parser)\n    add_app(parser)\n    add_properties(parser)\n    parser.add_argument('-b', '--base', help='Base AMI name to use, e.g. fedora, tomcat')\n    parser.add_argument(\"--triggerjob\", help=\"The jenkins job to monitor for pipeline triggering\", required=True)\n    parser.add_argument('--onetime', required=False, choices=ENVS, help='Onetime deployment environment')\n    parser.add_argument(\n        '-t', '--type', dest='type', required=False, default='ec2', help='Deployment type, e.g. ec2, lambda')\n    args = parser.parse_args()\n\n    if args.base and '\"' in args.base:\n        args.base = args.base.strip('\"')\n\n    logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n\n    log.debug('Parsed arguments: %s', args)\n\n    if args.onetime:\n        spinnakerapps = SpinnakerPipelineOnetime(\n            app=args.app, onetime=args.onetime, trigger_job=args.triggerjob, prop_path=args.properties, base=args.base)\n        spinnakerapps.create_pipeline()\n    else:\n\n        if args.type == \"ec2\":\n            spinnakerapps = SpinnakerPipeline(\n                app=args.app, trigger_job=args.triggerjob, prop_path=args.properties, base=args.base)\n            spinnakerapps.create_pipeline()\n        elif args.type == \"lambda\":\n            spinnakerapps = SpinnakerPipelineLambda(\n                app=args.app, trigger_job=args.triggerjob, prop_path=args.properties, base=args.base)\n            spinnakerapps.create_pipeline()\n        elif args.type == \"s3\":\n            spinnakerapps = SpinnakerPipelineS3(\n                app=args.app, trigger_job=args.triggerjob, prop_path=args.properties, base=args.base)\n            spinnakerapps.create_pipeline()", "response": "Main function for the Spinnaker pipeline creation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_ini(config_dict):\n    config_lines = []\n\n    for env, configs in sorted(config_dict.items()):\n        for resource, app_properties in sorted(configs.items()):\n            try:\n                for app_property, value in sorted(app_properties.items()):\n                    variable = '{env}_{resource}_{app_property}'.format(\n                        env=env, resource=resource, app_property=app_property).upper()\n\n                    if isinstance(value, (dict, DeepChainMap)):\n                        safe_value = \"'{0}'\".format(json.dumps(dict(value)))\n                    else:\n                        safe_value = json.dumps(value)\n\n                    line = \"{variable}={value}\".format(variable=variable, value=safe_value)\n\n                    LOG.debug('INI line: %s', line)\n                    config_lines.append(line)\n            except AttributeError:\n                resource = resource.upper()\n                app_properties = \"'{}'\".format(json.dumps(app_properties))\n                line = '{0}={1}'.format(resource, app_properties)\n\n                LOG.debug('INI line: %s', line)\n                config_lines.append(line)\n    return config_lines", "response": "Convert INI formatted strings into a list of INI formatted strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_variables(app_configs=None, out_file='', git_short=''):\n    generated = gogoutils.Generator(*gogoutils.Parser(git_short).parse_url(), formats=APP_FORMATS)\n\n    json_configs = {}\n    for env, configs in app_configs.items():\n        if env != 'pipeline':\n            instance_profile = generated.iam()['profile']\n            rendered_configs = json.loads(\n                get_template(\n                    'configs/configs.json.j2',\n                    env=env,\n                    app=generated.app_name(),\n                    profile=instance_profile,\n                    formats=generated))\n            json_configs[env] = dict(DeepChainMap(configs, rendered_configs))\n            region_list = configs.get('regions', rendered_configs['regions'])\n            json_configs[env]['regions'] = region_list  # removes regions defined in templates but not configs.\n            for region in region_list:\n                region_config = json_configs[env][region]\n                json_configs[env][region] = dict(DeepChainMap(region_config, rendered_configs))\n        else:\n            default_pipeline_json = json.loads(get_template('configs/pipeline.json.j2', formats=generated))\n            json_configs['pipeline'] = dict(DeepChainMap(configs, default_pipeline_json))\n\n    LOG.debug('Compiled configs:\\n%s', pformat(json_configs))\n\n    config_lines = convert_ini(json_configs)\n\n    with open(out_file, 'at') as jenkins_vars:\n        LOG.info('Appending variables to %s.', out_file)\n        jenkins_vars.write('\\n'.join(config_lines))\n\n    with open(out_file + '.exports', 'wt') as export_vars:\n        LOG.info('Writing sourceable variables to %s.', export_vars.name)\n        export_vars.write('\\n'.join('export {0}'.format(line) for line in config_lines))\n\n    with open(out_file + '.json', 'wt') as json_handle:\n        LOG.info('Writing JSON to %s.', json_handle.name)\n        LOG.debug('Total JSON dict:\\n%s', json_configs)\n        json.dump(json_configs, json_handle)\n\n    return json_configs", "response": "Append _application. json_ configs to _out_file_ and. json."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist SNS lambda subscriptions.", "response": "def get_sns_subscriptions(app_name, env, region):\n    \"\"\"List SNS lambda subscriptions.\n\n    Returns:\n        list: List of Lambda subscribed SNS ARNs.\n\n    \"\"\"\n    session = boto3.Session(profile_name=env, region_name=region)\n    sns_client = session.client('sns')\n\n    lambda_alias_arn = get_lambda_alias_arn(app=app_name, account=env, region=region)\n\n    lambda_subscriptions = []\n    subscriptions = sns_client.list_subscriptions()\n\n    for subscription in subscriptions['Subscriptions']:\n        if subscription['Protocol'] == \"lambda\" and subscription['Endpoint'] == lambda_alias_arn:\n            lambda_subscriptions.append(subscription['SubscriptionArn'])\n\n    if not lambda_subscriptions:\n        LOG.debug('SNS subscription for function %s not found', lambda_alias_arn)\n\n    return lambda_subscriptions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndestroy Cloudwatch log event.", "response": "def destroy_cloudwatch_log_event(app='', env='dev', region=''):\n    \"\"\"Destroy Cloudwatch log event.\n\n    Args:\n        app (str): Spinnaker Application name.\n        env (str): Deployment environment.\n        region (str): AWS region.\n    Returns:\n        bool: True upon successful completion.\n    \"\"\"\n\n    session = boto3.Session(profile_name=env, region_name=region)\n    cloudwatch_client = session.client('logs')\n\n    # FIXME: see below\n    # TODO: Log group name is required, where do we get it if it is not in application-master-env.json?\n    cloudwatch_client.delete_subscription_filter(logGroupName='/aws/lambda/awslimitchecker', filterName=app)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_accounts(self, provider='aws'):\n        url = '{gate}/credentials'.format(gate=API_URL)\n        response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n        assert response.ok, 'Failed to get accounts: {0}'.format(response.text)\n\n        all_accounts = response.json()\n        self.log.debug('Accounts in Spinnaker:\\n%s', all_accounts)\n\n        filtered_accounts = []\n        for account in all_accounts:\n            if account['type'] == provider:\n                filtered_accounts.append(account)\n\n        if not filtered_accounts:\n            raise ForemastError('No Accounts matching {0}.'.format(provider))\n\n        return filtered_accounts", "response": "Get Accounts added to Spinnaker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_app(self):\n        self.appinfo['accounts'] = self.get_accounts()\n        self.log.debug('Pipeline Config\\n%s', pformat(self.pipeline_config))\n        self.log.debug('App info:\\n%s', pformat(self.appinfo))\n        jsondata = self.retrieve_template()\n        wait_for_task(jsondata)\n\n        self.log.info(\"Successfully created %s application\", self.appname)\n        return jsondata", "response": "Send a POST to spinnaker to create a new application with class variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retrieve_template(self):\n        links = self.retrieve_instance_links()\n        self.log.debug('Links is \\n%s', pformat(links))\n        self.pipeline_config['instance_links'].update(links)\n        jsondata = get_template(\n            template_file='infrastructure/app_data.json.j2',\n            appinfo=self.appinfo,\n            pipeline_config=self.pipeline_config,\n            formats=self.generated,\n            run_as_user=DEFAULT_RUN_AS_USER)\n        self.log.debug('jsondata is %s', pformat(jsondata))\n        return jsondata", "response": "Retrieves the template files for the current instance of the current application and returns the json data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary containing all the instance links in LINKS and not in pipeline_config", "response": "def retrieve_instance_links(self):\n        \"\"\"Appends on existing instance links\n\n        Returns:\n            instance_links: A dictionary containing all the instance links in LINKS and not in pipeline_config\n        \"\"\"\n        instance_links = {}\n        self.log.debug(\"LINKS IS %s\", LINKS)\n        for key, value in LINKS.items():\n            if value not in self.pipeline_config['instance_links'].values():\n                instance_links[key] = value\n        return instance_links"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting CloudWatch Event rule names.", "response": "def get_cloudwatch_event_rule(app_name, account, region):\n    \"\"\"Get CloudWatch Event rule names.\"\"\"\n    session = boto3.Session(profile_name=account, region_name=region)\n    cloudwatch_client = session.client('events')\n\n    lambda_alias_arn = get_lambda_alias_arn(app=app_name, account=account, region=region)\n    rule_names = cloudwatch_client.list_rule_names_by_target(TargetArn=lambda_alias_arn)\n\n    if rule_names['RuleNames']:\n        all_rules = rule_names['RuleNames']\n    else:\n        LOG.debug(\"No event rules found\")\n        all_rules = []\n    return all_rules"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats pathing for S3 deployments.", "response": "def setup_pathing(self):\n        \"\"\"Format pathing for S3 deployments.\"\"\"\n        self.s3_version_uri = self._path_formatter(self.version)\n        self.s3_latest_uri = self._path_formatter(\"LATEST\")\n        self.s3_canary_uri = self._path_formatter(\"CANARY\")\n        self.s3_alpha_uri = self._path_formatter(\"ALPHA\")\n        self.s3_mirror_uri = self._path_formatter(\"MIRROR\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _path_formatter(self, suffix):\n        if suffix.lower() == \"mirror\":\n            path_items = [self.bucket, self.s3path]\n        else:\n            path_items = [self.bucket, self.s3path, suffix]\n\n        path = '/'.join(path_items)\n        s3_format = \"s3://{}\"\n        formatted_path = path.replace('//', '/')  # removes configuration errors\n        full_path = s3_format.format(formatted_path)\n        return full_path", "response": "Format the s3 path properly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_artifacts(self):\n        deploy_strategy = self.properties[\"deploy_strategy\"]\n\n        mirror = False\n        if deploy_strategy == \"mirror\":\n            mirror = True\n\n        self._upload_artifacts_to_path(mirror=mirror)\n        if deploy_strategy == \"highlander\":\n            self._sync_to_uri(self.s3_latest_uri)\n        elif deploy_strategy == \"canary\":\n            self._sync_to_uri(self.s3_canary_uri)\n        elif deploy_strategy == \"alpha\":\n            self._sync_to_uri(self.s3_alpha_uri)\n        elif deploy_strategy == \"mirror\":\n            pass  # Nothing extra needed for mirror deployments\n        else:\n            raise NotImplementedError", "response": "Upload artifacts to S3 and copy to correct path depending on deploy_strategy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef promote_artifacts(self, promote_stage='latest'):\n        if promote_stage.lower() == 'alpha':\n            self._sync_to_uri(self.s3_canary_uri)\n        elif promote_stage.lower() == 'canary':\n            self._sync_to_uri(self.s3_latest_uri)\n        else:\n            self._sync_to_uri(self.s3_latest_uri)", "response": "Promote artifact version to dest."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the S3 CLI upload command.", "response": "def _get_upload_cmd(self, mirror=False):\n        \"\"\"Generate the S3 CLI upload command\n\n        Args:\n            mirror (bool): If true, uses a flat directory structure instead of nesting under a version.\n\n        Returns:\n            str: The full CLI command to run.\n        \"\"\"\n        if mirror:\n            dest_uri = self.s3_mirror_uri\n        else:\n            dest_uri = self.s3_version_uri\n\n        cmd = 'aws s3 sync {} {} --delete --exact-timestamps --profile {}'.format(self.artifact_path,\n                                                                                  dest_uri, self.env)\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef content_metadata_uploads(self, mirror=False):\n        excludes_str = ''\n        includes_cmds = []\n        cmd_base = self._get_upload_cmd(mirror=mirror)\n\n        for content in self.s3props.get('content_metadata'):\n            full_path = os.path.join(self.artifact_path, content['path'])\n            if not os.listdir(full_path):\n                raise S3ArtifactNotFound\n\n            excludes_str += '--exclude \"{}/*\" '.format(content['path'])\n            include_cmd = '{} --exclude \"*\", --include \"{}/*\"'.format(cmd_base, content['path'])\n            include_cmd += ' --content-encoding {} --metadata-directive REPLACE'.format(content['content-encoding'])\n            includes_cmds.append(include_cmd)\n\n        exclude_cmd = '{} {}'.format(cmd_base, excludes_str)\n        result = subprocess.run(exclude_cmd, check=True, shell=True, stdout=subprocess.PIPE)\n        LOG.info(\"Uploaded files without metadata with command: %s\", exclude_cmd)\n        LOG.debug(\"Upload Command Output: %s\", result.stdout)\n\n        for include_cmd in includes_cmds:\n            result = subprocess.run(include_cmd, check=True, shell=True, stdout=subprocess.PIPE)\n            LOG.info(\"Uploaded files with metadata with command: %s\", include_cmd)\n            LOG.debug(\"Upload Command Output: %s\", result.stdout)\n\n        return True", "response": "Finds all specified encoded directories and uploads in multiple parts and sets the metadata for objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sync_to_uri(self, uri):\n        cmd_cp = 'aws s3 cp {} {} --recursive --profile {}'.format(self.s3_version_uri, uri, self.env)\n        # AWS CLI sync does not work as expected bucket to bucket with exact timestamp sync.\n        cmd_sync = 'aws s3 sync {} {} --delete --exact-timestamps --profile {}'.format(\n            self.s3_version_uri, uri, self.env)\n\n        cp_result = subprocess.run(cmd_cp, check=True, shell=True, stdout=subprocess.PIPE)\n        LOG.debug(\"Copy to %s before sync output: %s\", uri, cp_result.stdout)\n        LOG.info(\"Copied version %s to %s\", self.version, uri)\n\n        sync_result = subprocess.run(cmd_sync, check=True, shell=True, stdout=subprocess.PIPE)\n        LOG.debug(\"Sync to %s command output: %s\", uri, sync_result.stdout)\n        LOG.info(\"Synced version %s to %s\", self.version, uri)", "response": "Copy and sync versioned directory to uri in S3."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the VPC ID configured for the account in the given region.", "response": "def get_vpc_id(account, region):\n    \"\"\"Get VPC ID configured for ``account`` in ``region``.\n\n    Args:\n        account (str): AWS account name.\n        region (str): Region name, e.g. us-east-1.\n\n    Returns:\n        str: VPC ID for the requested ``account`` in ``region``.\n\n    Raises:\n        :obj:`foremast.exceptions.SpinnakerVPCIDNotFound`: VPC ID not found for\n            ``account`` in ``region``.\n        :obj:`foremast.exceptions.SpinnakerVPCNotFound`: Spinnaker has no VPCs\n            configured.\n\n    \"\"\"\n    url = '{0}/networks/aws'.format(API_URL)\n    response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    if not response.ok:\n        raise SpinnakerVPCNotFound(response.text)\n\n    vpcs = response.json()\n\n    for vpc in vpcs:\n        LOG.debug('VPC: %(name)s, %(account)s, %(region)s => %(id)s', vpc)\n        if 'name' in vpc and all([vpc['name'] == 'vpc', vpc['account'] == account, vpc['region'] == region]):\n            LOG.info('Found VPC ID for %s in %s: %s', account, region, vpc['id'])\n            vpc_id = vpc['id']\n            break\n    else:\n        LOG.fatal('VPC list: %s', vpcs)\n        raise SpinnakerVPCIDNotFound('No VPC available for {0} [{1}].'.format(account, region))\n\n    return vpc_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all subnets for a given target.", "response": "def get_subnets(\n        target='ec2',\n        purpose='internal',\n        env='',\n        region='', ):\n    \"\"\"Get all availability zones for a given target.\n\n    Args:\n        target (str): Type of subnets to look up (ec2 or elb).\n        env (str): Environment to look up.\n        region (str): AWS Region to find Subnets for.\n\n    Returns:\n        az_dict: dictionary of  availbility zones, structured like\n        { $region: [ $avaibilityzones ] }\n        or\n        { $account: $region: [ $availabilityzone] }\n    \"\"\"\n    account_az_dict = defaultdict(defaultdict)\n    subnet_id_dict = defaultdict(defaultdict)\n\n    subnet_url = '{0}/subnets/aws'.format(API_URL)\n    subnet_response = requests.get(subnet_url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    if not subnet_response.ok:\n        raise SpinnakerTimeout(subnet_response.text)\n\n    subnet_list = subnet_response.json()\n    for subnet in subnet_list:\n        LOG.debug('Subnet: %(account)s\\t%(region)s\\t%(target)s\\t%(vpcId)s\\t' '%(availabilityZone)s', subnet)\n\n        if subnet.get('target', '') == target:\n            availability_zone = subnet['availabilityZone']\n            account = subnet['account']\n            subnet_region = subnet['region']\n            subnet_id = subnet['id']\n            try:\n                if availability_zone not in account_az_dict[account][subnet_region]:\n                    account_az_dict[account][subnet_region].append(availability_zone)\n            except KeyError:\n                account_az_dict[account][subnet_region] = [availability_zone]\n            # get list of all subnet IDs with correct purpose\n            if subnet['purpose'] == purpose:\n                try:\n                    subnet_id_dict[account][subnet_region].append(subnet_id)\n                except KeyError:\n                    subnet_id_dict[account][subnet_region] = [subnet_id]\n\n            LOG.debug('%s regions: %s', account, list(account_az_dict[account].keys()))\n\n    if all([env, region]):\n        try:\n            region_dict = {region: account_az_dict[env][region]}\n            region_dict['subnet_ids'] = {region: subnet_id_dict[env][region]}\n            LOG.debug('Region dict: %s', region_dict)\n            return region_dict\n        except KeyError:\n            raise SpinnakerSubnetError(env=env, region=region)\n\n    LOG.debug('AZ dict:\\n%s', pformat(dict(account_az_dict)))\n\n    return account_az_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef construct_datapipeline(env='',\n                           generated=None,\n                           previous_env=None,\n                           region='us-east-1',\n                           settings=None,\n                           pipeline_data=None):\n    \"\"\"Create the Pipeline JSON from template.\n\n    This handles the common repeatable patterns in a pipeline, such as\n    judgement, infrastructure, tagger and qe.\n\n    Args:\n        env (str): Deploy environment name, e.g. dev, stage, prod.\n        generated (gogoutils.Generator): Gogo Application name generator.\n        previous_env (str): The previous deploy environment to use as\n            Trigger.\n        region (str): AWS Region to deploy to.\n        settings (dict): Environment settings from configurations.\n\n    Returns:\n        dict: Pipeline JSON template rendered with configurations.\n    \"\"\"\n    LOG.info('%s block for [%s].', env, region)\n\n    if env.startswith('prod'):\n        template_name = 'pipeline/pipeline_{}_datapipeline.json.j2'.format(env)\n    else:\n        template_name = 'pipeline/pipeline_stages_datapipeline.json.j2'\n\n    LOG.debug('%s info:\\n%s', env, pformat(settings))\n\n    gen_app_name = generated.app_name()\n\n    data = copy.deepcopy(settings)\n\n    data['app'].update({\n        'appname': gen_app_name,\n        'repo_name': generated.repo,\n        'group_name': generated.project,\n        'environment': env,\n        'region': region,\n        'previous_env': previous_env,\n        'promote_restrict': pipeline_data['promote_restrict'],\n        'owner_email': pipeline_data['owner_email']\n    })\n\n    LOG.debug('Block data:\\n%s', pformat(data))\n\n    pipeline_json = get_template(template_file=template_name, data=data, formats=generated)\n    return pipeline_json", "response": "Constructs the JSON template for a data pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate all defined lambda events for an lambda application.", "response": "def create_lambda_events(self):\n        \"\"\"Create all defined lambda events for an lambda application.\"\"\"\n\n        # Clean up lambda permissions before creating triggers\n        remove_all_lambda_permissions(app_name=self.app_name, env=self.env, region=self.region)\n\n        triggers = self.properties['lambda_triggers']\n\n        for trigger in triggers:\n\n            if trigger['type'] == 'sns':\n                create_sns_event(app_name=self.app_name, env=self.env, region=self.region, rules=trigger)\n\n            if trigger['type'] == 'cloudwatch-event':\n                create_cloudwatch_event(app_name=self.app_name, env=self.env, region=self.region, rules=trigger)\n\n            if trigger['type'] == 'cloudwatch-logs':\n                create_cloudwatch_log_event(app_name=self.app_name, env=self.env, region=self.region, rules=trigger)\n\n            if trigger['type'] == 'api-gateway':\n                apigateway = APIGateway(\n                    app=self.app_name, env=self.env, region=self.region, rules=trigger, prop_path=self.prop_path)\n                apigateway.setup_lambda_api()\n\n        # filter all triggers to isolate s3 triggers so we can operate on the entire group\n        s3_triggers = [x for x in triggers if x['type'] == 's3']\n\n        # group triggers by unique target bucket\n        bucket_triggers = dict()\n        for s3_trigger in s3_triggers:\n            bucket = s3_trigger.get('bucket')\n            if bucket in bucket_triggers:\n                bucket_triggers[bucket].append(s3_trigger)\n            else:\n                bucket_triggers[bucket] = [s3_trigger]\n\n        # apply relevant triggers to each respective bucket all at once.\n        for bucket, triggers in bucket_triggers.items():\n            create_s3_event(app_name=self.app_name, env=self.env, region=self.region, bucket=bucket, triggers=triggers)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_managed_pipeline(name='', app_name=''):\n    *pipeline_name_prefix, bracket_region = name.split()\n    region = bracket_region.strip('[]')\n\n    not_managed_message = '\"{0}\" is not managed.'.format(name)\n\n    if 'onetime' in region:\n        LOG.info('\"%s\" is a onetime, marked for cleaning.', name)\n        return region\n\n    if not all([bracket_region.startswith('['), bracket_region.endswith(']')]):\n        LOG.debug('\"%s\" does not end with \"[region]\".', name)\n        raise ValueError(not_managed_message)\n\n    if len(pipeline_name_prefix) is not 1:\n        LOG.debug('\"%s\" does not only have one word before [region].', name)\n        raise ValueError(not_managed_message)\n\n    if app_name not in pipeline_name_prefix:\n        LOG.debug('\"%s\" does not use \"%s\" before [region].', name, app_name)\n        raise ValueError(not_managed_message)\n\n    return region", "response": "Check a Pipeline name is a managed format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all_pipelines(app=''):\n    url = '{host}/applications/{app}/pipelineConfigs'.format(host=API_URL, app=app)\n    response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    assert response.ok, 'Could not retrieve Pipelines for {0}.'.format(app)\n\n    pipelines = response.json()\n    LOG.debug('Pipelines:\\n%s', pipelines)\n\n    return pipelines", "response": "Get a list of all the Pipelines in _app_."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pipeline_id(app='', name=''):\n    return_id = None\n\n    pipelines = get_all_pipelines(app=app)\n\n    for pipeline in pipelines:\n        LOG.debug('ID of %(name)s: %(id)s', pipeline)\n\n        if pipeline['name'] == name:\n            return_id = pipeline['id']\n            LOG.info('Pipeline %s found, ID: %s', name, return_id)\n            break\n\n    return return_id", "response": "Get the ID for a given Pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_pipeline_name(name=''):\n    normalized_name = name\n    for bad in '\\\\/?%#':\n        normalized_name = normalized_name.replace(bad, '_')\n    return normalized_name", "response": "Translate unsafe characters to underscores."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of all applications in Spinnaker.", "response": "def get_all_apps():\n    \"\"\"Get a list of all applications in Spinnaker.\n\n    Returns:\n        requests.models.Response: Response from Gate containing list of all apps.\n\n    \"\"\"\n    LOG.info('Retreiving list of all Spinnaker applications')\n    url = '{}/applications'.format(API_URL)\n    response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    assert response.ok, 'Could not retrieve application list'\n\n    pipelines = response.json()\n    LOG.debug('All Applications:\\n%s', pipelines)\n\n    return pipelines"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts details for an application.", "response": "def get_details(app='groupproject', env='dev', region='us-east-1'):\n    \"\"\"Extract details for Application.\n\n    Args:\n        app (str): Application Name\n        env (str): Environment/account to get details from\n\n    Returns:\n        collections.namedtuple with _group_, _policy_, _profile_, _role_,\n            _user_.\n\n    \"\"\"\n    url = '{host}/applications/{app}'.format(host=API_URL, app=app)\n\n    request = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    if not request.ok:\n        raise SpinnakerAppNotFound('\"{0}\" not found.'.format(app))\n\n    app_details = request.json()\n\n    LOG.debug('App details: %s', app_details)\n    group = app_details['attributes'].get('repoProjectKey')\n    project = app_details['attributes'].get('repoSlug')\n    generated = gogoutils.Generator(group, project, env=env, region=region, formats=APP_FORMATS)\n\n    LOG.debug('Application details: %s', generated)\n    return generated"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the base Pipeline wrapper.", "response": "def render_wrapper(self, region='us-east-1'):\n        \"\"\"Generate the base Pipeline wrapper.\n\n        This renders the non-repeatable stages in a pipeline, like jenkins, baking, tagging and notifications.\n\n        Args:\n            region (str): AWS Region.\n\n        Returns:\n            dict: Rendered Pipeline wrapper.\n        \"\"\"\n        base = self.settings['pipeline']['base']\n\n        if self.base:\n            base = self.base\n\n        email = self.settings['pipeline']['notifications']['email']\n        slack = self.settings['pipeline']['notifications']['slack']\n        deploy_type = self.settings['pipeline']['type']\n        pipeline_id = self.compare_with_existing(region=region)\n\n        data = {\n            'app': {\n                'appname': self.app_name,\n                'group_name': self.group_name,\n                'repo_name': self.repo_name,\n                'base': base,\n                'deploy_type': deploy_type,\n                'environment': 'packaging',\n                'region': region,\n                'triggerjob': self.trigger_job,\n                'run_as_user': DEFAULT_RUN_AS_USER,\n                'email': email,\n                'slack': slack,\n                'pipeline': self.settings['pipeline']\n            },\n            'id': pipeline_id\n        }\n\n        self.log.debug('Wrapper app data:\\n%s', pformat(data))\n\n        wrapper = get_template(template_file='pipeline/pipeline_wrapper.json.j2', data=data, formats=self.generated)\n\n        return json.loads(wrapper)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new pipeline for all environments and regions.", "response": "def create_pipeline(self):\n        \"\"\"Main wrapper for pipeline creation.\n        1. Runs clean_pipelines to clean up existing ones\n        2. determines which environments the pipeline needs\n        3. Renders all of the pipeline blocks as defined in configs\n        4. Runs post_pipeline to create pipeline\n        \"\"\"\n        clean_pipelines(app=self.app_name, settings=self.settings)\n\n        pipeline_envs = self.environments\n        self.log.debug('Envs from pipeline.json: %s', pipeline_envs)\n\n        regions_envs = collections.defaultdict(list)\n        for env in pipeline_envs:\n            for region in self.settings[env]['regions']:\n                regions_envs[region].append(env)\n        self.log.info('Environments and Regions for Pipelines:\\n%s', json.dumps(regions_envs, indent=4))\n\n        pipelines = {}\n        for region, envs in regions_envs.items():\n            # TODO: Overrides for an environment no longer makes sense. Need to\n            # provide override for entire Region possibly.\n            pipelines[region] = self.render_wrapper(region=region)\n\n            previous_env = None\n            for env in envs:\n\n                block = construct_pipeline_block_s3(\n                    env=env,\n                    generated=self.generated,\n                    previous_env=previous_env,\n                    region=region,\n                    settings=self.settings[env][region],\n                    pipeline_data=self.settings['pipeline'])\n                pipelines[region]['stages'].extend(json.loads(block))\n\n                previous_env = env\n\n        self.log.debug('Assembled Pipelines:\\n%s', pformat(pipelines))\n\n        for region, pipeline in pipelines.items():\n            renumerate_stages(pipeline)\n\n            self.post_pipeline(pipeline)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_lambda(self):\n        exists = False\n        try:\n            self.lambda_client.get_function(FunctionName=self.app_name)\n            exists = True\n        except boto3.exceptions.botocore.exceptions.ClientError:\n            pass\n        return exists", "response": "Check if lambda function exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if lambda alias exists.", "response": "def _check_lambda_alias(self):\n        \"\"\"Check if lambda alias exists.\n\n        Returns:\n            True if alias exists\n            False if alias does not exist\n        \"\"\"\n        aliases = self.lambda_client.list_aliases(FunctionName=self.app_name)\n\n        matched_alias = False\n        for alias in aliases['Aliases']:\n            if alias['Name'] == self.env:\n                LOG.info('Found alias %s for function %s', self.env, self.app_name)\n                matched_alias = True\n                break\n        else:\n            LOG.info('No alias %s found for function %s', self.env, self.app_name)\n        return matched_alias"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_sg_ids(self):\n        try:\n            lambda_extras = self.settings['security_groups']['lambda_extras']\n        except KeyError:\n            lambda_extras = []\n\n        security_groups = [self.app_name] + lambda_extras\n        sg_ids = []\n        for security_group in security_groups:\n            sg_id = get_security_group_id(name=security_group, env=self.env, region=self.region)\n            sg_ids.append(sg_id)\n        return sg_ids", "response": "Get all security group IDs for all defined lambda_extras."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating lambda alias with env name and points it to LATEST.", "response": "def create_alias(self):\n        \"\"\"Create lambda alias with env name and points it to $LATEST.\"\"\"\n        LOG.info('Creating alias %s', self.env)\n\n        try:\n            self.lambda_client.create_alias(\n                FunctionName=self.app_name,\n                Name=self.env,\n                FunctionVersion='$LATEST',\n                Description='Alias for {}'.format(self.env))\n        except boto3.exceptions.botocore.exceptions.ClientError as error:\n            LOG.debug('Create alias error: %s', error)\n            LOG.info(\"Alias creation failed. Retrying...\")\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_alias(self):\n        LOG.info('Updating alias %s to point to $LATEST', self.env)\n\n        try:\n            self.lambda_client.update_alias(FunctionName=self.app_name, Name=self.env, FunctionVersion='$LATEST')\n        except boto3.exceptions.botocore.exceptions.ClientError as error:\n            LOG.debug('Update alias error: %s', error)\n            LOG.info(\"Alias update failed. Retrying...\")\n            raise", "response": "Update lambda alias to point to LATEST."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates existing Lambda function configuration.", "response": "def update_function_configuration(self, vpc_config):\n        \"\"\"Update existing Lambda function configuration.\n\n        Args:\n            vpc_config (dict): Dictionary of SubnetIds and SecurityGroupsIds for using\n                               a VPC in lambda\n        \"\"\"\n        LOG.info('Updating configuration for lambda function: %s', self.app_name)\n\n        try:\n            self.lambda_client.update_function_configuration(\n                Environment=self.lambda_environment,\n                FunctionName=self.app_name,\n                Runtime=self.runtime,\n                Role=self.role_arn,\n                Handler=self.handler,\n                Description=self.description,\n                Timeout=int(self.timeout),\n                MemorySize=int(self.memory),\n                VpcConfig=vpc_config)\n\n            if self.concurrency_limit:\n                self.lambda_client.put_function_concurrency(\n                    FunctionName=self.app_name,\n                    ReservedConcurrentExecutions=self.concurrency_limit\n                )\n            else:\n                self.lambda_client.delete_function_concurrency(FunctionName=self.app_name)\n\n        except boto3.exceptions.botocore.exceptions.ClientError as error:\n            if 'CreateNetworkInterface' in error.response['Error']['Message']:\n                message = '{0} is missing \"ec2:CreateNetworkInterface\"'.format(self.role_arn)\n                LOG.debug(message)\n                raise SystemExit(message)\n\n            raise\n        LOG.info('Updating Lambda function tags')\n\n        lambda_arn = get_lambda_arn(self.app_name, self.env, self.region)\n        self.lambda_client.tag_resource(Resource=lambda_arn, Tags={'app_group': self.group, 'app_name': self.app_name})\n\n        LOG.info(\"Successfully updated Lambda configuration.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating lambda function and upload non - zero zip to zip file.", "response": "def create_function(self, vpc_config):\n        \"\"\"Create lambda function, configures lambda parameters.\n\n        We need to upload non-zero zip when creating function. Uploading\n        hello_world python lambda function since AWS doesn't care which\n        executable is in ZIP.\n\n        Args:\n            vpc_config (dict): Dictionary of SubnetIds and SecurityGroupsIds for using\n                               a VPC in lambda\n        \"\"\"\n        zip_file = 'lambda-holder.zip'\n        with zipfile.ZipFile(zip_file, mode='w') as zipped:\n            zipped.writestr('index.py', 'print \"Hello world\"')\n\n        contents = ''\n        with open('lambda-holder.zip', 'rb') as openfile:\n            contents = openfile.read()\n\n        LOG.info('Creating lambda function: %s', self.app_name)\n\n        try:\n            self.lambda_client.create_function(\n                Environment=self.lambda_environment,\n                FunctionName=self.app_name,\n                Runtime=self.runtime,\n                Role=self.role_arn,\n                Handler=self.handler,\n                Code={'ZipFile': contents},\n                Description=self.description,\n                Timeout=int(self.timeout),\n                MemorySize=int(self.memory),\n                Publish=False,\n                VpcConfig=vpc_config,\n                Tags={'app_group': self.group,\n                      'app_name': self.app_name})\n        except boto3.exceptions.botocore.exceptions.ClientError as error:\n            if 'CreateNetworkInterface' in error.response['Error']['Message']:\n                message = '{0} is missing \"ec2:CreateNetworkInterface\"'.format(self.role_arn)\n                LOG.critical(message)\n                raise SystemExit(message)\n\n            raise\n\n        LOG.info(\"Successfully created Lambda function and alias\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_lambda_function(self):\n        vpc_config = self._vpc_config()\n\n        if self._check_lambda():\n            self.update_function_configuration(vpc_config)\n        else:\n            self.create_function(vpc_config)\n\n        if self._check_lambda_alias():\n            self.update_alias()\n        else:\n            self.create_alias()", "response": "Create or update Lambda function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef destroy_sg(app='', env='', region='', **_):\n    vpc = get_vpc_id(account=env, region=region)\n\n    url = '{api}/securityGroups/{env}/{region}/{app}'.format(api=API_URL, env=env, region=region, app=app)\n    payload = {'vpcId': vpc}\n    security_group = requests.get(url, params=payload, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    if not security_group:\n        LOG.info('Nothing to delete.')\n    else:\n        LOG.info('Found Security Group in %(region)s: %(name)s', security_group)\n\n        destroy_request = get_template('destroy/destroy_sg.json.j2', app=app, env=env, region=region, vpc=vpc)\n        wait_for_task(destroy_request)\n\n    return True", "response": "Destroy a security group."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndestroys S3 Resources for the application with the given name.", "response": "def destroy_s3(app='', env='dev', **_):\n    \"\"\"Destroy S3 Resources for _app_ in _env_.\n\n    Args:\n        app (str): Application name\n        env (str): Deployment environment/account name\n\n    Returns:\n        boolean: True if destroyed sucessfully\n    \"\"\"\n    session = boto3.Session(profile_name=env)\n    client = session.resource('s3')\n\n    generated = get_details(app=app, env=env)\n    archaius = generated.archaius()\n\n    bucket = client.Bucket(archaius['bucket'])\n\n    for item in bucket.objects.filter(Prefix=archaius['path']):\n        item.Object().delete()\n        LOG.info('Deleted: %s/%s', item.bucket_name, item.key)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef destroy_s3_event(app, env, region):\n\n    # TODO: how do we know which bucket to process if triggers dict is empty?\n    # Maybe list buckets and see which has notification to that lambda defined?\n    # TODO: buckets should be named the same as apps, what if one app has multiple buckets?\n    # bucket = rules.get('bucket')\n    generated = get_details(app=app, env=env)\n\n    bucket = generated.s3_app_bucket()\n\n    session = boto3.Session(profile_name=env, region_name=region)\n    s3_client = session.client('s3')\n\n    config = {}\n\n    s3_client.put_bucket_notification_configuration(Bucket=bucket, NotificationConfiguration=config)\n    LOG.debug(\"Deleted Lambda S3 notification\")\n\n    return True", "response": "Destroy S3 event.\n\n    Args:\n        app (str): Spinnaker Application name.\n        env (str): Deployment environment.\n        region (str): AWS region.\n    Returns:\n        bool: True upon successful completion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndestroying IAM Resources. Args: app (str): Spinnaker Application name. env (str): Deployment environment, i.e. dev, stage, prod. Returns: True upon successful completion.", "response": "def destroy_iam(app='', env='dev', **_):\n    \"\"\"Destroy IAM Resources.\n\n    Args:\n        app (str): Spinnaker Application name.\n        env (str): Deployment environment, i.e. dev, stage, prod.\n\n    Returns:\n        True upon successful completion.\n    \"\"\"\n    session = boto3.Session(profile_name=env)\n    client = session.client('iam')\n\n    generated = get_details(env=env, app=app)\n    generated_iam = generated.iam()\n    app_details = collections.namedtuple('AppDetails', generated_iam.keys())\n    details = app_details(**generated_iam)\n\n    LOG.debug('Application details: %s', details)\n\n    resource_action(\n        client,\n        action='remove_user_from_group',\n        log_format='Removed user from group: %(UserName)s ~> %(GroupName)s',\n        GroupName=details.group,\n        UserName=details.user)\n    resource_action(client, action='delete_user', log_format='Destroyed user: %(UserName)s', UserName=details.user)\n    resource_action(client, action='delete_group', log_format='Destroyed group: %(GroupName)s', GroupName=details.group)\n\n    resource_action(\n        client,\n        action='remove_role_from_instance_profile',\n        log_format='Destroyed Instance Profile from Role: '\n        '%(InstanceProfileName)s ~> %(RoleName)s',\n        InstanceProfileName=details.profile,\n        RoleName=details.role)\n    resource_action(\n        client,\n        action='delete_instance_profile',\n        log_format='Destroyed Instance Profile: %(InstanceProfileName)s',\n        InstanceProfileName=details.profile)\n\n    role_policies = []\n    try:\n        role_policies = resource_action(\n            client,\n            action='list_role_policies',\n            log_format='Found Role Policies for %(RoleName)s.',\n            RoleName=details.role)['PolicyNames']\n    except TypeError:\n        LOG.info('Role %s not found.', details.role)\n\n    for policy in role_policies:\n        resource_action(\n            client,\n            action='delete_role_policy',\n            log_format='Removed Inline Policy from Role: '\n            '%(PolicyName)s ~> %(RoleName)s',\n            RoleName=details.role,\n            PolicyName=policy)\n\n    attached_role_policies = []\n    try:\n        attached_role_policies = resource_action(\n            client,\n            action='list_attached_role_policies',\n            log_format='Found attached Role Polices for %(RoleName)s.',\n            RoleName=details.role)['AttachedPolicies']\n    except TypeError:\n        LOG.info('Role %s not found.', details.role)\n\n    for policy in attached_role_policies:\n        resource_action(\n            client,\n            action='detach_role_policy',\n            log_format='Detached Policy from Role: '\n            '%(PolicyArn)s ~> %(RoleName)s',\n            RoleName=details.role,\n            PolicyArn=policy['PolicyArn'])\n\n    resource_action(client, action='delete_role', log_format='Destroyed Role: %(RoleName)s', RoleName=details.role)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the ARN given a role name.", "response": "def get_role_arn(role_name, env, region):\n    \"\"\"Get role ARN given role name.\n\n    Args:\n        role_name (str): Role name to lookup\n        env (str): Environment in which to lookup\n        region (str): Region\n\n    Returns:\n        ARN if role found\n\n    \"\"\"\n    session = boto3.Session(profile_name=env, region_name=region)\n    iam_client = session.client('iam')\n\n    LOG.debug('Searching for %s.', role_name)\n\n    role = iam_client.get_role(RoleName=role_name)\n    role_arn = role['Role']['Arn']\n\n    LOG.debug(\"Found role's %s ARN %s\", role_name, role_arn)\n\n    return role_arn"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_policy_template(  # pylint: disable=too-many-arguments\n        account_number='',\n        app='coreforrest',\n        env='dev',\n        group='forrest',\n        items=None,\n        pipeline_settings=None,\n        region='us-east-1',\n        service=''):\n    \"\"\"Render IAM Policy template.\n\n    To support multiple statement blocks, JSON objects can be separated by a\n    comma. This function attempts to turn any invalid JSON into a valid list\n    based on this comma separated assumption.\n\n    Args:\n        account_number (str): AWS Account number.\n        app (str): Name of Spinnaker Application.\n        env (str): Environment/Account in AWS\n        group (str):A Application group/namespace\n        items (list): Resource names used to create a Policy per Resource.\n        region (str): AWS region.\n        pipeline_settings (dict): Settings from *pipeline.json*.\n        service (str): Name of cloud service to find matching IAM Policy\n            template.\n\n    Returns:\n        list: IAM Policy :obj:`dict` statements for the given service.\n\n    \"\"\"\n    statements = []\n\n    rendered_service_policy = get_template(\n        'infrastructure/iam/{0}.json.j2'.format(service),\n        account_number=account_number,\n        app=app,\n        env=env,\n        group=group,\n        region=region,\n        items=items,\n        settings=pipeline_settings)\n\n    try:\n        statement_block = json.loads(rendered_service_policy)\n        statements.append(statement_block)\n    except ValueError:\n        LOG.debug('Need to make %s template into list.', service)\n        statements = json.loads('[{0}]'.format(rendered_service_policy))\n\n    LOG.debug('Rendered IAM Policy statements: %s', statements)\n\n    return statements", "response": "Render IAM Policy template."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassemble a custom IAM Policy for the application.", "response": "def construct_policy(app='coreforrest', env='dev', group='forrest', region='us-east-1', pipeline_settings=None):\n    \"\"\"Assemble IAM Policy for _app_.\n\n    Args:\n        app (str): Name of Spinnaker Application.\n        env (str): Environment/Account in AWS\n        group (str):A Application group/namespace\n        region (str): AWS region\n        pipeline_settings (dict): Settings from *pipeline.json*.\n\n    Returns:\n        json: Custom IAM Policy for _app_.\n        None: When no *services* have been defined in *pipeline.json*.\n    \"\"\"\n    LOG.info('Create custom IAM Policy for %s.', app)\n\n    services = pipeline_settings.get('services', {})\n    LOG.debug('Found requested services: %s', services)\n\n    services = auto_service(pipeline_settings=pipeline_settings, services=services)\n\n    if services:\n        credential = get_env_credential(env=env)\n        account_number = credential['accountId']\n\n    statements = []\n    for service, value in services.items():\n        if value is True:\n            items = []\n        elif isinstance(value, str):\n            items = [value]\n        else:\n            items = value\n\n        rendered_statements = render_policy_template(\n            account_number=account_number,\n            app=app,\n            env=env,\n            group=group,\n            items=items,\n            pipeline_settings=pipeline_settings,\n            region=region,\n            service=service)\n\n        statements.extend(rendered_statements)\n\n    if statements:\n        policy_json = get_template('infrastructure/iam/wrapper.json.j2', statements=json.dumps(statements))\n    else:\n        LOG.info('No services defined for %s.', app)\n        policy_json = None\n\n    return policy_json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct the pipeline block for a given environment.", "response": "def construct_pipeline_block_lambda(env='',\n                                    generated=None,\n                                    previous_env=None,\n                                    region='us-east-1',\n                                    region_subnets=None,\n                                    settings=None,\n                                    pipeline_data=None):\n    \"\"\"Create the Pipeline JSON from template.\n\n    This handles the common repeatable patterns in a pipeline, such as\n    judgement, infrastructure, tagger and qe.\n\n    Args:\n        env (str): Deploy environment name, e.g. dev, stage, prod.\n        generated (gogoutils.Generator): Gogo Application name generator.\n        previous_env (str): The previous deploy environment to use as\n            Trigger.\n        region (str): AWS Region to deploy to.\n        settings (dict): Environment settings from configurations.\n        region_subnets (dict): Subnets for a Region, e.g.\n            {'us-west-2': ['us-west-2a', 'us-west-2b', 'us-west-2c']}.\n\n    Returns:\n        dict: Pipeline JSON template rendered with configurations.\n\n    \"\"\"\n    LOG.info('%s block for [%s].', env, region)\n\n    if env.startswith('prod'):\n        template_name = 'pipeline/pipeline_{}_lambda.json.j2'.format(env)\n    else:\n        template_name = 'pipeline/pipeline_stages_lambda.json.j2'\n\n    LOG.debug('%s info:\\n%s', env, pformat(settings))\n\n    gen_app_name = generated.app_name()\n    user_data = generate_encoded_user_data(\n        env=env,\n        region=region,\n        generated=generated,\n        group_name=generated.project,\n    )\n\n    # Use different variable to keep template simple\n    instance_security_groups = sorted(DEFAULT_EC2_SECURITYGROUPS[env])\n    instance_security_groups.append(gen_app_name)\n    instance_security_groups.extend(settings['security_group']['instance_extras'])\n    instance_security_groups = remove_duplicate_sg(instance_security_groups)\n\n    LOG.info('Instance security groups to attach: %s', instance_security_groups)\n\n    data = copy.deepcopy(settings)\n\n    data['app'].update({\n        'appname': gen_app_name,\n        'repo_name': generated.repo,\n        'group_name': generated.project,\n        'environment': env,\n        'region': region,\n        'az_dict': json.dumps(region_subnets),\n        'previous_env': previous_env,\n        'encoded_user_data': user_data,\n        'instance_security_groups': json.dumps(instance_security_groups),\n        'promote_restrict': pipeline_data['promote_restrict'],\n        'owner_email': pipeline_data['owner_email'],\n        'function_name': pipeline_data['lambda']['handler']\n    })\n\n    LOG.debug('Block data:\\n%s', pformat(data))\n\n    pipeline_json = get_template(template_file=template_name, data=data, formats=generated)\n    return pipeline_json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate S3 lambda events from triggers", "response": "def create_s3_event(app_name, env, region, bucket, triggers):\n    \"\"\"Create S3 lambda events from triggers\n\n    Args:\n        app_name (str): name of the lambda function\n        env (str): Environment/Account for lambda function\n        region (str): AWS region of the lambda function\n        triggers (list): List of triggers from the settings\n    \"\"\"\n    session = boto3.Session(profile_name=env, region_name=region)\n    s3_client = session.client('s3')\n\n    lambda_alias_arn = get_lambda_alias_arn(app_name, env, region)\n\n    LOG.debug(\"Lambda ARN for lambda function %s is %s.\", app_name, lambda_alias_arn)\n    LOG.debug(\"Creating S3 events for bucket %s\", bucket)\n\n    # allow lambda trigger permission from bucket\n    principal = 's3.amazonaws.com'\n    statement_id = \"{}_s3_{}\".format(app_name, bucket).replace('.', '')\n    source_arn = \"arn:aws:s3:::{}\".format(bucket)\n    add_lambda_permissions(\n        function=lambda_alias_arn,\n        env=env,\n        region=region,\n        principal=principal,\n        statement_id=statement_id,\n        source_arn=source_arn)\n\n    # configure events on s3 bucket to trigger lambda function\n    template_kwargs = {\"lambda_arn\": lambda_alias_arn, \"triggers\": triggers}\n    config = get_template(template_file='infrastructure/lambda/s3_event.json.j2', **template_kwargs)\n    s3_client.put_bucket_notification_configuration(Bucket=bucket, NotificationConfiguration=json.loads(config))\n\n    LOG.info(\"Created lambda %s S3 event on bucket %s\", app_name, bucket)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a filename to be used by packer.", "response": "def generate_packer_filename(provider, region, builder):\n    \"\"\"Generate a filename to be used by packer.\n\n    Args:\n        provider (str): Name of Spinnaker provider.\n        region (str): Name of provider region to use.\n        builder (str): Name of builder process type.\n\n    Returns:\n        str: Generated filename based on parameters.\n\n    \"\"\"\n    filename = '{0}_{1}_{2}.json'.format(provider, region, builder)\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves template. Args: template_file (str): Name of template file. Returns: jinja2.Template: Template ready to render. Raises: AssertionError: Configured path for templates does not exist. :obj:`foremast.exceptions.ForemastTemplateNotFound`: Requested template is not available.", "response": "def get_template_object(template_file=''):\n    \"\"\"Retrieve template.\n\n    Args:\n        template_file (str): Name of template file.\n\n    Returns:\n        jinja2.Template: Template ready to render.\n\n    Raises:\n        AssertionError: Configured path for templates does not exist.\n        :obj:`foremast.exceptions.ForemastTemplateNotFound`: Requested template\n            is not available.\n\n    \"\"\"\n    jinja_template_paths_obj = []\n\n    if TEMPLATES_PATH:\n        external_templates = pathlib.Path(TEMPLATES_PATH).expanduser().resolve()\n        assert os.path.isdir(external_templates), 'External template path \"{0}\" not found'.format(external_templates)\n        jinja_template_paths_obj.append(external_templates)\n\n    jinja_template_paths_obj.append(LOCAL_TEMPLATES)\n    jinja_template_paths = [str(path) for path in jinja_template_paths_obj]\n\n    jinjaenv = jinja2.Environment(loader=jinja2.FileSystemLoader(jinja_template_paths))\n\n    try:\n        template = jinjaenv.get_template(template_file)\n    except jinja2.TemplateNotFound:\n        message = 'Unable to find template \"{template_file}\" in paths {paths}'.format(\n            template_file=template_file, paths=jinjaenv.loader.searchpath)\n        LOG.error(message)\n        raise ForemastTemplateNotFound(message)\n\n    return template"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the Jinja2 template and renders with dict _kwargs_", "response": "def get_template(template_file='', **kwargs):\n    \"\"\"Get the Jinja2 template and renders with dict _kwargs_.\n\n    Args:\n        template_file (str): name of the template file\n        kwargs: Keywords to use for rendering the Jinja2 template.\n\n    Returns:\n        String of rendered JSON template.\n\n    \"\"\"\n    template = get_template_object(template_file)\n\n    LOG.info('Rendering template %s', template.filename)\n    for key, value in kwargs.items():\n        LOG.debug('%s => %s', key, value)\n\n    rendered_json = template.render(**kwargs)\n    LOG.debug('Rendered JSON:\\n%s', rendered_json)\n\n    return rendered_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef renumerate_stages(pipeline):\n    stages = pipeline['stages']\n\n    main_index = 0\n    branch_index = 0\n    previous_refid = ''\n\n    for stage in stages:\n        current_refid = stage['refId'].lower()\n        if current_refid == 'master':\n            if main_index == 0:\n                stage['requisiteStageRefIds'] = []\n            else:\n                stage['requisiteStageRefIds'] = [str(main_index)]\n            main_index += 1\n            stage['refId'] = str(main_index)\n        elif current_refid == 'branch':\n            # increments a branch_index to account for multiple parrallel stages\n            if previous_refid == 'branch':\n                branch_index += 1\n            else:\n                branch_index = 0\n            stage['refId'] = str((main_index * 100) + branch_index)\n            stage['requisiteStageRefIds'] = [str(main_index)]\n        elif current_refid == 'merge':\n            # TODO: Added logic to handle merge stages.\n            pass\n\n        previous_refid = current_refid\n        LOG.debug('step=%(name)s\\trefId=%(refId)s\\t' 'requisiteStageRefIds=%(requisiteStageRefIds)s', stage)\n\n    return pipeline", "response": "Renumber the stages in a pipeline to account for dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post_task(task_data, task_uri='/tasks'):\n    url = '{}/{}'.format(API_URL, task_uri.lstrip('/'))\n\n    if isinstance(task_data, str):\n        task_json = task_data\n    else:\n        task_json = json.dumps(task_data)\n\n    resp = requests.post(url, data=task_json, headers=HEADERS, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n    resp_json = resp.json()\n\n    LOG.debug(resp_json)\n\n    assert resp.ok, 'Spinnaker communication error: {0}'.format(resp.text)\n\n    return resp_json['ref']", "response": "Create Spinnaker Task.\n\n    Args:\n        task_data (str): Task JSON definition.\n\n    Returns:\n        str: Spinnaker Task ID.\n\n    Raises:\n        AssertionError: Error response from Spinnaker."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks Spinnaker Task status.", "response": "def _check_task(taskid):\n    \"\"\"Check Spinnaker Task status.\n\n    Args:\n        taskid (str): Existing Spinnaker Task ID.\n\n    Returns:\n        str: Task status.\n\n    \"\"\"\n    try:\n        taskurl = taskid.get('ref', '0000')\n    except AttributeError:\n        taskurl = taskid\n\n    taskid = taskurl.split('/tasks/')[-1]\n\n    LOG.info('Checking taskid %s', taskid)\n\n    url = '{}/tasks/{}'.format(API_URL, taskid)\n    task_response = requests.get(url, headers=HEADERS, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    LOG.debug(task_response.json())\n\n    assert task_response.ok, 'Spinnaker communication error: {0}'.format(task_response.text)\n\n    task_state = task_response.json()\n    status = task_state['status']\n    LOG.info('Current task status: %s', status)\n\n    if status == 'SUCCEEDED':  # pylint: disable=no-else-return\n        return status\n    elif status == 'TERMINAL':\n        raise SpinnakerTaskError(task_state)\n    else:\n        raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_task(taskid, timeout=DEFAULT_TASK_TIMEOUT, wait=2):\n    max_attempts = int(timeout / wait)\n    try:\n        return retry_call(\n            partial(_check_task, taskid),\n            max_attempts=max_attempts,\n            wait=wait,\n            exceptions=(AssertionError, ValueError), )\n    except ValueError:\n        raise SpinnakerTaskInconclusiveError('Task failed to complete in {0} seconds: {1}'.format(timeout, taskid))", "response": "Wrap check_task.\n\n    Args:\n        taskid (str): Existing Spinnaker Task ID.\n        timeout (int, optional): Consider Task failed after given seconds.\n        wait (int, optional): Seconds to pause between polling attempts.\n\n    Returns:\n        str: Task status.\n\n    Raises:\n        AssertionError: API did not respond with a 200 status code.\n        :obj:`foremast.exceptions.SpinnakerTaskInconclusiveError`: Task did not\n            reach a terminal state before the given time out."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_task(task_data, task_uri='/tasks'):\n    taskid = post_task(task_data, task_uri)\n\n    if isinstance(task_data, str):\n        json_data = json.loads(task_data)\n    else:\n        json_data = task_data\n\n    # inspect the task to see if a timeout is configured\n    job = json_data['job'][0]\n    env = job.get('credentials')\n    task_type = job.get('type')\n\n    timeout = TASK_TIMEOUTS.get(env, dict()).get(task_type, DEFAULT_TASK_TIMEOUT)\n\n    LOG.debug(\"Task %s will timeout after %s\", task_type, timeout)\n\n    return check_task(taskid, timeout)", "response": "Run a task and check the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates application. properties for a given application.", "response": "def main():\n    \"\"\"Create application.properties for a given application.\"\"\"\n    logging.basicConfig(format=LOGGING_FORMAT)\n\n    parser = argparse.ArgumentParser(description=main.__doc__)\n\n    add_debug(parser)\n    add_app(parser)\n    add_env(parser)\n    add_properties(parser)\n    add_region(parser)\n    add_artifact_path(parser)\n    add_artifact_version(parser)\n\n    args = parser.parse_args()\n\n    logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n\n    LOG.debug('Args: %s', vars(args))\n\n    rendered_props = get_properties(args.properties)\n    if rendered_props['pipeline']['type'] == 's3':\n        s3app = S3Apps(app=args.app, env=args.env, region=args.region, prop_path=args.properties)\n        s3app.create_bucket()\n\n        s3deploy = S3Deployment(\n            app=args.app,\n            env=args.env,\n            region=args.region,\n            prop_path=args.properties,\n            artifact_path=args.artifact_path,\n            artifact_version=args.artifact_version)\n        s3deploy.upload_artifacts()\n    else:\n        init_properties(**vars(args))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing application. properties file in S3.", "response": "def init_properties(env='dev', app='unnecessary', **_):\n    \"\"\"Make sure _application.properties_ file exists in S3.\n\n    For Applications with Archaius support, there needs to be a file where the\n    cloud environment variable points to.\n\n    Args:\n        env (str): Deployment environment/account, i.e. dev, stage, prod.\n        app (str): GitLab Project name.\n\n    Returns:\n        True when application.properties was found.\n        False when application.properties needed to be created.\n    \"\"\"\n    aws_env = boto3.session.Session(profile_name=env)\n    s3client = aws_env.resource('s3')\n\n    generated = get_details(app=app, env=env)\n    archaius = generated.archaius()\n\n    archaius_file = ('{path}/application.properties').format(path=archaius['path'])\n\n    try:\n        s3client.Object(archaius['bucket'], archaius_file).get()\n        LOG.info('Found: %(bucket)s/%(file)s', {'bucket': archaius['bucket'], 'file': archaius_file})\n        return True\n    except boto3.exceptions.botocore.client.ClientError:\n        s3client.Object(archaius['bucket'], archaius_file).put()\n        LOG.info('Created: %(bucket)s/%(file)s', {'bucket': archaius['bucket'], 'file': archaius_file})\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_cloudwatch_event(app_name, env, region, rules):\n    session = boto3.Session(profile_name=env, region_name=region)\n    cloudwatch_client = session.client('events')\n\n    rule_name = rules.get('rule_name')\n    schedule = rules.get('schedule')\n    rule_description = rules.get('rule_description')\n    json_input = rules.get('json_input', {})\n\n    if schedule is None:\n        LOG.critical('Schedule is required and no schedule is defined!')\n        raise InvalidEventConfiguration('Schedule is required and no schedule is defined!')\n\n    if rule_name is None:\n        LOG.critical('Rule name is required and no rule_name is defined!')\n        raise InvalidEventConfiguration('Rule name is required and no rule_name is defined!')\n    else:\n        LOG.info('%s and %s', app_name, rule_name)\n        rule_name = \"{}_{}\".format(app_name, rule_name.replace(' ', '_'))\n\n    if rule_description is None:\n        rule_description = \"{} - {}\".format(app_name, rule_name)\n\n    lambda_arn = get_lambda_arn(app=app_name, account=env, region=region)\n\n    # Add lambda permissions\n    account_id = get_env_credential(env=env)['accountId']\n    principal = \"events.amazonaws.com\"\n    statement_id = '{}_cloudwatch_{}'.format(app_name, rule_name)\n    source_arn = 'arn:aws:events:{}:{}:rule/{}'.format(region, account_id, rule_name)\n    add_lambda_permissions(\n        function=lambda_arn,\n        statement_id=statement_id,\n        action='lambda:InvokeFunction',\n        principal=principal,\n        source_arn=source_arn,\n        env=env,\n        region=region, )\n\n    # Create Cloudwatch rule\n    cloudwatch_client.put_rule(\n        Name=rule_name,\n        ScheduleExpression=schedule,\n        State='ENABLED',\n        Description=rule_description, )\n\n    targets = []\n    # TODO: read this one from file event-config-*.json\n    json_payload = '{}'.format(json.dumps(json_input))\n\n    target = {\n        \"Id\": app_name,\n        \"Arn\": lambda_arn,\n        \"Input\": json_payload,\n    }\n\n    targets.append(target)\n\n    put_targets_response = cloudwatch_client.put_targets(Rule=rule_name, Targets=targets)\n    LOG.debug('Cloudwatch put targets response: %s', put_targets_response)\n\n    LOG.info('Created Cloudwatch event \"%s\" with schedule: %s', rule_name, schedule)", "response": "Create a CloudWatch event for the given app_name and environment and region."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_env_credential(env='dev'):\n    url = '/'.join([API_URL, 'credentials', env])\n    credential_response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    assert credential_response.ok, 'Could not get credentials from Spinnaker.'\n\n    credential = credential_response.json()\n    LOG.debug('Credentials found:\\n%s', credential)\n    return credential", "response": "Get Account Credential from Spinnaker for a specific environment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_api_id(self):\n        allapis = self.client.get_rest_apis()\n        api_name = self.trigger_settings['api_name']\n        api_id = None\n        for api in allapis['items']:\n            if api['name'] == api_name:\n                api_id = api['id']\n                self.log.info(\"Found API for: %s\", api_name)\n                break\n        else:\n            api_id = self.create_api()\n\n        return api_id", "response": "Given API name find API ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a resource path and API Id find resource Id.", "response": "def find_resource_ids(self):\n        \"\"\"Given a resource path and API Id, find resource Id.\"\"\"\n        all_resources = self.client.get_resources(restApiId=self.api_id)\n        parent_id = None\n        resource_id = None\n        for resource in all_resources['items']:\n            if resource['path'] == \"/\":\n                parent_id = resource['id']\n            if resource['path'] == self.trigger_settings['resource']:\n                resource_id = resource['id']\n                self.log.info(\"Found Resource ID for: %s\", resource['path'])\n        return resource_id, parent_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattaches lambda found to API.", "response": "def add_lambda_integration(self):\n        \"\"\"Attach lambda found to API.\"\"\"\n        lambda_uri = self.generate_uris()['lambda_uri']\n        self.client.put_integration(\n            restApiId=self.api_id,\n            resourceId=self.resource_id,\n            httpMethod=self.trigger_settings['method'],\n            integrationHttpMethod='POST',\n            uri=lambda_uri,\n            type='AWS')\n        self.add_integration_response()\n        self.log.info(\"Successfully added Lambda intergration to API\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_integration_response(self):\n        self.client.put_integration_response(\n            restApiId=self.api_id,\n            resourceId=self.resource_id,\n            httpMethod=self.trigger_settings['method'],\n            statusCode='200',\n            responseTemplates={'application/json': ''})", "response": "Add an intergation response to the API for the lambda integration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_permission(self):\n        statement_id = '{}_api_{}'.format(self.app_name, self.trigger_settings['api_name'])\n        principal = 'apigateway.amazonaws.com'\n        lambda_alias_arn = get_lambda_alias_arn(self.app_name, self.env, self.region)\n        lambda_unqualified_arn = get_lambda_arn(self.app_name, self.env, self.region)\n        resource_name = self.trigger_settings.get('resource', '')\n        resource_name = resource_name.replace('/', '')\n        method_api_source_arn = 'arn:aws:execute-api:{}:{}:{}/{}/{}/{}'.format(\n            self.region, self.account_id, self.api_id, self.env, self.trigger_settings['method'], resource_name)\n        global_api_source_arn = 'arn:aws:execute-api:{}:{}:{}/*/*/{}'.format(self.region, self.account_id, self.api_id,\n                                                                             resource_name)\n        add_lambda_permissions(\n            function=lambda_alias_arn,\n            statement_id=statement_id + self.trigger_settings['method'],\n            action='lambda:InvokeFunction',\n            principal=principal,\n            env=self.env,\n            region=self.region,\n            source_arn=method_api_source_arn)\n        add_lambda_permissions(\n            function=lambda_alias_arn,\n            statement_id=statement_id,\n            action='lambda:InvokeFunction',\n            principal=principal,\n            env=self.env,\n            region=self.region,\n            source_arn=global_api_source_arn)\n        add_lambda_permissions(\n            function=lambda_unqualified_arn,\n            statement_id=statement_id + self.trigger_settings['method'],\n            action='lambda:InvokeFunction',\n            principal=principal,\n            env=self.env,\n            region=self.region,\n            source_arn=method_api_source_arn)\n        add_lambda_permissions(\n            function=lambda_unqualified_arn,\n            statement_id=statement_id,\n            action='lambda:InvokeFunction',\n            principal=principal,\n            env=self.env,\n            region=self.region,\n            source_arn=global_api_source_arn)", "response": "Add permission to Lambda for the API Trigger."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating API deployment of ENV name.", "response": "def create_api_deployment(self):\n        \"\"\"Create API deployment of ENV name.\"\"\"\n        try:\n            self.client.create_deployment(restApiId=self.api_id, stageName=self.env)\n            self.log.info('Created a deployment resource.')\n        except botocore.exceptions.ClientError as error:\n            error_code = error.response['Error']['Code']\n            if error_code == 'TooManyRequestsException':\n                self.log.debug('Retrying. We have hit api limit.')\n            else:\n                self.log.debug('Retrying. We received %s.', error_code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_api_key(self):\n        apikeys = self.client.get_api_keys()\n        for key in apikeys['items']:\n            if key['name'] == self.app_name:\n                self.log.info(\"Key %s already exists\", self.app_name)\n                break\n        else:\n            self.client.create_api_key(\n                name=self.app_name, enabled=True, stageKeys=[{\n                    'restApiId': self.api_id,\n                    'stageName': self.env\n                }])\n            self.log.info(\"Successfully created API Key %s. Look in the AWS console for the key\", self.app_name)", "response": "Create API Key for API access."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _format_base_path(self, api_name):\n        name = self.app_name\n        if self.app_name != api_name:\n            name = '{0}-{1}'.format(self.app_name, api_name)\n        return name", "response": "Format the base path name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_api_mappings(self):\n        response_provider = None\n        response_action = None\n        domain = self.generated.apigateway()['domain']\n        try:\n            response_provider = self.client.create_base_path_mapping(\n                domainName=domain,\n                basePath=self._format_base_path(self.trigger_settings['api_name']),\n                restApiId=self.api_id,\n                stage=self.env, )\n            response_action = 'API mapping added.'\n        except botocore.exceptions.ClientError as error:\n            error_code = error.response['Error']['Code']\n            if error_code == 'ConflictException':\n                response_action = 'API mapping already exist.'\n            else:\n                response_action = 'Unknown error: {0}'.format(error_code)\n\n        self.log.debug('Provider response: %s', response_provider)\n        self.log.info(response_action)\n        return response_provider", "response": "Create a cname for the API deployment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating several lambda uris.", "response": "def generate_uris(self):\n        \"\"\"Generate several lambda uris.\"\"\"\n        lambda_arn = \"arn:aws:execute-api:{0}:{1}:{2}/*/{3}/{4}\".format(self.region, self.account_id, self.api_id,\n                                                                        self.trigger_settings['method'],\n                                                                        self.trigger_settings['resource'])\n\n        lambda_uri = (\"arn:aws:apigateway:{0}:lambda:path/{1}/functions/\"\n                      \"arn:aws:lambda:{0}:{2}:function:{3}/invocations\").format(self.region, self.api_version,\n                                                                                self.account_id, self.app_name)\n\n        api_dns = \"https://{0}.execute-api.{1}.amazonaws.com/{2}\".format(self.api_id, self.region, self.env)\n\n        uri_dict = {'lambda_arn': lambda_arn, 'lambda_uri': lambda_uri, 'api_dns': api_dns}\n        return uri_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_api(self):\n        created_api = self.client.create_rest_api(name=self.trigger_settings.get('api_name', self.app_name))\n        api_id = created_api['id']\n        self.log.info(\"Successfully created API\")\n        return api_id", "response": "Create the REST API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_resource(self, parent_id=\"\"):\n        resource_name = self.trigger_settings.get('resource', '')\n        resource_name = resource_name.replace('/', '')\n        if not self.resource_id:\n            created_resource = self.client.create_resource(\n                restApiId=self.api_id, parentId=parent_id, pathPart=resource_name)\n            self.resource_id = created_resource['id']\n            self.log.info(\"Successfully created resource\")\n        else:\n            self.log.info(\"Resource already exists. To update resource please delete existing resource: %s\",\n                          resource_name)", "response": "Create the resource in API Gateway."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattaches the defined method to the resource.", "response": "def attach_method(self, resource_id):\n        \"\"\"Attach the defined method.\"\"\"\n        try:\n            _response = self.client.put_method(\n                restApiId=self.api_id,\n                resourceId=resource_id,\n                httpMethod=self.trigger_settings['method'],\n                authorizationType=\"NONE\",\n                apiKeyRequired=False, )\n            self.log.debug('Response for resource (%s) push authorization: %s', resource_id, _response)\n            _response = self.client.put_method_response(\n                restApiId=self.api_id,\n                resourceId=resource_id,\n                httpMethod=self.trigger_settings['method'],\n                statusCode='200')\n            self.log.debug('Response for resource (%s) no authorization: %s', resource_id, _response)\n\n            self.log.info(\"Successfully attached method: %s\", self.trigger_settings['method'])\n        except botocore.exceptions.ClientError:\n            self.log.info(\"Method %s already exists\", self.trigger_settings['method'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_cidr(self, rule):\n        try:\n            network = ipaddress.IPv4Network(rule['app'])\n        except (ipaddress.NetmaskValueError, ValueError) as error:\n            raise SpinnakerSecurityGroupCreationFailed(error)\n\n        self.log.debug('Validating CIDR: %s', network.exploded)\n\n        return True", "response": "Validate the cidr block in a rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses rules into cidr and non - cidr lists.", "response": "def _process_rules(self, rules):\n        \"\"\"Process rules into cidr and non-cidr lists.\n\n        Args:\n            rules (list): Allowed Security Group ports and protocols.\n\n        Returns:\n            (list, list): Security Group reference rules and custom CIDR rules.\n        \"\"\"\n        cidr = []\n        non_cidr = []\n\n        for rule in rules:\n            if '.' in rule['app']:\n                self.log.debug('Custom CIDR rule: %s', rule)\n                self._validate_cidr(rule)\n                cidr.append(rule)\n            else:\n                self.log.debug('SG reference rule: %s', rule)\n                non_cidr.append(rule)\n\n        self.log.debug('Custom CIDR rules: %s', cidr)\n        self.log.debug('SG reference rules: %s', non_cidr)\n        return non_cidr, cidr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding tags to security group.", "response": "def add_tags(self):\n        \"\"\"Add tags to security group.\n\n        Returns:\n            True: Upon successful completion.\n        \"\"\"\n        session = boto3.session.Session(profile_name=self.env, region_name=self.region)\n        resource = session.resource('ec2')\n        group_id = get_security_group_id(self.app_name, self.env, self.region)\n        security_group = resource.SecurityGroup(group_id)\n\n        try:\n            tag = security_group.create_tags(\n                DryRun=False,\n                Tags=[{\n                    'Key': 'app_group',\n                    'Value': self.group\n                }, {\n                    'Key': 'app_name',\n                    'Value': self.app_name\n                }])\n            self.log.debug('Security group has been tagged: %s', tag)\n        except botocore.exceptions.ClientError as error:\n            self.log.warning(error)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd CIDR rules to security group via boto3.", "response": "def add_cidr_rules(self, rules):\n        \"\"\"Add cidr rules to security group via boto.\n\n        Args:\n            rules (list): Allowed Security Group ports and protocols.\n\n        Returns:\n            True: Upon successful completion.\n\n        Raises:\n            SpinnakerSecurityGroupError: boto3 call failed to add CIDR block to\n                Security Group.\n        \"\"\"\n        session = boto3.session.Session(profile_name=self.env, region_name=self.region)\n        client = session.client('ec2')\n\n        group_id = get_security_group_id(self.app_name, self.env, self.region)\n\n        for rule in rules:\n            data = {\n                'DryRun':\n                False,\n                'GroupId':\n                group_id,\n                'IpPermissions': [{\n                    'IpProtocol': rule['protocol'],\n                    'FromPort': rule['start_port'],\n                    'ToPort': rule['end_port'],\n                    'IpRanges': [{\n                        'CidrIp': rule['app']\n                    }]\n                }]\n            }\n            self.log.debug('Security Group rule: %s', data)\n\n            try:\n                client.authorize_security_group_ingress(**data)\n            except botocore.exceptions.ClientError as error:\n                if 'InvalidPermission.Duplicate' in str(error):\n                    self.log.debug('Duplicate rule exist, that is OK.')\n                else:\n                    msg = 'Unable to add cidr rules to {}'.format(rule.get('app'))\n                    self.log.error(msg)\n                    raise SpinnakerSecurityGroupError(msg)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves self references to actual application name in security group rules.", "response": "def resolve_self_references(self, rules):\n        \"\"\"Resolves `$self` references to actual application name in security group rules.\"\"\"\n        with suppress(KeyError):\n            rule = rules.pop('$self')\n            rules[self.app_name] = rule\n        return rules"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_default_rules(self):\n        app_ingress = self.properties['security_group']['ingress']\n        ingress = conservative_merger.merge(DEFAULT_SECURITYGROUP_RULES, app_ingress)\n        resolved_ingress = self.resolve_self_references(ingress)\n        self.log.info('Updated default rules:\\n%s', ingress)\n        return resolved_ingress", "response": "Concatinate application and global security group rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_security_group(self, ingress):\n        template_kwargs = {\n            'app': self.app_name,\n            'env': self.env,\n            'region': self.region,\n            'vpc': get_vpc_id(self.env, self.region),\n            'description': self.properties['security_group']['description'],\n            'ingress': ingress,\n        }\n\n        secgroup_json = get_template(\n            template_file='infrastructure/securitygroup_data.json.j2', formats=self.generated, **template_kwargs)\n\n        wait_for_task(secgroup_json)\n        return True", "response": "Send a POST to spinnaker to create a new security group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a security group.", "response": "def create_security_group(self):  # noqa\n        \"\"\"Send a POST to spinnaker to create or update a security group.\n\n        Returns:\n            boolean: True if created successfully\n\n        Raises:\n            ForemastConfigurationFileError: Missing environment configuration or\n                misconfigured Security Group definition.\n        \"\"\"\n        ingress_rules = []\n\n        try:\n            security_id = get_security_group_id(name=self.app_name, env=self.env, region=self.region)\n        except (SpinnakerSecurityGroupError, AssertionError):\n            self._create_security_group(ingress_rules)\n        else:\n            self.log.debug('Security Group ID %s found for %s.', security_id, self.app_name)\n\n        try:\n            ingress = self.update_default_rules()\n        except KeyError:\n            msg = 'Possible missing configuration for \"{0}\".'.format(self.env)\n            self.log.error(msg)\n            raise ForemastConfigurationFileError(msg)\n\n        for app in ingress:\n            rules = ingress[app]\n\n            # Essentially we have two formats: simple, advanced\n            # - simple: is just a list of ports\n            # - advanced: selects ports ranges and protocols\n            for rule in rules:\n                ingress_rule = self.create_ingress_rule(app, rule)\n                ingress_rules.append(ingress_rule)\n\n        ingress_rules_no_cidr, ingress_rules_cidr = self._process_rules(ingress_rules)\n\n        self._create_security_group(ingress_rules_no_cidr)\n\n        # Append cidr rules\n        self.add_cidr_rules(ingress_rules_cidr)\n\n        # Tag security group\n        self.add_tags()\n\n        self.log.info('Successfully created %s security group', self.app_name)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_ingress_rule(self, app, rule):\n        if isinstance(rule, dict):\n            # Advanced\n            start_port = rule.get('start_port')\n            end_port = rule.get('end_port')\n            protocol = rule.get('protocol', 'tcp')\n\n            requested_cross_account = rule.get('env', self.env)\n            if self.env == requested_cross_account:\n                # We are trying to use cross-account security group settings within the same account\n                # We should not allow this.\n                cross_account_env = None\n                cross_account_vpc_id = None\n            else:\n                cross_account_env = requested_cross_account\n                cross_account_vpc_id = get_vpc_id(cross_account_env, self.region)\n\n        else:\n            start_port = rule\n            end_port = rule\n            protocol = 'tcp'\n            cross_account_env = None\n            cross_account_vpc_id = None\n\n        created_rule = {\n            'app': app,\n            'start_port': start_port,\n            'end_port': end_port,\n            'protocol': protocol,\n            'cross_account_env': cross_account_env,\n            'cross_account_vpc_id': cross_account_vpc_id\n        }\n        self.log.debug('Normalized ingress rule: %s', created_rule)\n        return created_rule", "response": "Create a normalized ingress rule."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_lambda_arn(app, account, region):\n    session = boto3.Session(profile_name=account, region_name=region)\n    lambda_client = session.client('lambda')\n\n    lambda_arn = None\n    paginator = lambda_client.get_paginator('list_functions')\n\n    for lambda_functions in paginator.paginate():\n        for lambda_function in lambda_functions['Functions']:\n            if lambda_function['FunctionName'] == app:\n                lambda_arn = lambda_function['FunctionArn']\n                LOG.debug(\"Lambda ARN for lambda function %s is %s.\", app, lambda_arn)\n                break\n        if lambda_arn:\n            break\n\n    if not lambda_arn:\n        LOG.fatal('Lambda function with name %s not found in %s %s', app, account, region)\n        raise LambdaFunctionDoesNotExist(\n            'Lambda function with name {0} not found in {1} {2}'.format(app, account, region))\n\n    return lambda_arn", "response": "Get lambda ARN for a given app name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_lambda_alias_arn(app, account, region):\n    session = boto3.Session(profile_name=account, region_name=region)\n    lambda_client = session.client('lambda')\n\n    lambda_aliases = lambda_client.list_aliases(FunctionName=app)\n\n    matched_alias = None\n    for alias in lambda_aliases['Aliases']:\n        if alias['Name'] == account:\n            lambda_alias_arn = alias['AliasArn']\n            LOG.info('Found ARN for alias %s for function %s', account, app)\n            matched_alias = lambda_alias_arn\n            break\n    else:\n        fatal_message = 'Lambda alias {0} of function {1} not found'.format(account, app)\n        LOG.fatal(fatal_message)\n        raise LambdaAliasDoesNotExist(fatal_message)\n    return matched_alias", "response": "Get lambda alias ARN."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd permissions to Lambda for the event trigger.", "response": "def add_lambda_permissions(function='',\n                           statement_id='',\n                           action='lambda:InvokeFunction',\n                           principal='',\n                           source_arn='',\n                           env='',\n                           region='us-east-1'):\n    \"\"\"Add permission to Lambda for the event trigger.\n\n    Args:\n        function (str): Lambda function name\n        statement_id (str): IAM policy statement (principal) id\n        action (str): Lambda action to allow\n        principal (str): AWS principal to add permissions\n        source_arn (str): ARN of the source of the event. Only needed for S3\n        env (str): Environment/account of function\n        region (str): AWS region of function\n    \"\"\"\n    session = boto3.Session(profile_name=env, region_name=region)\n    lambda_client = session.client('lambda')\n    response_action = None\n    prefixed_sid = FOREMAST_PREFIX + statement_id\n\n    add_permissions_kwargs = {\n        'FunctionName': function,\n        'StatementId': prefixed_sid,\n        'Action': action,\n        'Principal': principal,\n    }\n\n    if source_arn:\n        add_permissions_kwargs['SourceArn'] = source_arn\n\n    try:\n        lambda_client.add_permission(**add_permissions_kwargs)\n        response_action = 'Add permission with Sid: {}'.format(prefixed_sid)\n    except boto3.exceptions.botocore.exceptions.ClientError as error:\n        LOG.debug('Add permission error: %s', error)\n        response_action = \"Did not add permissions\"\n\n    LOG.debug('Related StatementId (SID): %s', prefixed_sid)\n    LOG.info(response_action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves all foremast - * permissions from lambda.", "response": "def remove_all_lambda_permissions(app_name='', env='', region='us-east-1'):\n    \"\"\"Remove all foremast-* permissions from lambda.\n\n    Args:\n        app_name (str): Application name\n        env (str): AWS environment\n        region (str): AWS region\n    \"\"\"\n    session = boto3.Session(profile_name=env, region_name=region)\n    lambda_client = session.client('lambda')\n    legacy_prefix = app_name + \"_\"\n\n    lambda_arn = get_lambda_arn(app_name, env, region)\n    lambda_alias_arn = get_lambda_alias_arn(app_name, env, region)\n    arns = (lambda_arn, lambda_alias_arn)\n\n    for arn in arns:\n        try:\n            response = lambda_client.get_policy(FunctionName=arn)\n        except boto3.exceptions.botocore.exceptions.ClientError as error:\n            LOG.info(\"No policy exists for function %s, skipping deletion\", arn)\n            LOG.debug(error)\n            continue\n\n        policy_json = json.loads(response['Policy'])\n        LOG.debug(\"Found Policy: %s\", response)\n        for perm in policy_json['Statement']:\n            if perm['Sid'].startswith(FOREMAST_PREFIX) or perm['Sid'].startswith(legacy_prefix):\n                lambda_client.remove_permission(FunctionName=arn, StatementId=perm['Sid'])\n                LOG.info('removed permission: %s', perm['Sid'])\n            else:\n                LOG.info('Skipping deleting permission %s - Not managed by Foremast', perm['Sid'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall _action_ using boto3 _client_ with _kwargs_. This is meant for _action_ methods that will create or implicitely prove a given Resource exists. The _log_failure_ flag is available for methods that should always succeed, but will occasionally fail due to unknown AWS issues. Args: client (botocore.client.IAM): boto3 client object. action (str): Client method to call. log_format (str): Generic log message format, 'Added' or 'Found' will be prepended depending on the scenario. prefix (str): Prefix word to use in successful INFO message. **kwargs: Keyword arguments to pass to _action_ method. Returns: dict: boto3 response.", "response": "def resource_action(client, action='', log_format='item: %(key)s', **kwargs):\n    \"\"\"Call _action_ using boto3 _client_ with _kwargs_.\n\n    This is meant for _action_ methods that will create or implicitely prove a\n    given Resource exists. The _log_failure_ flag is available for methods that\n    should always succeed, but will occasionally fail due to unknown AWS\n    issues.\n\n    Args:\n        client (botocore.client.IAM): boto3 client object.\n        action (str): Client method to call.\n        log_format (str): Generic log message format, 'Added' or 'Found' will\n            be prepended depending on the scenario.\n        prefix (str): Prefix word to use in successful INFO message.\n        **kwargs: Keyword arguments to pass to _action_ method.\n\n    Returns:\n        dict: boto3 response.\n    \"\"\"\n    result = None\n\n    try:\n        result = getattr(client, action)(**kwargs)\n        LOG.info(log_format, kwargs)\n    except botocore.exceptions.ClientError as error:\n        error_code = error.response['Error']['Code']\n\n        if error_code == 'AccessDenied':\n            LOG.fatal(error)\n            raise\n        elif error_code == 'EntityAlreadyExists':\n            LOG.info(' '.join(('Found', log_format)), kwargs)\n        else:\n            LOG.fatal(error)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    logging.basicConfig(format=LOGGING_FORMAT)\n\n    parser = argparse.ArgumentParser(description='Example with non-optional arguments')\n\n    add_debug(parser)\n    add_app(parser)\n    add_env(parser)\n    add_region(parser)\n    add_properties(parser)\n\n    args = parser.parse_args()\n\n    logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n\n    elb = SpinnakerELB(app=args.app, env=args.env, region=args.region, prop_path=args.properties)\n    elb.create_elb()", "response": "Entry point for ELB creation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_elb_json(self):\n        env = self.env\n        region = self.region\n        elb_settings = self.properties['elb']\n        LOG.debug('Block ELB Settings:\\n%s', pformat(elb_settings))\n\n        health_settings = elb_settings['health']\n        elb_subnet_purpose = elb_settings.get('subnet_purpose', 'internal')\n\n        region_subnets = get_subnets(target='elb', purpose=elb_subnet_purpose, env=env, region=region)\n        region_subnets.pop(\"subnet_ids\", None)\n\n        # CAVEAT: Setting the ELB to public, you must use a public subnet,\n        #         otherwise AWS complains about missing IGW on subnet.\n\n        if elb_subnet_purpose == 'internal':\n            is_internal = 'true'\n        else:\n            is_internal = 'false'\n\n        target = elb_settings.get('target', 'HTTP:80/health')\n        health = splay_health(target)\n\n        listeners = format_listeners(elb_settings=elb_settings, env=self.env, region=region)\n\n        idle_timeout = elb_settings.get('idle_timeout', None)\n        access_log = elb_settings.get('access_log', {})\n        connection_draining_timeout = elb_settings.get('connection_draining_timeout', None)\n\n        security_groups = DEFAULT_ELB_SECURITYGROUPS[env]\n        security_groups.append(self.app)\n        security_groups.extend(self.properties['security_group']['elb_extras'])\n        security_groups = remove_duplicate_sg(security_groups)\n\n        template_kwargs = {\n            'access_log': json.dumps(access_log),\n            'app_name': self.app,\n            'availability_zones': json.dumps(region_subnets),\n            'connection_draining_timeout': json.dumps(connection_draining_timeout),\n            'env': env,\n            'hc_string': target,\n            'health_interval': health_settings['interval'],\n            'health_path': health.path,\n            'health_port': health.port,\n            'health_protocol': health.proto,\n            'health_timeout': health_settings['timeout'],\n            'healthy_threshold': health_settings['threshold'],\n            'idle_timeout': json.dumps(idle_timeout),\n            'isInternal': is_internal,\n            'listeners': json.dumps(listeners),\n            'region_zones': json.dumps(region_subnets[region]),\n            'region': region,\n            'security_groups': json.dumps(security_groups),\n            'subnet_type': elb_subnet_purpose,\n            'unhealthy_threshold': health_settings['unhealthy_threshold'],\n            'vpc_id': get_vpc_id(env, region),\n        }\n\n        rendered_template = get_template(template_file='infrastructure/elb_data.json.j2', **template_kwargs)\n\n        return rendered_template", "response": "Render the JSON template with arguments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_elb(self):\n\n        json_data = self.make_elb_json()\n        LOG.debug('Block ELB JSON Data:\\n%s', pformat(json_data))\n\n        wait_for_task(json_data)\n\n        self.add_listener_policy(json_data)\n        self.add_backend_policy(json_data)\n\n        self.configure_attributes(json_data)", "response": "Create or Update the ELB after rendering JSON data from configs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_listener_policy(self, json_data):\n        env = boto3.session.Session(profile_name=self.env, region_name=self.region)\n        elbclient = env.client('elb')\n\n        # create stickiness policy if set in configs\n        stickiness = {}\n        elb_settings = self.properties['elb']\n        if elb_settings.get('ports'):\n            ports = elb_settings['ports']\n            for listener in ports:\n                if listener.get(\"stickiness\"):\n                    stickiness = self.add_stickiness()\n                    LOG.info('Stickiness Found: %s', stickiness)\n                    break\n\n        # Attach policies to created ELB\n        for job in json.loads(json_data)['job']:\n            for listener in job['listeners']:\n                policies = []\n                ext_port = listener['externalPort']\n                if listener['listenerPolicies']:\n                    policies.extend(listener['listenerPolicies'])\n                if stickiness.get(ext_port):\n                    policies.append(stickiness.get(ext_port))\n                if policies:\n                    LOG.info('Adding listener policies: %s', policies)\n                    elbclient.set_load_balancer_policies_of_listener(\n                        LoadBalancerName=self.app, LoadBalancerPort=ext_port, PolicyNames=policies)", "response": "Attaches listerner policies to an ELB"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_backend_policy(self, json_data):\n        env = boto3.session.Session(profile_name=self.env, region_name=self.region)\n        elbclient = env.client('elb')\n\n        # Attach backend server policies to created ELB\n        for job in json.loads(json_data)['job']:\n            for listener in job['listeners']:\n                instance_port = listener['internalPort']\n                backend_policy_list = listener['backendPolicies']\n                if backend_policy_list:\n                    LOG.info('Adding backend server policies: %s', backend_policy_list)\n                    elbclient.set_load_balancer_policies_for_backend_server(\n                        LoadBalancerName=self.app, InstancePort=instance_port, PolicyNames=backend_policy_list)", "response": "Attaches backend server policies to an ELB"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd stickiness policy to created ELB", "response": "def add_stickiness(self):\n        \"\"\" Adds stickiness policy to created ELB\n\n        Returns:\n            dict: A dict of stickiness policies and ports::\n\n                example:\n                {\n                    80: \"$policy_name\"\n                }\n        \"\"\"\n        stickiness_dict = {}\n        env = boto3.session.Session(profile_name=self.env, region_name=self.region)\n        elbclient = env.client('elb')\n        elb_settings = self.properties['elb']\n        for listener in elb_settings.get('ports'):\n            if listener.get(\"stickiness\"):\n                sticky_type = listener['stickiness']['type'].lower()\n                externalport = int(listener['loadbalancer'].split(\":\")[-1])\n                policyname_tmp = \"{0}-{1}-{2}-{3}\"\n                if sticky_type == 'app':\n                    cookiename = listener['stickiness']['cookie_name']\n                    policy_key = cookiename.replace('.', '')\n                    policyname = policyname_tmp.format(self.app, sticky_type, externalport, policy_key)\n                    elbclient.create_app_cookie_stickiness_policy(\n                        LoadBalancerName=self.app, PolicyName=policyname, CookieName=cookiename)\n                    stickiness_dict[externalport] = policyname\n                elif sticky_type == 'elb':\n                    cookie_ttl = listener['stickiness'].get('cookie_ttl', None)\n                    policyname = policyname_tmp.format(self.app, sticky_type, externalport, cookie_ttl)\n                    if cookie_ttl:\n                        elbclient.create_lb_cookie_stickiness_policy(\n                            LoadBalancerName=self.app, PolicyName=policyname, CookieExpirationPeriod=cookie_ttl)\n                    else:\n                        elbclient.create_lb_cookie_stickiness_policy(LoadBalancerName=self.app, PolicyName=policyname)\n                    stickiness_dict[externalport] = policyname\n        return stickiness_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configure_attributes(self, json_data):\n        env = boto3.session.Session(profile_name=self.env, region_name=self.region)\n        elbclient = env.client('elb')\n\n        elb_settings = self.properties['elb']\n        LOG.debug('Block ELB Settings Pre Configure Load Balancer Attributes:\\n%s', pformat(elb_settings))\n\n        # FIXME: Determine why 'job' is not being used\n        # pylint: disable=unused-variable\n        for job in json.loads(json_data)['job']:\n            load_balancer_attributes = {\n                'CrossZoneLoadBalancing': {\n                    'Enabled': True\n                },\n                'AccessLog': {\n                    'Enabled': False,\n                },\n                'ConnectionDraining': {\n                    'Enabled': False,\n                },\n                'ConnectionSettings': {\n                    'IdleTimeout': 60\n                }\n            }\n            if elb_settings.get('connection_draining_timeout'):\n                connection_draining_timeout = int(elb_settings['connection_draining_timeout'])\n                LOG.info('Applying Custom Load Balancer Connection Draining Timeout: %d', connection_draining_timeout)\n                load_balancer_attributes['ConnectionDraining'] = {\n                    'Enabled': True,\n                    'Timeout': connection_draining_timeout\n                }\n            if elb_settings.get('idle_timeout'):\n                idle_timeout = int(elb_settings['idle_timeout'])\n                LOG.info('Applying Custom Load Balancer Idle Timeout: %d', idle_timeout)\n                load_balancer_attributes['ConnectionSettings'] = {'IdleTimeout': idle_timeout}\n            if elb_settings.get('access_log'):\n                access_log_bucket_name = elb_settings['access_log']['bucket_name']\n                access_log_bucket_prefix = elb_settings['access_log']['bucket_prefix']\n                access_log_emit_interval = int(elb_settings['access_log']['emit_interval'])\n                LOG.info('Applying Custom Load Balancer Access Log: %s/%s every %d minutes', access_log_bucket_name,\n                         access_log_bucket_prefix, access_log_emit_interval)\n                load_balancer_attributes['AccessLog'] = {\n                    'Enabled': True,\n                    'S3BucketName': access_log_bucket_name,\n                    'EmitInterval': access_log_emit_interval,\n                    'S3BucketPrefix': access_log_bucket_prefix\n                }\n\n            LOG.info('Applying Load Balancer Attributes')\n            LOG.debug('Load Balancer Attributes:\\n%s', pformat(load_balancer_attributes))\n            elbclient.modify_load_balancer_attributes(\n                LoadBalancerName=self.app, LoadBalancerAttributes=load_balancer_attributes)", "response": "Configure attributes of load balancer"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset health check path port and protocol.", "response": "def splay_health(health_target):\n    \"\"\"Set Health Check path, port, and protocol.\n\n    Args:\n        health_target (str): The health target. ie ``HTTP:80``\n    Returns:\n        HealthCheck: A **collections.namedtuple** class with *path*, *port*,\n        *proto*, and *target* attributes.\n    \"\"\"\n    HealthCheck = collections.namedtuple('HealthCheck', ['path', 'port', 'proto', 'target'])\n\n    proto, health_port_path = health_target.split(':')\n    port, *health_path = health_port_path.split('/')\n\n    if proto == 'TCP':\n        path = ''\n    elif not health_path:\n        path = '/healthcheck'\n    else:\n        path = '/{0}'.format('/'.join(health_path))\n\n    target = '{0}:{1}{2}'.format(proto, port, path)\n\n    health = HealthCheck(path, port, proto, target)\n    LOG.info(health)\n\n    return health"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post_pipeline(self, pipeline):\n        if isinstance(pipeline, str):\n            pipeline_str = pipeline\n        else:\n            pipeline_str = json.dumps(pipeline)\n\n        pipeline_json = json.loads(pipeline_str)\n\n        # Note pipeline name is manual\n        name = '{0} (onetime-{1})'.format(pipeline_json['name'], self.environments[0])\n        pipeline_json['name'] = name\n\n        # Inject pipeline Id so that it does not override existing pipelines\n        pipeline_id = super().compare_with_existing(onetime=True)\n        if pipeline_id:\n            pipeline_json['id'] = pipeline_id\n        else:\n            del pipeline_json['id']\n\n        # disable trigger as not to accidently kick off multiple deployments\n        for trigger in pipeline_json['triggers']:\n            trigger['enabled'] = False\n\n        self.log.debug('Manual Pipeline JSON:\\n%s', pipeline_json)\n        super().post_pipeline(pipeline_json)", "response": "Send JSON to Spinnaker."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend Application Configurations to a given file in multiple formats.", "response": "def main():\n    \"\"\"Append Application Configurations to a given file in multiple formats.\"\"\"\n    logging.basicConfig(format=LOGGING_FORMAT)\n\n    parser = argparse.ArgumentParser(description=main.__doc__)\n    add_debug(parser)\n    parser.add_argument('-o', '--output', required=True, help='Name of environment file to append to')\n    parser.add_argument(\n        '-g', '--git-short', metavar='GROUP/PROJECT', required=True, help='Short name for Git, e.g. forrest/core')\n    parser.add_argument('-r', '--runway-dir', help='Runway directory with app.json files, requires --git-short')\n    args = parser.parse_args()\n\n    LOG.setLevel(args.debug)\n    logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n\n    generated = gogoutils.Generator(*gogoutils.Parser(args.git_short).parse_url(), formats=APP_FORMATS)\n    git_short = generated.gitlab()['main']\n\n    if args.runway_dir:\n        configs = process_runway_configs(runway_dir=args.runway_dir)\n    else:\n        configs = process_git_configs(git_short=git_short)\n\n    write_variables(app_configs=configs, out_file=args.output, git_short=git_short)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_autoscaling(subparsers):\n    autoscaling_parser = subparsers.add_parser(\n        'autoscaling',\n        help=runner.create_scaling_policy.__doc__,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    autoscaling_parser.set_defaults(func=runner.create_scaling_policy)", "response": "Auto Scaling Group Policy subcommands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(manual_args=None):\n    parser = argparse.ArgumentParser(description=main.__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.set_defaults(func=parser.print_help)\n    add_debug(parser)\n    parser.add_argument(\n        '-s',\n        '--short-log',\n        action='store_const',\n        const=SHORT_LOGGING_FORMAT,\n        default=LOGGING_FORMAT,\n        help='Truncated logging format')\n    parser.add_argument('-v', '--version', action='store_true', help=print_version.__doc__)\n\n    subparsers = parser.add_subparsers(title='Commands', description='Available activies')\n\n    add_infra(subparsers)\n    add_pipeline(subparsers)\n    add_rebuild(subparsers)\n    add_autoscaling(subparsers)\n    add_validate(subparsers)\n\n    CliArgs = collections.namedtuple('CliArgs', ['parsed', 'extra'])\n\n    parsed, extra = parser.parse_known_args(args=manual_args)\n    args = CliArgs(parsed, extra)\n\n    logging.basicConfig(format=args.parsed.short_log)\n\n    package, *_ = __package__.split('.')\n    logging.getLogger(package).setLevel(args.parsed.debug)\n\n    LOG.debug('Arguments: %s', args)\n\n    if args.parsed.version:\n        args.parsed.func = print_version\n\n    try:\n        args.parsed.func(args)\n    except (AttributeError, TypeError):\n        args.parsed.func()", "response": "Main function for the forest s support."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_pipeline(self, pipeline):\n        url = \"{0}/pipelines\".format(API_URL)\n\n        if isinstance(pipeline, str):\n            pipeline_json = pipeline\n        else:\n            pipeline_json = json.dumps(pipeline)\n\n        pipeline_dict = json.loads(pipeline_json)\n\n        self.log.debug('Pipeline JSON:\\n%s', pipeline_json)\n\n        pipeline_response = requests.post(\n            url, data=pipeline_json, headers=self.header, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n        self.log.debug('Pipeline creation response:\\n%s', pipeline_response.text)\n\n        if not pipeline_response.ok:\n            raise SpinnakerPipelineCreationFailed('Pipeline for {0}: {1}'.format(self.app_name,\n                                                                                 pipeline_response.json()))\n\n        self.log.info('Successfully created \"%s\" pipeline in application \"%s\".', pipeline_dict['name'],\n                      pipeline_dict['application'])", "response": "Send JSON to Spinnaker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_wrapper(self, region='us-east-1'):\n        base = self.settings['pipeline']['base']\n\n        if self.base:\n            base = self.base\n\n        email = self.settings['pipeline']['notifications']['email']\n        slack = self.settings['pipeline']['notifications']['slack']\n        baking_process = self.settings['pipeline']['image']['builder']\n        provider = 'aws'\n        root_volume_size = self.settings['pipeline']['image']['root_volume_size']\n        bake_instance_type = self.settings['pipeline']['image']['bake_instance_type']\n\n        ami_id = ami_lookup(name=base, region=region)\n\n        ami_template_file = generate_packer_filename(provider, region, baking_process)\n\n        pipeline_id = self.compare_with_existing(region=region)\n\n        data = {\n            'app': {\n                'ami_id': ami_id,\n                'appname': self.app_name,\n                'group_name': self.group_name,\n                'repo_name': self.repo_name,\n                'base': base,\n                'environment': 'packaging',\n                'region': region,\n                'triggerjob': self.trigger_job,\n                'run_as_user': DEFAULT_RUN_AS_USER,\n                'email': email,\n                'slack': slack,\n                'root_volume_size': root_volume_size,\n                'bake_instance_type': bake_instance_type,\n                'ami_template_file': ami_template_file,\n                'pipeline': self.settings['pipeline']\n            },\n            'id': pipeline_id\n        }\n\n        self.log.debug('Wrapper app data:\\n%s', pformat(data))\n\n        wrapper = get_template(template_file='pipeline/pipeline_wrapper.json.j2', data=data, formats=self.generated)\n\n        return json.loads(wrapper)", "response": "Generate the base Pipeline wrapper."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting existing pipeline configs for specific application.", "response": "def get_existing_pipelines(self):\n        \"\"\"Get existing pipeline configs for specific application.\n\n        Returns:\n            str: Pipeline config json\n\n        \"\"\"\n        url = \"{0}/applications/{1}/pipelineConfigs\".format(API_URL, self.app_name)\n        resp = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n        assert resp.ok, 'Failed to lookup pipelines for {0}: {1}'.format(self.app_name, resp.text)\n\n        return resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compare_with_existing(self, region='us-east-1', onetime=False):\n        pipelines = self.get_existing_pipelines()\n        pipeline_id = None\n        found = False\n        for pipeline in pipelines:\n            correct_app_and_region = (pipeline['application'] == self.app_name) and (region in pipeline['name'])\n            if onetime:\n                onetime_str = \"(onetime-{})\".format(self.environments[0])\n                if correct_app_and_region and onetime_str in pipeline['name']:\n                    found = True\n            elif correct_app_and_region:\n                found = True\n\n            if found:\n                self.log.info('Existing pipeline found - %s', pipeline['name'])\n                pipeline_id = pipeline['id']\n                break\n        else:\n            self.log.info('No existing pipeline found')\n\n        return pipeline_id", "response": "Compare the desired pipeline with existing pipelines."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_pipeline(self):\n        clean_pipelines(app=self.app_name, settings=self.settings)\n\n        pipeline_envs = self.environments\n        self.log.debug('Envs from pipeline.json: %s', pipeline_envs)\n\n        regions_envs = collections.defaultdict(list)\n        for env in pipeline_envs:\n            for region in self.settings[env]['regions']:\n                regions_envs[region].append(env)\n        self.log.info('Environments and Regions for Pipelines:\\n%s', json.dumps(regions_envs, indent=4))\n\n        subnets = None\n        pipelines = {}\n        for region, envs in regions_envs.items():\n            self.generated.data.update({\n                'region': region,\n            })\n\n            # TODO: Overrides for an environment no longer makes sense. Need to\n            # provide override for entire Region possibly.\n            pipelines[region] = self.render_wrapper(region=region)\n\n            previous_env = None\n            for env in envs:\n                self.generated.data.update({\n                    'env': env,\n                })\n\n                pipeline_block_data = {\n                    \"env\": env,\n                    \"generated\": self.generated,\n                    \"previous_env\": previous_env,\n                    \"region\": region,\n                    \"settings\": self.settings[env][region],\n                    \"pipeline_data\": self.settings['pipeline'],\n                }\n\n                if self.settings['pipeline']['type'] in EC2_PIPELINE_TYPES:\n                    if not subnets:\n                        subnets = get_subnets()\n                    try:\n                        region_subnets = {region: subnets[env][region]}\n                    except KeyError:\n                        self.log.info('%s is not available for %s.', env, region)\n                        continue\n                    pipeline_block_data['region_subnets'] = region_subnets\n\n                block = construct_pipeline_block(**pipeline_block_data)\n                pipelines[region]['stages'].extend(json.loads(block))\n                previous_env = env\n\n        self.log.debug('Assembled Pipelines:\\n%s', pformat(pipelines))\n\n        for region, pipeline in pipelines.items():\n            renumerate_stages(pipeline)\n\n            self.post_pipeline(pipeline)\n\n        return True", "response": "Create a new pipeline and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlooks up AMI ID.", "response": "def ami_lookup(region='us-east-1', name='tomcat8'):\n    \"\"\"Look up AMI ID.\n\n    Use _name_ to find AMI ID. If no ami_base_url or gitlab_token is provided,\n    _name_ is returned as the ami id.\n\n    Args:\n        region (str): AWS Region to find AMI ID.\n        name (str): Simple AMI base name to lookup.\n\n    Returns:\n        str: AMI ID for _name_ in _region_.\n\n    \"\"\"\n    if AMI_JSON_URL:\n        ami_dict = _get_ami_dict(AMI_JSON_URL)\n        ami_id = ami_dict[region][name]\n    elif GITLAB_TOKEN:\n        warn_user('Use AMI_JSON_URL feature instead.')\n        ami_contents = _get_ami_file(region=region)\n        ami_dict = json.loads(ami_contents)\n        ami_id = ami_dict[name]\n    else:\n        ami_id = name\n\n    LOG.info('Using AMI: %s', ami_id)\n\n    return ami_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_ami_file(region='us-east-1'):\n    LOG.info(\"Getting AMI from Gitlab\")\n    lookup = FileLookup(git_short='devops/ansible')\n    filename = 'scripts/{0}.json'.format(region)\n    ami_contents = lookup.remote_file(filename=filename, branch='master')\n    LOG.debug('AMI file contents in %s: %s', filename, ami_contents)\n    return ami_contents", "response": "Get AMI file from Gitlab."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_ami_dict(json_url):\n    LOG.info(\"Getting AMI from %s\", json_url)\n    response = requests.get(json_url)\n    assert response.ok, \"Error getting ami info from {}\".format(json_url)\n    ami_dict = response.json()\n    LOG.debug('AMI json contents: %s', ami_dict)\n    return ami_dict", "response": "Get AMI from a web url."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets numerical GitLab Project ID.", "response": "def get_gitlab_project(self):\n        \"\"\"Get numerical GitLab Project ID.\n\n        Returns:\n            int: Project ID number.\n\n        Raises:\n            foremast.exceptions.GitLabApiError: GitLab responded with bad status\n                code.\n\n        \"\"\"\n        self.server = gitlab.Gitlab(GIT_URL, private_token=GITLAB_TOKEN, api_version=4)\n        project = self.server.projects.get(self.git_short)\n\n        if not project:\n            raise GitLabApiError('Could not get Project \"{0}\" from GitLab API.'.format(self.git_short))\n\n        self.project = project\n        return self.project"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef local_file(self, filename):\n        LOG.info('Retrieving \"%s\" from \"%s\".', filename, self.runway_dir)\n\n        file_contents = ''\n\n        file_path = os.path.join(self.runway_dir, filename)\n\n        try:\n            with open(file_path, 'rt') as lookup_file:\n                file_contents = lookup_file.read()\n        except FileNotFoundError:\n            LOG.warning('File missing \"%s\".', file_path)\n            raise\n\n        LOG.debug('Local file contents:\\n%s', file_contents)\n        return file_contents", "response": "Read the local file in _self. runway_dir_."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remote_file(self, branch='master', filename=''):\n        LOG.info('Retrieving \"%s\" from \"%s\".', filename, self.git_short)\n\n        file_contents = ''\n\n        try:\n            file_blob = self.project.files.get(file_path=filename, ref=branch)\n        except gitlab.exceptions.GitlabGetError:\n            file_blob = None\n\n        LOG.debug('GitLab file response:\\n%s', file_blob)\n\n        if not file_blob:\n            msg = 'Project \"{0}\" is missing file \"{1}\" in \"{2}\" branch.'.format(self.git_short, filename, branch)\n            LOG.warning(msg)\n            raise FileNotFoundError(msg)\n        else:\n            file_contents = b64decode(file_blob.content).decode()\n\n        LOG.debug('Remote file contents:\\n%s', file_contents)\n        return file_contents", "response": "Read the remote file on Git Server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves _filename_ from GitLab.", "response": "def get(self, branch='master', filename=''):\n        \"\"\"Retrieve _filename_ from GitLab.\n\n        Args:\n            branch (str): Git Branch to find file.\n            filename (str): Name of file to retrieve relative to root of Git\n                repository, or _runway_dir_ if specified.\n\n        Returns:\n            str: Contents of file.\n\n        \"\"\"\n        file_contents = ''\n\n        if self.runway_dir:\n            file_contents = self.local_file(filename=filename)\n        else:\n            file_contents = self.remote_file(branch=branch, filename=filename)\n\n        return file_contents"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves _filename_ from GitLab.", "response": "def json(self, branch='master', filename=''):\n        \"\"\"Retrieve _filename_ from GitLab.\n\n        Args:\n            branch (str): Git Branch to find file.\n            filename (str): Name of file to retrieve.\n\n        Returns:\n            dict: Decoded JSON.\n\n        Raises:\n            SystemExit: Invalid JSON provided.\n\n        \"\"\"\n        file_contents = self.get(branch=branch, filename=filename)\n\n        try:\n            json_dict = json.loads(file_contents)\n        # TODO: Use json.JSONDecodeError when Python 3.4 has been deprecated\n        except ValueError as error:\n            msg = ('\"{filename}\" appears to be invalid json. '\n                   'Please validate it with http://jsonlint.com. '\n                   'JSON decoder error:\\n'\n                   '{error}').format(\n                       filename=filename, error=error)\n            raise SystemExit(msg)\n\n        LOG.debug('JSON object:\\n%s', json_dict)\n        return json_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncentering text in a banner with _border_ characters.", "response": "def banner(text, border='=', width=80):\n    \"\"\"Center _text_ in a banner _width_ wide with _border_ characters.\n\n    Args:\n        text (str): What to write in the banner\n        border (str): Border character\n        width (int): How long the border should be\n    \"\"\"\n    text_padding = '{0:^%d}' % (width)\n    LOG.info(border * width)\n    LOG.info(text_padding.format(text))\n    LOG.info(border * width)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sns_topic_arn(topic_name, account, region):\n    if topic_name.count(':') == 5 and topic_name.startswith('arn:aws:sns:'):\n        return topic_name\n    session = boto3.Session(profile_name=account, region_name=region)\n    sns_client = session.client('sns')\n\n    topics = sns_client.list_topics()['Topics']\n\n    matched_topic = None\n    for topic in topics:\n        topic_arn = topic['TopicArn']\n        if topic_name == topic_arn.split(':')[-1]:\n            matched_topic = topic_arn\n            break\n    else:\n        LOG.critical(\"No topic with name %s found.\", topic_name)\n        raise SNSTopicNotFound('No topic with name {0} found'.format(topic_name))\n    return matched_topic", "response": "Get SNS topic ARN."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nposts message to a defined Slack channel.", "response": "def notify_slack_channel(self):\n        \"\"\"Post message to a defined Slack channel.\"\"\"\n        message = get_template(template_file='slack/pipeline-prepare-ran.j2', info=self.info)\n\n        if self.settings['pipeline']['notifications']['slack']:\n            post_slack_message(\n                message=message,\n                channel=self.settings['pipeline']['notifications']['slack'],\n                username='pipeline-bot',\n                icon_emoji=':gear:')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget contents of _properties_file_ for the given environment.", "response": "def get_properties(properties_file='raw.properties.json', env=None, region=None):\n    \"\"\"Get contents of _properties_file_ for the _env_.\n\n    Args:\n        properties_file (str): File name of `create-configs` JSON output.\n        env (str): Environment to read optionally.\n        region (str): Region to get specific configs for.\n\n    Returns:\n        dict: JSON loaded Application properties for _env_.\n        None: Given _env_ was not found in `create-configs` JSON output.\n\n    \"\"\"\n    with open(properties_file, 'rt') as file_handle:\n        properties = json.load(file_handle)\n\n    env_properties = properties.get(env, properties)\n    contents = env_properties.get(region, env_properties)\n    LOG.debug('Found properties for %s:\\n%s', env, contents)\n    return contents"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    logging.basicConfig(format=LOGGING_FORMAT)\n\n    parser = argparse.ArgumentParser(description=main.__doc__)\n    add_debug(parser)\n    add_app(parser)\n    add_env(parser)\n    add_region(parser)\n    args = parser.parse_args()\n\n    logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n\n    assert destroy_elb(**vars(args))", "response": "Destroy any ELB related Resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a security group ID.", "response": "def get_security_group_id(name='', env='', region=''):\n    \"\"\"Get a security group ID.\n\n    Args:\n        name (str): Security Group name to find.\n        env (str): Deployment environment to search.\n        region (str): AWS Region to search.\n\n    Returns:\n        str: ID of Security Group, e.g. sg-xxxx.\n\n    Raises:\n        AssertionError: Call to Gate API was not successful.\n        SpinnakerSecurityGroupError: Security Group _name_ was not found for\n            _env_ in _region_.\n\n    \"\"\"\n    vpc_id = get_vpc_id(env, region)\n\n    LOG.info('Find %s sg in %s [%s] in %s', name, env, region, vpc_id)\n\n    url = '{0}/securityGroups/{1}/{2}/{3}?vpcId={4}'.format(API_URL, env, region, name, vpc_id)\n    response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n    assert response.ok\n\n    result = response.json()\n    try:\n        security_group_id = result['id']\n    except KeyError:\n        msg = 'Security group ({0}) not found'.format(name)\n        raise SpinnakerSecurityGroupError(msg)\n\n    LOG.info('Found: %s', security_group_id)\n    return security_group_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves duplicate Security Groups that share a same name alias.", "response": "def remove_duplicate_sg(security_groups):\n    \"\"\"Removes duplicate Security Groups that share a same name alias\n\n    Args:\n        security_groups (list): A list of security group id to compare against SECURITYGROUP_REPLACEMENTS\n\n    Returns:\n        security_groups (list): A list of security groups with duplicate aliases removed\n    \"\"\"\n    for each_sg, duplicate_sg_name in SECURITYGROUP_REPLACEMENTS.items():\n        if each_sg in security_groups and duplicate_sg_name in security_groups:\n            LOG.info('Duplicate SG found. Removing %s in favor of %s.', duplicate_sg_name, each_sg)\n            security_groups.remove(duplicate_sg_name)\n\n    return security_groups"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare the infrastructure for a specific environment.", "response": "def prepare_infrastructure():\n    \"\"\"Entry point for preparing the infrastructure in a specific env.\"\"\"\n    runner = ForemastRunner()\n\n    runner.write_configs()\n    runner.create_app()\n\n    archaius = runner.configs[runner.env]['app']['archaius_enabled']\n    eureka = runner.configs[runner.env]['app']['eureka_enabled']\n    deploy_type = runner.configs['pipeline']['type']\n\n    if deploy_type not in ['s3', 'datapipeline']:\n        runner.create_iam()\n        # TODO: Refactor Archaius to be fully featured\n        if archaius:\n            runner.create_archaius()\n        runner.create_secgroups()\n\n    if eureka:\n        LOG.info(\"Eureka Enabled, skipping ELB and DNS setup\")\n    elif deploy_type == \"lambda\":\n        LOG.info(\"Lambda Enabled, skipping ELB and DNS setup\")\n        runner.create_awslambda()\n    elif deploy_type == \"s3\":\n        runner.create_s3app()\n    elif deploy_type == 'datapipeline':\n        runner.create_datapipeline()\n    else:\n        LOG.info(\"No Eureka, running ELB and DNS setup\")\n        runner.create_elb()\n        runner.create_dns()\n\n    runner.slack_notify()\n    runner.cleanup()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rebuild_pipelines(*args):\n    rebuild_all = False\n    rebuild_project = os.getenv(\"REBUILD_PROJECT\")\n\n    if args:\n        LOG.debug('Incoming arguments: %s', args)\n        command_args, *_ = args\n        rebuild_all = command_args.parsed.all\n        rebuild_project = command_args.parsed.project\n\n    if rebuild_project == 'ALL':\n        rebuild_all = True\n\n    if rebuild_all:\n        LOG.info('Rebuilding all projects.')\n    elif rebuild_project is None:\n        msg = 'No REBUILD_PROJECT variable found'\n        LOG.fatal(msg)\n        raise SystemExit('Error: {0}'.format(msg))\n    else:\n        LOG.info('Rebuilding project: %s', rebuild_project)\n\n    all_apps = utils.get_all_apps()\n\n    for apps in all_apps:\n        if 'repoProjectKey' not in apps:\n            LOG.info('Skipping %s. No project key found', apps['name'])\n            continue\n\n        app_name = '{}/{}'.format(apps['repoProjectKey'], apps['repoSlug'])\n        if apps['repoProjectKey'].lower() == rebuild_project.lower() or rebuild_all:\n            os.environ[\"PROJECT\"] = apps['repoProjectKey']\n            os.environ[\"GIT_REPO\"] = apps['repoSlug']\n            LOG.info('Rebuilding pipelines for %s', app_name)\n            runner = ForemastRunner()\n            try:\n                runner.write_configs()\n                runner.create_pipeline()\n                runner.cleanup()\n            except Exception:  # pylint: disable=broad-except\n                LOG.warning('Error updating pipeline for %s', app_name)", "response": "Entry point for rebuilding pipelines."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef debug_flag():\n    logging.basicConfig(format=consts.LOGGING_FORMAT)\n\n    parser = argparse.ArgumentParser(description=debug_flag.__doc__)\n    add_debug(parser)\n    args, _extra_args = parser.parse_known_args()\n\n    package, *_ = __package__.split('.')\n    logging.getLogger(package).setLevel(args.debug)", "response": "Set logging level for entry points."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the configurations needed for pipes.", "response": "def write_configs(self):\n        \"\"\"Generate the configurations needed for pipes.\"\"\"\n        utils.banner(\"Generating Configs\")\n        if not self.runway_dir:\n            app_configs = configs.process_git_configs(git_short=self.git_short)\n        else:\n            app_configs = configs.process_runway_configs(runway_dir=self.runway_dir)\n\n        self.configs = configs.write_variables(\n            app_configs=app_configs, out_file=self.raw_path, git_short=self.git_short)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_app(self):\n        utils.banner(\"Creating Spinnaker App\")\n        spinnakerapp = app.SpinnakerApp(app=self.app, email=self.email, project=self.group, repo=self.repo,\n                                        pipeline_config=self.configs['pipeline'])\n        spinnakerapp.create_app()", "response": "Create the spinnaker application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_pipeline(self, onetime=None):\n        utils.banner(\"Creating Pipeline\")\n\n        kwargs = {\n            'app': self.app,\n            'trigger_job': self.trigger_job,\n            'prop_path': self.json_path,\n            'base': None,\n            'runway_dir': self.runway_dir,\n        }\n\n        pipeline_type = self.configs['pipeline']['type']\n\n        if pipeline_type not in consts.ALLOWED_TYPES:\n            raise NotImplementedError('Pipeline type \"{0}\" not permitted.'.format(pipeline_type))\n\n        if not onetime:\n            if pipeline_type == 'lambda':\n                spinnakerpipeline = pipeline.SpinnakerPipelineLambda(**kwargs)\n            elif pipeline_type == 's3':\n                spinnakerpipeline = pipeline.SpinnakerPipelineS3(**kwargs)\n            elif pipeline_type == 'datapipeline':\n                spinnakerpipeline = pipeline.SpinnakerPipelineDataPipeline(**kwargs)\n            elif pipeline_type == 'manual':\n                spinnakerpipeline = pipeline.SpinnakerPipelineManual(**kwargs)\n            else:\n                # Handles all other pipelines\n                spinnakerpipeline = pipeline.SpinnakerPipeline(**kwargs)\n        else:\n            spinnakerpipeline = pipeline.SpinnakerPipelineOnetime(onetime=onetime, **kwargs)\n\n        spinnakerpipeline.create_pipeline()", "response": "Create the spinnaker pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_archaius(self):\n        utils.banner(\"Creating S3\")\n        s3.init_properties(env=self.env, app=self.app)", "response": "Create Archaius bucket for Archaius."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating S3 infra for s3 applications", "response": "def create_s3app(self):\n        \"\"\"Create S3 infra for s3 applications\"\"\"\n        utils.banner(\"Creating S3 App Infrastructure\")\n        primary_region = self.configs['pipeline']['primary_region']\n        s3obj = s3.S3Apps(app=self.app,\n                          env=self.env,\n                          region=self.region,\n                          prop_path=self.json_path,\n                          primary_region=primary_region)\n        s3obj.create_bucket()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeploys artifacts contents to S3 bucket", "response": "def deploy_s3app(self):\n        \"\"\"Deploys artifacts contents to S3 bucket\"\"\"\n        utils.banner(\"Deploying S3 App\")\n        primary_region = self.configs['pipeline']['primary_region']\n        s3obj = s3.S3Deployment(\n            app=self.app,\n            env=self.env,\n            region=self.region,\n            prop_path=self.json_path,\n            artifact_path=self.artifact_path,\n            artifact_version=self.artifact_version,\n            primary_region=primary_region)\n        s3obj.upload_artifacts()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npromote S3 deployment to LATEST", "response": "def promote_s3app(self):\n        \"\"\"promotes S3 deployment to LATEST\"\"\"\n        utils.banner(\"Promoting S3 App\")\n        primary_region = self.configs['pipeline']['primary_region']\n        s3obj = s3.S3Deployment(\n            app=self.app,\n            env=self.env,\n            region=self.region,\n            prop_path=self.json_path,\n            artifact_path=self.artifact_path,\n            artifact_version=self.artifact_version,\n            primary_region=primary_region)\n        s3obj.promote_artifacts(promote_stage=self.promote_stage)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_secgroups(self):\n        utils.banner(\"Creating Security Group\")\n        sgobj = securitygroup.SpinnakerSecurityGroup(\n            app=self.app, env=self.env, region=self.region, prop_path=self.json_path)\n        sgobj.create_security_group()", "response": "Create security groups as defined in the configs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate security groups as defined in the configs.", "response": "def create_awslambda(self):\n        \"\"\"Create security groups as defined in the configs.\"\"\"\n        utils.banner(\"Creating Lambda Function\")\n        awslambdaobj = awslambda.LambdaFunction(\n            app=self.app, env=self.env, region=self.region, prop_path=self.json_path)\n        awslambdaobj.create_lambda_function()\n\n        utils.banner(\"Creating Lambda Event\")\n        lambdaeventobj = awslambda.LambdaEvent(app=self.app, env=self.env, region=self.region, prop_path=self.json_path)\n        lambdaeventobj.create_lambda_events()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the ELB for the defined environment.", "response": "def create_elb(self):\n        \"\"\"Create the ELB for the defined environment.\"\"\"\n        utils.banner(\"Creating ELB\")\n        elbobj = elb.SpinnakerELB(app=self.app, env=self.env, region=self.region, prop_path=self.json_path)\n        elbobj.create_elb()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates DNS for the defined app and environment.", "response": "def create_dns(self):\n        \"\"\"Create DNS for the defined app and environment.\"\"\"\n        utils.banner(\"Creating DNS\")\n        elb_subnet = self.configs[self.env]['elb']['subnet_purpose']\n        regions = self.configs[self.env]['regions']\n        failover = self.configs[self.env]['dns']['failover_dns']\n        primary_region = self.configs['pipeline']['primary_region']\n        regionspecific_dns = self.configs[self.env]['dns']['region_specific']\n\n        dnsobj = dns.SpinnakerDns(\n            app=self.app, env=self.env, region=self.region, prop_path=self.json_path, elb_subnet=elb_subnet)\n\n        if len(regions) > 1 and failover:\n            dnsobj.create_elb_dns(regionspecific=True)\n            dnsobj.create_failover_dns(primary_region=primary_region)\n        else:\n            if regionspecific_dns:\n                dnsobj.create_elb_dns(regionspecific=True)\n\n            if self.region == primary_region:\n                dnsobj.create_elb_dns(regionspecific=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_autoscaling_policy(self):\n        utils.banner(\"Creating Scaling Policy\")\n        policyobj = autoscaling_policy.AutoScalingPolicy(\n            app=self.app, env=self.env, region=self.region, prop_path=self.json_path)\n        policyobj.create_policy()", "response": "Create Auto Scaling Policy for app in environment"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates data pipeline and adds definition", "response": "def create_datapipeline(self):\n        \"\"\"Creates data pipeline and adds definition\"\"\"\n        utils.banner(\"Creating Data Pipeline\")\n        dpobj = datapipeline.AWSDataPipeline(app=self.app, env=self.env, region=self.region, prop_path=self.json_path)\n        dpobj.create_datapipeline()\n        dpobj.set_pipeline_definition()\n        if self.configs[self.env].get('datapipeline').get('activate_on_deploy'):\n            dpobj.activate_pipeline()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef slack_notify(self):\n        utils.banner(\"Sending slack notification\")\n\n        if self.env.startswith(\"prod\"):\n            notify = slacknotify.SlackNotification(app=self.app, env=self.env, prop_path=self.json_path)\n            notify.post_message()\n        else:\n            LOG.info(\"No slack message sent, not production environment\")", "response": "Send out a slack notification."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_debug(parser):\n    parser.add_argument(\n        '-d', '--debug', action='store_const', const=logging.DEBUG, default=logging.INFO, help='Set DEBUG output')", "response": "Add a debug flag to the _parser_."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_env(parser):\n    parser.add_argument(\n        '-e', '--env', choices=ENVS, default=os.getenv('ENV', default='dev'), help='Deploy environment, overrides $ENV')", "response": "Add an env flag to the _parser_."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    logging.basicConfig(format=LOGGING_FORMAT)\n    log = logging.getLogger(__name__)\n\n    parser = argparse.ArgumentParser(description=main.__doc__)\n    add_debug(parser)\n    add_app(parser)\n    add_env(parser)\n    add_properties(parser)\n    add_region(parser)\n    args = parser.parse_args()\n\n    logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n\n    log.debug('Parsed arguments: %s', args)\n\n    lambda_function = LambdaFunction(app=args.app, env=args.env, region=args.region, prop_path=args.properties)\n\n    lambda_function.create_lambda_function()\n\n    lambda_event = LambdaEvent(app=args.app, env=args.env, region=args.region, prop_path=args.properties)\n    lambda_event.create_lambda_events()", "response": "Create Lambda functions and Lambda events."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npost a message to the appropriate slack channel.", "response": "def post_slack_message(message=None, channel=None, username=None, icon_emoji=None):\n    \"\"\"Format the message and post to the appropriate slack channel.\n\n    Args:\n        message (str): Message to post to slack\n        channel (str): Desired channel. Must start with #\n\n    \"\"\"\n    LOG.debug('Slack Channel: %s\\nSlack Message: %s', channel, message)\n    slack = slacker.Slacker(SLACK_TOKEN)\n    try:\n        slack.chat.post_message(channel=channel, text=message, username=username, icon_emoji=icon_emoji)\n        LOG.info('Message posted to %s', channel)\n    except slacker.Error:\n        LOG.info(\"error posted message to %s\", channel)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting dict to list of dict.", "response": "def generated_tag_data(tags):\n    \"\"\"Convert :obj:`dict` to S3 Tag list.\n\n    Args:\n        tags (dict): Dictonary of tag key and tag value passed.\n\n    Returns:\n        list: List of dictionaries.\n\n    \"\"\"\n    generated_tags = []\n    for key, value in tags.items():\n        generated_tags.append({\n            'Key': key,\n            'Value': value,\n        })\n    return generated_tags"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndestroying DNS records. Args: app (str): Spinnaker Application name. env (str): Deployment environment. regions (str): AWS region. Returns: bool: True upon successful completion.", "response": "def destroy_dns(app='', env='dev', **_):\n    \"\"\"Destroy DNS records.\n\n    Args:\n        app (str): Spinnaker Application name.\n        env (str): Deployment environment.\n        regions (str): AWS region.\n\n    Returns:\n        bool: True upon successful completion.\n    \"\"\"\n    client = boto3.Session(profile_name=env).client('route53')\n\n    generated = get_details(app=app, env=env)\n    record = generated.dns_elb()\n\n    zone_ids = get_dns_zone_ids(env=env, facing='external')\n\n    for zone_id in zone_ids:\n        record_sets = client.list_resource_record_sets(\n            HostedZoneId=zone_id, StartRecordName=record, StartRecordType='CNAME', MaxItems='1')\n\n        for found_record in record_sets['ResourceRecordSets']:\n            assert destroy_record(client=client, found_record=found_record, record=record, zone_id=zone_id)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef destroy_record(client=None, found_record=None, record='', zone_id=''):\n    LOG.debug('Found DNS record: %s', found_record)\n\n    if found_record['Name'].strip('.') == record:\n        dns_json = get_template(template_file='destroy/destroy_dns.json.j2', record=json.dumps(found_record))\n        dns_dict = json.loads(dns_json)\n\n        client.change_resource_record_sets(HostedZoneId=zone_id, ChangeBatch=dns_dict)\n        LOG.info('Destroyed \"%s\" in %s', found_record['Name'], zone_id)\n    else:\n        LOG.info('DNS record \"%s\" missing from %s.', record, zone_id)\n        LOG.debug('Found someone else\\'s record: %s', found_record['Name'])\n\n    return True", "response": "Destroy an application DNS record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating SNS lambda event from rules.", "response": "def create_sns_event(app_name, env, region, rules):\n    \"\"\"Create SNS lambda event from rules.\n\n    Args:\n        app_name (str): name of the lambda function\n        env (str): Environment/Account for lambda function\n        region (str): AWS region of the lambda function\n        rules (str): Trigger rules from the settings\n    \"\"\"\n    session = boto3.Session(profile_name=env, region_name=region)\n    sns_client = session.client('sns')\n\n    topic_name = rules.get('topic')\n    lambda_alias_arn = get_lambda_alias_arn(app=app_name, account=env, region=region)\n    topic_arn = get_sns_topic_arn(topic_name=topic_name, account=env, region=region)\n    protocol = 'lambda'\n\n    statement_id = '{}_sns_{}'.format(app_name, topic_name)\n    principal = 'sns.amazonaws.com'\n    add_lambda_permissions(\n        function=lambda_alias_arn,\n        statement_id=statement_id,\n        action='lambda:InvokeFunction',\n        principal=principal,\n        source_arn=topic_arn,\n        env=env,\n        region=region)\n\n    sns_client.subscribe(TopicArn=topic_arn, Protocol=protocol, Endpoint=lambda_alias_arn)\n    LOG.debug(\"SNS Lambda event created\")\n\n    LOG.info(\"Created SNS event subscription on topic %s\", topic_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef destroy_sns_event(app_name, env, region):\n    session = boto3.Session(profile_name=env, region_name=region)\n    sns_client = session.client('sns')\n\n    lambda_subscriptions = get_sns_subscriptions(app_name=app_name, env=env, region=region)\n\n    for subscription_arn in lambda_subscriptions:\n        sns_client.unsubscribe(SubscriptionArn=subscription_arn)\n\n    LOG.debug(\"Lambda SNS event deleted\")\n    return True", "response": "Destroy all Lambda SNS subscriptions for a given lambda function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndestroying an ELB resource.", "response": "def destroy_elb(app='', env='dev', region='us-east-1', **_):\n    \"\"\"Destroy ELB Resources.\n\n    Args:\n        app (str): Spinnaker Application name.\n        env (str): Deployment environment.\n        region (str): AWS region.\n\n    Returns:\n        True upon successful completion.\n    \"\"\"\n    task_json = get_template(\n        template_file='destroy/destroy_elb.json.j2',\n        app=app,\n        env=env,\n        region=region,\n        vpc=get_vpc_id(account=env, region=region))\n\n    wait_for_task(task_json)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_pipeline(app='', pipeline_name=''):\n    safe_pipeline_name = normalize_pipeline_name(name=pipeline_name)\n\n    LOG.warning('Deleting Pipeline: %s', safe_pipeline_name)\n\n    url = '{host}/pipelines/{app}/{pipeline}'.format(host=API_URL, app=app, pipeline=safe_pipeline_name)\n    response = requests.delete(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    if not response.ok:\n        LOG.debug('Delete response code: %d', response.status_code)\n        if response.status_code == requests.status_codes.codes['method_not_allowed']:\n            raise SpinnakerPipelineDeletionFailed('Failed to delete \"{0}\" from \"{1}\", '\n                                                  'possibly invalid Pipeline name.'.format(safe_pipeline_name, app))\n        else:\n            LOG.debug('Pipeline missing, no delete required.')\n\n    LOG.debug('Deleted \"%s\" Pipeline response:\\n%s', safe_pipeline_name, response.text)\n\n    return response.text", "response": "Delete _pipeline_name_ from _app_."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting Pipelines for regions not defined in application. json files.", "response": "def clean_pipelines(app='', settings=None):\n    \"\"\"Delete Pipelines for regions not defined in application.json files.\n\n    For Pipelines named **app_name [region]**, _region_ will need to appear\n    in at least one application.json file. All other names are assumed\n    unamanaged.\n\n    Args:\n        app (str): Application name\n        settings (dict): imported configuration settings\n\n    Returns:\n        True: Upon successful completion.\n\n    Raises:\n        SpinnakerPipelineCreationFailed: Missing application.json file from\n        `create-configs`.\n    \"\"\"\n    pipelines = get_all_pipelines(app=app)\n    envs = settings['pipeline']['env']\n\n    LOG.debug('Find Regions in: %s', envs)\n\n    regions = set()\n    for env in envs:\n        try:\n            regions.update(settings[env]['regions'])\n        except KeyError:\n            error_msg = 'Missing \"{}/application-master-{}.json\".'.format(RUNWAY_BASE_PATH, env)\n            raise SpinnakerPipelineCreationFailed(error_msg)\n    LOG.debug('Regions defined: %s', regions)\n\n    for pipeline in pipelines:\n        pipeline_name = pipeline['name']\n\n        try:\n            region = check_managed_pipeline(name=pipeline_name, app_name=app)\n        except ValueError:\n            LOG.info('\"%s\" is not managed.', pipeline_name)\n            continue\n\n        LOG.debug('Check \"%s\" in defined Regions.', region)\n\n        if region not in regions:\n            delete_pipeline(app=app, pipeline_name=pipeline_name)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_key_values(config_handle, section, key, default=None):\n    if section not in config_handle:\n        LOG.info('Section missing from configurations: [%s]', section)\n\n    try:\n        value = config_handle[section][key]\n    except KeyError:\n        LOG.warning('[%s] missing key \"%s\", using %r.', section, key, default)\n        value = default\n\n    return value", "response": "Validate key values in configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts formats from the application.", "response": "def extract_formats(config_handle):\n    \"\"\"Get application formats.\n\n    See :class:`gogoutils.Formats` for available options.\n\n    Args:\n        config_handle (configparser.ConfigParser): Instance of configurations.\n\n    Returns:\n        dict: Formats in ``{$format_type: $format_pattern}``.\n\n    \"\"\"\n    configurations = dict(config_handle)\n    formats = dict(configurations.get('formats', {}))\n    return formats"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_dynamic_config(config_file=DEFAULT_DYNAMIC_CONFIG_FILE):\n    dynamic_configurations = {}\n\n    # Insert config path so we can import it\n    sys.path.insert(0, path.dirname(path.abspath(config_file)))\n    try:\n        config_module = __import__('config')\n\n        dynamic_configurations = config_module.CONFIG\n    except ImportError:\n        # Provide a default if config not found\n        LOG.error('ImportError: Unable to load dynamic config. Check config.py file imports!')\n\n    return dynamic_configurations", "response": "Load and parse dynamic config file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlooking for a configuration file in config_locations or. py.", "response": "def find_config():\n    \"\"\"Look for **foremast.cfg** in config_locations or ``./config.py``.\n\n    Raises:\n        SystemExit: No configuration file found.\n\n    Returns:\n        dict: Found dynamic or static configuration.\n\n    \"\"\"\n    config_locations = [\n        '/etc/foremast/foremast.cfg',\n        expanduser('~/.foremast/foremast.cfg'),\n        './.foremast/foremast.cfg',\n    ]\n    configurations = ConfigParser()\n\n    cfg_file = configurations.read(config_locations)\n    dynamic_config_file = getenv('FOREMAST_CONFIG_FILE', DEFAULT_DYNAMIC_CONFIG_FILE)\n\n    if cfg_file:\n        LOG.info('Loading static configuration file.')\n    elif exists(dynamic_config_file):\n        LOG.info('Loading dynamic configuration file.')\n        configurations = load_dynamic_config(config_file=dynamic_config_file)\n    else:\n        config_locations.append(dynamic_config_file)\n        LOG.warning('No configuration found in the following locations:\\n%s', '\\n'.join(config_locations))\n        LOG.warning('Using defaults...')\n\n    return dict(configurations)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _remove_empty_entries(entries):\n    valid_entries = []\n    for entry in set(entries):\n        if entry:\n            valid_entries.append(entry)\n    return sorted(valid_entries)", "response": "Remove empty entries in a list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a string to its native python type", "response": "def _convert_string_to_native(value):\n    \"\"\"Convert a string to its native python type\"\"\"\n    result = None\n\n    try:\n        result = ast.literal_eval(str(value))\n    except (SyntaxError, ValueError):\n        # Likely a string\n        result = value.split(',')\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads config file and generate security group dict by environment.", "response": "def _generate_security_groups(config_key):\n    \"\"\"Read config file and generate security group dict by environment.\n\n    Args:\n        config_key (str): Configuration file key\n\n    Returns:\n        dict: of environments in {'env1': ['group1', 'group2']} format\n    \"\"\"\n    raw_default_groups = validate_key_values(CONFIG, 'base', config_key, default='')\n    default_groups = _convert_string_to_native(raw_default_groups)\n    LOG.debug('Default security group for %s is %s', config_key, default_groups)\n\n    entries = {}\n    for env in ENVS:\n        entries[env] = []\n\n    if isinstance(default_groups, (list)):\n        groups = _remove_empty_entries(default_groups)\n        for env in entries:\n            entries[env] = groups\n    elif isinstance(default_groups, (dict)):\n        entries.update(default_groups)\n\n    LOG.debug('Generated security group: %s', entries)\n    return entries"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the data pipeline if it does not already exist", "response": "def create_datapipeline(self):\n        \"\"\"Creates the data pipeline if it does not already exist\n\n        Returns:\n                dict: the response of the Boto3 command\n        \"\"\"\n\n        tags = [{\"key\": \"app_group\", \"value\": self.group}, {\"key\": \"app_name\", \"value\": self.app_name}]\n        response = self.client.create_pipeline(\n            name=self.datapipeline_data.get('name', self.app_name),\n            uniqueId=self.app_name,\n            description=self.datapipeline_data['description'],\n            tags=tags)\n        self.pipeline_id = response.get('pipelineId')\n\n        LOG.debug(response)\n        LOG.info(\"Successfully configured Data Pipeline - %s\", self.app_name)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_pipeline_definition(self):\n\n        if not self.pipeline_id:\n            self.get_pipeline_id()\n\n        json_def = self.datapipeline_data['json_definition']\n        try:\n            pipelineobjects = translator.definition_to_api_objects(json_def)\n            parameterobjects = translator.definition_to_api_parameters(json_def)\n            parametervalues = translator.definition_to_parameter_values(json_def)\n        except translator.PipelineDefinitionError as error:\n            LOG.warning(error)\n            raise DataPipelineDefinitionError\n\n        response = self.client.put_pipeline_definition(\n            pipelineId=self.pipeline_id,\n            pipelineObjects=pipelineobjects,\n            parameterObjects=parameterobjects,\n            parameterValues=parametervalues)\n        LOG.debug(response)\n        LOG.info(\"Successfully applied pipeline definition\")\n        return response", "response": "Translates the json definition and puts it on the pipeline"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pipeline_id(self):\n\n        all_pipelines = []\n        paginiator = self.client.get_paginator('list_pipelines')\n        for page in paginiator.paginate():\n            all_pipelines.extend(page['pipelineIdList'])\n\n        for pipeline in all_pipelines:\n            if pipeline['name'] == self.datapipeline_data.get('name', self.app_name):\n                self.pipeline_id = pipeline['id']\n                LOG.info(\"Pipeline ID Found\")\n                return\n        LOG.info(\"Pipeline ID Not Found for %s\", self.app_name)", "response": "Finds the pipeline ID for the configured pipeline"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef activate_pipeline(self):\n        self.client.activate_pipeline(pipelineId=self.pipeline_id)\n        LOG.info(\"Activated Pipeline %s\", self.pipeline_id)", "response": "Activates a deployed pipeline useful for OnDemand pipelines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dns_zone_ids(env='dev', facing='internal'):\n    client = boto3.Session(profile_name=env).client('route53')\n\n    zones = client.list_hosted_zones_by_name(DNSName='.'.join([env, DOMAIN]))\n\n    zone_ids = []\n    for zone in zones['HostedZones']:\n        LOG.debug('Found Hosted Zone: %s', zone)\n\n        if facing == 'external' or zone['Config']['PrivateZone']:\n            LOG.info('Using %(Id)s for \"%(Name)s\", %(Config)s', zone)\n            zone_ids.append(zone['Id'])\n\n    LOG.debug('Zone IDs: %s', zone_ids)\n    return zone_ids", "response": "Get Route 53 Hosted Zone IDs for the environment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Route53 CNAME record in the specified environment and zone.", "response": "def update_dns_zone_record(env, zone_id, **kwargs):\n    \"\"\"Create a Route53 CNAME record in _env_ zone.\n\n    Args:\n        env (str): Deployment environment.\n        zone_id (str): Route53 zone id.\n\n    Keyword Args:\n        dns_name (str): FQDN of application's dns entry to add/update.\n        dns_name_aws (str): FQDN of AWS resource\n        dns_ttl (int): DNS time-to-live (ttl)\n    \"\"\"\n    client = boto3.Session(profile_name=env).client('route53')\n    response = {}\n\n    hosted_zone_info = client.get_hosted_zone(Id=zone_id)\n    zone_name = hosted_zone_info['HostedZone']['Name'].rstrip('.')\n    dns_name = kwargs.get('dns_name')\n\n    if dns_name and dns_name.endswith(zone_name):\n        dns_name_aws = kwargs.get('dns_name_aws')\n        # This is what will be added to DNS\n        dns_json = get_template(template_file='infrastructure/dns_upsert.json.j2', **kwargs)\n        LOG.info('Attempting to create DNS record %s (%s) in Hosted Zone %s (%s)', dns_name, dns_name_aws, zone_id,\n                 zone_name)\n        try:\n            response = client.change_resource_record_sets(\n                HostedZoneId=zone_id,\n                ChangeBatch=json.loads(dns_json), )\n            LOG.info('Upserted DNS record %s (%s) in Hosted Zone %s (%s)', dns_name, dns_name_aws, zone_id, zone_name)\n        except botocore.exceptions.ClientError as error:\n            LOG.info('Error creating DNS record %s (%s) in Hosted Zone %s (%s)', dns_name, dns_name_aws, zone_id,\n                     zone_name)\n            LOG.debug(error)\n    else:\n        LOG.info('Skipping creating DNS record %s in non-matching Hosted Zone %s (%s)', dns_name, zone_id, zone_name)\n\n    LOG.debug('Route53 JSON Response: \\n%s', pformat(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds an existing DNS record in a specific environment.", "response": "def find_existing_record(env, zone_id, dns_name, check_key=None, check_value=None):\n    \"\"\"Check if a specific DNS record exists.\n\n    Args:\n        env (str): Deployment environment.\n        zone_id (str): Route53 zone id.\n        dns_name (str): FQDN of application's dns entry to add/update.\n        check_key(str): Key to look for in record. Example: \"Type\"\n        check_value(str): Value to look for with check_key. Example: \"CNAME\"\n\n    Returns:\n        json: Found Record. Returns None if no record found\n\n    \"\"\"\n    client = boto3.Session(profile_name=env).client('route53')\n    pager = client.get_paginator('list_resource_record_sets')\n    existingrecord = None\n    for rset in pager.paginate(HostedZoneId=zone_id):\n        for record in rset['ResourceRecordSets']:\n            if check_key:\n                if record['Name'].rstrip('.') == dns_name and record.get(check_key) == check_value:\n                    LOG.info(\"Found existing record: %s\", record)\n                    existingrecord = record\n                    break\n    return existingrecord"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_existing_cname(env, zone_id, dns_name):\n    client = boto3.Session(profile_name=env).client('route53')\n    startrecord = None\n    newrecord_name = dns_name\n    startrecord = find_existing_record(env, zone_id, newrecord_name, check_key='Type', check_value='CNAME')\n    if startrecord:\n        LOG.info(\"Deleting old record: %s\", newrecord_name)\n        _response = client.change_resource_record_sets(\n            HostedZoneId=zone_id, ChangeBatch={'Changes': [{\n                'Action': 'DELETE',\n                'ResourceRecordSet': startrecord\n            }]})\n        LOG.debug('Response from deleting %s: %s', dns_name, _response)", "response": "Delete an existing CNAME record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_failover_dns_record(env, zone_id, **kwargs):\n    client = boto3.Session(profile_name=env).client('route53')\n    response = {}\n\n    hosted_zone_info = client.get_hosted_zone(Id=zone_id)\n    zone_name = hosted_zone_info['HostedZone']['Name'].rstrip('.')\n    dns_name = kwargs.get('dns_name')\n\n    # Check that the primary record exists\n    failover_state = kwargs.get('failover_state')\n    if failover_state.lower() != 'primary':\n        primary_record = find_existing_record(env, zone_id, dns_name, check_key='Failover', check_value='PRIMARY')\n        if not primary_record:\n            raise PrimaryDNSRecordNotFound(\"Primary Failover DNS record not found: {}\".format(dns_name))\n\n    if dns_name and dns_name.endswith(zone_name):\n        dns_json = get_template(template_file='infrastructure/dns_failover_upsert.json.j2', **kwargs)\n        LOG.info('Attempting to create DNS Failover record %s (%s) in Hosted Zone %s (%s)', dns_name,\n                 kwargs['elb_aws_dns'], zone_id, zone_name)\n        try:\n            delete_existing_cname(env, zone_id, dns_name)\n            response = client.change_resource_record_sets(\n                HostedZoneId=zone_id,\n                ChangeBatch=json.loads(dns_json), )\n            LOG.info('Upserted DNS Failover record %s (%s) in Hosted Zone %s (%s)', dns_name, kwargs['elb_aws_dns'],\n                     zone_id, zone_name)\n        except botocore.exceptions.ClientError as error:\n            LOG.info('Error creating DNS Failover record %s (%s) in Hosted Zone %s (%s)', dns_name,\n                     kwargs['elb_aws_dns'], zone_id, zone_name)\n            LOG.debug(error)\n    else:\n        LOG.info('Skipping creating DNS record %s in non-matching Hosted Zone %s (%s)', dns_name, zone_id, zone_name)\n\n    LOG.debug('Route53 JSON Response: \\n%s', pformat(response))", "response": "Create a Failover Route53 alias record in the specified environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_cloudwatch_log_event(app_name, env, region, rules):\n\n    session = boto3.Session(profile_name=env, region_name=region)\n    cloudwatch_client = session.client('logs')\n\n    log_group = rules.get('log_group')\n    filter_name = rules.get('filter_name')\n    filter_pattern = rules.get('filter_pattern')\n\n    if not log_group:\n        LOG.critical('Log group is required and no \"log_group\" is defined!')\n        raise InvalidEventConfiguration('Log group is required and no \"log_group\" is defined!')\n\n    if not filter_name:\n        LOG.critical('Filter name is required and no filter_name is defined!')\n        raise InvalidEventConfiguration('Filter name is required and no filter_name is defined!')\n\n    if filter_pattern is None:\n        LOG.critical('Filter pattern is required and no filter_pattern is defined!')\n        raise InvalidEventConfiguration('Filter pattern is required and no filter_pattern is defined!')\n\n    lambda_alias_arn = get_lambda_alias_arn(app=app_name, account=env, region=region)\n\n    statement_id = '{}_cloudwatchlog_{}'.format(app_name, filter_name.replace(\" \", \"_\"))\n    principal = 'logs.{}.amazonaws.com'.format(region)\n    account_id = get_env_credential(env=env)['accountId']\n    source_arn = \"arn:aws:logs:{0}:{1}:log-group:{2}:*\".format(region, account_id, log_group)\n    add_lambda_permissions(\n        function=lambda_alias_arn,\n        statement_id=statement_id,\n        action='lambda:InvokeFunction',\n        principal=principal,\n        source_arn=source_arn,\n        env=env,\n        region=region)\n\n    cloudwatch_client.put_subscription_filter(\n        logGroupName=log_group, filterName=filter_name, filterPattern=filter_pattern, destinationArn=lambda_alias_arn)\n\n    LOG.info(\"Created Cloudwatch log event with filter: %s\", filter_pattern)", "response": "Create a CloudWatch log event from the rules."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_policy_template(self, scaling_type, period_sec, server_group):\n        template_kwargs = {\n            'app': self.app,\n            'env': self.env,\n            'region': self.region,\n            'server_group': server_group,\n            'period_sec': period_sec,\n            'scaling_policy': self.settings['asg']['scaling_policy'],\n        }\n        if scaling_type == 'scale_up':\n            template_kwargs['operation'] = 'increase'\n            template_kwargs['comparisonOperator'] = 'GreaterThanThreshold'\n            template_kwargs['scalingAdjustment'] = 1\n\n        elif scaling_type == 'scale_down':\n            cur_threshold = int(self.settings['asg']['scaling_policy']['threshold'])\n            self.settings['asg']['scaling_policy']['threshold'] = floor(cur_threshold * 0.5)\n            template_kwargs['operation'] = 'decrease'\n            template_kwargs['comparisonOperator'] = 'LessThanThreshold'\n            template_kwargs['scalingAdjustment'] = -1\n\n        rendered_template = get_template(template_file='infrastructure/autoscaling_policy.json.j2', **template_kwargs)\n        self.log.info('Creating a %s policy in %s for %s', scaling_type, self.env, self.app)\n        wait_for_task(rendered_template)\n        self.log.info('Successfully created a %s policy in %s for %s', scaling_type, self.env, self.app)", "response": "Renders the scaling policy template based on configs and variables and POSTs the json to Spinnaker for creation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping function. Creates a new policy for the current server group and returns the new version of the policy.", "response": "def create_policy(self):\n        \"\"\"Wrapper function. Gets the server group, sets sane defaults,\n        deletes existing policies, and then runs self.prepare_policy_template\n        for scaling up and scaling down policies.\n        This function acts as the main driver for the scaling policy creationprocess\n        \"\"\"\n        if not self.settings['asg']['scaling_policy']:\n            self.log.info(\"No scaling policy found, skipping...\")\n            return\n\n        server_group = self.get_server_group()\n\n        # Find all existing and remove them\n        scaling_policies = self.get_all_existing(server_group)\n        for policy in scaling_policies:\n            for subpolicy in policy:\n                self.delete_existing_policy(subpolicy, server_group)\n\n        if self.settings['asg']['scaling_policy']['period_minutes']:\n            period_sec = int(self.settings['asg']['scaling_policy']['period_minutes']) * 60\n        else:\n            period_sec = 1800\n\n        self.prepare_policy_template('scale_up', period_sec, server_group)\n        if self.settings['asg']['scaling_policy'].get('scale_down', True):\n            self.prepare_policy_template('scale_down', period_sec, server_group)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_server_group(self):\n        api_url = \"{0}/applications/{1}\".format(API_URL, self.app)\n        response = requests.get(api_url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n        for server_group in response.json()['clusters'][self.env]:\n            return server_group['serverGroups'][-1]", "response": "Finds the most recently deployed server group for the application."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_existing_policy(self, scaling_policy, server_group):\n        self.log.info(\"Deleting policy %s on %s\", scaling_policy['policyName'], server_group)\n        delete_dict = {\n            \"application\":\n            self.app,\n            \"description\":\n            \"Delete scaling policy\",\n            \"job\": [{\n                \"policyName\": scaling_policy['policyName'],\n                \"serverGroupName\": server_group,\n                \"credentials\": self.env,\n                \"region\": self.region,\n                \"provider\": \"aws\",\n                \"type\": \"deleteScalingPolicy\",\n                \"user\": \"foremast-autoscaling-policy\"\n            }]\n        }\n        wait_for_task(json.dumps(delete_dict))", "response": "Deletes the existing scaling policy on a server group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_existing(self, server_group):\n        self.log.info(\"Checking for existing scaling policy\")\n        url = \"{0}/applications/{1}/clusters/{2}/{1}/serverGroups\".format(API_URL, self.app, self.env)\n        response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n        assert response.ok, \"Error looking for existing Autoscaling Policy for {0}: {1}\".format(self.app, response.text)\n\n        scalingpolicies = []\n        for servergroup in response.json():\n            if servergroup['scalingPolicies'] and servergroup['asg']['autoScalingGroupName'] == server_group:\n                self.log.info(\"Found policies on %s\", server_group)\n                scalingpolicies.append(servergroup['scalingPolicies'])\n        self.log.debug(\"Scaling policies: %s\", scalingpolicies)\n        return scalingpolicies", "response": "Finds all existing scaling policies for an application"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef destroy_cloudwatch_event(app='', env='dev', region=''):\n\n    session = boto3.Session(profile_name=env, region_name=region)\n    cloudwatch_client = session.client('events')\n\n    event_rules = get_cloudwatch_event_rule(app_name=app, account=env, region=region)\n\n    for rule in event_rules:\n        cloudwatch_client.remove_targets(Rule=rule, Ids=[app])\n\n    return True", "response": "Destroy Cloudwatch event subscription."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate or update a bucket based on app name.", "response": "def create_bucket(self):\n        \"\"\"Create or update bucket based on app name.\"\"\"\n        bucket_exists = self._bucket_exists()\n        if self.s3props.get('shared_bucket_target'):\n            if bucket_exists:\n                LOG.info('App uses shared bucket - %s ', self.bucket)\n            else:\n                LOG.error(\"Shared bucket %s does not exist\", self.bucket)\n                raise S3SharedBucketNotFound\n        else:\n            if self.region == 'us-east-1':\n                _response = self.s3client.create_bucket(ACL=self.s3props['bucket_acl'], Bucket=self.bucket)\n            else:\n                if not bucket_exists:\n                    _response = self.s3client.create_bucket(ACL=self.s3props['bucket_acl'], Bucket=self.bucket,\n                                                            CreateBucketConfiguration={\n                                                                'LocationConstraint': self.region})\n                else:\n                    _response = \"bucket already exists, skipping create for non-standard region buckets.\"\n            LOG.debug('Response creating bucket: %s', _response)\n            LOG.info('%s - S3 Bucket Upserted', self.bucket)\n            self._put_bucket_policy()\n            self._put_bucket_website()\n            self._put_bucket_logging()\n            self._put_bucket_lifecycle()\n            self._put_bucket_versioning()\n            self._put_bucket_encryption()\n            self._put_bucket_tagging()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the bucket exists.", "response": "def _bucket_exists(self):\n        \"\"\"Check if the bucket exists.\"\"\"\n        try:\n            self.s3client.get_bucket_location(Bucket=self.bucket)\n            return True\n        except ClientError as error:\n            LOG.error(error)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _put_bucket_policy(self):\n        if self.s3props['bucket_policy']:\n            policy_str = json.dumps(self.s3props['bucket_policy'])\n            _response = self.s3client.put_bucket_policy(Bucket=self.bucket, Policy=policy_str)\n        else:\n            _response = self.s3client.delete_bucket_policy(Bucket=self.bucket)\n        LOG.debug('Response adding bucket policy: %s', _response)\n        LOG.info('S3 Bucket Policy Attached')", "response": "Attach a bucket policy to app bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring static website on S3 bucket.", "response": "def _put_bucket_website(self):\n        \"\"\"Configure static website on S3 bucket.\"\"\"\n        if self.s3props['website']['enabled']:\n            website_config = {\n                'ErrorDocument': {\n                    'Key': self.s3props['website']['error_document']\n                },\n                'IndexDocument': {\n                    'Suffix': self.s3props['website']['index_suffix']\n                }\n            }\n            _response = self.s3client.put_bucket_website(Bucket=self.bucket, WebsiteConfiguration=website_config)\n            self._put_bucket_cors()\n            self._set_bucket_dns()\n        else:\n            _response = self.s3client.delete_bucket_website(Bucket=self.bucket)\n            self._put_bucket_cors()\n        LOG.debug('Response setting up S3 website: %s', _response)\n        LOG.info('S3 website settings updated')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_bucket_dns(self):\n        # Different regions have different s3 endpoint formats\n        dotformat_regions = [\"eu-west-2\", \"eu-central-1\", \"ap-northeast-2\", \"ap-south-1\", \"ca-central-1\", \"us-east-2\"]\n        if self.region in dotformat_regions:\n            s3_endpoint = \"{0}.s3-website.{1}.amazonaws.com\".format(self.bucket, self.region)\n        else:\n            s3_endpoint = \"{0}.s3-website-{1}.amazonaws.com\".format(self.bucket, self.region)\n\n        zone_ids = get_dns_zone_ids(env=self.env, facing=\"public\")\n        dns_kwargs = {\n            'dns_name': self.bucket,\n            'dns_name_aws': s3_endpoint,\n            'dns_ttl': self.properties['dns']['ttl']\n        }\n\n        for zone_id in zone_ids:\n            LOG.debug('zone_id: %s', zone_id)\n            update_dns_zone_record(self.env, zone_id, **dns_kwargs)\n        LOG.info(\"Created DNS %s for Bucket\", self.bucket)", "response": "Create DNS name for S3 endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding bucket cors configuration.", "response": "def _put_bucket_cors(self):\n        \"\"\"Adds bucket cors configuration.\"\"\"\n        if self.s3props['cors']['enabled'] and self.s3props['website']['enabled']:\n            cors_config = {}\n            cors_rules = []\n            for each_rule in self.s3props['cors']['cors_rules']:\n                cors_rules.append({\n                    'AllowedHeaders': each_rule['cors_headers'],\n                    'AllowedMethods': each_rule['cors_methods'],\n                    'AllowedOrigins': each_rule['cors_origins'],\n                    'ExposeHeaders': each_rule['cors_expose_headers'],\n                    'MaxAgeSeconds': each_rule['cors_max_age']\n                })\n            cors_config = {\n                'CORSRules': cors_rules\n            }\n            LOG.debug(cors_config)\n            _response = self.s3client.put_bucket_cors(Bucket=self.bucket, CORSConfiguration=cors_config)\n        else:\n            _response = self.s3client.delete_bucket_cors(Bucket=self.bucket)\n        LOG.debug('Response setting up S3 CORS: %s', _response)\n        LOG.info('S3 CORS configuration updated')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd bucket encryption configuration.", "response": "def _put_bucket_encryption(self):\n        \"\"\"Adds bucket encryption configuration.\"\"\"\n        if self.s3props['encryption']['enabled']:\n            encryption_config = {'Rules': [{}]}\n            encryption_config = {\n                'Rules': self.s3props['encryption']['encryption_rules']\n            }\n            LOG.debug(encryption_config)\n            _response = self.s3client.put_bucket_encryption(Bucket=self.bucket,\n                                                            ServerSideEncryptionConfiguration=encryption_config)\n        else:\n            _response = self.s3client.delete_bucket_encryption(Bucket=self.bucket)\n        LOG.debug('Response setting up S3 encryption: %s', _response)\n        LOG.info('S3 encryption configuration updated')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd bucket lifecycle configuration.", "response": "def _put_bucket_lifecycle(self):\n        \"\"\"Adds bucket lifecycle configuration.\"\"\"\n        status = 'deleted'\n        if self.s3props['lifecycle']['enabled']:\n            lifecycle_config = {\n                'Rules': self.s3props['lifecycle']['lifecycle_rules']\n            }\n            LOG.debug('Lifecycle Config: %s', lifecycle_config)\n            _response = self.s3client.put_bucket_lifecycle_configuration(Bucket=self.bucket,\n                                                                         LifecycleConfiguration=lifecycle_config)\n            status = 'applied'\n        else:\n            _response = self.s3client.delete_bucket_lifecycle(Bucket=self.bucket)\n        LOG.debug('Response setting up S3 lifecycle: %s', _response)\n        LOG.info('S3 lifecycle configuration %s', status)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding bucket logging policy to s3 access requests", "response": "def _put_bucket_logging(self):\n        \"\"\"Adds bucket logging policy to bucket for s3 access requests\"\"\"\n        logging_config = {}\n        if self.s3props['logging']['enabled']:\n            logging_config = {\n                'LoggingEnabled': {\n                    'TargetBucket': self.s3props['logging']['logging_bucket'],\n                    'TargetGrants': self.s3props['logging']['logging_grants'],\n                    'TargetPrefix': self.s3props['logging']['logging_bucket_prefix']\n                }\n            }\n        _response = self.s3client.put_bucket_logging(Bucket=self.bucket, BucketLoggingStatus=logging_config)\n        LOG.debug('Response setting up S3 logging: %s', _response)\n        LOG.info('S3 logging configuration updated')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds bucket tags to bucket.", "response": "def _put_bucket_tagging(self):\n        \"\"\"Add bucket tags to bucket.\"\"\"\n        all_tags = self.s3props['tagging']['tags']\n        all_tags.update({'app_group': self.group, 'app_name': self.app_name})\n        tag_set = generate_s3_tags.generated_tag_data(all_tags)\n\n        tagging_config = {'TagSet': tag_set}\n\n        self.s3client.put_bucket_tagging(Bucket=self.bucket, Tagging=tagging_config)\n        LOG.info(\"Adding tagging %s for Bucket\", tag_set)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds bucket versioning policy to bucket", "response": "def _put_bucket_versioning(self):\n        \"\"\"Adds bucket versioning policy to bucket\"\"\"\n        status = 'Suspended'\n        if self.s3props['versioning']['enabled']:\n            status = 'Enabled'\n\n        versioning_config = {\n            'MFADelete': self.s3props['versioning']['mfa_delete'],\n            'Status': status\n        }\n\n        _response = self.s3client.put_bucket_versioning(Bucket=self.bucket, VersioningConfiguration=versioning_config)\n        LOG.debug('Response setting up S3 versioning: %s', _response)\n        LOG.info('S3 versioning configuration updated')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving _application. json_ files from GitLab and process them.", "response": "def process_git_configs(git_short=''):\n    \"\"\"Retrieve _application.json_ files from GitLab.\n\n    Args:\n        git_short (str): Short Git representation of repository, e.g.\n            forrest/core.\n\n    Returns:\n        collections.defaultdict: Configurations stored for each environment\n        found.\n    \"\"\"\n    LOG.info('Processing application.json files from GitLab \"%s\".', git_short)\n    file_lookup = FileLookup(git_short=git_short)\n    app_configs = process_configs(file_lookup,\n                                  RUNWAY_BASE_PATH + '/application-master-{env}.json',\n                                  RUNWAY_BASE_PATH + '/pipeline.json')\n    commit_obj = file_lookup.project.commits.get('master')\n    config_commit = commit_obj.attributes['id']\n    LOG.info('Commit ID used: %s', config_commit)\n    app_configs['pipeline']['config_commit'] = config_commit\n    return app_configs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the application. json files and process them.", "response": "def process_runway_configs(runway_dir=''):\n    \"\"\"Read the _application.json_ files.\n\n    Args:\n        runway_dir (str): Name of runway directory with app.json files.\n\n    Returns:\n        collections.defaultdict: Configurations stored for each environment\n        found.\n    \"\"\"\n    LOG.info('Processing application.json files from local directory \"%s\".', runway_dir)\n    file_lookup = FileLookup(runway_dir=runway_dir)\n    app_configs = process_configs(file_lookup, 'application-master-{env}.json', 'pipeline.json')\n    return app_configs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the configs from the given file_lookup and pipeline_config.", "response": "def process_configs(file_lookup, app_config_format, pipeline_config):\n    \"\"\"Processes the configs from lookup sources.\n\n    Args:\n        file_lookup (FileLookup): Source to look for file/config\n        app_config_format (str): The format for application config files.\n        pipeline_config (str): Name/path of the pipeline config\n\n    Returns:\n        dict: Retreived application config\n    \"\"\"\n    app_configs = collections.defaultdict(dict)\n    for env in ENVS:\n        file_json = app_config_format.format(env=env)\n        try:\n            env_config = file_lookup.json(filename=file_json)\n            app_configs[env] = apply_region_configs(env_config)\n        except FileNotFoundError:\n            LOG.critical('Application configuration not available for %s.', env)\n            continue\n\n    try:\n        app_configs['pipeline'] = file_lookup.json(filename=pipeline_config)\n    except FileNotFoundError:\n        LOG.warning('Unable to process pipeline.json. Using defaults.')\n        app_configs['pipeline'] = {'env': ['stage', 'prod']}\n\n    LOG.debug('Application configs:\\n%s', app_configs)\n    return app_configs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride default env configs with region specific configs and nest all values under a region", "response": "def apply_region_configs(env_config):\n    \"\"\"Override default env configs with region specific configs and nest\n    all values under a region\n\n    Args:\n        env_config (dict): The environment specific config.\n\n    Return:\n        dict: Newly updated dictionary with region overrides applied.\n    \"\"\"\n    new_config = env_config.copy()\n    for region in env_config.get('regions', REGIONS):\n        if isinstance(env_config.get('regions'), dict):\n            region_specific_config = env_config['regions'][region]\n            new_config[region] = dict(DeepChainMap(region_specific_config, env_config))\n        else:\n            new_config[region] = env_config.copy()\n    LOG.debug('Region Specific Config:\\n%s', new_config)\n    return new_config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the IAM Resources for the application.", "response": "def create_iam_resources(env='dev', app='', **_):\n    \"\"\"Create the IAM Resources for the application.\n\n    Args:\n        env (str): Deployment environment/account, i.e. dev, stage, prod.\n        app (str): Spinnaker Application name.\n\n    Returns:\n        True upon successful completion.\n    \"\"\"\n    session = boto3.session.Session(profile_name=env)\n    client = session.client('iam')\n\n    app_properties = get_properties(env='pipeline')\n\n    generated = get_details(env=env, app=app)\n    generated_iam = generated.iam()\n    app_details = collections.namedtuple('AppDetails', generated_iam.keys())\n    details = app_details(**generated_iam)\n\n    LOG.debug('Application details: %s', details)\n\n    deployment_type = app_properties['type']\n    role_trust_template = get_template(\n        'infrastructure/iam/trust/{0}_role.json.j2'.format(deployment_type), formats=generated)\n\n    resource_action(\n        client,\n        action='create_role',\n        log_format='Created Role: %(RoleName)s',\n        RoleName=details.role,\n        AssumeRolePolicyDocument=role_trust_template)\n    resource_action(\n        client,\n        action='create_instance_profile',\n        log_format='Created Instance Profile: %(InstanceProfileName)s',\n        InstanceProfileName=details.profile)\n    attach_profile_to_role(client, role_name=details.role, profile_name=details.profile)\n\n    iam_policy = construct_policy(app=app, group=details.group, env=env, pipeline_settings=app_properties)\n    if iam_policy:\n        resource_action(\n            client,\n            action='put_role_policy',\n            log_format='Added IAM Policy: %(PolicyName)s',\n            RoleName=details.role,\n            PolicyName=details.policy,\n            PolicyDocument=iam_policy)\n\n    resource_action(client, action='create_user', log_format='Created User: %(UserName)s', UserName=details.user)\n    resource_action(client, action='create_group', log_format='Created Group: %(GroupName)s', GroupName=details.group)\n    resource_action(\n        client,\n        action='add_user_to_group',\n        log_format='Added User to Group: %(UserName)s -> %(GroupName)s',\n        GroupName=details.group,\n        UserName=details.user)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattaches an IAM Instance Profile to an IAM Role.", "response": "def attach_profile_to_role(client, role_name='forrest_unicorn_role', profile_name='forrest_unicorn_profile'):\n    \"\"\"Attach an IAM Instance Profile _profile_name_ to Role _role_name_.\n\n    Args:\n        role_name (str): Name of Role.\n        profile_name (str): Name of Instance Profile.\n\n    Returns:\n        True upon successful completion.\n    \"\"\"\n    current_instance_profiles = resource_action(\n        client,\n        action='list_instance_profiles_for_role',\n        log_format='Found Instance Profiles for %(RoleName)s.',\n        RoleName=role_name)['InstanceProfiles']\n\n    for profile in current_instance_profiles:\n        if profile['InstanceProfileName'] == profile_name:\n            LOG.info('Found Instance Profile attached to Role: %s -> %s', profile_name, role_name)\n            break\n    else:\n        for remove_profile in current_instance_profiles:\n            resource_action(\n                client,\n                action='remove_role_from_instance_profile',\n                log_format='Removed Instance Profile from Role: '\n                '%(InstanceProfileName)s -> %(RoleName)s',\n                InstanceProfileName=remove_profile['InstanceProfileName'],\n                RoleName=role_name)\n\n        resource_action(\n            client,\n            action='add_role_to_instance_profile',\n            log_format='Added Instance Profile to Role: '\n            '%(InstanceProfileName)s -> %(RoleName)s',\n            InstanceProfileName=profile_name,\n            RoleName=role_name)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_elb(name='', env='', region=''):\n    LOG.info('Find %s ELB in %s [%s].', name, env, region)\n\n    url = '{0}/applications/{1}/loadBalancers'.format(API_URL, name)\n    response = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n    assert response.ok\n\n    elb_dns = None\n    accounts = response.json()\n    for account in accounts:\n        if account['account'] == env and account['region'] == region:\n            elb_dns = account['dnsname']\n            break\n    else:\n        raise SpinnakerElbNotFound('Elb for \"{0}\" in region {1} not found'.format(name, region))\n\n    LOG.info('Found: %s', elb_dns)\n    return elb_dns", "response": "Find an application s AWS elb dns name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_elb_dns_zone_id(name='', env='dev', region='us-east-1'):\n    LOG.info('Find %s ELB DNS Zone ID in %s [%s].', name, env, region)\n    client = boto3.Session(profile_name=env).client('elb', region_name=region)\n    elbs = client.describe_load_balancers(LoadBalancerNames=[name])\n    return elbs['LoadBalancerDescriptions'][0]['CanonicalHostedZoneNameID']", "response": "Find an application s AWS elb dns zone id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reload():\n    try:\n        # Reload and replace current process\n        os.execv(sys.executable, [sys.executable] + sys.argv)\n\n    except OSError:\n        # Ugh, that failed\n        # Try spawning a new process and exitj\n        os.spawnv(os.P_NOWAIT, sys.executable, [sys.executable] + sys.argv)\n        os._exit(os.EX_OK)", "response": "Reloads the current process and replaces it with the new one"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun coroutine with reloader", "response": "async def run_with_reloader(loop, coroutine, cleanup=None, *args, **kwargs):\n    \"\"\" Run coroutine with reloader \"\"\"\n\n    clear_screen()\n    print(\"\ud83e\udd16  Running in debug mode with live reloading\")\n    print(\"    (don't forget to disable it for production)\")\n\n    # Create watcher\n    handler = Handler(loop)\n    watcher = Observer()\n\n    # Setup\n    path = realpath(os.getcwd())\n    watcher.schedule(handler, path=path, recursive=True)\n    watcher.start()\n\n    print(\"    (watching {})\".format(path))\n\n    # Run watcher and coroutine together\n    done, pending = await asyncio.wait(\n        [coroutine, handler.changed], return_when=asyncio.FIRST_COMPLETED\n    )\n\n    # Cleanup\n    cleanup and cleanup()\n    watcher.stop()\n\n    for fut in done:\n        # If change event, then reload\n        if isinstance(fut.result(), Event):\n            print(\"Reloading...\")\n            reload()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning bot s main loop as coroutine. Use with asyncio.", "response": "async def loop(self):\n        \"\"\"\n        Return bot's main loop as coroutine. Use with asyncio.\n\n        :Example:\n\n        >>> loop = asyncio.get_event_loop()\n        >>> loop.run_until_complete(bot.loop())\n\n        or\n\n        >>> loop = asyncio.get_event_loop()\n        >>> loop.create_task(bot.loop())\n        \"\"\"\n        self._running = True\n        while self._running:\n            updates = await self.api_call(\n                \"getUpdates\", offset=self._offset + 1, timeout=self.api_timeout\n            )\n            self._process_updates(updates)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a new command :param str regexp: Regular expression matching the command to register :Example: >>> @bot.command(r\"/echo (.+)\") >>> def echo(chat, match): >>> return chat.reply(match.group(1))", "response": "def command(self, regexp):\n        \"\"\"\n        Register a new command\n\n        :param str regexp: Regular expression matching the command to register\n\n        :Example:\n\n        >>> @bot.command(r\"/echo (.+)\")\n        >>> def echo(chat, match):\n        >>>     return chat.reply(match.group(1))\n        \"\"\"\n\n        def decorator(fn):\n            self.add_command(regexp, fn)\n            return fn\n\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the callback for inline queries", "response": "def inline(self, callback):\n        \"\"\"\n        Set callback for inline queries\n\n        :Example:\n\n        >>> @bot.inline\n        >>> def echo(iq):\n        >>>     return iq.answer([\n        >>>         {\"type\": \"text\", \"title\": \"test\", \"id\": \"0\"}\n        >>>     ])\n\n        >>> @bot.inline(r\"myinline-(.+)\")\n        >>> def echo(chat, iq, match):\n        >>>     return iq.answer([\n        >>>         {\"type\": \"text\", \"title\": \"test\", \"id\": \"0\"}\n        >>>     ])\n        \"\"\"\n        if callable(callback):\n            self._default_inline = callback\n            return callback\n        elif isinstance(callback, str):\n\n            def decorator(fn):\n                self.add_inline(callback, fn)\n                return fn\n\n            return decorator\n        else:\n            raise TypeError(\"str expected {} given\".format(type(callback)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the callback for the user to query for the current locale.", "response": "def callback(self, callback):\n        \"\"\"\n        Set callback for callback queries\n\n        :Example:\n\n        >>> @bot.callback\n        >>> def echo(chat, cq):\n        >>>     return cq.answer()\n\n        >>> @bot.callback(r\"buttonclick-(.+)\")\n        >>> def echo(chat, cq, match):\n        >>>     return chat.reply(match.group(1))\n        \"\"\"\n        if callable(callback):\n            self._default_callback = callback\n            return callback\n        elif isinstance(callback, str):\n\n            def decorator(fn):\n                self.add_callback(callback, fn)\n                return fn\n\n            return decorator\n        else:\n            raise TypeError(\"str expected {} given\".format(type(callback)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets handler for specific message type :Example: >>> @bot.handle(\"audio\") >>> def handle(chat, audio): >>> pass", "response": "def handle(self, msg_type):\n        \"\"\"\n        Set handler for specific message type\n\n        :Example:\n\n        >>> @bot.handle(\"audio\")\n        >>> def handle(chat, audio):\n        >>>     pass\n        \"\"\"\n\n        def wrap(callback):\n            self._handlers[msg_type] = callback\n            return callback\n\n        return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling Telegram API method with params and return a future", "response": "def api_call(self, method, **params):\n        \"\"\"\n        Call Telegram API.\n\n        See https://core.telegram.org/bots/api for reference.\n\n        :param str method: Telegram API method\n        :param params: Arguments for the method call\n        \"\"\"\n        coro = self._api_call(method, **params)\n        # Explicitly ensure that API call is executed\n        return asyncio.ensure_future(coro)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing this method to send a text message to a chat.", "response": "def send_message(self, chat_id, text, **options):\n        \"\"\"\n        Send a text message to chat\n\n        :param int chat_id: ID of the chat to send the message to\n        :param str text: Text to send\n        :param options: Additional sendMessage options\n            (see https://core.telegram.org/bots/api#sendmessage)\n        \"\"\"\n        return self.api_call(\"sendMessage\", chat_id=chat_id, text=text, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing this method to edit a text message in a chat", "response": "def edit_message_text(self, chat_id, message_id, text, **options):\n        \"\"\"\n        Edit a text message in a chat\n\n        :param int chat_id: ID of the chat the message to edit is in\n        :param int message_id: ID of the message to edit\n        :param str text: Text to edit the message to\n        :param options: Additional API options\n        \"\"\"\n        return self.api_call(\n            \"editMessageText\",\n            chat_id=chat_id,\n            message_id=message_id,\n            text=text,\n            **options\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef edit_message_reply_markup(self, chat_id, message_id, reply_markup, **options):\n        return self.api_call(\n            \"editMessageReplyMarkup\",\n            chat_id=chat_id,\n            message_id=message_id,\n            reply_markup=reply_markup,\n            **options\n        )", "response": "Use this method to edit inline keyboard markup for a message in a chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads a file from Telegram servers", "response": "def download_file(self, file_path, range=None):\n        \"\"\"\n        Download a file from Telegram servers\n        \"\"\"\n        headers = {\"range\": range} if range else None\n        url = \"{0}/file/bot{1}/{2}\".format(API_URL, self.api_token, file_path)\n        return self.session.get(\n            url, headers=headers, proxy=self.proxy, proxy_auth=self.proxy_auth\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of profile pictures for a user", "response": "def get_user_profile_photos(self, user_id, **options):\n        \"\"\"\n        Get a list of profile pictures for a user\n\n        :param int user_id: Unique identifier of the target user\n        :param options: Additional getUserProfilePhotos options (see\n            https://core.telegram.org/bots/api#getuserprofilephotos)\n        \"\"\"\n        return self.api_call(\"getUserProfilePhotos\", user_id=str(user_id), **options)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrack a message using http://chatbase. com", "response": "def track(self, message, name=\"Message\"):\n        \"\"\"\n        Track message using http://chatbase.com\n        Set chatbase_token to make it work\n        \"\"\"\n        if self.chatbase_token:\n            asyncio.ensure_future(self._track(message, name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def webhook_handle(self, request):\n        update = await request.json(loads=self.json_deserialize)\n        self._process_update(update)\n        return web.Response()", "response": "A aiohttp. web handle for processing web hooks"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_webhook_app(self, path, loop=None):\n        app = web.Application(loop=loop)\n        app.router.add_route(\"POST\", path, self.webhook_handle)\n        return app", "response": "Create a web. Application with registered webhook hanlde\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_text(self, text, **options):\n        return self.bot.send_message(self.id, text, **options)", "response": "Send a text message to the chat."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplies to the message with the given text.", "response": "def reply(self, text, markup=None, parse_mode=None):\n        \"\"\"\n        Reply to the message this `Chat` object is based on.\n\n        :param str text: Text of the message to send\n        :param dict markup: Markup options\n        :param str parse_mode: Text parsing mode (``\"Markdown\"``, ``\"HTML\"`` or\n            ``None``)\n        \"\"\"\n        if markup is None:\n            markup = {}\n\n        return self.send_text(\n            text,\n            reply_to_message_id=self.message[\"message_id\"],\n            disable_web_page_preview=\"true\",\n            reply_markup=self.bot.json_serialize(markup),\n            parse_mode=parse_mode,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef edit_text(self, message_id, text, markup=None, parse_mode=None):\n        if markup is None:\n            markup = {}\n\n        return self.bot.edit_message_text(\n            self.id,\n            message_id,\n            text,\n            reply_markup=self.bot.json_serialize(markup),\n            parse_mode=parse_mode,\n        )", "response": "Edit the text of a message in this chat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nedit only reply markup of the message in this chat.", "response": "def edit_reply_markup(self, message_id, markup):\n        \"\"\"\n        Edit only reply markup of the message in this chat.\n\n        :param int message_id: ID of the message to edit\n        :param dict markup: Markup options\n        \"\"\"\n        return self.bot.edit_message_reply_markup(\n            self.id, message_id, reply_markup=self.bot.json_serialize(markup)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget information about a member of a chat.", "response": "def get_chat_member(self, user_id):\n        \"\"\"\n        Get information about a member of a chat.\n\n        :param int user_id: Unique identifier of the target user\n        \"\"\"\n        return self.bot.api_call(\n            \"getChatMember\", chat_id=str(self.id), user_id=str(user_id)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a sticker to the chat.", "response": "def send_sticker(self, sticker, **options):\n        \"\"\"\n        Send a sticker to the chat.\n\n        :param sticker: Sticker to send (file or string)\n        :param options: Additional sendSticker options (see\n            https://core.telegram.org/bots/api#sendsticker)\n        \"\"\"\n        return self.bot.api_call(\n            \"sendSticker\", chat_id=str(self.id), sticker=sticker, **options\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends an mp3 audio file to the chat.", "response": "def send_audio(self, audio, **options):\n        \"\"\"\n        Send an mp3 audio file to the chat.\n\n        :param audio: Object containing the audio data\n        :param options: Additional sendAudio options (see\n            https://core.telegram.org/bots/api#sendaudio)\n\n        :Example:\n\n        >>> with open(\"foo.mp3\", \"rb\") as f:\n        >>>     await chat.send_audio(f, performer=\"Foo\", title=\"Eversong\")\n        \"\"\"\n        return self.bot.api_call(\n            \"sendAudio\", chat_id=str(self.id), audio=audio, **options\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a photo to the chat.", "response": "def send_photo(self, photo, caption=\"\", **options):\n        \"\"\"\n        Send a photo to the chat.\n\n        :param photo: Object containing the photo data\n        :param str caption: Photo caption (optional)\n        :param options: Additional sendPhoto options (see\n            https://core.telegram.org/bots/api#sendphoto)\n\n        :Example:\n\n        >>> with open(\"foo.png\", \"rb\") as f:\n        >>>     await chat.send_photo(f, caption=\"Would you look at this!\")\n        \"\"\"\n        return self.bot.api_call(\n            \"sendPhoto\", chat_id=str(self.id), photo=photo, caption=caption, **options\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an mp4 video file to the chat.", "response": "def send_video(self, video, caption=\"\", **options):\n        \"\"\"\n        Send an mp4 video file to the chat.\n\n        :param video: Object containing the video data\n        :param str caption: Video caption (optional)\n        :param options: Additional sendVideo options (see\n            https://core.telegram.org/bots/api#sendvideo)\n\n        :Example:\n\n        >>> with open(\"foo.mp4\", \"rb\") as f:\n        >>>     await chat.send_video(f)\n        \"\"\"\n        return self.bot.api_call(\n            \"sendVideo\", chat_id=str(self.id), video=video, caption=caption, **options\n        )"}
