{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a new library function with the current manager.", "response": "def register(lib_name: str, cbl: Callable[[_AsyncLib], None]):\n    '''\n    Registers a new library function with the current manager.\n    '''\n    return manager.register(lib_name, cbl)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init(library: typing.Union[str, types.ModuleType]) -> None:\n    '''\n    Must be called at some point after import and before your event loop\n    is run.\n\n    Populates the asynclib instance of _AsyncLib with methods relevant to the\n    async library you are using.\n\n    The supported libraries at the moment are:\n    - curio\n    - trio\n\n    Args:\n        library (str or module): Either the module name as a string or the\n                                 imported module itself. E.g. ``multio.init(curio)``.\n    '''\n    if isinstance(library, types.ModuleType):\n        library = library.__name__\n\n    if library not in manager._handlers:\n        raise ValueError(\"Possible values are <{}>, not <{}>\".format(manager._handlers.keys(),\n                                                                     library))\n\n    manager.init(library, asynclib)\n    asynclib.lib_name = library\n    asynclib._init = True", "response": "Initializes the async library with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the appropriate library run function.", "response": "def run(*args, **kwargs):\n    '''\n    Runs the appropriate library run function.\n    '''\n    lib = sys.modules[asynclib.lib_name]\n    lib.run(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreceive some data on the socket.", "response": "async def recv(self, nbytes: int = -1, **kwargs) -> bytes:\n        '''\n        Receives some data on the socket.\n        '''\n        return await asynclib.recv(self.sock, nbytes, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend some data on the socket.", "response": "async def sendall(self, data: bytes, *args, **kwargs):\n        '''\n        Sends some data on the socket.\n        '''\n        return await asynclib.sendall(self.sock, data, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wrap(cls, meth):\n        '''\n        Wraps a connection opening method in this class.\n        '''\n\n        async def inner(*args, **kwargs):\n            sock = await meth(*args, **kwargs)\n            return cls(sock)\n\n        return inner", "response": "Wraps a connection opening method in this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def set(self, *args, **kwargs):\n        '''\n        Sets the value of the event.\n        '''\n        return await _maybe_await(self.event.set(*args, **kwargs))", "response": "Sets the value of the event.\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(self, library: str, cbl: Callable[['_AsyncLib'], None]):\n        '''\n        Registers a callable to set up a library.\n        '''\n        self._handlers[library] = cbl", "response": "Registers a callable to set up a library."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def trio_open_connection(host, port, *, ssl=False, **kwargs):\n    '''\n    Allows connections to be made that may or may not require ssl.\n    Somewhat surprisingly trio doesn't have an abstraction for this like\n    curio even though it's fairly trivial to write. Down the line hopefully.\n\n    Args:\n        host (str): Network location, either by domain or IP.\n        port (int): The requested port.\n        ssl (bool or SSLContext): If False or None, SSL is not required. If\n            True, the context returned by trio.ssl.create_default_context will\n            be used. Otherwise, this may be an SSLContext object.\n        kwargs: A catch all to soak up curio's additional kwargs and\n            ignore them.\n    '''\n    import trio\n    if not ssl:\n        sock = await trio.open_tcp_stream(host, port)\n    else:\n        if isinstance(ssl, bool):\n            ssl_context = None\n        else:\n            ssl_context = ssl\n        sock = await trio.open_ssl_over_tcp_stream(host, port, ssl_context=ssl_context)\n        await sock.do_handshake()\n\n    sock.close = sock.aclose\n    return sock", "response": "Open a connection to the curio."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef agent(state, host, server=None, port=None):\n\n    args = []\n\n    if server:\n        args.append('--server=%s' % server)\n    if port:\n        args.append('--masterport=%s' % port)\n\n    yield 'puppet agent -t %s' % ' '.join(args)", "response": "Run puppet agent\n\n    + server: master server URL\n    + port: puppet master port"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading any local config. py file.", "response": "def load_config(deploy_dir):\n    '''\n    Loads any local config.py file.\n    '''\n\n    config = Config()\n    config_filename = path.join(deploy_dir, 'config.py')\n\n    if path.exists(config_filename):\n        extract_file_config(config_filename, config)\n\n        # Now execute the file to trigger loading of any hooks\n        exec_file(config_filename)\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_deploy_config(deploy_filename, config=None):\n    '''\n    Loads any local config overrides in the deploy file.\n    '''\n\n    if not config:\n        config = Config()\n\n    if not deploy_filename:\n        return\n\n    if path.exists(deploy_filename):\n        extract_file_config(deploy_filename, config)\n\n    return config", "response": "Loads any local config overrides in the deploy file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert ls mode output to integer.", "response": "def _parse_mode(mode):\n    '''\n    Converts ls mode output (rwxrwxrwx) -> integer (755).\n    '''\n\n    result = ''\n    # owner, group, world\n    for group in [mode[0:3], mode[3:6], mode[6:9]]:\n        if group in SYMBOL_TO_OCTAL_PERMISSIONS:\n            result = '{0}{1}'.format(result, SYMBOL_TO_OCTAL_PERMISSIONS[group])\n        else:\n            result = '{0}0'.format(result)\n\n    # Return as an integer\n    return int(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse one iptables rule. Returns a dict where each key is mapped to a name using IPTABLES_ARGS.", "response": "def parse_iptables_rule(line):\n    '''\n    Parse one iptables rule. Returns a dict where each iptables code argument\n    is mapped to a name using IPTABLES_ARGS.\n    '''\n\n    bits = line.split()\n\n    definition = {}\n\n    key = None\n    args = []\n    not_arg = False\n\n    def add_args():\n        arg_string = ' '.join(args)\n\n        if key in IPTABLES_ARGS:\n            definition_key = (\n                'not_{0}'.format(IPTABLES_ARGS[key])\n                if not_arg\n                else IPTABLES_ARGS[key]\n            )\n            definition[definition_key] = arg_string\n        else:\n            definition.setdefault('extras', []).extend((key, arg_string))\n\n    for bit in bits:\n        if bit == '!':\n            if key:\n                add_args()\n                args = []\n                key = None\n\n            not_arg = True\n\n        elif bit.startswith('-'):\n            if key:\n                add_args()\n                args = []\n                not_arg = False\n\n            key = bit\n\n        else:\n            args.append(bit)\n\n    if key:\n        add_args()\n\n    if 'extras' in definition:\n        definition['extras'] = set(definition['extras'])\n\n    return definition"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef packages(state, host, packages=None, present=True, latest=False):\n    '''\n    Add/remove/update gem packages.\n\n    + packages: list of packages to ensure\n    + present: whether the packages should be installed\n    + latest: whether to upgrade packages without a specified version\n\n    Versions:\n        Package versions can be pinned like gem: ``<pkg>:<version>``.\n    '''\n\n    yield ensure_packages(\n        packages, host.fact.gem_packages, present,\n        install_command='gem install',\n        uninstall_command='gem uninstall',\n        upgrade_command='gem update',\n        version_join=':',\n        latest=latest,\n    )", "response": "Add or remove gem packages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_op(state, op_func, *args, **kwargs):\n    '''\n    Prepare & add an operation to ``pyinfra.state`` by executing it on all hosts.\n\n    Args:\n        state (``pyinfra.api.State`` obj): the deploy state to add the operation\n        to op_func (function): the operation function from one of the modules,\n        ie ``server.user``\n        args/kwargs: passed to the operation function\n    '''\n\n    frameinfo = get_caller_frameinfo()\n    kwargs['frameinfo'] = frameinfo\n\n    for host in state.inventory:\n        op_func(state, host, *args, **kwargs)", "response": "Prepare & add an operation to all hosts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_limited_op(state, op_func, hosts, *args, **kwargs):\n    '''\n    DEPRECATED: please use ``add_op`` with the ``hosts`` kwarg.\n    '''\n\n    # COMPAT w/ <0.4\n    # TODO: remove this function\n\n    logger.warning((\n        'Use of `add_limited_op` is deprecated, '\n        'please use `add_op` with the `hosts` kwarg instead.'\n    ))\n\n    if not isinstance(hosts, (list, tuple)):\n        hosts = [hosts]\n\n    # Set the limit\n    state.limit_hosts = hosts\n\n    # Add the op\n    add_op(state, op_func, *args, **kwargs)\n\n    # Remove the limit\n    state.limit_hosts = []", "response": "Add an operation that is limited to the hosts list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef operation(func=None, pipeline_facts=None):\n    '''\n    Decorator that takes a simple module function and turn it into the internal\n    operation representation that consists of a list of commands + options\n    (sudo, (sudo|su)_user, env).\n    '''\n\n    # If not decorating, return function with config attached\n    if func is None:\n        def decorator(f):\n            setattr(f, 'pipeline_facts', pipeline_facts)\n            return operation(f)\n\n        return decorator\n\n    # Index the operation!\n    module_bits = func.__module__.split('.')\n    module_name = module_bits[-1]\n    op_name = '.'.join((module_name, func.__name__))\n    OPERATIONS.append(op_name)\n\n    # Actually decorate!\n    @wraps(func)\n    def decorated_func(*args, **kwargs):\n        # Prepare state/host\n        #\n\n        # If we're in CLI mode, there's no state/host passed down, we need to\n        # use the global \"pseudo\" modules.\n        if len(args) < 2 or not (\n            isinstance(args[0], (State, PseudoModule))\n            and isinstance(args[1], (Host, PseudoModule))\n        ):\n            state = pseudo_state._module\n            host = pseudo_host._module\n\n            if state.in_op:\n                raise PyinfraError((\n                    'Nested operation called without state/host: {0} ({1})'\n                ).format(op_name, _get_call_location()))\n\n            if state.in_deploy:\n                raise PyinfraError((\n                    'Nested deploy operation called without state/host: {0} ({1})'\n                ).format(op_name, _get_call_location()))\n\n        # Otherwise (API mode) we just trim off the commands\n        else:\n            args_copy = list(args)\n            state, host = args[0], args[1]\n            args = args_copy[2:]\n\n        # In API mode we have the kwarg - if a nested operation call we have\n        # current_frameinfo.\n        frameinfo = kwargs.pop('frameinfo', get_caller_frameinfo())\n\n        # Configure operation\n        #\n\n        # Name the operation\n        names = None\n        autoname = False\n\n        # Look for a set as the first argument\n        if len(args) > 0 and isinstance(args[0], set):\n            names = args[0]\n            args_copy = list(args)\n            args = args[1:]\n\n        # Generate an operation name if needed (Module/Operation format)\n        else:\n            autoname = True\n            module_bits = func.__module__.split('.')\n            module_name = module_bits[-1]\n            names = {\n                '{0}/{1}'.format(module_name.title(), func.__name__.title()),\n            }\n\n        if state.deploy_name:\n            names = {\n                '{0} | {1}'.format(state.deploy_name, name)\n                for name in names\n            }\n\n        # Get the meta kwargs (globals that apply to all hosts)\n        op_meta_kwargs = pop_op_kwargs(state, kwargs)\n\n        # If this op is being called inside another, just return here\n        # (any unwanted/op-related kwargs removed above).\n        if state.in_op:\n            return func(state, host, *args, **kwargs) or []\n\n        filename = frameinfo.filename\n        line_number = frameinfo.lineno\n\n        # Figure out the lines this operation was called from (essentially like\n        # a line-call-stack).\n        op_lines = [line_number]\n\n        if state.deploy_line_numbers:\n            op_lines = list(state.deploy_line_numbers) + op_lines\n\n        # Inject the current op file number (only incremented in CLI mode)\n        op_lines.insert(0, state.current_op_file)\n\n        # Make a hash from the call stack lines\n        op_hash = make_hash(op_lines)\n\n        # Avoid adding duplicates! This happens if an operation is called within\n        # a loop - such that the filename/lineno/code _are_ the same, but the\n        # arguments might be different. We just append an increasing number to\n        # the op hash and also handle below with the op order.\n        host_op_hashes = state.meta[host]['op_hashes']\n        duplicate_op_count = 0\n        while op_hash in host_op_hashes:\n            logger.debug('Duplicate hash ({0}) detected!'.format(op_hash))\n            op_hash = '{0}-{1}'.format(op_hash, duplicate_op_count)\n            duplicate_op_count += 1\n\n        host_op_hashes.add(op_hash)\n\n        if duplicate_op_count:\n            op_lines.append(duplicate_op_count)\n\n        op_lines = tuple(op_lines)\n        state.op_line_numbers_to_hash[op_lines] = op_hash\n        logger.debug('Adding operation, {0}, called @ {1}:{2}, opLines={3}, opHash={4}'.format(\n            names, filename, line_number, op_lines, op_hash,\n        ))\n\n        # Ensure shared (between servers) operation meta\n        op_meta = state.op_meta.setdefault(op_hash, {\n            'names': set(),\n            'args': [],\n        })\n\n        # Add any meta kwargs (sudo, etc) to the meta - first parse any strings\n        # as jinja templates.\n        actual_op_meta_kwargs = {\n            key: get_arg_value(state, host, a)\n            for key, a in six.iteritems(op_meta_kwargs)\n        }\n        op_meta.update(actual_op_meta_kwargs)\n\n        # Add any new names to the set\n        op_meta['names'].update(names)\n\n        # Attach normal args, if we're auto-naming this operation\n        if autoname:\n            for arg in args:\n                if isinstance(arg, FunctionType):\n                    arg = arg.__name__\n\n                if arg not in op_meta['args']:\n                    op_meta['args'].append(arg)\n\n            # Attach keyword args\n            for key, value in six.iteritems(kwargs):\n                arg = '='.join((str(key), str(value)))\n                if arg not in op_meta['args']:\n                    op_meta['args'].append(arg)\n\n        # Check if we're actually running the operation on this host\n        #\n\n        # Run once and we've already added meta for this op? Stop here.\n        if op_meta_kwargs['run_once']:\n            has_run = False\n            for ops in six.itervalues(state.ops):\n                if op_hash in ops:\n                    has_run = True\n                    break\n\n            if has_run:\n                return OperationMeta(op_hash)\n\n        # If we're limited, stop here - *after* we've created op_meta. This\n        # ensures the meta object always exists, even if no hosts actually ever\n        # execute the op (due to limit or otherwise).\n        hosts = op_meta_kwargs['hosts']\n        when = op_meta_kwargs['when']\n\n        if (\n            # Limited by the state's limit_hosts?\n            (state.limit_hosts is not None and host not in state.limit_hosts)\n            # Limited by the operation kwarg hosts?\n            or (hosts is not None and host not in hosts)\n            # Limited by the operation kwarg when? We check == because when is\n            # normally attribute wrapped as a AttrDataBool, which is actually\n            # an integer (Python doesn't allow subclassing bool!).\n            or when == False  # noqa\n        ):\n            return OperationMeta(op_hash)\n\n        # \"Run\" operation\n        #\n\n        # Otherwise, flag as in-op and run it to get the commands\n        state.in_op = True\n        state.current_op_hash = op_hash\n\n        # Generate actual arguments by parsing strings as jinja2 templates. This\n        # means you can string format arguments w/o generating multiple\n        # operations. Only affects top level operations, as must be run \"in_op\"\n        # so facts are gathered correctly.\n        actual_args = [\n            get_arg_value(state, host, a)\n            for a in args\n        ]\n\n        actual_kwargs = {\n            key: get_arg_value(state, host, a)\n            for key, a in six.iteritems(kwargs)\n        }\n\n        # Convert to list as the result may be a generator\n        commands = unroll_generators(func(\n            state, host,\n            *actual_args,\n            **actual_kwargs\n        ))\n\n        state.in_op = False\n        state.current_op_hash = None\n\n        # Add host-specific operation data to state\n        #\n\n        # We're doing some commands, meta/ops++\n        state.meta[host]['ops'] += 1\n        state.meta[host]['commands'] += len(commands)\n\n        # Add the server-relevant commands\n        state.ops[host][op_hash] = {\n            'commands': commands,\n        }\n\n        # Return result meta for use in deploy scripts\n        return OperationMeta(op_hash, commands)\n\n    decorated_func._pyinfra_op = func\n    return decorated_func", "response": "Decorator that takes a simple module function and turn it into the internal internal"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing & add an deploy to pyinfra. state by executing it on all hosts.", "response": "def add_deploy(state, deploy_func, *args, **kwargs):\n    '''\n    Prepare & add an deploy to pyinfra.state by executing it on all hosts.\n\n    Args:\n        state (``pyinfra.api.State`` obj): the deploy state to add the operation\n        deploy_func (function): the operation function from one of the modules,\n        ie ``server.user``\n        args/kwargs: passed to the operation function\n    '''\n\n    frameinfo = get_caller_frameinfo()\n    kwargs['frameinfo'] = frameinfo\n\n    for host in state.inventory:\n        deploy_func(state, host, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_legacy_argstring(argstring):\n    '''\n    Preparses CLI input:\n\n    ``arg1,arg2`` => ``['arg1', 'arg2']``\n    ``[item1, item2],arg2`` => ``[['item1', 'item2'], arg2]``\n    '''\n\n    argstring = argstring.replace(',', ' , ')\n    argstring = argstring.replace('[', ' [ ')\n    argstring = argstring.replace(']', ' ] ')\n\n    argbits = shlex.split(argstring)\n    args = []\n    arg_buff = []\n    list_buff = []\n    in_list = False\n\n    for bit in argbits:\n        if bit == '[' and not in_list:\n            in_list = True\n            continue\n\n        elif bit == ']' and in_list:\n            in_list = False\n            args.append(list_buff)\n            list_buff = []\n            continue\n\n        elif bit == ',':\n            if not in_list and arg_buff:\n                args.append(''.join(arg_buff))\n                arg_buff = []\n\n            continue\n\n        # Restore any broken up ,[]s\n        bit = bit.replace(' , ', ',')\n        bit = bit.replace(' [ ', '[')\n        bit = bit.replace(' ] ', ']')\n\n        if in_list:\n            list_buff.append(bit)\n        else:\n            arg_buff.append(bit)\n\n    if arg_buff:\n        args.append(' '.join(arg_buff))\n\n    return args", "response": "Parses legacy command line arguments into a list of lists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_arguments(arguments):\n    '''\n    Prepares argumnents output by docopt.\n    '''\n\n    # Ensure parallel/port are numbers\n    for key in ('--parallel', '--port', '--fail-percent'):\n        if arguments[key]:\n            try:\n                arguments[key] = int(arguments[key])\n            except ValueError:\n                raise CliError('{0} is not a valid integer for {1}'.format(\n                    arguments[key], key,\n                ))\n\n    # Prep --run OP ARGS\n    if arguments['--run']:\n        op, args = setup_op_and_args(arguments['--run'], arguments['ARGS'])\n    else:\n        op = args = None\n\n    # Check deploy file exists\n    if arguments['DEPLOY']:\n        if not path.exists(arguments['DEPLOY']):\n            raise CliError('Deploy file not found: {0}'.format(arguments['DEPLOY']))\n\n    # Check our key file exists\n    if arguments['--key']:\n        if not path.exists(arguments['--key']):\n            raise CliError('Private key file not found: {0}'.format(arguments['--key']))\n\n    # Setup the rest\n    return {\n        # Deploy options\n        'inventory': arguments['-i'],\n        'deploy': arguments['DEPLOY'],\n        'verbose': arguments['-v'],\n        'dry': arguments['--dry'],\n        'serial': arguments['--serial'],\n        'no_wait': arguments['--no-wait'],\n        'debug': arguments['--debug'],\n\n        'debug_data': arguments['--debug-data'],\n        'debug_state': arguments['--debug-state'],\n\n        'fact': arguments['--fact'],\n\n        'limit': arguments['--limit'],\n        'op': op,\n        'op_args': args,\n\n        # Config options\n        'user': arguments['--user'],\n        'key': arguments['--key'],\n        'key_password': arguments['--key-password'],\n        'password': arguments['--password'],\n        'port': arguments['--port'],\n        'sudo': arguments['--sudo'],\n        'sudo_user': arguments['--sudo-user'],\n        'su_user': arguments['--su-user'],\n        'parallel': arguments['--parallel'],\n        'fail_percent': arguments['--fail-percent'],\n    }", "response": "Setup the arguments for the next iteration of the application."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes arbitrary SQL against MySQL.", "response": "def sql(\n    state, host, sql,\n    database=None,\n    # Details for speaking to MySQL via `mysql` CLI\n    mysql_user=None, mysql_password=None,\n    mysql_host=None, mysql_port=None,\n):\n    '''\n    Execute arbitrary SQL against MySQL.\n\n    + sql: SQL command(s) to execute\n    + database: optional database to open the connection with\n    + mysql_*: global module arguments, see above\n    '''\n\n    yield make_execute_mysql_command(\n        sql,\n        database=database,\n        user=mysql_user,\n        password=mysql_password,\n        host=mysql_host,\n        port=mysql_port,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate or update a user in the cluster.", "response": "def user(\n    state, host, name,\n    # Desired user settings\n    present=True,\n    user_hostname='localhost', password=None, privileges=None,\n    # Details for speaking to MySQL via `mysql` CLI via `mysql` CLI\n    mysql_user=None, mysql_password=None,\n    mysql_host=None, mysql_port=None,\n):\n    '''\n    Add/remove/update MySQL users.\n\n    + name: the name of the user\n    + present: whether the user should exist or not\n    + user_hostname: the hostname of the user\n    + password: the password of the user (if created)\n    + privileges: the global privileges for this user\n    + mysql_*: global module arguments, see above\n\n    Hostname:\n        this + ``name`` makes the username - so changing this will create a new\n        user, rather than update users with the same ``name``.\n\n    Password:\n        will only be applied if the user does not exist - ie pyinfra cannot\n        detect if the current password doesn't match the one provided, so won't\n        attempt to change it.\n    '''\n\n    current_users = host.fact.mysql_users(\n        mysql_user, mysql_password, mysql_host, mysql_port,\n    )\n\n    user_host = '{0}@{1}'.format(name, user_hostname)\n    is_present = user_host in current_users\n\n    # User not wanted?\n    if not present:\n        if is_present:\n            yield make_execute_mysql_command(\n                'DROP USER \"{0}\"@\"{1}\"'.format(name, user_hostname),\n                user=mysql_user,\n                password=mysql_password,\n                host=mysql_host,\n                port=mysql_port,\n            )\n        return\n\n    # If we want the user and they don't exist\n    if present and not is_present:\n        sql_bits = ['CREATE USER \"{0}\"@\"{1}\"'.format(name, user_hostname)]\n        if password:\n            sql_bits.append('IDENTIFIED BY \"{0}\"'.format(password))\n\n        yield make_execute_mysql_command(\n            ' '.join(sql_bits),\n            user=mysql_user,\n            password=mysql_password,\n            host=mysql_host,\n            port=mysql_port,\n        )\n\n    # If we're here either the user exists or we just created them; either way\n    # now we can check any privileges are set.\n    if privileges:\n        yield _privileges(\n            state, host, name, privileges,\n            user_hostname=user_hostname,\n            mysql_user=mysql_user, mysql_password=mysql_password,\n            mysql_host=mysql_host, mysql_port=mysql_port,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef database(\n    state, host, name,\n    # Desired database settings\n    present=True,\n    collate=None, charset=None,\n    user=None, user_hostname='localhost', user_privileges='ALL',\n    # Details for speaking to MySQL via `mysql` CLI\n    mysql_user=None, mysql_password=None,\n    mysql_host=None, mysql_port=None,\n):\n    '''\n    Add/remove MySQL databases.\n\n    + name: the name of the database\n    + present: whether the database should exist or not\n    + collate: the collate to use when creating the database\n    + charset: the charset to use when creating the database\n    + user: MySQL user to grant privileges on this database to\n    + user_hostname: the hostname of the MySQL user to grant\n    + user_privileges: privileges to grant to any specified user\n    + mysql_*: global module arguments, see above\n\n    Collate/charset:\n        these will only be applied if the database does not exist - ie pyinfra\n        will not attempt to alter the existing databases collate/character sets.\n    '''\n\n    current_databases = host.fact.mysql_databases(\n        mysql_user, mysql_password,\n        mysql_host, mysql_port,\n    )\n\n    is_present = name in current_databases\n\n    if not present:\n        if is_present:\n            yield make_execute_mysql_command(\n                'DROP DATABASE {0}'.format(name),\n                user=mysql_user,\n                password=mysql_password,\n                host=mysql_host,\n                port=mysql_port,\n            )\n        return\n\n    # We want the database but it doesn't exist\n    if present and not is_present:\n        sql_bits = ['CREATE DATABASE {0}'.format(name)]\n\n        if collate:\n            sql_bits.append('COLLATE {0}'.format(collate))\n\n        if charset:\n            sql_bits.append('CHARSET {0}'.format(charset))\n\n        yield make_execute_mysql_command(\n            ' '.join(sql_bits),\n            user=mysql_user,\n            password=mysql_password,\n            host=mysql_host,\n            port=mysql_port,\n        )\n\n    # Ensure any user privileges for this database\n    if user and user_privileges:\n        yield privileges(\n            state, host, user,\n            user_hostname=user_hostname,\n            privileges=user_privileges,\n            database=name,\n        )", "response": "Create a MySQL database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef privileges(\n    state, host,\n    user, privileges,\n    user_hostname='localhost',\n    database='*', table='*',\n    present=True,\n    flush=True,\n    # Details for speaking to MySQL via `mysql` CLI\n    mysql_user=None, mysql_password=None,\n    mysql_host=None, mysql_port=None,\n):\n    '''\n    Add/remove MySQL privileges for a user, either global, database or table specific.\n\n    + user: name of the user to manage privileges for\n    + privileges: list of privileges the user should have\n    + user_hostname: the hostname of the user\n    + database: name of the database to grant privileges to (defaults to all)\n    + table: name of the table to grant privileges to (defaults to all)\n    + present: whether these privileges should exist (False to ``REVOKE)\n    + flush: whether to flush (and update) the privileges table after any changes\n    + mysql_*: global module arguments, see above\n    '''\n\n    # Ensure we have a list\n    if isinstance(privileges, six.string_types):\n        privileges = [privileges]\n\n    if database != '*':\n        database = '`{0}`'.format(database)\n\n    if table != '*':\n        table = '`{0}`'.format(table)\n\n        # We can't set privileges on *.tablename as MySQL won't allow it\n        if database == '*':\n            raise OperationError((\n                'Cannot apply MySQL privileges on {0}.{1}, no database provided'\n            ).format(database, table))\n\n    database_table = '{0}.{1}'.format(database, table)\n    user_grants = host.fact.mysql_user_grants(\n        user, user_hostname,\n        mysql_user, mysql_password,\n        mysql_host, mysql_port,\n    )\n\n    has_privileges = False\n\n    if database_table in user_grants:\n        existing_privileges = [\n            'ALL' if privilege == 'ALL PRIVILEGES' else privilege\n            for privilege in user_grants[database_table]['privileges']\n        ]\n\n        has_privileges = (\n            database_table in user_grants\n            and all(\n                privilege in existing_privileges\n                for privilege in privileges\n            )\n        )\n\n    target = action = None\n\n    # No privilege and we want it\n    if not has_privileges and present:\n        action = 'GRANT'\n        target = 'TO'\n\n    # Permission we don't want\n    elif has_privileges and not present:\n        action = 'REVOKE'\n        target = 'FROM'\n\n    if target and action:\n        command = (\n            '{action} {privileges} '\n            'ON {database}.{table} '\n            '{target} \"{user}\"@\"{user_hostname}\"'\n        ).format(\n            privileges=', '.join(privileges),\n            action=action, target=target,\n            database=database, table=table,\n            user=user, user_hostname=user_hostname,\n        ).replace('`', '\\`')\n\n        yield make_execute_mysql_command(\n            command,\n            user=mysql_user,\n            password=mysql_password,\n            host=mysql_host,\n            port=mysql_port,\n        )\n\n        if flush:\n            yield make_execute_mysql_command(\n                'FLUSH PRIVILEGES',\n                user=mysql_user,\n                password=mysql_password,\n                host=mysql_host,\n                port=mysql_port,\n            )", "response": "Return a list of all privileges for a user in a specific MySQL database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(\n    state, host,\n    remote_filename, database=None,\n    # Details for speaking to MySQL via `mysql` CLI\n    mysql_user=None, mysql_password=None,\n    mysql_host=None, mysql_port=None,\n):\n    '''\n    Dump a MySQL database into a ``.sql`` file. Requires ``mysqldump``.\n\n    + database: name of the database to dump\n    + remote_filename: name of the file to dump the SQL to\n    + mysql_*: global module arguments, see above\n    '''\n\n    yield '{0} > {1}'.format(make_mysql_command(\n        executable='mysqldump',\n        database=database,\n        user=mysql_user,\n        password=mysql_password,\n        host=mysql_host,\n        port=mysql_port,\n    ), remote_filename)", "response": "Dump a MySQL database into a. sql file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef container(\n    state, host, name,\n    present=True, image='ubuntu:16.04',\n):\n    '''\n    Add/remove LXD containers.\n\n    Note: does not check if an existing container is based on the specified\n    image.\n\n    + name: name of the container\n    + image: image to base the container on\n    + present: whether the container should be present or absent\n    '''\n\n    container = get_container_named(name, host.fact.lxd_containers)\n\n    # Container exists and we don't want it\n    if container and not present:\n        if container['status'] == 'Running':\n            yield 'lxc stop {0}'.format(name)\n\n        # Command to remove the container:\n        yield 'lxc delete {0}'.format(name)\n\n    # Container doesn't exist and we want it\n    if not container and present:\n        # Command to create the container:\n        yield 'lxc launch {image} {name}'.format(name=name, image=image)", "response": "Add or remove a container"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a single host by name.", "response": "def get_host(self, name, default=NoHostError):\n        '''\n        Get a single host by name.\n        '''\n\n        if name in self.hosts:\n            return self.hosts[name]\n\n        if default is NoHostError:\n            raise NoHostError('No such host: {0}'.format(name))\n\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of hosts belonging to a group.", "response": "def get_group(self, name, default=NoGroupError):\n        '''\n        Get a list of hosts belonging to a group.\n        '''\n\n        if name in self.groups:\n            return self.groups[name]\n\n        if default is NoGroupError:\n            raise NoGroupError('No such group: {0}'.format(name))\n\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_groups_data(self, groups):\n        '''\n        Gets aggregated data from a list of groups. Vars are collected in order so, for\n        any groups which define the same var twice, the last group's value will hold.\n        '''\n\n        data = {}\n\n        for group in groups:\n            data.update(self.get_group_data(group))\n\n        return data", "response": "Gets aggregated data from a list of groups."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets any default data attached to the current deploy if any.", "response": "def get_deploy_data(self):\n        '''\n        Gets any default data attached to the current deploy, if any.\n        '''\n\n        if self.state and self.state.deploy_data:\n            return self.state.deploy_data\n\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmanages git config for a specific key and value", "response": "def config(\n    state, host, key, value,\n    repo=None,\n):\n    '''\n    Manage git config for a repository or globally.\n\n    + key: the key of the config to ensure\n    + value: the value this key should have\n    + repo: specify the git repo path to edit local config (defaults to global)\n    '''\n\n    existing_config = host.fact.git_config(repo)\n\n    if key not in existing_config or existing_config[key] != value:\n        if repo is None:\n            yield 'git config --global {0} \"{1}\"'.format(key, value)\n        else:\n            yield 'cd {0} && git config --local {1} \"{2}\"'.format(repo, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef repo(\n    state, host, source, target,\n    branch='master', pull=True, rebase=False,\n    user=None, group=None, use_ssh_user=False, ssh_keyscan=False,\n):\n    '''\n    Clone/pull git repositories.\n\n    + source: the git source URL\n    + target: target directory to clone to\n    + branch: branch to pull/checkout\n    + pull: pull any changes for the branch\n    + rebase: when pulling, use ``--rebase``\n    + user: chown files to this user after\n    + group: chown files to this group after\n    + ssh_keyscan: keyscan the remote host if not in known_hosts before clone/pull\n\n    + [DEPRECATED] use_ssh_user: whether to use the SSH user to clone/pull\n\n    SSH user (deprecated, please use ``preserve_sudo_env``):\n        This is an old hack from pyinfra <0.4 which did not support the global\n        kwarg ``preserve_sudo_env``. It does the following:\n\n        * makes the target directory writeable by all\n        * clones/pulls w/o sudo as the connecting SSH user\n        * removes other/group write permissions - unless group is defined, in\n          which case only other\n    '''\n\n    if use_ssh_user:\n        logger.warning(\n            'Use of `use_ssh_user` is deprecated, please use `preserve_sudo_env` instead.',\n        )\n\n    # Ensure our target directory exists\n    yield files.directory(state, host, target)\n\n    # If we're going to chown this after clone/pull, and we're sudo'd, we need to make the\n    # directory writeable by the SSH user\n    if use_ssh_user:\n        yield chmod(target, 'go+w', recursive=True)\n\n    # Do we need to scan for the remote host key?\n    if ssh_keyscan:\n        # Attempt to parse the domain from the git repository\n        domain = re.match(r'^[a-zA-Z0-9]+@([0-9a-zA-Z\\.\\-]+)', source)\n\n        if domain:\n            yield ssh.keyscan(state, host, domain.group(1))\n        else:\n            raise OperationError(\n                'Could not parse domain (to SSH keyscan) from: {0}'.format(source),\n            )\n\n    # Store git commands for directory prefix\n    git_commands = []\n    is_repo = host.fact.directory('/'.join((target, '.git')))\n\n    # Cloning new repo?\n    if not is_repo:\n        git_commands.append('clone {0} --branch {1} .'.format(source, branch))\n\n    # Ensuring existing repo\n    else:\n        current_branch = host.fact.git_branch(target)\n        if current_branch != branch:\n            git_commands.append('fetch')  # fetch to ensure we have the branch locally\n            git_commands.append('checkout {0}'.format(branch))\n\n        if pull:\n            if rebase:\n                git_commands.append('pull --rebase')\n            else:\n                git_commands.append('pull')\n\n    # Attach prefixes for directory\n    command_prefix = 'cd {0} && git'.format(target)\n    git_commands = [\n        '{0} {1}'.format(command_prefix, command)\n        for command in git_commands\n    ]\n\n    if use_ssh_user:\n        git_commands = [\n            {\n                'command': command,\n                'sudo': False,\n                'sudo_user': False,\n            }\n            for command in git_commands\n        ]\n\n    for cmd in git_commands:\n        yield cmd\n\n    if use_ssh_user:\n        # Remove write permissions from other or other+group when no group\n        yield chmod(\n            target,\n            'o-w' if group else 'go-w',\n            recursive=True,\n        )\n\n    # Apply any user or group\n    if user or group:\n        yield chown(target, user, group, recursive=True)", "response": "Clone and pull git repositories."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef virtualenv(\n    state, host, path,\n    python=None, site_packages=False, always_copy=False, present=True,\n):\n    '''\n    Add/remove Python virtualenvs.\n\n    + python: python interpreter to use\n    + site_packages: give access to the global site-packages\n    + always_copy: always copy files rather than symlinking\n    + present: whether the virtualenv should exist\n    '''\n\n    if present is False and host.fact.directory(path):\n        yield files.directory(state, host, path, present=False)\n\n    elif present and not host.fact.directory(path):\n        # Create missing virtualenv\n        command = ['virtualenv']\n\n        if python:\n            command.append('-p {0}'.format(python))\n\n        if site_packages:\n            command.append('--system-site-packages')\n\n        if always_copy:\n            command.append('--always-copy')\n\n        command.append(path)\n\n        yield ' '.join(command)", "response": "Create a virtualenv if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef include(filename, hosts=False, when=True):\n    '''\n    Executes a local python file within the ``pyinfra.pseudo_state.deploy_dir``\n    directory.\n\n    Args:\n        hosts (string, list): group name or list of hosts to limit this include to\n        when (bool): indicate whether to trigger operations in this include\n    '''\n\n    if not pyinfra.is_cli:\n        raise PyinfraError('local.include is only available in CLI mode.')\n\n    if not when:\n        return\n\n    if hosts is not False:\n        hosts = ensure_host_list(hosts, inventory=pseudo_state.inventory)\n        if pseudo_host not in hosts:\n            return\n\n    if pseudo_state.deploy_dir:\n        filename = path.join(pseudo_state.deploy_dir, filename)\n\n    frameinfo = get_caller_frameinfo()\n\n    logger.debug('Including local file: {0}'.format(filename))\n\n    try:\n        # Fixes a circular import because `pyinfra.local` is really a CLI\n        # only thing (so should be `pyinfra_cli.local`). It is kept here\n        # to maintain backwards compatability and the nicer public import\n        # (ideally users never need to import from `pyinfra_cli`).\n\n        from pyinfra_cli.config import extract_file_config\n        from pyinfra_cli.util import exec_file\n\n        # Load any config defined in the file and setup like a @deploy\n        config_data = extract_file_config(filename)\n        kwargs = {\n            key.lower(): value\n            for key, value in six.iteritems(config_data)\n            if key in [\n                'SUDO', 'SUDO_USER', 'SU_USER',\n                'PRESERVE_SUDO_ENV', 'IGNORE_ERRORS',\n            ]\n        }\n        with pseudo_state.deploy(\n            filename, kwargs, None, frameinfo.lineno,\n            in_deploy=False,\n        ):\n            exec_file(filename)\n\n        # One potential solution to the above is to add local as an actual\n        # module, ie `pyinfra.modules.local`.\n\n    except IOError as e:\n        raise PyinfraError(\n            'Could not include local file: {0}\\n{1}'.format(filename, e),\n        )", "response": "Executes a local python file within the pyinfra. deploy_dir directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing a new virtual environment.", "response": "def init_virtualenv():\n    '''\n    Add a virtualenv to sys.path so the user can import modules from it.\n    This isn't perfect: it doesn't use the Python interpreter with which the\n    virtualenv was built, and it ignores the --no-site-packages option. A\n    warning will appear suggesting the user installs IPython in the\n    virtualenv, but for many cases, it probably works well enough.\n\n    Adapted from IPython's implementation:\n    https://github.com/ipython/ipython/blob/master/IPython/core/interactiveshell.py\n    '''\n\n    if 'VIRTUAL_ENV' not in os.environ:\n        # Not in a virtualenv\n        return\n\n    p = os.path.normcase(sys.executable)\n    p_venv = os.path.normcase(os.environ['VIRTUAL_ENV'])\n\n    # executable path should end like /bin/python or \\\\scripts\\\\python.exe\n    p_exe_up2 = os.path.dirname(os.path.dirname(p))\n    if p_exe_up2 and os.path.samefile(p_exe_up2, p_venv):\n        # Our exe is inside the virtualenv, don't need to do anything.\n        return\n\n    # fallback venv detection:\n    # stdlib venv may symlink sys.executable, so we can't use realpath.\n    # but others can symlink *to* the venv Python, so we can't just use sys.executable.\n    # So we just check every item in the symlink tree (generally <= 3)\n    paths = [p]\n    while os.path.islink(p):\n        p = os.path.normcase(os.path.join(os.path.dirname(p), os.readlink(p)))\n        paths.append(p)\n\n    # In Cygwin paths like 'c:\\...' and '\\cygdrive\\c\\...' are possible\n    if p_venv.startswith('\\\\cygdrive'):\n        p_venv = p_venv[11:]\n    elif len(p_venv) >= 2 and p_venv[1] == ':':\n        p_venv = p_venv[2:]\n\n    if any(p_venv in p for p in paths):\n        # Running properly in the virtualenv, don't need to do anything\n        return\n\n    logger.warning((\n        'Attempting to work in a virtualenv.\\n'\n        '    If you encounter problems, please install pyinfra inside the virtualenv.'\n    ))\n    print()\n\n    if sys.platform == 'win32':\n        virtual_env = os.path.join(\n            os.environ['VIRTUAL_ENV'],\n            'Lib',\n            'site-packages',\n        )\n    else:\n        virtual_env = os.path.join(\n            os.environ['VIRTUAL_ENV'],\n            'lib',\n            'python%d.%d' % sys.version_info[:2],\n            'site-packages',\n        )\n\n    import site\n    sys.path.insert(0, virtual_env)\n    site.addsitedir(virtual_env)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chain(\n    state, host, name, present=True,\n    table='filter', policy=None, version=4,\n):\n    '''\n    Add/remove/update iptables chains.\n\n    + name: the name of the chain\n    + present: whether the chain should exist\n    + table: the iptables table this chain should belong to\n    + policy: the policy this table should have\n    + version: whether to target iptables or ip6tables\n\n    Policy:\n        These can only be applied to system chains (FORWARD, INPUT, OUTPUT, etc).\n    '''\n\n    chains = (\n        host.fact.iptables_chains(table)\n        if version == 4\n        else host.fact.ip6tables_chains(table)\n    )\n\n    command = 'iptables' if version == 4 else 'ip6tables'\n    command = '{0} -t {1}'.format(command, table)\n\n    # Doesn't exist but we want it?\n    if present and name not in chains:\n        yield '{0} -N {1}'.format(command, name)\n\n        if policy:\n            yield '{0} -P {1} {2}'.format(command, name, policy)\n\n    # Exists and we don't want it?\n    if not present and name in chains:\n        yield '{0} -X {1}'.format(command, name)\n\n    # Exists, we want it, but the policies don't match?\n    if present and name in chains and policy and chains[name] != policy:\n        yield '{0} -P {1} {2}'.format(command, name, policy)", "response": "Add or remove iptables chains."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rule(\n    state, host, chain, jump, present=True,\n    table='filter', append=True, version=4,\n    # Core iptables filter arguments\n    protocol=None, not_protocol=None,\n    source=None, not_source=None,\n    destination=None, not_destination=None,\n    in_interface=None, not_in_interface=None,\n    out_interface=None, not_out_interface=None,\n    # After-rule arguments\n    to_destination=None, to_source=None, to_ports=None, log_prefix=None,\n    # Extras and extra shortcuts\n    destination_port=None, source_port=None, extras='',\n):\n    '''\n    Add/remove iptables rules.\n\n    + chain: the chain this rule should live in\n    + jump: the target of the rule\n    + table: the iptables table this rule should belong to\n    + append: whether to append or insert the rule (if not present)\n    + version: whether to target iptables or ip6tables\n\n    Iptables args:\n\n    + protocol/not_protocol: filter by protocol (tcp or udp)\n    + source/not_source: filter by source IPs\n    + destination/not_destination: filter by destination IPs\n    + in_interface/not_in_interface: filter by incoming interface\n    + out_interface/not_out_interface: filter by outgoing interface\n    + to_destination: where to route to when jump=DNAT\n    + to_source: where to route to when jump=SNAT\n    + to_ports: where to route to when jump=REDIRECT\n    + log_prefix: prefix for the log of this rule when jump=LOG\n\n    Extras:\n\n    + extras: a place to define iptables extension arguments (eg --limit, --physdev)\n    + destination_port: destination port (requires protocol)\n    + source_port: source port (requires protocol)\n\n    Examples:\n\n    .. code:: python\n\n        # Block SSH traffic\n\n        iptables.rule(\n            'INPUT', 'DROP',\n            destination_port=22\n        )\n\n\n        # NAT traffic on from 8.8.8.8:53 to 8.8.4.4:8080\n\n        iptables.rule(\n            'PREROUTING', 'DNAT', table='nat',\n            source='8.8.8.8', destination_port=53,\n            to_destination='8.8.4.4:8080'\n        )\n    '''\n\n    # These are only shortcuts for extras\n    if destination_port:\n        extras = '{0} --dport {1}'.format(extras, destination_port)\n\n    if source_port:\n        extras = '{0} --sport {1}'.format(extras, source_port)\n\n    # Convert the extras string into a set to enable comparison with the fact\n    extras_set = set(extras.split())\n\n    # When protocol is set, the extension is automagically added by iptables (which shows\n    # in iptables-save): http://ipset.netfilter.org/iptables-extensions.man.html\n    if protocol:\n        extras_set.add('-m')\n        extras_set.add(protocol)\n\n    # --dport and --sport do not work without a protocol (because they need -m [tcp|udp]\n    elif destination_port or source_port:\n        raise OperationError(\n            'iptables cannot filter by destination_port/source_port without a protocol',\n        )\n\n    # Verify NAT arguments, --to-destination only w/table=nat, jump=DNAT\n    if to_destination and (table != 'nat' or jump != 'DNAT'):\n        raise OperationError(\n            'iptables only supports to_destination on the nat table and the DNAT jump '\n            '(table={0}, jump={1})'.format(table, jump),\n        )\n\n    # As above, --to-source only w/table=nat, jump=SNAT\n    if to_source and (table != 'nat' or jump != 'SNAT'):\n        raise OperationError(\n            'iptables only supports to_source on the nat table and the SNAT jump '\n            '(table={0}, jump={1})'.format(table, jump),\n        )\n\n    # As above, --to-ports only w/table=nat, jump=REDIRECT\n    if to_ports and (table != 'nat' or jump != 'REDIRECT'):\n        raise OperationError(\n            'iptables only supports to_ports on the nat table and the REDIRECT jump '\n            '(table={0}, jump={1})'.format(table, jump),\n        )\n\n    # --log-prefix is only supported with jump=LOG\n    if log_prefix and jump != 'LOG':\n        raise OperationError(\n            'iptables only supports log_prefix with the LOG jump '\n            '(jump={0})'.format(jump),\n        )\n\n    definition = {\n        'chain': chain,\n        'jump': jump,\n\n        'protocol': protocol,\n        'source': source,\n        'destination': destination,\n        'in_interface': in_interface,\n        'out_interface': out_interface,\n\n        'not_protocol': not_protocol,\n        'not_source': not_source,\n        'not_destination': not_destination,\n        'not_in_interface': not_in_interface,\n        'not_out_interface': not_out_interface,\n\n        # These go *after* the jump argument\n        'log_prefix': log_prefix,\n        'to_destination': to_destination,\n        'to_source': to_source,\n        'to_ports': to_ports,\n        'extras': extras_set,\n    }\n\n    definition = {\n        key: (\n            '{0}/32'.format(value)\n            if (\n                '/' not in value\n                and key in ('source', 'not_source', 'destination', 'not_destination')\n            )\n            else value\n        )\n        for key, value in six.iteritems(definition)\n        if value\n    }\n\n    rules = (\n        host.fact.iptables_rules(table)\n        if version == 4\n        else host.fact.ip6tables_rules(table)\n    )\n\n    action = None\n\n    # Definition doesn't exist and we want it\n    if present and definition not in rules:\n        action = '-A' if append else '-I'\n\n    # Definition exists and we don't want it\n    if not present and definition in rules:\n        action = '-D'\n\n    # Are we adding/removing a rule? Lets build it\n    if action:\n        args = [\n            'iptables' if version == 4 else 'ip6tables',\n            # Add the table\n            '-t', table,\n            # Add the action and target chain\n            action, chain,\n        ]\n\n        if protocol:\n            args.extend(('-p', protocol))\n\n        if source:\n            args.extend(('-s', source))\n\n        if in_interface:\n            args.extend(('-i', in_interface))\n\n        if out_interface:\n            args.extend(('-o', out_interface))\n\n        if not_protocol:\n            args.extend(('!', '-p', not_protocol))\n\n        if not_source:\n            args.extend(('!', '-s', not_source))\n\n        if not_in_interface:\n            args.extend(('!', '-i', not_in_interface))\n\n        if not_out_interface:\n            args.extend(('!', '-o', not_out_interface))\n\n        if extras:\n            args.append(extras.strip())\n\n        # Add the jump\n        args.extend(('-j', jump))\n\n        if log_prefix:\n            args.extend(('--log-prefix', log_prefix))\n\n        if to_destination:\n            args.extend(('--to-destination', to_destination))\n\n        if to_source:\n            args.extend(('--to-source', to_source))\n\n        if to_ports:\n            args.extend(('--to-ports', to_ports))\n\n        # Build the final iptables command\n        yield ' '.join(args)", "response": "This function creates a new iptables rule in the chain."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares and send a request to the specified URL.", "response": "def send(self, request, stem=None):\n        \"\"\"Prepare and send a request\n\n        Arguments:\n            request: a Request object that is not yet prepared\n            stem: a path to append to the root URL\n\n        Returns:\n            The response to the request\n\n        \"\"\"\n        if stem is not None:\n            request.url = request.url + \"/\" + stem.lstrip(\"/\")\n\n        prepped = self.session.prepare_request(request)\n        settings = self.session.merge_environment_settings(url=prepped.url,\n                                                           proxies={},\n                                                           stream=None,\n                                                           verify=None,\n                                                           cert=None)\n\n        return self.session.send(prepped, **settings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of the users gists as GistInfo objects", "response": "def list(self):\n        \"\"\"Returns a list of the users gists as GistInfo objects\n\n        Returns:\n            a list of GistInfo objects\n\n        \"\"\"\n        # Define the basic request. The per_page parameter is set to 100, which\n        # is the maximum github allows. If the user has more than one page of\n        # gists, this request object will be modified to retrieve each\n        # successive page of gists.\n        request = requests.Request(\n                'GET',\n                'https://api.github.com/gists',\n                headers={\n                    'Accept-Encoding': 'identity, deflate, compress, gzip',\n                    'User-Agent': 'python-requests/1.2.0',\n                    'Accept': 'application/vnd.github.v3.base64',\n                    },\n                params={\n                    'access_token': self.token,\n                    'per_page': 100,\n                    },\n                )\n\n        # Github provides a 'link' header that contains information to\n        # navigate through a users page of gists. This regex is used to\n        # extract the URLs contained in this header, and to find the next page\n        # of gists.\n        pattern = re.compile(r'<([^>]*)>; rel=\"([^\"]*)\"')\n\n        gists = []\n        while True:\n\n            # Retrieve the next page of gists\n            try:\n                response = self.send(request).json()\n\n            except Exception:\n                break\n\n            # Extract the list of gists\n            for gist in response:\n                try:\n                    gists.append(\n                            GistInfo(\n                                gist['id'],\n                                gist['public'],\n                                gist['description'],\n                                )\n                            )\n\n                except KeyError:\n                    continue\n\n            try:\n                link = response.headers['link']\n\n                # Search for the next page of gist. If a 'next' page is found,\n                # the URL is set to this new page and the iteration continues.\n                # If there is no next page, return the list of gists.\n                for result in pattern.finditer(link):\n                    url = result.group(1)\n                    rel = result.group(2)\n                    if rel == 'next':\n                        request.url = url\n                        break\n                else:\n                    return gists\n\n            except Exception:\n                break\n\n        return gists"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a gist Arguments: request: an initial request object desc: the gist description files: a list of files to add to the gist public: a flag to indicate whether the gist is public or not Returns: The URL to the newly created gist.", "response": "def create(self, request, desc, files, public=False):\n        \"\"\"Creates a gist\n\n        Arguments:\n            request: an initial request object\n            desc:    the gist description\n            files:   a list of files to add to the gist\n            public:  a flag to indicate whether the gist is public or not\n\n        Returns:\n            The URL to the newly created gist.\n\n        \"\"\"\n        request.data = json.dumps({\n                \"description\": desc,\n                \"public\": public,\n                \"files\": files,\n                })\n        return self.send(request).json()['html_url']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef files(self, request, id):\n        gist = self.send(request, id).json()\n        return gist['files']", "response": "Returns a list of files in the gist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the content of the gist with the given id", "response": "def content(self, request, id):\n        \"\"\"Returns the content of the gist\n\n        Arguments:\n            request: an initial request object\n            id:      the gist identifier\n\n        Returns:\n            A dict containing the contents of each file in the gist\n\n        \"\"\"\n        gist = self.send(request, id).json()\n\n        def convert(data):\n            return base64.b64decode(data).decode('utf-8')\n\n        content = {}\n        for name, data in gist['files'].items():\n            content[name] = convert(data['content'])\n\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an archive of a gist", "response": "def archive(self, request, id):\n        \"\"\"Create an archive of a gist\n\n        The files in the gist are downloaded and added to a compressed archive\n        (tarball). If the ID of the gist was c78d925546e964b4b1df, the\n        resulting archive would be,\n\n            c78d925546e964b4b1df.tar.gz\n\n        The archive is created in the directory where the command is invoked.\n\n        Arguments:\n            request: an initial request object\n            id:      the gist identifier\n\n        \"\"\"\n        gist = self.send(request, id).json()\n\n        with tarfile.open('{}.tar.gz'.format(id), mode='w:gz') as archive:\n            for name, data in gist['files'].items():\n                with tempfile.NamedTemporaryFile('w+') as fp:\n                    fp.write(data['content'])\n                    fp.flush()\n                    archive.add(fp.name, arcname=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef edit(self, request, id):\n        with pushd(tempfile.gettempdir()):\n            try:\n                self.clone(id)\n                with pushd(id):\n                    files = [f for f in os.listdir('.') if os.path.isfile(f)]\n                    quoted = ['\"{}\"'.format(f) for f in files]\n                    os.system(\"{} {}\".format(self.editor, ' '.join(quoted)))\n                    os.system('git commit -av && git push')\n\n            finally:\n                shutil.rmtree(id)", "response": "Edit a gist s contents"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the description of a gist", "response": "def description(self, request, id, description):\n        \"\"\"Updates the description of a gist\n\n        Arguments:\n            request:     an initial request object\n            id:          the id of the gist we want to edit the description for\n            description: the new description\n\n        \"\"\"\n        request.data = json.dumps({\n            \"description\": description\n        })\n        return self.send(request, id).json()['html_url']"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclones a gist with the given id and optional name", "response": "def clone(self, id, name=None):\n        \"\"\"Clone a gist\n\n        Arguments:\n            id:   the gist identifier\n            name: the name to give the cloned repo\n\n        \"\"\"\n        url = 'git@gist.github.com:/{}'.format(id)\n\n        if name is None:\n            os.system('git clone {}'.format(url))\n        else:\n            os.system('git clone {} {}'.format(url, name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef repo(\n    state, host, name, baseurl,\n    present=True, description=None, enabled=True, gpgcheck=True, gpgkey=None,\n):\n    '''\n    Add/remove/update yum repositories.\n\n    + name: filename for the repo (in ``/etc/yum/repos.d/``)\n    + baseurl: the baseurl of the repo\n    + present: whether the ``.repo`` file should be present\n    + description: optional verbose description\n    + gpgcheck: whether set ``gpgcheck=1``\n    + gpgkey: the URL to the gpg key for this repo\n    '''\n\n    # Description defaults to name\n    description = description or name\n\n    filename = '/etc/yum.repos.d/{0}.repo'.format(name)\n\n    # If we don't want the repo, just remove any existing file\n    if not present:\n        yield files.file(state, host, filename, present=False)\n        return\n\n    # Build the repo file from string\n    repo_lines = [\n        '[{0}]'.format(name),\n        'name={0}'.format(description),\n        'baseurl={0}'.format(baseurl),\n        'enabled={0}'.format(1 if enabled else 0),\n        'gpgcheck={0}'.format(1 if gpgcheck else 0),\n    ]\n\n    if gpgkey:\n        repo_lines.append('gpgkey={0}'.format(gpgkey))\n\n    repo_lines.append('')\n    repo = '\\n'.join(repo_lines)\n    repo = StringIO(repo)\n\n    # Ensure this is the file on the server\n    yield files.put(state, host, repo, filename)", "response": "Add or update a repository file for a new object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rpm(state, host, source, present=True):\n    '''\n    Add/remove ``.rpm`` file packages.\n\n    + source: filename or URL of the ``.rpm`` package\n    + present: whether ore not the package should exist on the system\n\n    URL sources with ``present=False``:\n        If the ``.rpm`` file isn't downloaded, pyinfra can't remove any existing\n        package as the file won't exist until mid-deploy.\n    '''\n\n    # If source is a url\n    if urlparse(source).scheme:\n        # Generate a temp filename (with .rpm extension to please yum)\n        temp_filename = '{0}.rpm'.format(state.get_temp_filename(source))\n\n        # Ensure it's downloaded\n        yield files.download(state, host, source, temp_filename)\n\n        # Override the source with the downloaded file\n        source = temp_filename\n\n    # Check for file .rpm information\n    info = host.fact.rpm_package(source)\n    exists = False\n\n    # We have info!\n    if info:\n        current_packages = host.fact.rpm_packages\n\n        if (\n            info['name'] in current_packages\n            and info['version'] in current_packages[info['name']]\n        ):\n            exists = True\n\n    # Package does not exist and we want?\n    if present and not exists:\n        # If we had info, always install\n        if info:\n            yield 'rpm -U {0}'.format(source)\n\n        # This happens if we download the package mid-deploy, so we have no info\n        # but also don't know if it's installed. So check at runtime, otherwise\n        # the install will fail.\n        else:\n            yield 'rpm -qa | grep `rpm -qp {0}` || rpm -U {0}'.format(source)\n\n    # Package exists but we don't want?\n    if exists and not present:\n        yield 'yum remove -y {0}'.format(info['name'])", "response": "Download and install the specified rpm package."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef keyscan(state, host, hostname, force=False):\n    '''\n    Check/add hosts to the ``~/.ssh/known_hosts`` file.\n\n    + hostname: hostname that should have a key in ``known_hosts``\n    + force: if the key already exists, remove and rescan\n    '''\n\n    yield files.directory(\n        state, host,\n        '~/.ssh',\n        mode=700,\n    )\n\n    hostname_present = host.fact.find_in_file(\n        '~/.ssh/known_hosts',\n        hostname,\n    )\n\n    keyscan_command = 'ssh-keyscan {0} >> ~/.ssh/known_hosts'.format(hostname)\n\n    if not hostname_present:\n        yield keyscan_command\n\n    elif force:\n        yield 'ssh-keygen -R {0}'.format(hostname)\n        yield keyscan_command", "response": "Keyscan the given hostname"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef command(state, host, hostname, command, ssh_user=None):\n    '''\n    Execute commands on other servers over SSH.\n\n    + hostname: the hostname to connect to\n    + command: the command to execute\n    + ssh_user: connect with this user\n    '''\n\n    connection_target = hostname\n    if ssh_user:\n        connection_target = '@'.join((ssh_user, hostname))\n\n    yield 'ssh {0} \"{1}\"'.format(connection_target, command)", "response": "Execute commands on other servers over SSH."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads a file to another server using scp.", "response": "def upload(\n    state, host, hostname, filename,\n    remote_filename=None, use_remote_sudo=False,\n    ssh_keyscan=False, ssh_user=None,\n):\n    '''\n    Upload files to other servers using ``scp``.\n\n    + hostname: hostname to upload to\n    + filename: file to upload\n    + remote_filename: where to upload the file to (defaults to ``filename``)\n    + use_remote_sudo: upload to a temporary location and move using sudo\n    + ssh_keyscan: execute ``ssh.keyscan`` before uploading the file\n    + ssh_user: connect with this user\n    '''\n\n    remote_filename = remote_filename or filename\n\n    # Figure out where we're connecting (host or user@host)\n    connection_target = hostname\n    if ssh_user:\n        connection_target = '@'.join((ssh_user, hostname))\n\n    if ssh_keyscan:\n        yield keyscan(state, host, hostname)\n\n    # If we're not using sudo on the remote side, just scp the file over\n    if not use_remote_sudo:\n        yield 'scp {0} {1}:{2}'.format(filename, connection_target, remote_filename)\n\n    else:\n        # Otherwise - we need a temporary location for the file\n        temp_remote_filename = state.get_temp_filename()\n\n        # scp it to the temporary location\n        upload_cmd = 'scp {0} {1}:{2}'.format(\n            filename, connection_target, temp_remote_filename,\n        )\n\n        yield upload_cmd\n\n        # And sudo sudo to move it\n        yield command(state, host, connection_target, 'sudo mv {0} {1}'.format(\n            temp_remote_filename, remote_filename,\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload files from other servers using scp.", "response": "def download(\n    state, host, hostname, filename,\n    local_filename=None, force=False,\n    ssh_keyscan=False, ssh_user=None,\n):\n    '''\n    Download files from other servers using ``scp``.\n\n    + hostname: hostname to upload to\n    + filename: file to download\n    + local_filename: where to download the file to (defaults to ``filename``)\n    + force: always download the file, even if present locally\n    + ssh_keyscan: execute ``ssh.keyscan`` before uploading the file\n    + ssh_user: connect with this user\n    '''\n\n    local_filename = local_filename or filename\n\n    # Get local file info\n    local_file_info = host.fact.file(local_filename)\n\n    # Local file exists but isn't a file?\n    if local_file_info is False:\n        raise OperationError(\n            'Local destination {0} already exists and is not a file'.format(\n                local_filename,\n            ),\n        )\n\n    # If the local file exists and we're not forcing a re-download, no-op\n    if local_file_info and not force:\n        return\n\n    # Figure out where we're connecting (host or user@host)\n    connection_target = hostname\n    if ssh_user:\n        connection_target = '@'.join((ssh_user, hostname))\n\n    if ssh_keyscan:\n        yield keyscan(state, host, hostname)\n\n    # Download the file with scp\n    yield 'scp {0}:{1} {2}'.format(connection_target, filename, local_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npops and return operation global keyword arguments.", "response": "def pop_op_kwargs(state, kwargs):\n    '''\n    Pop and return operation global keyword arguments.\n    '''\n\n    meta_kwargs = state.deploy_kwargs or {}\n\n    def get_kwarg(key, default=None):\n        return kwargs.pop(key, meta_kwargs.get(key, default))\n\n    # Get the env for this host: config env followed by command-level env\n    env = state.config.ENV.copy()\n    env.update(get_kwarg('env', {}))\n\n    hosts = get_kwarg('hosts')\n    hosts = ensure_host_list(hosts, inventory=state.inventory)\n\n    # Filter out any hosts not in the meta kwargs (nested support)\n    if meta_kwargs.get('hosts') is not None:\n        hosts = [\n            host for host in hosts\n            if host in meta_kwargs['hosts']\n        ]\n\n    return {\n        # ENVars for commands in this operation\n        'env': env,\n        # Hosts to limit the op to\n        'hosts': hosts,\n        # When to limit the op (default always)\n        'when': get_kwarg('when', True),\n        # Locally & globally configurable\n        'sudo': get_kwarg('sudo', state.config.SUDO),\n        'sudo_user': get_kwarg('sudo_user', state.config.SUDO_USER),\n        'su_user': get_kwarg('su_user', state.config.SU_USER),\n        # Whether to preserve ENVars when sudoing (eg SSH forward agent socket)\n        'preserve_sudo_env': get_kwarg(\n            'preserve_sudo_env', state.config.PRESERVE_SUDO_ENV,\n        ),\n        # Ignore any errors during this operation\n        'ignore_errors': get_kwarg(\n            'ignore_errors', state.config.IGNORE_ERRORS,\n        ),\n        # Timeout on running the command\n        'timeout': get_kwarg('timeout'),\n        # Get a PTY before executing commands\n        'get_pty': get_kwarg('get_pty', False),\n        # Forces serial mode for this operation (--serial for one op)\n        'serial': get_kwarg('serial', False),\n        # Only runs this operation once\n        'run_once': get_kwarg('run_once', False),\n        # Execute in batches of X hosts rather than all at once\n        'parallel': get_kwarg('parallel'),\n        # Callbacks\n        'on_success': get_kwarg('on_success'),\n        'on_error': get_kwarg('on_error'),\n        # Operation hash\n        'op': get_kwarg('op'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unroll_generators(generator):\n    '''\n    Take a generator and unroll any sub-generators recursively. This is\n    essentially a Python 2 way of doing `yield from` in Python 3 (given\n    iterating the entire thing).\n    '''\n\n    # Ensure we have a generator (prevents ccommands returning lists)\n    if not isinstance(generator, GeneratorType):\n        raise TypeError('{0} is not a generator'.format(generator))\n\n    items = []\n\n    for item in generator:\n        if isinstance(item, GeneratorType):\n            items.extend(unroll_generators(item))\n        else:\n            items.append(item)\n\n    return items", "response": "Takes a generator and unroll any sub - generators recursively."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_template(filename_or_string, is_string=False):\n    '''\n    Gets a jinja2 ``Template`` object for the input filename or string, with caching\n    based on the filename of the template, or the SHA1 of the input string.\n    '''\n\n    # Cache against string sha or just the filename\n    cache_key = sha1_hash(filename_or_string) if is_string else filename_or_string\n\n    if cache_key in TEMPLATES:\n        return TEMPLATES[cache_key]\n\n    if is_string:\n        # Set the input string as our template\n        template_string = filename_or_string\n\n    else:\n        # Load template data into memory\n        with open(filename_or_string, 'r') as file_io:\n            template_string = file_io.read()\n\n    TEMPLATES[cache_key] = Template(template_string, keep_trailing_newline=True)\n    return TEMPLATES[cache_key]", "response": "Gets a jinja2 Template object for the input filename or string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms CamelCase to snake_case.", "response": "def underscore(name):\n    '''\n    Transform CamelCase -> snake_case.\n    '''\n\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sha1_hash(string):\n    '''\n    Return the SHA1 of the input string.\n    '''\n\n    hasher = sha1()\n    hasher.update(string.encode())\n    return hasher.hexdigest()", "response": "Returns the SHA1 of the input string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a shell command with various kwargs.", "response": "def make_command(\n    command,\n    env=None,\n    su_user=None,\n    sudo=False,\n    sudo_user=None,\n    preserve_sudo_env=False,\n):\n    '''\n    Builds a shell command with various kwargs.\n    '''\n\n    debug_meta = {}\n\n    for key, value in (\n        ('sudo', sudo),\n        ('sudo_user', sudo_user),\n        ('su_user', su_user),\n        ('env', env),\n    ):\n        if value:\n            debug_meta[key] = value\n\n    logger.debug('Building command ({0}): {1}'.format(' '.join(\n        '{0}: {1}'.format(key, value)\n        for key, value in six.iteritems(debug_meta)\n    ), command))\n\n    # Use env & build our actual command\n    if env:\n        env_string = ' '.join([\n            '{0}={1}'.format(key, value)\n            for key, value in six.iteritems(env)\n        ])\n        command = 'export {0}; {1}'.format(env_string, command)\n\n    # Quote the command as a string\n    command = shlex_quote(command)\n\n    # Switch user with su\n    if su_user:\n        command = 'su {0} -c {1}'.format(su_user, command)\n\n    # Otherwise just sh wrap the command\n    else:\n        command = 'sh -c {0}'.format(command)\n\n    # Use sudo (w/user?)\n    if sudo:\n        sudo_bits = ['sudo', '-H']\n\n        if preserve_sudo_env:\n            sudo_bits.append('-E')\n\n        if sudo_user:\n            sudo_bits.extend(('-u', sudo_user))\n\n        command = '{0} {1}'.format(' '.join(sudo_bits), command)\n\n    return command"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_arg_value(state, host, arg):\n    '''\n    Runs string arguments through the jinja2 templating system with a state and\n    host. Used to avoid string formatting in deploy operations which result in\n    one operation per host/variable. By parsing the commands after we generate\n    the ``op_hash``, multiple command variations can fall under one op.\n    '''\n\n    if isinstance(arg, six.string_types):\n        data = {\n            'host': host,\n            'inventory': state.inventory,\n        }\n\n        try:\n            return get_template(arg, is_string=True).render(data)\n        except (TemplateSyntaxError, UndefinedError) as e:\n            raise PyinfraError('Error in template string: {0}'.format(e))\n\n    elif isinstance(arg, list):\n        return [get_arg_value(state, host, value) for value in arg]\n\n    elif isinstance(arg, tuple):\n        return tuple(get_arg_value(state, host, value) for value in arg)\n\n    elif isinstance(arg, dict):\n        return {\n            key: get_arg_value(state, host, value)\n            for key, value in six.iteritems(arg)\n        }\n\n    return arg", "response": "Returns the value of the argument passed through the jinja2 templating system with a state and\nTaxonomy host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_hash(obj):\n    '''\n    Make a hash from an arbitrary nested dictionary, list, tuple or set, used to generate\n    ID's for operations based on their name & arguments.\n    '''\n\n    if isinstance(obj, (set, tuple, list)):\n        hash_string = ''.join([make_hash(e) for e in obj])\n\n    elif isinstance(obj, dict):\n        hash_string = ''.join(\n            ''.join((key, make_hash(value)))\n            for key, value in six.iteritems(obj)\n        )\n\n    else:\n        hash_string = (\n            # Constants - the values can change between hosts but we should still\n            # group them under the same operation hash.\n            '_PYINFRA_CONSTANT' if obj in (True, False, None)\n            # Plain strings\n            else obj if isinstance(obj, six.string_types)\n            # Objects with __name__s\n            else obj.__name__ if hasattr(obj, '__name__')\n            # Objects with names\n            else obj.name if hasattr(obj, 'name')\n            # Repr anything else\n            else repr(obj)\n        )\n\n    return sha1_hash(hash_string)", "response": "Make a hash from an arbitrary nested dictionary list tuple or set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the SHA1 of a file or file object using a buffer to handle larger files.", "response": "def get_file_sha1(filename_or_io):\n    '''\n    Calculates the SHA1 of a file or file object using a buffer to handle larger files.\n    '''\n\n    file_data = get_file_io(filename_or_io)\n    cache_key = file_data.cache_key\n\n    if cache_key and cache_key in FILE_SHAS:\n        return FILE_SHAS[cache_key]\n\n    with file_data as file_io:\n        hasher = sha1()\n        buff = file_io.read(BLOCKSIZE)\n\n        while len(buff) > 0:\n            if isinstance(buff, six.text_type):\n                buff = buff.encode('utf-8')\n\n            hasher.update(buff)\n            buff = file_io.read(BLOCKSIZE)\n\n    digest = hasher.hexdigest()\n\n    if cache_key:\n        FILE_SHAS[cache_key] = digest\n\n    return digest"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a file - like buffer object into lines and optionally prints the output.", "response": "def read_buffer(io, print_output=False, print_func=None):\n    '''\n    Reads a file-like buffer object into lines and optionally prints the output.\n    '''\n\n    # TODO: research this further - some steps towards handling stdin (ie password requests\n    # from programs that don't notice there's no TTY to accept passwords from!). This just\n    # prints output as below, but stores partial lines in a buffer, which could be printed\n    # when ready to accept input. Or detected and raise an error.\n\n    # GitHub issue: https://github.com/Fizzadar/pyinfra/issues/40\n\n    # buff = ''\n    # data = io.read(1)\n\n    # while data:\n    #     # Append to the buffer\n    #     buff += data\n\n    #     # Newlines in the buffer? Break them out\n    #     if '\\n' in buff:\n    #         lines = buff.split('\\n')\n\n    #         # Set the buffer back to just the last line\n    #         buff = lines[-1]\n\n    #         # Get the other lines, strip them\n    #         lines = [\n    #             line.strip()\n    #             for line in lines[:-1]\n    #         ]\n\n    #         out.extend(lines)\n\n    #         for line in lines:\n    #             _print(line)\n\n    #     # Get next data\n    #     data = io.read(1)\n\n    # if buff:\n    #     line = buff.strip()\n    #     out.append(line)\n    #     _print(line)\n\n    def _print(line):\n        if print_output:\n            if print_func:\n                formatted_line = print_func(line)\n            else:\n                formatted_line = line\n            encoded_line = unicode(formatted_line).encode('utf-8')\n            print(encoded_line)\n\n    out = []\n\n    for line in io:\n        # Handle local Popen shells returning list of bytes, not strings\n        if not isinstance(line, six.text_type):\n            line = line.decode('utf-8')\n\n        line = line.strip()\n        out.append(line)\n\n        _print(line)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef packages(state, host, packages=None, present=True, latest=False, directory=None):\n    '''\n    Install/remove/update npm packages.\n\n    + packages: list of packages to ensure\n    + present: whether the packages should be present\n    + latest: whether to upgrade packages without a specified version\n    + directory: directory to manage packages for, defaults to global\n\n    Versions:\n        Package versions can be pinned like npm: ``<pkg>@<version>``.\n    '''\n\n    current_packages = host.fact.npm_packages(directory)\n\n    install_command = (\n        'npm install -g'\n        if directory is None\n        else 'cd {0} && npm install'.format(directory)\n    )\n\n    uninstall_command = (\n        'npm uninstall -g'\n        if directory is None\n        else 'cd {0} && npm uninstall'.format(directory)\n    )\n\n    upgrade_command = (\n        'npm update -g'\n        if directory is None\n        else 'cd {0} && npm update'.format(directory)\n    )\n\n    yield ensure_packages(\n        packages, current_packages, present,\n        install_command=install_command,\n        uninstall_command=uninstall_command,\n        upgrade_command=upgrade_command,\n        version_join='@',\n        latest=latest,\n    )", "response": "A generator that returns a list of packages that can be installed uninstall and update."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(state, host, ctid, force=False):\n    '''\n    Start OpenVZ containers.\n\n    + ctid: CTID of the container to start\n    + force: whether to force container start\n    '''\n\n    args = ['{0}'.format(ctid)]\n\n    if force:\n        args.append('--force')\n\n    yield 'vzctl start {0}'.format(' '.join(args))", "response": "Start OpenVZ containers.\n\n    + ctid: CTID of the container to start\n    + force: whether to force container start"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops OpenVZ containers. + ctid: CTID of the container to stop", "response": "def stop(state, host, ctid):\n    '''\n    Stop OpenVZ containers.\n\n    + ctid: CTID of the container to stop\n    '''\n\n    args = ['{0}'.format(ctid)]\n\n    yield 'vzctl stop {0}'.format(' '.join(args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restart(state, host, ctid, force=False):\n    '''\n    Restart OpenVZ containers.\n\n    + ctid: CTID of the container to restart\n    + force: whether to force container start\n    '''\n\n    yield stop(state, host, ctid)\n    yield start(state, host, ctid, force=force)", "response": "Restart OpenVZ containers.\n\n    + ctid: CTID of the container to restart\n    + force: whether to force container start"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(state, host, ctid, template=None):\n    '''\n    Create OpenVZ containers.\n\n    + ctid: CTID of the container to create\n    '''\n\n    # Check we don't already have a container with this CTID\n    current_containers = host.fact.openvz_containers\n    if ctid in current_containers:\n        raise OperationError(\n            'An OpenVZ container with CTID {0} already exists'.format(ctid),\n        )\n\n    args = ['{0}'.format(ctid)]\n\n    if template:\n        args.append('--ostemplate {0}'.format(template))\n\n    yield 'vzctl create {0}'.format(' '.join(args))", "response": "Create OpenVZ containers.\n\n    + ctid: CTID of the container to create"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset OpenVZ container details.", "response": "def set(state, host, ctid, save=True, **settings):\n    '''\n    Set OpenVZ container details.\n\n    + ctid: CTID of the container to set\n    + save: whether to save the changes\n    + settings: settings/arguments to apply to the container\n\n    Settings/arguments:\n        these are mapped directly to ``vztctl`` arguments, eg\n        ``hostname='my-host.net'`` becomes ``--hostname my-host.net``.\n    '''\n\n    args = ['{0}'.format(ctid)]\n\n    if save:\n        args.append('--save')\n\n    for key, value in six.iteritems(settings):\n        # Handle list values (eg --nameserver X --nameserver X)\n        if isinstance(value, list):\n            args.extend('--{0} {1}'.format(key, v) for v in value)\n        else:\n            args.append('--{0} {1}'.format(key, value))\n\n    yield 'vzctl set {0}'.format(' '.join(args))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting a Python file and optionally return it s attributes as a dict.", "response": "def exec_file(filename, return_locals=False, is_deploy_code=False):\n    '''\n    Execute a Python file and optionally return it's attributes as a dict.\n    '''\n\n    if filename not in PYTHON_CODES:\n        with open(filename, 'r') as f:\n            code = f.read()\n\n        code = compile(code, filename, 'exec')\n        PYTHON_CODES[filename] = code\n\n    # Create some base attributes for our \"module\"\n    data = {\n        '__file__': filename,\n        'state': pseudo_state,\n    }\n\n    # Execute the code with locals/globals going into the dict above\n    exec(PYTHON_CODES[filename], data)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shell(state, host, commands, chdir=None):\n    '''\n    Run raw shell code.\n\n    + commands: command or list of commands to execute on the remote server\n    + chdir: directory to cd into before executing commands\n    '''\n\n    # Ensure we have a list\n    if isinstance(commands, six.string_types):\n        commands = [commands]\n\n    for command in commands:\n        if chdir:\n            yield 'cd {0} && ({1})'.format(chdir, command)\n        else:\n            yield command", "response": "Run raw shell code."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads and execute a local script on the remote host.", "response": "def script(state, host, filename, chdir=None):\n    '''\n    Upload and execute a local script on the remote host.\n\n    + filename: local script filename to upload & execute\n    + chdir: directory to cd into before executing the script\n    '''\n\n    temp_file = state.get_temp_filename(filename)\n    yield files.put(state, host, filename, temp_file)\n\n    yield chmod(temp_file, '+x')\n\n    if chdir:\n        yield 'cd {0} && {1}'.format(chdir, temp_file)\n    else:\n        yield temp_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef script_template(state, host, template_filename, chdir=None, **data):\n    '''\n    Generate, upload and execute a local script template on the remote host.\n\n    + template_filename: local script template filename\n    + chdir: directory to cd into before executing the script\n    '''\n\n    temp_file = state.get_temp_filename(template_filename)\n    yield files.template(state, host, template_filename, temp_file, **data)\n\n    yield chmod(temp_file, '+x')\n\n    if chdir:\n        yield 'cd {0} && {1}'.format(chdir, temp_file)\n    else:\n        yield temp_file", "response": "Generate upload and execute a local script template on the remote host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload or unload kernel modules.", "response": "def modprobe(state, host, name, present=True, force=False):\n    '''\n    Load/unload kernel modules.\n\n    + name: name of the module to manage\n    + present: whether the module should be loaded or not\n    + force: whether to force any add/remove modules\n    '''\n\n    modules = host.fact.kernel_modules\n    is_present = name in modules\n\n    args = ''\n    if force:\n        args = ' -f'\n\n    # Module is loaded and we don't want it?\n    if not present and is_present:\n        yield 'modprobe{0} -r {1}'.format(args, name)\n\n    # Module isn't loaded and we want it?\n    elif present and not is_present:\n        yield 'modprobe{0} {1}'.format(args, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hostname(state, host, hostname, hostname_file=None):\n    '''\n    Set the system hostname.\n\n    + hostname: the hostname that should be set\n    + hostname_file: the file that permanently sets the hostname\n\n    Hostname file:\n        By default pyinfra will auto detect this by targetting ``/etc/hostname``\n        on Linux and ``/etc/myname`` on OpenBSD.\n    '''\n\n    if hostname_file is None:\n        os = host.fact.os\n\n        if os == 'Linux':\n            hostname_file = '/etc/hostname'\n        elif os == 'OpenBSD':\n            hostname_file = '/etc/myname'\n\n    current_hostname = host.fact.hostname\n\n    if current_hostname != hostname:\n        yield 'hostname {0}'.format(hostname)\n\n    if hostname_file:\n        # Create a whole new hostname file\n        file = six.StringIO('{0}\\n'.format(hostname))\n\n        # And ensure it exists\n        yield files.put(\n            state, host,\n            file, hostname_file,\n        )", "response": "Set the system hostname."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sysctl(\n    state, host, name, value,\n    persist=False, persist_file='/etc/sysctl.conf',\n):\n    '''\n    Edit sysctl configuration.\n\n    + name: name of the sysctl setting to ensure\n    + value: the value or list of values the sysctl should be\n    + persist: whether to write this sysctl to the config\n    + persist_file: file to write the sysctl to persist on reboot\n    '''\n\n    string_value = (\n        ' '.join(value)\n        if isinstance(value, list)\n        else value\n    )\n\n    existing_value = host.fact.sysctl.get(name)\n    if not existing_value or existing_value != value:\n        yield 'sysctl {0}={1}'.format(name, string_value)\n\n    if persist:\n        yield files.line(\n            state, host,\n            persist_file,\n            '{0}[[:space:]]*=[[:space:]]*{1}'.format(name, string_value),\n            replace='{0} = {1}'.format(name, string_value),\n        )", "response": "Edit sysctl configuration.\n\n    + name: name of the sysctl setting to ensure\n    + value: the value or list of values the sysctl should be\n    + persist: whether to write this sysctl to the config\n    + persist_file: file to write the sysctl to persist on reboot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the crontab entry for the given command.", "response": "def crontab(\n    state, host, command, present=True, user=None,\n    minute='*', hour='*', month='*', day_of_week='*', day_of_month='*',\n):\n    '''\n    Add/remove/update crontab entries.\n\n    + command: the command for the cron\n    + present: whether this cron command should exist\n    + user: the user whose crontab to manage\n    + minute: which minutes to execute the cron\n    + hour: which hours to execute the cron\n    + month: which months to execute the cron\n    + day_of_week: which day of the week to execute the cron\n    + day_of_month: which day of the month to execute the cron\n\n    Cron commands:\n        The command is used to identify crontab entries - this means commands\n        must be unique within a given users crontab. If you require multiple\n        identical commands, prefix each with a nonce environment variable.\n    '''\n\n    crontab = host.fact.crontab(user)\n    exists = command in crontab\n\n    edit_commands = []\n    temp_filename = state.get_temp_filename()\n\n    new_crontab_line = '{minute} {hour} {day_of_month} {month} {day_of_week} {command}'.format(\n        command=command,\n        minute=minute,\n        hour=hour,\n        month=month,\n        day_of_week=day_of_week,\n        day_of_month=day_of_month,\n    )\n    existing_crontab_match = '.*{0}.*'.format(command)\n\n    # Don't want the cron and it does exist? Remove the line\n    if not present and exists:\n        edit_commands.append(sed_replace(\n            temp_filename, existing_crontab_match, '',\n        ))\n\n    # Want the cron but it doesn't exist? Append the line\n    elif present and not exists:\n        edit_commands.append('echo {0} >> {1}'.format(\n            shlex_quote(new_crontab_line), temp_filename,\n        ))\n\n    # We have the cron and it exists, do it's details? If not, replace the line\n    elif present and exists:\n        existing_details = crontab[command]\n\n        if any((\n            minute != existing_details['minute'],\n            hour != existing_details['hour'],\n            month != existing_details['month'],\n            day_of_week != existing_details['day_of_week'],\n            day_of_month != existing_details['day_of_month'],\n        )):\n            edit_commands.append(sed_replace(\n                temp_filename, existing_crontab_match, new_crontab_line,\n            ))\n\n    if edit_commands:\n        crontab_args = []\n        if user:\n            crontab_args.append('-u {0}'.format(user))\n\n        # List the crontab into a temporary file if it exists\n        if crontab:\n            yield 'crontab -l {0} > {1}'.format(' '.join(crontab_args), temp_filename)\n\n        # Now yield any edits\n        for command in edit_commands:\n            yield command\n\n        # Finally, use the tempfile to write a new crontab\n        yield 'crontab {0} {1}'.format(' '.join(crontab_args), temp_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef group(\n    state, host, name, present=True, system=False, gid=None,\n):\n    '''\n    Add/remove system groups.\n\n    + name: name of the group to ensure\n    + present: whether the group should be present or not\n    + system: whether to create a system group\n\n    System users:\n        System users don't exist on BSD, so the argument is ignored for BSD targets.\n    '''\n\n    groups = host.fact.groups or []\n    is_present = name in groups\n\n    # Group exists but we don't want them?\n    if not present and is_present:\n        yield 'groupdel {0}'.format(name)\n\n    # Group doesn't exist and we want it?\n    elif present and not is_present:\n        args = []\n\n        # BSD doesn't do system users\n        if system and 'BSD' not in host.fact.os:\n            args.append('-r')\n\n        args.append(name)\n\n        if gid:\n            args.append('--gid {0}'.format(gid))\n\n        yield 'groupadd {0}'.format(' '.join(args))", "response": "Add or remove a group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user(\n    state, host, name,\n    present=True, home=None, shell=None, group=None, groups=None,\n    public_keys=None, delete_keys=False, ensure_home=True,\n    system=False, uid=None,\n):\n    '''\n    Add/remove/update system users & their ssh `authorized_keys`.\n\n    + name: name of the user to ensure\n    + present: whether this user should exist\n    + home: the users home directory\n    + shell: the users shell\n    + group: the users primary group\n    + groups: the users secondary groups\n    + public_keys: list of public keys to attach to this user, ``home`` must be specified\n    + delete_keys: whether to remove any keys not specified in ``public_keys``\n    + ensure_home: whether to ensure the ``home`` directory exists\n    + system: whether to create a system account\n\n    Home directory:\n        When ``ensure_home`` or ``public_keys`` are provided, ``home`` defaults to\n        ``/home/{name}``.\n    '''\n\n    users = host.fact.users or {}\n    user = users.get(name)\n\n    if groups is None:\n        groups = []\n\n    if home is None:\n        home = '/home/{0}'.format(name)\n\n    # User not wanted?\n    if not present:\n        if user:\n            yield 'userdel {0}'.format(name)\n        return\n\n    # User doesn't exist but we want them?\n    if present and user is None:\n        # Create the user w/home/shell\n        args = []\n\n        if home:\n            args.append('-d {0}'.format(home))\n\n        if shell:\n            args.append('-s {0}'.format(shell))\n\n        if group:\n            args.append('-g {0}'.format(group))\n\n        if groups:\n            args.append('-G {0}'.format(','.join(groups)))\n\n        if system and host.fact.os not in ('OpenBSD', 'NetBSD'):\n            args.append('-r')\n\n        if uid:\n            args.append('--uid {0}'.format(uid))\n\n        yield 'useradd {0} {1}'.format(' '.join(args), name)\n\n    # User exists and we want them, check home/shell/keys\n    else:\n        args = []\n\n        # Check homedir\n        if home and user['home'] != home:\n            args.append('-d {0}'.format(home))\n\n        # Check shell\n        if shell and user['shell'] != shell:\n            args.append('-s {0}'.format(shell))\n\n        # Check primary group\n        if group and user['group'] != group:\n            args.append('-g {0}'.format(group))\n\n        # Check secondary groups, if defined\n        if groups and set(user['groups']) != set(groups):\n            args.append('-G {0}'.format(','.join(groups)))\n\n        # Need to mod the user?\n        if args:\n            yield 'usermod {0} {1}'.format(' '.join(args), name)\n\n    # Ensure home directory ownership\n    if ensure_home:\n        yield files.directory(\n            state, host, home,\n            user=name, group=name,\n        )\n\n    # Add SSH keys\n    if public_keys is not None:\n        # Ensure .ssh directory\n        # note that this always outputs commands unless the SSH user has access to the\n        # authorized_keys file, ie the SSH user is the user defined in this function\n        yield files.directory(\n            state, host,\n            '{0}/.ssh'.format(home),\n            user=name, group=name,\n            mode=700,\n        )\n\n        filename = '{0}/.ssh/authorized_keys'.format(home)\n\n        if delete_keys:\n            # Create a whole new authorized_keys file\n            keys_file = six.StringIO('{0}\\n'.format(\n                '\\n'.join(public_keys),\n            ))\n\n            # And ensure it exists\n            yield files.put(\n                state, host,\n                keys_file, filename,\n                user=name, group=name,\n                mode=600,\n            )\n\n        else:\n            # Ensure authorized_keys exists\n            yield files.file(\n                state, host, filename,\n                user=name, group=name,\n                mode=600,\n            )\n\n            # And every public key is present\n            for key in public_keys:\n                yield files.line(\n                    state, host,\n                    filename, key,\n                )", "response": "Add or remove a user to the system"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(\n    state, host, source_url, destination,\n    user=None, group=None, mode=None, cache_time=None, force=False,\n):\n    '''\n    Download files from remote locations.\n\n    + source_url: source URl of the file\n    + destination: where to save the file\n    + user: user to own the files\n    + group: group to own the files\n    + mode: permissions of the files\n    + cache_time: if the file exists already, re-download after this time (in s)\n    + force: always download the file, even if it already exists\n    '''\n\n    # Get destination info\n    info = host.fact.file(destination)\n\n    # Destination is a directory?\n    if info is False:\n        raise OperationError(\n            'Destination {0} already exists and is not a file'.format(destination),\n        )\n\n    # Do we download the file? Force by default\n    download = force\n\n    # Doesn't exist, lets download it\n    if info is None:\n        download = True\n\n    # Destination file exists & cache_time: check when the file was last modified,\n    # download if old\n    elif cache_time:\n        # Time on files is not tz-aware, and will be the same tz as the server's time,\n        # so we can safely remove the tzinfo from host.fact.date before comparison.\n        cache_time = host.fact.date.replace(tzinfo=None) - timedelta(seconds=cache_time)\n        if info['mtime'] and info['mtime'] > cache_time:\n            download = True\n\n    # If we download, always do user/group/mode as SSH user may be different\n    if download:\n        yield 'wget -q {0} -O {1}'.format(source_url, destination)\n\n        if user or group:\n            yield chown(destination, user, group)\n\n        if mode:\n            yield chmod(destination, mode)", "response": "Download files from remote locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef line(state, host, name, line, present=True, replace=None, flags=None):\n    '''\n    Ensure lines in files using grep to locate and sed to replace.\n\n    + name: target remote file to edit\n    + line: string or regex matching the target line\n    + present: whether the line should be in the file\n    + replace: text to replace entire matching lines when ``present=True``\n    + flags: list of flags to pass to sed when replacing/deleting\n\n    Regex line matching:\n        Unless line matches a line (starts with ^, ends $), pyinfra will wrap it such that\n        it does, like: ``^.*LINE.*$``. This means we don't swap parts of lines out. To\n        change bits of lines, see ``files.replace``.\n\n    Regex line escaping:\n        If matching special characters (eg a crontab line containing *), remember to escape\n        it first using Python's ``re.escape``.\n    '''\n\n    match_line = line\n\n    # Ensure we're matching a whole ^line$\n    if not match_line.startswith('^'):\n        match_line = '^.*{0}'.format(match_line)\n\n    if not match_line.endswith('$'):\n        match_line = '{0}.*$'.format(match_line)\n\n    # Is there a matching line in this file?\n    present_lines = host.fact.find_in_file(name, match_line)\n\n    # If replace present, use that over the matching line\n    if replace:\n        line = replace\n    # We must provide some kind of replace to sed_replace_command below\n    else:\n        replace = ''\n\n    # Save commands for re-use in dynamic script when file not present at fact stage\n    echo_command = 'echo \"{0}\" >> {1}'.format(line, name)\n    sed_replace_command = sed_replace(\n        name, match_line, replace,\n        flags=flags,\n    )\n\n    # No line and we want it, append it\n    if not present_lines and present:\n        # If the file does not exist - it *might* be created, so we handle it\n        # dynamically with a little script.\n        if present_lines is None:\n            yield '''\n                # If the file now exists\n                if [ -f \"{target}\" ]; then\n                    # Grep for the line, sed if matches\n                    (grep \"{match_line}\" \"{target}\" && {sed_replace_command}) || \\\n                    # Else echo\n                    {echo_command}\n\n                # No file, just echo\n                else\n                    {echo_command}\n                fi\n            '''.format(\n                target=name,\n                match_line=match_line,\n                echo_command=echo_command,\n                sed_replace_command=sed_replace_command,\n            )\n\n        # Otherwise the file exists and there is no matching line, so append it\n        else:\n            yield echo_command\n\n    # Line(s) exists and we want to remove them, replace with nothing\n    elif present_lines and not present:\n        yield sed_replace(name, match_line, '', flags=flags)\n\n    # Line(s) exists and we have want to ensure they're correct\n    elif present_lines and present:\n        # If any of lines are different, sed replace them\n        if replace and any(line != replace for line in present_lines):\n            yield sed_replace_command", "response": "Add a line to the file and return the keybase object that can be used to edit the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuploads a file to the remote system.", "response": "def put(\n    state, host, local_filename, remote_filename,\n    user=None, group=None, mode=None, add_deploy_dir=True,\n):\n    '''\n    Copy a local file to the remote system.\n\n    + local_filename: local filename\n    + remote_filename: remote filename\n    + user: user to own the files\n    + group: group to own the files\n    + mode: permissions of the files\n    '''\n\n    # Upload IO objects as-is\n    if hasattr(local_filename, 'read'):\n        local_file = local_filename\n\n    # Assume string filename\n    else:\n        # Add deploy directory?\n        if add_deploy_dir and state.deploy_dir:\n            local_filename = path.join(state.deploy_dir, local_filename)\n\n        local_file = local_filename\n\n        if not path.isfile(local_file):\n            raise IOError('No such file: {0}'.format(local_file))\n\n    mode = ensure_mode_int(mode)\n    remote_file = host.fact.file(remote_filename)\n\n    # No remote file, always upload and user/group/mode if supplied\n    if not remote_file:\n        yield (local_file, remote_filename)\n\n        if user or group:\n            yield chown(remote_filename, user, group)\n\n        if mode:\n            yield chmod(remote_filename, mode)\n\n    # File exists, check sum and check user/group/mode if supplied\n    else:\n        local_sum = get_file_sha1(local_filename)\n        remote_sum = host.fact.sha1_file(remote_filename)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield (local_file, remote_filename)\n\n            if user or group:\n                yield chown(remote_filename, user, group)\n\n            if mode:\n                yield chmod(remote_filename, mode)\n\n        else:\n            # Check mode\n            if mode and remote_file['mode'] != mode:\n                yield chmod(remote_filename, mode)\n\n            # Check user/group\n            if (\n                (user and remote_file['user'] != user)\n                or (group and remote_file['group'] != group)\n            ):\n                yield chown(remote_filename, user, group)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef template(\n    state, host, template_filename, remote_filename,\n    user=None, group=None, mode=None, **data\n):\n    '''\n    Generate a template and write it to the remote system.\n\n    + template_filename: local template filename\n    + remote_filename: remote filename\n    + user: user to own the files\n    + group: group to own the files\n    + mode: permissions of the files\n    '''\n\n    if state.deploy_dir:\n        template_filename = path.join(state.deploy_dir, template_filename)\n\n    # Ensure host is always available inside templates\n    data['host'] = host\n    data['inventory'] = state.inventory\n\n    # Render and make file-like it's output\n    try:\n        output = get_template(template_filename).render(data)\n    except (TemplateSyntaxError, UndefinedError) as e:\n        _, _, trace = sys.exc_info()\n\n        # Jump through to the *second last* traceback, which contains the line number\n        # of the error within the in-memory Template object\n        while trace.tb_next:\n            if trace.tb_next.tb_next:\n                trace = trace.tb_next\n            else:\n                break\n\n        line_number = trace.tb_frame.f_lineno\n\n        # Quickly read the line in question and one above/below for nicer debugging\n        template_lines = open(template_filename, 'r').readlines()\n        template_lines = [line.strip() for line in template_lines]\n        relevant_lines = template_lines[max(line_number - 2, 0):line_number + 1]\n\n        raise OperationError('Error in template: {0} (L{1}): {2}\\n...\\n{3}\\n...'.format(\n            template_filename, line_number, e, '\\n'.join(relevant_lines),\n        ))\n\n    output_file = six.StringIO(output)\n    # Set the template attribute for nicer debugging\n    output_file.template = template_filename\n\n    # Pass to the put function\n    yield put(\n        state, host,\n        output_file, remote_filename,\n        user=user, group=group, mode=mode,\n        add_deploy_dir=False,\n    )", "response": "Generate a template and write it to the remote system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef link(\n    state, host, name,\n    target=None, present=True, user=None, group=None, symbolic=True,\n):\n    '''\n    Add/remove/update links.\n\n    + name: the name of the link\n    + target: the file/directory the link points to\n    + present: whether the link should exist\n    + user: user to own the link\n    + group: group to own the link\n    + symbolic: whether to make a symbolic link (vs hard link)\n\n    Source changes:\n        If the link exists and points to a different target, pyinfra will remove it and\n        recreate a new one pointing to then new target.\n    '''\n\n    if present and not target:\n        raise OperationError('If present is True target must be provided')\n\n    info = host.fact.link(name)\n\n    # Not a link?\n    if info is False:\n        raise OperationError('{0} exists and is not a link'.format(name))\n\n    add_cmd = 'ln{0} {1} {2}'.format(\n        ' -s' if symbolic else '',\n        target, name,\n    )\n\n    remove_cmd = 'rm -f {0}'.format(name)\n\n    # No link and we want it\n    if info is None and present:\n        yield add_cmd\n        if user or group:\n            yield chown(name, user, group, dereference=False)\n\n    # It exists and we don't want it\n    elif info and not present:\n        yield remove_cmd\n\n    # Exists and want to ensure it's state\n    elif info and present:\n        # If we have an absolute name - prepend to any non-absolute values from the fact\n        # and/or the soruce.\n        if path.isabs(name):\n            link_dirname = path.dirname(name)\n\n            if not path.isabs(target):\n                target = path.normpath('/'.join((link_dirname, target)))\n\n            if not path.isabs(info['link_target']):\n                info['link_target'] = path.normpath(\n                    '/'.join((link_dirname, info['link_target'])),\n                )\n\n        # If the target is wrong, remove & recreate the link\n        if info['link_target'] != target:\n            yield remove_cmd\n            yield add_cmd\n\n        # Check user/group\n        if (user and info['user'] != user) or (group and info['group'] != group):\n            yield chown(name, user, group, dereference=False)", "response": "Link a file or directory into a new one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify that a module - level variable is a valid inventory group.", "response": "def _is_inventory_group(key, value):\n    '''\n    Verify that a module-level variable (key = value) is a valid inventory group.\n    '''\n\n    if (\n        key.startswith('_')\n        or not isinstance(value, (list, tuple, GeneratorType))\n    ):\n        return False\n\n    # If the group is a tuple of (hosts, data), check the hosts\n    if isinstance(value, tuple):\n        value = value[0]\n\n    # Expand any generators of hosts\n    if isinstance(value, GeneratorType):\n        value = list(value)\n\n    return all(\n        isinstance(item, ALLOWED_HOST_TYPES)\n        for item in value\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a pyinfra. api. Inventory from the given file.", "response": "def make_inventory(\n    inventory_filename,\n    deploy_dir=None,\n    ssh_port=None,\n    ssh_user=None,\n    ssh_key=None,\n    ssh_key_password=None,\n    ssh_password=None,\n):\n    '''\n    Builds a ``pyinfra.api.Inventory`` from the filesystem. If the file does not exist\n    and doesn't contain a / attempts to use that as the only hostname.\n    '''\n\n    if ssh_port is not None:\n        ssh_port = int(ssh_port)\n\n    file_groupname = None\n\n    # If we're not a valid file we assume a list of comma separated hostnames\n    if not path.exists(inventory_filename):\n        groups = {\n            'all': inventory_filename.split(','),\n        }\n    else:\n        groups = _get_groups_from_filename(inventory_filename)\n        # Used to set all the hosts to an additional group - that of the filename\n        # ie inventories/dev.py means all the hosts are in the dev group, if not present\n        file_groupname = path.basename(inventory_filename).rsplit('.')[0]\n\n    all_data = {}\n\n    if 'all' in groups:\n        all_hosts = groups.pop('all')\n\n        if isinstance(all_hosts, tuple):\n            all_hosts, all_data = all_hosts\n\n    # Build all out of the existing hosts if not defined\n    else:\n        all_hosts = []\n        for hosts in groups.values():\n            # Groups can be a list of hosts or tuple of (hosts, data)\n            hosts = hosts[0] if isinstance(hosts, tuple) else hosts\n\n            for host in hosts:\n                # Hosts can be a hostname or tuple of (hostname, data)\n                hostname = host[0] if isinstance(host, tuple) else host\n\n                if hostname not in all_hosts:\n                    all_hosts.append(hostname)\n\n    groups['all'] = (all_hosts, all_data)\n\n    # Apply the filename group if not already defined\n    if file_groupname and file_groupname not in groups:\n        groups[file_groupname] = all_hosts\n\n    # In pyinfra an inventory is a combination of (hostnames + data). However, in CLI\n    # mode we want to be define this in separate files (inventory / group data). The\n    # issue is we want inventory access within the group data files - but at this point\n    # we're not ready to make an Inventory. So here we just create a fake one, and\n    # attach it to pseudo_inventory while we import the data files.\n    logger.debug('Creating fake inventory...')\n\n    fake_groups = {\n        # In API mode groups *must* be tuples of (hostnames, data)\n        name: group if isinstance(group, tuple) else (group, {})\n        for name, group in six.iteritems(groups)\n    }\n    fake_inventory = Inventory((all_hosts, all_data), **fake_groups)\n    pseudo_inventory.set(fake_inventory)\n\n    # Get all group data (group_data/*.py)\n    group_data = _get_group_data(deploy_dir)\n\n    # Reset the pseudo inventory\n    pseudo_inventory.reset()\n\n    # For each group load up any data\n    for name, hosts in six.iteritems(groups):\n        data = {}\n\n        if isinstance(hosts, tuple):\n            hosts, data = hosts\n\n        if name in group_data:\n            data.update(group_data.pop(name))\n\n        # Attach to group object\n        groups[name] = (hosts, data)\n\n    # Loop back through any leftover group data and create an empty (for now)\n    # group - this is because inventory @connectors can attach arbitrary groups\n    # to hosts, so we need to support that.\n    for name, data in six.iteritems(group_data):\n        groups[name] = ([], data)\n\n    return Inventory(\n        groups.pop('all'),\n        ssh_user=ssh_user,\n        ssh_key=ssh_key,\n        ssh_key_password=ssh_key_password,\n        ssh_port=ssh_port,\n        ssh_password=ssh_password,\n        **groups\n    ), file_groupname and file_groupname.lower()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting arbitrary SQL against PostgreSQL.", "response": "def sql(\n    state, host, sql,\n    database=None,\n    # Details for speaking to PostgreSQL via `psql` CLI\n    postgresql_user=None, postgresql_password=None,\n    postgresql_host=None, postgresql_port=None,\n):\n    '''\n    Execute arbitrary SQL against PostgreSQL.\n\n    + sql: SQL command(s) to execute\n    + database: optional database to execute against\n    + postgresql_*: global module arguments, see above\n    '''\n\n    yield make_execute_psql_command(\n        sql,\n        database=database,\n        user=postgresql_user,\n        password=postgresql_password,\n        host=postgresql_host,\n        port=postgresql_port,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate or update a PostgreSQL role.", "response": "def role(\n    state, host, name,\n    present=True,\n    password=None, login=True, superuser=False, inherit=False,\n    createdb=False, createrole=False, replication=False, connection_limit=None,\n    # Details for speaking to PostgreSQL via `psql` CLI\n    postgresql_user=None, postgresql_password=None,\n    postgresql_host=None, postgresql_port=None,\n):\n    '''\n    Add/remove PostgreSQL roles.\n\n    + name: name of the role\n    + present: whether the role should be present or absent\n    + login: whether the role can login\n    + superuser: whether role will be a superuser\n    + inherit: whether the role inherits from other roles\n    + createdb: whether the role is allowed to create databases\n    + createrole: whether the role is allowed to create new roles\n    + replication: whether this role is allowed to replicate\n    + connection_limit: the connection limit for the role\n    + postgresql_*: global module arguments, see above\n\n    Updates:\n        pyinfra will not attempt to change existing roles - it will either\n        create or drop roles, but not alter them (if the role exists this\n        operation will make no changes).\n    '''\n\n    roles = host.fact.postgresql_roles(\n        postgresql_user, postgresql_password,\n        postgresql_host, postgresql_port,\n    )\n\n    is_present = name in roles\n\n    # User not wanted?\n    if not present:\n        if is_present:\n            yield make_execute_psql_command(\n                'DROP ROLE {0}'.format(name),\n                user=postgresql_user,\n                password=postgresql_password,\n                host=postgresql_host,\n                port=postgresql_port,\n            )\n        return\n\n    # If we want the user and they don't exist\n    if not is_present:\n        sql_bits = ['CREATE ROLE {0}'.format(name)]\n\n        for key, value in (\n            ('LOGIN', login),\n            ('SUPERUSER', superuser),\n            ('INHERIT', inherit),\n            ('CREATEDB', createdb),\n            ('CREATEROLE', createrole),\n            ('REPLICATION', replication),\n        ):\n            if value:\n                sql_bits.append(key)\n\n        if connection_limit:\n            sql_bits.append('CONNECTION LIMIT {0}'.format(connection_limit))\n\n        if password:\n            sql_bits.append(\"PASSWORD '{0}'\".format(password))\n\n        yield make_execute_psql_command(\n            ' '.join(sql_bits),\n            user=postgresql_user,\n            password=postgresql_password,\n            host=postgresql_host,\n            port=postgresql_port,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps a PostgreSQL database into a. sql file.", "response": "def dump(\n    state, host,\n    remote_filename, database=None,\n    # Details for speaking to PostgreSQL via `psql` CLI\n    postgresql_user=None, postgresql_password=None,\n    postgresql_host=None, postgresql_port=None,\n):\n    '''\n    Dump a PostgreSQL database into a ``.sql`` file. Requires ``mysqldump``.\n\n    + database: name of the database to dump\n    + remote_filename: name of the file to dump the SQL to\n    + postgresql_*: global module arguments, see above\n    '''\n\n    yield '{0} > {1}'.format(make_psql_command(\n        executable='pg_dump',\n        database=database,\n        user=postgresql_user,\n        password=postgresql_password,\n        host=postgresql_host,\n        port=postgresql_port,\n    ), remote_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a single fact for all hosts in the state.", "response": "def get_facts(state, name, args=None, ensure_hosts=None):\n    '''\n    Get a single fact for all hosts in the state.\n    '''\n\n    # Create an instance of the fact\n    fact = FACTS[name]()\n\n    if isinstance(fact, ShortFactBase):\n        return get_short_facts(state, fact, args=args, ensure_hosts=ensure_hosts)\n\n    logger.debug('Getting fact: {0} (ensure_hosts: {1})'.format(\n        name, ensure_hosts,\n    ))\n\n    args = args or []\n\n    # Apply args or defaults\n    sudo = state.config.SUDO\n    sudo_user = state.config.SUDO_USER\n    su_user = state.config.SU_USER\n    ignore_errors = state.config.IGNORE_ERRORS\n\n    # Timeout for operations !== timeout for connect (config.CONNECT_TIMEOUT)\n    timeout = None\n\n    # Get the current op meta\n    current_op_hash = state.current_op_hash\n    current_op_meta = state.op_meta.get(current_op_hash)\n\n    # If inside an operation, fetch config meta\n    if current_op_meta:\n        sudo = current_op_meta['sudo']\n        sudo_user = current_op_meta['sudo_user']\n        su_user = current_op_meta['su_user']\n        ignore_errors = current_op_meta['ignore_errors']\n        timeout = current_op_meta['timeout']\n\n    # Make a hash which keeps facts unique - but usable cross-deploy/threads.\n    # Locks are used to maintain order.\n    fact_hash = make_hash((name, args, sudo, sudo_user, su_user, ignore_errors))\n\n    # Already got this fact? Unlock and return 'em\n    current_facts = state.facts.get(fact_hash, {})\n    if current_facts:\n        if not ensure_hosts or all(\n            host in current_facts for host in ensure_hosts\n        ):\n            return current_facts\n\n    with FACT_LOCK:\n        # Add any hosts we must have, whether considered in the inventory or not\n        # (these hosts might be outside the --limit or current op limit_hosts).\n        hosts = set(state.inventory)\n        if ensure_hosts:\n            hosts.update(ensure_hosts)\n\n        # Execute the command for each state inventory in a greenlet\n        greenlet_to_host = {}\n\n        for host in hosts:\n            if host in current_facts:\n                continue\n\n            # if host in state.ready_hosts:\n            #     continue\n\n            # Work out the command\n            command = fact.command\n\n            if ismethod(command) or isfunction(command):\n                # Generate actual arguments by pasing strings as jinja2 templates\n                host_args = [get_arg_value(state, host, arg) for arg in args]\n\n                command = command(*host_args)\n\n            greenlet = state.fact_pool.spawn(\n                host.run_shell_command, state, command,\n                sudo=sudo, sudo_user=sudo_user,\n                su_user=su_user, timeout=timeout,\n                print_output=state.print_fact_output,\n            )\n            greenlet_to_host[greenlet] = host\n\n        # Wait for all the commands to execute\n        progress_prefix = 'fact: {0}'.format(name)\n        if args:\n            progress_prefix = '{0}{1}'.format(progress_prefix, args)\n\n        with progress_spinner(\n            greenlet_to_host.values(),\n            prefix_message=progress_prefix,\n        ) as progress:\n            for greenlet in gevent.iwait(greenlet_to_host.keys()):\n                host = greenlet_to_host[greenlet]\n                progress(host)\n\n        hostname_facts = {}\n        failed_hosts = set()\n\n        # Collect the facts and any failures\n        for greenlet, host in six.iteritems(greenlet_to_host):\n            status = False\n            stdout = []\n\n            try:\n                status, stdout, _ = greenlet.get()\n\n            except (timeout_error, socket_error, SSHException) as e:\n                failed_hosts.add(host)\n                log_host_command_error(\n                    host, e,\n                    timeout=timeout,\n                )\n\n            data = fact.default()\n\n            if status and stdout:\n                data = fact.process(stdout)\n\n            hostname_facts[host] = data\n\n        log_name = click.style(name, bold=True)\n\n        filtered_args = list(filter(None, args))\n        if filtered_args:\n            log = 'Loaded fact {0}: {1}'.format(log_name, tuple(filtered_args))\n        else:\n            log = 'Loaded fact {0}'.format(log_name)\n\n        if state.print_fact_info:\n            logger.info(log)\n        else:\n            logger.debug(log)\n\n        # Check we've not failed\n        if not ignore_errors:\n            state.fail_hosts(failed_hosts)\n\n        # Assign the facts\n        state.facts.setdefault(fact_hash, {}).update(hostname_facts)\n\n    return state.facts[fact_hash]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap around get_facts returning facts for one host or a function ArcGIS function that does.", "response": "def get_fact(state, host, name):\n    '''\n    Wrapper around ``get_facts`` returning facts for one host or a function\n    that does.\n    '''\n\n    # Expecting a function to return\n    if callable(getattr(FACTS[name], 'command', None)):\n        def wrapper(*args):\n            fact_data = get_facts(state, name, args=args, ensure_hosts=(host,))\n\n            return fact_data.get(host)\n        return wrapper\n\n    # Expecting the fact as a return value\n    else:\n        # Get the fact\n        fact_data = get_facts(state, name, ensure_hosts=(host,))\n\n        return fact_data.get(host)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect_all(state):\n    '''\n    Connect to all the configured servers in parallel. Reads/writes state.inventory.\n\n    Args:\n        state (``pyinfra.api.State`` obj): the state containing an inventory to connect to\n    '''\n\n    hosts = [\n        host for host in state.inventory\n        if state.is_host_in_limit(host)\n    ]\n\n    greenlet_to_host = {\n        state.pool.spawn(host.connect, state): host\n        for host in hosts\n    }\n\n    with progress_spinner(greenlet_to_host.values()) as progress:\n        for greenlet in gevent.iwait(greenlet_to_host.keys()):\n            host = greenlet_to_host[greenlet]\n            progress(host)\n\n    # Get/set the results\n    failed_hosts = set()\n\n    for greenlet, host in six.iteritems(greenlet_to_host):\n        # Raise any unexpected exception\n        greenlet.get()\n\n        if host.connection:\n            state.activate_host(host)\n        else:\n            failed_hosts.add(host)\n\n    # Remove those that failed, triggering FAIL_PERCENT check\n    state.fail_hosts(failed_hosts, activated_count=len(hosts))", "response": "Connects to all the configured servers in parallel. Reads and writes state. inventory.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a callback on the given host.", "response": "def execute(state, host, callback, *args, **kwargs):\n    '''\n    [DEPRECATED], please use ``python.call``.\n    '''\n\n    # COMPAT w/ <0.4\n    # TODO: remove this function\n\n    logger.warning((\n        'Use of `python.execute` is deprecated, '\n        'please use `python.call` instead.'\n    ))\n\n    # Pre pyinfra 0.4 the operation execution passed (state, host, host.name)\n    # as args, so here we replicate that - hence ``python.call`` which replaces\n    # this function going forward.\n    args = (host.name,) + args\n\n    yield (callback, args, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef key(state, host, key=None, keyserver=None, keyid=None):\n    '''\n    Add apt gpg keys with ``apt-key``.\n\n    + key: filename or URL\n    + keyserver: URL of keyserver to fetch key from\n    + keyid: key identifier when using keyserver\n\n    Note:\n        Always returns an add command, not state checking.\n\n    keyserver/id:\n        These must be provided together.\n    '''\n\n    if key:\n        # If URL, wget the key to stdout and pipe into apt-key, because the \"adv\"\n        # apt-key passes to gpg which doesn't always support https!\n        if urlparse(key).scheme:\n            yield 'wget -O- {0} | apt-key add -'.format(key)\n        else:\n            yield 'apt-key add {0}'.format(key)\n\n    if keyserver and keyid:\n        yield 'apt-key adv --keyserver {0} --recv-keys {1}'.format(keyserver, keyid)", "response": "Add apt gpg keys to apt - key list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef repo(state, host, name, present=True, filename=None):\n    '''\n    Add/remove apt repositories.\n\n    + name: apt source string eg ``deb http://X hardy main``\n    + present: whether the repo should exist on the system\n    + filename: optional filename to use ``/etc/apt/sources.list.d/<filename>.list``. By\n      default uses ``/etc/apt/sources.list``.\n    '''\n\n    # Get the target .list file to manage\n    if filename:\n        filename = '/etc/apt/sources.list.d/{0}.list'.format(filename)\n    else:\n        filename = '/etc/apt/sources.list'\n\n    # Work out if the repo exists already\n    apt_sources = host.fact.apt_sources\n\n    is_present = False\n    repo = parse_apt_repo(name)\n    if repo and repo in apt_sources:\n        is_present = True\n\n    # Doesn't exist and we want it\n    if not is_present and present:\n        yield files.line(\n            state, host, filename, name,\n        )\n\n    # Exists and we don't want it\n    if is_present and not present:\n        yield files.line(\n            state, host, filename, name,\n            present=False,\n        )", "response": "Add/remove apt repositories.\n\n    + name: apt source string eg ``deb http://X hardy main``\n    + present: whether the repo should exist on the system\n    + filename: optional filename to use ``/etc/apt/sources.list.d/<filename>.list``. By\n      default uses ``/etc/apt/sources.list``."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ppa(state, host, name, present=True):\n    '''\n    Add/remove Ubuntu ppa repositories.\n\n    + name: the PPA name (full ppa:user/repo format)\n    + present: whether it should exist\n\n    Note:\n        requires ``apt-add-repository`` on the remote host\n    '''\n\n    if present:\n        yield 'apt-add-repository -y \"{0}\"'.format(name)\n\n    if not present:\n        yield 'apt-add-repository -y --remove \"{0}\"'.format(name)", "response": "Add or remove Ubuntu ppa repositories."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload and install a. deb file and install it if it doesn t exist.", "response": "def deb(state, host, source, present=True, force=False):\n    '''\n    Add/remove ``.deb`` file packages.\n\n    + source: filename or URL of the ``.deb`` file\n    + present: whether or not the package should exist on the system\n    + force: whether to force the package install by passing `--force-yes` to apt\n\n    Note:\n        When installing, ``apt-get install -f`` will be run to install any unmet\n        dependencies.\n\n    URL sources with ``present=False``:\n        If the ``.deb`` file isn't downloaded, pyinfra can't remove any existing\n        package as the file won't exist until mid-deploy.\n    '''\n\n    # If source is a url\n    if urlparse(source).scheme:\n        # Generate a temp filename\n        temp_filename = state.get_temp_filename(source)\n\n        # Ensure it's downloaded\n        yield files.download(state, host, source, temp_filename)\n\n        # Override the source with the downloaded file\n        source = temp_filename\n\n    # Check for file .deb information (if file is present)\n    info = host.fact.deb_package(source)\n\n    exists = False\n\n    # We have deb info! Check against installed packages\n    if info:\n        current_packages = host.fact.deb_packages\n\n        if (\n            info['name'] in current_packages\n            and info.get('version') in current_packages[info['name']]\n        ):\n            exists = True\n\n    # Package does not exist and we want?\n    if present and not exists:\n        # Install .deb file - ignoring failure (on unmet dependencies)\n        yield 'dpkg --force-confdef --force-confold -i {0} || true'.format(source)\n        # Attempt to install any missing dependencies\n        yield '{0} -f'.format(noninteractive_apt('install', force=force))\n        # Now reinstall, and critically configure, the package - if there are still\n        # missing deps, now we error\n        yield 'dpkg --force-confdef --force-confold -i {0}'.format(source)\n\n    # Package exists but we don't want?\n    if exists and not present:\n        yield '{0} {1}'.format(\n            noninteractive_apt('remove', force=force),\n            info['name'],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(state, host, cache_time=None, touch_periodic=False):\n    '''\n    Updates apt repos.\n\n    + cache_time: cache updates for this many seconds\n    + touch_periodic: touch ``/var/lib/apt/periodic/update-success-stamp`` after update\n    '''\n\n    # If cache_time check when apt was last updated, prevent updates if within time\n    if cache_time:\n        # Ubuntu provides this handy file\n        cache_info = host.fact.file(APT_UPDATE_FILENAME)\n\n        # Time on files is not tz-aware, and will be the same tz as the server's time,\n        # so we can safely remove the tzinfo from host.fact.date before comparison.\n        host_cache_time = host.fact.date.replace(tzinfo=None) - timedelta(seconds=cache_time)\n        if cache_info and cache_info['mtime'] and cache_info['mtime'] > host_cache_time:\n            return\n\n    yield 'apt-get update'\n\n    # Some apt systems (Debian) have the /var/lib/apt/periodic directory, but\n    # don't bother touching anything in there - so pyinfra does it, enabling\n    # cache_time to work.\n    if cache_time:\n        yield 'touch {0}'.format(APT_UPDATE_FILENAME)", "response": "Updates apt repos.\n\n    + cache_time: cache updates for this many seconds\n    + touch_periodic: touch ``/var/lib/apt/periodic/update-success-stamp`` after update"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef packages(\n    state, host,\n    packages=None, present=True, latest=False,\n    update=False, cache_time=None, upgrade=False,\n    force=False, no_recommends=False,\n    allow_downgrades=False,\n):\n    '''\n    Install/remove/update packages & update apt.\n\n    + packages: list of packages to ensure\n    + present: whether the packages should be installed\n    + latest: whether to upgrade packages without a specified version\n    + update: run apt update\n    + cache_time: when used with update, cache for this many seconds\n    + upgrade: run apt upgrade\n    + force: whether to force package installs by passing `--force-yes` to apt\n    + no_recommends: don't install recommended packages\n    + allow_downgrades: allow downgrading packages with version (--allow-downgrades)\n\n    Versions:\n        Package versions can be pinned like apt: ``<pkg>=<version>``\n\n    Cache time:\n        When ``cache_time`` is set the ``/var/lib/apt/periodic/update-success-stamp`` file\n        is touched upon successful update. Some distros already do this (Ubuntu), but others\n        simply leave the periodic directory empty (Debian).\n    '''\n\n    if update:\n        yield _update(state, host, cache_time=cache_time)\n\n    if upgrade:\n        yield _upgrade(state, host)\n\n    install_command = 'install'\n    if no_recommends is True:\n        install_command += ' --no-install-recommends'\n    if allow_downgrades:\n        install_command += ' --allow-downgrades'\n\n    # Compare/ensure packages are present/not\n    yield ensure_packages(\n        packages, host.fact.deb_packages, present,\n        install_command=noninteractive_apt(install_command, force=force),\n        uninstall_command=noninteractive_apt('remove', force=force),\n        upgrade_command=noninteractive_apt(install_command, force=force),\n        version_join='=',\n        latest=latest,\n    )", "response": "A generator that returns a list of packages that can be installed on the host."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread an OpenSSH config file from the given file - like object.", "response": "def parse(self, file_obj, parsed_files=None):\n        '''\n        Read an OpenSSH config from the given file object.\n\n        :param file_obj: a file-like object to read the config file from\n        '''\n        host = {'host': ['*'], 'config': {}}\n        for line in file_obj:\n            # Strip any leading or trailing whitespace from the line.\n            # Refer to https://github.com/paramiko/paramiko/issues/499\n            line = line.strip()\n            if not line or line.startswith('#'):\n                continue\n\n            match = re.match(self.SETTINGS_REGEX, line)\n            if not match:\n                raise Exception('Unparsable line {}'.format(line))\n            key = match.group(1).lower()\n            value = match.group(2)\n\n            if key == 'host':\n                self._config.append(host)\n                host = {\n                    'host': self._get_hosts(value),\n                    'config': {}\n                }\n            elif key == 'proxycommand' and value.lower() == 'none':\n                # Store 'none' as None; prior to 3.x, it will get stripped out\n                # at the end (for compatibility with issue #415). After 3.x, it\n                # will simply not get stripped, leaving a nice explicit marker.\n                host['config'][key] = None\n            elif key == 'include':\n                # support for Include directive in ssh_config\n                path = value\n                # the path can be relative to its parent configuration file\n                if os.path.isabs(path) is False and path[0] != '~':\n                    folder = os.path.dirname(file_obj.name)\n                    path = os.path.join(folder, path)\n\n                # expand the user home path\n                path = os.path.expanduser(path)\n                if parsed_files is None:\n                    parsed_files = []\n\n                # parse every included file\n                for filename in glob.iglob(path):\n                    if os.path.isfile(filename):\n                        if filename in parsed_files:\n                            raise Exception('Include loop detected in ssh config file: %s' % filename)\n                        with open(filename) as fd:\n                            parsed_files.append(filename)\n                            self.parse(fd, parsed_files)\n\n            else:\n                if value.startswith('\"') and value.endswith('\"'):\n                    value = value[1:-1]\n\n                # identityfile, localforward, remoteforward keys are special\n                # cases, since they are allowed to be specified multiple times\n                # and they should be tried in order of specification.\n                if key in ['identityfile', 'localforward', 'remoteforward']:\n                    if key in host['config']:\n                        host['config'][key].append(value)\n                    else:\n                        host['config'][key] = [value]\n                elif key not in host['config']:\n                    host['config'][key] = value\n        self._config.append(host)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a command on the local machine and return the exit code stdout and stderr.", "response": "def run_shell_command(\n    state, host, command,\n    get_pty=False, timeout=None, print_output=False,\n    **command_kwargs\n):\n    '''\n    Execute a command on the local machine.\n\n    Args:\n        state (``pyinfra.api.State`` obj): state object for this command\n        hostname (string): hostname of the target\n        command (string): actual command to execute\n        sudo (boolean): whether to wrap the command with sudo\n        sudo_user (string): user to sudo to\n        get_pty (boolean): whether to get a PTY before executing the command\n        env (dict): envrionment variables to set\n        timeout (int): timeout for this command to complete before erroring\n\n    Returns:\n        tuple: (exit_code, stdout, stderr)\n        stdout and stderr are both lists of strings from each buffer.\n    '''\n\n    command = make_command(command, **command_kwargs)\n\n    logger.debug('--> Running command on localhost: {0}'.format(command))\n\n    if print_output:\n        print('{0}>>> {1}'.format(host.print_prefix, command))\n\n    process = Popen(command, shell=True, stdout=PIPE, stderr=PIPE)\n\n    # Iterate through outputs to get an exit status and generate desired list\n    # output, done in two greenlets so stdout isn't printed before stderr. Not\n    # attached to state.pool to avoid blocking it with 2x n-hosts greenlets.\n    stdout_reader = gevent.spawn(\n        read_buffer, process.stdout,\n        print_output=print_output,\n        print_func=lambda line: '{0}{1}'.format(host.print_prefix, line),\n    )\n    stderr_reader = gevent.spawn(\n        read_buffer, process.stderr,\n        print_output=print_output,\n        print_func=lambda line: '{0}{1}'.format(\n            host.print_prefix, click.style(line, 'red'),\n        ),\n    )\n\n    # Wait on output, with our timeout (or None)\n    greenlets = gevent.wait((stdout_reader, stderr_reader), timeout=timeout)\n\n    # Timeout doesn't raise an exception, but gevent.wait returns the greenlets\n    # which did complete. So if both haven't completed, we kill them and fail\n    # with a timeout.\n    if len(greenlets) != 2:\n        stdout_reader.kill()\n        stderr_reader.kill()\n\n        raise timeout_error()\n\n    # Read the buffers into a list of lines\n    stdout = stdout_reader.get()\n    stderr = stderr_reader.get()\n\n    logger.debug('--> Waiting for exit status...')\n    process.wait()\n\n    # Close any open file descriptor\n    process.stdout.close()\n\n    logger.debug('--> Command exit status: {0}'.format(process.returncode))\n    return process.returncode == 0, stdout, stderr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef d(\n    state, host, name,\n    running=True, restarted=False, reloaded=False,\n    enabled=None, command=None,\n):\n    '''\n    Manage the state of SysV Init (/etc/init.d) services.\n\n    + name: name of the service to manage\n    + running: whether the service should be running\n    + restarted: whether the service should be restarted\n    + reloaded: whether the service should be reloaded\n    + enabled: whether this service should be enabled/disabled\n    + command: command (eg. reload) to run like: ``/etc/init.d/<name> <command>``\n\n    Enabled:\n        Because managing /etc/rc.d/X files is a mess, only certain Linux distributions\n        support enabling/disabling services:\n\n        + Ubuntu/Debian (``update-rc.d``)\n        + CentOS/Fedora/RHEL (``chkconfig``)\n        + Gentoo (``rc-update``)\n\n        For other distributions and more granular service control, see the\n        ``init.d_enable`` operation.\n    '''\n\n    yield _handle_service_control(\n        name, host.fact.initd_status,\n        '/etc/init.d/{0} {1}',\n        running, restarted, reloaded, command,\n    )\n\n    if isinstance(enabled, bool):\n        start_links = host.fact.find_links('/etc/rc*.d/S*{0}'.format(name)) or []\n\n        # If no links exist, attempt to enable the service using distro-specific commands\n        if enabled is True and not start_links:\n            distro = host.fact.linux_distribution.get('name')\n\n            if distro in ('Ubuntu', 'Debian'):\n                yield 'update-rc.d {0} defaults'.format(name)\n\n            elif distro in ('CentOS', 'Fedora', 'Red Hat Enterprise Linux'):\n                yield 'chkconfig {0} --add'.format(name)\n                yield 'chkconfig {0} on'.format(name)\n\n            elif distro == 'Gentoo':\n                yield 'rc-update add {0} default'.format(name)\n\n        # Remove any /etc/rcX.d/<name> start links\n        elif enabled is False:\n            # No state checking, just blindly remove any that exist\n            for link in start_links:\n                yield 'rm -f {0}'.format(link)", "response": "Manage the state of a Linux service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef d_enable(\n    state, host, name,\n    start_priority=20, stop_priority=80,\n    start_levels=(2, 3, 4, 5), stop_levels=(0, 1, 6),\n):\n    '''\n    Manually enable /etc/init.d scripts by creating /etc/rcX.d/Y links.\n\n    + name: name of the service to enable\n    + start_priority: priority to start the service\n    + stop_priority: priority to stop the service\n    + start_levels: which runlevels should the service run when enabled\n    + stop_levels: which runlevels should the service stop when enabled\n    '''\n\n    # Build link list\n    links = []\n\n    for level in start_levels:\n        links.append('/etc/rc{0}.d/S{1}{2}'.format(level, start_priority, name))\n\n    for level in stop_levels:\n        links.append('/etc/rc{0}.d/K{1}{2}'.format(level, stop_priority, name))\n\n    # Ensure all the new links exist\n    for link in links:\n        yield files.link(state, host, link, '/etc/init.d/{0}'.format(name))", "response": "Enable a service in the specified state"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rc(\n    state, host, name,\n    running=True, restarted=False, reloaded=False,\n    command=None, enabled=None,\n):\n    '''\n    Manage the state of BSD init (/etc/rc.d) services.\n\n    + name: name of the service to manage\n    + running: whether the service should be running\n    + restarted: whether the service should be restarted\n    + reloaded: whether the service should be reloaded\n    + command: custom command to pass like: ``/etc/rc.d/<name> <command>``\n    + enabled: whether this service should be enabled/disabled on boot\n    '''\n\n    yield _handle_service_control(\n        name, host.fact.rcd_status,\n        '/etc/rc.d/{0} {1}',\n        running, restarted, reloaded, command,\n        status_argument='check',\n    )\n\n    # BSD init is simple, just add/remove <name>_enabled=\"YES\"\n    if isinstance(enabled, bool):\n        yield files.line(\n            state, host,\n            '/etc/rc.conf.local',\n            '^{0}_enable='.format(name),\n            replace='{0}_enable=\"YES\"'.format(name),\n            present=enabled,\n        )", "response": "Manage the state of a BSD init service"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmanage the state of upstart managed services.", "response": "def upstart(\n    state, host, name,\n    running=True, restarted=False, reloaded=False,\n    command=None, enabled=None,\n):\n    '''\n    Manage the state of upstart managed services.\n\n    + name: name of the service to manage\n    + running: whether the service should be running\n    + restarted: whether the service should be restarted\n    + reloaded: whether the service should be reloaded\n    + command: custom command to pass like: ``/etc/rc.d/<name> <command>``\n    + enabled: whether this service should be enabled/disabled on boot\n\n    Enabling/disabling services:\n        Upstart jobs define runlevels in their config files - as such there is no way to\n        edit/list these without fiddling with the config. So pyinfra simply manages the\n        existence of a ``/etc/init/<service>.override`` file, and sets its content to\n        \"manual\" to disable automatic start of services.\n    '''\n\n    yield _handle_service_control(\n        name, host.fact.upstart_status,\n        'initctl {1} {0}',\n        running, restarted, reloaded, command,\n    )\n\n    # Upstart jobs are setup w/runlevels etc in their config files, so here we just check\n    # there's no override file.\n    if enabled is True:\n        yield files.file(\n            state, host,\n            '/etc/init/{0}.override'.format(name),\n            present=False,\n        )\n\n    # Set the override file to \"manual\" to disable automatic start\n    elif enabled is False:\n        yield 'echo \"manual\" > /etc/init/{0}.override'.format(name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmanaging the state of a systemd service", "response": "def systemd(\n    state, host, name,\n    running=True, restarted=False, reloaded=False,\n    command=None, enabled=None, daemon_reload=False,\n):\n    '''\n    Manage the state of systemd managed services.\n\n    + name: name of the service to manage\n    + running: whether the service should be running\n    + restarted: whether the service should be restarted\n    + reloaded: whether the service should be reloaded\n    + command: custom command to pass like: ``/etc/rc.d/<name> <command>``\n    + enabled: whether this service should be enabled/disabled on boot\n    + daemon_reload: reload the systemd daemon to read updated unit files\n    '''\n\n    if daemon_reload:\n        yield 'systemctl daemon-reload'\n\n    yield _handle_service_control(\n        name, host.fact.systemd_status,\n        'systemctl {1} {0}.service',\n        running, restarted, reloaded, command,\n    )\n\n    if isinstance(enabled, bool):\n        is_enabled = host.fact.systemd_enabled.get(name, False)\n\n        # Isn't enabled and want enabled?\n        if not is_enabled and enabled is True:\n            yield 'systemctl enable {0}.service'.format(name)\n\n        # Is enabled and want disabled?\n        elif is_enabled and enabled is False:\n            yield 'systemctl disable {0}.service'.format(name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmanage the state of a systemd managed service.", "response": "def launchd(\n    state, host, name,\n    running=True, restarted=False, command=None,\n):\n    '''\n    Manage the state of systemd managed services.\n\n    + name: name of the service to manage\n    + running: whether the service should be running\n    + restarted: whether the service should be restarted\n    + command: custom command to pass like: ``/etc/rc.d/<name> <command>``\n    + enabled: whether this service should be enabled/disabled on boot\n    + daemon_reload: reload the systemd daemon to read updated unit files\n    '''\n\n    yield _handle_service_control(\n        name, host.fact.launchd_status,\n        'launchctl {1} {0}',\n        # No support for restart/reload/command\n        running, None, None, None,\n    )\n\n    # No restart command, so just stop/start\n    is_running = host.fact.launchd_status.get(name, None)\n    if restarted and is_running:\n        yield 'launchctl stop {0}'.format(name)\n        yield 'launchctl start {0}'.format(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef service(\n    state, host,\n    *args, **kwargs\n):\n    '''\n    Manage the state of services. This command checks for the presence of all the\n    init systems pyinfra can handle and executes the relevant operation. See init\n    system sepcific operation for arguments.\n    '''\n\n    if host.fact.which('systemctl'):\n        yield systemd(state, host, *args, **kwargs)\n        return\n\n    if host.fact.which('initctl'):\n        yield upstart(state, host, *args, **kwargs)\n        return\n\n    if host.fact.directory('/etc/init.d'):\n        yield d(state, host, *args, **kwargs)\n        return\n\n    if host.fact.directory('/etc/rc.d'):\n        yield rc(state, host, *args, **kwargs)\n        return\n\n    raise OperationError((\n        'No init system found '\n        '(no systemctl, initctl, /etc/init.d or /etc/rc.d found)'\n    ))", "response": "Manage the state of services. This command checks for presence of all the systemctl initctl and initctl directories and executes the relevant operation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(state, host, for_fact=None):\n    '''\n    Connect to a single host. Returns the SSH client if succesful. Stateless by\n    design so can be run in parallel.\n    '''\n\n    kwargs = _make_paramiko_kwargs(state, host)\n    logger.debug('Connecting to: {0} ({1})'.format(host.name, kwargs))\n\n    # Hostname can be provided via SSH config (alias), data, or the hosts name\n    hostname = kwargs.pop(\n        'hostname',\n        host.data.ssh_hostname or host.name,\n    )\n\n    try:\n        # Create new client & connect to the host\n        client = SSHClient()\n        client.set_missing_host_key_policy(MissingHostKeyPolicy())\n        client.connect(hostname, **kwargs)\n\n        # Enable SSH forwarding\n        session = client.get_transport().open_session()\n        AgentRequestHandler(session)\n\n        # Log\n        log_message = '{0}{1}'.format(\n            host.print_prefix,\n            click.style('Connected', 'green'),\n        )\n\n        if for_fact:\n            log_message = '{0}{1}'.format(\n                log_message,\n                ' (for {0} fact)'.format(for_fact),\n            )\n\n        logger.info(log_message)\n\n        return client\n\n    except AuthenticationException:\n        auth_kwargs = {}\n\n        for key, value in kwargs.items():\n            if key in ('username', 'password'):\n                auth_kwargs[key] = value\n                continue\n\n            if key == 'pkey' and value:\n                auth_kwargs['key'] = host.data.ssh_key\n\n        auth_args = ', '.join(\n            '{0}={1}'.format(key, value)\n            for key, value in auth_kwargs.items()\n        )\n\n        _log_connect_error(host, 'Authentication error', auth_args)\n\n    except SSHException as e:\n        _log_connect_error(host, 'SSH error', e)\n\n    except gaierror:\n        _log_connect_error(host, 'Could not resolve hostname', hostname)\n\n    except socket_error as e:\n        _log_connect_error(host, 'Could not connect', e)\n\n    except EOFError as e:\n        _log_connect_error(host, 'EOF error', e)", "response": "Connect to a single host. Returns the SSH client if succesful."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a command on the specified host and return the output of the command.", "response": "def run_shell_command(\n    state, host, command,\n    get_pty=False, timeout=None, print_output=False,\n    **command_kwargs\n):\n    '''\n    Execute a command on the specified host.\n\n    Args:\n        state (``pyinfra.api.State`` obj): state object for this command\n        hostname (string): hostname of the target\n        command (string): actual command to execute\n        sudo (boolean): whether to wrap the command with sudo\n        sudo_user (string): user to sudo to\n        get_pty (boolean): whether to get a PTY before executing the command\n        env (dict): envrionment variables to set\n        timeout (int): timeout for this command to complete before erroring\n\n    Returns:\n        tuple: (exit_code, stdout, stderr)\n        stdout and stderr are both lists of strings from each buffer.\n    '''\n\n    command = make_command(command, **command_kwargs)\n\n    logger.debug('Running command on {0}: (pty={1}) {2}'.format(\n        host.name, get_pty, command,\n    ))\n\n    if print_output:\n        print('{0}>>> {1}'.format(host.print_prefix, command))\n\n    # Run it! Get stdout, stderr & the underlying channel\n    _, stdout_buffer, stderr_buffer = host.connection.exec_command(\n        command,\n        get_pty=get_pty,\n    )\n\n    channel = stdout_buffer.channel\n\n    # Iterate through outputs to get an exit status and generate desired list\n    # output, done in two greenlets so stdout isn't printed before stderr. Not\n    # attached to state.pool to avoid blocking it with 2x n-hosts greenlets.\n    stdout_reader = gevent.spawn(\n        read_buffer, stdout_buffer,\n        print_output=print_output,\n        print_func=lambda line: '{0}{1}'.format(host.print_prefix, line),\n    )\n    stderr_reader = gevent.spawn(\n        read_buffer, stderr_buffer,\n        print_output=print_output,\n        print_func=lambda line: '{0}{1}'.format(\n            host.print_prefix, click.style(line, 'red'),\n        ),\n    )\n\n    # Wait on output, with our timeout (or None)\n    greenlets = gevent.wait((stdout_reader, stderr_reader), timeout=timeout)\n\n    # Timeout doesn't raise an exception, but gevent.wait returns the greenlets\n    # which did complete. So if both haven't completed, we kill them and fail\n    # with a timeout.\n    if len(greenlets) != 2:\n        stdout_reader.kill()\n        stderr_reader.kill()\n\n        raise timeout_error()\n\n    # Read the buffers into a list of lines\n    stdout = stdout_reader.get()\n    stderr = stderr_reader.get()\n\n    logger.debug('Waiting for exit status...')\n    exit_status = channel.recv_exit_status()\n\n    logger.debug('Command exit status: {0}'.format(exit_status))\n    return exit_status == 0, stdout, stderr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nuploading a file to the specified host using SFTP.", "response": "def put_file(\n    state, host, filename_or_io, remote_filename,\n    sudo=False, sudo_user=None, su_user=None, print_output=False,\n):\n    '''\n    Upload file-ios to the specified host using SFTP. Supports uploading files\n    with sudo by uploading to a temporary directory then moving & chowning.\n    '''\n\n    # sudo/su are a little more complicated, as you can only sftp with the SSH\n    # user connected, so upload to tmp and copy/chown w/sudo and/or su_user\n    if sudo or su_user:\n        # Get temp file location\n        temp_file = state.get_temp_filename(remote_filename)\n        _put_file(host, filename_or_io, temp_file)\n\n        if print_output:\n            print('{0}file uploaded: {1}'.format(host.print_prefix, remote_filename))\n\n        # Execute run_shell_command w/sudo and/or su_user\n        command = 'mv {0} {1}'.format(temp_file, remote_filename)\n\n        # Move it to the su_user if present\n        if su_user:\n            command = '{0} && chown {1} {2}'.format(command, su_user, remote_filename)\n\n        # Otherwise any sudo_user\n        elif sudo_user:\n            command = '{0} && chown {1} {2}'.format(command, sudo_user, remote_filename)\n\n        status, _, stderr = run_shell_command(\n            state, host, command,\n            sudo=sudo, sudo_user=sudo_user, su_user=su_user,\n            print_output=print_output,\n        )\n\n        if status is False:\n            logger.error('File error: {0}'.format('\\n'.join(stderr)))\n            return False\n\n    # No sudo and no su_user, so just upload it!\n    else:\n        _put_file(host, filename_or_io, remote_filename)\n\n        if print_output:\n            print('{0}file uploaded: {1}'.format(host.print_prefix, remote_filename))\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a name for a similarly named nested container.", "response": "def _make_name(current, new):\n    '''\n    Stops duplication between similarly named nested deploys, eg:\n\n    Turn:\n        Deploy Kubernetes master/Configure Kubernetes\n    Into:\n        Deploy Kubernetes master/Configure\n    '''\n\n    current_tokens = current.split()\n    new_tokens = new.split()\n\n    new = ' '.join(\n        new_token for new_token in new_tokens\n        if new_token not in current_tokens\n    )\n\n    return '/'.join((current, new))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deploy(self, name, kwargs, data, line_number, in_deploy=True):\n        '''\n        Wraps a group of operations as a deploy, this should not be used\n        directly, instead use ``pyinfra.api.deploy.deploy``.\n        '''\n\n        # Handle nested deploy names\n        if self.deploy_name:\n            name = _make_name(self.deploy_name, name)\n\n        # Store the previous values\n        old_in_deploy = self.in_deploy\n        old_deploy_name = self.deploy_name\n        old_deploy_kwargs = self.deploy_kwargs\n        old_deploy_data = self.deploy_data\n        old_deploy_line_numbers = self.deploy_line_numbers\n        self.in_deploy = in_deploy\n\n        # Limit the new hosts to a subset of the old hosts if they existed\n        if (\n            old_deploy_kwargs\n            and old_deploy_kwargs.get('hosts') is not None\n        ):\n            # If we have hosts - subset them based on the old hosts\n            if 'hosts' in kwargs:\n                kwargs['hosts'] = [\n                    host for host in kwargs['hosts']\n                    if host in old_deploy_kwargs['hosts']\n                ]\n            # Otherwise simply carry the previous hosts\n            else:\n                kwargs['hosts'] = old_deploy_kwargs['hosts']\n\n        # Make new line numbers - note convert from and back to tuple to avoid\n        # keeping deploy_line_numbers mutable.\n        new_line_numbers = list(self.deploy_line_numbers or [])\n        new_line_numbers.append(line_number)\n        new_line_numbers = tuple(new_line_numbers)\n\n        # Set the new values\n        self.deploy_name = name\n        self.deploy_kwargs = kwargs\n        self.deploy_data = data\n        self.deploy_line_numbers = new_line_numbers\n        logger.debug('Starting deploy {0} (args={1}, data={2})'.format(\n            name, kwargs, data,\n        ))\n\n        yield\n\n        # Restore the previous values\n        self.in_deploy = old_in_deploy\n        self.deploy_name = old_deploy_name\n        self.deploy_kwargs = old_deploy_kwargs\n        self.deploy_data = old_deploy_data\n        self.deploy_line_numbers = old_deploy_line_numbers\n\n        logger.debug('Reset deploy to {0} (args={1}, data={2})'.format(\n            old_deploy_name, old_deploy_kwargs, old_deploy_data,\n        ))", "response": "A context manager that provides a context manager for deploying a new entry point for the current state of the current entry point."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef activate_host(self, host):\n        '''\n        Flag a host as active.\n        '''\n\n        logger.debug('Activating host: {0}'.format(host))\n\n        # Add to *both* activated and active - active will reduce as hosts fail\n        # but connected will not, enabling us to track failed %.\n        self.activated_hosts.add(host)\n        self.active_hosts.add(host)", "response": "Flag a host as active."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nflag a set of hosts as failed error for config. FAIL_PERCENT.", "response": "def fail_hosts(self, hosts_to_fail, activated_count=None):\n        '''\n        Flag a ``set`` of hosts as failed, error for ``config.FAIL_PERCENT``.\n        '''\n\n        if not hosts_to_fail:\n            return\n\n        activated_count = activated_count or len(self.activated_hosts)\n\n        logger.debug('Failing hosts: {0}'.format(', '.join(\n            (host.name for host in hosts_to_fail),\n        )))\n\n        # Remove the failed hosts from the inventory\n        self.active_hosts -= hosts_to_fail\n\n        # Check we're not above the fail percent\n        active_hosts = self.active_hosts\n\n        # No hosts left!\n        if not active_hosts:\n            raise PyinfraError('No hosts remaining!')\n\n        if self.config.FAIL_PERCENT is not None:\n            percent_failed = (\n                1 - len(active_hosts) / activated_count\n            ) * 100\n\n            if percent_failed > self.config.FAIL_PERCENT:\n                raise PyinfraError('Over {0}% of hosts failed ({1}%)'.format(\n                    self.config.FAIL_PERCENT,\n                    int(round(percent_failed)),\n                ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a boolean indicating if the host is within the current state limit.", "response": "def is_host_in_limit(self, host):\n        '''\n        Returns a boolean indicating if the host is within the current state limit.\n        '''\n\n        limit_hosts = self.limit_hosts\n\n        if not isinstance(limit_hosts, list):\n            return True\n        return host in limit_hosts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_temp_filename(self, hash_key=None):\n        '''\n        Generate a temporary filename for this deploy.\n        '''\n\n        if not hash_key:\n            hash_key = six.text_type(uuid4())\n\n        temp_filename = '{0}/{1}'.format(\n            self.config.TEMP_DIR, sha1_hash(hash_key),\n        )\n\n        return temp_filename", "response": "Generate a temporary filename for this deploy."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns all ops for a single server.", "response": "def _run_server_ops(state, host, progress=None):\n    '''\n    Run all ops for a single server.\n    '''\n\n    logger.debug('Running all ops on {0}'.format(host))\n\n    for op_hash in state.get_op_order():\n        op_meta = state.op_meta[op_hash]\n\n        logger.info('--> {0} {1} on {2}'.format(\n            click.style('--> Starting operation:', 'blue'),\n            click.style(', '.join(op_meta['names']), bold=True),\n            click.style(host.name, bold=True),\n        ))\n\n        result = _run_server_op(state, host, op_hash)\n\n        # Trigger CLI progress if provided\n        if progress:\n            progress((host, op_hash))\n\n        if result is False:\n            raise PyinfraError('Error in operation {0} on {1}'.format(\n                ', '.join(op_meta['names']), host,\n            ))\n\n        if pyinfra.is_cli:\n            print()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning all ops for all servers one server at a time.", "response": "def _run_serial_ops(state):\n    '''\n    Run all ops for all servers, one server at a time.\n    '''\n\n    for host in list(state.inventory):\n        host_operations = product([host], state.get_op_order())\n        with progress_spinner(host_operations) as progress:\n            try:\n                _run_server_ops(\n                    state, host,\n                    progress=progress,\n                )\n            except PyinfraError:\n                state.fail_hosts({host})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_no_wait_ops(state):\n    '''\n    Run all ops for all servers at once.\n    '''\n\n    hosts_operations = product(state.inventory, state.get_op_order())\n    with progress_spinner(hosts_operations) as progress:\n        # Spawn greenlet for each host to run *all* ops\n        greenlets = [\n            state.pool.spawn(\n                _run_server_ops, state, host,\n                progress=progress,\n            )\n            for host in state.inventory\n        ]\n        gevent.joinall(greenlets)", "response": "Run all ops for all servers at once."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a single operation for all servers.", "response": "def _run_single_op(state, op_hash):\n    '''\n    Run a single operation for all servers. Can be configured to run in serial.\n    '''\n\n    op_meta = state.op_meta[op_hash]\n\n    op_types = []\n\n    if op_meta['serial']:\n        op_types.append('serial')\n\n    if op_meta['run_once']:\n        op_types.append('run once')\n\n    logger.info('{0} {1} {2}'.format(\n        click.style('--> Starting{0}operation:'.format(\n            ' {0} '.format(', '.join(op_types)) if op_types else ' ',\n        ), 'blue'),\n        click.style(', '.join(op_meta['names']), bold=True),\n        tuple(op_meta['args']) if op_meta['args'] else '',\n    ))\n\n    failed_hosts = set()\n\n    if op_meta['serial']:\n        with progress_spinner(state.inventory) as progress:\n            # For each host, run the op\n            for host in state.inventory:\n                result = _run_server_op(state, host, op_hash)\n                progress(host)\n\n                if not result:\n                    failed_hosts.add(host)\n\n    else:\n        # Start with the whole inventory in one batch\n        batches = [state.inventory]\n\n        # If parallel set break up the inventory into a series of batches\n        if op_meta['parallel']:\n            parallel = op_meta['parallel']\n            hosts = list(state.inventory)\n\n            batches = [\n                hosts[i:i + parallel]\n                for i in range(0, len(hosts), parallel)\n            ]\n\n        for batch in batches:\n            with progress_spinner(batch) as progress:\n                # Spawn greenlet for each host\n                greenlet_to_host = {\n                    state.pool.spawn(_run_server_op, state, host, op_hash): host\n                    for host in batch\n                }\n\n                # Trigger CLI progress as hosts complete if provided\n                for greenlet in gevent.iwait(greenlet_to_host.keys()):\n                    host = greenlet_to_host[greenlet]\n                    progress(host)\n\n                # Get all the results\n                for greenlet, host in six.iteritems(greenlet_to_host):\n                    if not greenlet.get():\n                        failed_hosts.add(host)\n\n    # Now all the batches/hosts are complete, fail any failures\n    if not op_meta['ignore_errors']:\n        state.fail_hosts(failed_hosts)\n\n    if pyinfra.is_cli:\n        print()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning all operations across all servers in a configurable manner.", "response": "def run_ops(state, serial=False, no_wait=False):\n    '''\n    Runs all operations across all servers in a configurable manner.\n\n    Args:\n        state (``pyinfra.api.State`` obj): the deploy state to execute\n        serial (boolean): whether to run operations host by host\n        no_wait (boolean): whether to wait for all hosts between operations\n    '''\n\n    # Flag state as deploy in process\n    state.deploying = True\n\n    # Run all ops, but server by server\n    if serial:\n        _run_serial_ops(state)\n\n    # Run all the ops on each server in parallel (not waiting at each operation)\n    elif no_wait:\n        _run_no_wait_ops(state)\n\n    # Default: run all ops in order, waiting at each for all servers to complete\n    for op_hash in state.get_op_order():\n        _run_single_op(state, op_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles this common scenario: + We have a list of packages(/versions) to ensure + We have a map of existing package -> versions + We have the common command bits (install, uninstall, version \"joiner\") + Outputs commands to ensure our desired packages/versions + Optionally upgrades packages w/o specified version when present Args: packages (list): list of packages or package/versions current_packages (fact): fact returning dict of package names -> version present (bool): whether packages should exist or not install_command (str): command to prefix to list of packages to install uninstall_command (str): as above for uninstalling packages latest (bool): whether to upgrade installed packages when present upgrade_command (str): as above for upgrading version_join (str): the package manager specific \"joiner\", ie ``=`` for \\ ``<apt_pkg>=<version>`` lower (bool): whether to lowercase package names", "response": "def ensure_packages(\n    packages, current_packages, present,\n    install_command, uninstall_command,\n    latest=False, upgrade_command=None,\n    version_join=None, lower=True,\n):\n    '''\n    Handles this common scenario:\n\n    + We have a list of packages(/versions) to ensure\n    + We have a map of existing package -> versions\n    + We have the common command bits (install, uninstall, version \"joiner\")\n    + Outputs commands to ensure our desired packages/versions\n    + Optionally upgrades packages w/o specified version when present\n\n    Args:\n        packages (list): list of packages or package/versions\n        current_packages (fact): fact returning dict of package names -> version\n        present (bool): whether packages should exist or not\n        install_command (str): command to prefix to list of packages to install\n        uninstall_command (str): as above for uninstalling packages\n        latest (bool): whether to upgrade installed packages when present\n        upgrade_command (str): as above for upgrading\n        version_join (str): the package manager specific \"joiner\", ie ``=`` for \\\n            ``<apt_pkg>=<version>``\n        lower (bool): whether to lowercase package names\n    '''\n\n    if packages is None:\n        return\n\n    # Accept a single package as string\n    if isinstance(packages, six.string_types):\n        packages = [packages]\n\n    # Lowercase packaging?\n    if lower:\n        packages = [\n            package.lower()\n            for package in packages\n        ]\n\n    # Version support?\n    if version_join:\n        # Split where versions present\n        packages = [\n            package.rsplit(version_join, 1)\n            for package in packages\n        ]\n\n        # Covert to either string or list\n        packages = [\n            package[0] if len(package) == 1\n            else package\n            for package in packages\n        ]\n\n    # Diff the ensured packages against the remote state/fact\n    diff_packages = []\n\n    # Packages to upgrade? (install only)\n    upgrade_packages = []\n\n    # Installing?\n    if present is True:\n        for package in packages:\n            # Tuple/version, check not in existing OR incorrect version\n            if isinstance(package, list) and (\n                package[0] not in current_packages\n                or package[1] not in current_packages[package[0]]\n            ):\n                diff_packages.append(package)\n\n            # String version, just check if not existing\n            if isinstance(package, six.string_types) and package not in current_packages:\n                diff_packages.append(package)\n\n            # Present packages w/o version spec ified - for upgrade if latest\n            if isinstance(package, six.string_types) and package in current_packages:\n                upgrade_packages.append(package)\n\n    # Uninstalling?\n    else:\n        for package in packages:\n            # Tuple/version, heck existing AND correct version\n            if isinstance(package, list) and (\n                package[0] in current_packages\n                and package[1] in current_packages[package[0]]\n            ):\n                diff_packages.append(package)\n\n            # String version, just check if existing\n            if isinstance(package, six.string_types) and package in current_packages:\n                diff_packages.append(package)\n\n    # Convert packages back to string(/version)\n    diff_packages = [\n        version_join.join(package)\n        if isinstance(package, list)\n        else package\n        for package in diff_packages\n    ]\n\n    if diff_packages:\n        command = install_command if present else uninstall_command\n\n        yield '{0} {1}'.format(\n            command,\n            ' '.join(diff_packages),\n        )\n\n    if latest and upgrade_command and upgrade_packages:\n        yield '{0} {1}'.format(\n            upgrade_command,\n            ' '.join(upgrade_packages),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_plan_id(self, plan_id) -> bool:\n        for plan in self.catalog().plans:\n            if plan.id == plan_id:\n                return True\n        return False", "response": "Checks that the plan_id exists in the catalog"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef provision(self, instance_id: str, service_details: ProvisionDetails,\n                  async_allowed: bool) -> ProvisionedServiceSpec:\n        \"\"\"\n        Further readings `CF Broker API#Provisioning <https://docs.cloudfoundry.org/services/api.html#provisioning>`_\n\n        :param instance_id: Instance id provided by the platform\n        :param service_details: Details about the service to create\n        :param async_allowed: Client allows async creation\n        :rtype: ProvisionedServiceSpec\n        :raises ErrInstanceAlreadyExists: If instance already exists\n        :raises ErrAsyncRequired: If async is required but not supported\n        \"\"\"\n        raise NotImplementedError()", "response": "Provision the specified instance with the specified details."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the service s metadata with the given details.", "response": "def update(self, instance_id: str, details: UpdateDetails, async_allowed: bool) -> UpdateServiceSpec:\n        \"\"\"\n        Further readings `CF Broker API#Update <https://docs.cloudfoundry.org/services/api.html#updating_service_instance>`_\n\n        :param instance_id: Instance id provided by the platform\n        :param details: Details about the service to update\n        :param async_allowed: Client allows async creation\n        :rtype: UpdateServiceSpec\n        :raises ErrAsyncRequired: If async is required but not supported\n        \"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deprovision(self, instance_id: str, details: DeprovisionDetails,\n                    async_allowed: bool) -> DeprovisionServiceSpec:\n        \"\"\"\n        Further readings `CF Broker API#Deprovisioning <https://docs.cloudfoundry.org/services/api.html#deprovisioning>`_\n\n        :param instance_id: Instance id provided by the platform\n        :param details: Details about the service to delete\n        :param async_allowed: Client allows async creation\n        :rtype: DeprovisionServiceSpec\n        :raises ErrInstanceDoesNotExist: If instance does not exists\n        :raises ErrAsyncRequired: If async is required but not supported\n        \"\"\"\n        raise NotImplementedError()", "response": "Deprovision the specified instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbind an instance to a specific binding.", "response": "def bind(self, instance_id: str, binding_id: str, details: BindDetails) -> Binding:\n        \"\"\"\n        Further readings `CF Broker API#Binding <https://docs.cloudfoundry.org/services/api.html#binding>`_\n\n        :param instance_id: Instance id provided by the platform\n        :param binding_id: Binding id provided by the platform\n        :param details: Details about the binding to create\n        :rtype: Binding\n        :raises ErrBindingAlreadyExists: If binding already exists\n        :raises ErrAppGuidNotProvided: If AppGuid is required but not provided\n        \"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconfigure a logger to log to stdout and > INFO to stderr", "response": "def basic_config(logger: logging.Logger = logging.root, level=logging.INFO):\n    \"\"\"\n    Configures a logger to log <=INFO to stdout and >INFO to stderr\n\n    :param logger: Logger to configure, defaults to logging.root\n    :param level: Defaults to INFO\n    :return: configured logger (logger from parameters)\n    \"\"\"\n    logger.setLevel(level)\n\n    class InfoFilter(logging.Filter):\n        def filter(self, rec):\n            return rec.levelno in (logging.DEBUG, logging.INFO)\n\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", \"%d/%m/%Y %H:%M:%S\")\n\n    std_out_handler = logging.StreamHandler(sys.stdout)\n    std_out_handler.setLevel(logging.DEBUG)\n    std_out_handler.setFormatter(formatter)\n    std_out_handler.addFilter(InfoFilter())\n\n    std_err_handler = logging.StreamHandler()\n    std_err_handler.setLevel(logging.WARNING)\n    std_err_handler.setFormatter(formatter)\n\n    logger.addHandler(std_out_handler)\n    logger.addHandler(std_err_handler)\n\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the blueprint that will register with the app instance.", "response": "def get_blueprint(service_brokers: Union[List[ServiceBroker], ServiceBroker],\n                  broker_credentials: Union[None, List[BrokerCredentials], BrokerCredentials],\n                  logger: logging.Logger) -> Blueprint:\n    \"\"\"\n    Returns the blueprint with service broker api.\n\n    :param service_brokers: Services that this broker exposes\n    :param broker_credentials: Optional Usernames and passwords that will be required to communicate with service broker\n    :param logger: Used for api logs. This will not influence Flasks logging behavior.\n    :return: Blueprint to register with Flask app instance\n    \"\"\"\n    openbroker = Blueprint('open_broker', __name__)\n    service_brokers = ensure_list(service_brokers)\n\n    # Apply filters\n    logger.debug(\"Apply print_request filter for debugging\")\n    openbroker.before_request(print_request)\n\n    if DISABLE_VERSION_CHECK:\n        logger.warning(\n            \"Minimum API version is not checked, this can cause illegal contracts between service broker and platform!\"\n        )\n    else:\n        logger.debug(\"Apply check_version filter for version %s\" % str(MIN_VERSION))\n        openbroker.before_request(check_version)\n\n    logger.debug(\"Apply check_originating_identity filter\")\n    openbroker.before_request(check_originating_identity)\n\n    if broker_credentials is not None:\n        broker_credentials = ensure_list(broker_credentials)\n        logger.debug(\"Apply check_auth filter with {} credentials\".format(len(broker_credentials)))\n        openbroker.before_request(get_auth_filter(broker_credentials))\n\n    def get_broker_by_id(service_id: str):\n        for service in service_brokers:\n            if service.service_id() == service_id:\n                return service\n        raise KeyError('Service {} not found'.format(service_id))\n\n    def add_service_id_to_async_response(response, service_id: str):\n        if response.is_async:\n            if response.operation is None:\n                response.operation = service_id\n            else:\n                response.operation = ' '.join((service_id, response.operation))\n\n    def extract_authorization_username(request: Request):\n        if request.authorization is not None:\n            return request.authorization.username\n        else:\n            return None\n\n    @openbroker.errorhandler(Exception)\n    def error_handler(e):\n        logger.exception(e)\n        return to_json_response(ErrorResponse(\n            description=str(e)\n        )), HTTPStatus.INTERNAL_SERVER_ERROR\n\n    @openbroker.errorhandler(NotImplementedError)\n    def error_handler(e):\n        logger.exception(e)\n        return to_json_response(ErrorResponse(\n            description=str(e)\n        )), HTTPStatus.NOT_IMPLEMENTED\n\n    @openbroker.route(\"/v2/catalog\", methods=['GET'])\n    def catalog():\n        \"\"\"\n        :return: Catalog of broker (List of services)\n        \"\"\"\n        return to_json_response(CatalogResponse(list(s.catalog() for s in service_brokers)))\n\n    @openbroker.route(\"/v2/service_instances/<instance_id>\", methods=['PUT'])\n    @requires_application_json\n    def provision(instance_id):\n        try:\n            accepts_incomplete = 'true' == request.args.get(\"accepts_incomplete\", 'false')\n\n            provision_details = ProvisionDetails(**json.loads(request.data))\n            provision_details.originating_identity = request.originating_identity\n            provision_details.authorization_username = extract_authorization_username(request)\n            broker = get_broker_by_id(provision_details.service_id)\n            if not broker.check_plan_id(provision_details.plan_id):\n                raise TypeError('plan_id not found in this service.')\n        except (TypeError, KeyError, JSONDecodeError) as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(description=str(e))), HTTPStatus.BAD_REQUEST\n\n        try:\n            result = broker.provision(instance_id, provision_details, accepts_incomplete)\n            add_service_id_to_async_response(result, broker.service_id())\n        except errors.ErrInstanceAlreadyExists as e:\n            logger.exception(e)\n            return to_json_response(EmptyResponse()), HTTPStatus.CONFLICT\n        except errors.ErrInvalidParameters as e:\n            return to_json_response(ErrorResponse('InvalidParameters', str(e))), HTTPStatus.BAD_REQUEST\n        except errors.ErrAsyncRequired as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(\n                error=\"AsyncRequired\",\n                description=\"This service plan requires client support for asynchronous service operations.\"\n            )), HTTPStatus.UNPROCESSABLE_ENTITY\n\n        if result.state == ProvisionState.IS_ASYNC:\n            return to_json_response(ProvisioningResponse(result.dashboard_url, result.operation)), HTTPStatus.ACCEPTED\n        elif result.state == ProvisionState.IDENTICAL_ALREADY_EXISTS:\n            return to_json_response(ProvisioningResponse(result.dashboard_url, result.operation)), HTTPStatus.OK\n        elif result.state == ProvisionState.SUCCESSFUL_CREATED:\n            return to_json_response(ProvisioningResponse(result.dashboard_url, result.operation)), HTTPStatus.CREATED\n        else:\n            raise errors.ServiceException('IllegalState, ProvisioningState unknown.')\n\n    @openbroker.route(\"/v2/service_instances/<instance_id>\", methods=['PATCH'])\n    @requires_application_json\n    def update(instance_id):\n        try:\n            accepts_incomplete = 'true' == request.args.get(\"accepts_incomplete\", 'false')\n\n            update_details = UpdateDetails(**json.loads(request.data))\n            update_details.originating_identity = request.originating_identity\n            update_details.authorization_username = extract_authorization_username(request)\n            broker = get_broker_by_id(update_details.service_id)\n            if not broker.check_plan_id(update_details.plan_id):\n                raise TypeError('plan_id not found in this service.')\n        except (TypeError, KeyError, JSONDecodeError) as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(description=str(e))), HTTPStatus.BAD_REQUEST\n\n        try:\n            result = broker.update(instance_id, update_details, accepts_incomplete)\n            add_service_id_to_async_response(result, broker.service_id())\n        except errors.ErrInvalidParameters as e:\n            return to_json_response(ErrorResponse('InvalidParameters', str(e))), HTTPStatus.BAD_REQUEST\n        except errors.ErrAsyncRequired as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(\n                error=\"AsyncRequired\",\n                description=\"This service plan requires client support for asynchronous service operations.\"\n            )), HTTPStatus.UNPROCESSABLE_ENTITY\n\n        if result.is_async:\n            return to_json_response(UpdateResponse(result.operation, result.dashboard_url)), HTTPStatus.ACCEPTED\n        else:\n            return to_json_response(UpdateResponse(None, result.dashboard_url)), HTTPStatus.OK\n\n    @openbroker.route(\"/v2/service_instances/<instance_id>/service_bindings/<binding_id>\", methods=['PUT'])\n    @requires_application_json\n    def bind(instance_id, binding_id):\n        try:\n            binding_details = BindDetails(**json.loads(request.data))\n            binding_details.originating_identity = request.originating_identity\n            binding_details.authorization_username = extract_authorization_username(request)\n            broker = get_broker_by_id(binding_details.service_id)\n            if not broker.check_plan_id(binding_details.plan_id):\n                raise TypeError('plan_id not found in this service.')\n        except (TypeError, KeyError, JSONDecodeError) as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(description=str(e))), HTTPStatus.BAD_REQUEST\n\n        try:\n            result = broker.bind(instance_id, binding_id, binding_details)\n        except errors.ErrBindingAlreadyExists as e:\n            logger.exception(e)\n            return to_json_response(EmptyResponse()), HTTPStatus.CONFLICT\n        except errors.ErrAppGuidNotProvided as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(\n                error=\"RequiresApp\",\n                description=\"This service supports generation of credentials through binding an application only.\"\n            )), HTTPStatus.UNPROCESSABLE_ENTITY\n\n        response = BindResponse(\n            credentials=result.credentials,\n            syslog_drain_url=result.syslog_drain_url,\n            route_service_url=result.route_service_url,\n            volume_mounts=result.volume_mounts\n        )\n        if result.state == BindState.SUCCESSFUL_BOUND:\n            return to_json_response(response), HTTPStatus.CREATED\n        elif result.state == BindState.IDENTICAL_ALREADY_EXISTS:\n            return to_json_response(response), HTTPStatus.OK\n        else:\n            raise errors.ServiceException('IllegalState, BindState unknown.')\n\n    @openbroker.route(\"/v2/service_instances/<instance_id>/service_bindings/<binding_id>\", methods=['DELETE'])\n    def unbind(instance_id, binding_id):\n        try:\n            plan_id = request.args[\"plan_id\"]\n            service_id = request.args[\"service_id\"]\n\n            unbind_details = UnbindDetails(plan_id, service_id)\n            unbind_details.originating_identity = request.originating_identity\n            unbind_details.authorization_username = extract_authorization_username(request)\n            broker = get_broker_by_id(unbind_details.service_id)\n            if not broker.check_plan_id(unbind_details.plan_id):\n                raise TypeError('plan_id not found in this service.')\n        except (TypeError, KeyError) as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(description=str(e))), HTTPStatus.BAD_REQUEST\n\n        try:\n            broker.unbind(instance_id, binding_id, unbind_details)\n        except errors.ErrBindingDoesNotExist as e:\n            logger.exception(e)\n            return to_json_response(EmptyResponse()), HTTPStatus.GONE\n\n        return to_json_response(EmptyResponse()), HTTPStatus.OK\n\n    @openbroker.route(\"/v2/service_instances/<instance_id>\", methods=['DELETE'])\n    def deprovision(instance_id):\n        try:\n            plan_id = request.args[\"plan_id\"]\n            service_id = request.args[\"service_id\"]\n            accepts_incomplete = 'true' == request.args.get(\"accepts_incomplete\", 'false')\n\n            deprovision_details = DeprovisionDetails(plan_id, service_id)\n            deprovision_details.originating_identity = request.originating_identity\n            deprovision_details.authorization_username = extract_authorization_username(request)\n            broker = get_broker_by_id(deprovision_details.service_id)\n            if not broker.check_plan_id(deprovision_details.plan_id):\n                raise TypeError('plan_id not found in this service.')\n        except (TypeError, KeyError) as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(description=str(e))), HTTPStatus.BAD_REQUEST\n\n        try:\n            result = broker.deprovision(instance_id, deprovision_details, accepts_incomplete)\n            add_service_id_to_async_response(result, broker.service_id())\n        except errors.ErrInstanceDoesNotExist as e:\n            logger.exception(e)\n            return to_json_response(EmptyResponse()), HTTPStatus.GONE\n        except errors.ErrAsyncRequired as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(\n                error=\"AsyncRequired\",\n                description=\"This service plan requires client support for asynchronous service operations.\"\n            )), HTTPStatus.UNPROCESSABLE_ENTITY\n\n        if result.is_async:\n            return to_json_response(DeprovisionResponse(result.operation)), HTTPStatus.ACCEPTED\n        else:\n            return to_json_response(EmptyResponse()), HTTPStatus.OK\n\n    @openbroker.route(\"/v2/service_instances/<instance_id>/last_operation\", methods=['GET'])\n    def last_operation(instance_id):\n        # Not required\n        # service_id = request.args.get(\"service_id\", None)\n        # plan_id = request.args.get(\"plan_id\", None)\n\n        operation_data = request.args.get(\"operation\", None)\n        data = operation_data.split(' ', maxsplit=1)\n        service_id = data[0]\n        if len(data) == 2:\n            operation_data = data[1]\n        else:\n            operation_data = None\n\n        try:\n            broker = get_broker_by_id(service_id)\n        except KeyError as e:\n            logger.exception(e)\n            return to_json_response(ErrorResponse(description=str(e))), HTTPStatus.BAD_REQUEST\n\n        result = broker.last_operation(instance_id, operation_data)\n        return to_json_response(LastOperationResponse(result.state, result.description)), HTTPStatus.OK\n\n    return openbroker"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting Flask with the given brokers.", "response": "def serve(service_brokers: Union[List[ServiceBroker], ServiceBroker],\n          credentials: Union[List[BrokerCredentials], BrokerCredentials, None],\n          logger: logging.Logger = logging.root,\n          port=5000,\n          debug=False):\n    \"\"\"\n    Starts flask with the given brokers.\n    You can provide a list or just one ServiceBroker\n\n    :param service_brokers: ServicesBroker for services to provide\n    :param credentials: Username and password that will be required to communicate with service broker\n    :param logger: Used for api logs. This will not influence Flasks logging behavior\n    :param port: Port\n    :param debug: Enables debugging in flask app\n    \"\"\"\n\n    from gevent.pywsgi import WSGIServer\n    from flask import Flask\n    app = Flask(__name__)\n    app.debug = debug\n\n    blueprint = get_blueprint(service_brokers, credentials, logger)\n\n    logger.debug(\"Register openbrokerapi blueprint\")\n    app.register_blueprint(blueprint)\n\n    logger.info(\"Start Flask on 0.0.0.0:%s\" % port)\n    http_server = WSGIServer(('0.0.0.0', port), app)\n    http_server.serve_forever()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_originating_identity():\n    from flask import request, json\n    if \"X-Broker-API-Originating-Identity\" in request.headers:\n        try:\n            platform, value = request.headers[\"X-Broker-API-Originating-Identity\"].split(None, 1)\n            request.originating_identity = {\n                'platform': platform,\n                'value': json.loads(base64.standard_b64decode(value))\n            }\n        except ValueError as e:\n            return to_json_response(ErrorResponse(\n                description='Improper \"X-Broker-API-Originating-Identity\" header. ' + str(e))\n            ), HTTPStatus.BAD_REQUEST\n    else:\n        request.originating_identity = None", "response": "Check and decode the X - Broker - API - Originating - Identity header and set the request. originating_identity field to the object that is returned by the servicebroker API."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multi_ping(dest_addrs, timeout, retry=0, ignore_lookup_errors=False):\n    retry = int(retry)\n    if retry < 0:\n        retry = 0\n\n    timeout = float(timeout)\n    if timeout < 0.1:\n        raise MultiPingError(\"Timeout < 0.1 seconds not allowed\")\n\n    retry_timeout = float(timeout) / (retry + 1)\n    if retry_timeout < 0.1:\n        raise MultiPingError(\"Time between ping retries < 0.1 seconds\")\n\n    mp = MultiPing(dest_addrs, ignore_lookup_errors=ignore_lookup_errors)\n\n    results = {}\n    retry_count = 0\n    while retry_count <= retry:\n        # Send a batch of pings\n        mp.send()\n        single_results, no_results = mp.receive(retry_timeout)\n        # Add the results from the last sending of pings to the overall results\n        results.update(single_results)\n        if not no_results:\n            # No addresses left? We are done.\n            break\n        retry_count += 1\n\n    return results, no_results", "response": "Send and receive packets to a set of addresses and return a list of the results."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _open_icmp_socket(self, family):\n        try:\n            proto = socket.IPPROTO_ICMP if family == socket.AF_INET \\\n                    else _IPPROTO_ICMPV6\n\n            return socket.socket(family, socket.SOCK_RAW, proto)\n\n        except socket.error as e:\n            if e.errno == 1:\n                raise MultiPingError(\"Root privileges required for sending \"\n                                     \"ICMP\")\n            # Re-raise any other error\n            raise", "response": "Opens a socket suitable for sending and receiving ICMP messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the checksum of a message.", "response": "def _checksum(self, msg):\n        \"\"\"\n        Calculate the checksum of a packet.\n\n        This is inspired by a response on StackOverflow here:\n        https://stackoverflow.com/a/1769267/7242672\n\n        Thank you to StackOverflow user Jason Orendorff.\n\n        \"\"\"\n        def carry_around_add(a, b):\n            c = a + b\n            return (c & 0xffff) + (c >> 16)\n\n        s = 0\n        for i in range(0, len(msg), 2):\n            w = (msg[i] << 8) + msg[i + 1]\n            s = carry_around_add(s, w)\n        s = ~s & 0xffff\n\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _send_ping(self, dest_addr, payload):\n        pkt_id = self._last_used_id\n\n        is_ipv6 = ':' in dest_addr\n        if is_ipv6:\n            self._ipv6_address_present = True\n            icmp_echo_request = _ICMPV6_ECHO_REQUEST\n        else:\n            icmp_echo_request = _ICMP_ECHO_REQUEST\n\n        # For checksum calculation we require a dummy header, with the checksum\n        # field set to zero. This header consists of:\n        # - ICMP type = 8 (v4) / 128 (v6) (unsigned byte)\n        # - ICMP code = 0 (unsigned byte)\n        # - checksum  = 0 (unsigned short)\n        # - packet id     (unsigned short)\n        # - sequence  = 0 (unsigned short)  This doesn't have to be 0.\n        dummy_header = bytearray(\n                            struct.pack(_ICMP_HDR_PACK_FORMAT,\n                                        icmp_echo_request, 0, 0,\n                                        pkt_id, self.ident))\n\n        # Calculate the checksum over the combined dummy header and payload\n        checksum = self._checksum(dummy_header + payload)\n\n        # We can now create the real header, which contains the correct\n        # checksum. Need to make sure to convert checksum to network byte\n        # order.\n        real_header = bytearray(\n                            struct.pack(_ICMP_HDR_PACK_FORMAT,\n                                        icmp_echo_request, 0, checksum,\n                                        pkt_id, self.ident))\n\n        # Full packet consists of header plus payload\n        full_pkt = real_header + payload\n\n        # The full address for a sendto operation consists of the IP address\n        # and a port. We don't really need a port for ICMP, so we just use 0\n        # for that.\n        full_dest_addr = (dest_addr, 0)\n\n        if is_ipv6:\n            socket.inet_pton(socket.AF_INET6, dest_addr)\n            try:\n                self._sock6.sendto(full_pkt, full_dest_addr)\n            except Exception:\n                # on systems without IPv6 connectivity, sendto will fail with\n                # 'No route to host'\n                pass\n        else:\n            self._sock.sendto(full_pkt, full_dest_addr)", "response": "Send a single ICMP echo packet to the specified destination."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self):\n        # Collect all the addresses for which we have not seen responses yet.\n        if not self._receive_has_been_called:\n            all_addrs = self._dest_addrs\n        else:\n            all_addrs = [a for (i, a) in list(self._id_to_addr.items())\n                         if i in self._remaining_ids]\n\n        if self._last_used_id is None:\n            # Will attempt to continue at the last request ID we used. But if\n            # we never sent anything before then we create a first ID\n            # 'randomly' from the current time. ID is only a 16 bit field, so\n            # need to trim it down.\n            self._last_used_id = int(time.time()) & 0xffff\n\n        # Send ICMPecho to all addresses...\n        for addr in all_addrs:\n            # Make a unique ID, wrapping around at 65535.\n            self._last_used_id = (self._last_used_id + 1) & 0xffff\n            # Remember the address for each ID so we can produce meaningful\n            # result lists later on.\n            self._id_to_addr[self._last_used_id] = addr\n            # Send an ICMPecho request packet. We specify a payload consisting\n            # of the current time stamp. This is returned to us in the\n            # response and allows us to calculate the 'ping time'.\n            self._send_ping(addr, payload=struct.pack(\"d\", time.time()))", "response": "Send ICMPecho requests to all addresses."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads all packets from the socket and return a list of tuples. Each tuple contains a packet and the time at which it was received.", "response": "def _read_all_from_socket(self, timeout):\n        \"\"\"\n        Read all packets we currently can on the socket.\n\n        Returns list of tuples. Each tuple contains a packet and the time at\n        which it was received. NOTE: The receive time is the time when our\n        recv() call returned, which greatly depends on when it was called. The\n        time is NOT the time at which the packet arrived at our host, but it's\n        the closest we can come to the real ping time.\n\n        If nothing was received within the timeout time, the return list is\n        empty.\n\n        First read is blocking with timeout, so we'll wait at least that long.\n        Then, in case any more packets have arrived, we read everything we can\n        from the socket in non-blocking mode.\n\n        \"\"\"\n        pkts = []\n        try:\n            self._sock.settimeout(timeout)\n            while True:\n                p = self._sock.recv(64)\n                # Store the packet and the current time\n                pkts.append((bytearray(p), time.time()))\n                # Continue the loop to receive any additional packets that\n                # may have arrived at this point. Changing the socket to\n                # non-blocking (by setting the timeout to 0), so that we'll\n                # only continue the loop until all current packets have been\n                # read.\n                self._sock.settimeout(0)\n        except socket.timeout:\n            # In the first blocking read with timout, we may not receive\n            # anything. This is not an error, it just means no data was\n            # available in the specified time.\n            pass\n        except socket.error as e:\n            # When we read in non-blocking mode, we may get this error with\n            # errno 11 to indicate that no more data is available. That's ok,\n            # just like the timeout.\n            if e.errno == errno.EWOULDBLOCK:\n                pass\n            else:\n                # We're not expecting any other socket exceptions, so we\n                # re-raise in that case.\n                raise\n\n        if self._ipv6_address_present:\n            try:\n                self._sock6.settimeout(timeout)\n                while True:\n                    p = self._sock6.recv(128)\n                    pkts.append((bytearray(p), time.time()))\n                    self._sock6.settimeout(0)\n            except socket.timeout:\n                pass\n            except socket.error as e:\n                if e.errno == errno.EWOULDBLOCK:\n                    pass\n                else:\n                    raise\n\n        return pkts"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreceives ping responses from the socket.", "response": "def receive(self, timeout):\n        \"\"\"\n        Receive ping responses from the socket. Attempts to read responses for\n        all stored IDs (as generated by send()).\n\n        Returns a tuple with a dict and a list:\n\n        - Dict contains IP addresses for which we received a response and the\n          time\n        - List contains IP addresses for which we have not received a response,\n          yet\n\n        \"\"\"\n        if not self._id_to_addr:\n            raise MultiPingError(\"No requests have been sent, yet.\")\n\n        self._receive_has_been_called = True\n\n        # Continue with any remaining IDs for which we hadn't received an\n        # answer, yet...\n        if self._remaining_ids is None:\n            # ... but if we don't have any stored yet, then we are just calling\n            # receive() for the first time afer a send. We initialize\n            # the list of expected IDs from all the IDs we created during the\n            # send().\n            self._remaining_ids = list(self._id_to_addr.keys())\n\n        if len(self._remaining_ids) == 0:\n            raise MultiPingError(\"No responses pending\")\n\n        remaining_time = timeout\n        results        = {}\n\n        # Keep looping until we either have responses for all request IDs, or\n        # no more time is left.\n        while self._remaining_ids and remaining_time > 0:\n            start_time = time.time()\n            pkts = self._read_all_from_socket(remaining_time)\n\n            for pkt, resp_receive_time in pkts:\n                # Extract the ICMP ID of the response\n\n                try:\n                    pkt_id = None\n                    pkt_ident = None\n                    if pkt[_ICMPV6_HDR_OFFSET] == _ICMPV6_ECHO_REPLY:\n\n                        pkt_id = (pkt[_ICMPV6_ID_OFFSET] << 8) + \\\n                            pkt[_ICMPV6_ID_OFFSET + 1]\n                        pkt_ident = (pkt[_ICMPV6_IDENT_OFFSET] << 8) + \\\n                            pkt[_ICMPV6_IDENT_OFFSET + 1]\n                        payload = pkt[_ICMPV6_PAYLOAD_OFFSET:]\n\n                    elif pkt[_ICMP_HDR_OFFSET] == _ICMP_ECHO_REPLY:\n\n                        pkt_id = (pkt[_ICMP_ID_OFFSET] << 8) + \\\n                            pkt[_ICMP_ID_OFFSET + 1]\n                        pkt_ident = (pkt[_ICMP_IDENT_OFFSET] << 8) + \\\n                            pkt[_ICMP_IDENT_OFFSET + 1]\n                        payload = pkt[_ICMP_PAYLOAD_OFFSET:]\n\n                    if pkt_ident == self.ident and \\\n                       pkt_id in self._remaining_ids:\n                        # The sending timestamp was encoded in the echo request\n                        # body and is now returned to us in the response. Note\n                        # that network byte order doesn't matter here, since we\n                        # get exactly the order of bytes back that we\n                        # originally sent from this host.\n                        req_sent_time = struct.unpack(\n                            \"d\", payload[:self._time_stamp_size])[0]\n                        results[self._id_to_addr[pkt_id]] = \\\n                            resp_receive_time - req_sent_time\n\n                        self._remaining_ids.remove(pkt_id)\n                except IndexError:\n                    # Silently ignore malformed packets\n                    pass\n\n            # Calculate how much of the available overall timeout time is left\n            end_time = time.time()\n            remaining_time = remaining_time - (end_time - start_time)\n\n        no_results_so_far = [self._id_to_addr[i] for i in self._remaining_ids]\n        if self._ignore_lookup_errors:\n            # With this flag set, names/addresses that we couldn't look up will\n            # just be added to the no-results return list. Without the flag\n            # those addresses would have caused an exception earlier.\n            no_results_so_far.extend(self._unprocessed_targets)\n        return (results, no_results_so_far)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting current state from the Midas gas detector.", "response": "async def get(self):\n        \"\"\"Get current state from the Midas gas detector.\"\"\"\n        try:\n            return self._parse(await self.read_registers(0, 16))\n        except TimeoutError:\n            return {'ip': self.ip, 'connected': False}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the response from the server into a dictionary.", "response": "def _parse(self, registers):\n        \"\"\"Parse the response, returning a dictionary.\"\"\"\n        result = {'ip': self.ip, 'connected': True}\n        decoder = BinaryPayloadDecoder.fromRegisters(registers,\n                                                     byteorder=Endian.Big,\n                                                     wordorder=Endian.Little)\n        # Register 40001 is a collection of alarm status signals\n        b = [decoder.decode_bits(), decoder.decode_bits()]\n        reg_40001 = b[1] + b[0]\n        # Bits 0-3 map to the monitor state\n        monitor_integer = sum(1 << i for i, b in enumerate(reg_40001[:4]) if b)\n        result['state'] = options['monitor state'][monitor_integer]\n        # Bits 4-5 map to fault status\n        fault_integer = sum(1 << i for i, b in enumerate(reg_40001[4:6]) if b)\n        result['fault'] = {'status': options['fault status'][fault_integer]}\n        # Bits 6 and 7 tell if low and high alarms are active\n        low, high = reg_40001[6:8]\n        result['alarm'] = options['alarm level'][low + high]\n        # Bits 8-10 tell if internal sensor relays 1-3 are energized. Skipping.\n        # Bit 11 is a heartbeat bit that toggles every two seconds. Skipping.\n        # Bit 12 tells if relays are under modbus control. Skipping.\n        # Remaining bits are empty. Skipping.\n        # Register 40002 has a gas ID and a sensor cartridge ID. Skipping.\n        decoder._pointer += 2\n        # Registers 40003-40004 are the gas concentration as a float\n        result['concentration'] = decoder.decode_32bit_float()\n        # Register 40005 is the concentration as an int. Skipping.\n        decoder._pointer += 2\n        # Register 40006 is the number of the most important fault.\n        fault_number = decoder.decode_16bit_uint()\n        if fault_number != 0:\n            code = ('m' if fault_number < 30 else 'F') + str(fault_number)\n            result['fault']['code'] = code\n            result['fault'].update(faults[code])\n        # Register 40007 holds the concentration unit in the second byte\n        # Instead of being an int, it's the position of the up bit\n        unit_bit = decoder.decode_bits().index(True)\n        result['units'] = options['concentration unit'][unit_bit]\n        decoder._pointer += 1\n        # Register 40008 holds the sensor temperature in Celsius\n        result['temperature'] = decoder.decode_16bit_int()\n        # Register 40009 holds number of hours remaining in cell life\n        result['life'] = decoder.decode_16bit_uint() / 24.0\n        # Register 40010 holds the number of heartbeats (16 LSB). Skipping.\n        decoder._pointer += 2\n        # Register 40011 is the sample flow rate in cc / min\n        result['flow'] = decoder.decode_16bit_uint()\n        # Register 40012 is blank. Skipping.\n        decoder._pointer += 2\n        # Registers 40013-40016 are the alarm concentration thresholds\n        result['low-alarm threshold'] = round(decoder.decode_32bit_float(), 6)\n        result['high-alarm threshold'] = round(decoder.decode_32bit_float(), 6)\n        # Despite what the manual says, thresholds are always reported in ppm.\n        # Let's fix that to match the concentration units.\n        if result['units'] == 'ppb':\n            result['concentration'] *= 1000\n            result['low-alarm threshold'] *= 1000\n            result['high-alarm threshold'] *= 1000\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts asynchronous reconnect loop.", "response": "async def _connect(self):\n        \"\"\"Start asynchronous reconnect loop.\"\"\"\n        self.waiting = True\n        await self.client.start(self.ip)\n        self.waiting = False\n        if self.client.protocol is None:\n            raise IOError(\"Could not connect to '{}'.\".format(self.ip))\n        self.open = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread modbus registers. The Modbus protocol doesn't allow responses longer than 250 bytes (ie. 125 registers, 62 DF addresses), which this function manages by chunking larger requests.", "response": "async def read_registers(self, address, count):\n        \"\"\"Read modbus registers.\n\n        The Modbus protocol doesn't allow responses longer than 250 bytes\n        (ie. 125 registers, 62 DF addresses), which this function manages by\n        chunking larger requests.\n        \"\"\"\n        registers = []\n        while count > 124:\n            r = await self._request('read_holding_registers', address, 124)\n            registers += r.registers\n            address, count = address + 124, count - 124\n        r = await self._request('read_holding_registers', address, count)\n        registers += r.registers\n        return registers"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a modbus register.", "response": "async def write_register(self, address, value, skip_encode=False):\n        \"\"\"Write a modbus register.\"\"\"\n        await self._request('write_registers', address, value, skip_encode=skip_encode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite modbus registers. The Modbus protocol doesn't allow requests longer than 250 bytes (ie. 125 registers, 62 DF addresses), which this function manages by chunking larger requests.", "response": "async def write_registers(self, address, values, skip_encode=False):\n        \"\"\"Write modbus registers.\n\n        The Modbus protocol doesn't allow requests longer than 250 bytes\n        (ie. 125 registers, 62 DF addresses), which this function manages by\n        chunking larger requests.\n        \"\"\"\n        while len(values) > 62:\n            await self._request('write_registers',\n                                address, values, skip_encode=skip_encode)\n            address, values = address + 124, values[62:]\n        await self._request('write_registers',\n                            address, values, skip_encode=skip_encode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _request(self, method, *args, **kwargs):\n        if not self.open:\n            await self._connect()\n        while self.waiting:\n            await asyncio.sleep(0.1)\n        if self.client.protocol is None or not self.client.protocol.connected:\n            raise TimeoutError(\"Not connected to device.\")\n        try:\n            future = getattr(self.client.protocol, method)(*args, **kwargs)\n        except AttributeError:\n            raise TimeoutError(\"Not connected to device.\")\n        self.waiting = True\n        try:\n            return await asyncio.wait_for(future, timeout=self.timeout)\n        except asyncio.TimeoutError as e:\n            if self.open:\n                # This came from reading through the pymodbus@python3 source\n                # Problem was that the driver was not detecting disconnect\n                if hasattr(self, 'modbus'):\n                    self.client.protocol_lost_connection(self.modbus)\n                self.open = False\n            raise TimeoutError(e)\n        except pymodbus.exceptions.ConnectionException as e:\n            raise ConnectionError(e)\n        finally:\n            self.waiting = False", "response": "Send a request to the device and awaits a response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _close(self):\n        self.client.stop()\n        self.open = False\n        self.waiting = False", "response": "Close the TCP connection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef command_line():\n    import argparse\n    import asyncio\n    import json\n\n    parser = argparse.ArgumentParser(description=\"Read a Honeywell Midas gas \"\n                                     \"detector state from the command line.\")\n    parser.add_argument('address', help=\"The IP address of the gas detector.\")\n    args = parser.parse_args()\n\n    async def get():\n        async with GasDetector(args.address) as detector:\n            print(json.dumps(await detector.get(), indent=4, sort_keys=True))\n\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(get())\n    loop.close()", "response": "Command - line tool for Midas gas detector communication."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the Frechet Distance between two multivariate Gaussians mu1 and mu2 and sigma1 and sigma2.", "response": "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n            \n    Stable version by Dougal J. Sutherland.\n\n    Params:\n    -- mu1:    The mean of the activations of preultimate layer of the\n               CHEMNET ( like returned by the function 'get_predictions')\n               for generated samples.\n    -- mu2:    The mean of the activations of preultimate layer of the\n               CHEMNET ( like returned by the function 'get_predictions')\n               for real samples.\n    -- sigma1: The covariance matrix of the activations of preultimate layer of the\n               CHEMNET ( like returned by the function 'get_predictions')\n               for generated samples.\n    -- sigma2: The covariance matrix of the activations of preultimate layer of the\n               CHEMNET ( like returned by the function 'get_predictions')\n               for real samples.\n\n    Returns:\n    --   : The Frechet Distance.\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n\n    diff = mu1 - mu2\n\n    # product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(\"Imaginary component {}\".format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_masked_loss(loss_function, mask_value):\n\n    def masked_loss_function(y_true, y_pred):\n        mask = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n        return loss_function(y_true * mask, y_pred * mask)\n\n    return masked_loss_function", "response": "Builds a loss function that masks based on targets\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetRealPath(filename):\n  if os.path.isabs(filename):                # already absolute\n    return filename\n\n  if filename.startswith('./') or  filename.startswith('../'): # relative\n    return os.path.abspath(filename)\n\n  path = os.getenv('PATH', '')\n  for directory in path.split(':'):\n    tryname = os.path.join(directory, filename)\n    if os.path.exists(tryname):\n      if not os.path.isabs(directory):  # relative directory\n        return os.path.abspath(tryname)\n      return tryname\n  if os.path.exists(filename):\n    return os.path.abspath(filename)\n  return None", "response": "Given an executable filename find in the PATH or find an absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Run(self):\n    if not self.executable:\n      logging.error('Could not locate \"%s\"' % self.long_name)\n      return 0\n\n    finfo = os.stat(self.executable)\n    self.date = time.localtime(finfo[stat.ST_MTIME])\n\n    logging.info('Running: %s %s </dev/null 2>&1'\n                 % (self.executable, FLAGS.help_flag))\n    # --help output is often routed to stderr, so we combine with stdout.\n    # Re-direct stdin to /dev/null to encourage programs that\n    # don't understand --help to exit.\n    (child_stdin, child_stdout_and_stderr) = os.popen4(\n      [self.executable, FLAGS.help_flag])\n    child_stdin.close()       # '</dev/null'\n    self.output = child_stdout_and_stderr.readlines()\n    child_stdout_and_stderr.close()\n    if len(self.output) < _MIN_VALID_USAGE_MSG:\n      logging.error('Error: \"%s %s\" returned only %d lines: %s'\n                    % (self.name, FLAGS.help_flag,\n                       len(self.output), self.output))\n      return 0\n    return 1", "response": "Runs the command and returns the output."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the initial description of the flags section.", "response": "def ParseDesc(self, start_line=0):\n    \"\"\"Parse the initial description.\n\n    This could be Python or C++.\n\n    Returns:\n      (start_line, lang_type)\n        start_line  Line to start parsing flags on (int)\n        lang_type   Either 'python' or 'c'\n       (-1, '')  if the flags start could not be found\n    \"\"\"\n    exec_mod_start = self.executable + ':'\n\n    after_blank = 0\n    start_line = 0             # ignore the passed-in arg for now (?)\n    for start_line in range(start_line, len(self.output)): # collect top description\n      line = self.output[start_line].rstrip()\n      # Python flags start with 'flags:\\n'\n      if ('flags:' == line\n          and len(self.output) > start_line+1\n          and '' == self.output[start_line+1].rstrip()):\n        start_line += 2\n        logging.debug('Flags start (python): %s' % line)\n        return (start_line, 'python')\n      # SWIG flags just have the module name followed by colon.\n      if exec_mod_start == line:\n        logging.debug('Flags start (swig): %s' % line)\n        return (start_line, 'python')\n      # C++ flags begin after a blank line and with a constant string\n      if after_blank and line.startswith('  Flags from '):\n        logging.debug('Flags start (c): %s' % line)\n        return (start_line, 'c')\n      # java flags begin with a constant string\n      if line == 'where flags are':\n        logging.debug('Flags start (java): %s' % line)\n        start_line += 2                        # skip \"Standard flags:\"\n        return (start_line, 'java')\n\n      logging.debug('Desc: %s' % line)\n      self.desc.append(line)\n      after_blank = (line == '')\n    else:\n      logging.warn('Never found the start of the flags section for \"%s\"!'\n                   % self.long_name)\n      return (-1, '')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse python style flags.", "response": "def ParsePythonFlags(self, start_line=0):\n    \"\"\"Parse python/swig style flags.\"\"\"\n    modname = None                      # name of current module\n    modlist = []\n    flag = None\n    for line_num in range(start_line, len(self.output)): # collect flags\n      line = self.output[line_num].rstrip()\n      if not line:                      # blank\n        continue\n\n      mobj = self.module_py_re.match(line)\n      if mobj:                          # start of a new module\n        modname = mobj.group(1)\n        logging.debug('Module: %s' % line)\n        if flag:\n          modlist.append(flag)\n        self.module_list.append(modname)\n        self.modules.setdefault(modname, [])\n        modlist = self.modules[modname]\n        flag = None\n        continue\n\n      mobj = self.flag_py_re.match(line)\n      if mobj:                          # start of a new flag\n        if flag:\n          modlist.append(flag)\n        logging.debug('Flag: %s' % line)\n        flag = Flag(mobj.group(1),  mobj.group(2))\n        continue\n\n      if not flag:                    # continuation of a flag\n        logging.error('Flag info, but no current flag \"%s\"' % line)\n      mobj = self.flag_default_py_re.match(line)\n      if mobj:                          # (default: '...')\n        flag.default = mobj.group(1)\n        logging.debug('Fdef: %s' % line)\n        continue\n      mobj = self.flag_tips_py_re.match(line)\n      if mobj:                          # (tips)\n        flag.tips = mobj.group(1)\n        logging.debug('Ftip: %s' % line)\n        continue\n      if flag and flag.help:\n        flag.help += line              # multiflags tack on an extra line\n      else:\n        logging.info('Extra: %s' % line)\n    if flag:\n      modlist.append(flag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses C style flags.", "response": "def ParseCFlags(self, start_line=0):\n    \"\"\"Parse C style flags.\"\"\"\n    modname = None                      # name of current module\n    modlist = []\n    flag = None\n    for line_num in range(start_line, len(self.output)):  # collect flags\n      line = self.output[line_num].rstrip()\n      if not line:                      # blank lines terminate flags\n        if flag:                        # save last flag\n          modlist.append(flag)\n          flag = None\n        continue\n\n      mobj = self.module_c_re.match(line)\n      if mobj:                          # start of a new module\n        modname = mobj.group(1)\n        logging.debug('Module: %s' % line)\n        if flag:\n          modlist.append(flag)\n        self.module_list.append(modname)\n        self.modules.setdefault(modname, [])\n        modlist = self.modules[modname]\n        flag = None\n        continue\n\n      mobj = self.flag_c_re.match(line)\n      if mobj:                          # start of a new flag\n        if flag:                        # save last flag\n          modlist.append(flag)\n        logging.debug('Flag: %s' % line)\n        flag = Flag(mobj.group(1),  mobj.group(2))\n        continue\n\n      # append to flag help.  type and default are part of the main text\n      if flag:\n        flag.help += ' ' + line.strip()\n      else:\n        logging.info('Extra: %s' % line)\n    if flag:\n      modlist.append(flag)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseJavaFlags(self, start_line=0):\n    # The java flags prints starts with a \"Standard flags\" \"module\"\n    # that doesn't follow the standard module syntax.\n    modname = 'Standard flags'          # name of current module\n    self.module_list.append(modname)\n    self.modules.setdefault(modname, [])\n    modlist = self.modules[modname]\n    flag = None\n\n    for line_num in range(start_line, len(self.output)): # collect flags\n      line = self.output[line_num].rstrip()\n      logging.vlog(2, 'Line: \"%s\"' % line)\n      if not line:                      # blank lines terminate module\n        if flag:                        # save last flag\n          modlist.append(flag)\n          flag = None\n        continue\n\n      mobj = self.module_java_re.match(line)\n      if mobj:                          # start of a new module\n        modname = mobj.group(1)\n        logging.debug('Module: %s' % line)\n        if flag:\n          modlist.append(flag)\n        self.module_list.append(modname)\n        self.modules.setdefault(modname, [])\n        modlist = self.modules[modname]\n        flag = None\n        continue\n\n      mobj = self.flag_java_re.match(line)\n      if mobj:                          # start of a new flag\n        if flag:                        # save last flag\n          modlist.append(flag)\n        logging.debug('Flag: %s' % line)\n        flag = Flag(mobj.group(1),  mobj.group(2))\n        continue\n\n      # append to flag help.  type and default are part of the main text\n      if flag:\n        flag.help += ' ' + line.strip()\n      else:\n        logging.info('Extra: %s' % line)\n    if flag:\n      modlist.append(flag)", "response": "Parse Java style flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Filter(self):\n    if not self.desc:\n      self.short_desc = ''\n      return\n\n    for i in range(len(self.desc)):   # replace full path with name\n      if self.desc[i].find(self.executable) >= 0:\n        self.desc[i] = self.desc[i].replace(self.executable, self.name)\n\n    self.short_desc = self.desc[0]\n    word_list = self.short_desc.split(' ')\n    all_names = [ self.name, self.short_name, ]\n    # Since the short_desc is always listed right after the name,\n    #  trim it from the short_desc\n    while word_list and (word_list[0] in all_names\n                         or word_list[0].lower() in all_names):\n      del word_list[0]\n      self.short_desc = ''              # signal need to reconstruct\n    if not self.short_desc and word_list:\n      self.short_desc = ' '.join(word_list)", "response": "Filter parsed data to create derived fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noutputting all sections of the page.", "response": "def Output(self):\n    \"\"\"Output all sections of the page.\"\"\"\n    self.Open()\n    self.Header()\n    self.Body()\n    self.Footer()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the module that s calling into this module.", "response": "def GetCallingModuleObjectAndName():\n  \"\"\"Returns the module that's calling into this module.\n\n  We generally use this function to get the name of the module calling a\n  DEFINE_foo... function.\n\n  Returns:\n    The module object that called into this one.\n\n  Raises:\n    AssertionError: if no calling module could be identified.\n  \"\"\"\n  range_func = range if sys.version_info[0] >= 3 else xrange\n  for depth in range_func(1, sys.getrecursionlimit()):\n    # sys._getframe is the right thing to use here, as it's the best\n    # way to walk up the call stack.\n    globals_for_frame = sys._getframe(depth).f_globals  # pylint: disable=protected-access\n    module, module_name = GetModuleObjectAndName(globals_for_frame)\n    if id(module) not in disclaim_module_ids and module_name is not None:\n      return _ModuleObjectAndName(module, module_name)\n  raise AssertionError('No module was found')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetFlagSuggestions(attempt, longopt_list):\n  # Don't suggest on very short strings, or if no longopts are specified.\n  if len(attempt) <= 2 or not longopt_list:\n    return []\n\n  option_names = [v.split('=')[0] for v in longopt_list]\n\n  # Find close approximations in flag prefixes.\n  # This also handles the case where the flag is spelled right but ambiguous.\n  distances = [(_DamerauLevenshtein(attempt, option[0:len(attempt)]), option)\n               for option in option_names]\n  distances.sort(key=lambda t: t[0])\n\n  least_errors, _ = distances[0]\n  # Don't suggest excessively bad matches.\n  if least_errors >= _SUGGESTION_ERROR_RATE_THRESHOLD * len(attempt):\n    return []\n\n  suggestions = []\n  for errors, name in distances:\n    if errors == least_errors:\n      suggestions.append(name)\n    else:\n      break\n  return suggestions", "response": "Get helpful similar matches for an invalid flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a dictionary of values into process call parameters.", "response": "def FlagDictToArgs(flag_map):\n  \"\"\"Convert a dict of values into process call parameters.\n\n  This method is used to convert a dictionary into a sequence of parameters\n  for a binary that parses arguments using this module.\n\n  Args:\n    flag_map: a mapping where the keys are flag names (strings).\n      values are treated according to their type:\n      * If value is None, then only the name is emitted.\n      * If value is True, then only the name is emitted.\n      * If value is False, then only the name prepended with 'no' is emitted.\n      * If value is a string then --name=value is emitted.\n      * If value is a collection, this will emit --name=value1,value2,value3.\n      * Everything else is converted to string an passed as such.\n  Yields:\n    sequence of string suitable for a subprocess execution.\n  \"\"\"\n  for key, value in six.iteritems(flag_map):\n    if value is None:\n      yield '--%s' % key\n    elif isinstance(value, bool):\n      if value:\n        yield '--%s' % key\n      else:\n        yield '--no%s' % key\n    elif isinstance(value, (bytes, type(u''))):\n      # We don't want strings to be handled like python collections.\n      yield '--%s=%s' % (key, value)\n    else:\n      # Now we attempt to deal with collections.\n      try:\n        yield '--%s=%s' % (key, ','.join(str(item) for item in value))\n      except TypeError:\n        # Default case.\n        yield '--%s=%s' % (key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a __doc__ string and reformats it as help.", "response": "def DocToHelp(doc):\n  \"\"\"Takes a __doc__ string and reformats it as help.\"\"\"\n\n  # Get rid of starting and ending white space. Using lstrip() or even\n  # strip() could drop more than maximum of first line and right space\n  # of last line.\n  doc = doc.strip()\n\n  # Get rid of all empty lines.\n  whitespace_only_line = re.compile('^[ \\t]+$', re.M)\n  doc = whitespace_only_line.sub('', doc)\n\n  # Cut out common space at line beginnings.\n  doc = pep257.trim(doc)\n\n  # Just like this module's comment, comments tend to be aligned somehow.\n  # In other words they all start with the same amount of white space.\n  # 1) keep double new lines;\n  # 2) keep ws after new lines if not empty line;\n  # 3) all other new lines shall be changed to a space;\n  # Solution: Match new lines between non white space and replace with space.\n  doc = re.sub(r'(?<=\\S)\\n(?=\\S)', ' ', doc, flags=re.M)\n\n  return doc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef define_both_methods(class_name, class_dict, old_name, new_name):  # pylint: disable=invalid-name\n  assert old_name not in class_dict or new_name not in class_dict, (\n      'Class \"{}\" cannot define both \"{}\" and \"{}\" methods.'.format(\n          class_name, old_name, new_name))\n  if old_name in class_dict:\n    class_dict[new_name] = class_dict[old_name]\n  elif new_name in class_dict:\n    class_dict[old_name] = class_dict[new_name]", "response": "Function to help CamelCase to PEP8 style class methods migration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _RegisterFlagByModule(self, module_name, flag):\n    flags_by_module = self.FlagsByModuleDict()\n    flags_by_module.setdefault(module_name, []).append(flag)", "response": "Registers a specific flag by module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _RegisterFlagByModuleId(self, module_id, flag):\n    flags_by_module_id = self.FlagsByModuleIdDict()\n    flags_by_module_id.setdefault(module_id, []).append(flag)", "response": "Registers a specific flag in the flags by module ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _RegisterKeyFlagForModule(self, module_name, flag):\n    key_flags_by_module = self.KeyFlagsByModuleDict()\n    # The list of key flags for the module named module_name.\n    key_flags = key_flags_by_module.setdefault(module_name, [])\n    # Add flag, but avoid duplicates.\n    if flag not in key_flags:\n      key_flags.append(flag)", "response": "Registers a flag that is a key flag for a Python module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking whether a Flag object is registered under long name or short name.", "response": "def _FlagIsRegistered(self, flag_obj):\n    \"\"\"Checks whether a Flag object is registered under long name or short name.\n\n    Args:\n      flag_obj: A Flag object.\n\n    Returns:\n      A boolean: True iff flag_obj is registered under long name or short name.\n    \"\"\"\n    flag_dict = self.FlagDict()\n    # Check whether flag_obj is registered under its long name.\n    name = flag_obj.name\n    if flag_dict.get(name, None) == flag_obj:\n      return True\n    # Check whether flag_obj is registered under its short name.\n    short_name = flag_obj.short_name\n    if (short_name is not None and\n        flag_dict.get(short_name, None) == flag_obj):\n      return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _CleanupUnregisteredFlagFromModuleDicts(self, flag_obj):\n    if self._FlagIsRegistered(flag_obj):\n      return\n    for flags_by_module_dict in (self.FlagsByModuleDict(),\n                                 self.FlagsByModuleIdDict(),\n                                 self.KeyFlagsByModuleDict()):\n      for flags_in_module in six.itervalues(flags_by_module_dict):\n        # While (as opposed to if) takes care of multiple occurrences of a\n        # flag in the list for the same module.\n        while flag_obj in flags_in_module:\n          flags_in_module.remove(flag_obj)", "response": "Cleanup unregistered flags from all module - level dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of flags defined by a module.", "response": "def _GetFlagsDefinedByModule(self, module):\n    \"\"\"Returns the list of flags defined by a module.\n\n    Args:\n      module: A module object or a module name (a string).\n\n    Returns:\n      A new list of Flag objects.  Caller may update this list as he\n      wishes: none of those changes will affect the internals of this\n      FlagValue object.\n    \"\"\"\n    if not isinstance(module, str):\n      module = module.__name__\n\n    return list(self.FlagsByModuleDict().get(module, []))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetKeyFlagsForModule(self, module):\n    if not isinstance(module, str):\n      module = module.__name__\n\n    # Any flag is a key flag for the module that defined it.  NOTE:\n    # key_flags is a fresh list: we can update it without affecting the\n    # internals of this FlagValues object.\n    key_flags = self._GetFlagsDefinedByModule(module)\n\n    # Take into account flags explicitly declared as key for a module.\n    for flag in self.KeyFlagsByModuleDict().get(module, []):\n      if flag not in key_flags:\n        key_flags.append(flag)\n    return key_flags", "response": "Returns the list of key flags defined by a module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch the module that defines a flag.", "response": "def FindModuleDefiningFlag(self, flagname, default=None):\n    \"\"\"Return the name of the module defining this flag, or default.\n\n    Args:\n      flagname: Name of the flag to lookup.\n      default: Value to return if flagname is not defined. Defaults\n          to None.\n\n    Returns:\n      The name of the module which registered the flag with this name.\n      If no such module exists (i.e. no flag with this name exists),\n      we return default.\n    \"\"\"\n    registered_flag = self.FlagDict().get(flagname)\n    if registered_flag is None:\n      return default\n    for module, flags in six.iteritems(self.FlagsByModuleDict()):\n      for flag in flags:\n        # It must compare the flag with the one in FlagDict. This is because a\n        # flag might be overridden only for its long name (or short name),\n        # and only its short name (or long name) is considered registered.\n        if (flag.name == registered_flag.name and\n            flag.short_name == registered_flag.short_name):\n          return module\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching the flag dictionary for a module ID that defines the given flagname.", "response": "def FindModuleIdDefiningFlag(self, flagname, default=None):\n    \"\"\"Return the ID of the module defining this flag, or default.\n\n    Args:\n      flagname: Name of the flag to lookup.\n      default: Value to return if flagname is not defined. Defaults\n          to None.\n\n    Returns:\n      The ID of the module which registered the flag with this name.\n      If no such module exists (i.e. no flag with this name exists),\n      we return default.\n    \"\"\"\n    registered_flag = self.FlagDict().get(flagname)\n    if registered_flag is None:\n      return default\n    for module_id, flags in six.iteritems(self.FlagsByModuleIdDict()):\n      for flag in flags:\n        # It must compare the flag with the one in FlagDict. This is because a\n        # flag might be overridden only for its long name (or short name),\n        # and only its short name (or long name) is considered registered.\n        if (flag.name == registered_flag.name and\n            flag.short_name == registered_flag.short_name):\n          return module_id\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _SetUnknownFlag(self, name, value):\n    setter = self.__dict__['__set_unknown']\n    if setter:\n      try:\n        setter(name, value)\n        return value\n      except (TypeError, ValueError):  # Flag value is not valid.\n        raise exceptions.IllegalFlagValueError('\"{1}\" is not valid for --{0}'\n                                               .format(name, value))\n      except NameError:  # Flag name is not valid.\n        pass\n    raise exceptions.UnrecognizedFlagError(name, value)", "response": "Sets the flag name to value if it is not set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending flags registered in another FlagValues instance to this FlagValues instance.", "response": "def AppendFlagValues(self, flag_values):\n    \"\"\"Appends flags registered in another FlagValues instance.\n\n    Args:\n      flag_values: registry to copy from\n    \"\"\"\n    for flag_name, flag in six.iteritems(flag_values.FlagDict()):\n      # Each flags with shortname appears here twice (once under its\n      # normal name, and again with its short name).  To prevent\n      # problems (DuplicateFlagError) with double flag registration, we\n      # perform a check to make sure that the entry we're looking at is\n      # for its normal name.\n      if flag_name == flag.name:\n        try:\n          self[flag_name] = flag\n        except exceptions.DuplicateFlagError:\n          raise exceptions.DuplicateFlagError.from_flag(\n              flag_name, self, other_flag_values=flag_values)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _IsUnparsedFlagAccessAllowed(self, name):\n    if _UNPARSED_FLAG_ACCESS_ENV_NAME in os.environ:\n      # We've been told explicitly what to do.\n      allow_unparsed_flag_access = (\n          os.getenv(_UNPARSED_FLAG_ACCESS_ENV_NAME) == '1')\n    elif self.__dict__['__reset_called']:\n      # Raise exception if .Reset() was called. This mostly happens in tests.\n      allow_unparsed_flag_access = False\n    elif _helpers.IsRunningTest():\n      # Staged \"rollout\", based on name of the flag so that we don't break\n      # everyone.  Hashing the flag is a way of choosing a random but\n      # consistent subset of flags to lock down which we can make larger\n      # over time.\n      name_bytes = name.encode('utf8') if not isinstance(name, bytes) else name\n      flag_percentile = (\n          struct.unpack('<I', hashlib.md5(name_bytes).digest()[:4])[0] % 100)\n      allow_unparsed_flag_access = (\n          _UNPARSED_ACCESS_DISABLED_PERCENT <= flag_percentile)\n    else:\n      allow_unparsed_flag_access = True\n    return allow_unparsed_flag_access", "response": "Determines whether unparsed flag access or not."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _AssertValidators(self, validators):\n    for validator in sorted(\n        validators, key=lambda validator: validator.insertion_index):\n      try:\n        validator.verify(self)\n      except exceptions.ValidationError as e:\n        message = validator.print_flags_with_values(self)\n        raise exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))", "response": "Assert if all validators in the list satisfy the flag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _RemoveAllFlagAppearances(self, name):\n    flag_dict = self.FlagDict()\n    if name not in flag_dict:\n      raise exceptions.UnrecognizedFlagError(name)\n    flag = flag_dict[name]\n    names_to_remove = {name}\n    names_to_remove.add(flag.name)\n    if flag.short_name:\n      names_to_remove.add(flag.short_name)\n    for n in names_to_remove:\n      self.__delattr__(n)", "response": "Removes all appearances of a flag with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SetDefault(self, name, value):\n    fl = self.FlagDict()\n    if name not in fl:\n      self._SetUnknownFlag(name, value)\n      return\n    if self.IsParsed():\n      logging.warn(\n          'FLAGS.SetDefault called on flag \"%s\" after flag parsing. Call this '\n          'method at the top level of a module to avoid overwriting the value '\n          'passed at the command line.',\n          name)\n    fl[name]._set_default(value)  # pylint: disable=protected-access\n    self._AssertValidators(fl[name].validators)", "response": "Sets the default value of the named flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ParseArgs(self, args, known_only):\n    unknown_flags, unparsed_args, undefok = [], [], set()\n\n    flag_dict = self.FlagDict()\n    args = iter(args)\n    for arg in args:\n      value = None\n\n      def GetValue():\n        # pylint: disable=cell-var-from-loop\n        try:\n          return next(args) if value is None else value\n        except StopIteration:\n          raise exceptions.Error('Missing value for flag ' + arg)\n\n      if not arg.startswith('-'):\n        # A non-argument: default is break, GNU is skip.\n        unparsed_args.append(arg)\n        if self.IsGnuGetOpt():\n          continue\n        else:\n          break\n\n      if arg == '--':\n        if known_only:\n          unparsed_args.append(arg)\n        break\n\n      if '=' in arg:\n        name, value = arg.lstrip('-').split('=', 1)\n      else:\n        name, value = arg.lstrip('-'), None\n\n      if not name:\n        # The argument is all dashes (including one dash).\n        unparsed_args.append(arg)\n        if self.IsGnuGetOpt():\n          continue\n        else:\n          break\n\n      # --undefok is a special case.\n      if name == 'undefok':\n        if known_only:\n          unparsed_args.append(arg)\n        value = GetValue()\n        undefok.update(v.strip() for v in value.split(','))\n        undefok.update('no' + v.strip() for v in value.split(','))\n        continue\n\n      flag = flag_dict.get(name)\n      if flag:\n        value = (flag.boolean and value is None) or GetValue()\n      elif name.startswith('no') and len(name) > 2:\n        # Boolean flags can take the form of --noflag, with no value.\n        noflag = flag_dict.get(name[2:])\n        if noflag and noflag.boolean:\n          if value is not None:\n            raise ValueError(arg + ' does not take an argument')\n          flag = noflag\n          value = False\n\n\n      if flag:\n        flag.parse(value)\n        flag.using_default_value = False\n      elif known_only:\n        unparsed_args.append(arg)\n      else:\n        unknown_flags.append((name, arg))\n\n    unparsed_args.extend(args)\n    return unknown_flags, unparsed_args, undefok", "response": "This function parses the arguments of a flag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Reset(self):\n    for f in self.FlagDict().values():\n      f.unparse()\n    # We log this message before marking flags as unparsed to avoid a\n    # problem when the logging library causes flags access.\n    logging.info('Reset() called; flags access will now raise errors.')\n    self.__dict__['__flags_parsed'] = False\n    self.__dict__['__reset_called'] = True", "response": "Resets the values to the point before FLAGS was called."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary that maps flag names to flag values.", "response": "def FlagValuesDict(self):\n    \"\"\"Returns: a dictionary that maps flag names to flag values.\"\"\"\n    flag_values = {}\n\n    for flag_name in self.RegisteredFlags():\n      flag = self.FlagDict()[flag_name]\n      flag_values[flag_name] = flag.value\n\n    return flag_values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetHelp(self, prefix='', include_special_flags=True):\n    # TODO(vrusinov): this function needs a test.\n    helplist = []\n\n    flags_by_module = self.FlagsByModuleDict()\n    if flags_by_module:\n      modules = sorted(flags_by_module)\n\n      # Print the help for the main module first, if possible.\n      main_module = sys.argv[0]\n      if main_module in modules:\n        modules.remove(main_module)\n        modules = [main_module] + modules\n\n      for module in modules:\n        self.__RenderOurModuleFlags(module, helplist)\n      if include_special_flags:\n        self.__RenderModuleFlags('gflags',\n                                 _helpers.SPECIAL_FLAGS.FlagDict().values(),\n                                 helplist)\n    else:\n      # Just print one long list of flags.\n      values = self.FlagDict().values()\n      if include_special_flags:\n        values.append(_helpers.SPECIAL_FLAGS.FlagDict().values())\n      self.__RenderFlagList(values, helplist, prefix)\n\n    return '\\n'.join(helplist)", "response": "Generates a help string for all known flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __RenderModuleFlags(self, module, flags, output_lines, prefix=''):\n    if not isinstance(module, str):\n      module = module.__name__\n    output_lines.append('\\n%s%s:' % (prefix, module))\n    self.__RenderFlagList(flags, output_lines, prefix + '  ')", "response": "Generates a help string for a given module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __RenderOurModuleFlags(self, module, output_lines, prefix=''):\n    flags = self._GetFlagsDefinedByModule(module)\n    if flags:\n      self.__RenderModuleFlags(module, flags, output_lines, prefix)", "response": "Generates a help string for a given module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __RenderOurModuleKeyFlags(self, module, output_lines, prefix=''):\n    key_flags = self._GetKeyFlagsForModule(module)\n    if key_flags:\n      self.__RenderModuleFlags(module, key_flags, output_lines, prefix)", "response": "Generates a help string for the key flags of a given module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the help text for a module.", "response": "def ModuleHelp(self, module):\n    \"\"\"Describe the key flags of a module.\n\n    Args:\n      module: A module object or a module name (a string).\n\n    Returns:\n      string describing the key flags of a module.\n    \"\"\"\n    helplist = []\n    self.__RenderOurModuleKeyFlags(module, helplist)\n    return '\\n'.join(helplist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __IsFlagFileDirective(self, flag_string):\n    if isinstance(flag_string, type('')):\n      if flag_string.startswith('--flagfile='):\n        return 1\n      elif flag_string == '--flagfile':\n        return 1\n      elif flag_string.startswith('-flagfile='):\n        return 1\n      elif flag_string == '-flagfile':\n        return 1\n      else:\n        return 0\n    return 0", "response": "Checks whether flag_string contains a flagfile=<foo > directive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract a filename from a flagfile string.", "response": "def ExtractFilename(self, flagfile_str):\n    \"\"\"Returns filename from a flagfile_str of form -[-]flagfile=filename.\n\n    The cases of --flagfile foo and -flagfile foo shouldn't be hitting\n    this function, as they are dealt with in the level above this\n    function.\n\n    Args:\n      flagfile_str: flagfile string.\n\n    Returns:\n      str filename from a flagfile_str of form -[-]flagfile=filename.\n\n    Raises:\n      Error: when illegal --flagfile provided.\n    \"\"\"\n    if flagfile_str.startswith('--flagfile='):\n      return os.path.expanduser((flagfile_str[(len('--flagfile=')):]).strip())\n    elif flagfile_str.startswith('-flagfile='):\n      return os.path.expanduser((flagfile_str[(len('-flagfile=')):]).strip())\n    else:\n      raise exceptions.Error(\n          'Hit illegal --flagfile type: %s' % flagfile_str)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __GetFlagFileLines(self, filename, parsed_file_stack=None):\n    if parsed_file_stack is None:\n      parsed_file_stack = []\n    # We do a little safety check for reparsing a file we've already encountered\n    # at a previous depth.\n    if filename in parsed_file_stack:\n      sys.stderr.write('Warning: Hit circular flagfile dependency. Ignoring'\n                       ' flagfile: %s\\n' % (filename,))\n      return []\n    else:\n      parsed_file_stack.append(filename)\n\n    line_list = []  # All line from flagfile.\n    flag_line_list = []  # Subset of lines w/o comments, blanks, flagfile= tags.\n    try:\n      file_obj = open(filename, 'r')\n    except IOError as e_msg:\n      raise exceptions.CantOpenFlagFileError(\n          'ERROR:: Unable to open flagfile: %s' % e_msg)\n\n    with file_obj:\n      line_list = file_obj.readlines()\n\n    # This is where we check each line in the file we just read.\n    for line in line_list:\n      if line.isspace():\n        pass\n      # Checks for comment (a line that starts with '#').\n      elif line.startswith('#') or line.startswith('//'):\n        pass\n      # Checks for a nested \"--flagfile=<bar>\" flag in the current file.\n      # If we find one, recursively parse down into that file.\n      elif self.__IsFlagFileDirective(line):\n        sub_filename = self.ExtractFilename(line)\n        included_flags = self.__GetFlagFileLines(\n            sub_filename, parsed_file_stack=parsed_file_stack)\n        flag_line_list.extend(included_flags)\n      else:\n        # Any line that's not a comment or a nested flagfile should get\n        # copied into 2nd position.  This leaves earlier arguments\n        # further back in the list, thus giving them higher priority.\n        flag_line_list.append(line.strip())\n\n    parsed_file_stack.pop()\n    return flag_line_list", "response": "Returns the useful lines from a flag file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ReadFlagsFromFiles(self, argv, force_gnu=True):\n    rest_of_args = argv\n    new_argv = []\n    while rest_of_args:\n      current_arg = rest_of_args[0]\n      rest_of_args = rest_of_args[1:]\n      if self.__IsFlagFileDirective(current_arg):\n        # This handles the case of -(-)flagfile foo.  In this case the\n        # next arg really is part of this one.\n        if current_arg == '--flagfile' or current_arg == '-flagfile':\n          if not rest_of_args:\n            raise exceptions.IllegalFlagValueError(\n                '--flagfile with no argument')\n          flag_filename = os.path.expanduser(rest_of_args[0])\n          rest_of_args = rest_of_args[1:]\n        else:\n          # This handles the case of (-)-flagfile=foo.\n          flag_filename = self.ExtractFilename(current_arg)\n        new_argv.extend(self.__GetFlagFileLines(flag_filename))\n      else:\n        new_argv.append(current_arg)\n        # Stop parsing after '--', like getopt and gnu_getopt.\n        if current_arg == '--':\n          break\n        # Stop parsing after a non-flag, like getopt.\n        if not current_arg.startswith('-'):\n          if not force_gnu and not self.__dict__['__use_gnu_getopt']:\n            break\n        else:\n          if ('=' not in current_arg and\n              rest_of_args and not rest_of_args[0].startswith('-')):\n            # If this is an occurence of a legitimate --x y, skip the value\n            # so that it won't be mistaken for a standalone arg.\n            fl = self.FlagDict()\n            name = current_arg.lstrip('-')\n            if name in fl and not fl[name].boolean:\n              current_arg = rest_of_args[0]\n              rest_of_args = rest_of_args[1:]\n              new_argv.append(current_arg)\n\n    if rest_of_args:\n      new_argv.extend(rest_of_args)\n\n    return new_argv", "response": "Reads all flags from a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef FlagsIntoString(self):\n    s = ''\n    for flag in self.FlagDict().values():\n      if flag.value is not None:\n        s += flag.serialize() + '\\n'\n    return s", "response": "Returns a string with the flags assignments from this FlagValues object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AppendFlagsIntoFile(self, filename):\n    with open(filename, 'a') as out_file:\n      out_file.write(self.FlagsIntoString())", "response": "Appends all flags assignments from this FlagInfo object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a constraint to validate multiple flags.", "response": "def register_multi_flags_validator(flag_names,\n                                   multi_flags_checker,\n                                   message='Flags validation failed',\n                                   flag_values=FLAGS):\n  \"\"\"Adds a constraint to multiple flags.\n\n  The constraint is validated when flags are initially parsed, and after each\n  change of the corresponding flag's value.\n\n  Args:\n    flag_names: [str], a list of the flag names to be checked.\n    multi_flags_checker: callable, a function to validate the flag.\n        input - dictionary, with keys() being flag_names, and value for each key\n            being the value of the corresponding flag (string, boolean, etc).\n        output - Boolean. Must return True if validator constraint is satisfied.\n            If constraint is not satisfied, it should either return False or\n            raise gflags.ValidationError.\n    message: Error text to be shown to the user if checker returns False.\n        If checker raises gflags.ValidationError, message from the raised error\n        will be shown.\n    flag_values: An optional FlagValues instance to validate against.\n\n  Raises:\n    AttributeError: If a flag is not registered as a valid flag name.\n  \"\"\"\n  v = gflags_validators.MultiFlagsValidator(\n      flag_names, multi_flags_checker, message)\n  _add_validator(flag_values, v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mark_flag_as_required(flag_name, flag_values=FLAGS):\n  if flag_values[flag_name].default is not None:\n    # TODO(vrusinov): Turn this warning into an exception.\n    warnings.warn(\n        'Flag %s has a non-None default value; therefore, '\n        'mark_flag_as_required will pass even if flag is not specified in the '\n        'command line!' % flag_name)\n  register_validator(flag_name,\n                     lambda value: value is not None,\n                     message='Flag --%s must be specified.' % flag_name,\n                     flag_values=flag_values)", "response": "Ensures that flag is not None during program execution."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that only one flag among flag_names is set.", "response": "def mark_flags_as_mutual_exclusive(flag_names, required=False,\n                                   flag_values=FLAGS):\n  \"\"\"Ensures that only one flag among flag_names is set.\n\n  Args:\n    flag_names: [str], a list of the flag names to be checked.\n    required: Boolean, if set, exactly one of the flags must be set.\n        Otherwise, it is also valid for none of the flags to be set.\n    flag_values: An optional FlagValues instance to validate against.\n  \"\"\"\n\n  def validate_mutual_exclusion(flags_dict):\n    flag_count = sum(1 for val in flags_dict.values() if val is not None)\n    if flag_count == 1 or (not required and flag_count == 0):\n      return True\n    message = ('%s one of (%s) must be specified.' %\n               ('Exactly' if required else 'At most', ', '.join(flag_names)))\n    raise ValidationError(message)\n\n  register_multi_flags_validator(\n      flag_names, validate_mutual_exclusion, flag_values=flag_values)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DEFINE_flag(flag, flag_values=FLAGS, module_name=None):  # pylint: disable=g-bad-name\n  # copying the reference to flag_values prevents pychecker warnings\n  fv = flag_values\n  fv[flag.name] = flag\n  # Tell flag_values who's defining the flag.\n  if isinstance(flag_values, FlagValues):\n    # Regarding the above isinstance test: some users pass funny\n    # values of flag_values (e.g., {}) in order to avoid the flag\n    # registration (in the past, there used to be a flag_values ==\n    # FLAGS test here) and redefine flags with the same name (e.g.,\n    # debug).  To avoid breaking their code, we perform the\n    # registration only if flag_values is a real FlagValues object.\n    if module_name:\n      module = sys.modules.get(module_name)\n    else:\n      module, module_name = _helpers.GetCallingModuleObjectAndName()\n    # TODO(vrusinov): _RegisterFlagByModule* should be public.\n    # pylint: disable=protected-access\n    flag_values._RegisterFlagByModule(module_name, flag)\n    flag_values._RegisterFlagByModuleId(id(module), flag)", "response": "Registers a flag with a FlagValues object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeclare a flag as key for the calling module. Internal function. User code should call DECLARE_key_flag or ADOPT_module_key_flags instead. Args: flag_names: A list of strings that are names of already-registered Flag objects. flag_values: A FlagValues object that the flags listed in flag_names have registered with (the value of the flag_values argument from the DEFINE_* calls that defined those flags). This should almost never need to be overridden. key_flag_values: A FlagValues object that (among possibly many other things) keeps track of the key flags for each module. Default None means \"same as flag_values\". This should almost never need to be overridden. Raises: UnrecognizedFlagError: when we refer to a flag that was not defined yet.", "response": "def _internal_declare_key_flags(flag_names,\n                                flag_values=FLAGS, key_flag_values=None):\n  \"\"\"Declares a flag as key for the calling module.\n\n  Internal function.  User code should call DECLARE_key_flag or\n  ADOPT_module_key_flags instead.\n\n  Args:\n    flag_names: A list of strings that are names of already-registered\n      Flag objects.\n    flag_values: A FlagValues object that the flags listed in\n      flag_names have registered with (the value of the flag_values\n      argument from the DEFINE_* calls that defined those flags).\n      This should almost never need to be overridden.\n    key_flag_values: A FlagValues object that (among possibly many\n      other things) keeps track of the key flags for each module.\n      Default None means \"same as flag_values\".  This should almost\n      never need to be overridden.\n\n  Raises:\n    UnrecognizedFlagError: when we refer to a flag that was not\n      defined yet.\n  \"\"\"\n  key_flag_values = key_flag_values or flag_values\n\n  module = _helpers.GetCallingModule()\n\n  for flag_name in flag_names:\n    flag = flag_values.GetFlag(flag_name)\n    # TODO(vrusinov): _RegisterKeyFlagForModule should be public.\n    key_flag_values._RegisterKeyFlagForModule(module, flag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef DECLARE_key_flag(  # pylint: disable=g-bad-name\n    flag_name, flag_values=FLAGS):\n  \"\"\"Declares one flag as key to the current module.\n\n  Key flags are flags that are deemed really important for a module.\n  They are important when listing help messages; e.g., if the\n  --helpshort command-line flag is used, then only the key flags of the\n  main module are listed (instead of all flags, as in the case of\n  --helpfull).\n\n  Sample usage:\n\n    gflags.DECLARE_key_flag('flag_1')\n\n  Args:\n    flag_name: A string, the name of an already declared flag.\n      (Redeclaring flags as key, including flags implicitly key\n      because they were declared in this module, is a no-op.)\n    flag_values: A FlagValues object.  This should almost never\n      need to be overridden.\n  \"\"\"\n  if flag_name in _helpers.SPECIAL_FLAGS:\n    # Take care of the special flags, e.g., --flagfile, --undefok.\n    # These flags are defined in _SPECIAL_FLAGS, and are treated\n    # specially during flag parsing, taking precedence over the\n    # user-defined flags.\n    _internal_declare_key_flags([flag_name],\n                                flag_values=_helpers.SPECIAL_FLAGS,\n                                key_flag_values=flag_values)\n    return\n  _internal_declare_key_flags([flag_name], flag_values=flag_values)", "response": "Declares one flag as key to the current module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ADOPT_module_key_flags(  # pylint: disable=g-bad-name\n    module, flag_values=FLAGS):\n  \"\"\"Declares that all flags key to a module are key to the current module.\n\n  Args:\n    module: A module object.\n    flag_values: A FlagValues object.  This should almost never need\n      to be overridden.\n\n  Raises:\n    Error: When given an argument that is a module name (a\n    string), instead of a module object.\n  \"\"\"\n  if not isinstance(module, types.ModuleType):\n    raise Error('Expected a module object, not %r.' % (module,))\n  # TODO(vrusinov): _GetKeyFlagsForModule should be public.\n  _internal_declare_key_flags(\n      [f.name for f in flag_values._GetKeyFlagsForModule(module.__name__)],  # pylint: disable=protected-access\n      flag_values=flag_values)\n  # If module is this flag module, take _helpers._SPECIAL_FLAGS into account.\n  if module == _helpers.GetModuleObjectAndName(globals())[0]:\n    _internal_declare_key_flags(\n        # As we associate flags with _GetCallingModuleObjectAndName(), the\n        # special flags defined in this module are incorrectly registered with\n        # a different module.  So, we can't use _GetKeyFlagsForModule.\n        # Instead, we take all flags from _helpers._SPECIAL_FLAGS (a private\n        # FlagValues, where no other module should register flags).\n        [f.name for f in six.itervalues(_helpers.SPECIAL_FLAGS.FlagDict())],\n        flag_values=_helpers.SPECIAL_FLAGS,\n        key_flag_values=flag_values)", "response": "Declares that all flags key to a module are key to the current module."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeclare that the current module will not define any more key flags.", "response": "def DISCLAIM_key_flags():  # pylint: disable=g-bad-name\n  \"\"\"Declares that the current module will not define any more key flags.\n\n  Normally, the module that calls the DEFINE_xxx functions claims the\n  flag to be its key flag.  This is undesirable for modules that\n  define additional DEFINE_yyy functions with its own flag parsers and\n  serializers, since that module will accidentally claim flags defined\n  by DEFINE_yyy as its key flags.  After calling this function, the\n  module disclaims flag definitions thereafter, so the key flags will\n  be correctly attributed to the caller of DEFINE_yyy.\n\n  After calling this function, the module will not be able to define\n  any more flags.  This function will affect all FlagValues objects.\n  \"\"\"\n  globals_for_caller = sys._getframe(1).f_globals  # pylint: disable=protected-access\n  module, _ = _helpers.GetModuleObjectAndName(globals_for_caller)\n  _helpers.disclaim_module_ids.add(id(module))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a flag whose value can be any string from enum_values.", "response": "def DEFINE_enum(  # pylint: disable=g-bad-name,redefined-builtin\n    name, default, enum_values, help, flag_values=FLAGS, module_name=None,\n    **args):\n  \"\"\"Registers a flag whose value can be any string from enum_values.\n\n  Args:\n    name: A string, the flag name.\n    default: The default value of the flag.\n    enum_values: A list of strings with the possible values for the flag.\n    help: A help string.\n    flag_values: FlagValues object with which the flag will be registered.\n    module_name: A string, the name of the Python module declaring this flag.\n        If not provided, it will be computed using the stack trace of this call.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  DEFINE_flag(EnumFlag(name, default, help, enum_values, ** args),\n              flag_values, module_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a flag whose value is a comma - separated list of strings.", "response": "def DEFINE_list(  # pylint: disable=g-bad-name,redefined-builtin\n    name, default, help, flag_values=FLAGS, **args):\n  \"\"\"Registers a flag whose value is a comma-separated list of strings.\n\n  The flag value is parsed with a CSV parser.\n\n  Args:\n    name: A string, the flag name.\n    default: The default value of the flag.\n    help: A help string.\n    flag_values: FlagValues object with which the flag will be registered.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = ListParser()\n  serializer = CsvListSerializer(',')\n  DEFINE(parser, name, default, help, flag_values, serializer, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef DEFINE_multi(  # pylint: disable=g-bad-name,redefined-builtin\n    parser, serializer, name, default, help, flag_values=FLAGS,\n    module_name=None, **args):\n  \"\"\"Registers a generic MultiFlag that parses its args with a given parser.\n\n  Auxiliary function.  Normal users should NOT use it directly.\n\n  Developers who need to create their own 'Parser' classes for options\n  which can appear multiple times can call this module function to\n  register their flags.\n\n  Args:\n    parser: ArgumentParser that is used to parse the flag arguments.\n    serializer: ArgumentSerializer that serializes the flag value.\n    name: A string, the flag name.\n    default: The default value of the flag.\n    help: A help string.\n    flag_values: FlagValues object with which the flag will be registered.\n    module_name: A string, the name of the Python module declaring this flag.\n        If not provided, it will be computed using the stack trace of this call.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  DEFINE_flag(MultiFlag(parser, serializer, name, default, help, **args),\n              flag_values, module_name)", "response": "Registers a generic MultiFlag that parses its args with a given parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a flag whose value can be a list of any string.", "response": "def DEFINE_multistring(  # pylint: disable=g-bad-name,redefined-builtin\n    name, default, help, flag_values=FLAGS, **args):\n  \"\"\"Registers a flag whose value can be a list of any strings.\n\n  Use the flag on the command line multiple times to place multiple\n  string values into the list.  The 'default' may be a single string\n  (which will be converted into a single-element list) or a list of\n  strings.\n\n\n  Args:\n    name: A string, the flag name.\n    default: The default value of the flag.\n    help: A help string.\n    flag_values: FlagValues object with which the flag will be registered.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = ArgumentParser()\n  serializer = ArgumentSerializer()\n  DEFINE_multi(parser, serializer, name, default, help, flag_values, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DEFINE_multi_int(  # pylint: disable=g-bad-name,redefined-builtin\n    name, default, help, lower_bound=None, upper_bound=None,\n    flag_values=FLAGS, **args):\n  \"\"\"Registers a flag whose value can be a list of arbitrary integers.\n\n  Use the flag on the command line multiple times to place multiple\n  integer values into the list.  The 'default' may be a single integer\n  (which will be converted into a single-element list) or a list of\n  integers.\n\n  Args:\n    name: A string, the flag name.\n    default: The default value of the flag.\n    help: A help string.\n    lower_bound: int, min values of the flag.\n    upper_bound: int, max values of the flag.\n    flag_values: FlagValues object with which the flag will be registered.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = IntegerParser(lower_bound, upper_bound)\n  serializer = ArgumentSerializer()\n  DEFINE_multi(parser, serializer, name, default, help, flag_values, **args)", "response": "Registers a flag whose value can be a list of arbitrary integers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a flag whose value can be a list of arbitrary floats.", "response": "def DEFINE_multi_float(  # pylint: disable=g-bad-name,redefined-builtin\n    name, default, help, lower_bound=None, upper_bound=None,\n    flag_values=FLAGS, **args):\n  \"\"\"Registers a flag whose value can be a list of arbitrary floats.\n\n  Use the flag on the command line multiple times to place multiple\n  float values into the list.  The 'default' may be a single float\n  (which will be converted into a single-element list) or a list of\n  floats.\n\n  Args:\n    name: A string, the flag name.\n    default: The default value of the flag.\n    help: A help string.\n    lower_bound: float, min values of the flag.\n    upper_bound: float, max values of the flag.\n    flag_values: FlagValues object with which the flag will be registered.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = FloatParser(lower_bound, upper_bound)\n  serializer = ArgumentSerializer()\n  DEFINE_multi(parser, serializer, name, default, help, flag_values, **args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefine an alias flag for an existing flag.", "response": "def DEFINE_alias(name, original_name, flag_values=FLAGS, module_name=None):  # pylint: disable=g-bad-name\n  \"\"\"Defines an alias flag for an existing one.\n\n  Args:\n    name: A string, name of the alias flag.\n    original_name: A string, name of the original flag.\n    flag_values: FlagValues object with which the flag will be registered.\n    module_name: A string, the name of the module that defines this flag.\n\n  Raises:\n    gflags.FlagError:\n      UnrecognizedFlagError: if the referenced flag doesn't exist.\n      DuplicateFlagError: if the alias name has been used by some existing flag.\n  \"\"\"\n  if original_name not in flag_values:\n    raise UnrecognizedFlagError(original_name)\n  flag = flag_values[original_name]\n\n  class _Parser(ArgumentParser):\n    \"\"\"The parser for the alias flag calls the original flag parser.\"\"\"\n\n    def parse(self, argument):\n      flag.parse(argument)\n      return flag.value\n\n  class _FlagAlias(Flag):\n    \"\"\"Overrides Flag class so alias value is copy of original flag value.\"\"\"\n\n    @property\n    def value(self):\n      return flag.value\n\n    @value.setter\n    def value(self, value):\n      flag.value = value\n\n  help_msg = 'Alias for --%s.' % flag.name\n  # If alias_name has been used, gflags.DuplicatedFlag will be raised.\n  DEFINE_flag(_FlagAlias(_Parser(), flag.serializer, name, flag.default,\n                         help_msg, boolean=flag.boolean),\n              flag_values, module_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_flag(cls, flagname, flag_values, other_flag_values=None):\n    first_module = flag_values.FindModuleDefiningFlag(\n        flagname, default='<unknown>')\n    if other_flag_values is None:\n      second_module = _helpers.GetCallingModule()\n    else:\n      second_module = other_flag_values.FindModuleDefiningFlag(\n          flagname, default='<unknown>')\n    flag_summary = flag_values[flagname].help\n    msg = (\"The flag '%s' is defined twice. First from %s, Second from %s.  \"\n           \"Description from first occurrence: %s\") % (\n               flagname, first_module, second_module, flag_summary)\n    return cls(msg)", "response": "Create a DuplicateFlagError by providing a flag name and a list of flag values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse one or more arguments with the installed parser.", "response": "def parse(self, arguments):\n    \"\"\"Parses one or more arguments with the installed parser.\n\n    Args:\n      arguments: a single argument or a list of arguments (typically a\n        list of default values); a single argument is converted\n        internally into a list containing one item.\n    \"\"\"\n    if not isinstance(arguments, list):\n      # Default value may be a list of values.  Most other arguments\n      # will not be, so convert them into a single-item list to make\n      # processing simpler below.\n      arguments = [arguments]\n\n    if self.present:\n      # keep a backup reference to list of previously supplied option values\n      values = self.value\n    else:\n      # \"erase\" the defaults with an empty list\n      values = []\n\n    for item in arguments:\n      # have Flag superclass parse argument, overwriting self.value reference\n      Flag.Parse(self, item)  # also increments self.present\n      values.append(self.value)\n\n    # put list of option values back in the 'value' attribute\n    self.value = values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert(self, argument):\n    if isinstance(argument, str):\n      if argument.lower() in ['true', 't', '1']:\n        return True\n      elif argument.lower() in ['false', 'f', '0']:\n        return False\n\n    bool_argument = bool(argument)\n    if argument == bool_argument:\n      # The argument is a valid boolean (True, False, 0, or 1), and not just\n      # something that always converts to bool (list, string, int, etc.).\n      return bool_argument\n\n    raise ValueError('Non-boolean argument to boolean flag', argument)", "response": "Converts the argument to a boolean ; raise ValueError on errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines validity of argument and return the correct element of enum.", "response": "def parse(self, argument):\n    \"\"\"Determine validity of argument and return the correct element of enum.\n\n    If self.enum_values is empty, then all arguments are valid and argument\n    will be returned.\n\n    Otherwise, if argument matches an element in enum, then the first\n    matching element will be returned.\n\n    Args:\n      argument: The supplied flag value.\n\n    Returns:\n      The matching element from enum_values, or argument if enum_values is\n      empty.\n\n    Raises:\n      ValueError: enum_values was non-empty, but argument didn't match\n        anything in enum.\n    \"\"\"\n    if not self.enum_values:\n      return argument\n    elif self.case_sensitive:\n      if argument not in self.enum_values:\n        raise ValueError('value should be one of <%s>' %\n                         '|'.join(self.enum_values))\n      else:\n        return argument\n    else:\n      if argument.upper() not in [value.upper() for value in self.enum_values]:\n        raise ValueError('value should be one of <%s>' %\n                         '|'.join(self.enum_values))\n      else:\n        return [value for value in self.enum_values\n                if value.upper() == argument.upper()][0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize a list as a string if possible or as a unicode string.", "response": "def serialize(self, value):\n    \"\"\"Serialize a list as a string, if possible, or as a unicode string.\"\"\"\n    if six.PY2:\n      # In Python2 csv.writer doesn't accept unicode, so we convert to UTF-8.\n      output = io.BytesIO()\n      csv.writer(output).writerow([unicode(x).encode('utf-8') for x in value])\n      serialized_value = output.getvalue().decode('utf-8').strip()\n    else:\n      # In Python3 csv.writer expects a text stream.\n      output = io.StringIO()\n      csv.writer(output).writerow([str(x) for x in value])\n      serialized_value = output.getvalue().strip()\n\n    # We need the returned value to be pure ascii or Unicodes so that\n    # when the xml help is generated they are usefully encodable.\n    return _helpers.StrOrUnicode(serialized_value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrecording PyAudio stream into StringIO output", "response": "def record(self):\n        \"\"\"\n        Record PyAudio stream into StringIO output\n\n        This coroutine keeps stream open; the stream is closed in stop()\n        \"\"\"\n\n        while True:\n            frames = []\n            self.stream.start_stream()\n            for i in range(self.num_frames):\n                data = self.stream.read(self.config.FRAMES_PER_BUFFER)\n                frames.append(data)\n            self.output.seek(0)\n            w = wave.open(self.output, 'wb')\n            w.setnchannels(self.config.CHANNELS)\n            w.setsampwidth(self.audio.get_sample_size(self.config.FORMAT))\n            w.setframerate(self.config.RATE)\n            w.writeframes(b''.join(frames))\n            w.close()\n            yield"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stop(self):\n        self.prestop()\n        if not self._graceful:\n            self._graceful = True\n        self.stream.stop_stream()\n        self.audio.terminate()\n        msg = 'Stopped'\n        self.verbose_info(msg, log=False)\n        # Log 'Stopped' anyway\n        if self.log:\n            self.logging.info(msg)\n        if self.collect:\n            if self._data:\n                print('Collected result:')\n                print('    min: %10d' % self._data['min'])\n                print('    max: %10d' % self._data['max'])\n                print('    avg: %10d' % int(self._data['avg']))\n        self.poststop()", "response": "Stop the stream and terminate PyAudio"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_threshold(self):\n\n        if self.threshold.startswith('+'):\n            if self.threshold[1:].isdigit():\n                self._threshold = int(self.threshold[1:])\n                self._upper = True\n        elif self.threshold.startswith('-'):\n            if self.threshold[1:].isdigit():\n                self._threshold = int(self.threshold[1:])\n                self._upper = False\n        else:\n            if self.threshold.isdigit():\n                self._threshold = int(self.threshold)\n                self._upper = True\n        if not hasattr(self, '_threshold'):\n            raise ValueError('Invalid threshold')", "response": "Get and validate raw RMS value from threshold"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef collect_rms(self, rms):\n        if self._data:\n            self._data['min'] = min(rms, self._data['min'])\n            self._data['max'] = max(rms, self._data['max'])\n            self._data['avg'] = float(rms + self._data['avg']) / 2\n        else:\n            self._data['min'] = rms\n            self._data['max'] = rms\n            self._data['avg'] = rms", "response": "Collect and calculate min max and average RMS values"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef b58encode(v):\n    '''Encode a string using Base58'''\n    v = scrub_input(v)\n\n    nPad = len(v)\n    v = v.lstrip(b'\\0')\n    nPad -= len(v)\n\n    p, acc = 1, 0\n    for c in iseq(reversed(v)):\n        acc += p * c\n        p = p << 8\n\n    result = b58encode_int(acc, default_one=False)\n\n    return (alphabet[0:1] * nPad + result)", "response": "Encode a string using Base58"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef b58decode_int(v):\n    '''Decode a Base58 encoded string as an integer'''\n    v = v.rstrip()\n    v = scrub_input(v)\n\n    decimal = 0\n    for char in v:\n        decimal = decimal * 58 + alphabet.index(char)\n    return decimal", "response": "Decode a Base58 encoded string as an integer"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef b58decode(v):\n    '''Decode a Base58 encoded string'''\n    v = v.rstrip()\n    v = scrub_input(v)\n\n    origlen = len(v)\n    v = v.lstrip(alphabet[0:1])\n    newlen = len(v)\n\n    acc = b58decode_int(v)\n\n    result = []\n    while acc > 0:\n        acc, mod = divmod(acc, 256)\n        result.append(mod)\n\n    return (b'\\0' * (origlen - newlen) + bseq(reversed(result)))", "response": "Decode a Base58 encoded string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_timedelta(cls, timedelta):\n        from math import ceil\n        units = ceil(timedelta.total_seconds() / cls.time_unit)\n        return cls.create(units)", "response": "expects a datetime. timedelta object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef b58encode(v: bytes) -> str:\n    '''Encode a string using Base58'''\n\n    origlen = len(v)\n    v = v.lstrip(b'\\0')\n    newlen = len(v)\n\n    p, acc = 1, 0\n    for c in iseq(v[::-1]):\n        acc += p * c\n        p = p << 8\n\n    result = ''\n    while acc > 0:\n        acc, mod = divmod(acc, 58)\n        result += alphabet[mod]\n\n    return (result + alphabet[0] * (origlen - newlen))[::-1]", "response": "Encode a string using Base58"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding a Base58 encoded string", "response": "def b58decode(v: str) -> bytes:\n    '''Decode a Base58 encoded string'''\n\n    origlen = len(v)\n    v = v.lstrip(alphabet[0])\n    newlen = len(v)\n\n    p, acc = 1, 0\n    for c in v[::-1]:\n        acc += p * alphabet.index(c)\n        p *= 58\n\n    result = []\n    while acc > 0:\n        acc, mod = divmod(acc, 256)\n        result.append(mod)\n\n    return (bseq(result) + b'\\0' * (origlen - newlen))[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes a string using Base58 with a 4 character checksum", "response": "def b58encode_check(v: bytes) -> str:\n    '''Encode a string using Base58 with a 4 character checksum'''\n\n    digest = sha256(sha256(v).digest()).digest()\n    return b58encode(v + digest[:4])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef b58decode_check(v: str) -> bytes:\n    '''Decode and verify the checksum of a Base58 encoded string'''\n\n    result = b58decode(v)\n    result, check = result[:-4], result[-4:]\n    digest = sha256(sha256(result).digest()).digest()\n\n    if check != digest[:4]:\n        raise ValueError(\"Invalid checksum\")\n\n    return result", "response": "Decode and verify the checksum of a Base58 encoded string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nderiving the ExtendedPublicKey from a master key or a relative path like m. 44. 0. 1. 10.", "response": "def derive(self, path):\n        \"\"\"\n        :param path: a path like \"m/44'/0'/1'/0/10\" if deriving from a master key,\n                     or a relative path like \"./0/10\"\n        :return: the derived ExtendedPublicKey if deriving from an ExtendedPublicKey,\n                 the derived ExtendedPrivateKey if deriving from an ExtendedPrivateKey\n        \"\"\"\n        steps = path.split('/')\n\n        if steps[0] not in {'m', '.'}:\n            raise ValueError('Invalid derivation path: {}'.format(path))\n\n        if steps[0] == 'm' and not self.is_master():\n            raise ValueError('Trying to derive absolute path from non-master key')\n\n        current = self\n        for step in steps[1:]:\n            hardened = False\n            if step[-1] == \"'\":\n                hardened = True\n                step = step[:-1]\n            index = int(step)\n            current = current.get_child(index, hardened)\n\n        return current"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bech32_decode(bech):\n    if ((any(ord(x) < 33 or ord(x) > 126 for x in bech)) or\n            (bech.lower() != bech and bech.upper() != bech)):\n        return None, None\n    bech = bech.lower()\n    pos = bech.rfind('1')\n    if pos < 1 or pos + 7 > len(bech) or len(bech) > 90:\n        return None, None\n    if not all(x in CHARSET for x in bech[pos+1:]):\n        return None, None\n    hrp = bech[:pos]\n    data = [CHARSET.find(x) for x in bech[pos+1:]]\n    if not bech32_verify_checksum(hrp, data):\n        return None, None\n    return hrp, data[:-6]", "response": "Validate a Bech32 string and determine HRP and data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _txins_data(self):\n        ntxins = self.parse_varint()\n        if ntxins == 0:\n            self.segwit = True\n            flag = next(self)\n            if flag != 1:\n                raise ValueError('Wrong flag in SegWit transaction: {}'.format(flag))\n            ntxins = self.parse_varint()\n        else:\n            self.segwit = False\n        self.txins = ntxins\n        return self.segwit, [self._txin_data() for _ in range(ntxins)]", "response": "Return a couple of segwit and txins."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_passive_request(self, request):\n        if request.path in PASSIVE_URLS:\n            return True\n\n        try:\n            match = resolve(request.path)\n            # TODO: check namespaces too\n            if match.url_name in PASSIVE_URL_NAMES:\n                return True\n        except Resolver404:\n            pass\n\n        return False", "response": "Should we skip activity update on this URL?"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_request(self, request):\n        \n        if django.VERSION < (1, 10):\n            is_authenticated = request.user.is_authenticated()\n        else:\n            is_authenticated = request.user.is_authenticated\n\n        if not is_authenticated:\n            return\n\n        now = datetime.now()\n        if '_session_security' not in request.session:\n            set_last_activity(request.session, now)\n            return\n\n        delta = now - get_last_activity(request.session)\n        expire_seconds = self.get_expire_seconds(request)\n        if delta >= timedelta(seconds=expire_seconds):\n            logout(request)\n        elif (request.path == reverse('session_security_ping') and\n                'idleFor' in request.GET):\n            self.update_last_activity(request, now)\n        elif not self.is_passive_request(request):\n            set_last_activity(request.session, now)", "response": "Update last activity time or logout."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_last_activity(self, request, now):\n        last_activity = get_last_activity(request.session)\n        server_idle_for = (now - last_activity).seconds\n\n        # Gracefully ignore non-integer values\n        try:\n            client_idle_for = int(request.GET['idleFor'])\n        except ValueError:\n            return\n\n        # Disallow negative values, causes problems with delta calculation\n        if client_idle_for < 0:\n            client_idle_for = 0\n\n        if client_idle_for < server_idle_for:\n            # Client has more recent activity than we have in the session\n            last_activity = now - timedelta(seconds=client_idle_for)\n\n        # Update the session\n        set_last_activity(request.session, last_activity)", "response": "Update the last activity of the current session."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the last activity datetime string from the session and return the python datetime object.", "response": "def get_last_activity(session):\n    \"\"\"\n    Get the last activity datetime string from the session and return the\n    python datetime object.\n    \"\"\"\n    try:\n        return datetime.strptime(session['_session_security'],\n                '%Y-%m-%dT%H:%M:%S.%f')\n    except AttributeError:\n        #################################################################\n        # * this is an odd bug in python\n        # bug report: http://bugs.python.org/issue7980\n        # bug explained here:\n        # http://code-trick.com/python-bug-attribute-error-_strptime/\n        # * sometimes, in multithreaded enviroments, we get AttributeError\n        #     in this case, we just return datetime.now(),\n        #     so that we are not logged out\n        #   \"./session_security/middleware.py\", in update_last_activity\n        #     last_activity = get_last_activity(request.session)\n        #   \"./session_security/utils.py\", in get_last_activity\n        #     '%Y-%m-%dT%H:%M:%S.%f')\n        #   AttributeError: _strptime\n        #\n        #################################################################\n\n        return datetime.now()\n    except TypeError:\n        return datetime.now()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef name(self):\n        return self.inquire(name=True, lifetime=False,\n                            usage=False, mechs=False).name", "response": "Get the name associated with these credentials"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nacquire GSSAPI credentials This method acquires credentials. If the `store` argument is used, the credentials will be acquired from the given credential store (if supported). Otherwise, the credentials are acquired from the default store. The credential store information is a dictionary containing mechanisms-specific keys and values pointing to a credential store or stores. Using a non-default store requires support for the credentials store extension. Args: name (Name): the name associated with the credentials, or None for the default name lifetime (int): the desired lifetime of the credentials, or None for indefinite mechs (list): the desired :class:`MechType` OIDs to be used with the credentials, or None for the default set usage (str): the usage for the credentials -- either 'both', 'initiate', or 'accept' store (dict): the credential store information pointing to the credential store from which to acquire the credentials, or None for the default store (:requires-ext:`cred_store`) Returns: AcquireCredResult: the acquired credentials and information about them Raises: BadMechanismError BadNameTypeError BadNameError ExpiredCredentialsError MissingCredentialsError", "response": "def acquire(cls, name=None, lifetime=None, mechs=None, usage='both',\n                store=None):\n        \"\"\"Acquire GSSAPI credentials\n\n        This method acquires credentials.  If the `store` argument is\n        used, the credentials will be acquired from the given\n        credential store (if supported).  Otherwise, the credentials are\n        acquired from the default store.\n\n        The credential store information is a dictionary containing\n        mechanisms-specific keys and values pointing to a credential store\n        or stores.\n\n        Using a non-default store requires support for the credentials store\n        extension.\n\n        Args:\n            name (Name): the name associated with the credentials,\n                or None for the default name\n            lifetime (int): the desired lifetime of the credentials, or None\n                for indefinite\n            mechs (list): the desired :class:`MechType` OIDs to be used\n                with the credentials, or None for the default set\n            usage (str): the usage for the credentials -- either 'both',\n                'initiate', or 'accept'\n            store (dict): the credential store information pointing to the\n                credential store from which to acquire the credentials,\n                or None for the default store (:requires-ext:`cred_store`)\n\n        Returns:\n            AcquireCredResult: the acquired credentials and information about\n                them\n\n        Raises:\n            BadMechanismError\n            BadNameTypeError\n            BadNameError\n            ExpiredCredentialsError\n            MissingCredentialsError\n        \"\"\"\n\n        if store is None:\n            res = rcreds.acquire_cred(name, lifetime,\n                                      mechs, usage)\n        else:\n            if rcred_cred_store is None:\n                raise NotImplementedError(\"Your GSSAPI implementation does \"\n                                          \"not have support for manipulating \"\n                                          \"credential stores\")\n\n            store = _encode_dict(store)\n\n            res = rcred_cred_store.acquire_cred_from(store, name,\n                                                     lifetime, mechs,\n                                                     usage)\n\n        return tuples.AcquireCredResult(cls(base=res.creds), res.mechs,\n                                        res.lifetime)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef store(self, store=None, usage='both', mech=None,\n              overwrite=False, set_default=False):\n        \"\"\"Store these credentials into the given store\n\n        This method stores the current credentials into the specified\n        credentials store.  If the default store is used, support for\n        :rfc:`5588` is required.  Otherwise, support for the credentials\n        store extension is required.\n\n        :requires-ext:`rfc5588` or :requires-ext:`cred_store`\n\n        Args:\n            store (dict): the store into which to store the credentials,\n                or None for the default store.\n            usage (str): the usage to store the credentials with -- either\n                'both', 'initiate', or 'accept'\n            mech (OID): the :class:`MechType` to associate with the\n                stored credentials\n            overwrite (bool): whether or not to overwrite existing credentials\n                stored with the same name, etc\n            set_default (bool): whether or not to set these credentials as\n                the default credentials for the given store.\n\n        Returns:\n            StoreCredResult: the results of the credential storing operation\n\n        Raises:\n            GSSError\n            ExpiredCredentialsError\n            MissingCredentialsError\n            OperationUnavailableError\n            DuplicateCredentialsElementError\n        \"\"\"\n\n        if store is None:\n            if rcred_rfc5588 is None:\n                raise NotImplementedError(\"Your GSSAPI implementation does \"\n                                          \"not have support for RFC 5588\")\n\n            return rcred_rfc5588.store_cred(self, usage, mech,\n                                            overwrite, set_default)\n        else:\n            if rcred_cred_store is None:\n                raise NotImplementedError(\"Your GSSAPI implementation does \"\n                                          \"not have support for manipulating \"\n                                          \"credential stores directly\")\n\n            store = _encode_dict(store)\n\n            return rcred_cred_store.store_cred_into(store, self, usage, mech,\n                                                    overwrite, set_default)", "response": "Store the current credentials into the given store."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimpersonates a name using the current credentials.", "response": "def impersonate(self, name=None, lifetime=None, mechs=None,\n                    usage='initiate'):\n        \"\"\"Impersonate a name using the current credentials\n\n        This method acquires credentials by impersonating another\n        name using the current credentials.\n\n        :requires-ext:`s4u`\n\n        Args:\n            name (Name): the name to impersonate\n            lifetime (int): the desired lifetime of the new credentials,\n                or None for indefinite\n            mechs (list): the desired :class:`MechType` OIDs for the new\n                credentials\n            usage (str): the desired usage for the new credentials -- either\n                'both', 'initiate', or 'accept'.  Note that some mechanisms\n                may only support 'initiate'.\n\n        Returns:\n            Credentials: the new credentials impersonating the given name\n        \"\"\"\n\n        if rcred_s4u is None:\n            raise NotImplementedError(\"Your GSSAPI implementation does not \"\n                                      \"have support for S4U\")\n\n        res = rcred_s4u.acquire_cred_impersonate_name(self, name,\n                                                      lifetime, mechs,\n                                                      usage)\n\n        return type(self)(base=res.creds)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninspecting these credentials for information about the current user.", "response": "def inquire(self, name=True, lifetime=True, usage=True, mechs=True):\n        \"\"\"Inspect these credentials for information\n\n        This method inspects these credentials for information about them.\n\n        Args:\n            name (bool): get the name associated with the credentials\n            lifetime (bool): get the remaining lifetime for the credentials\n            usage (bool): get the usage for the credentials\n            mechs (bool): get the mechanisms associated with the credentials\n\n        Returns:\n            InquireCredResult: the information about the credentials,\n                with None used when the corresponding argument was False\n\n        Raises:\n            MissingCredentialsError\n            InvalidCredentialsError\n            ExpiredCredentialsError\n        \"\"\"\n\n        res = rcreds.inquire_cred(self, name, lifetime, usage, mechs)\n\n        if res.name is not None:\n            res_name = names.Name(res.name)\n        else:\n            res_name = None\n\n        return tuples.InquireCredResult(res_name, res.lifetime,\n                                        res.usage, res.mechs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninspect these credentials for the specified mechanism and return the information about the credentials.", "response": "def inquire_by_mech(self, mech, name=True, init_lifetime=True,\n                        accept_lifetime=True, usage=True):\n        \"\"\"Inspect these credentials for per-mechanism information\n\n        This method inspects these credentials for per-mechanism information\n        about them.\n\n        Args:\n            mech (OID): the mechanism for which to retrive the information\n            name (bool): get the name associated with the credentials\n            init_lifetime (bool): get the remaining initiate lifetime for\n                the credentials\n            accept_lifetime (bool): get the remaining accept lifetime for\n                the credentials\n            usage (bool): get the usage for the credentials\n\n        Returns:\n            InquireCredByMechResult: the information about the credentials,\n                with None used when the corresponding argument was False\n        \"\"\"\n\n        res = rcreds.inquire_cred_by_mech(self, mech, name, init_lifetime,\n                                          accept_lifetime, usage)\n\n        if res.name is not None:\n            res_name = names.Name(res.name)\n        else:\n            res_name = None\n\n        return tuples.InquireCredByMechResult(res_name,\n                                              res.init_lifetime,\n                                              res.accept_lifetime,\n                                              res.usage)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, name, mech, usage='both',\n            init_lifetime=None, accept_lifetime=None, impersonator=None,\n            store=None):\n        \"\"\"Acquire more credentials to add to the current set\n\n        This method works like :meth:`acquire`, except that it adds the\n        acquired credentials for a single mechanism to a copy of the current\n        set, instead of creating a new set for multiple mechanisms.\n        Unlike :meth:`acquire`, you cannot pass None desired name or\n        mechanism.\n\n        If the `impersonator` argument is used, the credentials will\n        impersonate the given name using the impersonator credentials\n        (:requires-ext:`s4u`).\n\n        If the `store` argument is used, the credentials will be acquired\n        from the given credential store (:requires-ext:`cred_store`).\n        Otherwise, the credentials are acquired from the default store.\n\n        The credential store information is a dictionary containing\n        mechanisms-specific keys and values pointing to a credential store\n        or stores.\n\n        Note that the `store` argument is not compatible with the\n        `impersonator` argument.\n\n        Args:\n            name (Name): the name associated with the\n                credentials\n            mech (OID): the desired :class:`MechType` to be used with the\n                credentials\n            usage (str): the usage for the credentials -- either 'both',\n                'initiate', or 'accept'\n            init_lifetime (int): the desired initiate lifetime of the\n                credentials, or None for indefinite\n            accept_lifetime (int): the desired accept lifetime of the\n                credentials, or None for indefinite\n            impersonator (Credentials): the credentials to use to impersonate\n                the given name, or None to not acquire normally\n                (:requires-ext:`s4u`)\n            store (dict): the credential store information pointing to the\n                credential store from which to acquire the credentials,\n                or None for the default store (:requires-ext:`cred_store`)\n\n        Returns:\n            Credentials: the credentials set containing the current credentials\n                and the newly acquired ones.\n\n        Raises:\n            BadMechanismError\n            BadNameTypeError\n            BadNameError\n            DuplicateCredentialsElementError\n            ExpiredCredentialsError\n            MissingCredentialsError\n        \"\"\"\n\n        if store is not None and impersonator is not None:\n            raise ValueError('You cannot use both the `impersonator` and '\n                             '`store` arguments at the same time')\n\n        if store is not None:\n            if rcred_cred_store is None:\n                raise NotImplementedError(\"Your GSSAPI implementation does \"\n                                          \"not have support for manipulating \"\n                                          \"credential stores\")\n            store = _encode_dict(store)\n\n            res = rcred_cred_store.add_cred_from(store, self, name, mech,\n                                                 usage, init_lifetime,\n                                                 accept_lifetime)\n        elif impersonator is not None:\n            if rcred_s4u is None:\n                raise NotImplementedError(\"Your GSSAPI implementation does \"\n                                          \"not have support for S4U\")\n            res = rcred_s4u.add_cred_impersonate_name(self, impersonator,\n                                                      name, mech, usage,\n                                                      init_lifetime,\n                                                      accept_lifetime)\n        else:\n            res = rcreds.add_cred(self, name, mech, usage, init_lifetime,\n                                  accept_lifetime)\n\n        return Credentials(res.creds)", "response": "Adds more credentials to the current set of credentials for a given mechanisms."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display_as(self, name_type):\n\n        if rname_rfc6680 is None:\n            raise NotImplementedError(\"Your GSSAPI implementation does not \"\n                                      \"support RFC 6680 (the GSSAPI naming \"\n                                      \"extensions)\")\n        return rname_rfc6680.display_name_ext(self, name_type).decode(\n            _utils._get_encoding())", "response": "Display this name as the given name type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport this name as a token.", "response": "def export(self, composite=False):\n        \"\"\"Export this name as a token.\n\n        This method exports the name into a byte string which can then be\n        imported by using the `token` argument of the constructor.\n\n        Args:\n            composite (bool): whether or not use to a composite token --\n                :requires-ext:`rfc6680`\n\n        Returns:\n            bytes: the exported name in token form\n\n        Raises:\n            MechanismNameRequiredError\n            BadNameTypeError\n            BadNameError\n        \"\"\"\n\n        if composite:\n            if rname_rfc6680 is None:\n                raise NotImplementedError(\"Your GSSAPI implementation does \"\n                                          \"not support RFC 6680 (the GSSAPI \"\n                                          \"naming extensions)\")\n\n            return rname_rfc6680.export_name_composite(self)\n        else:\n            return rname.export_name(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _inquire(self, **kwargs):\n\n        if rname_rfc6680 is None:\n            raise NotImplementedError(\"Your GSSAPI implementation does not \"\n                                      \"support RFC 6680 (the GSSAPI naming \"\n                                      \"extensions)\")\n\n        if not kwargs:\n            default_val = True\n        else:\n            default_val = False\n\n        attrs = kwargs.get('attrs', default_val)\n        mech_name = kwargs.get('mech_name', default_val)\n\n        return rname_rfc6680.inquire_name(self, mech_name=mech_name,\n                                          attrs=attrs)", "response": "Inspect this name for information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Mechanism from its SASL name.", "response": "def from_sasl_name(cls, name=None):\n        \"\"\"\n        Create a Mechanism from its SASL name\n\n        Args:\n            name (str): SASL name of the desired mechanism\n\n        Returns:\n            Mechanism: the desired mechanism\n\n        Raises:\n            GSSError\n\n        :requires-ext:`rfc5801`\n        \"\"\"\n        if rfc5801 is None:\n            raise NotImplementedError(\"Your GSSAPI implementation does not \"\n                                      \"have support for RFC 5801\")\n        if isinstance(name, six.text_type):\n            name = name.encode(_utils._get_encoding())\n\n        m = rfc5801.inquire_mech_for_saslname(name)\n\n        return cls(m)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator of mechanisms that are supporting the specified attributes.", "response": "def from_attrs(cls, desired_attrs=None, except_attrs=None,\n                   critical_attrs=None):\n        \"\"\"\n        Get a generator of mechanisms supporting the specified attributes. See\n        RFC 5587's :func:`indicate_mechs_by_attrs` for more information.\n\n        Args:\n            desired_attrs ([OID]): Desired attributes\n            except_attrs ([OID]): Except attributes\n            critical_attrs ([OID]): Critical attributes\n\n        Returns:\n            [Mechanism]: A set of mechanisms having the desired features.\n\n        Raises:\n            GSSError\n\n        :requires-ext:`rfc5587`\n        \"\"\"\n        if isinstance(desired_attrs, roids.OID):\n            desired_attrs = set([desired_attrs])\n        if isinstance(except_attrs, roids.OID):\n            except_attrs = set([except_attrs])\n        if isinstance(critical_attrs, roids.OID):\n            critical_attrs = set([critical_attrs])\n\n        if rfc5587 is None:\n            raise NotImplementedError(\"Your GSSAPI implementation does not \"\n                                      \"have support for RFC 5587\")\n\n        mechs = rfc5587.indicate_mechs_by_attrs(desired_attrs,\n                                                except_attrs,\n                                                critical_attrs)\n        return (cls(mech) for mech in mechs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting a GSSAPI extension module based on the name of the extension.", "response": "def import_gssapi_extension(name):\n    \"\"\"Import a GSSAPI extension module\n\n    This method imports a GSSAPI extension module based\n    on the name of the extension (not including the\n    'ext_' prefix).  If the extension is not available,\n    the method retuns None.\n\n    Args:\n        name (str): the name of the extension\n\n    Returns:\n        module: Either the extension module or None\n    \"\"\"\n\n    try:\n        path = 'gssapi.raw.ext_{0}'.format(name)\n        __import__(path)\n        return sys.modules[path]\n    except ImportError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a property based on an inquire result", "response": "def inquire_property(name, doc=None):\n    \"\"\"Creates a property based on an inquire result\n\n    This method creates a property that calls the\n    :python:`_inquire` method, and return the value of the\n    requested information.\n\n    Args:\n        name (str): the name of the 'inquire' result information\n\n    Returns:\n        property: the created property\n    \"\"\"\n\n    def inquire_property(self):\n        if not self._started:\n            msg = (\"Cannot read {0} from a security context whose \"\n                   \"establishment has not yet been started.\")\n            raise AttributeError(msg)\n\n        return getattr(self._inquire(**{name: True}), name)\n\n    return property(inquire_property, doc=doc)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _encode_dict(d):\n    def enc(x):\n        if isinstance(x, six.text_type):\n            return x.encode(_ENCODING)\n        else:\n            return x\n\n    return dict((enc(k), enc(v)) for k, v in six.iteritems(d))", "response": "Encodes any relevant strings in a dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef catch_and_return_token(func, self, *args, **kwargs):\n\n    try:\n        return func(self, *args, **kwargs)\n    except GSSError as e:\n        if e.token is not None and self.__DEFER_STEP_ERRORS__:\n            self._last_err = e\n            # skip the \"return func\" line above in the traceback\n            if six.PY2:\n                self._last_tb = sys.exc_info()[2].tb_next.tb_next\n            else:\n                self._last_err.__traceback__ = e.__traceback__.tb_next\n\n            return e.token\n        else:\n            raise", "response": "Catch and return a token instead of a token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking and raise deferred errors before running the wrapped function.", "response": "def check_last_err(func, self, *args, **kwargs):\n    \"\"\"Check and raise deferred errors before running the function\n\n    This method checks :python:`_last_err` before running the wrapped\n    function.  If present and not None, the exception will be raised\n    with its original traceback.\n    \"\"\"\n\n    if self._last_err is not None:\n        try:\n            if six.PY2:\n                six.reraise(type(self._last_err), self._last_err,\n                            self._last_tb)\n            else:\n                # NB(directxman12): not using six.reraise in Python 3 leads\n                #                   to cleaner tracebacks, and raise x is valid\n                #                   syntax in Python 3 (unlike raise x, y, z)\n                raise self._last_err\n        finally:\n            if six.PY2:\n                del self._last_tb  # in case of cycles, break glass\n\n            self._last_err = None\n    else:\n        return func(self, *args, **kwargs)\n\n    @deco.decorator\n    def check_last_err(func, self, *args, **kwargs):\n        if self._last_err is not None:\n            try:\n                raise self._last_err\n            finally:\n                self._last_err = None\n        else:\n            return func(self, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef strings_to_categoricals(adata):\n    from pandas.api.types import is_string_dtype\n    from pandas import Categorical\n    for df in [adata.obs, adata.var]:\n        string_cols = [key for key in df.columns if is_string_dtype(df[key])]\n        for key in string_cols:\n            c = df[key]\n            c = Categorical(c)\n            if len(c.categories) < len(c): df[key] = c", "response": "Transform string annotations to categoricals."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a velocity graph.", "response": "def velocity_graph(adata, basis=None, vkey='velocity', which_graph='velocity', n_neighbors=10,\n                   alpha=.8, perc=90, edge_width=.2, edge_color='grey', color=None, use_raw=None, layer=None,\n                   color_map=None, colorbar=True, palette=None, size=None,  sort_order=True, groups=None,\n                   components=None, projection='2d', legend_loc='on data', legend_fontsize=None, legend_fontweight=None,\n                   right_margin=None, left_margin=None, xlabel=None, ylabel=None, title=None, fontsize=None,\n                   figsize=None, dpi=None, frameon=None, show=True, save=None, ax=None):\n    \"\"\"\\\n    Plot of the velocity graph.\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` or `None` (default: `None`)\n        Key for annotations of observations/cells or variables/genes.\n    which_graph: `'velocity'` or `'neighbors'` (default: `'velocity'`)\n        Whether to show transitions from velocity graph or connectivities from neighbors graph.\n\n    {scatter}\n\n    Returns\n    -------\n        `matplotlib.Axis` if `show==False`\n    \"\"\"\n    basis = default_basis(adata) if basis is None else basis\n    title = which_graph + ' graph' if title is None else title\n    scatter_kwargs = {\"basis\": basis, \"perc\": perc, \"use_raw\": use_raw, \"sort_order\": sort_order, \"alpha\": alpha,\n                      \"components\": components, \"projection\": projection, \"legend_loc\": legend_loc, \"groups\": groups,\n                      \"legend_fontsize\": legend_fontsize, \"legend_fontweight\": legend_fontweight, \"palette\": palette,\n                      \"color_map\": color_map, \"frameon\": frameon, \"title\": title, \"xlabel\": xlabel, \"ylabel\": ylabel,\n                      \"right_margin\": right_margin, \"left_margin\": left_margin, \"colorbar\": colorbar, \"dpi\": dpi,\n                      \"fontsize\": fontsize, \"show\": False, \"save\": None, \"figsize\": figsize, }\n    ax = scatter(adata, layer=layer, color=color, size=size, ax=ax, zorder=0, **scatter_kwargs)\n\n    from networkx import Graph, draw_networkx_edges\n    if which_graph == 'neighbors':\n        T = adata.uns['neighbors']['connectivities']\n        if perc is not None:\n            threshold = np.percentile(T.data, perc)\n            T.data[T.data < threshold] = 0\n            T.eliminate_zeros()\n    else:\n        T = transition_matrix(adata, vkey=vkey, weight_indirect_neighbors=0, n_neighbors=n_neighbors, perc=perc)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        edges = draw_networkx_edges(Graph(T), adata.obsm['X_' + basis], width=edge_width, edge_color=edge_color, ax=ax)\n        edges.set_zorder(-2)\n        edges.set_rasterized(settings._vector_friendly)\n\n    savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n    if not show: return ax"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef velocity_embedding(data, basis=None, vkey='velocity', scale=10, self_transitions=True, use_negative_cosines=True,\n                       direct_projection=None, pca_transform=None, retain_scale=False, autoscale=True, all_comps=True,\n                       T=None, copy=False):\n    \"\"\"Computes the single cell velocities in the embedding\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    basis: `str` (default: `'tsne'`)\n        Which embedding to use.\n    vkey: `str` (default: `'velocity'`)\n        Name of velocity estimates to be used.\n    scale: `int` (default: 10)\n        Scale parameter of gaussian kernel for transition matrix.\n    self_transitions: `bool` (default: `True`)\n        Whether to allow self transitions, based on the confidences of transitioning to neighboring cell.\n    use_negative_cosines: `bool` (default: `True`)\n        Whether to use not only positive, but also negative cosines and use those transitions to the opposite way.\n    direct_projection: `bool` (default: `True`)\n        Whether to directly project the velocities into PCA space, thus skipping velocity graph.\n    pca_transform: `bool` (default: `None`)\n        same as direct_projection (deprecated)\n    retain_scale: `bool` (default: `False`)\n        Whether to retain scale from high dimensional space in embedding.\n    autoscale: `bool` (default: `True`)\n        Whether to scale the embedded velocities by a scalar multiplier,\n        which simply ensures that the arrows in the embedding are properly scaled.\n    all_comps: `bool` (default: `True`)\n        Whether to compute the velocities on all embedding components or just the first two.\n    T: `csr_matrix` (default: `None`)\n        Allows the user to directly pass a transition matrix.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    velocity_basis: `.obsm`\n        coordinates of velocity projection on embedding\n    \"\"\"\n    adata = data.copy() if copy else data\n\n    if basis is None:\n        keys = [key for key in ['pca', 'tsne', 'umap'] if 'X_' + key in adata.obsm.keys()]\n        if len(keys) > 0: basis = keys[-1]\n        else: raise ValueError('No basis specified')\n\n    if 'X_' + basis not in adata.obsm_keys():\n        raise ValueError('You need compute the embedding first.')\n\n    logg.info('computing velocity embedding', r=True)\n\n    if pca_transform is None and direct_projection is None:\n        pca_transform = True if 'pca' in basis else False\n    if 'pca' in basis and (direct_projection or pca_transform):\n        V = adata.layers[vkey]\n        PCs = adata.varm['PCs'] if all_comps else adata.varm['PCs'][:, :2]\n\n        if vkey + '_genes' in adata.var.keys():\n            V = V[:, adata.var[vkey + '_genes']]\n            PCs = PCs[adata.var[vkey + '_genes']]\n        nans = np.isnan(V.sum(0))\n        if np.any(nans):\n            V = V[:, ~nans]\n            PCs = PCs[~nans]\n\n        X_emb = adata.obsm['X_' + basis]\n        V_emb = (V - V.mean(0)).dot(PCs)\n\n    else:\n        X_emb = adata.obsm['X_' + basis] if all_comps else adata.obsm['X_' + basis][:, :2]\n        V_emb = np.zeros(X_emb.shape)\n\n        T = transition_matrix(adata, vkey=vkey, scale=scale, self_transitions=self_transitions,\n                              use_negative_cosines=use_negative_cosines) if T is None else T\n        T.setdiag(0)\n        T.eliminate_zeros()\n\n        densify = adata.n_obs < 1e4\n        TA = T.A if densify else None\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for i in range(adata.n_obs):\n                indices = T[i].indices\n                dX = X_emb[indices] - X_emb[i, None]  # shape (n_neighbors, 2)\n                if not retain_scale: dX /= norm(dX)[:, None]\n                dX[np.isnan(dX)] = 0  # zero diff in a steady-state\n                probs = TA[i, indices] if densify else T[i].data\n                V_emb[i] = probs.dot(dX) - probs.mean() * dX.sum(0)  # probs.sum() / len(indices)\n\n        if retain_scale:\n            delta = T.dot(adata.X) - adata.X\n            if issparse(delta): delta = delta.A\n            cos_proj = (adata.layers[vkey] * delta).sum(1) / norm(delta)\n            V_emb *= np.clip(cos_proj[:, None] * 10, 0, 1)\n\n    if autoscale: V_emb /= (3 * quiver_autoscale(X_emb, V_emb))\n\n    vkey += '_' + basis\n    adata.obsm[vkey] = V_emb\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added\\n'\n        '    \\'' + vkey + '\\', embedded velocity vectors (adata.obsm)')\n\n    return adata if copy else None", "response": "Computes the velocities of a single cell embedding in the PCA space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_groups(adata, groups='all', key='louvain'):\n    strings_to_categoricals(adata)\n    if isinstance(groups, list) and isinstance(groups[0], int): groups = [str(n) for n in groups]\n    categories = adata.obs[key].cat.categories\n    groups_masks = np.array([categories[i] == adata.obs[key].values for i, name in enumerate(categories)])\n    if groups == 'all':\n        groups = categories.values\n    else:\n        groups_ids = [np.where(categories.values == name)[0][0] for name in groups]\n        groups_masks = groups_masks[groups_ids]\n        groups = categories[groups_ids].values\n    return groups, groups_masks", "response": "Get subset of groups in adata. obs [ key ].\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nranking the velocities of the given group by the given key.", "response": "def rank_velocity_genes(data, vkey='velocity', n_genes=10, groupby=None, match_with=None, resolution=None,\n                        min_counts=None, min_r2=None, min_dispersion=None, copy=False):\n    \"\"\"Rank genes for velocity characterizing groups.\n\n    Arguments\n    ----------\n    data : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` (default: `'velocity'`)\n        Key of velocities computed in `tl.velocity`\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    groupby: `str`, `list` or `np.ndarray` (default: `None`)\n        Key of observations grouping to consider.\n    min_counts: `float` (default: None)\n        Minimum count of genes for consideration.\n    min_r2: `float` (default: None)\n        Minimum r2 value of genes for consideration.\n    min_dispersion: `float` (default: None)\n        Minimum dispersion norm value of genes for consideration.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to data.\n\n    Returns\n    -------\n    Returns or updates `data` with the attributes\n    rank_velocity_genes : `.uns`\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    velocity_score : `.var`\n        Storing the score for each gene for each group. Ordered according to scores.\n    \"\"\"\n    adata = data.copy() if copy else data\n\n    if groupby is None or groupby == 'velocity_clusters':\n        velocity_clusters(adata, vkey=vkey, match_with=match_with, resolution=resolution)\n        groupby = 'velocity_clusters'\n\n    logg.info('ranking velocity genes', r=True)\n\n    tmp_filter = np.ones(adata.n_vars, dtype=bool)\n    if vkey + '_genes' in adata.var.keys():\n        tmp_filter &= adata.var[vkey + '_genes']\n\n    if 'unspliced' in adata.layers.keys():\n        n_counts = (adata.layers['unspliced'] > 0).sum(0)\n        n_counts = n_counts.A1 if issparse(adata.layers['unspliced']) else n_counts\n        min_counts = min(50, np.percentile(n_counts, 50)) if min_counts is None else min_counts\n        tmp_filter &= (n_counts > min_counts)\n\n    if 'r2' in adata.var.keys():\n        r2 = adata.var.velocity_r2\n        min_r2 = .1 if min_r2 is None else min_r2  # np.percentile(r2[r2 > 0], 50)\n        tmp_filter &= (r2 > min_r2)\n\n    if 'dispersions_norm' in adata.var.keys():\n        dispersions = adata.var.dispersions_norm\n        min_dispersion = 0 if min_dispersion is None else min_dispersion  # np.percentile(dispersions, 20)\n        tmp_filter &= (dispersions > min_dispersion)\n\n    X = adata[:, tmp_filter].layers[vkey]\n    groups, groups_masks = select_groups(adata[:, tmp_filter], key=groupby)\n\n    n_groups = groups_masks.shape[0]\n    sizes = groups_masks.sum(1)\n\n    mean, var = np.zeros((n_groups, X.shape[1])), np.zeros((n_groups, X.shape[1]))\n    for i, mask in enumerate(groups_masks): mean[i], var[i] = get_mean_var(X[mask])\n\n    # test each against the union of all other groups\n    rankings_gene_names, rankings_gene_scores, indices = [], [], []\n    for i in range(n_groups):\n        mask_rest = ~groups_masks[i]\n        mean_rest, var_rest = get_mean_var(X[mask_rest])\n        size_rest = sizes[i]  # else mask_rest.sum() if method == 't-test'\n\n        scores = (mean[i] - mean_rest) / np.sqrt(var[i] / sizes[i] + var_rest / size_rest)\n        scores = np.nan_to_num(scores)\n\n        # equivalent to but much faster than np.argsort(scores)[-10:]\n        if n_genes > X.shape[1]: n_genes = X.shape[1]\n        idx = np.argpartition(scores, -n_genes)[-n_genes:]\n        idx = idx[np.argsort(scores[idx])[::-1]]\n\n        rankings_gene_names.append(adata[:, tmp_filter].var_names[idx].values)\n        rankings_gene_scores.append(scores[idx])\n\n    rankings_gene_names = np.array([list(n) for n in rankings_gene_names])\n    rankings_gene_scores = np.array([list(n) for n in rankings_gene_scores])\n\n    all_names = rankings_gene_names.T.flatten()\n    all_scores = rankings_gene_scores.T.flatten()\n    vscore = np.zeros(adata.n_vars, dtype=np.int)\n    for i, name in enumerate(adata.var_names):\n        if name in all_names: vscore[i] = all_scores[np.where(name == all_names)[0][0]]\n    adata.var['velocity_score'] = vscore\n\n    key = 'rank_velocity_genes'\n    if key not in adata.uns.keys(): adata.uns[key] = {}\n    #adata.uns[key] = {'groups': groups, 'names': rankings_gene_names, 'scores': rankings_gene_scores.round(0)}\n\n    adata.uns[key] = \\\n        {'names': np.rec.fromarrays([n for n in rankings_gene_names], dtype=[(rn, 'U50') for rn in groups]),\n         'scores': np.rec.fromarrays([n.round(2) for n in rankings_gene_scores], dtype=[(rn, 'float32') for rn in groups]),\n         'params': {'groupby': groupby, 'reference': 'rest', 'method': 't-test_overestim_var', 'use_raw': True}}\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added \\n'\n        '    \\'' + key + '\\', sorted scores by group ids (adata.uns)')\n\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show_proportions(adata):\n    layers_keys = [key for key in ['spliced', 'unspliced', 'ambiguous'] if key in adata.layers.keys()]\n    tot_mol_cell_layers = [adata.layers[key].sum(1) for key in layers_keys]\n\n    mean_abundances = np.round(\n        [np.mean(tot_mol_cell / np.sum(tot_mol_cell_layers, 0)) for tot_mol_cell in tot_mol_cell_layers], 2)\n\n    print('Abundance of ' + str(layers_keys) + ': ' + str(mean_abundances))", "response": "Prints the fractions of spliced unspliced and ambiguous abundances."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes attributes not needed.", "response": "def cleanup(data, clean='layers', keep=None, copy=False):\n    \"\"\"Deletes attributes not needed.\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    clean: `str` or list of `str` (default: `layers`)\n        Which attributes to consider for freeing memory.\n    keep: `str` or list of `str` (default: None)\n        Which attributes to keep.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Returns or updates `adata` with selection of attributes kept.\n    \"\"\"\n    adata = data.copy() if copy else data\n\n    keep = list([keep]) if isinstance(keep, str) else list() if keep is None else list(keep)\n    keep.extend(['spliced', 'unspliced', 'Ms', 'Mu', 'clusters', 'neighbors'])\n\n    ann_dict = {'obs': adata.obs_keys(), 'var': adata.var_keys(),\n                'uns': adata.uns_keys(), 'layers': list(adata.layers.keys())}\n\n    if 'all' not in clean:\n        ann_dict = {ann: values for (ann, values) in ann_dict.items() if ann in clean}\n\n    for (ann, values) in ann_dict.items():\n        for value in values:\n            if value not in keep: del(getattr(adata, ann)[value])\n\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_genes(data, min_counts=None, min_cells=None, max_counts=None, max_cells=None,\n                 min_counts_u=None,  min_cells_u=None, max_counts_u=None, max_cells_u=None,\n                 min_shared_counts=None, min_shared_cells=None, copy=False):\n    \"\"\"Filter genes based on number of cells or counts.\n    Keep genes that have at least `min_counts` counts or are expressed in at\n    least `min_cells` cells or have at most `max_counts` counts or are expressed\n    in at most `max_cells` cells.\n    Only provide one of the optional parameters `min_counts`, `min_cells`,\n    `max_counts`, `max_cells` per call.\n\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.spmatrix`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    min_counts : `int`, optional (default: `None`)\n        Minimum number of counts required for a gene to pass filtering.\n    min_cells : `int`, optional (default: `None`)\n        Minimum number of cells expressed required for a gene to pass filtering.\n    max_counts : `int`, optional (default: `None`)\n        Maximum number of counts required for a gene to pass filtering.\n    max_cells : `int`, optional (default: `None`)\n        Maximum number of cells expressed required for a gene to pass filtering.\n    min_counts_u : `int`, optional (default: `None`)\n        Minimum number of unspliced counts required for a gene to pass filtering.\n    min_cells_u : `int`, optional (default: `None`)\n        Minimum number of unspliced cells expressed required for a gene to pass filtering.\n    max_counts_u : `int`, optional (default: `None`)\n        Maximum number of unspliced counts required for a gene to pass filtering.\n    max_cells_u : `int`, optional (default: `None`)\n        Maximum number of unspliced cells expressed required for a gene to pass filtering.\n    min_shared_counts: `int`, optional (default: `None`)\n        Minimum number of counts (in cells expressed simultaneously in unspliced and spliced) required for a gene.\n    min_shared_cells: `int`, optional (default: `None`)\n        Minimum number of cells required for a gene to be expressed simultaneously in unspliced and spliced.\n    copy : `bool`, optional (default: `False`)\n        Determines whether a copy is returned.\n\n    Returns\n    -------\n    Filters the object and adds `n_counts` to `adata.var`.\n    \"\"\"\n    adata = data.copy() if copy else data\n\n    # set initial cell sizes before filtering\n    set_initial_size(adata)\n\n    layers = [layer for layer in ['spliced', 'unspliced'] if layer in adata.layers.keys()]\n    if min_shared_counts is not None or min_shared_cells is not None: layers.extend(['shared'])\n\n    for layer in layers:\n\n        if layer is 'spliced':\n            _min_counts, _min_cells, _max_counts, _max_cells = min_counts, min_cells, max_counts, max_cells\n        elif layer is 'unspliced':\n            _min_counts, _min_cells, _max_counts, _max_cells = min_counts_u, min_cells_u, max_counts_u, max_cells_u\n        else:  # shared counts/cells\n            _min_counts, _min_cells, _max_counts, _max_cells = min_shared_counts, min_shared_cells, None, None\n\n        if layer in adata.layers.keys():\n            X = adata.layers[layer]\n        else:  # shared counts/cells\n            Xs, Xu = adata.layers['spliced'], adata.layers['unspliced']\n            nonzeros = (Xs > 0).multiply(Xu > 0) if issparse(Xs) else (Xs > 0) * (Xu > 0)\n            X = nonzeros.multiply(Xs) + nonzeros.multiply(Xu) if issparse(nonzeros) else nonzeros * (Xs + Xu)\n\n        gene_subset = np.ones(adata.n_vars, dtype=bool)\n\n        if _min_counts is not None or _max_counts is not None:\n            gene_subset, _ = filter(X, min_counts=_min_counts, max_counts=_max_counts)\n            adata._inplace_subset_var(gene_subset)\n\n        if _min_cells is not None or _max_cells is not None:\n            gene_subset, _ = filter(X, min_cells=_min_cells, max_cells=_max_cells)\n            adata._inplace_subset_var(gene_subset)\n\n        s = np.sum(~gene_subset)\n        if s > 0:\n            logg.info('Filtered out {} genes that are detected'.format(s), end=' ')\n            if _min_cells is not None or _min_counts is not None:\n                logg.info('in less than', str(_min_cells) + ' cells (' + str(layer) + ').' if _min_counts is None\n                          else str(_min_counts) + ' counts (' + str(layer) + ').', no_indent=True)\n            if max_cells is not None or max_counts is not None:\n                logg.info('in more than ', str(_max_cells) + ' cells(' + str(layer) + ').' if _max_counts is None\n                          else str(_max_counts) + ' counts (' + str(layer) + ').', no_indent=True)\n\n    return adata if copy else None", "response": "Filter genes based on number of cells or counts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering highly variable genes by dispersion.", "response": "def filter_genes_dispersion(data, flavor='seurat', min_disp=None, max_disp=None, min_mean=None, max_mean=None,\n                            n_bins=20, n_top_genes=None, log=True, copy=False):\n    \"\"\"Extract highly variable genes.\n    The normalized dispersion is obtained by scaling with the mean and standard\n    deviation of the dispersions for genes falling into a given bin for mean\n    expression of genes. This means that for each bin of mean expression, highly\n    variable genes are selected.\n\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    flavor : {'seurat', 'cell_ranger', 'svr'}, optional (default: 'seurat')\n        Choose the flavor for computing normalized dispersion. If choosing\n        'seurat', this expects non-logarithmized data - the logarithm of mean\n        and dispersion is taken internally when `log` is at its default value\n        `True`. For 'cell_ranger', this is usually called for logarithmized data\n        - in this case you should set `log` to `False`. In their default\n        workflows, Seurat passes the cutoffs whereas Cell Ranger passes\n        `n_top_genes`.\n    min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=`None` : `float`, optional\n        If `n_top_genes` unequals `None`, these cutoffs for the means and the\n        normalized dispersions are ignored.\n    n_bins : `int` (default: 20)\n        Number of bins for binning the mean gene expression. Normalization is\n        done with respect to each bin. If just a single gene falls into a bin,\n        the normalized dispersion is artificially set to 1. You'll be informed\n        about this if you set `settings.verbosity = 4`.\n    n_top_genes : `int` or `None` (default: `None`)\n        Number of highly-variable genes to keep.\n    log : `bool`, optional (default: `True`)\n        Use the logarithm of the mean to variance ratio.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n\n    Returns\n    -------\n    If an AnnData `adata` is passed, returns or updates `adata` depending on \\\n    `copy`. It filters the `adata` and adds the annotations\n    \"\"\"\n    adata = data.copy() if copy else data\n    set_initial_size(adata)\n    if n_top_genes is not None and adata.n_vars < n_top_genes:\n        logg.info('Skip filtering by dispersion since number of variables are less than `n_top_genes`')\n    else:\n        if flavor is 'svr':\n            mu = adata.X.mean(0).A1 if issparse(adata.X) else adata.X.mean(0)\n            sigma = np.sqrt(adata.X.multiply(adata.X).mean(0).A1 - mu ** 2) if issparse(adata.X) else adata.X.std(0)\n            log_mu = np.log2(mu)\n            log_cv = np.log2(sigma / mu)\n\n            from sklearn.svm import SVR\n            clf = SVR(gamma=150. / len(mu))\n            clf.fit(log_mu[:, None], log_cv)\n            score = log_cv - clf.predict(log_mu[:, None])\n            nth_score = np.sort(score)[::-1][n_top_genes]\n            adata._inplace_subset_var(score >= nth_score)\n        else:\n            from scanpy.api.pp import filter_genes_dispersion\n            filter_genes_dispersion(adata, flavor=flavor, min_disp=min_disp, max_disp=max_disp, min_mean=min_mean,\n                                    max_mean=max_mean, n_bins=n_bins, n_top_genes=n_top_genes, log=log)\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing each cell by total counts over all genes.", "response": "def normalize_per_cell(data, counts_per_cell_after=None, counts_per_cell=None, key_n_counts=None,\n                       max_proportion_per_cell=None, use_initial_size=True, layers=['spliced', 'unspliced'],\n                       enforce=False, copy=False):\n    \"\"\"Normalize each cell by total counts over all genes.\n\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    counts_per_cell_after : `float` or `None`, optional (default: `None`)\n        If `None`, after normalization, each cell has a total count equal\n        to the median of the *counts_per_cell* before normalization.\n    counts_per_cell : `np.array`, optional (default: `None`)\n        Precomputed counts per cell.\n    key_n_counts : `str`, optional (default: `'n_counts'`)\n        Name of the field in `adata.obs` where the total counts per cell are\n        stored.\n    max_proportion_per_cell : `int` (default: `None`)\n        Exclude genes counts that account for more than a specific proportion of cell size, e.g. 0.05.\n    use_initial_size : `bool` (default: `True`)\n        Whether to use initial cell sizes oder actual cell sizes.\n    layers : `str` or `list` (default: `{'spliced', 'unspliced'}`)\n        Keys for layers to be also considered for normalization.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n\n    Returns\n    -------\n    Returns or updates `adata` with normalized version of the original `adata.X`, depending on `copy`.\n    \"\"\"\n    adata = data.copy() if copy else data\n    layers = adata.layers.keys() if layers is 'all' else [layers] if isinstance(layers, str) \\\n        else [layer for layer in layers if layer in adata.layers.keys()]\n    layers = ['X'] + layers\n    modified_layers = []\n\n    for layer in layers:\n        X = adata.X if layer is 'X' else adata.layers[layer]\n        if not_yet_normalized(X) or enforce:\n            counts = counts_per_cell if counts_per_cell is not None \\\n                else get_initial_size(adata, layer) if use_initial_size else get_size(adata, layer)\n            if max_proportion_per_cell is not None and (0 < max_proportion_per_cell < 1):\n                counts = counts_per_cell_quantile(X, max_proportion_per_cell, counts)\n            # equivalent to scanpy.pp.normalize_per_cell(X, counts_per_cell_after, counts)\n            counts_after = np.median(counts) if counts_per_cell_after is None else counts_per_cell_after\n            counts /= counts_after + (counts_after == 0)\n            counts += counts == 0  # to avoid division by zero\n            if issparse(X):\n                sparsefuncs.inplace_row_scale(X, 1 / counts)\n            else:\n                X /= np.array(counts[:, None])\n            modified_layers.append(layer)\n\n    adata.obs['n_counts' if key_n_counts is None else key_n_counts] = get_size(adata)\n    if len(modified_layers) > 0:\n        logg.info('Normalized count data:', ', '.join(modified_layers) + '.')\n\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log1p(data, copy=False):\n    adata = data.copy() if copy else data\n    X = (adata.X.data if issparse(adata.X) else adata.X) if isinstance(adata, AnnData) else adata\n    np.log1p(X, out=X)\n    return adata if copy else None", "response": "Logarithmize the data matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_and_normalize(data, min_counts=None, min_counts_u=None, min_cells=None, min_cells_u=None,\n                         min_shared_counts=None, min_shared_cells=None, n_top_genes=None, flavor='seurat', log=True,\n                         copy=False):\n    \"\"\"Filtering, normalization and log transform\n\n    Expects non-logarithmized data. If using logarithmized data, pass `log=False`.\n\n    Runs the following steps\n\n    .. code:: python\n\n        scv.pp.filter_genes(adata)\n        scv.pp.normalize_per_cell(adata)\n        if n_top_genes is not None:\n            scv.pp.filter_genes_dispersion(adata)\n        if log:\n            scv.pp.log1p(adata)\n\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    min_counts: `int` (default: `None`)\n        Minimum number of counts required for a gene to pass filtering (spliced).\n    min_counts_u: `int` (default: `None`)\n        Minimum number of counts required for a gene to pass filtering (unspliced).\n    min_cells: `int` (default: `None`)\n        Minimum number of cells expressed required for a gene to pass filtering (spliced).\n    min_cells_u: `int` (default: `None`)\n        Minimum number of cells expressed required for a gene to pass filtering (unspliced).\n    min_shared_counts: `int`, optional (default: `None`)\n        Minimum number of counts (in cells expressed simultaneously in unspliced and spliced) required for a gene.\n    min_shared_cells: `int`, optional (default: `None`)\n        Minimum number of cells required for a gene to be expressed simultaneously in unspliced and spliced.\n    n_top_genes: `int` (default: `None`)\n        Number of genes to keep.\n    flavor: {'seurat', 'cell_ranger', 'svr'}, optional (default: 'seurat')\n        Choose the flavor for computing normalized dispersion. If choosing 'seurat', this expects non-logarithmized data.\n    log: `bool` (default: `True`)\n        Take logarithm.\n    copy: `bool` (default: `False`)\n        Return a copy of `adata` instead of updating it.\n\n    Returns\n    -------\n    Returns or updates `adata` depending on `copy`.\n    \"\"\"\n    adata = data.copy() if copy else data\n\n    if 'spliced' not in adata.layers.keys() or 'unspliced' not in adata.layers.keys():\n        raise ValueError('Could not find spliced / unspliced counts.')\n\n    filter_genes(adata, min_counts=min_counts, min_counts_u=min_counts_u, min_cells=min_cells, min_cells_u=min_cells_u,\n                 min_shared_counts=min_shared_counts, min_shared_cells=min_shared_cells,)\n    normalize_per_cell(adata)\n    if n_top_genes is not None:\n        filter_genes_dispersion(adata, n_top_genes=n_top_genes, flavor=flavor)\n\n    log_advised = np.allclose(adata.X[:10].sum(), adata.layers['spliced'][:10].sum())\n    if log and log_advised:\n        log1p(adata)\n\n    logg.info('Logarithmized X.' if log and log_advised else\n              'Did not modify X as it looks preprocessed already.' if log else\n              'Consider logarithmizing X with `scv.pp.log1p` for better results.' if log_advised else '')\n\n    return adata if copy else None", "response": "Filter and normalize a set of genes and cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recipe_velocity(adata, min_counts=3, min_counts_u=3, n_top_genes=None, n_pcs=30, n_neighbors=30, log=True, copy=False):\n    from .moments import moments\n    filter_and_normalize(adata, min_counts=min_counts, min_counts_u=min_counts_u, n_top_genes=n_top_genes, log=log)\n    moments(adata, n_neighbors=n_neighbors, n_pcs=n_pcs)\n    return adata if copy else None", "response": "Runs filter_and_normalize and moments on the adata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef velocity_embedding(adata, basis=None, vkey='velocity', density=None, arrow_size=None, arrow_length=None, scale=None,\n                       X=None, V=None, color=None, use_raw=None, layer=None, color_map=None, colorbar=True,\n                       palette=None, size=None, alpha=.2, perc=None, sort_order=True, groups=None, components=None,\n                       projection='2d', legend_loc='none', legend_fontsize=None, legend_fontweight=None,\n                       right_margin=None, left_margin=None, xlabel=None, ylabel=None, title=None, fontsize=None,\n                       figsize=None, dpi=None, frameon=None, show=True, save=None, ax=None, ncols=None, **kwargs):\n    \"\"\"\\\n    Scatter plot of velocities on the embedding.\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    x: `str`, `np.ndarray` or `None` (default: `None`)\n        x coordinate\n    y: `str`, `np.ndarray` or `None` (default: `None`)\n        y coordinate\n    vkey: `str` or `None` (default: `None`)\n        Key for annotations of observations/cells or variables/genes.\n    density: `float` (default: 1)\n        Amount of velocities to show - 0 none to 1 all\n    arrow_size: `float` or 3-tuple for headlength, headwidth and headaxislength (default: 1)\n        Size of arrows.\n    arrow_length: `float` (default: 1)\n        Length of arrows.\n    scale: `float` (default: 1)\n        Length of velocities in the embedding.\n    {scatter}\n\n    Returns\n    -------\n        `matplotlib.Axis` if `show==False`\n    \"\"\"\n    basis = default_basis(adata) if basis is None else basis\n    colors, layers, vkeys = make_unique_list(color, allow_array=True), make_unique_list(layer), make_unique_list(vkey)\n    for key in vkeys:\n        if key + '_' + basis not in adata.obsm_keys() and V is None:\n            compute_velocity_embedding(adata, basis=basis, vkey=key)\n\n    scatter_kwargs = {\"basis\": basis, \"perc\": perc, \"use_raw\": use_raw, \"sort_order\": sort_order, \"alpha\": alpha,\n                      \"components\": components, \"projection\": projection, \"legend_loc\": legend_loc, \"groups\": groups,\n                      \"legend_fontsize\": legend_fontsize, \"legend_fontweight\": legend_fontweight, \"palette\": palette,\n                      \"color_map\": color_map, \"frameon\": frameon, \"xlabel\": xlabel, \"ylabel\": ylabel,\n                      \"right_margin\": right_margin, \"left_margin\": left_margin, \"colorbar\": colorbar, \"dpi\": dpi,\n                      \"fontsize\": fontsize, \"show\": False, \"save\": None}\n\n    multikey = colors if len(colors) > 1 else layers if len(layers) > 1 else vkeys if len(vkeys) > 1 else None\n    if multikey is not None:\n        if isinstance(title, (list, tuple)): title *= int(np.ceil(len(multikey) / len(title)))\n        ncols = len(multikey) if ncols is None else min(len(multikey), ncols)\n        nrows = int(np.ceil(len(multikey) / ncols))\n        figsize = rcParams['figure.figsize'] if figsize is None else figsize\n        for i, gs in enumerate(\n                pl.GridSpec(nrows, ncols, pl.figure(None, (figsize[0] * ncols, figsize[1] * nrows), dpi=dpi))):\n            if i < len(multikey):\n                velocity_embedding(adata, density=density, scale=scale, size=size, ax=pl.subplot(gs),\n                                   arrow_size=arrow_size, arrow_length=arrow_length,\n                                   color=colors[i] if len(colors) > 1 else color,\n                                   layer=layers[i] if len(layers) > 1 else layer,\n                                   vkey=vkeys[i] if len(vkeys) > 1 else vkey,\n                                   title=title[i] if isinstance(title, (list, tuple)) else title,\n                                   **scatter_kwargs, **kwargs)\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax\n\n    else:\n        if projection == '3d':\n            from mpl_toolkits.mplot3d import Axes3D\n            ax = pl.figure(None, figsize, dpi=dpi).gca(projection=projection) if ax is None else ax\n        else:\n            ax = pl.figure(None, figsize, dpi=dpi).gca() if ax is None else ax\n\n        color, layer, vkey = colors[0], layers[0], vkeys[0]\n        color = default_color(adata) if color is None else color\n        size = default_size(adata) / 2 if size is None else size\n        _adata = adata[groups_to_bool(adata, groups, groupby=color)] if groups is not None and color in adata.obs.keys() else adata\n\n        density = 1 if density is None or density > 1 else density\n        ix_choice = np.random.choice(_adata.n_obs, size=int(density * _adata.n_obs), replace=False)\n\n        x, y = None if X is None else X[:, 0], None if X is None else X[:, 1]\n        X = _adata.obsm['X_' + basis][:, get_components(components, basis, projection)][ix_choice] if X is None else X[:, :2][ix_choice]\n        V = _adata.obsm[vkey + '_' + basis][:, get_components(components, basis, projection)][ix_choice] if V is None else V[:, :2][ix_choice]\n\n        hl, hw, hal = default_arrow(arrow_size)\n        scale = 1 / arrow_length if arrow_length is not None else scale if scale is not None else 1\n        quiver_kwargs = {\"scale\": scale, \"cmap\": color_map, \"angles\": 'xy', \"scale_units\": 'xy', \"width\": .0005,\n                         \"edgecolors\": 'k', \"headlength\": hl, \"headwidth\": hw, \"headaxislength\": hal, \"linewidth\": .1}\n        quiver_kwargs.update(kwargs)\n\n        c = interpret_colorkey(_adata, color, layer, perc)\n        c = c[ix_choice] if len(c) == _adata.n_obs else c\n\n        if projection == '3d' and X.shape[1] > 2 and V.shape[1] > 2:\n            V, size = V / scale / 5, size / 10\n            x0, x1, x2, v0, v1, v2 = X[:, 0], X[:, 1], X[:, 2], V[:, 0], V[:, 1], V[:, 2]\n            quiver3d_kwargs = {\"zorder\": 3, \"linewidth\": .5, \"arrow_length_ratio\": .3}\n            c = list(c) + [element for element in list(c) for _ in range(2)]\n            if is_color_like(c[0]): pl.quiver(x0, x1, x2, v0, v1, v2, color=c, **quiver3d_kwargs)\n            else: pl.quiver(x0, x1, x2, v0, v1, v2, c, **quiver3d_kwargs)\n        else:\n            if is_color_like(c[0]): pl.quiver(X[:, 0], X[:, 1], V[:, 0], V[:, 1], color=c, zorder=3, **quiver_kwargs)\n            else: pl.quiver(X[:, 0], X[:, 1], V[:, 0], V[:, 1], c, zorder=3, **quiver_kwargs)\n\n        ax = scatter(adata, x=x, y=y, layer=layer, color=color, size=size, title=title, ax=ax, zorder=0, **scatter_kwargs)\n\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax", "response": "Generate a scatter plot of velocities on the embedding."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toy_data(n_obs):\n\n    \"\"\"Random samples from Dentate Gyrus.\n    \"\"\"\n    adata = dentategyrus()\n    indices = np.random.choice(adata.n_obs, n_obs)\n    adata = adata[indices]\n    adata.obs_names = np.array(range(adata.n_obs), dtype='str')\n    adata.var_names_make_unique()\n    return adata", "response": "Randomly samples from the Dentate Gyrus dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forebrain():\n    filename = 'data/ForebrainGlut/hgForebrainGlut.loom'\n    url = 'http://pklab.med.harvard.edu/velocyto/hgForebrainGlut/hgForebrainGlut.loom'\n    adata = read(filename, backup_url=url, cleanup=True, sparse=True, cache=True)\n    adata.var_names_make_unique()\n    return adata", "response": "Developing human forebrain.\n    Forebrain tissue of a week 10 embryo focusing on the glutamatergic neuronal lineage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsimulate a mRNA metabolism with transcription splicing and degradation.", "response": "def simulation(n_obs=300, n_vars=None, alpha=None, beta=None, gamma=None, alpha_=None, t_max=None,\n               noise_model='normal', noise_level=1, switches=None):\n    \"\"\"Simulation of mRNA metabolism with transcription, splicing and degradation\n\n    Returns\n    -------\n    Returns `adata` object\n    \"\"\"\n    from .tools.dynamical_model_utils import unspliced, spliced, vectorize\n\n    def draw_poisson(n):\n        from random import uniform  # draw from poisson\n        t = np.cumsum([- .1 * np.log(uniform(0, 1)) for _ in range(n - 1)])\n        return np.insert(t, 0, 0)  # prepend t0=0\n\n    def simulate_dynamics(tau, alpha, beta, gamma, u0, s0, noise_model, noise_level):\n        ut = unspliced(tau, u0, alpha, beta)\n        st = spliced(tau, s0, u0, alpha, beta, gamma)\n\n        if noise_model is 'normal':  # add noise\n            ut += np.random.normal(scale=noise_level * np.percentile(ut, 99) / 10, size=len(ut))\n            st += np.random.normal(scale=noise_level * np.percentile(st, 99) / 10, size=len(st))\n        ut, st = np.clip(ut, 0, None), np.clip(st, 0, None)\n        return ut, st\n\n    def simulate_gillespie(alpha, beta, gamma):\n        # update rules: transcription (u+1,s), splicing (u-1,s+1), degradation (u,s-1), nothing (u,s)\n        update_rule = np.array([[1, 0], [-1, 1], [0, -1], [0, 0]])\n\n        def update(props):\n            if np.sum(props) > 0:\n                props /= np.sum(props)\n            p_cumsum = props.cumsum()\n            p = np.random.rand()\n            i = 0\n            while p > p_cumsum[i]:\n                i += 1\n            return update_rule[i]\n\n        u, s = np.zeros(len(alpha)), np.zeros(len(alpha))\n        for i, alpha_i in enumerate(alpha):\n            u_, s_ = (u[i - 1], s[i - 1]) if i > 0 else (0, 0)\n            du, ds = update(props=np.array([alpha_i, beta * u_, gamma * s_]))\n            u[i], s[i] = (u_ + du, s_ + ds)\n        return u, s\n\n    alpha = 5 if alpha is None else alpha\n    beta = .5 if beta is None else beta\n    gamma = .3 if gamma is None else gamma\n    alpha_ = 0 if alpha_ is None else alpha_\n\n    t = draw_poisson(n_obs)\n    if t_max is not None:\n        t *= t_max / np.max(t)\n    t_max = np.max(t)\n\n    def cycle(array, n_vars=None):\n        if isinstance(array, (list, tuple)):\n            return array if n_vars is None else array * int(np.ceil(n_vars / len(array)))\n        else:\n            return [array] if n_vars is None else [array] * n_vars\n\n    # switching time point obtained as fraction of t_max rounded down\n    switches = cycle([.4, .7, 1, .1], n_vars) if switches is None else cycle(switches, n_vars)\n    t_ = np.array([np.max(t[t < t_i * t_max]) for t_i in switches])\n\n    noise_level = cycle(noise_level, len(switches) if n_vars is None else n_vars)\n\n    n_vars = max(len(switches), len(noise_level))\n    U = np.zeros(shape=(len(t), n_vars))\n    S = np.zeros(shape=(len(t), n_vars))\n    for i in range(n_vars):\n        alpha_i = alpha[i] if isinstance(alpha, (tuple, list, np.ndarray)) else alpha\n        beta_i = beta[i] if isinstance(beta, (tuple, list, np.ndarray)) else beta\n        gamma_i = gamma[i] if isinstance(gamma, (tuple, list, np.ndarray)) else gamma\n        tau, alpha_vec, u0_vec, s0_vec = vectorize(t, t_[i], alpha_i, beta_i, gamma_i, alpha_=alpha_, u0=0, s0=0)\n        beta_i\n        if noise_model is 'gillespie':\n            U[:, i], S[:, i] = simulate_gillespie(alpha_vec, beta, gamma)\n        else:\n            U[:, i], S[:, i] = simulate_dynamics(tau, alpha_vec, beta_i, gamma_i,\n                                                 u0_vec, s0_vec, noise_model, noise_level[i])\n\n    obs = {'true_t': t.round(2)}\n    var = {'true_t_': t_[:n_vars],\n           'true_alpha': np.ones(n_vars) * alpha,\n           'true_beta': np.ones(n_vars) * beta,\n           'true_gamma': np.ones(n_vars) * gamma,\n           'true_scaling': np.ones(n_vars)}\n    layers = {'unspliced': U, 'spliced': S}\n\n    return AnnData(S, obs, var, layers=layers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_rcParams_scvelo(fontsize=8, color_map=None, frameon=None):\n\n    # dpi options (mpl default: 100, 100)\n    rcParams['figure.dpi'] = 100\n    rcParams['savefig.dpi'] = 150\n\n    # figure (mpl default: 0.125, 0.96, 0.15, 0.91)\n    rcParams['figure.figsize'] = (7, 5)\n    rcParams['figure.subplot.left'] = 0.18\n    rcParams['figure.subplot.right'] = 0.96\n    rcParams['figure.subplot.bottom'] = 0.15\n    rcParams['figure.subplot.top'] = 0.91\n\n    # lines (defaults:  1.5, 6, 1)\n    rcParams['lines.linewidth'] = 1.5  # the line width of the frame\n    rcParams['lines.markersize'] = 6\n    rcParams['lines.markeredgewidth'] = 1\n\n    # font\n    rcParams['font.sans-serif'] = \\\n        ['Arial', 'Helvetica', 'DejaVu Sans',\n         'Bitstream Vera Sans', 'sans-serif']\n\n    fontsize = fontsize\n    labelsize = 0.92 * fontsize\n\n    # fonsizes (mpl default: 10, medium, large, medium)\n    rcParams['font.size'] = fontsize\n    rcParams['legend.fontsize'] = labelsize\n    rcParams['axes.titlesize'] = fontsize\n    rcParams['axes.labelsize'] = labelsize\n\n    # legend (mpl default: 1, 1, 2, 0.8)\n    rcParams['legend.numpoints'] = 1\n    rcParams['legend.scatterpoints'] = 1\n    rcParams['legend.handlelength'] = 0.5\n    rcParams['legend.handletextpad'] = 0.4\n\n    # color cycle\n    rcParams['axes.prop_cycle'] = cycler(color=vega_10)\n\n    # axes\n    rcParams['axes.linewidth'] = 0.8\n    rcParams['axes.edgecolor'] = 'black'\n    rcParams['axes.facecolor'] = 'white'\n\n    # ticks (mpl default: k, k, medium, medium)\n    rcParams['xtick.color'] = 'k'\n    rcParams['ytick.color'] = 'k'\n    rcParams['xtick.labelsize'] = labelsize\n    rcParams['ytick.labelsize'] = labelsize\n\n    # axes grid (mpl default: False, #b0b0b0)\n    rcParams['axes.grid'] = False\n    rcParams['grid.color'] = '.8'\n\n    # color map\n    rcParams['image.cmap'] = 'RdBu_r' if color_map is None else color_map\n\n    # frame (mpl default: True)\n    frameon = False if frameon is None else frameon\n    global _frameon\n    _frameon = frameon", "response": "Set matplotlib. rcParams to scvelo defaults."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset matplotlib. rcParams to Scanpy defaults.", "response": "def set_rcParams_scanpy(fontsize=14, color_map=None, frameon=None):\n    \"\"\"Set matplotlib.rcParams to Scanpy defaults.\"\"\"\n\n    # dpi options\n    rcParams['figure.dpi'] = 100\n    rcParams['savefig.dpi'] = 150\n\n    # figure\n    rcParams['figure.figsize'] = (4, 4)\n    rcParams['figure.subplot.left'] = 0.18\n    rcParams['figure.subplot.right'] = 0.96\n    rcParams['figure.subplot.bottom'] = 0.15\n    rcParams['figure.subplot.top'] = 0.91\n\n    rcParams['lines.linewidth'] = 1.5  # the line width of the frame\n    rcParams['lines.markersize'] = 6\n    rcParams['lines.markeredgewidth'] = 1\n\n    # font\n    rcParams['font.sans-serif'] = [\n        'Arial', 'Helvetica', 'DejaVu Sans',\n        'Bitstream Vera Sans', 'sans-serif']\n\n    fontsize = fontsize\n    labelsize = 0.92 * fontsize\n\n    rcParams['font.size'] = fontsize\n    rcParams['legend.fontsize'] = labelsize\n    rcParams['axes.titlesize'] = fontsize\n    rcParams['axes.labelsize'] = fontsize\n\n    # legend\n    rcParams['legend.numpoints'] = 1\n    rcParams['legend.scatterpoints'] = 1\n    rcParams['legend.handlelength'] = 0.5\n    rcParams['legend.handletextpad'] = 0.4\n\n    # color cycle\n    rcParams['axes.prop_cycle'] = cycler(color=vega_20)\n\n    # lines\n    rcParams['axes.linewidth'] = 0.8\n    rcParams['axes.edgecolor'] = 'black'\n    rcParams['axes.facecolor'] = 'white'\n\n    # ticks\n    rcParams['xtick.color'] = 'k'\n    rcParams['ytick.color'] = 'k'\n    rcParams['xtick.labelsize'] = fontsize\n    rcParams['ytick.labelsize'] = fontsize\n\n    # axes grid\n    rcParams['axes.grid'] = True\n    rcParams['grid.color'] = '.8'\n\n    # color map\n    rcParams['image.cmap'] = rcParams['image.cmap'] if color_map is None else color_map\n\n    # frame\n    frameon = True if frameon is None else frameon\n    global _frameon\n    _frameon = frameon"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the default parameters for the figure.", "response": "def set_figure_params(style='scvelo', figsize=None, dpi=None, dpi_save=None, frameon=None, vector_friendly=True,\n                      color_map=None, format='pdf', transparent=False, ipython_format='png2x'):\n    \"\"\"Set resolution/size, styling and format of figures.\n\n    Arguments\n    ---------\n    style : `str` (default: `None`)\n        Init default values for ``matplotlib.rcParams`` suited for `scvelo` or `scanpy`.\n        Use `None` for the default matplotlib values.\n    figsize: `[float, float]` (default: `None`)\n        Width and height for default figure size.\n    dpi : `int` (default: `None`)\n        Resolution of rendered figures - this influences the size of figures in notebooks.\n    dpi_save : `int` (default: `None`)\n        Resolution of saved figures. This should typically be higher to achieve\n        publication quality.\n    frameon : `bool` (default: `None`)\n        Add frames and axes labels to scatter plots.\n    vector_friendly : `bool` (default: `True`)\n        Plot scatter plots using `png` backend even when exporting as `pdf` or `svg`.\n    color_map : `str` (default: `None`)\n        Convenience method for setting the default color map.\n    format : {'png', 'pdf', 'svg', etc.} (default: 'pdf')\n        This sets the default format for saving figures: `file_format_figs`.\n    transparent : `bool` (default: `True`)\n        Save figures with transparent back ground. Sets\n        `rcParams['savefig.transparent']`.\n    ipython_format : list of `str` (default: 'png2x')\n        Only concerns the notebook/IPython environment; see\n        `IPython.core.display.set_matplotlib_formats` for more details.\n    \"\"\"\n    try:\n        import IPython\n        IPython.core.display.set_matplotlib_formats(ipython_format)\n    except:\n        pass\n    from matplotlib import rcParams\n    global _rcParams_style\n    _rcParams_style = style\n    global _vector_friendly\n    _vector_friendly = vector_friendly\n    global file_format_figs\n    file_format_figs = format\n    if transparent is not None:\n        rcParams['savefig.transparent'] = transparent\n    if style is 'scvelo':\n        set_rcParams_scvelo(color_map=color_map, frameon=frameon)\n    elif style is 'scanpy':\n        set_rcParams_scanpy(color_map=color_map, frameon=frameon)\n    # Overwrite style options if given\n    if figsize is not None:\n        rcParams['figure.figsize'] = figsize\n    if dpi is not None:\n        rcParams['figure.dpi'] = dpi\n    if dpi_save is not None:\n        rcParams['savefig.dpi'] = dpi_save"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sum_obs(A):\n    return A.sum(0).A1 if issparse(A) else np.einsum('ij -> j', A)", "response": "summation over axis 0 equivalent to np. sum ( A 1 )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndotting product and sum over axis 1 equivalent to np. sum over axis 1", "response": "def prod_sum_var(A, B):\n    \"\"\"dot product and sum over axis 1 (var) equivalent to np.sum(A * B, 1)\n    \"\"\"\n    return A.multiply(B).sum(1).A1 if issparse(A) else np.einsum('ij, ij -> i', A, B)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef norm(A):\n    return np.sqrt(A.multiply(A).sum(1).A1) if issparse(A) else np.sqrt(np.einsum('ij, ij -> i', A, A))", "response": "computes the L2 - norm along axis 1 equivalent to np. linalg. norm"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef strings_to_categoricals(adata):\n    from pandas import Categorical\n\n    def is_valid_dtype(values):\n        from pandas.api.types import is_string_dtype, is_integer_dtype, is_bool_dtype\n        return is_string_dtype(values) or is_integer_dtype(values) or is_bool_dtype(values)\n\n    for df in [adata.obs, adata.var]:\n        string_cols = [key for key in df.columns if is_valid_dtype(df[key])]\n        for key in string_cols:\n            c = df[key]\n            c = Categorical(c)\n            if len(c.categories) < min(len(c), 100): df[key] = c", "response": "Transform string annotations to categoricals."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef principal_curve(data, basis='pca', n_comps=4, clusters_list=None, copy=False):\n    adata = data.copy() if copy else data\n    import rpy2.robjects as robjects\n    from rpy2.robjects.packages import importr\n\n    if clusters_list is not None:\n        cell_subset = np.array([label in clusters_list for label in adata.obs['clusters']])\n        X_emb = adata[cell_subset].obsm['X_' + basis][:, :n_comps]\n    else:\n        cell_subset = None\n        X_emb = adata.obsm['X_' + basis][:, :n_comps]\n\n    n_obs, n_dim = X_emb.shape\n\n    # convert array to R matrix\n    xvec = robjects.FloatVector(X_emb.T.reshape((X_emb.size)))\n    X_R = robjects.r.matrix(xvec, nrow=n_obs, ncol=n_dim)\n\n    fit = importr(\"princurve\").principal_curve(X_R)\n\n    adata.uns['principal_curve'] = dict()\n    adata.uns['principal_curve']['ixsort'] = ixsort = np.array(fit[1])-1\n    adata.uns['principal_curve']['projections'] = np.array(fit[0])[ixsort]\n    adata.uns['principal_curve']['arclength'] = np.array(fit[2])\n    adata.uns['principal_curve']['cell_subset'] = cell_subset\n\n    return adata if copy else None", "response": "Computes the principal curve of a single cell."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef velocity_embedding_stream(adata, basis=None, vkey='velocity', density=None, smooth=None, linewidth=None,\n                              n_neighbors=None, X=None, V=None, X_grid=None, V_grid=None, color=None, use_raw=None,\n                              layer=None, color_map=None, colorbar=True, palette=None, size=None, alpha=.1, perc=None,\n                              sort_order=True, groups=None, components=None, legend_loc='on data',\n                              legend_fontsize=None, legend_fontweight=None, right_margin=None, left_margin=None,\n                              xlabel=None, ylabel=None, title=None, fontsize=None, figsize=None, dpi=None, frameon=None,\n                              show=True, save=None, ax=None, ncols=None, **kwargs):\n    \"\"\"\\\n    Stream plot of velocities on the embedding.\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    x: `str`, `np.ndarray` or `None` (default: `None`)\n        x coordinate\n    y: `str`, `np.ndarray` or `None` (default: `None`)\n        y coordinate\n    vkey: `str` or `None` (default: `None`)\n        Key for annotations of observations/cells or variables/genes.\n    density: `float` (default: 1)\n        Amount of velocities to show - 0 none to 1 all\n    smooth: `float` (default: 0.5)\n        Multiplication factor for scale in Gaussian kernel around grid point.\n    linewidth: `float` (default: 1)\n        Line width for streamplot.\n    n_neighbors: `int` (default: None)\n        Number of neighbors to consider around grid point.\n    X: `np.ndarray` (default: None)\n        Embedding grid point coordinates\n    V: `np.ndarray` (default: None)\n        Embedding grid velocity coordinates\n    {scatter}\n\n    Returns\n    -------\n        `matplotlib.Axis` if `show==False`\n    \"\"\"\n    basis = default_basis(adata) if basis is None else basis\n    colors, layers, vkeys = make_unique_list(color, allow_array=True), make_unique_list(layer), make_unique_list(vkey)\n    for key in vkeys:\n        if key + '_' + basis not in adata.obsm_keys() and V is None:\n            velocity_embedding(adata, basis=basis, vkey=key)\n    color, layer, vkey = colors[0], layers[0], vkeys[0]\n    color = default_color(adata) if color is None else color\n\n    if X_grid is None or V_grid is None:\n        _adata = adata[groups_to_bool(adata, groups, groupby=color)] \\\n            if groups is not None and color in adata.obs.keys() else adata\n        X_emb = _adata.obsm['X_' + basis][:, get_components(components, basis)] if X is None else X[:, :2]\n        V_emb = _adata.obsm[vkey + '_' + basis][:, get_components(components, basis)] if V is None else V[:, :2]\n        X_grid, V_grid = compute_velocity_on_grid(X_emb=X_emb, V_emb=V_emb, density=1, smooth=smooth,\n                                                  n_neighbors=n_neighbors, autoscale=False, adjust_for_stream=True)\n        lengths = np.sqrt((V_grid ** 2).sum(0))\n        linewidth = 1 if linewidth is None else linewidth\n        linewidth *= 2 * lengths / lengths[~np.isnan(lengths)].max()\n\n    scatter_kwargs = {\"basis\": basis, \"perc\": perc, \"use_raw\": use_raw, \"sort_order\": sort_order, \"alpha\": alpha,\n                      \"components\": components, \"legend_loc\": legend_loc, \"groups\": groups,\n                      \"legend_fontsize\": legend_fontsize, \"legend_fontweight\": legend_fontweight, \"palette\": palette,\n                      \"color_map\": color_map, \"frameon\": frameon, \"xlabel\": xlabel, \"ylabel\": ylabel,\n                      \"right_margin\": right_margin, \"left_margin\": left_margin, \"colorbar\": colorbar, \"dpi\": dpi,\n                      \"fontsize\": fontsize, \"show\": False, \"save\": None}\n\n    multikey = colors if len(colors) > 1 else layers if len(layers) > 1 else vkeys if len(vkeys) > 1 else None\n    if multikey is not None:\n        if isinstance(title, (list, tuple)): title *= int(np.ceil(len(multikey) / len(title)))\n        ncols = len(multikey) if ncols is None else min(len(multikey), ncols)\n        nrows = int(np.ceil(len(multikey) / ncols))\n        figsize = rcParams['figure.figsize'] if figsize is None else figsize\n        for i, gs in enumerate(\n                pl.GridSpec(nrows, ncols, pl.figure(None, (figsize[0] * ncols, figsize[1] * nrows), dpi=dpi))):\n            if i < len(multikey):\n                velocity_embedding_stream(adata, density=density, size=size, smooth=smooth, n_neighbors=n_neighbors,\n                                          linewidth=linewidth, ax=pl.subplot(gs),\n                                          color=colors[i] if len(colors) > 1 else color,\n                                          layer=layers[i] if len(layers) > 1 else layer,\n                                          vkey=vkeys[i] if len(vkeys) > 1 else vkey,\n                                          title=title[i] if isinstance(title, (list, tuple)) else title,\n                                          X_grid=None if len(vkeys) > 1 else X_grid,\n                                          V_grid=None if len(vkeys) > 1 else V_grid, **scatter_kwargs, **kwargs)\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax\n\n    else:\n        ax = pl.figure(None, figsize, dpi=dpi).gca() if ax is None else ax\n\n        density = 1 if density is None else density\n        stream_kwargs = {\"linewidth\": linewidth, \"density\": 2 * density}\n        stream_kwargs.update(kwargs)\n        pl.streamplot(X_grid[0], X_grid[1], V_grid[0], V_grid[1], color='grey', zorder=3, **stream_kwargs)\n\n        size = 4 * default_size(adata) if size is None else size\n        ax = scatter(adata, layer=layer, color=color, size=size, title=title, ax=ax, zorder=0, **scatter_kwargs)\n\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax", "response": "Streamplot of velocities on the embedding grid point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_obs_names(data, base='[AGTCBDHKMNRSVWY]', ID_length=12, copy=False):\n    def get_base_list(name, base):\n        base_list = base\n        while re.search(base_list + base, name) is not None:\n            base_list += base\n        if len(base_list) == 0:\n            raise ValueError('Encountered an invalid ID in obs_names: ', name)\n        return base_list\n\n    adata = data.copy() if copy else data\n\n    names = adata.obs_names\n    base_list = get_base_list(names[0], base)\n\n    if len(np.unique([len(name) for name in adata.obs_names])) == 1:\n        start, end = re.search(base_list, names[0]).span()\n        newIDs = [name[start:end] for name in names]\n        start, end = 0, len(newIDs[0])\n        for i in range(end - ID_length):\n            if np.any([ID[i] not in base for ID in newIDs]): start += 1\n            if np.any([ID[::-1][i] not in base for ID in newIDs]): end -= 1\n\n        newIDs = [ID[start:end] for ID in newIDs]\n        prefixes = [names[i].replace(newIDs[i], '') for i in range(len(names))]\n    else:\n        prefixes, newIDs = [], []\n        for name in names:\n            match = re.search(base_list, name)\n            newID = re.search(get_base_list(name, base), name).group() if match is None else match.group()\n            newIDs.append(newID)\n            prefixes.append(name.replace(newID, ''))\n\n    adata.obs_names = newIDs\n    if len(prefixes[0]) > 0 and len(np.unique(prefixes)) > 1:\n        #idx_names = np.random.choice(len(names), size=20, replace=False)\n        #for i in range(len(names[0])):\n        #    if np.all([re.search(names[0][:i], names[ix]) for ix in idx_names]) is not None: obs_key = names[0][:i]\n        adata.obs['sample_batch'] = pd.Categorical(prefixes) if len(np.unique(prefixes)) < adata.n_obs else prefixes\n\n    adata.obs_names_make_unique()\n    return adata if copy else None", "response": "Cleans up the obs_names and identifies sample names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge(adata, ldata, copy=True):\n    if 'spliced' in ldata.layers.keys() and 'initial_size_spliced' not in ldata.obs.keys(): set_initial_size(ldata)\n    elif 'spliced' in adata.layers.keys() and 'initial_size_spliced' not in adata.obs.keys(): set_initial_size(adata)\n\n    common_obs = adata.obs_names.intersection(ldata.obs_names)\n    common_vars = adata.var_names.intersection(ldata.var_names)\n\n    if len(common_obs) == 0:\n        clean_obs_names(adata)\n        clean_obs_names(ldata)\n        common_obs = adata.obs_names.intersection(ldata.obs_names)\n\n    if copy:\n        _adata = adata[common_obs].copy() if adata.shape[1] >= ldata.shape[1] else ldata[common_obs].copy()\n        _ldata = ldata[common_obs].copy() if adata.shape[1] >= ldata.shape[1] else adata[common_obs].copy()\n    else:\n        adata._inplace_subset_obs(common_obs)\n        _adata, _ldata = adata, ldata[common_obs]\n\n    same_vars = (len(_adata.var_names) == len(_ldata.var_names) and np.all(_adata.var_names == _ldata.var_names))\n    if len(common_vars) > 0 and not same_vars:\n        _adata._inplace_subset_var(common_vars)\n        _ldata._inplace_subset_var(common_vars)\n\n    for attr in _ldata.obs.keys():\n        _adata.obs[attr] = _ldata.obs[attr]\n    for attr in _ldata.obsm.keys():\n        _adata.obsm[attr] = _ldata.obsm[attr]\n    for attr in _ldata.uns.keys():\n        _adata.uns[attr] = _ldata.uns[attr]\n    for attr in _ldata.layers.keys():\n        _adata.layers[attr] = _ldata.layers[attr]\n\n    if _adata.shape[1] == _ldata.shape[1]:\n        same_vars = (len(_adata.var_names) == len(_ldata.var_names) and np.all(_adata.var_names == _ldata.var_names))\n        if same_vars:\n            for attr in _ldata.var.keys():\n                _adata.var[attr] = _ldata.var[attr]\n            for attr in _ldata.varm.keys():\n                _adata.varm[attr] = _ldata.varm[attr]\n        else:\n            raise ValueError('Variable names are not identical.')\n\n    return _adata if copy else None", "response": "Merges two annotated data matrices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recover_dynamics(data, var_names='all', max_iter=100, learning_rate=None, add_key='fit', t_max=None, use_raw=False,\n                     min_loss=True, fix_scaling=None, load_pars=None, return_model=False, plot_results=False, copy=False, **kwargs):\n    \"\"\"Estimates velocities in a gene-specific manner\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n\n    Returns\n    -------\n    Returns or updates `adata`\n    \"\"\"\n    adata = data.copy() if copy else data\n    logg.info('recovering dynamics', r=True)\n\n    if var_names is 'all':\n        var_names = adata.var_names.tolist()\n    else:\n        var_names = make_unique_list(var_names, allow_array=True)\n        var_names = [name for name in var_names if name in adata.var_names]\n\n    if 'velocity_genes' in var_names and 'velocity_genes' in adata.var.keys():\n        var_names = [name for name in var_names if adata[:, name].var.velocity_genes.values]\n\n    alpha, beta, gamma, t_, scaling = read_pars(adata)\n    idx = []\n    L, P, T = [], [], adata.layers['fit_t'] if 'fit_t' in adata.layers.keys() else np.zeros(adata.shape) * np.nan\n\n    for i, gene in enumerate(var_names):\n        dm = DynamicsRecovery(adata, gene, use_raw=use_raw, load_pars=load_pars, fix_scaling=fix_scaling)\n        if max_iter > 1:\n            dm.fit(max_iter, learning_rate, **kwargs)\n\n        ix = np.where(adata.var_names == gene)[0][0]\n        idx.append(ix)\n\n        alpha[ix], beta[ix], gamma[ix], t_[ix], scaling[ix] = dm.pars[:, np.argmin(dm.loss) if min_loss else -1]\n        T[:, ix] = dm.t\n        L.append(dm.loss)\n        if plot_results and i < 4:\n            P.append(dm.pars)\n\n    T_max = np.percentile(T, 95, axis=0) - np.percentile(T, 5, axis=0)\n    m = t_max / T_max if t_max is not None else np.ones(adata.n_vars)\n    alpha, beta, gamma, T, t_ = alpha / m, beta / m, gamma / m, T * m, t_ * m\n\n    write_pars(adata, [alpha, beta, gamma, t_, scaling])\n    adata.layers['fit_t'] = T\n\n    cur_len = adata.varm['loss'].shape[1] if 'loss' in adata.varm.keys() else 2\n    max_len = max(np.max([len(l) for l in L]), cur_len)\n    loss = np.ones((adata.n_vars, max_len)) * np.nan\n\n    if 'loss' in adata.varm.keys():\n        loss[:, :cur_len] = adata.varm['loss']\n\n    loss[idx] = np.vstack([np.concatenate([l, np.ones(max_len-len(l)) * np.nan]) for l in L])\n    adata.varm['loss'] = loss\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added \\n' \n              '    \\'' + add_key + '_pars' + '\\', fitted parameters for splicing dynamics (adata.var)')\n\n    if plot_results:  # Plot Parameter Stats\n        figsize = [12, 5]  # rcParams['figure.figsize']\n        fontsize = rcParams['font.size']\n        fig, axes = pl.subplots(nrows=len(var_names[:4]), ncols=6, figsize=figsize)\n        pl.subplots_adjust(wspace=0.7, hspace=0.5)\n        for i, gene in enumerate(var_names[:4]):\n            P[i] *= np.array([1 / m[idx[i]], 1 / m[idx[i]], 1 / m[idx[i]], m[idx[i]], 1])[:, None]\n            for j, pij in enumerate(P[i]):\n                axes[i][j].plot(pij)\n            axes[i][len(P[i])].plot(L[i])\n            if i == 0:\n                for j, name in enumerate(['alpha', 'beta', 'gamma', 't_', 'scaling', 'loss']):\n                    axes[i][j].set_title(name, fontsize=fontsize)\n\n    return dm if return_model else adata if copy else None", "response": "This function calculates dynamics in a gene - specific manner."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_obj_module(qualname):\n    modname = qualname\n    classname = None\n    attrname = None\n    while modname not in sys.modules:\n        attrname = classname\n        modname, classname = modname.rsplit('.', 1)\n\n    # retrieve object and find original module name\n    if classname:\n        cls = getattr(sys.modules[modname], classname)\n        modname = cls.__module__\n        obj = getattr(cls, attrname) if attrname else cls\n    else:\n        obj = None\n\n    return obj, sys.modules[modname]", "response": "Get a module class attribute and its original module by qualname"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets an object s line numbers", "response": "def get_linenos(obj):\n    \"\"\"Get an object\u2019s line numbers\"\"\"\n    try:\n        lines, start = inspect.getsourcelines(obj)\n    except TypeError:\n        return None, None\n    else:\n        return start, start + len(lines) - 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef modurl(qualname):\n    obj, module = get_obj_module(qualname)\n    github_url = github_url1\n    try:\n        path = PurePosixPath(Path(module.__file__).resolve().relative_to(project_dir))\n    except ValueError:\n        # trying to document something from another package\n        github_url = github_url2\n        path = '/'.join(module.__file__.split('/')[-2:])\n    start, end = get_linenos(obj)\n    fragment = '#L{}-L{}'.format(start, end) if start and end else ''\n    return '{}/{}{}'.format(github_url, path, fragment)", "response": "Get the full GitHub URL for some object s qualname."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_version_from_dirname(name, parent):\n    parent = parent.resolve()\n\n    re_dirname = re.compile(f\"{name}-{RE_VERSION}$\")\n    if not re_dirname.match(parent.name):\n        return None\n\n    return Version.parse(parent.name[len(name) + 1 :])", "response": "Extracted sdist from parent directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_version(package: Union[Path, str]) -> str:\n    path = Path(package)\n    if not path.suffix and len(path.parts) == 1:  # Is probably not a path\n        v = get_version_from_metadata(package)\n        if v:\n            return str(v)\n\n    if path.suffix != \".py\":\n        msg = f\"\u201cpackage\u201d is neither the name of an installed module nor the path to a .py file.\"\n        if path.suffix:\n            msg += f\" Unknown file suffix {path.suffix}\"\n        raise ValueError(msg)\n    if path.name == \"__init__.py\":\n        name = path.parent.name\n        parent = path.parent.parent\n    else:\n        name = path.with_suffix(\"\").name\n        parent = path.parent\n\n    return str(\n        get_version_from_dirname(name, parent)\n        or get_version_from_git(parent)\n        or get_version_from_metadata(name, parent)\n        or \"0.0.0\"\n    )", "response": "Get the version of a module or a package."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef velocity(adata, var_names=None, basis=None, groupby=None, groups=None, mode=None, fits='all', layers='all',\n             color=None, color_map='RdBu_r', colorbar=False, perc=[2,98], use_raw=False, size=None, alpha=.5,\n             fontsize=None, figsize=None, dpi=None, show=True, save=None, ax=None, ncols=None, **kwargs):\n    \"\"\"Phase and velocity plot for set of genes.\n\n    The phase plot shows spliced against unspliced expressions with steady-state fit.\n    Further the embedding is shown colored by velocity and expression.\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    var_names: `str` or list of `str` (default: `None`)\n        Which variables to show.\n    basis: `str` (default: `'umap'`)\n        Key for embedding coordinates.\n    mode: `'stochastic'` or `None` (default: `None`)\n        Whether to show show covariability phase portrait.\n    fits: `str` or list of `str` (default: `'all'`)\n        Which steady-state estimates to show.\n    layers: `str` or list of `str` (default: `'all'`)\n        Which layers to show.\n    color: `str`,  list of `str` or `None` (default: `None`)\n        Key for annotations of observations/cells or variables/genes\n    color_map: `str` (default: `matplotlib.rcParams['image.cmap']`)\n        String denoting matplotlib color map.\n    perc: tuple, e.g. [2,98] (default: `None`)\n        Specify percentile for continuous coloring.\n    size: `float` (default: 5)\n        Point size.\n    alpha: `float` (default: 1)\n        Set blending - 0 transparent to 1 opaque.\n    fontsize: `float` (default: `None`)\n        Label font size.\n    figsize: tuple (default: `(7,5)`)\n        Figure size.\n    dpi: `int` (default: 80)\n        Figure dpi.\n    show: `bool`, optional (default: `None`)\n        Show the plot, do not return axis.\n    save: `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the default filename.\n        Infer the filetype if ending on {'.pdf', '.png', '.svg'}.\n    ax: `matplotlib.Axes`, optional (default: `None`)\n        A matplotlib axes object. Only works if plotting a single component.\n\n    \"\"\"\n    basis = default_basis(adata) if basis is None else basis\n\n    if isinstance(groupby, str) and groupby in adata.obs.keys():\n        if 'rank_velocity_genes' not in adata.uns.keys() or adata.uns['rank_velocity_genes']['params']['groupby'] != groupby:\n            rank_velocity_genes(adata, vkey='velocity', n_genes=10, groupby=groupby)\n        names = np.array(adata.uns['rank_velocity_genes']['names'].tolist())\n        if groups is None:\n            var_names = names[:, 0]\n        else:\n            groups = [groups] if isinstance(groups, str) else groups\n            idx = np.array([any([g in group for g in groups]) for group in adata.obs[groupby].cat.categories])\n            var_names = np.hstack(names[idx, :int(10 / idx.sum())])\n    elif var_names is not None:\n        var_names = [var_names] if isinstance(var_names, str) else [var for var in var_names if var in adata.var_names]\n    else:\n        raise ValueError('No var_names or groups specified.')\n    var_names = pd.unique(var_names)\n\n    (skey, ukey) = ('spliced', 'unspliced') if use_raw else ('Ms', 'Mu')\n    layers = ['velocity', skey, 'variance_velocity'] if layers == 'all' else layers\n    layers = [layer for layer in layers if layer in adata.layers.keys()]\n\n    fits = adata.layers.keys() if fits == 'all' else fits\n    fits = [fit for fit in fits if all(['velocity' in fit, fit + '_gamma' in adata.var.keys()])]\n    stochastic_fits = [fit for fit in fits if 'variance_' + fit in adata.layers.keys()]\n\n    nplts = (1 + len(layers) + (mode == 'stochastic') * 2)\n    ncols = 1 if ncols is None else ncols\n    nrows = int(np.ceil(len(var_names) / ncols))\n    ncols = int(ncols * nplts)\n    figsize = rcParams['figure.figsize'] if figsize is None else figsize\n    ax = pl.figure(figsize=(figsize[0] * ncols / 2, figsize[1] * nrows / 2), dpi=dpi) if ax is None else ax\n    gs = pl.GridSpec(nrows, ncols, wspace=0.3, hspace=0.5)\n\n    size = default_size(adata) / 2 if size is None else size  # since fontsize is halved in width and height\n    fontsize = rcParams['font.size'] if fontsize is None else fontsize\n    for v, var in enumerate(var_names):\n        _adata = adata[:, var]\n        s, u = _adata.layers[skey], _adata.layers[ukey]\n        if issparse(s): s, u = s.A, u.A\n\n        # spliced/unspliced phase portrait with steady-state estimate\n        ax = pl.subplot(gs[v * nplts])\n        scatter(adata, basis=var, color=color, colorbar=colorbar, frameon=True, title=var, size=size, use_raw=use_raw,\n                alpha=alpha, fontsize=fontsize, xlabel='spliced', ylabel='unspliced', show=False, ax=ax, save=False,\n                legend_loc=None if v < len(var_names)-1 else 'lower right', **kwargs)\n\n        # velocity and expression plots\n        for l, layer in enumerate(layers):\n            ax = pl.subplot(gs[v * nplts + l + 1])\n            title = 'expression' if layer == skey else layer\n            scatter(adata, basis=basis, color=var, layer=layer, color_map=color_map, colorbar=colorbar, title=title,\n                    perc=perc, use_raw=use_raw, fontsize=fontsize, size=size, alpha=alpha, frameon=False, show=False, ax=ax, save=False, **kwargs)\n\n        if mode == 'stochastic':\n            ss, us = second_order_moments(_adata)\n            ss, us = ss.flatten(), us.flatten()\n            fit = stochastic_fits[0]\n\n            ax = pl.subplot(gs[v * nplts + len(layers) + 1])\n            offset = _adata.var[fit + '_offset'] if fit + '_offset' in adata.var.keys() else 0\n            beta = _adata.var[fit + '_beta'] if fit + '_beta' in adata.var.keys() else 1\n            x = 2 * (ss - s**2) - s\n            y = 2 * (us - u * s) + u + 2 * s * offset / beta\n\n            scatter(adata, x=x, y=y, color=color, colorbar=colorbar, title=var, fontsize=40/ncols, size=size, perc=perc,\n                    xlabel=r'2 $\\Sigma_s - \\langle s \\rangle$', ylabel=r'2 $\\Sigma_{us} + \\langle u \\rangle$',\n                    use_raw=use_raw, frameon=True, ax=ax, save=False, show=False, **kwargs)\n\n            xnew = np.linspace(x.min(), x.max() * 1.02)\n            for fit in stochastic_fits:\n                gamma = _adata.var[fit + '_gamma'].values if fit + '_gamma' in adata.var.keys() else 1\n                beta = _adata.var[fit + '_beta'].values if fit + '_beta' in adata.var.keys() else 1\n                offset2 = _adata.var[fit + '_offset2'].values if fit + '_offset2' in adata.var.keys() else 0\n\n                pl.plot(xnew, gamma / beta * xnew + offset2 / beta, c='k', linestyle='--')\n            if v == len(var_names) - 1: pl.legend(fits, loc='lower right', prop={'size': 34/ncols})\n\n    savefig_or_show('', dpi=dpi, save=save, show=show)\n    if not show: return ax", "response": "Phase and velocity plot for set of genes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the velocity graph based on cosine similarities.", "response": "def velocity_graph(data, vkey='velocity', xkey='Ms', tkey=None, basis=None, n_neighbors=None, n_recurse_neighbors=None,\n                   random_neighbors_at_max=None, sqrt_transform=False, approx=False, copy=False):\n    \"\"\"Computes velocity graph based on cosine similarities.\n\n    The cosine similarities are computed between velocities and potential cell state transitions.\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` (default: `'velocity'`)\n        Name of velocity estimates to be used.\n    n_neighbors: `int` or `None` (default: None)\n        Use fixed number of neighbors or do recursive neighbor search (if `None`).\n    n_recurse_neighbors: `int` (default: 2)\n        Number of recursions to be done for neighbors search.\n    random_neighbors_at_max: `int` or `None` (default: `None`)\n        If number of iterative neighbors for an individual cell is higher than this threshold,\n        a random selection of such are chosen as reference neighbors.\n    sqrt_transform: `bool` (default: `False`)\n        Whether to variance-transform the cell states changes and velocities before computing cosine similarities.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    velocity_graph: `.uns`\n        sparse matrix with transition probabilities\n    \"\"\"\n    adata = data.copy() if copy else data\n    if vkey not in adata.layers.keys(): velocity(adata, vkey=vkey)\n\n    vgraph = VelocityGraph(adata, vkey=vkey, xkey=xkey, tkey=tkey, basis=basis, n_neighbors=n_neighbors, approx=approx,\n                           n_recurse_neighbors=n_recurse_neighbors, random_neighbors_at_max=random_neighbors_at_max,\n                           sqrt_transform=sqrt_transform, report=True)\n\n    logg.info('computing velocity graph', r=True)\n    vgraph.compute_cosines()\n\n    adata.uns[vkey+'_graph'] = vgraph.graph\n    adata.uns[vkey+'_graph_neg'] = vgraph.graph_neg\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added \\n'\n        '    \\'' + vkey + '_graph\\', sparse matrix with cosine correlations (adata.uns)')\n\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef msg(*msg, v=4, time=False, memory=False, reset=False, end='\\n',\n        no_indent=False, t=None, m=None, r=None):\n    \"\"\"Write message to logging output.\n    Log output defaults to standard output but can be set to a file\n    by setting `sc.settings.log_file = 'mylogfile.txt'`.\n    v : {'error', 'warn', 'info', 'hint'} or int, (default: 4)\n        0/'error', 1/'warn', 2/'info', 3/'hint', 4, 5, 6...\n    time, t : bool, optional (default: False)\n        Print timing information; restart the clock.\n    memory, m : bool, optional (default: Faulse)\n        Print memory information.\n    reset, r : bool, optional (default: False)\n        Reset timing and memory measurement. Is automatically reset\n        when passing one of ``time`` or ``memory``.\n    end : str (default: '\\n')\n        Same meaning as in builtin ``print()`` function.\n    no_indent : bool (default: False)\n        Do not indent for ``v >= 4``.\n    \"\"\"\n    # variable shortcuts\n    if t is not None: time = t\n    if m is not None: memory = m\n    if r is not None: reset = r\n    if isinstance(v, str):\n        v = _VERBOSITY_LEVELS_FROM_STRINGS[v]\n    if v == 3:  # insert \"--> \" before hints\n        msg = ('-->',) + msg\n    if v >= 4 and not no_indent:\n        msg = ('   ',) + msg\n    if _settings_verbosity_greater_or_equal_than(v):\n        if not time and not memory and len(msg) > 0:\n            _write_log(*msg, end=end)\n        if reset:\n            try:\n                settings._previous_memory_usage, _ = get_memory_usage()\n            except:\n                pass\n            settings._previous_time = get_time()\n        if time:\n            elapsed = get_passed_time()\n            msg = msg + ('({})'.format(_sec_to_str(elapsed)),)\n            _write_log(*msg, end=end)\n        if memory:\n            _write_log(get_memory_usage(), end=end)", "response": "Write a message to the log file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_log(*msg, end='\\n'):\n    from .settings import logfile\n    if logfile == '':\n        print(*msg, end=end)\n    else:\n        out = ''\n        for s in msg:\n            out += str(s) + ' '\n        with open(logfile, 'a') as f:\n            f.write(out + end)", "response": "Write a message to log file ignoring the verbosity level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heatmap(adata, var_names, groups=None, groupby=None, annotations=None, use_raw=False, layers=['X'], color_map=None,\n            color_map_anno=None, colorbar=True, row_width=None, xlabel=None, title=None, figsize=None, dpi=None,\n            show=True, save=None, ax=None, **kwargs):\n\n    \"\"\"\\\n    Plot pseudotimeseries for genes as heatmap.\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    var_names: `str`,  list of `str`\n        Names of variables to use for the plot.\n    groups: `str`,  list of `str` or `None` (default: `None`)\n        Groups selected to plot. Must be an element of adata.obs[groupby].\n    groupby: `str` or `None` (default: `None`)\n        Key in adata.obs. Indicates how to group the plot.\n    annotations: `str`,  list of `str` or `None` (default: `None`)\n        Key in adata.obs. Annotations are plotted in the last row.\n    use_raw: `bool` (default: `False`)\n        If true, moments are used instead of raw data.\n    layers: `str`,  list of `str` or `None` (default: `['X']`)\n        Selected layers.\n    color_map: `str`,  list of `str` or `None` (default: `None`)\n        String denoting matplotlib color map for the heat map.\n        There must be one list entry for each layer.\n    color_map_anno: `str`,  list of `str` or `None` (default: `None`)\n        String denoting matplotlib color map for the annotations.\n        There must be one list entry for each annotation.\n    colorbar: `bool` (default: `True`)\n        If True, a colormap for each layer is added on the right bottom corner.\n    row_width: `float` (default: `None`)\n        Constant width of all rows.\n    xlabel:\n        Label for the x-axis.\n    title: `str` or `None` (default: `None`)\n        Main plot title.\n    figsize: tuple (default: `(7,5)`)\n        Figure size.\n    dpi: `int` (default: 80)\n        Figure dpi.\n    show: `bool`, optional (default: `None`)\n        Show the plot, do not return axis.\n    save: `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the default filename.\n        Infer the filetype if ending on {'.pdf', '.png', '.svg'}.\n    ax: `matplotlib.Axes`, optional (default: `None`)\n        A matplotlib axes object. Only works if plotting a single component.\n\n    Returns\n    -------\n        If `show==False` a `matplotlib.Axis`\n    \"\"\"\n\n    # catch\n    if 'velocity_pseudotime' not in adata.obs.keys():\n        raise ValueError(\n            'A function requires computation of the pseudotime'\n            'for ordering at single-cell resolution')\n    if annotations is None:\n        annotations = []\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    if len(var_names) == 0:\n        var_names = np.arange(adata.X.shape[1])\n    if var_names.ndim == 2:\n        var_names = var_names[:, 0]\n    var_names = [name for name in var_names if name in adata.var_names]\n    if len(var_names) == 0:\n        raise ValueError(\n            'The specified var_names are all not'\n            'contained in the adata.var_names.')\n\n    if layers is None:\n        layers = ['X']\n    if isinstance(layers, str):\n        layers = [layers]\n    layers = [layer for layer in layers if layer in adata.layers.keys() or layer == 'X']\n    if len(layers) == 0:\n        raise ValueError(\n            'The selected layers are not contained'\n            'in adata.layers.keys().')\n    if not use_raw:\n        layers = np.array(layers)\n        if 'X' in layers: layers[np.array([layer == 'X' for layer in layers])] = 'Ms'\n        if 'spliced' in layers: layers[np.array([layer == 'spliced' for layer in layers])] = 'Ms'\n        if 'unspliced' in layers: layers[np.array([layer == 'unspliced' for layer in layers])] = 'Ms'\n        layers = list(layers)\n    if 'Ms' in layers and 'Ms' not in adata.layers.keys():\n        raise ValueError(\n            'Moments have to be computed before'\n            'using this plot function.')\n    if 'Mu' in layers and 'Mu' not in adata.layers.keys():\n        raise ValueError(\n            'Moments have to be computed before'\n            'using this plot function.')\n    layers = unique(layers)\n\n    # Number of rows to plot\n    tot_len = len(var_names) * len(layers) + len(annotations)\n\n    # init main figure\n    figsize = rcParams['figure.figsize'] if figsize is None else figsize\n    if row_width is not None: figsize[1] = row_width * tot_len\n    ax = pl.figure(figsize=figsize, dpi=dpi).gca() if ax is None else ax\n    ax.set_yticks([])\n    ax.set_xticks([])\n\n    # groups bar\n    ax_bounds = ax.get_position().bounds\n    if groupby is not None:\n        # catch\n        if groupby not in adata.obs_keys():\n            raise ValueError(\n                'The selected groupby is not contained'\n                'in adata.obs_keys().')\n        if groups is None:  # Then use everything of that obs\n            groups = unique(adata.obs.clusters.values)\n\n        imlist = []\n\n        for igroup, group in enumerate(groups):\n            for ivar, var in enumerate(var_names):\n                for ilayer, layer in enumerate(layers):\n                    groups_axis = pl.axes([ax_bounds[0] + igroup * ax_bounds[2] / len(groups),\n                                           ax_bounds[1] + ax_bounds[3] * (\n                                                       tot_len - ivar * len(layers) - ilayer - 1) / tot_len,\n                                           ax_bounds[2] / len(groups),\n                                           (ax_bounds[3] - ax_bounds[3] / tot_len * len(annotations)) / (\n                                                       len(var_names) * len(layers))])\n\n                    # Get data to fill and reshape\n                    dat = adata[:, var]\n\n                    idx_group = [adata.obs[groupby] == group]\n                    idx_group = np.array(idx_group[0].tolist())\n                    idx_var = [vn in var_names for vn in adata.var_names]\n                    idx_pt = np.array(adata.obs.velocity_pseudotime).argsort()\n                    idx_pt = idx_pt[np.array(np.isnan(np.array(dat.obs.velocity_pseudotime)[idx_pt]) == False)]\n\n                    if layer == 'X':\n                        laydat = dat.X\n                    else:\n                        laydat = dat.layers[layer]\n\n                    t1, t2, t3 = idx_group, idx_var, idx_pt\n                    t1 = t1[t3]\n                    # laydat = laydat[:, t2]  # select vars\n                    laydat = laydat[t3]\n                    laydat = laydat[t1]  # select ordered groups\n\n                    if issparse(laydat):\n                        laydat = laydat.A\n\n                    # transpose X for ordering in direction var_names: up->downwards\n                    laydat = laydat.T[::-1]\n                    laydat = laydat.reshape((1, len(laydat)))  # ensure 1dimty\n\n                    # plot\n                    im = groups_axis.imshow(laydat, aspect='auto', interpolation=\"nearest\", cmap=color_map[ilayer])\n\n                    # Frames\n                    if ilayer == 0:\n                        groups_axis.spines['bottom'].set_visible(False)\n                    elif ilayer == len(layer) - 1:\n                        groups_axis.spines['top'].set_visible(False)\n                    else:\n                        groups_axis.spines['top'].set_visible(False)\n                        groups_axis.spines['bottom'].set_visible(False)\n\n                    # Further visuals\n                    if igroup == 0:\n                        if colorbar:\n                            if len(layers) % 2 == 0:\n                                if ilayer == len(layers) / 2 - 1:\n                                    pl.yticks([0.5], [var])\n                                else:\n                                    groups_axis.set_yticks([])\n                            else:\n                                if ilayer == (len(layers) - 1) / 2:\n                                    pl.yticks([0], [var])\n                                else:\n                                    groups_axis.set_yticks([])\n                        else:\n                            pl.yticks([0], [layer + ' ' + var])\n                    else:\n                        groups_axis.set_yticks([])\n\n                    groups_axis.set_xticks([])\n                    if ilayer == 0 and ivar == 0:\n                        groups_axis.set_title(str(group))\n                    groups_axis.grid(False)\n\n                    # handle needed as mappable for colorbar\n                    if igroup == len(groups) - 1:\n                        imlist.append(im)\n\n            # further annotations for each group\n            if annotations is not None:\n                for ianno, anno in enumerate(annotations):\n                    anno_axis = pl.axes([ax_bounds[0] + igroup * ax_bounds[2] / len(groups),\n                                         ax_bounds[1] + ax_bounds[3] / tot_len * (len(annotations) - ianno - 1),\n                                         ax_bounds[2] / len(groups),\n                                         ax_bounds[3] / tot_len])\n                    if is_categorical(adata, anno):\n                        colo = interpret_colorkey(adata, anno)[t3][t1]\n                        colo.reshape(1, len(colo))\n                        mapper = np.vectorize(ColorConverter.to_rgb)\n                        a = mapper(colo)\n                        a = np.array(a).T\n                        Y = a.reshape(1, len(colo), 3)\n                    else:\n                        Y = np.array(interpret_colorkey(adata, anno))[t3][t1]\n                        Y = Y.reshape(1, len(Y))\n                    img = anno_axis.imshow(Y, aspect='auto',\n                                           interpolation='nearest', cmap=color_map_anno)\n                    if igroup == 0:\n                        anno_axis.set_yticklabels(['', anno, ''])  # , fontsize=ytick_fontsize)\n                        anno_axis.tick_params(axis='both', which='both', length=0)\n                    else:\n                        anno_axis.set_yticklabels([])\n                        anno_axis.set_yticks([])\n                    anno_axis.set_xticks([])\n                    anno_axis.set_xticklabels([])\n                    anno_axis.grid(False)\n                    pl.ylim([.5, -.5])  # center ticks\n\n    else:  # groupby is False\n        imlist = []\n        for ivar, var in enumerate(var_names):\n            for ilayer, layer in enumerate(layers):\n                ax_bounds = ax.get_position().bounds\n                groups_axis = pl.axes([ax_bounds[0],\n                                       ax_bounds[1] + ax_bounds[3] * (\n                                                   tot_len - ivar * len(layers) - ilayer - 1) / tot_len,\n                                       ax_bounds[2],\n                                       (ax_bounds[3] - ax_bounds[3] / tot_len * len(annotations)) / (\n                                                   len(var_names) * len(layers))])\n                # Get data to fill\n                dat = adata[:, var]\n                idx = np.array(dat.obs.velocity_pseudotime).argsort()\n                idx = idx[np.array(np.isnan(np.array(dat.obs.velocity_pseudotime)[idx]) == False)]\n\n                if layer == 'X':\n                    laydat = dat.X\n                else:\n                    laydat = dat.layers[layer]\n                laydat = laydat[idx]\n                if issparse(laydat):\n                    laydat = laydat.A\n\n                # transpose X for ordering in direction var_names: up->downwards\n                laydat = laydat.T[::-1]\n                laydat = laydat.reshape((1, len(laydat)))\n\n                # plot\n                im = groups_axis.imshow(laydat, aspect='auto', interpolation=\"nearest\", cmap=color_map[ilayer])\n                imlist.append(im)\n\n                # Frames\n                if ilayer == 0:\n                    groups_axis.spines['bottom'].set_visible(False)\n                elif ilayer == len(layer) - 1:\n                    groups_axis.spines['top'].set_visible(False)\n                else:\n                    groups_axis.spines['top'].set_visible(False)\n                    groups_axis.spines['bottom'].set_visible(False)\n\n                # Further visuals\n                groups_axis.set_xticks([])\n                groups_axis.grid(False)\n                pl.ylim([.5, -.5])  # center\n                if colorbar:\n                    if len(layers) % 2 == 0:\n                        if ilayer == len(layers) / 2 - 1:\n                            pl.yticks([0.5], [var])\n                        else:\n                            groups_axis.set_yticks([])\n                    else:\n                        if ilayer == (len(layers) - 1) / 2:\n                            pl.yticks([0], [var])\n                        else:\n                            groups_axis.set_yticks([])\n                else:\n                    pl.yticks([0], [layer + ' ' + var])\n\n        # further annotations bars\n        if annotations is not None:\n            for ianno, anno in enumerate(annotations):\n                anno_axis = pl.axes([ax_bounds[0],\n                                     ax_bounds[1] + ax_bounds[3] / tot_len * (len(annotations) - ianno - 1),\n                                     ax_bounds[2],\n                                     ax_bounds[3] / tot_len])\n                dat = adata[:, var_names]\n                if is_categorical(dat, anno):\n                    colo = interpret_colorkey(dat, anno)[idx]\n                    colo.reshape(1, len(colo))\n                    mapper = np.vectorize(ColorConverter.to_rgb)\n                    a = mapper(colo)\n                    a = np.array(a).T\n                    Y = a.reshape(1, len(idx), 3)\n                else:\n                    Y = np.array(interpret_colorkey(dat, anno)[idx]).reshape(1, len(idx))\n                img = anno_axis.imshow(Y, aspect='auto', interpolation='nearest', cmap=color_map_anno)\n\n                anno_axis.set_yticklabels(['', anno, ''])  # , fontsize=ytick_fontsize)\n                anno_axis.tick_params(axis='both', which='both', length=0)\n                anno_axis.grid(False)\n                anno_axis.set_xticks([])\n                anno_axis.set_xticklabels([])\n                pl.ylim([-.5, +.5])\n\n    # Colorbar\n    if colorbar:\n        if len(layers) > 1:\n            # I must admit, this part is chaotic\n            for ilayer, layer in enumerate(layers):\n                w = 0.015 * 10 / figsize[0]  # 0.02 * ax_bounds[2]\n                x = ax_bounds[0] + ax_bounds[2] * 0.99 + 1.5 * w + w * 1.2 * ilayer\n                y = ax_bounds[1]\n                h = ax_bounds[3] * .3\n                cbaxes = pl.axes([x, y, w, h])\n                cb = pl.colorbar(mappable=imlist[ilayer], cax=cbaxes)\n                pl.text(x - 40 * w, y + h * 4, layer, rotation=45, horizontalalignment='left',\n                        verticalalignment='bottom')\n                if ilayer == len(layers) - 1:\n                    ext = abs(cb.vmin - cb.vmax)\n                    cb.set_ticks([cb.vmin + 0.07 * ext, cb.vmax - 0.07 * ext])\n                    cb.ax.set_yticklabels(['Low', 'High'])  # vertical colorbar\n                else:\n                    cb.set_ticks([])\n        else:\n            cbaxes = pl.axes([ax_bounds[0] + ax_bounds[2] + .01,\n                              ax_bounds[1],\n                              0.02,\n                              ax_bounds[3] * .3])\n            cb = pl.colorbar(mappable=im, cax=cbaxes)\n            cb.set_ticks([cb.vmin, cb.vmax])\n            cb.ax.set_yticklabels(['Low', 'High'])\n\n    if xlabel is None: xlabel = 'velocity' + ' ' + 'pseudotime'\n    if title is not None: ax.set_title(title, pad=30)\n    if len(annotations) == 0:\n        ax.set_xlabel(xlabel)\n        ax.xaxis.labelpad = 20\n\n    # set_label(xlabel, None, fontsize, basis)\n    # set_title(title, None, None, fontsize)\n    # ax = update_axes(ax, fontsize)\n\n    savefig_or_show('heatmap', dpi=dpi, save=save, show=show)\n    if not show: return ax", "response": "Plot pseudotimeseries for genes as heatmap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef velocity(data, vkey='velocity', mode=None, fit_offset=False, fit_offset2=False, filter_genes=False,\n             groups=None, groupby=None, groups_for_fit=None, use_raw=False, perc=[5, 95], copy=False):\n    \"\"\"Estimates velocities in a gene-specific manner\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` (default: `'velocity'`)\n        Name under which to refer to the computed velocities for `velocity_graph` and `velocity_embedding`.\n    mode: `'deterministic'`, `'stochastic'` or `'bayes'` (default: `'stochastic'`)\n        Whether to run the estimation using the deterministic or stochastic model of transcriptional dynamics.\n        `'bayes'` solves the stochastic model and accounts for heteroscedasticity, but is slower than `'stochastic'`.\n    fit_offset: `bool` (default: `False`)\n        Whether to fit with offset for first order moment dynamics.\n    fit_offset2: `bool`, (default: `False`)\n        Whether to fit with offset for second order moment dynamics.\n    filter_genes: `bool` (default: `True`)\n        Whether to remove genes that are not used for further velocity analysis.\n    groups: `str`, `list` (default: `None`)\n        Subset of groups, e.g. [\u2018g1\u2019, \u2018g2\u2019, \u2018g3\u2019], to which velocity analysis shall be restricted.\n    groupby: `str`, `list` or `np.ndarray` (default: `None`)\n        Key of observations grouping to consider.\n    groups_for_fit: `str`, `list` or `np.ndarray` (default: `None`)\n        Subset of groups, e.g. [\u2018g1\u2019, \u2018g2\u2019, \u2018g3\u2019], to which steady-state fitting shall be restricted.\n    use_raw: `bool` (default: `False`)\n        Whether to use raw data for estimation.\n    perc: `float` (default: `None`)\n        Percentile, e.g. 98, upon for extreme quantile fit (to better capture steady states for velocity estimation).\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to `adata`.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    velocity: `.layers`\n        velocity vectors for each individual cell\n    variance_velocity: `.layers`\n        velocity vectors for the cell variances\n    velocity_offset, velocity_beta, velocity_gamma, velocity_r2: `.var`\n        parameters\n    \"\"\"\n    adata = data.copy() if copy else data\n    if not use_raw and 'Ms' not in adata.layers.keys(): moments(adata)\n\n    logg.info('computing velocities', r=True)\n\n    strings_to_categoricals(adata)\n    categories = adata.obs[groupby].cat.categories \\\n        if groupby is not None and groups is None and groups_for_fit is None else [None]\n\n    for cat in categories:\n        groups = cat if cat is not None else groups\n\n        cell_subset = groups_to_bool(adata, groups, groupby)\n        _adata = adata if groups is None else adata[cell_subset]\n\n        velo = Velocity(_adata, groups_for_fit=groups_for_fit, groupby=groupby, use_raw=use_raw)\n        velo.compute_deterministic(fit_offset, perc=perc)\n\n        if any([mode is not None and mode in item for item in ['stochastic', 'bayes', 'alpha']]):\n            if filter_genes and len(set(velo._velocity_genes)) > 1:\n                adata._inplace_subset_var(velo._velocity_genes)\n                residual = velo._residual[:, velo._velocity_genes]\n                _adata = adata if groups is None else adata[cell_subset]\n                velo = Velocity(_adata, residual=residual, groups_for_fit=groups_for_fit, groupby=groupby)\n            velo.compute_stochastic(fit_offset, fit_offset2, mode, perc=perc)\n\n        write_residuals(adata, vkey, velo._residual, cell_subset)\n        write_residuals(adata, 'variance_' + vkey, velo._residual2, cell_subset)\n        write_pars(adata, vkey, velo.get_pars(), velo.get_pars_names(), add_key=cat)\n\n        if filter_genes and len(set(velo._velocity_genes)) > 1:\n            adata._inplace_subset_var(velo._velocity_genes)\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added \\n' \n              '    \\'' + vkey + '\\', velocity vectors for each individual cell (adata.layers)')\n\n    return adata if copy else None", "response": "Estimates velocities in a gene - specific manner."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef velocity_genes(data, vkey='velocity', min_r2=0.01, highly_variable=None, copy=False):\n    adata = data.copy() if copy else data\n    if vkey + '_genes' not in adata.var.keys(): velocity(data, vkey)\n\n    adata.var[vkey + '_genes'] = np.array(adata.var[vkey + '_genes'], dtype=bool) & (adata.var[vkey + '_r2'] > min_r2)\n    if highly_variable and 'highly_variable' in adata.var.keys():\n        adata.var[vkey + '_genes'] &= adata.var['highly_variable']\n\n    logg.info('Number of obtained velocity_genes:', np.sum(adata.var[vkey + '_genes']))\n\n    return adata if copy else None", "response": "Estimates velocities in a gene - specific manner."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optimize_NxN(x, y, fit_offset=False, perc=None):\n    if perc is not None:\n        if not fit_offset and isinstance(perc, (list, tuple)): perc = perc[1]\n        weights = get_weight(x, y, perc).astype(bool)\n        if issparse(weights): weights = weights.A\n    else:\n        weights = None\n\n    x, y = x.astype(np.float64), y.astype(np.float64)\n\n    n_vars = x.shape[1]\n    offset, gamma = np.zeros(n_vars), np.zeros(n_vars)\n\n    for i in range(n_vars):\n        xi = x[:, i] if weights is None else x[:, i][weights[:, i]]\n        yi = y[:, i] if weights is None else y[:, i][weights[:, i]]\n\n        if fit_offset:\n            offset[i], gamma[i] = minimize(lambda m: np.sum((-yi + xi * m[1] + m[0])**2), method=\"L-BFGS-B\",\n                                           x0=(0, 0.1), bounds=[(0, None), (None, None)]).x\n        else:\n            gamma[i] = minimize(lambda m: np.sum((-yi + xi * m) ** 2), x0=0.1, method=\"L-BFGS-B\").x\n    offset[np.isnan(offset)], gamma[np.isnan(gamma)] = 0, 0\n    return offset, gamma", "response": "This function minimizes the NxN problem with a closed - form solution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef leastsq_generalized(x, y, x2, y2, res_std=None, res2_std=None, fit_offset=False, fit_offset2=False, perc=None):\n    if perc is not None:\n        if not fit_offset and isinstance(perc, (list, tuple)): perc = perc[1]\n        weights = csr_matrix(get_weight(x, y, perc)).astype(bool)\n        x, y = weights.multiply(x).tocsr(), weights.multiply(y).tocsr()\n\n    n_obs, n_var = x.shape\n    offset, offset_ss = np.zeros(n_var, dtype=\"float32\"), np.zeros(n_var, dtype=\"float32\")\n    gamma = np.ones(n_var, dtype=\"float32\")\n\n    if (res_std is None) or (res2_std is None): res_std, res2_std = np.ones(n_var), np.ones(n_var)\n    ones, zeros = np.ones(n_obs), np.zeros(n_obs)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        x, y = np.vstack((make_dense(x)/res_std, x2/res2_std)), np.vstack((make_dense(y)/res_std, y2/res2_std))\n\n    if fit_offset and fit_offset2:\n        for i in range(n_var):\n            A = np.c_[np.vstack((np.c_[ones/res_std[i], zeros], np.c_[zeros, ones/res2_std[i]])), x[:, i]]\n            offset[i], offset_ss[i], gamma[i] = np.linalg.pinv(A.T.dot(A)).dot(A.T.dot(y[:, i]))\n    elif fit_offset:\n        for i in range(n_var):\n            A = np.c_[np.hstack((ones/res_std[i], zeros)), x[:, i]]\n            offset[i], gamma[i] = np.linalg.pinv(A.T.dot(A)).dot(A.T.dot(y[:, i]))\n    elif fit_offset2:\n        for i in range(n_var):\n            A = np.c_[np.hstack((zeros, ones/res2_std[i])), x[:, i]]\n            offset_ss[i], gamma[i] = np.linalg.pinv(A.T.dot(A)).dot(A.T.dot(y[:, i]))\n    else:\n        for i in range(n_var):\n            A = np.c_[x[:, i]]\n            gamma[i] = np.linalg.pinv(A.T.dot(A)).dot(A.T.dot(y[:, i]))\n\n    offset[np.isnan(offset)] = 0\n    offset_ss[np.isnan(offset_ss)] = 0\n    gamma[np.isnan(gamma)] = 0\n\n    return offset, offset_ss, gamma", "response": "Solution to the 2 - dim generalized least squares problem."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmaximizes the log likelihood using weights according to empirical bayes", "response": "def maximum_likelihood(Ms, Mu, Mus, Mss, fit_offset=False, fit_offset2=False):\n    \"\"\"Maximizing the log likelihood using weights according to empirical bayes\n    \"\"\"\n    n_obs, n_var = Ms.shape\n    offset, offset_ss = np.zeros(n_var, dtype=\"float32\"), np.zeros(n_var, dtype=\"float32\")\n    gamma = np.ones(n_var, dtype=\"float32\")\n\n    def sse(A, data, b):\n        sigma = (A.dot(data) - b).std(1)\n        return np.log(sigma).sum()  # np.log(np.sqrt(2*np.pi) * sigma).sum() + (.5 * (res/sigma[:, None])**2).sum()\n\n    if fit_offset and fit_offset2:\n        for i in range(n_var):\n            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))\n            offset[i], offset_ss[i], gamma[i] = \\\n                minimize(lambda m: sse(np.array([[1, -m[2], 0, 0], [1, m[2], 2, -2 * m[2]]]),\n                                       data, b=np.array(m[0], m[1])), x0=(1e-4, 1e-4, 1), method=\"L-BFGS-B\").x\n    elif fit_offset:\n        for i in range(n_var):\n            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))\n            offset[i], gamma[i] = \\\n                minimize(lambda m: sse(np.array([[1, -m[1], 0, 0], [1, m[1], 2, -2 * m[1]]]),\n                                       data, b=np.array(m[0], 0)), x0=(1e-4, 1), method=\"L-BFGS-B\").x\n    elif fit_offset2:\n        for i in range(n_var):\n            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))\n            offset_ss[i], gamma[i] = \\\n                minimize(lambda m: sse(np.array([[1, -m[1], 0, 0], [1, m[1], 2, -2 * m[1]]]),\n                                       data, b=np.array(0, m[0])), x0=(1e-4, 1), method=\"L-BFGS-B\").x\n    else:\n        for i in range(n_var):\n            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))\n            gamma[i] = \\\n                minimize(lambda m: sse(np.array([[1, -m, 0, 0], [1, m, 2, -2 * m]]), data, b=0),\n                         x0=gamma[i], method=\"L-BFGS-B\").x\n    return offset, offset_ss, gamma"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the confidences of velocities.", "response": "def velocity_confidence(data, vkey='velocity', copy=False):\n    \"\"\"Computes confidences of velocities.\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` (default: `'velocity'`)\n        Name of velocity estimates to be used.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    velocity_length: `.obs`\n        Length of the velocity vectors for each individual cell\n    velocity_confidence: `.obs`\n        Confidence for each cell\n    \"\"\"\n    adata = data.copy() if copy else data\n    if vkey not in adata.layers.keys():\n        raise ValueError(\n            'You need to run `tl.velocity` first.')\n\n    idx = np.array(adata.var[vkey + '_genes'].values, dtype=bool)\n    X, V = adata.layers['Ms'][:, idx].copy(), adata.layers[vkey][:, idx].copy()\n    indices = get_indices(dist=adata.uns['neighbors']['distances'])[0]\n\n    V -= V.mean(1)[:, None]\n    V_norm = norm(V)\n    R = np.zeros(adata.n_obs)\n\n    for i in range(adata.n_obs):\n        Vi_neighs = V[indices[i]]\n        Vi_neighs -= Vi_neighs.mean(1)[:, None]\n        R[i] = np.mean(np.einsum('ij, j', Vi_neighs, V[i]) / (norm(Vi_neighs) * V_norm[i])[None, :])\n\n    adata.obs[vkey + '_length'] = V_norm.round(2)\n    adata.obs[vkey + '_confidence'] = R\n\n    logg.hint('added \\'' + vkey + '_confidence\\' (adata.obs)')\n\n    if vkey + '_confidence_transition' not in adata.obs.keys():\n        velocity_confidence_transition(adata, vkey)\n\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the confidence of transition for each cell in the hierarchy.", "response": "def velocity_confidence_transition(data, vkey='velocity', scale=10, copy=False):\n    \"\"\"Computes confidences of velocity transitions.\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` (default: `'velocity'`)\n        Name of velocity estimates to be used.\n    scale: `float` (default: 10)\n        Scale parameter of gaussian kernel.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    velocity_confidence_transition: `.obs`\n        Confidence of transition for each cell\n    \"\"\"\n    adata = data.copy() if copy else data\n    if vkey not in adata.layers.keys():\n        raise ValueError(\n            'You need to run `tl.velocity` first.')\n\n    idx = np.array(adata.var[vkey + '_genes'].values, dtype=bool)\n    T = transition_matrix(adata, vkey=vkey, scale=scale)\n    dX = T.dot(adata.layers['Ms'][:, idx]) - adata.layers['Ms'][:, idx]\n    dX -= dX.mean(1)[:, None]\n\n    V = adata.layers[vkey][:, idx].copy()\n    V -= V.mean(1)[:, None]\n\n    adata.obs[vkey + '_confidence_transition'] = prod_sum_var(dX, V) / (norm(dX) * norm(V))\n\n    logg.hint('added \\'' + vkey + '_confidence_transition\\' (adata.obs)')\n\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute a neighborhood graph of observations.", "response": "def neighbors(adata, n_neighbors=30, n_pcs=30, use_rep=None, knn=True, random_state=0, method='umap',\n              metric='euclidean', metric_kwds={}, copy=False):\n    \"\"\"\n    Compute a neighborhood graph of observations [McInnes18]_.\n    The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,\n    which also provides a method for estimating connectivities of data points -\n    the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,\n    connectivities are computed according to [Coifman05]_, in the adaption of\n    [Haghverdi16]_.\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_neighbors\n        The size of local neighborhood (in terms of number of neighboring data\n        points) used for manifold approximation. Larger values result in more\n        global views of the manifold, while smaller values result in more local\n        data being preserved. In general values should be in the range 2 to 100.\n        If `knn` is `True`, number of nearest neighbors to be searched. If `knn`\n        is `False`, a Gaussian kernel width is set to the distance of the\n        `n_neighbors` neighbor.\n    n_pcs : `int` or `None` (default: None)\n        Use this many PCs. If n_pcs==0 use .X if use_rep is None.\n\n    use_rep : `None`, `'X'` or any key for `.obsm` (default: None)\n        Use the indicated representation. If `None`, the representation is chosen automatically:\n        for .n_vars < 50, .X is used, otherwise \u2018X_pca\u2019 is used.\n    knn\n        If `True`, use a hard threshold to restrict the number of neighbors to\n        `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian\n        Kernel to assign low weights to neighbors more distant than the\n        `n_neighbors` nearest neighbor.\n    random_state\n        A numpy random seed.\n    method : {{'umap', 'gauss', `sklearn`, `None`}}  (default: `'umap'`)\n        Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_\n        with adaptive width [Haghverdi16]_) for computing connectivities.\n    metric\n        A known metric\u2019s name or a callable that returns a distance.\n    metric_kwds\n        Options for the metric.\n    copy\n        Return a copy instead of writing to adata.\n    Returns\n    -------\n    Depending on `copy`, updates or returns `adata` with the following:\n    connectivities : sparse matrix (`.uns['neighbors']`, dtype `float32`)\n        Weighted adjacency matrix of the neighborhood graph of data\n        points. Weights should be interpreted as connectivities.\n    distances : sparse matrix (`.uns['neighbors']`, dtype `float32`)\n        Instead of decaying weights, this stores distances for each pair of\n        neighbors.\n    \"\"\"\n    logg.info('computing neighbors', r=True)\n    adata = adata.copy() if copy else adata\n    if adata.isview: adata._init_as_actual(adata.copy())\n\n    if (use_rep is None or use_rep is 'X_pca') \\\n            and ('X_pca' not in adata.obsm.keys() or n_pcs > adata.obsm['X_pca'].shape[1]):\n        pca(adata, n_comps=n_pcs, svd_solver='arpack')\n\n    adata.uns['neighbors'] = {}\n    adata.uns['neighbors']['params'] = {'n_neighbors': n_neighbors, 'method': method}\n\n    if method is 'sklearn':\n        from sklearn.neighbors import NearestNeighbors\n        neighbors = NearestNeighbors(n_neighbors=n_neighbors)\n        neighbors.fit(adata.obsm['X_pca'] if use_rep is None else adata.obsm[use_rep])\n        adata.uns['neighbors']['distances'] = neighbors.kneighbors_graph(mode='distance')\n        adata.uns['neighbors']['connectivities'] = neighbors.kneighbors_graph(mode='connectivity')\n\n    else:\n        neighbors = Neighbors(adata)\n        neighbors.compute_neighbors(n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep, method=method,\n                                    metric=metric, metric_kwds=metric_kwds, random_state=random_state, write_knn_indices=True)\n        adata.uns['neighbors']['distances'] = neighbors.distances\n        adata.uns['neighbors']['connectivities'] = neighbors.connectivities\n        adata.uns['neighbors']['indices'] = neighbors.knn_indices\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added to `.uns[\\'neighbors\\']`\\n'\n        '    \\'distances\\', weighted adjacency matrix\\n'\n        '    \\'connectivities\\', weighted adjacency matrix')\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef velocity_embedding_grid(adata, basis=None, vkey='velocity', density=None, smooth=None, min_mass=None, arrow_size=None,\n                            arrow_length=None, arrow_color=None, scale=None, autoscale=True, n_neighbors=None,\n                            X=None, V=None, X_grid=None, V_grid=None, principal_curve=False, color=None, use_raw=None,\n                            layer=None, color_map=None, colorbar=True, palette=None, size=None, alpha=.2, perc=None,\n                            sort_order=True, groups=None, components=None, projection='2d', legend_loc='none',\n                            legend_fontsize=None, legend_fontweight=None, right_margin=None, left_margin=None,\n                            xlabel=None, ylabel=None, title=None, fontsize=None, figsize=None, dpi=None, frameon=None,\n                            show=True, save=None, ax=None, ncols=None, **kwargs):\n    \"\"\"\\\n    Scatter plot of velocities on a grid.\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    x: `str`, `np.ndarray` or `None` (default: `None`)\n        x coordinate\n    y: `str`, `np.ndarray` or `None` (default: `None`)\n        y coordinate\n    vkey: `str` or `None` (default: `None`)\n        Key for annotations of observations/cells or variables/genes.\n    density: `float` (default: 1)\n        Amount of velocities to show - 0 none to 1 all\n    arrow_size: `float` or 3-tuple for headlength, headwidth and headaxislength (default: 1)\n        Size of arrows.\n    arrow_length: `float` (default: 1)\n        Length of arrows.\n    scale: `float` (default: 1)\n        Length of velocities in the embedding.\n    min_mass: `float` (default: 0.5)\n        Minimum threshold for mass to be shown.\n    smooth: `float` (default: 0.5)\n        Multiplication factor for scale in Gaussian kernel around grid point.\n    n_neighbors: `int` (default: None)\n        Number of neighbors to consider around grid point.\n    X: `np.ndarray` (default: None)\n        embedding grid point coordinates\n    V: `np.ndarray` (default: None)\n        embedding grid velocity coordinates\n    {scatter}\n\n    Returns\n    -------\n        `matplotlib.Axis` if `show==False`\n    \"\"\"\n    basis = default_basis(adata) if basis is None else basis\n    colors, layers, vkeys = make_unique_list(color, allow_array=True), make_unique_list(layer), make_unique_list(vkey)\n    for key in vkeys:\n        if key + '_' + basis not in adata.obsm_keys() and V is None:\n            velocity_embedding(adata, basis=basis, vkey=key)\n    color, layer, vkey = colors[0], layers[0], vkeys[0]\n    color = default_color(adata) if color is None else color\n\n    if X_grid is None or V_grid is None:\n        _adata = adata[groups_to_bool(adata, groups, groupby=color)] \\\n            if groups is not None and color in adata.obs.keys() else adata\n        X_emb  = _adata.obsm['X_' + basis][:, get_components(components, basis)] if X is None else X[:, :2]\n        V_emb = _adata.obsm[vkey + '_' + basis][:, get_components(components, basis)] if V is None else V[:, :2]\n        X_grid, V_grid = compute_velocity_on_grid(X_emb=X_emb, V_emb=V_emb, density=density, autoscale=autoscale,\n                                                  smooth=smooth, n_neighbors=n_neighbors, min_mass=min_mass)\n\n    scatter_kwargs = {\"basis\": basis, \"perc\": perc, \"use_raw\": use_raw, \"sort_order\": sort_order, \"alpha\": alpha,\n                      \"components\": components, \"projection\": projection, \"legend_loc\": legend_loc, \"groups\": groups,\n                      \"legend_fontsize\": legend_fontsize, \"legend_fontweight\": legend_fontweight, \"palette\": palette,\n                      \"color_map\": color_map, \"frameon\": frameon, \"xlabel\": xlabel, \"ylabel\": ylabel,\n                      \"right_margin\": right_margin, \"left_margin\": left_margin, \"colorbar\": colorbar, \"dpi\": dpi,\n                      \"fontsize\": fontsize, \"show\": False, \"save\": None}\n\n    multikey = colors if len(colors) > 1 else layers if len(layers) > 1 else vkeys if len(vkeys) > 1 else None\n    if multikey is not None:\n        if isinstance(title, (list, tuple)): title *= int(np.ceil(len(multikey) / len(title)))\n        ncols = len(multikey) if ncols is None else min(len(multikey), ncols)\n        nrows = int(np.ceil(len(multikey) / ncols))\n        figsize = rcParams['figure.figsize'] if figsize is None else figsize\n        for i, gs in enumerate(\n                pl.GridSpec(nrows, ncols, pl.figure(None, (figsize[0] * ncols, figsize[1] * nrows), dpi=dpi))):\n            if i < len(multikey):\n                velocity_embedding_grid(adata, density=density, scale=scale, size=size, min_mass=min_mass, smooth=smooth,\n                                        n_neighbors=n_neighbors, principal_curve=principal_curve, ax=pl.subplot(gs),\n                                        arrow_size=arrow_size, arrow_length=arrow_length,\n                                        color=colors[i] if len(colors) > 1 else color,\n                                        layer=layers[i] if len(layers) > 1 else layer,\n                                        vkey=vkeys[i] if len(vkeys) > 1 else vkey,\n                                        title=title[i] if isinstance(title, (list, tuple)) else title,\n                                        X_grid=None if len(vkeys) > 1 else X_grid,\n                                        V_grid=None if len(vkeys) > 1 else V_grid,\n                                        autoscale=False if len(vkeys) > 1 else autoscale, **scatter_kwargs, **kwargs)\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax\n\n    else:\n        ax = pl.figure(None, figsize, dpi=dpi).gca() if ax is None else ax\n\n        hl, hw, hal = default_arrow(arrow_size)\n        scale = 1 / arrow_length if arrow_length is not None else scale if scale is not None else 1\n        quiver_kwargs = {\"scale\": scale, \"angles\": 'xy', \"scale_units\": 'xy', \"width\": .001,\n                         \"color\": 'grey' if arrow_color is None else arrow_color, \"edgecolors\": 'k',\n                         \"headlength\": hl/2, \"headwidth\": hw/2, \"headaxislength\": hal/2, \"linewidth\": .2}\n        quiver_kwargs.update(kwargs)\n        pl.quiver(X_grid[:, 0], X_grid[:, 1], V_grid[:, 0], V_grid[:, 1], **quiver_kwargs, zorder=3)\n\n        if principal_curve:\n            curve = adata.uns['principal_curve']['projections']\n            pl.plot(curve[:, 0], curve[:, 1], c=\"w\", lw=6, zorder=4)\n            pl.plot(curve[:, 0], curve[:, 1], c=\"k\", lw=3, zorder=5)\n\n        size = 4 * default_size(adata) if size is None else size\n        ax = scatter(adata, layer=layer, color=color, size=size, title=title, ax=ax, zorder=0, **scatter_kwargs)\n\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax", "response": "Generate a velocity embedding grid for a single cell."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scatter(adata=None, x=None, y=None, basis=None, vkey=None, color=None, use_raw=None, layer=None, color_map=None,\n            colorbar=True, palette=None, size=None, alpha=None, linewidth=None, perc=None, sort_order=True, groups=None,\n            components=None, projection='2d', legend_loc='none', legend_fontsize=None, legend_fontweight=None,\n            right_margin=None, left_margin=None, xlabel=None, ylabel=None, title=None, fontsize=None, figsize=None,\n            dpi=None, frameon=None, show=True, save=None, ax=None, zorder=None, ncols=None, **kwargs):\n    \"\"\"\\\n    Scatter plot along observations or variables axes.\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    x: `str`, `np.ndarray` or `None` (default: `None`)\n        x coordinate\n    y: `str`, `np.ndarray` or `None` (default: `None`)\n        y coordinate\n    {scatter}\n\n    Returns\n    -------\n        If `show==False` a `matplotlib.Axis`\n    \"\"\"\n    scatter_kwargs = {\"use_raw\": use_raw, \"sort_order\": sort_order, \"alpha\": alpha, \"components\": components,\n                      \"projection\": projection, \"legend_loc\": legend_loc, \"groups\": groups, \"palette\": palette,\n                      \"legend_fontsize\": legend_fontsize, \"legend_fontweight\": legend_fontweight,\n                      \"right_margin\": right_margin, \"left_margin\": left_margin, \"show\": False, \"save\": None}\n\n    adata = AnnData(np.stack([x, y]).T) if adata is None and (x is not None and y is not None) else adata\n    colors, layers, bases = make_unique_list(color, allow_array=True), make_unique_list(layer), make_unique_list(basis)\n    multikey = colors if len(colors) > 1 else layers if len(layers) > 1 else bases if len(bases) > 1 else None\n    if multikey is not None:\n        if isinstance(title, (list, tuple)): title *= int(np.ceil(len(multikey) / len(title)))\n        ncols = len(multikey) if ncols is None else min(len(multikey), ncols)\n        nrows = int(np.ceil(len(multikey) / ncols))\n        figsize = rcParams['figure.figsize'] if figsize is None else figsize\n        for i, gs in enumerate(\n                pl.GridSpec(nrows, ncols, pl.figure(None, (figsize[0] * ncols, figsize[1] * nrows), dpi=dpi))):\n            if i < len(multikey):\n                scatter(adata, x=x, y=y, size=size, linewidth=linewidth, xlabel=xlabel, ylabel=ylabel, vkey=vkey,\n                        color_map=color_map, colorbar=colorbar, perc=perc, frameon=frameon, ax=pl.subplot(gs), zorder=zorder,\n                        color=colors[i] if len(colors) > 1 else color,\n                        layer=layers[i] if len(layers) > 1 else layer,\n                        basis=bases[i] if len(bases) > 1 else basis,\n                        title=title[i] if isinstance(title, (list, tuple)) else title, **scatter_kwargs, **kwargs)\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax\n\n    else:\n        color, layer, basis = colors[0], layers[0], bases[0]\n        color = default_color(adata) if color is None else color\n        color_map = default_color_map(adata, color) if color_map is None else color_map\n\n        is_embedding = ((x is None) | (y is None)) and basis not in adata.var_names\n        basis = default_basis(adata) if basis is None and is_embedding else basis\n        size = default_size(adata) if size is None else size\n        linewidth = 1 if linewidth is None else linewidth\n        frameon = frameon if frameon is not None else True if not is_embedding else settings._frameon\n\n        if projection == '3d':\n            from mpl_toolkits.mplot3d import Axes3D\n            ax = pl.figure(None, figsize, dpi=dpi).gca(projection=projection) if ax is None else ax\n        else:\n            ax = pl.figure(None, figsize, dpi=dpi).gca() if ax is None else ax\n\n        if is_categorical(adata, color) and is_embedding:\n            from scanpy.api.pl import scatter as scatter_\n            ax = scatter_(adata, basis=basis, color=color, color_map=color_map, size=size, frameon=frameon, ax=ax,\n                          title=title, **scatter_kwargs, **kwargs)\n\n        else:\n            if basis in adata.var_names:\n                xkey, ykey = ('spliced', 'unspliced') if use_raw or 'Ms' not in adata.layers.keys() else ('Ms', 'Mu')\n                x = make_dense(adata[:, basis].layers[xkey])\n                y = make_dense(adata[:, basis].layers[ykey])\n                xlabel, ylabel = 'spliced', 'unspliced'\n                title = basis if title is None else title\n\n            elif is_embedding:\n                X_emb = adata.obsm['X_' + basis][:, get_components(components, basis)]\n                x, y = X_emb[:, 0], X_emb[:, 1]\n\n            elif isinstance(x, str) and isinstance(y, str) and x in adata.var_names and y in adata.var_names:\n                x = adata[:, x].layers[layer] if layer in adata.layers.keys() else adata[:, x].X\n                y = adata[:, y].layers[layer] if layer in adata.layers.keys() else adata[:, y].X\n\n            if basis in adata.var_names and isinstance(color, str) and color in adata.layers.keys():\n                c = interpret_colorkey(adata, basis, color, perc)\n            else:\n                c = interpret_colorkey(adata, color, layer, perc)\n\n            if layer is not None and 'velocity' in layer and isinstance(color, str) and color in adata.var_names:\n                ub = np.percentile(np.abs(c), 98)\n                kwargs.update({\"vmin\": -ub, \"vmax\": ub})\n            if layer is not None and ('spliced' in layer or 'Ms' in layer or 'Mu' in layer) \\\n                    and isinstance(color, str) and color in adata.var_names:\n                ub = np.percentile(c, 98)\n                kwargs.update({\"vmax\": ub})\n\n            if groups is not None or np.any(pd.isnull(c)):\n                zorder = 0 if zorder is None else zorder\n                ax = scatter(adata, basis=basis, color='lightgrey', ax=ax, zorder=zorder, **scatter_kwargs)\n                zorder += 1\n\n            if basis in adata.var_names:\n                fits = show_linear_fit(adata, basis, vkey, xkey, linewidth)\n                from .simulation import show_full_dynamics\n                if 'true_alpha' in adata.var.keys():\n                    fit = show_full_dynamics(adata, basis, 'true', use_raw, linewidth)\n                    fits.append(fit)\n                if 'fit_alpha' in adata.var.keys() and (vkey is None or 'dynamics' in vkey):\n                    fit = show_full_dynamics(adata, basis, 'fit', use_raw, linewidth,\n                                             show_assigments=vkey is not None and 'assignment' in vkey)\n                    fits.append(fit)\n                if vkey is not None and 'density' in vkey:\n                    show_density(x, y)\n\n                if len(fits) > 0 and legend_loc is not None:\n                    pl.legend(fits, loc=legend_loc if legend_loc is not 'none' else 'lower right')\n                if use_raw and perc is not None:\n                    pl.xlim(right=np.percentile(x, 99.9 if not isinstance(perc, int) else perc) * 1.05)\n                    pl.ylim(top=np.percentile(y, 99.9 if not isinstance(perc, int) else perc) * 1.05)\n\n            pl.scatter(x, y, c=c, cmap=color_map, s=size, alpha=alpha, edgecolors='none', marker='.', zorder=zorder, **kwargs)\n\n            set_label(xlabel, ylabel, fontsize, basis)\n            set_title(title, layer, color, fontsize)\n            ax = update_axes(ax, fontsize, is_embedding, frameon)\n            if colorbar and not is_categorical(adata, color): set_colorbar(ax)\n\n        savefig_or_show('' if basis is None else basis, dpi=dpi, save=save, show=show)\n        if not show: return ax", "response": "Scatter plot along observations or variables axes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the cell fates for each cluster in the cluster.", "response": "def cell_fate(data, groupby='clusters', disconnected_groups=None, self_transitions=False, n_neighbors=None, copy=False):\n    \"\"\"Computes individual cell endpoints\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby: `str` (default: `'clusters'`)\n        Key to which to assign the fates.\n    disconnected_groups: list of `str` (default: `None`)\n        Which groups to treat as disconnected for fate assignment.\n    n_neighbors: `int` (default: `None`)\n        Number of neighbors to restrict transitions to.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to `adata`.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    cell_fate: `.obs`\n        most likely cell fate for each individual cell\n    cell_fate_confidence: `.obs`\n        confidence of transitioning to the assigned fate\n    \"\"\"\n    adata = data.copy() if copy else data\n    logg.info('computing cell fates', r=True)\n\n    n_neighbors = 10 if n_neighbors is None else n_neighbors\n    _adata = adata.copy()\n    vgraph = VelocityGraph(_adata, n_neighbors=n_neighbors, approx=True, n_recurse_neighbors=1)\n    vgraph.compute_cosines()\n    _adata.uns['velocity_graph'] = vgraph.graph\n    _adata.uns['velocity_graph_neg'] = vgraph.graph_neg\n\n    T = transition_matrix(_adata, self_transitions=self_transitions)\n    I = np.eye(_adata.n_obs)\n    fate = np.linalg.inv(I - T)\n    if issparse(T): fate = fate.A\n    cell_fates = np.array(_adata.obs[groupby][fate.argmax(1)])\n    if disconnected_groups is not None:\n        idx = _adata.obs[groupby].isin(disconnected_groups)\n        cell_fates[idx] = _adata.obs[groupby][idx]\n\n    adata.obs['cell_fate'] = cell_fates\n    adata.obs['cell_fate_confidence'] = fate.max(1) / fate.sum(1)\n    strings_to_categoricals(adata)\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added\\n'\n        '    \\'cell_fate\\', most likely cell fate (adata.obs)\\n'\n        '    \\'cell_fate_confidence\\', confidence of transitioning to the assigned fate (adata.obs)')\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the terminal states of a node.", "response": "def terminal_states(data, vkey='velocity', groupby=None, groups=None, self_transitions=False, basis=None,\n                    weight_diffusion=0, scale_diffusion=1, eps=1e-3, copy=False):\n    \"\"\"Computes terminal states (root and end points).\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` (default: `'velocity'`)\n        Name of velocity estimates to be used.\n    self_transitions: `bool` (default: `False`)\n        Allow transitions from one node to itself.\n    basis: `str` (default: `None`)\n        Basis to use.\n    weight_diffusion: `float` (default: 0)\n        Relative weight to be given to diffusion kernel (Brownian motion)\n    scale_diffusion: `float` (default: 1)\n        Scale of diffusion kernel.\n    eps: `float` (default: 1e-3)\n        Tolerance for eigenvalue selection.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to data.\n\n    Returns\n    -------\n    Returns or updates `data` with the attributes\n    root: `.obs`\n        sparse matrix with transition probabilities.\n    end: `.obs`\n        sparse matrix with transition probabilities.\n    \"\"\"\n    adata = data.copy() if copy else data\n    logg.info('computing terminal states', r=True)\n\n    strings_to_categoricals(adata)\n\n    groupby = 'cell_fate' if groupby is None and 'cell_fate' in adata.obs.keys() else groupby\n    categories = adata.obs[groupby].cat.categories if groupby is not None and groups is None else [None]\n    for cat in categories:\n        groups = cat if cat is not None else groups\n        cell_subset = groups_to_bool(adata, groups=groups, groupby=groupby)\n        _adata = adata if groups is None else adata[cell_subset]\n        connectivities = get_connectivities(_adata, 'distances')\n\n        T = transition_matrix(_adata, vkey=vkey, basis=basis, weight_diffusion=weight_diffusion,\n                              scale_diffusion=scale_diffusion, self_transitions=self_transitions, backward=True)\n        eigvecs_roots = eigs(T, eps=eps, perc=[2, 98])[1]\n        roots = csr_matrix.dot(connectivities, eigvecs_roots).sum(1)\n        roots = scale(np.clip(roots, 0, np.percentile(roots, 98)))\n        write_to_obs(adata, 'root_cells', roots, cell_subset)\n\n        T = transition_matrix(_adata, vkey=vkey, basis=basis, weight_diffusion=weight_diffusion,\n                              scale_diffusion=scale_diffusion, self_transitions=self_transitions, backward=False)\n        eigvecs_ends = eigs(T, eps=eps, perc=[2, 98])[1]\n        ends = csr_matrix.dot(connectivities, eigvecs_ends).sum(1)\n        ends = scale(np.clip(ends, 0, np.percentile(ends, 98)))\n        write_to_obs(adata, 'end_points', ends, cell_subset)\n\n        n_roots, n_ends = eigvecs_roots.shape[1], eigvecs_ends.shape[1]\n        groups_str = ' (' + groups + ')' if isinstance(groups, str) else ''\n        logg.info('    identified ' + str(n_roots) + ' root cells and ' + str(n_ends) + ' end points' + groups_str)\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added\\n'\n        '    \\'root_cells\\', root cells of Markov diffusion process (adata.obs)\\n'\n        '    \\'end_points\\', end points of Markov diffusion process (adata.obs)')\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the moments for velocity estimation.", "response": "def moments(data, n_neighbors=30, n_pcs=30, mode='connectivities', method='umap', metric='euclidean', use_rep=None,\n            recurse_neighbors=False, renormalize=False, copy=False):\n    \"\"\"Computes moments for velocity estimation.\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    n_neighbors: `int` (default: 30)\n        Number of neighbors to use.\n    n_pcs: `int` (default: 30)\n        Number of principal components to use.\n    mode: `'connectivities'` or `'distances'`  (default: `'connectivities'`)\n        Distance metric to use for moment computation.\n    renormalize: `bool` (default: `False`)\n        Renormalize the moments by total counts per cell to its median.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    Ms: `.layers`\n        dense matrix with first order moments of spliced counts.\n    Mu: `.layers`\n        dense matrix with first order moments of unspliced counts.\n    \"\"\"\n    adata = data.copy() if copy else data\n\n    if 'spliced' not in adata.layers.keys() or 'unspliced' not in adata.layers.keys():\n        raise ValueError('Could not find spliced / unspliced counts.')\n    if any([not_yet_normalized(adata.layers[layer]) for layer in {'spliced', 'unspliced'}]):\n        normalize_per_cell(adata)\n    if 'neighbors' not in adata.uns.keys() or neighbors_to_be_recomputed(adata, n_neighbors=n_neighbors):\n        if use_rep is None: use_rep = 'X_pca'\n        neighbors(adata, n_neighbors=n_neighbors, use_rep=use_rep, n_pcs=n_pcs, method=method, metric=metric)\n    if mode not in adata.uns['neighbors']:\n        raise ValueError('mode can only be \\'connectivities\\' or \\'distances\\'')\n\n    logg.info('computing moments based on ' + str(mode), r=True)\n\n    connectivities = get_connectivities(adata, mode, n_neighbors=n_neighbors, recurse_neighbors=recurse_neighbors)\n\n    adata.layers['Ms'] = csr_matrix.dot(connectivities, csr_matrix(adata.layers['spliced'])).astype(np.float32).A\n    adata.layers['Mu'] = csr_matrix.dot(connectivities, csr_matrix(adata.layers['unspliced'])).astype(np.float32).A\n    if renormalize: normalize_per_cell(adata, layers={'Ms', 'Mu'}, enforce=True)\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added \\n'\n        '    \\'Ms\\' and \\'Mu\\', moments of spliced/unspliced abundances (adata.layers)')\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the second order moments for spliced abundances.", "response": "def second_order_moments(adata, adjusted=False):\n    \"\"\"Computes second order moments for stochastic velocity estimation.\n\n    Arguments\n    ---------\n    adata: `AnnData`\n        Annotated data matrix.\n\n    Returns\n    -------\n    Mss: Second order moments for spliced abundances\n    Mus: Second order moments for spliced with unspliced abundances\n    \"\"\"\n    if 'neighbors' not in adata.uns:\n        raise ValueError('You need to run `pp.neighbors` first to compute a neighborhood graph.')\n\n    connectivities = get_connectivities(adata)\n    s, u = csr_matrix(adata.layers['spliced']), csr_matrix(adata.layers['unspliced'])\n    Mss = csr_matrix.dot(connectivities, s.multiply(s)).astype(np.float32).A\n    Mus = csr_matrix.dot(connectivities, s.multiply(u)).astype(np.float32).A\n    if adjusted:\n        Mss = 2 * Mss - adata.layers['Ms'].reshape(Mss.shape)\n        Mus = 2 * Mus - adata.layers['Mu'].reshape(Mus.shape)\n    return Mss, Mus"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing second order moments for unspliced abundances.", "response": "def second_order_moments_u(adata):\n    \"\"\"Computes second order moments for stochastic velocity estimation.\n\n    Arguments\n    ---------\n    adata: `AnnData`\n        Annotated data matrix.\n\n    Returns\n    -------\n    Muu: Second order moments for unspliced abundances\n    \"\"\"\n    if 'neighbors' not in adata.uns:\n        raise ValueError('You need to run `pp.neighbors` first to compute a neighborhood graph.')\n\n    connectivities = get_connectivities(adata)\n    u = csr_matrix(adata.layers['unspliced'])\n    Muu = csr_matrix.dot(connectivities, u.multiply(u)).astype(np.float32).A\n\n    return Muu"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute transition matrix from velocity graph.", "response": "def transition_matrix(adata, vkey='velocity', basis=None, backward=False, self_transitions=True, scale=10, perc=None,\n                      use_negative_cosines=False, weight_diffusion=0, scale_diffusion=1, weight_indirect_neighbors=None,\n                      n_neighbors=None, vgraph=None):\n    \"\"\"Computes transition probabilities from velocity graph\n\n    Arguments\n    ---------\n    adata: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    vkey: `str` (default: `'velocity'`)\n        Name of velocity estimates to be used.\n    basis: `str` or `None` (default: `None`)\n        Restrict transition to embedding if specified\n    backward: `bool` (default: `False`)\n        Whether to use the transition matrix to push forward (`False`) or to pull backward (`True`)\n    scale: `float` (default: 10)\n        Scale parameter of gaussian kernel.\n    weight_diffusion: `float` (default: 0)\n        Relative weight to be given to diffusion kernel (Brownian motion)\n    scale_diffusion: `float` (default: 1)\n        Scale of diffusion kernel. \n\n    Returns\n    -------\n    Returns sparse matrix with transition probabilities.\n    \"\"\"\n    if vkey+'_graph' not in adata.uns:\n        raise ValueError('You need to run `tl.velocity_graph` first to compute cosine correlations.')\n\n    graph = csr_matrix(adata.uns[vkey + '_graph']).copy() if vgraph is None else vgraph.copy()\n\n    if self_transitions:\n        confidence = graph.max(1).A.flatten()\n        ub = np.percentile(confidence, 98)\n        self_prob = np.clip(ub - confidence, 0, 1)\n        graph.setdiag(self_prob)\n\n    T = np.expm1(graph * scale)  # equivalent to np.exp(graph.A * scale) - 1\n    if vkey + '_graph_neg' in adata.uns.keys():\n        graph_neg = adata.uns[vkey + '_graph_neg']\n        if use_negative_cosines:\n            T -= np.expm1(-graph_neg * scale)\n        else:\n            T += np.expm1(graph_neg * scale)\n            T.data += 1\n\n    # weight direct and indirect (recursed) neighbors\n    if 'neighbors' in adata.uns.keys() and weight_indirect_neighbors is not None and weight_indirect_neighbors < 1:\n        direct_neighbors = adata.uns['neighbors']['distances'] > 0\n        direct_neighbors.setdiag(1)\n        w = weight_indirect_neighbors\n        T = w * T + (1-w) * direct_neighbors.multiply(T)\n\n    if backward: T = T.T\n    T = normalize(T)\n\n    if n_neighbors is not None:\n        T = T.multiply(get_connectivities(adata, mode='distances', n_neighbors=n_neighbors, recurse_neighbors=True))\n\n    if perc is not None:\n        threshold = np.percentile(T.data, perc)\n        T.data[T.data < threshold] = 0\n        T.eliminate_zeros()\n\n    if 'X_' + str(basis) in adata.obsm.keys():\n        dists_emb = (T > 0).multiply(squareform(pdist(adata.obsm['X_' + basis])))\n        scale_diffusion *= dists_emb.data.mean()\n        \n        diffusion_kernel = dists_emb.copy()\n        diffusion_kernel.data = np.exp(-.5 * dists_emb.data ** 2 / scale_diffusion ** 2)\n        T = T.multiply(diffusion_kernel)  # combine velocity based kernel with diffusion based kernel\n\n        if 0 < weight_diffusion < 1:  # add another diffusion kernel (Brownian motion - like)\n            diffusion_kernel.data = np.exp(-.5 * dists_emb.data ** 2 / (scale_diffusion/2) ** 2)\n            T = (1-weight_diffusion) * T + weight_diffusion * diffusion_kernel\n\n        T = normalize(T)\n\n    return T"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding a resource in the current template.", "response": "def material_find_resource(filename, cdn, use_minified=None, local=True):\n    \"\"\"Resource finding function, also available in templates.\n\n    Tries to find a resource, will force SSL depending on\n    ``MATERIAL_CDN_FORCE_SSL`` settings.\n\n    :param filename: File to find a URL for.\n    :param cdn: Name of the CDN to use.\n    :param use_minified': If set to ``True``/``False``, use/don't use\n                          minified. If ``None``, honors\n                          ``MATERIAL_USE_MINIFIED``.\n    :param local: If ``True``, uses the ``local``-CDN when\n                  ``MATERIAL_SERVE_LOCAL`` is enabled. If ``False``, uses\n                  the ``static``-CDN instead.\n    :return: A URL.\n    \"\"\"\n    config = current_app.config\n\n    if config['MATERIAL_SERVE_LOCAL']:\n        if 'css/' not in filename and 'js/' not in filename:\n            filename = 'js/' + filename\n\n    if None == use_minified:\n        use_minified = config['MATERIAL_USE_MINIFIED']\n\n    if use_minified:\n        filename = '%s.min.%s' % tuple(filename.rsplit('.', 1))\n\n    cdns = current_app.extensions['material']['cdns']\n    resource_url = cdns[cdn].get_resource_url(filename)\n\n    if resource_url.startswith('//') and config['MATERIAL_CDN_FORCE_SSL']:\n        resource_url = 'https:%s' % resource_url\n\n    return resource_url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the rules of the context to its occurrences.", "response": "def apply(self):\n        \"\"\"Apply the rules of the context to its occurrences.\n\n        This method executes all the functions defined in\n        self.tasks in the order they are listed.\n\n        Every function that acts as a context task receives the\n        Context object itself as its only argument.\n\n        The contextualized occurrences are then stored in\n        Context.contextualized.\n\n        The original Occurrence instances are not modified.\n        \"\"\"\n        raw_operations = copy.deepcopy(self.occurrences)\n        for task in self.tasks:\n            task(self)\n        self.occurrences = raw_operations"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef average_price(quantity_1, price_1, quantity_2, price_2):\n    return (quantity_1 * price_1 + quantity_2 * price_2) / \\\n            (quantity_1 + quantity_2)", "response": "Calculates the average price between two asset states."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the state of the Holder according to the occurrence.", "response": "def update_holder(self, holder):\n        \"\"\"Udpate the Holder state according to the occurrence.\n\n        This implementation is a example of how a Occurrence object\n        can update the Holder state; this method should be overriden\n        by classes that inherit from the Occurrence class.\n\n        This sample implementation simply update the quantity and the average\n        price of the Subject in the Holder's possession every time objects\n        from this class are passed to Holder.trade().\n\n        This sample implementation considers the following signature for\n        the Holder.state dict:\n        \n        .. code:: python\n\n            {\n                \"SUBJECT SYMBOL\": {\n                    \"quantity\": 0,\n                    \"value\": 0\n                }\n            }\n\n        And the following signature for the Occurrance.details dict:\n        \n        .. code:: python\n\n            {\n                \"quantity\": 0,\n                \"value\": 0\n            }\n        \"\"\"\n\n        subject_symbol = self.subject.symbol\n\n        # If the Holder already have a state regarding this Subject,\n        # update that state\n        if subject_symbol in holder.state:\n\n            # If the Holder have zero units of this subject, the average\n            # value paid/received for the subject is the value of the trade itself\n            if not holder.state[subject_symbol]['quantity']:\n                holder.state[subject_symbol]['value'] = self.details['value']\n\n            # If the Holder owns units of this subject then the average value\n            # paid/received for the subject may need to be updated with\n            # this occurrence details\n\n            # If the occurrence have the same sign as the quantity in the Holder\n            # state, a new average value needs to be calculated for the subject\n            elif same_sign(\n                    holder.state[subject_symbol]['quantity'],\n                    self.details['quantity']):\n                holder.state[subject_symbol]['value'] = average_price(\n                    holder.state[subject_symbol]['quantity'],\n                    holder.state[subject_symbol]['value'],\n                    self.details['quantity'],\n                    self.details['value']\n                )\n\n            # If the occurrence does not have the same sign of the quantity in the\n            # Holder state, then do other stuff.\n            # A trade app would normally implement some sort of profit/loss logic\n            # here.\n            # This sample implementation only checks if the average value\n            # of the subject needs to be updated and then update it as needed.\n            else:\n                if same_sign(\n                        self.details['quantity'],\n                        holder.state[subject_symbol]['quantity'] + self.details['quantity']):\n                    holder.state[subject_symbol]['value'] = self.details['value']\n\n            # Update the quantity of the subject in the Holder's posession\n            holder.state[subject_symbol]['quantity'] += self.details['quantity']\n\n        # If the Holder don't have a state with this occurrence's Subject,\n        # then register this occurrence as the first state of the Subject\n        # in the Holder's possession\n        else:\n            holder.state[subject_symbol] = {\n                'quantity': self.details['quantity'],\n                'value': self.details['value']\n            }\n\n        # If the Holder knows about this Subject but don't have any unit\n        # of it, the paid value of the subject in the Holder state should\n        # be zero.\n        if not holder.state[subject_symbol]['quantity']:\n            holder.state[subject_symbol]['value'] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a sub - fitter. This function creates a sub - fitter.", "response": "def fitter(self, n=0, ftype=\"real\", colfac=1.0e-8, lmfac=1.0e-3):\n        \"\"\"Create a sub-fitter.\n\n        The created sub-fitter can be used in the same way as a fitter\n        default fitter. This function returns an identification, which has to\n        be used in the `fid` argument of subsequent calls. The call can\n        specify the standard constructor arguments (`n`, `type`, `colfac`,\n        `lmfac`), or can specify them later in a :meth:`set` statement.\n\n        :param n:      number of unknowns\n        :param ftype:  type of solution\n                       Allowed: real, complex, separable, asreal, conjugate\n        :param colfac: collinearity factor\n        :param lmfac:  Levenberg-Marquardt factor\n        :param fid:    the id of a sub-fitter\n\n        \"\"\"\n        fid = self._fitproxy.getid()\n        ftype = self._gettype(ftype)\n        n = len(self._fitids)\n        if 0 <= fid < n:\n            self._fitids[fid] = {}\n        elif fid == n:\n            self._fitids.append({})\n        else:\n            # shouldn't happen\n            raise RangeError(\"fit id out of range\")\n        self.init(n=n, ftype=ftype, colfac=colfac, lmfac=lmfac, fid=fid)\n        return fid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(self, n=0, ftype=\"real\", colfac=1.0e-8, lmfac=1.0e-3, fid=0):\n        ftype = self._gettype(ftype)\n        self._fitids[fid][\"stat\"] = False\n        self._fitids[fid][\"solved\"] = False\n        self._fitids[fid][\"haserr\"] = False\n        self._fitids[fid][\"fit\"] = False\n        self._fitids[fid][\"looped\"] = False\n        if self._fitproxy.init(fid, n, ftype, colfac, lmfac):\n            self._fitids[fid][\"stat\"] = self._getstate(fid)\n        else:\n            return False", "response": "Initialize the properties of the fitserver instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the selected properties of the fitserver instance.", "response": "def set(self, n=None, ftype=None, colfac=None, lmfac=None, fid=0):\n        \"\"\"Set selected properties of the fitserver instance.\n\n        All unset properties remain the same (in the :meth:`init` method all\n        properties are (re-)initialized). Like in the constructor, the number\n        of unknowns to be solved for; the number of simultaneous solutions;\n        the ftype (as code); and the collinearity and Levenberg-Marquardt\n        factor can be specified.\n\n        :param n: number of unknowns\n        :param ftype: type of solution\n                    Allowed: real, complex, separable, asreal, conjugate\n        :param colfac:\tcollinearity factor\n        :param lmfac: Levenberg-Marquardt factor\n        :param fid: the id of a sub-fitter\n        \"\"\"\n        self._checkid(fid)\n        if ftype is None:\n            ftype = -1\n        else:\n            ftype = self._gettype(ftype)\n        if n is None:\n            n = -1\n        elif n < 0:\n            raise ValueError(\"Illegal set argument n\")\n        if colfac is None:\n            colfac = -1\n        elif colfac < 0:\n            raise ValueError(\"Illegal set argument colfac\")\n        if lmfac is None:\n            lmfac = -1\n        elif lmfac < 0:\n            raise ValueError(\"Illegal set argument lmfac\")\n\n        self._fitids[fid][\"stat\"] = False\n        self._fitids[fid][\"solved\"] = False\n        self._fitids[fid][\"haserr\"] = False\n        self._fitids[fid][\"fit\"] = True\n        self._fitids[fid][\"looped\"] = False\n        if n != -1 or ftype != -1 or colfac != -1 or lmfac != -1:\n            if not self._fitproxy.set(fid, n, ftype, colfac, lmfac):\n                return False\n        self._fitids[fid][\"stat\"] = self._getstate(fid)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the object s resources to its initialized state.", "response": "def reset(self, fid=0):\n        \"\"\"Reset the object's resources to its initialized state.\n\n        :param fid: the id of a sub-fitter\n        \"\"\"\n        self._checkid(fid)\n        self._fitids[fid][\"solved\"] = False\n        self._fitids[fid][\"haserr\"] = False\n        if not self._fitids[fid][\"looped\"]:\n            return self._fitproxy.reset(fid)\n        else:\n            self._fitids[fid][\"looped\"] = False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a polynomial fit from the specified condition equations and solve the resulting polynomial.", "response": "def fitspoly(self, n, x, y, sd=None, wt=1.0, fid=0):\n        \"\"\"Create normal equations from the specified condition equations, and\n        solve the resulting normal equations. It is in essence a combination.\n\n        The method expects that the properties of the fitter to be used have\n        been initialized or set (like the number of simultaneous solutions m\n        the type; factors). The main reason is to limit the number of\n        parameters on the one hand, and on the other hand not to depend\n        on the actual array structure to get the variables and type. Before\n        fitting the x-range is normalized to values less than 1 to cater for\n        large difference in x raised to large powers. Later a shift to make x\n        around zero will be added as well.\n\n        :param n: the order of the polynomial to solve for\n        :param x: the abscissa values\n        :param y: the ordinate values\n        :param sd: standard deviation of equations (one or more values used\n                   cyclically)\n        :param wt: an optional alternate for `sd`\n        :param fid: the id of the sub-fitter (numerical)\n\n        Example::\n\n            fit = fitserver()\n            x = N.arange(1,11) # we have values at 10 'x' values\n            y = 2. + 0.5*x - 0.1*x**2 # which are 2 +0.5x -0.1x^2\n            fit.fitspoly(3, x, y) # fit a 3-degree polynomial\n            print fit.solution(), fit.error() #  show solution and their errors\n\n        \"\"\"\n        a = max(abs(max(x)), abs(min(x)))\n        if a == 0:\n            a = 1\n        a = 1.0 / a\n        b = NUM.power(a, range(n + 1))\n        if self.set(n=n + 1, fid=fid):\n            self.linear(poly(n), x * a, y, sd, wt, fid)\n            self._fitids[fid][\"sol\"] *= b\n            self._fitids[fid][\"error\"] *= b\n            return self.linear(poly(n), x, y, sd, wt, fid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a non - linear least squares solution for the points at the ordinate values x y sd wt mxit and fid", "response": "def functional(self, fnct, x, y, sd=None, wt=1.0, mxit=50, fid=0):\n        \"\"\"Make a non-linear least squares solution.\n\n        This will make a non-linear least squares solution for the points\n        through the ordinates at the abscissa values, using the specified\n        `fnct`. Details can be found in the :meth:`linear` description.\n\n        :param fnct: the functional to fit\n        :param x: the abscissa values\n        :param y: the ordinate values\n        :param sd: standard deviation of equations (one or more values used\n                   cyclically)\n        :param wt: an optional alternate for `sd`\n        :param mxit: the maximum number of iterations\n        :param fid: the id of the sub-fitter (numerical)\n\n        \"\"\"\n        self._fit(fitfunc=\"functional\", fnct=fnct, x=x, y=y, sd=sd, wt=wt,\n                  mxit=mxit, fid=fid)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a linear least squares solution for the points through the base set of the entry sets.", "response": "def linear(self, fnct, x, y, sd=None, wt=1.0, fid=0):\n        \"\"\"Make a linear least squares solution.\n\n        Makes a linear least squares solution for the points through the\n        ordinates at the x values, using the specified fnct. The x can be of\n        any dimension, depending on the number of arguments needed in the\n        functional evaluation. The values should be given in the order:\n        x0[1], x0[2], ..., x1[1], ..., xn[m] if there are n observations,\n        and m arguments. x should be a vector of m*n length; y (the\n        observations) a vector of length n.\n\n        :param fnct: the functional to fit\n        :param x: the abscissa values\n        :param y: the ordinate values\n        :param sd: standard deviation of equations (one or more values used\n                   cyclically)\n        :param wt: an optional alternate for `sd`\n        :param fid: the id of the sub-fitter (numerical)\n\n        \"\"\"\n        self._fit(fitfunc=\"linear\", fnct=fnct, x=x, y=y, sd=sd, wt=wt, fid=fid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nobtain the set of orthogonal equations that make the solution of the rank deficient normal equations possible.", "response": "def constraint(self, n=-1, fid=0):\n        \"\"\"Obtain the set of orthogonal equations that make the solution of\n        the rank deficient normal equations possible.\n\n        :param fid: the id of the sub-fitter (numerical)\n\n        \"\"\"\n        c = self._getval(\"constr\", fid)\n        if n < 0 or n > self.deficiency(fid):\n            return c\n        else:\n            raise RuntimeError(\"Not yet implemented\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests if enough Levenberg - Marquardt loops have been done.", "response": "def fitted(self, fid=0):\n        \"\"\"Test if enough Levenberg-Marquardt loops have been done.\n\n        It returns True if no improvement possible.\n\n        :param fid: the id of the sub-fitter (numerical)\n        \"\"\"\n        self._checkid(fid)\n        return not (self._fitids[fid][\"fit\"] > 0\n                    or self._fitids[fid][\"fit\"] < -0.001)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copydoc(fromfunc, sep=\"\\n\"):\n    def _decorator(func):\n        sourcedoc = fromfunc.__doc__\n        if func.__doc__ is None:\n            func.__doc__ = sourcedoc\n        else:\n            func.__doc__ = sep.join([sourcedoc, func.__doc__])\n        return func\n    return _decorator", "response": "Decorator that copies the docstring of fromfunc to the new one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f(self, x):\n\n        \"\"\"Calculate the value of the functional for the specified arguments\n        (taking any specified mask into account).\n\n\n        :param x: the value(s) to evaluate at\n        \"\"\"\n        x = self._flatten(x)\n        if self._dtype == 0:\n            return numpy.array(_functional._f(self, x))\n        else:\n            return numpy.array(_functional._fc(self, x))", "response": "Calculate the value of the functional for the specified arguments at\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the value of the functional for the specified arguments and the derivatives with respect to the parameters.", "response": "def fdf(self, x):\n        \"\"\"Calculate the value of the functional for the specified arguments,\n        and the derivatives with respect to the parameters (taking any\n        specified mask into account).\n\n\n       :param x: the value(s) to evaluate at\n       \"\"\"\n        x = self._flatten(x)\n        n = 1\n        if hasattr(x, \"__len__\"):\n            n = len(x)\n        if self._dtype == 0:\n            retval = _functional._fdf(self, x)\n        else:\n            retval = _functional._fdfc(self, x)\n        if len(retval) == n:\n            return numpy.array(retval)\n        return numpy.array(retval).reshape(self.npar() + 1,\n                                           n // self.ndim()).transpose()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the location of the measures data directory.", "response": "def set_data_path(self, pth):\n        \"\"\"Set the location of the measures data directory.\n\n        :param pth: The absolute path to the measures data directory.\n        \"\"\"\n        if os.path.exists(pth):\n            if not os.path.exists(os.path.join(pth, 'data', 'geodetic')):\n                raise IOError(\"The given path doesn't contain a 'data' \"\n                              \"subdirectory\")\n            os.environ[\"AIPSPATH\"] = \"%s dummy dummy\" % pth"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef measure(self, v, rf, off=None):\n        if off is None:\n            off = {}\n        keys = [\"m0\", \"m1\", \"m2\"]\n        for key in keys:\n            if key in v:\n                if dq.is_quantity(v[key]):\n                    v[key] = v[key].to_dict()\n        return _measures.measure(self, v, rf, off)", "response": "Create a measure using the frame state set on the measures\n        server instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef direction(self, rf='', v0='0..', v1='90..', off=None):\n        loc = {'type': 'direction', 'refer': rf}\n        loc['m0'] = dq.quantity(v0)\n        loc['m1'] = dq.quantity(v1)\n        if is_measure(off):\n            if not off['type'] == \"direction\":\n                raise TypeError('Illegal offset type specified.')\n            loc[\"offset\"] = off\n        return self.measure(loc, rf)", "response": "Defines a direction measure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndefines a doppler measure. It can be used to measure a doppler of a specific type of object.", "response": "def doppler(self, rf='', v0=0.0, off=None):\n        \"\"\"Defines a doppler measure. It has to specify a reference code,\n        doppler quantity value (see introduction for the action on a scalar\n        quantity with either a vector or scalar value, and when a vector of\n        quantities is given), and optionally it can specify an offset, which\n        in itself has to be a doppler.\n\n        :param rf: reference code string; Allowable reference\n                   codes are: *RADIO OPTICAL Z RATIO RELATIVISTIC BETA GAMMA*.\n                   Note that additional ones may become available. Check with::\n\n                       dm.list_codes(dm.doppler())\n\n        :param v0: doppler ratio as quantity, string or float value. It\n                   should be either non-dimensioned to specify a ratio of\n                   the light velocity, or in velocity. (examples all give\n                   same doppler):\n        :param off: an optional offset measure of same type\n\n        Example::\n\n            >>> from casacore import quanta\n            >>> dm.doppler('radio', 0.4)\n            >>> dm.doppler('radio', '0.4')\n            >>> dm.doppler('RADIO', quanta.constants['c']*0.4))\n\n        \"\"\"\n        if isinstance(v0, float):\n            v0 = str(v0)\n        loc = {'type': \"doppler\",\n               'refer': rf,\n               'm0': dq.quantity(v0)}\n        if is_measure(off):\n            if not off['type'] == \"doppler\":\n                raise TypeError('Illegal offset type specified.')\n            loc[\"offset\"] = off\n        return self.measure(loc, rf)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefine a radialvelocity measure. It can specify a radialvelocity measure. It can specify a radialvelocity offset measure.", "response": "def radialvelocity(self, rf='', v0='0m/s', off=None):\n        \"\"\"Defines a radialvelocity measure. It has to specify a reference\n        code, radialvelocity quantity value (see introduction for the action\n        on a scalar quantity with either a vector or scalar value, and when\n        a vector of quantities is given), and optionally it can specify an\n        offset, which in itself has to be a radialvelocity.\n\n\n        :param rf: reference code string; Allowable reference\n                   codes are: *LSRK LSRD BARY GEO TOPO GALACTO*\n                   Note that additional ones may become available. Check with::\n\n                       dm.list_codes(dm.radialvelocity())\n\n        :param v0: longitude or x as quantity or string\n        :param off: an optional offset measure of same type\n\n        \"\"\"\n        loc = {'type': \"radialvelocity\",\n               'refer': rf,\n               'm0': dq.quantity(v0)}\n        if is_measure(off):\n            if not off['type'] == \"radialvelocity\":\n                raise TypeError('Illegal offset type specified.')\n            loc[\"offset\"] = off\n        return self.measure(loc, rf)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef baseline(self, rf='', v0='0..', v1='', v2='', off=None):\n        loc = {'type': \"baseline\", 'refer': rf}\n        loc['m0'] = dq.quantity(v0)\n        loc['m1'] = dq.quantity(v1)\n        loc['m2'] = dq.quantity(v2)\n        if is_measure(off):\n            if not off['type'] == \"doppler\":\n                raise TypeError('Illegal offset type specified.')\n            loc[\"offset\"] = off\n        return self.measure(loc, rf)", "response": "Defines a baselin measure. It has to specify a baseline measure. It can specify a baseline measure. It can specify a offset measure and offset of the baseline measure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tofrequency(self, rf, v0, rfq):\n        if is_measure(rfq) and rfq['type'] == 'frequency':\n            rfq = dq.quantity(rfq['m0'])\n        elif isinstance(rfq, str):\n            rfq = dq.quantity(rfq)\n        if is_measure(v0) and v0['type'] == 'doppler' \\\n                and dq.is_quantity(rfq) \\\n                and rfq.conforms(dq.quantity('Hz')):\n            return self.doptofreq(v0, rf, rfq)\n        else:\n            raise TypeError('Illegal Doppler or rest frequency specified')", "response": "Convert a Doppler type value to a frequency."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a frequency measure and a doppler measure to a rest frequency.", "response": "def torestfrequency(self, f0, d0):\n        \"\"\"Convert a frequency measure and a doppler measure (e.g.\n        obtained from another spectral line with a known rest frequency) to\n        a rest frequency.\n\n        :param f0: frequency reference code (see :meth:`frequency`)\n        :param v0: a doppler measure\n\n        Example::\n\n            dp = dm.doppler('radio', '2196.24984km/s')  # a measured doppler speed\n            f = dm.frequency('lsrk','1410MHz')    # a measured frequency\n            dm.torestfrequency(f, dp)        # the corresponding rest frequency\n\n        \"\"\"\n        if is_measure(f0) and f0['type'] == 'frequency' \\\n                and is_measure(d0) and d0['type'] == 'doppler':\n            return self.torest(f0, d0)\n        else:\n            raise TypeError('Illegal Doppler or rest frequency specified')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert radial velocity or frequency measure to doppler.", "response": "def todoppler(self, rf, v0, rfq):\n        \"\"\"Convert a radialvelocity measure or a frequency measure to a\n        doppler measure. In the case of a frequency, a rest frequency has\n        to be specified. The type of doppler wanted (e.g. *RADIO*) has to be\n        specified.\n\n        :param rf: doppler reference code (see :meth:`doppler`)\n        :param v0: a radialvelocity or frequency measure\n        :param rfq: frequency measure or quantity\n\n        Example::\n\n            f = dm.frequency('lsrk','1410MHz')     # specify a frequency\n            dm.todoppler('radio', f, dm.constants('HI')) # give doppler, using HI rest\n\n        \"\"\"\n        if is_measure(rfq) and rfq['type'] == 'frequency':\n            rfq = dq.quantity(rfq['m0'])\n        elif isinstance(rfq, str):\n            rfq = dq.quantity(rfq)\n        if is_measure(v0):\n            if v0['type'] == 'radialvelocity':\n                return self.todop(v0, dq.quantity(1., 'Hz'))\n            elif v0['type'] == 'frequency' and dq.is_quantity(rfq) \\\n                    and rfq.conforms(dq.quantity('Hz')):\n                return self.todop(v0, rfq)\n            else:\n                raise TypeError('Illegal Doppler or rest frequency specified')\n        else:\n            raise TypeError('Illegal Frequency specified')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a Doppler type value ( e. g. in radio mode v0 should be specified.", "response": "def toradialvelocity(self, rf, v0):\n        \"\"\"Convert a Doppler type value (e.g. in radio mode) to a real\n        radialvelocity. The type of velocity (e.g. *LSRK*) should be specified\n\n        :param rf: radialvelocity reference code (see :meth:`radialvelocity`)\n        :param v0: a doppler measure\n\n        Example::\n\n            a = dm.doppler('radio',0.4)\n            dm.toradialvelocity('topo',a)\n\n        \"\"\"\n        if is_measure(v0) and v0['type'] == 'doppler':\n            return self.doptorv(rf, v0)\n        else:\n            raise TypeError('Illegal Doppler specified')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef touvw(self, v):\n        if is_measure(v) and v['type'] == 'baseline':\n            m = _measures.uvw(self, v)\n            m['xyz'] = dq.quantity(m['xyz'])\n            m['dot'] = dq.quantity(m['dot'])\n            return m\n        else:\n            raise TypeError('Illegal Baseline specified')", "response": "Calculates a uvw measure from a baseline."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expand(self, v):\n        if not is_measure(v) or v['type'] not in ['baseline',\n                                                  'position', 'uvw']:\n            raise TypeError(\"Can only expand baselines, positions, or uvw\")\n        vw = v.copy()\n        vw['type'] = \"uvw\"\n        vw['refer'] = \"J2000\"\n        outm = _measures.expand(self, vw)\n        outm['xyz'] = dq.quantity(outm['xyz'])\n        outm['measure']['type'] = v['type']\n        outm['measure']['refer'] = v['refer']\n        return outm", "response": "Calculates the differences between a series of given measure values and a series of given measure values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a position measure into a baseline measure.", "response": "def asbaseline(self, pos):\n        \"\"\"Convert a position measure into a baseline measure. No actual\n        baseline is calculated, since operations can be done on positions,\n        with subtractions to obtain baselines at a later stage.\n\n        :param pos: a position measure\n        :returns: a baseline measure\n\n        \"\"\"\n        if not is_measure(pos) or pos['type'] not in ['position', 'baseline']:\n            raise TypeError('Argument is not a position/baseline measure')\n        if pos['type'] == 'position':\n            loc = self.measure(pos, 'itrf')\n            loc['type'] = 'baseline'\n            return self.measure(loc, 'j2000')\n        return pos"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of quantities making up the measures value.", "response": "def getvalue(self, v):\n        \"\"\"\n        Return a list of quantities making up the measures' value.\n\n        :param v: a measure\n        \"\"\"\n        if not is_measure(v):\n            raise TypeError('Incorrect input type for getvalue()')\n        import re\n        rx = re.compile(\"m\\d+\")\n        out = []\n        keys = v.keys()[:]\n        keys.sort()\n        for key in keys:\n            if re.match(rx, key):\n                out.append(dq.quantity(v.get(key)))\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef riseset(self, crd, ev=\"5deg\"):\n\n        a = self.rise(crd, ev)\n        if isinstance(a['rise'], str):\n            return {\"rise\": {\"last\": a[0], \"utc\": a[0]},\n                    \"set\": {\"last\": a[1], \"utc\": a[1]},\n                    \"solved\": False}\n        ofe = self.measure(self._framestack[\"epoch\"], \"utc\")\n        if not is_measure(ofe):\n            ofe = self.epoch('utc', 'today')\n        x = a.copy()\n        for k in x:\n            x[k] = self.measure(\n                self.epoch(\"last\",\n                           a[k].totime(),\n                           off=self.epoch(\"r_utc\",\n                                          (dq.quantity(ofe[\"m0\"])\n                                           + dq.quantity(\"0.5d\")\n                                           ))\n                           ),\n                \"utc\")\n        return {\"rise\": {\"last\": self.epoch(\"last\",\n                                            a[\"rise\"].totime()),\n                         \"utc\": x[\"rise\"]},\n\n                \"set\": {\"last\": self.epoch(\"last\",\n                                           a[\"set\"].totime()),\n                        \"utc\": x[\"set\"]},\n                \"solved\": True\n                }", "response": "This will give the rise and set times of a source. It needs the position in the frame and a time. It needs the elevation limit as a quantity or string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rownrs(self, key, upperkey={}, lowerincl=True, upperincl=True):\n        lkey = self._makekey(key)\n        ukey = self._makekey(upperkey)\n        if len(ukey) == 0:\n            return self._rownrs(lkey)\n        return self._rownrsrange(lkey, ukey, lowerincl, upperincl)", "response": "Get a sequence of row numbers containing the key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the columns MODEL_DATA CORRECTED_DATA and IMAGING_WEIGHT to an MS.", "response": "def addImagingColumns(msname, ack=True):\n    \"\"\" Add the columns to an MS needed for the casa imager.\n\n    It adds the columns MODEL_DATA, CORRECTED_DATA, and IMAGING_WEIGHT.\n    It also sets the CHANNEL_SELECTION keyword needed for the older casa\n    imagers.\n\n    A column is not added if already existing.\n    \"\"\"\n    # numpy is needed\n    import numpy as np\n    # Open the MS\n    t = table(msname, readonly=False, ack=False)\n    cnames = t.colnames()\n    # Get the description of the DATA column.\n    try:\n        cdesc = t.getcoldesc('DATA')\n    except:\n        raise ValueError('Column DATA does not exist')\n    # Determine if the DATA storage specification is tiled.\n    hasTiled = False\n    try:\n        dminfo = t.getdminfo(\"DATA\")\n        if dminfo['TYPE'][:5] == 'Tiled':\n            hasTiled = True\n    except:\n        hasTiled = False\n    # Use TiledShapeStMan if needed.\n    if not hasTiled:\n        dminfo = {'TYPE': 'TiledShapeStMan',\n                  'SPEC': {'DEFAULTTILESHAPE': [4, 32, 128]}}\n    # Add the columns(if not existing). Use the description of the DATA column.\n    if 'MODEL_DATA' in cnames:\n        six.print_(\"Column MODEL_DATA not added; it already exists\")\n    else:\n        dminfo['NAME'] = 'modeldata'\n        cdesc['comment'] = 'The model data column'\n        t.addcols(maketabdesc(makecoldesc('MODEL_DATA', cdesc)), dminfo)\n        if ack:\n            six.print_(\"added column MODEL_DATA\")\n    if 'CORRECTED_DATA' in cnames:\n        six.print_(\"Column CORRECTED_DATA not added; it already exists\")\n    else:\n        dminfo['NAME'] = 'correcteddata'\n        cdesc['comment'] = 'The corrected data column'\n        t.addcols(maketabdesc(makecoldesc('CORRECTED_DATA', cdesc)), dminfo)\n        if ack:\n            six.print_(\"'added column CORRECTED_DATA\")\n    if 'IMAGING_WEIGHT' in cnames:\n        six.print_(\"Column IMAGING_WEIGHT not added; it already exists\")\n    else:\n        # Add IMAGING_WEIGHT which is 1-dim and has type float.\n        # It needs a shape, otherwise the CASA imager complains.\n        shp = []\n        if 'shape' in cdesc:\n            shp = cdesc['shape']\n        if len(shp) > 0:\n            shp = [shp[0]]  # use nchan from shape\n        else:\n            shp = [t.getcell('DATA', 0).shape[0]]  # use nchan from actual data\n        cd = makearrcoldesc('IMAGING_WEIGHT', 0, ndim=1, shape=shp,\n                            valuetype='float')\n        dminfo = {'TYPE': 'TiledShapeStMan',\n                  'SPEC': {'DEFAULTTILESHAPE': [32, 128]}}\n        dminfo['NAME'] = 'imagingweight'\n        t.addcols(maketabdesc(cd), dminfo)\n        if ack:\n            six.print_(\"added column IMAGING_WEIGHT\")\n    # Add or overwrite keyword CHANNEL_SELECTION.\n    if 'CHANNEL_SELECTION' in t.colkeywordnames('MODEL_DATA'):\n        t.removecolkeyword('MODEL_DATA', 'CHANNEL_SELECTION')\n    # Define the CHANNEL_SELECTION keyword containing the channels of\n    # all spectral windows.\n    tspw = table(t.getkeyword('SPECTRAL_WINDOW'), ack=False)\n    nchans = tspw.getcol('NUM_CHAN')\n    chans = [[0, nch] for nch in nchans]\n    t.putcolkeyword('MODEL_DATA', 'CHANNEL_SELECTION', np.int32(chans))\n    if ack:\n        six.print_(\"defined keyword CHANNEL_SELECTION in column MODEL_DATA\")\n    # Flush the table to make sure it is written.\n    t.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef addDerivedMSCal(msname):\n\n    # Open the MS\n    t = table(msname, readonly=False, ack=False)\n    colnames = t.colnames()\n    # Check that the columns needed by DerivedMSCal are present.\n    # Note that ANTENNA2 and FEED2 are not required.\n    for col in [\"TIME\", \"ANTENNA1\", \"FIELD_ID\", \"FEED1\"]:\n        if col not in colnames:\n            raise ValueError(\"Columns \" + colnames +\n                             \" should be present in table \" + msname)\n    scols1 = ['HA', 'HA1', 'HA2', 'PA1', 'PA2']\n    scols2 = ['LAST', 'LAST1', 'LAST2']\n    acols1 = ['AZEL1', 'AZEL2']\n    acols2 = ['UVW_J2000']\n    descs = []\n    # Define the columns and their units.\n    for col in scols1:\n        descs.append(makescacoldesc(col, 0.,\n                                    keywords={\"QuantumUnits\": [\"rad\"]}))\n    for col in scols2:\n        descs.append(makescacoldesc(col, 0.,\n                                    keywords={\"QuantumUnits\": [\"d\"]}))\n    for col in acols1:\n        descs.append(makearrcoldesc(col, 0.,\n                                    keywords={\"QuantumUnits\": [\"rad\", \"rad\"]}))\n    for col in acols2:\n        descs.append(makearrcoldesc(col, 0.,\n                                    keywords={\"QuantumUnits\": [\"m\", \"m\", \"m\"],\n                                              \"MEASINFO\": {\"Ref\": \"J2000\",\n                                                           \"type\": \"uvw\"}}))\n    # Add all columns using DerivedMSCal as data manager.\n    dminfo = {\"TYPE\": \"DerivedMSCal\", \"NAME\": \"\", \"SPEC\": {}}\n    t.addcols(maketabdesc(descs), dminfo)\n    # Flush the table to make sure it is written.\n    t.flush()", "response": "Add the derived columns like HA to an MS or CalTable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef removeDerivedMSCal(msname):\n\n    # Open the MS\n    t = table(msname, readonly=False, ack=False)\n    # Remove the columns stored as DerivedMSCal.\n    dmi = t.getdminfo()\n    for x in dmi.values():\n        if x['TYPE'] == 'DerivedMSCal':\n            t.removecols(x['COLUMNS'])\n    t.flush()", "response": "Remove the derived columns from an MS or CalTable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef msregularize(msname, newname):\n\n    # Find out all baselines.\n    t = table(msname)\n    t1 = t.sort('unique ANTENNA1,ANTENNA2')\n    nadded = 0\n    # Now iterate in time,band over the MS.\n    for tsub in t.iter(['TIME', 'DATA_DESC_ID']):\n        nmissing = t1.nrows() - tsub.nrows()\n        if nmissing < 0:\n            raise ValueError(\"A time/band chunk has too many rows\")\n        if nmissing > 0:\n            # Rows needs to be added for the missing baselines.\n            ant1 = str(t1.getcol('ANTENNA1')).replace(' ', ',')\n            ant2 = str(t1.getcol('ANTENNA2')).replace(' ', ',')\n            ant1 = tsub.getcol('ANTENNA1')\n            ant2 = tsub.getcol('ANTENNA2')\n            t2 = taql('select from $t1 where !any(ANTENNA1 == $ant1 &&' +\n                      ' ANTENNA2 == $ant2)')\n            six.print_(nmissing, t1.nrows(), tsub.nrows(), t2.nrows())\n            if t2.nrows() != nmissing:\n                raise ValueError(\"A time/band chunk behaves strangely\")\n            # If nothing added yet, create a new table.\n            # (which has to be reopened for read/write).\n            # Otherwise append to that new table.\n            if nadded == 0:\n                tnew = t2.copy(newname + \"_add\", deep=True)\n                tnew = table(newname + \"_add\", readonly=False)\n            else:\n                t2.copyrows(tnew)\n            # Set the correct time and band in the new rows.\n            tnew.putcell('TIME',\n                         range(nadded, nadded + nmissing),\n                         tsub.getcell('TIME', 0))\n            tnew.putcell('DATA_DESC_ID',\n                         range(nadded, nadded + nmissing),\n                         tsub.getcell('DATA_DESC_ID', 0))\n            nadded += nmissing\n    # Combine the existing table and new table.\n    if nadded > 0:\n        # First initialize data and flags in the added rows.\n        taql('update $tnew set DATA=0+0i')\n        taql('update $tnew set FLAG=True')\n        tcomb = table([t, tnew])\n        tcomb.rename(newname + '_adds')\n        tcombs = tcomb.sort('TIME,DATA_DESC_ID,ANTENNA1,ANTENNA2')\n    else:\n        tcombs = t.query(offset=0)\n    tcombs.rename(newname)\n    six.print_(newname, 'has been created; it references the original MS')\n    if nadded > 0:\n        six.print_(' and', newname + '_adds', 'containing', nadded, 'new rows')\n    else:\n        six.print_(' no rows needed to be added')", "response": "Regularize an MS\n\n    The output MS will be such that it has the same number of baselines\n    for each time stamp. Where needed fully flagged rows are added.\n\n    Possibly missing rows are written into a separate MS <newname>-add.\n    It is concatenated with the original MS and sorted in order of TIME,\n    DATADESC_ID, ANTENNA1,ANTENNA2 to form a new regular MS. Note that\n    the new MS references the input MS (it does not copy the data).\n    It means that changes made in the new MS are also made in the input MS.\n\n    If no rows were missing, the new MS is still created referencing the\n    input MS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getshapestring(self, startrow=1, nrow=-1, rowincr=1):\n        return self._table.getcolshapestring(self._column,\n                                             startrow, nrow, rowincr)", "response": "Get the shapes of all cells in the column in string format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a slice from a column cell holding an array.", "response": "def getcellslice(self, rownr, blc, trc, inc=[]):\n        \"\"\"Get a slice from a column cell holding an array.\n        (see :func:`table.getcellslice`)\"\"\"\n        return self._table.getcellslice(self._column, rownr, blc, trc, inc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the contents of the column or part of it.", "response": "def getcol(self, startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Get the contents of the column or part of it.\n        (see :func:`table.getcol`)\"\"\"\n        return self._table.getcol(self._column, startrow, nrow, rowincr)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getvarcol(self, startrow=0, nrow=-1, rowincr=1):\n        return self._table.getvarcol(self._column, startrow, nrow, rowincr)", "response": "Get the contents of the column or part of it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getcolslice(self, blc, trc, inc=[], startrow=0, nrow=-1, rowincr=1):\n        return self._table.getcolslice(self._column, blc, trc, inc, startrow, nrow, rowincr)", "response": "Get a slice from a table column holding arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef putcell(self, rownr, value):\n        return self._table.putcell(self._column, rownr, value)", "response": "Put a value into one or more table cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nput into a slice of a table cell holding an array.", "response": "def putcellslice(self, rownr, value, blc, trc, inc=[]):\n        \"\"\"Put into a slice of a table cell holding an array.\n        (see :func:`table.putcellslice`)\"\"\"\n        return self._table.putcellslice(self._column, rownr, value, blc, trc, inc)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef putcol(self, value, startrow=0, nrow=-1, rowincr=1):\n        return self._table.putcol(self._column, value, startrow, nrow, rowincr)", "response": "Put a column or part of it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nputs a value into a column of a variable table.", "response": "def putvarcol(self, value, startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Put an entire column or part of it.\n        (see :func:`table.putvarcol`)\"\"\"\n        return self._table.putvarcol(self._column, value, startrow, nrow, rowincr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef putcolslice(self, value, blc, trc, inc=[], startrow=0, nrow=-1, rowincr=1):\n        return self._table.putcolslice(self._column, value, blc, trc, inc, startrow, nrow, rowincr)", "response": "Put into a slice in a table column holding arrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nput the value of a column keyword.", "response": "def putkeyword(self, keyword, value, makesubrecord=False):\n        \"\"\"Put the value of a column keyword.\n        (see :func:`table.putcolkeyword`)\"\"\"\n        return self._table.putcolkeyword(self._column, keyword, value, makesubrecord)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter(self, order='', sort=True):\n        from casacore.tables import tableiter\n        return tableiter(self._table, [self._column], order, sort)", "response": "Return a tableiter object on this column."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index(self, sort=True):\n        from casacore.tables import tableindex\n        return tableindex(self._table, [self._column], sort)", "response": "Return a tableindex object on this column."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _repr_html_(self):\n        out=\"<table class='taqltable'>\\n\"\n\n        # Print column name (not if it is auto-generated)\n        if not(self.name()[:4]==\"Col_\"):\n            out+=\"<tr>\"\n            out+=\"<th><b>\"+self.name()+\"</b></th>\"\n            out+=\"</tr>\"\n\n        cropped=False\n        rowcount=0\n        colkeywords=self.getkeywords()\n        for row in self:\n            out +=\"\\n<tr>\"\n            out += \"<td>\" + _format_cell(row, colkeywords) + \"</td>\\n\"\n            out += \"</tr>\\n\"\n            rowcount+=1\n            out+=\"\\n\"\n            if rowcount>=20:\n                cropped=True\n                break\n\n        if out[-2:]==\"\\n\\n\":\n            out=out[:-1]\n\n        out+=\"</table>\"\n\n        if cropped:\n            out+=\"<p style='text-align:center'>(\"+str(self.nrows()-20)+\" more rows)</p>\\n\"\n\n        return out", "response": "Give a nice representation of columns in notebooks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_coordinatenames(self):\n        validnames = (\"direction\", \"spectral\", \"linear\", \"stokes\", \"tabular\")\n        self._names = [\"\"] * len(validnames)\n        n = 0\n        for key in self._csys.keys():\n            for name in validnames:\n                if key.startswith(name):\n                    idx = int(key[len(name):])\n                    self._names[idx] = name\n                    n += 1\n        # reverse as we are c order in python\n        self._names = self._names[:n][::-1]\n\n        if len(self._names) == 0:\n            raise LookupError(\"Coordinate record doesn't contain valid coordinates\")", "response": "Create ordered list of coordinate names"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_referencepixel(self, pix):\n        assert len(pix) == len(self._coord[\"crpix\"])\n        self._coord[\"crpix\"] = pix[::-1]", "response": "Set the reference pixel of the given axis in this coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the reference pixel of the given axis in this coordinate.", "response": "def set_referencevalue(self, val):\n        \"\"\"Set the reference pixel of the given axis in this coordinate.\"\"\"\n        assert len(val) == len(self._coord[\"crval\"])\n        self._coord[\"crval\"] = val[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the projection of the given axis in this coordinate.", "response": "def set_projection(self, val):\n        \"\"\"Set the projection of the given axis in this coordinate.\n\n        The known projections are SIN, ZEA, TAN, NCP, AIT, ZEA\n        \"\"\"\n        knownproj = [\"SIN\", \"ZEA\", \"TAN\", \"NCP\", \"AIT\", \"ZEA\"]  # etc\n        assert val.upper() in knownproj\n        self._coord[\"projection\"] = val.upper()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a table object from an ASCII file.", "response": "def tablefromascii(tablename, asciifile,\n                   headerfile='',\n                   autoheader=False, autoshape=[],\n                   columnnames=[], datatypes=[],\n                   sep=' ',\n                   commentmarker='',\n                   firstline=1, lastline=-1,\n                   readonly=True,\n                   lockoptions='default', ack=True):\n    \"\"\"Create a table from an ASCII file.\n\n    Create a table from a file in ASCII format. Columnar data as well as\n    table and column keywords may be specified.\n    Once the table is created from the ASCII data, it is opened in the\n    specified mode and a table object is returned.\n\n    The table columns are filled from a file containing the data values\n    separated by a separator (one line per table row). The default\n    separator is a blank. Blanks before and after the separator are ignored.\n    If a non-blank separator is used, values can be empty. Such values\n    default to 0, empty string, or F depending on the data type. E.g.\n    1,,2, has 4 values of which the 2nd and 4th are empty and default to 0.\n    Similarly if fewer values are given than needed, the missing values\n    get the default value.\n\n    Either the data format can be explicitly specified or it can be found\n    automatically. The former gives more control in ambiguous situations.\n    Both scalar and array columns can be generated from the ASCII input.\n    The format string determines the type and optional shape.\n\n    It is possible to give the column names and their data types in\n    various ways:\n\n    - Using 2 header lines (as described below) as the first two lines\n      in the data file or in a separate header file. This is the default way.\n    - Derive them automatically from the data (`autoheader=True`).\n    - Using the arguments `columnnames` and\n      `datatypes` (as non-empty vectors of strings).\n      It implies (`autoheader=False`). The data types should be\n      given in the same way as done in headers.\n\n    In automatic mode (`autoheader=True`) the first line\n    of the ASCII data is analyzed\n    to deduce the data types. Only the types I, D, and A can be\n    recognized. A number without decimal point or exponent is I (integer),\n    otherwise it is D (double). Any other string is A (string).\n    Note that a number may contain a leading sign (+ or -).\n    The `autoshape` argument can be used to specify if the input\n    should be stored as multiple scalars (the default) or as a single\n    array. In the latter case one axis in the shape can be defined as\n    variable length by giving it the value 0. It means that the actual\n    array shape in a row is determined by the number of values in the\n    corresponding input line.\n    Columns get the names `Column1`, `Column2`, etc..\n    For example:\n\n    1. `autoshape=[]` (which is the default) means that all values\n       are to be stored as scalar columns.\n    2. `autoshape=0` means that all values in a row are to be stored as\n       a variable length vector.\n    3. `autoshape=10` defines a fixed length vector. If an input\n       line contains less than 10 values, the vector is filled with default\n       values. If more than 10 values, the latter values are ignored.\n    4. `autoshape=[5,0]` defines a 2-dim array of which the 2nd axis is\n       variable. Note that if an input line does not contain a multiple of 5\n       values, the array is filled with default values.\n\n    If the format of the table is explicitly specified, it has to be done\n    either in the first two lines of the data file (named by the\n    argument filename), or in a separate header file (named by the\n    argument headerfile). In both forms, table keywords may also be\n    specified before the column definitions.\n    The column names and types can be described by two lines:\n\n    1. The first line contains the names of the columns.\n       These names may be enclosed in quotes (either single or double).\n    2. The second line contains the data type and optionally the shape\n       of each column. Valid types are:\n\n       - S for Short data\n       - I for Integer data\n       - R for Real data\n       - D for Double Precision data\n       - X for Complex data (Real followed by Imaginary)\n       - Z for Complex data (Amplitude then Phase)\n       - DX for Double Precision Complex data (Real followed by Imaginary)\n       - DZ for Double Precision Complex data (Amplitude then Phase)\n       - A for ASCII data (a value must be enclosed in single or double quotes\n         if it contains whitespace)\n       - B for Boolean data (False are empty string, 0, or any string\n         starting with F, f, N, or n).\n\n    If a column is an array, the shape has to be given after the data type\n    without any whitespace. E.g. `I10` defines an integer vector\n    of length 10. `A2,5` defines a 2-dim string array with shape\n    [2,5]. Note that `I` is not the same as `I1` as the\n    first one defines a scalar and the other one a vector with length 1.\n    The last column can have one variable length axis denoted by the value 0.\n    It \"consumes\" the remainder of the input line.\n\n    If the argument headerfile is set then the header information is\n    read from that file instead of the first lines of the data file.\n\n    To give a simple example of the form where the header information\n    is located at the top of the data file::\n\n      COLI   COLF   COLD       COLX        COLZ       COLS\n        I      R      D          X           Z          A\n        1      1.1    1.11       1.12 1.13   1.14 1.15  Str1\n        10     11     12         13   14     15   16    \"\"\n\n    Note that a complex number consists of 2 numbers.\n    Also note that an empty string can be given.\n\n    Let us now give an example of a separate header file that one might use to\n    get interferometer data into casacore::\n\n      U     V      W         TIME        ANT1       ANT2      DATA\n      R     R      R          D           I          I        X1,0\n\n    The data file would then look like::\n\n      124.011 54560.0  3477.1  43456789.0990    1      2        4.327 -0.1132\n      34561.0 45629.3  3900.5  43456789.0990    1      3        5.398 0.4521\n\n    Note that the DATA column is defined as a 2-dim array of 1\n    correlation and a variable number of channels, so the actual number of\n    channels is determined by the input. In this example both rows will\n    have 1 channel (note that a complex value contains 2 values).\n\n    Tables may have keywords in addition to the columns. The keywords\n    are useful for holding information that is global to the entire\n    table (such as author, revision, history, etc.).\n    The keywords in the header definitions must preceed the column descriptions.\n    They must be enclosed between a line that starts with \".key...\" and\n    a line that starts with \".endkey...\" (where ... can be anything).\n    A table keywordset and column keywordsets can be specified.\n    The latter can be specified by specifying the column name after\n    the .keywords string.\n    Between these two lines each line should contain the following:\n\n    - The keyword name, e.g., ANYKEY\n    - The datatype and optional  shape of the keyword\n      (cf. list of valid types above)\n    - The value or values for the keyword (the keyword may contain\n      a scalar or an array of values). e.g., 3.14159 21.78945\n\n    Thus to continue the example above, one might wish to add keywords\n    as follows::\n\n      .keywords\n      DATE        A  \"97/1/16\"\n      REVISION    D 2.01\n      AUTHOR      A \"Tim Cornwell\"\n      INSTRUMENT  A \"VLA\"\n      .endkeywords\n      .keywords TIME\n      UNIT A \"s\"\n      .endkeywords\n      U     V      W         TIME        ANT1       ANT2      DATA\n      R     R      R          D           I          I        X1,0\n\n    Similarly to the column format string, the keyword formats can also\n    contain shape information. The only difference is that if no shape is\n    given, a keyword can have multiple values (making it a vector).\n\n    It is possible to ignore comment lines in the header and data file\n    by giving the `commentmarker`. It indicates that lines\n    starting with the given marker are ignored. Note that the marker can\n    be a regular expression (e.g. `' *//'` tells that lines starting\n    with // and optionally preceeded by blanks have to be ignored).\n\n    With the arguments `firstline` and `lastline` one can\n    specify which lines have to be taken from the input file. A negative value\n    means 1 for `firstline` or end-of-file for `lastline`.\n    Note that if the headers and data are combined in one file,\n    these line arguments apply to the whole file. If headers and data are in\n    separate files, these line arguments apply to the data file only.\n\n    Also note that ignored comment lines are counted, thus are used to\n    determine which lines are in the line range.\n\n    The number of rows is determined by the number of lines read from the data\n    file.\n\n    \"\"\"\n    import os.path\n    filename = os.path.expandvars(asciifile)\n    filename = os.path.expanduser(filename)\n    if not os.path.exists(filename):\n        s = \"File '%s' not found\" % (filename)\n        raise IOError(s)\n    if headerfile != '':\n        filename = os.path.expandvars(headerfile)\n        filename = os.path.expanduser(filename)\n        if not os.path.exists(filename):\n            s = \"File '%s' not found\" % (filename)\n            raise IOError(s)\n    tab = table(asciifile, headerfile, tablename, autoheader, autoshape,\n                sep, commentmarker, firstline, lastline,\n                _columnnames=columnnames, _datatypes=datatypes, _oper=1)\n    six.print_('Input format: [' + tab._getasciiformat() + ']')\n    # Close table and reopen it in correct way.\n    tab = 0\n    return table(tablename, readonly=readonly, lockoptions=lockoptions,\n                 ack=ack)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a description of a scalar column.", "response": "def makescacoldesc(columnname, value,\n                   datamanagertype='',\n                   datamanagergroup='',\n                   options=0, maxlen=0, comment='',\n                   valuetype='', keywords={}):\n    \"\"\"Create description of a scalar column.\n\n    A description for a scalar column can be created from a name for\n    the column and a data value, which is used only to determine the\n    type of the column. Note that a dict value is also possible.\n\n    It is possible to create the column description in more detail\n    by giving the data manager name, group, option, and comment as well.\n\n    The data manager type tells which data manager (storage manager)\n    is used to store the columns. The data manager type and group are\n    explained in more detail in the `casacore Tables\n    <../../casacore/doc/html/group__Tables__module.html>`_ documentation.\n\n    It returns a dict with fields `name` and `desc` which can thereafter be used\n    to build a table description using function :func:`maketabdesc`.\n\n    `columname`\n      Name of column\n    `value`\n      Example data value used to determine the column's data type.\n      It is only used if argument `valuetype` is not given.\n    `datamanagertype`\n      Type of data manager which can be one of StandardStMan (default)\n      or IncrementalStMan. The latter one can save disk space if many subsequent\n      cells in the column will have the same value.\n    `datamanagergroup`\n      Data manager group. Only for the expert user.\n    `options`\n      Options. Need not be filled in.\n    `maxlen`\n      Maximum length of string values in a column.\n      Default 0 means unlimited.\n    `comment`\n      Comment: informational for user.\n    `valuetype`\n      A string giving the column's data type. Possible data types are\n      bool (or boolean), uchar (or byte), short, int (or integer), uint,\n      float, double, complex, dcomplex, and string.\n    'keywords'\n      A dict defining initial keywords for the column.\n\n    For example::\n\n      scd1 = makescacoldesc(\"col2\", \"\"))\n      scd2 = makescacoldesc(\"col1\", 1, \"IncrementalStMan\")\n      td = maketabdesc([scd1, scd2])\n\n    This creates a table description consisting of an integer column `col1`,\n    and a string column `col2`. `col1` uses the IncrementalStMan storage manager,\n    while `col2` uses the default storage manager StandardStMan.\n\n    \"\"\"\n    vtype = valuetype\n    if vtype == '':\n        vtype = _value_type_name(value)\n    rec2 = {'valueType': vtype,\n            'dataManagerType': datamanagertype,\n            'dataManagerGroup': datamanagergroup,\n            'option': options,\n            'maxlen': maxlen,\n            'comment': comment,\n            'keywords': keywords}\n    return {'name': columnname,\n            'desc': rec2}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a description of an array column.", "response": "def makearrcoldesc(columnname, value, ndim=0,\n                   shape=[], datamanagertype='',\n                   datamanagergroup='',\n                   options=0, maxlen=0, comment='',\n                   valuetype='', keywords={}):\n    \"\"\"Create description of an array column.\n\n    A description for a scalar column can be created from a name for\n    the column and a data value, which is used only to determine the\n    type of the column. Note that a dict value is also possible.\n\n    It is possible to create the column description in more detail\n    by giving the dimensionality, shape, data manager name, group, option,\n    and comment as well.\n\n    The data manager type tells which data manager (storage manager)\n    is used to store the columns. The data manager type and group are\n    explained in more detail in the `casacore Tables\n    <../../casacore/doc/html/group__Tables__module.html>`_ documentation.\n\n    It returns a dict with fields `name` and `desc` which can thereafter be used\n    to build a table description using function :func:`maketabdesc`.\n\n    `name`\n      The name of the column.\n    `value`\n      A data value, which is only used to determine the data type of the column.\n      It is only used if argument `valuetype` is not given.\n    `ndim`\n      Optionally the number of dimensions. A value > 0 means that all\n      arrays in the column must have that dimensionality. Note that the\n      arrays can still differ in shape unless the shape vector is also given.\n    `shape`\n      An optional sequence of integers giving the shape of the array in each\n      cell. If given, it forces option FixedShape (see below) and sets the\n      number of dimensions (if not given). All arrays in the column get the\n      given shape and the array is created as soon as a row is added.\n      Note that the shape vector gives the shape in each table cell; the\n      number of rows in the table should NOT be part of it.\n    `datamanagertype`\n      Type of data manager which can be one of StandardStMan (default),\n      IncrementalStMan, TiledColumnStMan, TiledCellStMan, or TiledShapeStMan.\n      The tiled storage managers are usually used for bigger data arrays.\n    `datamanagergroup`\n      Data manager group. Only for the expert user.\n    `options`\n      Optionally numeric array options which can be added to combine them.\n\n      `1` means Direct.\n          It tells that the data are directly stored in the table. Direct\n          forces option FixedShape. If not given, the array is indirect, which\n          means that the data will be stored in a separate file.\n      `4` means FixedShape.\n          This option does not need to be given, because it is enforced if\n          the shape is given. FixedShape means that the shape of the array must\n          be the same in each cell of the column. Otherwise the array shapes may\n          be different in each column cell and is it possible that a cell does\n          not contain an array at all.\n          Note that when given (or implicitly by option Direct), the\n          shape argument must be given as well.\n\n      Default is 0, thus indirect and variable shaped.\n    `maxlen`\n      Maximum length of string values in a column.\n      Default 0 means unlimited.\n    `comment`\n      Comment: informational for user.\n    `valuetype`\n      A string giving the column's data type. Possible data types are\n      bool (or boolean), uchar (or byte), short, int (or integer), uint,\n      float, double, complex, dcomplex, and string.\n    'keywords'\n      A dict defining initial keywords for the column.\n\n    For example::\n\n      acd1= makescacoldesc(\"arr1\", 1., 0, [2,3,4])\n      td = maketabdesc(acd1)\n\n    This creates a table description consisting of an array column `arr1`\n    containing 3-dim arrays of doubles with shape [2,3,4].\n\n    \"\"\"\n    vtype = valuetype\n    if vtype == '':\n        vtype = _value_type_name(value)\n    if len(shape) > 0:\n        if ndim <= 0:\n            ndim = len(shape)\n    rec2 = {'valueType': vtype,\n            'dataManagerType': datamanagertype,\n            'dataManagerGroup': datamanagergroup,\n            'ndim': ndim,\n            'shape': shape,\n            '_c_order': True,\n            'option': options,\n            'maxlen': maxlen,\n            'comment': comment,\n            'keywords': keywords}\n    return {'name': columnname,\n            'desc': rec2}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maketabdesc(descs=[]):\n    rec = {}\n    # If a single dict is given, make a list of it.\n    if isinstance(descs, dict):\n        descs = [descs]\n    for desc in descs:\n        colname = desc['name']\n        if colname in rec:\n            raise ValueError('Column name ' + colname + ' multiply used in table description')\n        rec[colname] = desc['desc']\n    return rec", "response": "Create a table description from a set of column descriptions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makedminfo(tabdesc, group_spec=None):\n  if group_spec is None:\n    group_spec = {}\n\n  class DMGroup(object):\n    \"\"\"\n    Keep track of the columns, type and spec of each data manager group\n    \"\"\"\n    def __init__(self):\n      self.columns = []\n      self.type = None\n      self.spec = None\n\n  dm_groups = defaultdict(DMGroup)\n\n  # Iterate through the table columns, grouping them\n  # by their dataManagerGroup\n  for c, d in six.iteritems(tabdesc):\n    if c in ('_define_hypercolumn_', '_keywords_', '_private_keywords_'):\n      continue\n\n    # Extract group and data manager type\n    group = d.get(\"dataManagerGroup\", \"StandardStMan\")\n    type_ = d.get(\"dataManagerType\", \"StandardStMan\")\n\n    # Set defaults if necessary\n    if not group:\n      group = \"StandardStMan\"\n\n    if not type_:\n      type_ = \"StandardStMan\"\n\n    # Obtain the (possibly empty) data manager group\n    dm_group = dm_groups[group]\n\n    # Add the column\n    dm_group.columns.append(c)\n\n    # Set the spec\n    if dm_group.spec is None:\n      dm_group.spec = group_spec.get(group, {})\n\n    # Check that the data manager type is consistent across columns\n    if dm_group.type is None:\n      dm_group.type = type_\n    elif not dm_group.type == type_:\n      raise ValueError(\"Mismatched dataManagerType '%s' \"\n                        \"for dataManagerGroup '%s' \"\n                        \"Previously, the type was '%s'\" %\n                            (type_, group, dm_group.type))\n\n  # Output a data manager entry\n  return {\n    '*%d'%(i+1): {\n      'COLUMNS': dm_group.columns,\n      'TYPE': dm_group.type,\n      'NAME': group,\n      'SPEC' : dm_group.spec,\n      'SEQNR': i\n    } for i, (group, dm_group)\n    in enumerate(six.iteritems(dm_groups))\n  }", "response": "Create a data manager information dictionary outline from a table description."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a hypercolumn to a table description. It defines a hypercolumn and adds it the given table description. A hypercolumn is an entity used by the Tiled Storage Managers (TSM). It defines which columns have to be stored together with a TSM. It should only be used by expert users who want to use a TSM to its full extent. For a basic TSM s hypercolumn definition is not needed. tabledesc A table description (result from :func:`maketabdesc`). name Name of hypercolumn ndim Dimensionality of hypercolumn; normally 1 more than the dimensionality of the arrays in the data columns to be stored with the TSM datacolumns Data columns to be stored with TSM coordcolumns Optional coordinate columns to be stored with TSM idcolumns Optional id columns to be stored with TSM For example:: scd1 = makescacoldesc(\"col2\", \"aa\") scd2 = makescacoldesc(\"col1\", 1, \"IncrementalStMan\") scd3 = makescacoldesc(\"colrec1\", {}) acd1 = makearrcoldesc(\"arr1\", 1, 0, [2,3,4]) acd2 = makearrcoldesc(\"arr2\", as_complex(0)) td = maketabdesc([scd1, scd2, scd3, acd1, acd2]) tabledefinehypercolumn(td, \"TiledArray\", 4, [\"arr1\"]) tab = table(\"mytable\", tabledesc=td, nrow=100) | This creates a table description `td` from five column descriptions and then creates a 100-row table called mytable from the table description. | The columns contain respectivily strings, integer scalars, records, 3D integer arrays with fixed shape [2,3,4], and complex arrays with variable shape. | The first array is stored with the Tiled Storage Manager (in this case the TiledColumnStMan).", "response": "def tabledefinehypercolumn(tabdesc,\n                           name, ndim, datacolumns,\n                           coordcolumns=False,\n                           idcolumns=False):\n    \"\"\"Add a hypercolumn to a table description.\n\n    It defines a hypercolumn and adds it the given table description.\n    A hypercolumn is an entity used by the Tiled Storage Managers (TSM). It\n    defines which columns have to be stored together with a TSM.\n\n    It should only be used by expert users who want to use a TSM to its\n    full extent. For a basic TSM s hypercolumn definition is not needed.\n\n    tabledesc\n      A table description (result from :func:`maketabdesc`).\n    name\n      Name of hypercolumn\n    ndim\n      Dimensionality of hypercolumn; normally 1 more than the dimensionality\n      of the arrays in the data columns to be stored with the TSM\n    datacolumns\n      Data columns to be stored with TSM\n    coordcolumns\n      Optional coordinate columns to be stored with TSM\n    idcolumns\n      Optional id columns to be stored with TSM\n\n    For example::\n\n      scd1 = makescacoldesc(\"col2\", \"aa\")\n      scd2 = makescacoldesc(\"col1\", 1, \"IncrementalStMan\")\n      scd3 = makescacoldesc(\"colrec1\", {})\n      acd1 = makearrcoldesc(\"arr1\", 1, 0, [2,3,4])\n      acd2 = makearrcoldesc(\"arr2\", as_complex(0))\n      td = maketabdesc([scd1, scd2, scd3, acd1, acd2])\n      tabledefinehypercolumn(td, \"TiledArray\", 4, [\"arr1\"])\n      tab = table(\"mytable\", tabledesc=td, nrow=100)\n\n    | This creates a table description `td` from five column descriptions\n      and then creates a 100-row table called mytable from the table\n      description.\n    | The columns contain respectivily strings, integer scalars, records,\n      3D integer arrays with fixed shape [2,3,4], and complex arrays with\n      variable shape.\n    | The first array is stored with the Tiled Storage Manager (in this case\n      the TiledColumnStMan).\n\n    \"\"\"\n    rec = {'HCndim': ndim,\n           'HCdatanames': datacolumns}\n    if not isinstance(coordcolumns, bool):\n        rec['HCcoordnames'] = coordcolumns\n    if not isinstance(idcolumns, bool):\n        rec['HCidnames'] = idcolumns\n    if '_define_hypercolumn_' not in tabdesc:\n        tabdesc['_define_hypercolumn_'] = {}\n    tabdesc['_define_hypercolumn_'][name] = rec"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a table on disk.", "response": "def tabledelete(tablename, checksubtables=False, ack=True):\n    \"\"\"Delete a table on disk.\n\n    It is the same as :func:`table.delete`, but without the need to open\n    the table first.\n\n    \"\"\"\n    tabname = _remove_prefix(tablename)\n    t = table(tabname, ack=False)\n    if t.ismultiused(checksubtables):\n        six.print_('Table', tabname, 'cannot be deleted; it is still in use')\n    else:\n        t = 0\n        table(tabname, readonly=False, _delete=True, ack=False)\n        if ack:\n            six.print_('Table', tabname, 'has been deleted')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tableexists(tablename):\n    result = True\n    try:\n        t = table(tablename, ack=False)\n    except:\n        result = False\n    return result", "response": "Test if a table exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tableiswritable(tablename):\n    result = True\n    try:\n        t = table(tablename, readonly=False, ack=False)\n        result = t.iswritable()\n    except:\n        result = False\n    return result", "response": "Test if a table is writable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tablecopy(tablename, newtablename, deep=False, valuecopy=False, dminfo={},\n              endian='aipsrc', memorytable=False, copynorows=False):\n    \"\"\"Copy a table.\n\n    It is the same as :func:`table.copy`, but without the need to open\n    the table first.\n\n    \"\"\"\n    t = table(tablename, ack=False)\n    return t.copy(newtablename, deep=deep, valuecopy=valuecopy,\n                  dminfo=dminfo, endian=endian, memorytable=memorytable,\n                  copynorows=copynorows)", "response": "Copy a table.\n\n    It is the same as :func:`table.copy`, but without the need to open\n    the table first."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrename a table. The table with the given name is renamed (or moved) to the new name.", "response": "def tablerename(tablename, newtablename):\n    \"\"\"Rename a table.\n\n    The table with the given name is renamed (or moved) to the new name.\n\n    \"\"\"\n    t = table(tablename, ack=False)\n    t.rename(newtablename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tablestructure(tablename, dataman=True, column=True, subtable=False,\n                   sort=False):\n    \"\"\"Print the structure of a table.\n\n    It is the same as :func:`table.showstructure`, but without the need to open\n    the table first.\n\n    \"\"\"\n    t = table(tablename, ack=False)\n    six.print_(t.showstructure(dataman, column, subtable, sort))", "response": "Print the structure of a table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the value of an attribute in the given row in a group.", "response": "def attrget(self, groupname, attrname, rownr):\n        \"\"\"Get the value of an attribute in the given row in a group.\"\"\"\n        return self._attrget(groupname, attrname, rownr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attrgetcol(self, groupname, attrname):\n        values = []\n        for rownr in range(self.attrnrows(groupname)):\n            values.append(self.attrget(groupname, attrname, rownr))\n        return values", "response": "Get the value of an attribute for all rows in a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef attrfindrows(self, groupname, attrname, value):\n        values = self.attrgetcol(groupname, attrname)\n        return [i for i in range(len(values)) if values[i] == value]", "response": "Get the row numbers of all rows where the attribute matches the given value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the values of all attributes of a group.", "response": "def attrgetrow(self, groupname, key, value=None):\n        \"\"\"Get the values of all attributes of a row in a group.\n\n        If the key is an integer, the key is the row number for which\n        the attribute values have to be returned.\n\n        Otherwise the key has to be a string and it defines the name of an\n        attribute. The attribute values of the row for which the key matches\n        the given value is returned.\n        It can only be used for unique attribute keys. An IndexError exception\n        is raised if no or multiple matches are found.\n        \"\"\"\n        if not isinstance(key, str):\n            return self._attrgetrow(groupname, key)\n        # The key is an attribute name whose value has to be found.\n        rownrs = self.attrfindrows(groupname, key, value)\n        if len(rownrs) == 0:\n            raise IndexError(\"Image attribute \" + key + \" in group \" +\n                             groupname + \" has no matches for value \" +\n                             str(value))\n        if len(rownrs) > 1:\n            raise IndexError(\"Image attribute \" + key + \" in group \" +\n                             groupname + \" has multiple matches for value \" +\n                             str(value))\n        return self._attrgetrow(groupname, rownrs[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attrput(self, groupname, attrname, rownr, value, unit=[], meas=[]):\n        return self._attrput(groupname, attrname, rownr, value, unit, meas)", "response": "Put the value and optionally unit and measinfo\n           of an attribute in a row in a group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget image data. Using the arguments blc (bottom left corner), trc (top right corner), and inc (stride) it is possible to get a data slice. The data is returned as a numpy array. Its dimensionality is the same as the dimensionality of the image, even if an axis has length 1.", "response": "def getdata(self, blc=(), trc=(), inc=()):\n        \"\"\"Get image data.\n\n        Using the arguments blc (bottom left corner), trc (top right corner),\n        and inc (stride) it is possible to get a data slice.\n\n        The data is returned as a numpy array. Its dimensionality is the same\n        as the dimensionality of the image, even if an axis has length 1.\n\n        \"\"\"\n        return self._getdata(self._adjustBlc(blc),\n                             self._adjustTrc(trc),\n                             self._adjustInc(inc))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget image mask. Using the arguments blc (bottom left corner), trc (top right corner), and inc (stride) it is possible to get a mask slice. Not all axes need to be specified. Missing values default to begin, end, and 1. The mask is returned as a numpy array. Its dimensionality is the same as the dimensionality of the image, even if an axis has length 1. Note that the casacore images use the convention that a mask value True means good and False means bad. However, numpy uses the opposite. Therefore the mask will be negated, so it can be used directly in numpy operations. If the image has no mask, an array will be returned with all values set to False.", "response": "def getmask(self, blc=(), trc=(), inc=()):\n        \"\"\"Get image mask.\n\n        Using the arguments blc (bottom left corner), trc (top right corner),\n        and inc (stride) it is possible to get a mask slice. Not all axes\n        need to be specified. Missing values default to begin, end, and 1.\n\n        The mask is returned as a numpy array. Its dimensionality is the same\n        as the dimensionality of the image, even if an axis has length 1.\n        Note that the casacore images use the convention that a mask value\n        True means good and False means bad. However, numpy uses the opposite.\n        Therefore the mask will be negated, so it can be used directly in\n        numpy operations.\n\n        If the image has no mask, an array will be returned with all values\n        set to False.\n\n        \"\"\"\n        return numpy.logical_not(self._getmask(self._adjustBlc(blc),\n                                               self._adjustTrc(trc),\n                                               self._adjustInc(inc)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the image data and mask.", "response": "def get(self, blc=(), trc=(), inc=()):\n        \"\"\"Get image data and mask.\n\n        Get the image data and mask (see ::func:`getdata` and :func:`getmask`)\n        as a numpy masked array.\n\n        \"\"\"\n        return nma.masked_array(self.getdata(blc, trc, inc),\n                                self.getmask(blc, trc, inc))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nput image data. Using the arguments blc (bottom left corner), trc (top right corner), and inc (stride) it is possible to put a data slice. Not all axes need to be specified. Missing values default to begin, end, and 1. The data should be a numpy array. Its dimensionality must be the same as the dimensionality of the image.", "response": "def putdata(self, value, blc=(), trc=(), inc=()):\n        \"\"\"Put image data.\n\n        Using the arguments blc (bottom left corner), trc (top right corner),\n        and inc (stride) it is possible to put a data slice. Not all axes\n        need to be specified. Missing values default to begin, end, and 1.\n\n        The data should be a numpy array. Its dimensionality must be the same\n        as the dimensionality of the image.\n\n        \"\"\"\n        return self._putdata(value, self._adjustBlc(blc),\n                             self._adjustInc(inc))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef putmask(self, value, blc=(), trc=(), inc=()):\n        # casa and numpy have opposite flags\n        return self._putmask(~value, self._adjustBlc(blc),\n                             self._adjustInc(inc))", "response": "Put image mask.\n\n        Using the arguments blc (bottom left corner), trc (top right corner),\n        and inc (stride) it is possible to put a data slice. Not all axes\n        need to be specified. Missing values default to begin, end, and 1.\n\n        The data should be a numpy array. Its dimensionality must be the same\n        as the dimensionality of the image.\n        Note that the casacore images use the convention that a mask value\n        True means good and False means bad. However, numpy uses the opposite.\n        Therefore the mask will be negated, so a numoy masked can be given\n        directly.\n\n        The mask is not written if the image has no mask and if it the entire\n        mask is False. In that case the mask most likely comes from a getmask\n        operation on an image without a mask."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put(self, value, blc=(), trc=(), inc=()):\n        if isinstance(value, nma.MaskedArray):\n            self.putdata(value.data, blc, trc, inc)\n            self.putmask(nma.getmaskarray(value), blc, trc, inc)\n        else:\n            self.putdata(value, blc, trc, inc)", "response": "Put the image data and optionally the mask."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nform a subimage. An image object containing a subset of an image is returned. The arguments blc (bottom left corner), trc (top right corner), and inc (stride) define the subset. Not all axes need to be specified. Missing values default to begin, end, and 1. By default axes with length 1 are left out. A subimage is a so-called virtual image. It is not stored, but only references the original image. It can be made persistent using the :func:`saveas` method.", "response": "def subimage(self, blc=(), trc=(), inc=(), dropdegenerate=True):\n        \"\"\"Form a subimage.\n\n        An image object containing a subset of an image is returned.\n        The arguments blc (bottom left corner), trc (top right corner),\n        and inc (stride) define the subset. Not all axes need to be specified.\n        Missing values default to begin, end, and 1.\n\n        By default axes with length 1 are left out.\n\n        A subimage is a so-called virtual image. It is not stored, but only\n        references the original image. It can be made persistent using the\n        :func:`saveas` method.\n\n        \"\"\"\n        return image(self._subimage(self._adjustBlc(blc),\n                                    self._adjustTrc(trc),\n                                    self._adjustInc(inc),\n                                    dropdegenerate))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget coordinates image info and unit", "response": "def info(self):\n        \"\"\"Get coordinates, image info, and unit\".\"\"\"\n        return {'coordinates': self._coordinates(),\n                'imageinfo': self._imageinfo(),\n                'miscinfo': self._miscinfo(),\n                'unit': self._unit()\n                }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tofits(self, filename, overwrite=True, velocity=True,\n               optical=True, bitpix=-32, minpix=1, maxpix=-1):\n        \"\"\"Write the image to a file in FITS format.\n\n        `filename`\n          FITS file name\n        `overwrite`\n          If False, an exception is raised if the new image file already exists.\n          Default is True.\n        `velocity`\n          By default a velocity primary spectral axis is written if possible.\n        `optical`\n          If writing a velocity, use the optical definition\n          (otherwise use radio).\n        `bitpix`\n          can be set to -32 (float) or 16 (short) only. When `bitpix` is\n          16 it will write BSCALE and BZERO into the FITS file. If minPix\n        `minpix` and `maxpix`\n          are used to determine BSCALE and BZERO if `bitpix=16`.\n          If `minpix` is greater than `maxpix` (which is the default),\n          the minimum and maximum pixel values will be determined from the ddta.\n          Oherwise the supplied values will be used and pixels outside that\n          range will be clipped to the minimum and maximum pixel values.\n          Note that this truncation does not occur for `bitpix=-32`.\n\n        \"\"\"\n        return self._tofits(filename, overwrite, velocity, optical,\n                            bitpix, minpix, maxpix)", "response": "Write the image to a FITS file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the image to disk.", "response": "def saveas(self, filename, overwrite=True, hdf5=False,\n               copymask=True, newmaskname=\"\", newtileshape=()):\n        \"\"\"Write the image to disk.\n\n        Note that the created disk file is a snapshot, so it is not updated\n        for possible later changes in the image object.\n\n        `overwrite`\n          If False, an exception is raised if the new image file already exists.\n          Default is True.\n        `ashdf5`\n          If True, the image is created in HDF5 format, otherwise in casacore\n          format. Default is casacore format.\n        `copymask`\n          By default the mask is written as well if the image has a mask.\n        'newmaskname`\n          If the mask is written, the name is the same the original or\n          `mask0` if the original mask has no name. Using this argument a\n          different mask name can be given.\n        `tileshape`\n          Advanced users can give a new tile shape. See the :mod:`tables`\n          module for more information about Tiled Storage Managers.\n\n        \"\"\"\n        self._saveas(filename, overwrite, hdf5,\n                     copymask, newmaskname,\n                     newtileshape)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating statistics for the image.", "response": "def statistics(self, axes=(), minmaxvalues=(), exclude=False, robust=True):\n        \"\"\"Calculate statistics for the image.\n\n        Statistics are returned in a dict for the given axes.\n        E.g. if axes [0,1] is given in a 3-dim image, the statistics are\n        calculated for each plane along the 3rd axis. By default statistics\n        are calculated for the entire image.\n\n        `minmaxvalues` can be given to include or exclude pixels\n        with values in the given range. If only one value is given,\n        min=-abs(val) and max=abs(val).\n\n        By default robust statistics (Median, MedAbsDevMed, and Quartile) are\n        calculated too.\n        \"\"\"\n        return self._statistics(self._adaptAxes(axes), \"\",\n                                minmaxvalues, exclude, robust)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef regrid(self, axes, coordsys, outname=\"\", overwrite=True,\n               outshape=(), interpolation=\"linear\",\n               decimate=10, replicate=False,\n               refchange=True, forceregrid=False):\n        \"\"\"Regrid the image to a new image object.\n\n         Regrid the image on the given axes to the given coordinate system.\n         The output is stored in the given file; it no file name is given a\n         temporary image is made.\n         If the output shape is empty, the old shape is used.\n         `replicate=True` means replication rather than regridding.\n\n        \"\"\"\n        return image(self._regrid(self._adaptAxes(axes),\n                                  outname, overwrite,\n                                  outshape, coordsys.dict(),\n                                  interpolation, decimate, replicate,\n                                  refchange, forceregrid))", "response": "Regrid the image on the given axes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying the image using casaviewer.", "response": "def view(self, tempname='/tmp/tempimage'):\n        \"\"\"Display the image using casaviewer.\n\n        If the image is not persistent, a copy will be made that the user\n        has to delete once viewing has finished. The name of the copy can be\n        given in argument `tempname`. Default is '/tmp/tempimage'.\n\n        \"\"\"\n        import os\n        # Test if casaviewer can be found.\n        # On OS-X 'which' always returns 0, so use test on top of it.\n        if os.system('test -x `which casaviewer` > /dev/null 2>&1') == 0:\n            six.print_(\"Starting casaviewer in the background ...\")\n            self.unlock()\n            if self.ispersistent():\n                os.system('casaviewer ' + self.name() + ' &')\n            elif len(tempname) > 0:\n                six.print_(\"  making a persistent copy in \" + tempname)\n                six.print_(\"  which should be deleted after the viewer has ended\")\n                self.saveas(tempname)\n                os.system('casaviewer ' + tempname + ' &')\n            else:\n                six.print_(\"Cannot view because the image is in memory only.\")\n                six.print_(\"You can browse a persistent copy of the image like:\")\n                six.print_(\"   t.view('/tmp/tempimage')\")\n        else:\n            six.print_(\"casaviewer cannot be found\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _do_remove_prefix(name):\n    res = name\n    if isinstance(res, str):\n        if (res.find('Table: ') == 0):\n            res = res.replace('Table: ', '', 1)\n    return res", "response": "Strip the possible prefix Table :'from a table name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstripping the possible prefix Table :", "response": "def _remove_prefix(name):\n    \"\"\"Strip the possible prefix 'Table: ' from one or more table names.\"\"\"\n    if isinstance(name, str):\n        return _do_remove_prefix(name)\n    return [_do_remove_prefix(nm) for nm in name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats a date in a node s tree.", "response": "def _format_date(val, unit):\n    \"\"\"\n    Format dates.\n    :param val: Value (just the value, not a quantity)\n    :param unit: Unit. Should be 'rad' or 's'\n    :return: A string representation of this date.\n\n    >>> _format_date(4914741782.503475, 's')\n    \"14-Aug-2014/14:03:03\"\n    \"\"\"\n    if val==numpy.floor(val) and unit=='d':\n        # Do not show time part if 0\n        return quantity(val,unit).formatted('YMD_ONLY')\n    else:\n        return quantity(val,unit).formatted('DMY')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a quantity with reasonable units.", "response": "def _format_quantum(val, unit):\n    \"\"\"\n    Format a quantity with reasonable units.\n    :param val: The value (just the value, not a quantity)\n    :param unit: Unit (something that can be fed to quanta).\n    :return: A string representation of this quantity.\n\n    >>> _format_quantum(3, 'm')\n    \"3 m\"\n    >>> _format_quantum(4914741782.503475, 's')\n    \"4.91474e+09 s\"\n    \"\"\"\n    q=quantity(val,unit)\n    if q.canonical().get_unit() in ['rad','s']:\n        return quantity(val, 'm').formatted()[:-1]+unit\n    else:\n        return q.formatted()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _format_cell(val, colkeywords):\n    out=\"\"\n\n    # String arrays are returned as dict, undo that for printing\n    if isinstance(val, dict):\n        tmpdict=numpy.array(val['array'])\n        tmpdict.reshape(val['shape'])\n        # Leave out quotes around strings\n        numpy.set_printoptions(formatter={'all':lambda x: str(x)})\n        out+=numpy.array2string(tmpdict, separator=', ')\n        # Revert previous numpy print options\n        numpy.set_printoptions(formatter=None)\n    else:\n        valtype='other'\n\n        # Check if the column unit is like 'm' or ['m','m','m']\n        singleUnit=('QuantumUnits' in colkeywords and\n                    (numpy.array(colkeywords['QuantumUnits'])==numpy.array(colkeywords['QuantumUnits'])[0]).all())\n        if colkeywords.get('MEASINFO',{}).get('type')=='epoch' and singleUnit:\n            # Format a date/time. Use quanta for scalars, use numpy for array logic around it\n            # (quanta does not support higher dimensional arrays)\n            valtype='epoch'\n            if isinstance(val, numpy.ndarray):\n                numpy.set_printoptions(formatter={'all':lambda x: _format_date(x,colkeywords['QuantumUnits'][0])})\n                out+=numpy.array2string(val,separator=', ')\n                numpy.set_printoptions(formatter=None)\n            else:\n                out+=_format_date(val,colkeywords['QuantumUnits'][0])\n        elif colkeywords.get('MEASINFO',{}).get('type')=='direction' and singleUnit and val.shape==(1,2):\n                # Format one direction. TODO: extend to array of directions\n                valtype='direction'\n                out+=\"[\"\n                part=quantity(val[0,0],'rad').formatted(\"TIME\",precision=9)\n                part=re.sub(r'(\\d+):(\\d+):(.*)',r'\\1h\\2m\\3',part)\n                out+=part+\", \"\n                part=quantity(val[0,1],'rad').formatted(\"ANGLE\",precision=9)\n                part=re.sub(r'(\\d+)\\.(\\d+)\\.(.*)',r'\\1d\\2m\\3',part)\n                out+=part+\"]\"\n        elif isinstance(val, numpy.ndarray) and singleUnit:\n            # Format any array with units\n            valtype='quanta'\n            numpy.set_printoptions(formatter={'all':lambda x: _format_quantum(x, colkeywords['QuantumUnits'][0])})\n            out+=numpy.array2string(val,separator=', ')\n            numpy.set_printoptions(formatter=None)\n        elif isinstance(val, numpy.ndarray):\n            valtype='other'\n            # Undo quotes around strings\n            numpy.set_printoptions(formatter={'all':lambda x: str(x)})\n            out+=numpy.array2string(val,separator=', ')\n            numpy.set_printoptions(formatter=None)\n        elif singleUnit:\n            valtype='onequantum'\n            out+=_format_quantum(val, colkeywords['QuantumUnits'][0])\n        else:\n            valtype='other'\n            out+=str(val)\n\n    if 'QuantumUnits' in colkeywords and valtype=='other':\n        # Print units if they haven't been taken care of\n        if not (numpy.array(colkeywords['QuantumUnits'])==numpy.array(colkeywords['QuantumUnits'])[0]).all():\n            # Multiple different units for element in an array.\n            # For now, just print the units and let the user figure out what it means\n            out+=\" \"+str(colkeywords['QuantumUnits'])\n        else:\n            out+=\" \"+colkeywords['QuantumUnits'][0]\n\n    # Numpy sometimes adds double newlines, don't do that\n    out=out.replace('\\n\\n','\\n')\n    return out", "response": "Format a single cell of the table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _format_row(row, colnames, tab):\n    out=\"\"\n\n    out+=\"\\n<tr>\"\n    for colname in colnames:\n        out+=\"<td style='vertical-align:top; white-space:pre'>\"\n        out+=_format_cell(row[colname], tab.getcolkeywords(colname))\n        out+=\"</td>\\n\"\n    out+=\"</tr>\\n\"\n    return out", "response": "Helper function for _repr_html. Formats one row."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to get the directory of the specified library.", "response": "def find_library_file(libname):\n    \"\"\"\n    Try to get the directory of the specified library.\n    It adds to the search path the library paths given to distutil's build_ext.\n    \"\"\"\n    # Use a dummy argument parser to get user specified library dirs\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument(\"--library-dirs\", \"-L\", default='')\n    args, unknown = parser.parse_known_args()\n    user_lib_dirs = args.library_dirs.split(':')\n    # Append default search path (not a complete list)\n    lib_dirs = user_lib_dirs + [os.path.join(sys.prefix, 'lib'),\n                              '/usr/local/lib',\n                              '/usr/lib',\n                              '/usr/lib/x86_64-linux-gnu']\n\n    compiler = ccompiler.new_compiler()\n    return compiler.find_library_file(lib_dirs, libname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the name of the boost - python library. Returns None if none is found.", "response": "def find_boost():\n    \"\"\"Find the name of the boost-python library. Returns None if none is found.\"\"\"\n    short_version = \"{}{}\".format(sys.version_info[0], sys.version_info[1])\n    boostlibnames = ['boost_python-py' + short_version,\n                     'boost_python' + short_version,\n                     'boost_python',\n                     ]\n\n    if sys.version_info[0] == 2:\n        boostlibnames += [\"boost_python-mt\"]\n    else:\n        boostlibnames += [\"boost_python3-mt\"]\n    for libboostname in boostlibnames:\n        if find_library_file(libboostname):\n            return libboostname\n    warnings.warn(no_boost_error)\n    return boostlibnames[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put(self, rownr, value, matchingfields=True):\n        self._put(rownr, value, matchingfields)", "response": "Put the values into the given row."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a quantity. This can be from a scalar or vector.", "response": "def quantity(*args):\n    \"\"\"Create a quantity. This can be from a scalar or vector.\n\n    Example::\n\n      q1 = quantity(1.0, \"km/s\")\n      q2 = quantity(\"1km/s\")\n      q1 = quantity([1.0,2.0], \"km/s\")\n\n    \"\"\"\n    if len(args) == 1:\n        if isinstance(args[0], str):\n            # use copy constructor to create quantity from string\n            return Quantity(from_string(args[0]))\n        elif isinstance(args[0], dict):\n            if hasattr(args[0][\"value\"], \"__len__\"):\n                return QuantVec(from_dict_v(args[0]))\n            else:\n                return Quantity(from_dict(args[0]))\n        elif isinstance(args[0], Quantity) or isinstance(args[0], QuantVec):\n            return args[0]\n        else:\n            raise TypeError(\"Invalid argument type for\")\n    else:\n        if hasattr(args[0], \"__len__\"):\n            return QuantVec(*args)\n        else:\n            return Quantity(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getlocals(back=2):\n    import inspect\n    fr = inspect.currentframe()\n    try:\n        while fr and back != 0:\n            fr1 = fr\n            fr = fr.f_back\n            back -= 1\n    except:\n        pass\n    return fr1.f_locals", "response": "Get the local variables some levels back."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the value of a local variable somewhere in the call stack.", "response": "def getvariable(name):\n    \"\"\"Get the value of a local variable somewhere in the call stack.\"\"\"\n    import inspect\n    fr = inspect.currentframe()\n    try:\n        while fr:\n            fr = fr.f_back\n            vars = fr.f_locals\n            if name in vars:\n                return vars[name]\n    except:\n        pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef substitute(s, objlist=(), globals={}, locals={}):\n    # Get the local variables at the caller level if not given.\n    if not locals:\n        locals = getlocals(3)\n    # Initialize some variables.\n    backslash = False\n    dollar = False\n    nparen = 0\n    name = ''\n    evalstr = ''\n    squote = False\n    dquote = False\n    out = ''\n    # Loop through the entire string.\n    for tmp in s:\n        if backslash:\n            out += tmp\n            backslash = False\n            continue\n        # If a dollar is found, we might have a name or expression.\n        # Alphabetics and underscore are always part of name.\n        if dollar and nparen == 0:\n            if tmp == '_' or ('a' <= tmp <= 'z') or ('A' <= tmp <= 'Z'):\n                name += tmp\n                continue\n            # Numerics are only part if not first character.\n            if '0' <= tmp <= '9' and name != '':\n                name += tmp\n                continue\n            # $( indicates the start of an expression to evaluate.\n            if tmp == '(' and name == '':\n                nparen = 1\n                evalstr = ''\n                continue\n            # End of name found. Try to substitute.\n            out += substitutename(name, objlist, globals, locals)\n            dollar = False\n\n        # Handle possible single or double quotes.\n        if tmp == '\"' and not squote:\n            dquote = not dquote\n        elif tmp == \"'\" and not dquote:\n            squote = not squote\n        if not dquote and not squote:\n            # Count the number of balanced parentheses\n            # (outside quoted strings) in the subexpression.\n            if nparen > 0:\n                if tmp == '(':\n                    nparen += 1\n                elif tmp == ')':\n                    nparen -= 1\n                    if nparen == 0:\n                        # The last closing parenthese is found.\n                        # Evaluate the subexpression.\n                        # Add the result to the output.\n                        out += substituteexpr(evalstr, globals, locals)\n                        dollar = False\n                evalstr += tmp\n                continue\n            # Set a switch if we have a dollar (outside quoted\n            # and eval strings).\n            if tmp == '$':\n                dollar = True\n                name = ''\n                continue\n        # No special character; add it to output or evalstr.\n        # Set a switch if we have a backslash.\n        if nparen == 0:\n            out += tmp\n        else:\n            evalstr += tmp\n        if tmp == '\\\\':\n            backslash = True\n\n        # The entire string has been handled.\n        # Substitute a possible last name.\n        # Insert a possible incomplete eval string as such.\n    if dollar:\n        out += substitutename(name, objlist, globals, locals)\n    else:\n        if nparen > 0:\n            out += '$(' + evalstr\n    return out", "response": "Substitute global python variables in a command string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a default Measurement Set with the given name.", "response": "def default_ms(name, tabdesc=None, dminfo=None):\n    \"\"\"\n    Creates a default Measurement Set called name. Any Table Description\n    elements in tabdesc will overwrite the corresponding element in a default\n    Measurement Set Table Description (columns, hypercolumns and keywords).\n\n    In practice, you probably want to specify columns such as DATA, MODEL_DATA\n    and CORRECTED_DATA (and their associated keywords and hypercolumns)\n    in tabdesc.\n    \"\"\"\n\n    # Default to empty dictionaries\n    if tabdesc is None:\n        tabdesc = {}\n\n    if dminfo is None:\n        dminfo = {}\n\n    # Wrap the Table object\n    return table(_default_ms(name, tabdesc, dminfo), _oper=3)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a default Measurement Set subtable.", "response": "def default_ms_subtable(subtable, name=None, tabdesc=None, dminfo=None):\n    \"\"\"\n    Creates a default Measurement Set subtable. Any Table Description\n    elements in tabdesc will overwrite the corresponding element in a default\n    Measurement Set Table Description (columns, hypercolumns and keywords).\n\n    if name is given, it will be treated as a path that the table should\n    be created in. Set to subtable if None\n\n    if subtable is \"\" or \"MAIN\" a standard MeasurementSet with subtables will\n    be created.\n    \"\"\"\n\n    if name is None:\n        name = subtable\n\n    # Default to empty dictionaries\n    if tabdesc is None:\n        tabdesc = {}\n\n    if dminfo is None:\n        dminfo = {}\n\n    # Wrap the Table object\n    return table(_default_ms_subtable(subtable, name, tabdesc, dminfo),\n                 _oper=3)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef taql(command, style='Python', tables=[], globals={}, locals={}):\n    # Substitute possible tables given as $name.\n    cmd = command\n    # Copy the tables argument and make sure it is a list\n    tabs = []\n    for tab in tables:\n        tabs += [tab]\n    try:\n        import casacore.util\n        if len(locals) == 0:\n            # local variables in caller are 3 levels up from getlocals\n            locals = casacore.util.getlocals(3)\n        cmd = casacore.util.substitute(cmd, [(table, '', tabs)],\n                                       globals, locals)\n    except Exception:\n        pass\n    if style:\n        cmd = 'using style ' + style + ' ' + cmd\n    tab = table(cmd, tabs, _oper=2)\n    result = tab._getcalcresult()\n    # If result is empty, it was a normal TaQL command resulting in a table.\n    # Otherwise it is a record containing calc values.\n    if len(result) == 0:\n        return tab\n    return result['values']", "response": "Execute a TaQL command and return a table object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a tablerow object which includes or excludes the given columns.", "response": "def row(self, columnnames=[], exclude=False):\n        \"\"\"Return a tablerow object which includes (or excludes) the\n        given columns.\n\n        :class:`tablerow` makes it possible to get/put values in one or\n        more rows.\n\n        \"\"\"\n        from .tablerow import tablerow\n        return tablerow(self, columnnames, exclude)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter(self, columnnames, order='', sort=True):\n        from .tableiter import tableiter\n        return tableiter(self, columnnames, order, sort)", "response": "Return an iterator over the table with the given columnnames."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef index(self, columnnames, sort=True):\n        from .tableindex import tableindex\n        return tableindex(self, columnnames, sort)", "response": "Return a tableindex object that lets one get the row numbers of the rows holding the column names given values for the given columnnames."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toascii(self, asciifile, headerfile='', columnnames=(), sep=' ',\n                precision=(), usebrackets=True):\n        \"\"\"Write the table in ASCII format.\n\n        It is approximately the inverse of the from-ASCII-contructor.\n\n        `asciifile`\n          The name of the resulting ASCII file.\n        `headerfile`\n          The name of an optional file containing the header info. If not\n          given or if equal to argument `asciifile`, the headers are written\n          at the beginning of the ASCII file.\n        `columnnames`\n          The names of the columns to be written. If not given or if the first\n          name is empty, all columns are written.\n        `sep`\n          The separator to be used between values. Only the first character\n          of a string is used. If not given or mepty, a blank is used.\n        `precision`\n          For each column the precision can be given. It is only used for\n          columns containing floating point numbers. A value <=0 means using\n          the default which is 9 for single and 18 for double precision.\n        `usebrackets`\n          If True, arrays and records are written enclosed in [].\n          Multi-dimensional arrays have [] per dimension. In this way variable\n          shaped array can be read back correctly. However, it is not supported\n          by :func:`tablefromascii`.\n          If False, records are not written and arrays are written linearly\n          with the shape defined in the header as supported byI\n          :func:`tablefromascii`.\n\n        Note that columns containing records or variable shaped arrays are\n        ignored, because they cannot be written to ASCII. It is told which\n        columns are ignored.\n\n        For example::\n\n          t  = table('3c343.MS')\n          t1 = t.query('ANTENNA1 != ANTENNA2')   # do row selection\n          t1.toascii ('3c343.txt')               # write selection as ASCII\n\n        \"\"\"\n        msg = self._toascii(asciifile, headerfile, columnnames, sep,\n                            precision, usebrackets)\n        if len(msg) > 0:\n            six.print_(msg)", "response": "Write the table in ASCII format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy the table and return a new table object for the copy.", "response": "def copy(self, newtablename, deep=False, valuecopy=False, dminfo={},\n             endian='aipsrc', memorytable=False, copynorows=False):\n        \"\"\"Copy the table and return a table object for the copy.\n\n        It copies all data in the columns and keywords.\n        Besides the table, all its subtables are copied too.\n        By default a shallow copy is made (usually by copying files).\n        It means that the copy of a reference table is also a reference table.\n        Use `deep=True` to make a deep copy which turns a reference table\n        into a normal table.\n\n        `deep=True`\n          a deep copy of a reference table is made.\n        `valuecopy=True`\n          values are copied, which reorganizes normal tables and removes wasted\n          space. It implies `deep=True`. It is slower than a normal copy.\n        `dminfo`\n          gives the option to specify data managers to change the way columns\n          are stored. This is a dict as returned by method :func:`getdminfo`.\n        `endian`\n          specifies the endianness of the new table when a deep copy is made:\n          |  'little' = as little endian\n          |  'big'    = as big endian\n          |  'local'  = use the endianness of the machine being used\n          |  'aipsrc' = use as defined in an .aipsrc file (defaults to local)\n        `memorytable=True`\n          do not copy to disk, but to a table kept in memory.\n        `copynorows=True`\n          only copy the column layout and keywords, but no data.\n\n        For example::\n\n          t  = table('3c343.MS')\n          t1 = t.query('ANTENNA1 != ANTENNA2')   # do row selection\n          t2 = t1.copy ('3c343.sel', True)       # make deep copy\n          t2 = t.copy ('new.tab', True, True)    # reorganize storage\n\n        \"\"\"\n        t = self._copy(newtablename, memorytable, deep, valuecopy,\n                       endian, dminfo, copynorows)\n        # copy returns a Table object, so turn that into table.\n        return table(t, _oper=3)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies the contents of rows from this table to outtable.", "response": "def copyrows(self, outtable, startrowin=0, startrowout=-1, nrow=-1):\n        \"\"\"Copy the contents of rows from this table to outtable.\n\n        The contents of the columns with matching names are copied.\n        The other arguments can be used to specify where to start copying.\n        By default the entire input table is appended to the output table.\n        Rows are added to the output table if needed.\n\n        `startrowin`\n          Row where to start in the input table.\n        `startrowout`\n          Row where to start in the output table,\n          | -1 means write at the end of the output table.\n        `nrow`\n          Number of rows to copy\n          | -1 means from startrowin till the end of the input table\n\n        The following example appends row to the table itself, thus doubles\n        the number of rows::\n\n          t:=table('test.ms',readonly=F)\n          t.copyrows(t)\n\n        \"\"\"\n        self._copyrows(outtable, startrowin, startrowout, nrow)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rownumbers(self, table=None):\n        if table is None:\n            return self._rownumbers(Table())\n        return self._rownumbers(table)", "response": "Return a list containing the row numbers of this table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the shapes of all cells in the column in string format.", "response": "def getcolshapestring(self, columnname,\n                          startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Get the shapes of all cells in the column in string format.\n\n        It returns the shape in a string like [10,20,30].\n\n        If the column contains fixed shape arrays, a single shape is returned.\n        Otherwise a list of shape strings is returned.\n\n        The column can be sliced by giving a start row (default 0), number of\n        rows (default all), and row stride (default 1).\n\n        \"\"\"\n        return self._getcolshapestring(columnname,\n                                       startrow, nrow, rowincr,\n                                       True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the contents of a column cell into the given numpy array.", "response": "def getcellnp(self, columnname, rownr, nparray):\n        \"\"\"Get data from a column cell into the given numpy array .\n\n        Get the contents of a cell containing an array into the\n        given numpy array. The numpy array has to be C-contiguous\n        with a shape matching the shape of the column cell.\n        Data type coercion will be done as needed.\n\n        \"\"\"\n        if not nparray.flags.c_contiguous or nparray.size == 0:\n            raise ValueError(\"Argument 'nparray' has to be a contiguous \" +\n                             \"numpy array\")\n        return self._getcellvh(columnname, rownr, nparray)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getcellslice(self, columnname, rownr, blc, trc, inc=[]):\n        return self._getcellslice(columnname, rownr,\n                                  blc, trc, inc)", "response": "Get a slice from a column cell holding an array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getcellslicenp(self, columnname, nparray, rownr, blc, trc, inc=[]):\n        if not nparray.flags.c_contiguous or nparray.size == 0:\n            raise ValueError(\"Argument 'nparray' has to be a contiguous \" +\n                             \"numpy array\")\n        return self._getcellslicevh(columnname, rownr,\n                                    blc, trc, inc, nparray)", "response": "Get a slice from a column cell into the given numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the contents of a column or part of it.", "response": "def getcol(self, columnname, startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Get the contents of a column or part of it.\n\n        It is returned as a numpy array.\n        If the column contains arrays, they should all have the same shape.\n        An exception is thrown if they differ in shape. In that case the\n        method :func:`getvarcol` should be used instead.\n\n        The column can be sliced by giving a start row (default 0), number of\n        rows (default all), and row stride (default 1).\n\n        \"\"\"\n        #        try:     # trial code to read using a vector of rownrs\n        #            nr = len(startrow)\n        #            if nrow < 0:\n        #                nrow = nr\n        #            if nrow == 0:\n        #                return numpy.array()\n        #            for inx in range(nrow):\n        #                i = inx*\n        #        except:\n        return self._getcol(columnname, startrow, nrow, rowincr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the contents of a column or part of it into a given numpy array.", "response": "def getcolnp(self, columnname, nparray, startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Get the contents of a column or part of it into the given\n        numpy array.\n\n        The numpy array has to be C-contiguous with a shape matching the\n        shape of the column (part). Data type coercion will be done as needed.\n\n        If the column contains arrays, they should all have the same shape.\n        An exception is thrown if they differ in shape. In that case the\n        method :func:`getvarcol` should be used instead.\n\n        The column can be sliced by giving a start row (default 0), number of\n        rows (default all), and row stride (default 1).\n\n        \"\"\"\n        if (not nparray.flags.c_contiguous) or nparray.size == 0:\n            raise ValueError(\"Argument 'nparray' has to be a contiguous \" +\n                             \"numpy array\")\n        return self._getcolvh(columnname, startrow, nrow, rowincr, nparray)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the contents of a column or part of it.", "response": "def getvarcol(self, columnname, startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Get the contents of a column or part of it.\n\n        It is similar to :func:`getcol`, but the result is returned as a\n        dict of numpy arrays.\n        It can deal with a column containing variable shaped arrays.\n\n        \"\"\"\n        return self._getvarcol(columnname, startrow, nrow, rowincr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a slice from a table column holding arrays.", "response": "def getcolslice(self, columnname, blc, trc, inc=[],\n                    startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Get a slice from a table column holding arrays.\n\n        The slice in each array is given by blc, trc, and inc\n        (as in getcellslice).\n        The column can be sliced by giving a start row (default 0), number of\n        rows (default all), and row stride (default 1).\n\n        It returns a numpy array where the first axis is formed by the column\n        cells. The other axes are the array axes.\n\n        \"\"\"\n        return self._getcolslice(columnname, blc, trc, inc,\n                                 startrow, nrow, rowincr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getcolslicenp(self, columnname, nparray, blc, trc, inc=[],\n                      startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Get a slice from a table column into the given numpy array.\n\n        The numpy array has to be C-contiguous with a shape matching the\n        shape of the column (slice). Data type coercion will be done as needed.\n\n        The slice in each array is given by blc, trc, and inc\n        (as in getcellslice).\n        The column can be sliced by giving a start row (default 0), number of\n        rows (default all), and row stride (default 1).\n\n        It returns a numpy array where the first axis is formed by the column\n        cells. The other axes are the array axes.\n\n        \"\"\"\n        if not nparray.flags.c_contiguous or nparray.size == 0:\n            raise ValueError(\"Argument 'nparray' has to be a contiguous \"\n                             + \"numpy array\")\n        return self._getcolslicevh(columnname, blc, trc, inc,\n                                   startrow, nrow, rowincr, nparray)", "response": "Get a slice from a table column into a given numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs a value into one or more table cells.", "response": "def putcell(self, columnname, rownr, value):\n        \"\"\"Put a value into one or more table cells.\n\n        The columnname and (0-relative) rownrs indicate the  table cells.\n        rownr can be a single row number or a sequence of row numbers.\n        If multiple rownrs are given, the given value is put in all those rows.\n\n        The given value has to be convertible to the data type of the column.\n        If the column contains scalar values, the given value must be a scalar.\n        The value for a column holding arrays can be given as:\n\n        - a scalar resulting in a 1-dim array of 1 element\n        - a sequence (list, tuple) resulting in a 1-dim array\n        - a numpy array of any dimensionality\n\n        Note that the arrays in a column may have a fixed dimensionality or\n        shape. In that case the dimensionality or shape of the array to put\n        has to conform.\n\n        \"\"\"\n        self._putcell(columnname, rownr, value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef putcellslice(self, columnname, rownr, value, blc, trc, inc=[]):\n        self._putcellslice(columnname, rownr, value,\n                           blc, trc, inc)", "response": "Put into a slice of a table cell holding an array."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nput a column or part of it.", "response": "def putcol(self, columnname, value, startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Put an entire column or part of it.\n\n        If the column contains scalar values, the given value should be a 1-dim\n        array. Otherwise it is a numpy array where the first axis is formed by\n        the column cells.\n\n        The column can be sliced by giving a start row (default 0), number of\n        rows (default all), and row stride (default 1).\n\n        \"\"\"\n        self._putcol(columnname, startrow, nrow, rowincr, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nputs a column or part of it.", "response": "def putvarcol(self, columnname, value, startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Put an entire column or part of it.\n\n        It is similar to putcol, but the shapes of the arrays in the column\n        can vary. The value has to be a dict of numpy arrays.\n\n        The column can be sliced by giving a start row (default 0), number of\n        rows (default all), and row stride (default 1).\n\n        \"\"\"\n        self._putvarcol(columnname, startrow, nrow, rowincr, value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef putcolslice(self, columnname, value, blc, trc, inc=[],\n                    startrow=0, nrow=-1, rowincr=1):\n        \"\"\"Put into a slice in a table column holding arrays.\n\n        Its arguments are the same as for getcolslice and putcellslice.\n\n        \"\"\"\n        self._putcolslice(columnname, value, blc, trc, inc,\n                          startrow, nrow, rowincr)", "response": "Put into a slice in a table column holding arrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding one or more columns to a reference table.", "response": "def addcols(self, desc, dminfo={}, addtoparent=True):\n        \"\"\"Add one or more columns.\n\n        Columns can always be added to a normal table.\n        They can also be added to a reference table and optionally to its\n        parent table.\n\n        `desc`\n          contains a description of the column(s) to be added. It can be given\n          in three ways:\n\n          - a dict created by :func:`maketabdesc`. In this way multiple\n            columns can be added.\n          - a dict created by :func:`makescacoldesc`, :func:`makearrcoldesc`,\n            or :func:`makecoldesc`. In this way a single column can be added.\n          - a dict created by :func:`getcoldesc`. The key 'name' containing\n            the column name has to be defined in such a dict.\n\n        `dminfo`\n          can be used to provide detailed data manager info to tell how the\n          column(s) have to be stored. The dminfo of an existing column can be\n          obtained using method :func:`getdminfo`.\n        `addtoparent`\n          defines if the column should also be added to the parent table in\n          case the current table is a reference table (result of selection).\n          If True, it will be added to the parent if it does not exist yet.\n\n        For example, add a column using the same data manager type as another\n        column::\n\n          coldmi = t.getdminfo('colarrtsm')     # get dminfo of existing column\n          coldmi[\"NAME\"] = 'tsm2'               # give it a unique name\n          t.addcols (maketabdesc(makearrcoldesc(\"colarrtsm2\",0., ndim=2)),\n                     coldmi)\n\n        \"\"\"\n        tdesc = desc\n        # Create a tabdesc if only a coldesc is given.\n        if 'name' in desc:\n            import casacore.tables.tableutil as pt\n            if len(desc) == 2 and 'desc' in desc:\n                # Given as output from makecoldesc\n                tdesc = pt.maketabdesc(desc)\n            elif 'valueType' in desc:\n                # Given as output of getcoldesc (with a name field added)\n                cd = pt.makecoldesc(desc['name'], desc)\n                tdesc = pt.maketabdesc(cd)\n        self._addcols(tdesc, dminfo, addtoparent)\n        self._makerow()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef renamecol(self, oldname, newname):\n        self._renamecol(oldname, newname)\n        self._makerow()", "response": "Rename a column in a single table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fieldnames(self, keyword=''):\n        if isinstance(keyword, str):\n            return self._getfieldnames('', keyword, -1)\n        else:\n            return self._getfieldnames('', '', keyword)", "response": "Get the names of the fields in a table keyword value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the names of the fields in a column.", "response": "def colfieldnames(self, columnname, keyword=''):\n        \"\"\"Get the names of the fields in a column keyword value.\n\n        The value of a keyword can be a struct (python dict). This method\n        returns the names of the fields in that struct.\n        Each field in a struct can be a struct in itself. Names of fields in a\n        sub-struct can be obtained by giving a keyword name consisting of\n        multiple parts separated by dots (e.g. 'key1.sub1.sub2').\n\n        If an empty keyword name is given (which is the default), all keyword\n        names of the column are shown and its behaviour is the same as\n        :func:`colkeywordnames`.\n\n        Instead of a keyword name an index can be given which returns the names\n        of the struct value of the i-th keyword.\n\n        \"\"\"\n        if isinstance(keyword, str):\n            return self._getfieldnames(columnname, keyword, -1)\n        else:\n            return self._getfieldnames(columnname, '', keyword)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getkeyword(self, keyword):\n        if isinstance(keyword, str):\n            return self._getkeyword('', keyword, -1)\n        else:\n            return self._getkeyword('', '', keyword)", "response": "Get the value of a table keyword."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getcolkeyword(self, columnname, keyword):\n        if isinstance(keyword, str):\n            return self._getkeyword(columnname, keyword, -1)\n        else:\n            return self._getkeyword(columnname, '', keyword)", "response": "Get the value of a column keyword."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getsubtables(self):\n        keyset = self.getkeywords()\n        names = []\n        for key, value in keyset.items():\n            if isinstance(value, str) and value.find('Table: ') == 0:\n                names.append(_do_remove_prefix(value))\n        return names", "response": "Get the names of all subtables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef putkeyword(self, keyword, value, makesubrecord=False):\n        val = value\n        if isinstance(val, table):\n            val = _add_prefix(val.name())\n        if isinstance(keyword, str):\n            return self._putkeyword('', keyword, -1, makesubrecord, val)\n        else:\n            return self._putkeyword('', '', keyword, makesubrecord, val)", "response": "Put the value of a table keyword."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef removekeyword(self, keyword):\n        if isinstance(keyword, str):\n            self._removekeyword('', keyword, -1)\n        else:\n            self._removekeyword('', '', keyword)", "response": "Removes a table keyword."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving a column keyword.", "response": "def removecolkeyword(self, columnname, keyword):\n        \"\"\"Remove a column keyword.\n\n        It is similar to :func:`removekeyword`.\n\n        \"\"\"\n        if isinstance(keyword, str):\n            self._removekeyword(columnname, keyword, -1)\n        else:\n            self._removekeyword(columnname, '', keyword)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the table description.", "response": "def getdesc(self, actual=True):\n        \"\"\"Get the table description.\n\n        By default it returns the actual description (thus telling the\n        actual array shapes and data managers used).\n        `actual=False` means that the original description as made by\n        :func:`maketabdesc` is returned.\n        \"\"\"\n\n        tabledesc = self._getdesc(actual, True)\n\n        # Strip out 0 length \"HCcoordnames\" and \"HCidnames\"\n        # as these aren't valid. (See tabledefinehypercolumn)\n        hcdefs = tabledesc.get('_define_hypercolumn_', {})\n\n        for c, hcdef in hcdefs.iteritems():\n            if \"HCcoordnames\" in hcdef and len(hcdef[\"HCcoordnames\"]) == 0:\n                del hcdef[\"HCcoordnames\"]\n            if \"HCidnames\" in hcdef and len(hcdef[\"HCidnames\"]) == 0:\n                del hcdef[\"HCidnames\"]\n\n        return tabledesc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getcoldesc(self, columnname, actual=True):\n        return self._getcoldesc(columnname, actual, True)", "response": "Get the description of a column."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef coldesc(self, columnname, actual=True):\n        import casacore.tables.tableutil as pt\n        return pt.makecoldesc(columnname, self.getcoldesc(columnname, actual))", "response": "Make the description of a column."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getdminfo(self, columnname=None):\n        dminfo = self._getdminfo()\n        if columnname is None:\n            return dminfo\n        # Find the info for the given column\n        for fld in dminfo.values():\n            if columnname in fld[\"COLUMNS\"]:\n                fldc = fld.copy()\n                del fldc['COLUMNS']  # remove COLUMNS field\n                return fldc\n        raise KeyError(\"Column \" + columnname + \" does not exist\")", "response": "Get the data manager info for a particular column."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the properties of a data manager.", "response": "def setdmprop(self, name, properties, bycolumn=True):\n        \"\"\"Set properties of a data manager.\n\n        Properties (e.g. cachesize) of a data manager can be changed by\n        defining them appropriately in the properties argument (a dict).\n        Current values can be obtained using function :func:`getdmprop` which\n        also serves as a template. The dict can contain more fields; only\n        the fields with the names as returned by getdmprop are handled.\n\n        The data manager can be specified in two ways: by data manager name\n        or by the name of a column using the data manager. The argument\n        `bycolumn` defines which way is used (default is by column name).\n\n        \"\"\"\n        return self._setdmprop(name, properties, bycolumn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing the structure of this table.", "response": "def showstructure(self, dataman=True, column=True, subtable=False,\n                      sort=False):\n        \"\"\"Show table structure in a formatted string.\n\n        The structure of this table and optionally its subtables is shown.\n        It shows the data manager info and column descriptions.\n        Optionally the columns are sorted in alphabetical order.\n\n        `dataman`\n          Show data manager info? If False, only column info is shown.\n          If True, data manager info and columns per data manager are shown.\n        `column`\n          Show column description per data manager? Only takes effect if\n          dataman=True.\n        `subtable`\n          Show the structure of all subtables (recursively).\n          The names of subtables are always shown.\n        'sort'\n          Sort the columns in alphabetical order?\n\n        \"\"\"\n        return self._showstructure(dataman, column, subtable, sort)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef summary(self, recurse=False):\n        six.print_('Table summary:', self.name())\n        six.print_('Shape:', self.ncols(), 'columns by', self.nrows(), 'rows')\n        six.print_('Info:', self.info())\n        tkeys = self.getkeywords()\n        if (len(tkeys) > 0):\n            six.print_('Table keywords:', tkeys)\n        columns = self.colnames()\n        if (len(columns) > 0):\n            six.print_('Columns:', columns)\n            for column in columns:\n                ckeys = self.getcolkeywords(column)\n                if (len(ckeys) > 0):\n                    six.print_(column, 'keywords:', ckeys)\n        if (recurse):\n            for key, value in tkeys.items():\n                tabname = _remove_prefix(value)\n                six.print_('Summarizing subtable:', tabname)\n                lt = table(tabname)\n                if (not lt.summary(recurse)):\n                    break\n        return True", "response": "Print a summary of the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef selectrows(self, rownrs):\n        t = self._selectrows(rownrs, name='')\n        # selectrows returns a Table object, so turn that into table.\n        return table(t, _oper=3)", "response": "Return a reference table containing the given rows."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquery the table and return the result as a reference table. This method queries the table. It forms a `TaQL <../../doc/199.html>`_ command from the given arguments and executes it using the :func:`taql` function. The result is returned in a so-called reference table which references the selected columns and rows in the original table. Usually a reference table is temporary, but it can be made persistent by giving it a name. Note that a reference table is handled as any table, thus can be queried again. All arguments are optional, but at least one of `query`, `name`, `sortlist`, and `columns` should be used. See the `TaQL note <../../doc/199.html>`_ for the detailed description of the the arguments representing the various parts of a TaQL command. `query` The WHERE part of a TaQL command. `name` The name of the reference table if it is to be made persistent. `sortlist` The ORDERBY part of a TaQL command. It is a single string in which commas have to be used to separate sort keys. `columns` The columns to be selected (projection in data base terms). It is a single string in which commas have to be used to separate column names. Apart from column names, expressions can be given as well. `limit` If > 0, maximum number of rows to be selected. `offset` If > 0, ignore the first N matches. `style` The TaQL syntax style to be used (defaults to Python).", "response": "def query(self, query='', name='', sortlist='', columns='',\n              limit=0, offset=0, style='Python'):\n        \"\"\"Query the table and return the result as a reference table.\n\n        This method queries the table. It forms a\n        `TaQL <../../doc/199.html>`_\n        command from the given arguments and executes it using the\n        :func:`taql` function.\n        The result is returned in a so-called reference table which\n        references the selected columns and rows in the original table.\n        Usually a reference table is temporary, but it can be made\n        persistent by giving it a name.\n        Note that a reference table is handled as any table, thus can be\n        queried again.\n\n        All arguments are optional, but at least one of `query`, `name`,\n        `sortlist`, and `columns` should be used.\n        See the `TaQL note <../../doc/199.html>`_ for the\n        detailed description of the the arguments representing the various\n        parts of a TaQL command.\n\n        `query`\n          The WHERE part of a TaQL command.\n        `name`\n          The name of the reference table if it is to be made persistent.\n        `sortlist`\n          The ORDERBY part of a TaQL command. It is a single string in which\n          commas have to be used to separate sort keys.\n        `columns`\n          The columns to be selected (projection in data base terms). It is a\n          single string in which commas have to be used to separate column\n          names. Apart from column names, expressions can be given as well.\n        `limit`\n          If > 0, maximum number of rows to be selected.\n        `offset`\n          If > 0, ignore the first N matches.\n        `style`\n          The TaQL syntax style to be used (defaults to Python).\n\n        \"\"\"\n        if not query and not sortlist and not columns and \\\n           limit <= 0 and offset <= 0:\n            raise ValueError('No selection done (arguments query, ' +\n                             'sortlist, columns, limit, and offset are empty)')\n        command = 'select '\n        if columns:\n            command += columns\n        command += ' from $1'\n        if query:\n            command += ' where ' + query\n        if sortlist:\n            command += ' orderby ' + sortlist\n        if limit > 0:\n            command += ' limit %d' % limit\n        if offset > 0:\n            command += ' offset %d' % offset\n        if name:\n            command += ' giving ' + name\n        return tablecommand(command, style, [self])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort the table and return the result as a reference table. This method sorts the table. It forms a `TaQL <../../doc/199.html>`_ command from the given arguments and executes it using the :func:`taql` function. The result is returned in a so-called reference table which references the columns and rows in the original table. Usually a reference table is temporary, but it can be made persistent by giving it a name. Note that a reference table is handled as any table, thus can be queried again. `sortlist` The ORDERBY part of a TaQL command. It is a single string in which commas have to be used to separate sort keys. A sort key can be the name of a column, but it can be an expression as well. `name` The name of the reference table if it is to be made persistent. `limit` If > 0, maximum number of rows to be selected after the sort step. It can, for instance, be used to select the N highest values. `offset` If > 0, ignore the first `offset` matches after the sort step. `style` The TaQL syntax style to be used (defaults to Python).", "response": "def sort(self, sortlist, name='',\n             limit=0, offset=0, style='Python'):\n        \"\"\"Sort the table and return the result as a reference table.\n\n        This method sorts the table. It forms a\n        `TaQL <../../doc/199.html>`_\n        command from the given arguments and executes it using the\n        :func:`taql` function.\n        The result is returned in a so-called reference table which references\n        the columns and rows in the original table. Usually a reference\n        table is temporary, but it can be made persistent by giving it a name.\n        Note that a reference table is handled as any table, thus can be\n        queried again.\n\n        `sortlist`\n          The ORDERBY part of a TaQL command. It is a single string in which\n          commas have to be used to separate sort keys. A sort key can be the\n          name of a column, but it can be an expression as well.\n        `name`\n          The name of the reference table if it is to be made persistent.\n        `limit`\n          If > 0, maximum number of rows to be selected after the sort step.\n          It can, for instance, be used to select the N highest values.\n        `offset`\n          If > 0, ignore the first `offset` matches after the sort step.\n        `style`\n          The TaQL syntax style to be used (defaults to Python).\n\n        \"\"\"\n        command = 'select from $1 orderby ' + sortlist\n        if limit > 0:\n            command += ' limit %d' % limit\n        if offset > 0:\n            command += ' offset %d' % offset\n        if name:\n            command += ' giving ' + name\n        return tablecommand(command, style, [self])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(self, columns, name='', style='Python'):\n        command = 'select ' + columns + ' from $1'\n        if name:\n            command += ' giving ' + name\n        return tablecommand(command, style, [self])", "response": "Select columns and return the result as a reference table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef browse(self, wait=True, tempname=\"/tmp/seltable\"):\n        import os\n        # Test if casabrowser can be found.\n        # On OS-X 'which' always returns 0, so use test on top of it.\n        # Nothing is written on stdout if not found.\n        if os.system('test `which casabrowser`x != x') == 0:\n            waitstr1 = \"\"\n            waitstr2 = \"foreground ...\"\n            if not wait:\n                waitstr1 = \" &\"\n                waitstr2 = \"background ...\"\n            if self.iswritable():\n                six.print_(\"Flushing data and starting casabrowser in the \" +\n                           waitstr2)\n            else:\n                six.print_(\"Starting casabrowser in the \" + waitstr2)\n            self.flush()\n            self.unlock()\n            if os.system('test -e ' + self.name() + '/table.dat') == 0:\n                os.system('casabrowser ' + self.name() + waitstr1)\n            elif len(tempname) > 0:\n                six.print_(\"  making a persistent copy in table \" + tempname)\n                self.copy(tempname)\n                os.system('casabrowser ' + tempname + waitstr1)\n                if wait:\n                    from casacore.tables import tabledelete\n                    six.print_(\"  finished browsing\")\n                    tabledelete(tempname)\n\n                else:\n                    six.print_(\" after browsing use tabledelete('\" + tempname +\n                               \"') to delete the copy\")\n            else:\n                six.print_(\"Cannot browse because the table is in memory only\")\n                six.print_(\"You can browse a (shallow) persistent copy \" +\n                           \"of the table like: \")\n                six.print_(\"   t.browse(True, '/tmp/tab1')\")\n        else:\n            try:\n                import wxPython\n            except ImportError:\n                six.print_('casabrowser nor wxPython can be found')\n                return\n            from wxPython.wx import wxPySimpleApp\n            import sys\n            app = wxPySimpleApp()\n            from wxtablebrowser import CasaTestFrame\n            frame = CasaTestFrame(None, sys.stdout, self)\n            frame.Show(True)\n            app.MainLoop()", "response": "Browse a table using casabrowser or simple wxwidget based browser."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nviewing a table using casaviewer casabrowser wxwidget or wxwidget based browser.", "response": "def view(self, wait=True, tempname=\"/tmp/seltable\"):\n        \"\"\" View a table using casaviewer, casabrowser, or wxwidget\n        based browser.\n\n        The table is viewed depending on the type:\n\n        MeasurementSet\n          is viewed using casaviewer.\n        Image\n          is viewed using casaviewer.\n        other\n          are browsed using the :func:`browse` function.\n\n        If the casaviewer cannot be found, all tables are browsed.\n\n        The casaviewer can only display tables that are persistent on disk.\n        This gives problems for tables resulting from a query because they are\n        held in memory only (unless an output table name was given).\n\n        To make viewing of such tables possible, the argument `tempname` can\n        be used to specify a table name that will be used to form a persistent\n        table that can be browsed. Note that such a table is very small as it\n        does not contain data, but only references to rows in the original\n        table. The default for `tempname` is '/tmp/seltable'.\n\n        If needed, the table can be deleted using the :func:`tabledelete`\n        function.\n\n        If `wait=False`, the casaviewer is started in the background.\n        In that case the user should delete a possibly created copy of a\n        temporary table.\n\n        \"\"\"\n        import os\n        # Determine the table type.\n        # Test if casaviewer can be found.\n        # On OS-X 'which' always returns 0, so use test on top of it.\n        viewed = False\n        type = self.info()[\"type\"]\n        if type == \"Measurement Set\" or type == \"Image\":\n            if os.system('test -x `which casaviewer` > /dev/null 2>&1') == 0:\n                waitstr1 = \"\"\n                waitstr2 = \"foreground ...\"\n                if not wait:\n                    waitstr1 = \" &\"\n                    waitstr2 = \"background ...\"\n                if self.iswritable():\n                    six.print_(\"Flushing data and starting casaviewer \" +\n                               \"in the \" + waitstr2)\n                else:\n                    six.print_(\"Starting casaviewer in the \" + waitstr2)\n                self.flush()\n                self.unlock()\n                if os.system('test -e ' + self.name() + '/table.dat') == 0:\n                    os.system('casaviewer ' + self.name() + waitstr1)\n                    viewed = True\n                elif len(tempname) > 0:\n                    six.print_(\"  making a persistent copy in table \" +\n                               tempname)\n                    self.copy(tempname)\n                    os.system('casaviewer ' + tempname + waitstr1)\n                    viewed = True\n                    if wait:\n                        from casacore.tables import tabledelete\n                        six.print_(\"  finished viewing\")\n                        tabledelete(tempname)\n                    else:\n                        six.print_(\"  after viewing use tabledelete('\" +\n                                   tempname + \"') to delete the copy\")\n                else:\n                    six.print_(\"Cannot browse because the table is \" +\n                               \"in memory only.\")\n                    six.print_(\"You can browse a (shallow) persistent \" +\n                               \"copy of the table like:\")\n                    six.print_(\"   t.view(True, '/tmp/tab1')\")\n        # Could not view the table, so browse it.\n        if not viewed:\n            self.browse(wait, tempname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _repr_html_(self):\n        out = \"<table class='taqltable' style='overflow-x:auto'>\\n\"\n\n        # Print column names (not if they are all auto-generated)\n        if not(all([colname[:4] == \"Col_\" for colname in self.colnames()])):\n            out += \"<tr>\"\n            for colname in self.colnames():\n                out += \"<th><b>\"+colname+\"</b></th>\"\n            out += \"</tr>\"\n\n        cropped = False\n        rowcount = 0\n        for row in self:\n            rowout = _format_row(row, self.colnames(), self)\n            rowcount += 1\n            out += rowout\n            if \"\\n\" in rowout:  # Double space after multiline rows\n                out += \"\\n\"\n            out += \"\\n\"\n            if rowcount >= 20:\n                cropped = True\n                break\n\n        if out[-2:] == \"\\n\\n\":\n            out = out[:-1]\n\n        out += \"</table>\"\n\n        if cropped:\n            out += (\"<p style='text-align:center'>(\" +\n                    str(self.nrows()-20)+\" more rows)</p>\\n\")\n\n        return out", "response": "Give a nice representation of tables in notebooks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmixing together two colors.", "response": "def mix(color1, color2, weight=Number(50, \"%\")):\n    \"\"\"\n    Mixes together two colors. Specifically, takes the average of each of the\n    RGB components, optionally weighted by the given percentage.\n    The opacity of the colors is also considered when weighting the components.\n\n    Specifically, takes the average of each of the RGB components,\n    optionally weighted by the given percentage.\n    The opacity of the colors is also considered when weighting the components.\n\n    The weight specifies the amount of the first color that should be included\n    in the returned color.\n    50%, means that half the first color\n        and half the second color should be used.\n    25% means that a quarter of the first color\n        and three quarters of the second color should be used.\n\n    For example:\n\n        mix(#f00, #00f) => #7f007f\n        mix(#f00, #00f, 25%) => #3f00bf\n        mix(rgba(255, 0, 0, 0.5), #00f) => rgba(63, 0, 191, 0.75)\n    \"\"\"\n    # This algorithm factors in both the user-provided weight\n    # and the difference between the alpha values of the two colors\n    # to decide how to perform the weighted average of the two RGB values.\n    #\n    # It works by first normalizing both parameters to be within [-1, 1],\n    # where 1 indicates \"only use color1\", -1 indicates \"only use color 0\",\n    # and all values in between indicated a proportionately weighted average.\n    #\n    # Once we have the normalized variables w and a,\n    # we apply the formula (w + a)/(1 + w*a)\n    # to get the combined weight (in [-1, 1]) of color1.\n    # This formula has two especially nice properties:\n    #\n    #   * When either w or a are -1 or 1, the combined weight is also that\n    #     number (cases where w * a == -1 are undefined, and handled as a\n    #     special case).\n    #\n    #   * When a is 0, the combined weight is w, and vice versa\n    #\n    # Finally, the weight of color1 is renormalized to be within [0, 1]\n    # and the weight of color2 is given by 1 minus the weight of color1.\n    #\n    # Algorithm from the Sass project: http://sass-lang.com/\n\n    p = _interpret_percentage(weight)\n\n    # Scale weight to [-1, 1]\n    w = p * 2 - 1\n    # Compute difference in alpha channels\n    a = color1.alpha - color2.alpha\n\n    # Weight of first color\n    if w * a == -1:\n        # Avoid zero-div case\n        scaled_weight1 = w\n    else:\n        scaled_weight1 = (w + a) / (1 + w * a)\n\n    # Unscale back to [0, 1] and get the weight of the other color\n    w1 = (scaled_weight1 + 1) / 2\n    w2 = 1 - w1\n\n    # Do the scaling.  Note that alpha isn't scaled by alpha, as that wouldn't\n    # make much sense; it uses the original untwiddled weight, p.\n    channels = [\n        ch1 * w1 + ch2 * w2\n        for (ch1, ch2) in zip(color1.rgba[:3], color2.rgba[:3])]\n    alpha = color1.alpha * p + color2.alpha * (1 - p)\n    return Color.from_rgb(*channels, alpha=alpha)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the inverse of a color.", "response": "def invert(color):\n    \"\"\"Returns the inverse (negative) of a color.  The red, green, and blue\n    values are inverted, while the opacity is left alone.\n    \"\"\"\n    if isinstance(color, Number):\n        # invert(n) and invert(n%) are CSS3 filters and should be left\n        # intact\n        return String.unquoted(\"invert(%s)\" % (color.render(),))\n\n    expect_type(color, Color)\n    r, g, b, a = color.rgba\n    return Color.from_rgb(1 - r, 1 - g, 1 - b, alpha=a)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nth(lst, n):\n    expect_type(n, (String, Number), unit=None)\n\n    if isinstance(n, String):\n        if n.value.lower() == 'first':\n            i = 0\n        elif n.value.lower() == 'last':\n            i = -1\n        else:\n            raise ValueError(\"Invalid index %r\" % (n,))\n    else:\n        # DEVIATION: nth treats lists as circular lists\n        i = n.to_python_index(len(lst), circular=True)\n\n    return lst[i]", "response": "Return the nth item in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_import(self, name, compilation, rule):\n        # TODO this is all not terribly well-specified by Sass.  at worst,\n        # it's unclear how far \"upwards\" we should be allowed to go.  but i'm\n        # also a little fuzzy on e.g. how relative imports work from within a\n        # file that's not actually in the search path.\n        # TODO i think with the new origin semantics, i've made it possible to\n        # import relative to the current file even if the current file isn't\n        # anywhere in the search path.  is that right?\n        path = PurePosixPath(name)\n\n        search_exts = list(compilation.compiler.dynamic_extensions)\n        if path.suffix and path.suffix in search_exts:\n            basename = path.stem\n        else:\n            basename = path.name\n        relative_to = path.parent\n        search_path = []  # tuple of (origin, start_from)\n        if relative_to.is_absolute():\n            relative_to = PurePosixPath(*relative_to.parts[1:])\n        elif rule.source_file.origin:\n            # Search relative to the current file first, only if not doing an\n            # absolute import\n            search_path.append((\n                rule.source_file.origin,\n                rule.source_file.relpath.parent / relative_to,\n            ))\n        search_path.extend(\n            (origin, relative_to)\n            for origin in compilation.compiler.search_path\n        )\n\n        for prefix, suffix in product(('_', ''), search_exts):\n            filename = prefix + basename + suffix\n            for origin, relative_to in search_path:\n                relpath = relative_to / filename\n                # Lexically (ignoring symlinks!) eliminate .. from the part\n                # of the path that exists within Sass-space.  pathlib\n                # deliberately doesn't do this, but os.path does.\n                relpath = PurePosixPath(os.path.normpath(str(relpath)))\n\n                if rule.source_file.key == (origin, relpath):\n                    # Avoid self-import\n                    # TODO is this what ruby does?\n                    continue\n\n                path = origin / relpath\n                if not path.exists():\n                    continue\n\n                # All good!\n                # TODO if this file has already been imported, we'll do the\n                # source preparation twice.  make it lazy.\n                return SourceFile.read(origin, relpath)", "response": "Implementation of the core Sass import mechanism which just looks\n        for files on disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a grammar from an input filename and an output filename.", "response": "def generate(inputfilename, outputfilename='', dump=0, **flags):\n    \"\"\"Generate a grammar, given an input filename (X.g)\n    and an output filename (defaulting to X.py).\"\"\"\n\n    if not outputfilename:\n        if inputfilename[-2:] == '.g':\n            outputfilename = inputfilename[:-2] + '.py'\n        else:\n            raise Exception(\"Missing output filename\")\n\n    print 'Input Grammar:', inputfilename\n    print 'Output File:', outputfilename\n\n    DIVIDER = '\\n%%\\n'  # This pattern separates the pre/post parsers\n    preparser, postparser = None, None  # Code before and after the parser desc\n\n    # Read the entire file\n    s = open(inputfilename, 'r').read()\n\n    # See if there's a separation between the pre-parser and parser\n    f = find(s, DIVIDER)\n    if f >= 0:\n        preparser, s = s[:f] + '\\n\\n', s[f + len(DIVIDER):]\n\n    # See if there's a separation between the parser and post-parser\n    f = find(s, DIVIDER)\n    if f >= 0:\n        s, postparser = s[:f], '\\n\\n' + s[f + len(DIVIDER):]\n\n    # Create the parser and scanner\n    p = ParserDescription(ParserDescriptionScanner(s))\n    if not p:\n        return\n\n    # Now parse the file\n    t = wrap_error_reporter(p, 'Parser')\n    if not t:\n        return  # Error\n    if preparser is not None:\n        t.preparser = preparser\n    if postparser is not None:\n        t.postparser = postparser\n\n    # Check the options\n    for f in t.options.keys():\n        for opt, _, _ in yapps_options:\n            if f == opt:\n                break\n        else:\n            print 'Warning: unrecognized option', f\n    # Add command line options to the set\n    for f in flags.keys():\n        t.options[f] = flags[f]\n\n    # Generate the output\n    if dump:\n        t.dump_information()\n    else:\n        t.output = open(outputfilename, 'w')\n        t.generate_output()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsees if a and b have the same elements", "response": "def equal_set(self, a, b):\n        \"See if a and b have the same elements\"\n        if len(a) != len(b):\n            return 0\n        if a == b:\n            return 1\n        return self.subset(a, b) and self.subset(b, a)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_to(self, parent, additions):\n        \"Modify parent to include all elements in additions\"\n        for x in additions:\n            if x not in parent:\n                parent.append(x)\n                self.changed()", "response": "Modify parent to include all elements in additions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _scan(self, type):\n        tok = self._scanner.token(self._pos, frozenset([type]))\n        if tok[2] != type:\n            err = SyntaxError(\"SyntaxError[@ char %s: %s]\" % (repr(tok[0]), \"Trying to find \" + type))\n            err.pos = tok[0]\n            raise err\n        self._pos += 1\n        return tok[3]", "response": "Scan for a specific type of ISO - 8601 tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeclaring a Python function into this Namespace with an explicit name.", "response": "def declare_alias(self, name):\n        \"\"\"Insert a Python function into this Namespace with an\n        explicitly-given name, but detect its argument count automatically.\n        \"\"\"\n        def decorator(f):\n            self._auto_register_function(f, name)\n            return f\n\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlikes declare but will automatically register the function with the current namespace as its first argument.", "response": "def declare_internal(self, function):\n        \"\"\"Like declare(), but the registered function will also receive the\n        current namespace as its first argument.  Useful for functions that\n        inspect the state of the compilation, like ``variable-exists()``.\n        Probably not so useful for anything else.\n        \"\"\"\n        function._pyscss_needs_namespace = True\n        self._auto_register_function(function, function.__name__, 1)\n        return function"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_filename_hash(key):\n    key_repr = repr(key).replace(BASE_DIR, '').encode('utf8')\n    # This is really stupid but necessary for making the repr()s be the same on\n    # Python 2 and 3 and thus allowing the test suite to run on both.\n    # TODO better solutions include: not using a repr, not embedding hashes in\n    # the expected test results\n    if sys.platform == 'win32':\n        # this is to make sure the hash is the same on win and unix platforms\n        key_repr = key_repr.replace(b'\\\\\\\\', b'/')\n    key_repr = re.sub(b\"\\\\bu'\", b\"'\", key_repr)\n    key_hash = hashlib.md5(key_repr).digest()\n    return base64.b64encode(key_hash, b'__').decode('ascii').rstrip('=')", "response": "Convert the given key to a unique -ish hash suitable for a filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear cache of results which have timed out", "response": "def collect(self):\n        \"\"\"Clear cache of results which have timed out\"\"\"\n        for func in self._caches:\n            cache = {}\n            for key in self._caches[func]:\n                if (time.time() - self._caches[func][key][1]) < self._timeouts[func]:\n                    cache[key] = self._caches[func][key]\n            self._caches[func] = cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extend_unique(seq, more):\n    seen = set(seq)\n    new = []\n    for item in more:\n        if item not in seen:\n            seen.add(item)\n            new.append(item)\n\n    return seq + type(seq)(new)", "response": "Return a new sequence containing the items in seq plus any items in more that aren t already in seq."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef file_and_line(self):\n        ret = \"%s:%d\" % (self.source_file.path, self.lineno)\n        if self.from_source_file:\n            ret += \" (%s:%d)\" % (self.from_source_file.path, self.from_lineno)\n        return ret", "response": "Return the filename and line number where this rule originally originally\n        appears in the form foo. scss. 3."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_empty(self):\n        if self.properties:\n            # Rules containing CSS properties are never empty\n            return False\n\n        if not self.descendants:\n            for header in self.ancestry.headers:\n                if header.is_atrule and header.directive != '@media':\n                    # At-rules should always be preserved, UNLESS they are @media\n                    # blocks, which are known to be noise if they don't have any\n                    # contents of their own\n                    return False\n\n        return True", "response": "Return whether this rule is considered empty."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new ancestry that also matches the given selectors.", "response": "def with_more_selectors(self, selectors):\n        \"\"\"Return a new ancestry that also matches the given selectors.  No\n        nesting is done.\n        \"\"\"\n        if self.headers and self.headers[-1].is_selector:\n            new_selectors = extend_unique(\n                self.headers[-1].selectors,\n                selectors)\n            new_headers = self.headers[:-1] + (\n                BlockSelectorHeader(new_selectors),)\n            return RuleAncestry(new_headers)\n        else:\n            new_headers = self.headers + (BlockSelectorHeader(selectors),)\n            return RuleAncestry(new_headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_glob_math(self, cont):\n        # TODO that's a lie!  this should be in the parser for most cases.\n        if not isinstance(cont, six.string_types):\n            warn(FutureWarning(\n                \"do_glob_math was passed a non-string {0!r} \"\n                \"-- this will no longer be supported in pyScss 2.0\"\n                .format(cont)\n            ))\n            cont = six.text_type(cont)\n        if '#{' not in cont:\n            return cont\n        cont = _expr_glob_re.sub(self._pound_substitute, cont)\n        return cont", "response": "Performs glob - math interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a string for interpolations and return an AST node.", "response": "def parse_interpolations(self, string):\n        \"\"\"Parse a string for interpolations, but don't treat anything else as\n        Sass syntax.  Returns an AST node.\n        \"\"\"\n        # Shortcut: if there are no #s in the string in the first place, it\n        # must not have any interpolations, right?\n        if '#' not in string:\n            return Literal(String.unquoted(string))\n        return self.parse_expression(string, 'goal_interpolated_literal')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a string for variables and interpolations but don t treat them as Sass syntax.", "response": "def parse_vars_and_interpolations(self, string):\n        \"\"\"Parse a string for variables and interpolations, but don't treat\n        anything else as Sass syntax.  Returns an AST node.\n        \"\"\"\n        # Shortcut: if there are no #s or $s in the string in the first place,\n        # it must not have anything of interest.\n        if '#' not in string and '$' not in string:\n            return Literal(String.unquoted(string))\n        return self.parse_expression(\n            string, 'goal_interpolated_literal_with_vars')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncompiling the Sass to CSS.", "response": "def compile(\n            self, scss_string=None, scss_file=None, source_files=None,\n            super_selector=None, filename=None, is_sass=None,\n            line_numbers=True, import_static_css=False):\n        \"\"\"Compile Sass to CSS.  Returns a single CSS string.\n\n        This method is DEPRECATED; see :mod:`scss.compiler` instead.\n        \"\"\"\n        # Derive our root namespace\n        self.scss_vars = _default_scss_vars.copy()\n        if self._scss_vars is not None:\n            self.scss_vars.update(self._scss_vars)\n\n        root_namespace = Namespace(\n            variables=self.scss_vars,\n            functions=self._library,\n        )\n\n        # Figure out search paths.  Fall back from provided explicitly to\n        # defined globally to just searching the current directory\n        search_paths = ['.']\n        if self._search_paths is not None:\n            assert not isinstance(self._search_paths, six.string_types), \\\n                \"`search_paths` should be an iterable, not a string\"\n            search_paths.extend(self._search_paths)\n        else:\n            if config.LOAD_PATHS:\n                if isinstance(config.LOAD_PATHS, six.string_types):\n                    # Back-compat: allow comma-delimited\n                    search_paths.extend(config.LOAD_PATHS.split(','))\n                else:\n                    search_paths.extend(config.LOAD_PATHS)\n\n            search_paths.extend(self._scss_opts.get('load_paths', []))\n\n        # Normalize a few old styles of options\n        output_style = self._scss_opts.get('style', config.STYLE)\n        if output_style is True:\n            output_style = 'compressed'\n        elif output_style is False:\n            output_style = 'legacy'\n\n        fixed_search_path = []\n        for path in search_paths:\n            if isinstance(path, six.string_types):\n                fixed_search_path.append(Path(path))\n            else:\n                fixed_search_path.append(path)\n\n        # Build the compiler\n        compiler = Compiler(\n            namespace=root_namespace,\n            extensions=[\n                CoreExtension,\n                ExtraExtension,\n                FontsExtension,\n                CompassExtension,\n                BootstrapExtension,\n            ],\n            search_path=fixed_search_path,\n            import_static_css=import_static_css,\n            live_errors=self.live_errors,\n            generate_source_map=self._scss_opts.get('debug_info', False),\n            output_style=output_style,\n            warn_unused_imports=self._scss_opts.get('warn_unused', False),\n            ignore_parse_errors=config.DEBUG,\n            loops_have_own_scopes=config.CONTROL_SCOPING,\n            undefined_variables_fatal=config.FATAL_UNDEFINED,\n            super_selector=super_selector or self.super_selector,\n        )\n        # Gonna add the source files manually\n        compilation = compiler.make_compilation()\n\n        # Inject the files we know about\n        # TODO how does this work with the expectation of absoluteness\n        if source_files is not None:\n            for source in source_files:\n                compilation.add_source(source)\n        elif scss_string is not None:\n            source = SourceFile.from_string(\n                scss_string,\n                relpath=filename,\n                is_sass=is_sass,\n            )\n            compilation.add_source(source)\n        elif scss_file is not None:\n            # This is now the only way to allow forcibly overriding the\n            # filename a source \"thinks\" it is\n            with open(scss_file, 'rb') as f:\n                source = SourceFile.from_file(\n                    f,\n                    relpath=filename or scss_file,\n                    is_sass=is_sass,\n                )\n            compilation.add_source(source)\n\n        # Plus the ones from the constructor\n        if self._scss_files:\n            for name, contents in list(self._scss_files.items()):\n                source = SourceFile.from_string(contents, relpath=name)\n                compilation.add_source(source)\n\n        compiled = compiler.call_and_catch_errors(compilation.run)\n        self.source_files = list(SourceFileTuple(*os.path.split(s.path)) for s in compilation.source_index.values())\n        return compiled"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn true when the object is false an empty string or an empty list", "response": "def blank(*objs):\n    \"\"\"Returns true when the object is false, an empty string, or an empty list\"\"\"\n    for o in objs:\n        if isinstance(o, Boolean):\n            is_blank = not o\n        elif isinstance(o, String):\n            is_blank = not len(o.value.strip())\n        elif isinstance(o, List):\n            is_blank = all(blank(el) for el in o)\n        else:\n            is_blank = False\n\n        if not is_blank:\n            return Boolean(False)\n\n    return Boolean(True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new list after removing any non - true values", "response": "def compact(*args):\n    \"\"\"Returns a new list after removing any non-true values\"\"\"\n    use_comma = True\n    if len(args) == 1 and isinstance(args[0], List):\n        use_comma = args[0].use_comma\n        args = args[0]\n\n    return List(\n        [arg for arg in args if arg],\n        use_comma=use_comma,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reject(lst, *values):\n    lst = List.from_maybe(lst)\n    values = frozenset(List.from_maybe_starargs(values))\n\n    ret = []\n    for item in lst:\n        if item not in values:\n            ret.append(item)\n    return List(ret, use_comma=lst.use_comma)", "response": "Removes the given values from the list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a url to an asset found relative to the project s font directory.", "response": "def font_url(path, only_path=False, cache_buster=True):\n    \"\"\"\n    Generates a path to an asset found relative to the project's font directory.\n    Passing a true value as the second argument will cause the only the path to\n    be returned instead of a `url()` function\n    \"\"\"\n    return _font_url(path, only_path, cache_buster, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stylesheet_url(path, only_path=False, cache_buster=True):\n    filepath = String.unquoted(path).value\n    if callable(config.STATIC_ROOT):\n        try:\n            _file, _storage = list(config.STATIC_ROOT(filepath))[0]\n        except IndexError:\n            filetime = None\n        else:\n            filetime = getmtime(_file, _storage)\n        if filetime is None:\n            filetime = 'NA'\n    else:\n        _path = os.path.join(config.STATIC_ROOT, filepath.strip('/'))\n        filetime = getmtime(_path)\n        if filetime is None:\n            filetime = 'NA'\n    BASE_URL = config.STATIC_URL\n\n    url = '%s%s' % (BASE_URL, filepath)\n    if cache_buster:\n        url = add_cache_buster(url, filetime)\n    if only_path:\n        return String.unquoted(url)\n    else:\n        return Url.unquoted(url)", "response": "Generates a url to an asset found relative to the project s css directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a caret marking a given position in a string of input.", "response": "def add_error_marker(text, position, start_line=1):\n    \"\"\"Add a caret marking a given position in a string of input.\n\n    Returns (new_text, caret_line).\n    \"\"\"\n    indent = \"    \"\n    lines = []\n    caret_line = start_line\n    for line in text.split(\"\\n\"):\n        lines.append(indent + line)\n\n        if 0 <= position <= len(line):\n            lines.append(indent + (\" \" * position) + \"^\")\n            caret_line = start_line\n\n        position -= len(line)\n        position -= 1  # for the newline\n        start_line += 1\n\n    return \"\\n\".join(lines), caret_line"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_sass_stack(self):\n        if not self.rule_stack:\n            return \"\"\n\n        ret = [\"on \", self.format_file_and_line(self.rule_stack[0]), \"\\n\"]\n        last_file = self.rule_stack[0].source_file\n\n        # TODO this could go away if rules knew their import chains...\n        # TODO this doesn't mention mixins or function calls.  really need to\n        # track the call stack better.  atm we skip other calls in the same\n        # file because most of them are just nesting, but they might not be!\n        # TODO the line number is wrong here for @imports, because we don't\n        # have access to the UnparsedBlock representing the import!\n        # TODO @content is completely broken; it's basically textual inclusion\n        for rule in self.rule_stack[1:]:\n            if rule.source_file is not last_file:\n                ret.extend((\n                    \"imported from \", self.format_file_and_line(rule), \"\\n\"))\n            last_file = rule.source_file\n\n        return \"\".join(ret)", "response": "Return a traceback of Sass imports."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a traceback of the original traceback", "response": "def format_python_stack(self):\n        \"\"\"Return a traceback of Python frames, from where the error occurred\n        to where it was first caught and wrapped.\n        \"\"\"\n        ret = [\"Traceback:\\n\"]\n        ret.extend(traceback.format_tb(self.original_traceback))\n        return \"\".join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_original_error(self):\n        # TODO eventually we'll have sass-specific errors that will want nicer\n        # \"names\" in browser display and stderr\n        return \"\".join((\n            type(self.exc).__name__, \": \", six.text_type(self.exc), \"\\n\",\n        ))", "response": "Return the typical TypeError blah for the original wrapped\n error."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a CSS stylesheet that will show the wrapped error at the top of the browser window.", "response": "def to_css(self):\n        \"\"\"Return a stylesheet that will show the wrapped error at the top of\n        the browser window.\n        \"\"\"\n        # TODO should this include the traceback?  any security concerns?\n        prefix = self.format_prefix()\n        original_error = self.format_original_error()\n        sass_stack = self.format_sass_stack()\n\n        message = prefix + \"\\n\" + sass_stack + original_error\n\n        # Super simple escaping: only quotes and newlines are illegal in css\n        # strings\n        message = message.replace('\\\\', '\\\\\\\\')\n        message = message.replace('\"', '\\\\\"')\n        # use the maximum six digits here so it doesn't eat any following\n        # characters that happen to look like hex\n        message = message.replace('\\n', '\\\\00000A')\n\n        return BROWSER_ERROR_TEMPLATE.format('\"' + message + '\"')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompile a single file into CSS.", "response": "def compile_file(filename, compiler_class=Compiler, **kwargs):\n    \"\"\"Compile a single file (provided as a :class:`pathlib.Path`), and return\n    a string of CSS.\n\n    Keyword arguments are passed along to the underlying `Compiler`.\n\n    Note that the search path is set to the file's containing directory by\n    default, unless you explicitly pass a ``search_path`` kwarg.\n\n    :param filename: Path to the file to compile.\n    :type filename: str, bytes, or :class:`pathlib.Path`\n    \"\"\"\n    filename = Path(filename)\n    if 'search_path' not in kwargs:\n        kwargs['search_path'] = [filename.parent.resolve()]\n\n    compiler = compiler_class(**kwargs)\n    return compiler.compile(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compile_string(string, compiler_class=Compiler, **kwargs):\n    compiler = compiler_class(**kwargs)\n    return compiler.compile_string(string)", "response": "Compile a single string and return a string of CSS."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses out the old xCSS foo extends bar syntax.", "response": "def parse_selectors(self, raw_selectors):\n        \"\"\"\n        Parses out the old xCSS \"foo extends bar\" syntax.\n\n        Returns a 2-tuple: a set of selectors, and a set of extended selectors.\n        \"\"\"\n        # Fix tabs and spaces in selectors\n        raw_selectors = _spaces_re.sub(' ', raw_selectors)\n\n        parts = _xcss_extends_re.split(raw_selectors, 1)  # handle old xCSS extends\n        if len(parts) > 1:\n            unparsed_selectors, unsplit_parents = parts\n            # Multiple `extends` are delimited by `&`\n            unparsed_parents = unsplit_parents.split('&')\n        else:\n            unparsed_selectors, = parts\n            unparsed_parents = ()\n\n        selectors = Selector.parse_many(unparsed_selectors)\n        parents = [Selector.parse_one(parent) for parent in unparsed_parents]\n\n        return selectors, parents"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _at_warn(self, calculator, rule, scope, block):\n        value = calculator.calculate(block.argument)\n        log.warn(repr(value))", "response": "Implements @warn\n        Implements @warn\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _at_print(self, calculator, rule, scope, block):\n        value = calculator.calculate(block.argument)\n        sys.stderr.write(\"%s\\n\" % value)", "response": "Implements @print\n        Implements @print\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle the raw rule.", "response": "def _at_raw(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @raw\n        \"\"\"\n        value = calculator.calculate(block.argument)\n        sys.stderr.write(\"%s\\n\" % repr(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _at_dump_context(self, calculator, rule, scope, block):\n        sys.stderr.write(\"%s\\n\" % repr(rule.namespace._variables))", "response": "Dump the current context to stderr."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndump the functions that are defined in the rule.", "response": "def _at_dump_functions(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @dump_functions\n        \"\"\"\n        sys.stderr.write(\"%s\\n\" % repr(rule.namespace._functions))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndump the list of mixins that are defined in the rule.", "response": "def _at_dump_mixins(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @dump_mixins\n        \"\"\"\n        sys.stderr.write(\"%s\\n\" % repr(rule.namespace._mixins))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndumping the current imports to stderr.", "response": "def _at_dump_imports(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @dump_imports\n        \"\"\"\n        sys.stderr.write(\"%s\\n\" % repr(rule.namespace._imports))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps the options of the current locale.", "response": "def _at_dump_options(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @dump_options\n        \"\"\"\n        sys.stderr.write(\"%s\\n\" % repr(rule.options))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _at_debug(self, calculator, rule, scope, block):\n        setting = block.argument.strip()\n        if setting.lower() in ('1', 'true', 't', 'yes', 'y', 'on'):\n            setting = True\n        elif setting.lower() in ('0', 'false', 'f', 'no', 'n', 'off', 'undefined'):\n            setting = False\n        self.ignore_parse_errors = setting\n        log.info(\"Debug mode is %s\", 'On' if self.ignore_parse_errors else 'Off')", "response": "Defines a debug mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls by the Pdb class when the rule is applied to the block.", "response": "def _at_pdb(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @pdb\n        \"\"\"\n        try:\n            import ipdb as pdb\n        except ImportError:\n            import pdb\n        pdb.set_trace()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _at_return(self, calculator, rule, scope, block):\n        # TODO should assert this only happens within a @function\n        ret = calculator.calculate(block.argument)\n        raise SassReturn(ret)", "response": "Raise an exception if the block is not a return."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimplementing a legacy option - based option.", "response": "def _at_option(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @option\n        \"\"\"\n        # TODO This only actually supports \"style\" (which only really makes\n        # sense as the first thing in a single input file) or \"warn_unused\"\n        # (which only makes sense at file level /at best/).  Explore either\n        # replacing this with a better mechanism or dropping it entirely.\n        # Note also that all rules share the same underlying legacy option\n        # dict, so the rules aren't even lexically scoped like you might think,\n        # and @importing a file can change the compiler!  That seems totally\n        # wrong.\n        for option in block.argument.split(','):\n            key, colon, value = option.partition(':')\n            key = key.strip().lower().replace('-', '_')\n            value = value.strip().lower()\n\n            if value in ('1', 'true', 't', 'yes', 'y', 'on'):\n                value = True\n            elif value in ('0', 'false', 'f', 'no', 'n', 'off', 'undefined'):\n                value = False\n            elif not colon:\n                value = True\n\n            if key == 'compress':\n                warn_deprecated(\n                    rule,\n                    \"The 'compress' @option is deprecated.  \"\n                    \"Please use 'style' instead.\"\n                )\n                key = 'style'\n                value = 'compressed' if value else 'legacy'\n\n            if key in ('short_colors', 'reverse_colors'):\n                warn_deprecated(\n                    rule,\n                    \"The '{0}' @option no longer has any effect.\"\n                    .format(key),\n                )\n                return\n            elif key == 'style':\n                try:\n                    OutputStyle[value]\n                except KeyError:\n                    raise SassError(\"No such output style: {0}\".format(value))\n            elif key in ('warn_unused', 'control_scoping'):\n                # TODO deprecate control_scoping?  or add it to compiler?\n                if not isinstance(value, bool):\n                    raise SassError(\"The '{0}' @option requires a bool, not {1!r}\".format(key, value))\n            else:\n                raise SassError(\"Unknown @option: {0}\".format(key))\n\n            rule.legacy_compiler_options[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _at_function(self, calculator, rule, scope, block):\n        if not block.argument:\n            raise SyntaxError(\"%s requires a function name (%s)\" % (block.directive, rule.file_and_line))\n\n        funct, argspec_node = self._get_funct_def(rule, calculator, block.argument)\n\n        defaults = {}\n        new_params = []\n\n        for var_name, default in argspec_node.iter_def_argspec():\n            new_params.append(var_name)\n            if default is not None:\n                defaults[var_name] = default\n\n        # TODO a function or mixin is re-parsed every time it's called; there's\n        # no AST for anything but expressions  :(\n        mixin = [rule.source_file, block.lineno, block.unparsed_contents, rule.namespace, argspec_node, rule.source_file]\n        if block.directive == '@function':\n            def _call(mixin):\n                def __call(namespace, *args, **kwargs):\n                    source_file = mixin[0]\n                    lineno = mixin[1]\n                    m_codestr = mixin[2]\n                    pristine_callee_namespace = mixin[3]\n                    callee_namespace = pristine_callee_namespace.derive()\n\n                    # TODO CallOp converts Sass names to Python names, so we\n                    # have to convert them back to Sass names.  would be nice\n                    # to avoid this back-and-forth somehow\n                    kwargs = OrderedDict(\n                        (normalize_var('$' + key), value)\n                        for (key, value) in kwargs.items())\n\n                    self._populate_namespace_from_call(\n                        \"Function {0}\".format(funct),\n                        callee_namespace, mixin, args, kwargs)\n\n                    _rule = SassRule(\n                        source_file=source_file,\n                        lineno=lineno,\n                        unparsed_contents=m_codestr,\n                        namespace=callee_namespace,\n\n                        # rule\n                        import_key=rule.import_key,\n                        legacy_compiler_options=rule.legacy_compiler_options,\n                        options=rule.options,\n                        properties=rule.properties,\n                        extends_selectors=rule.extends_selectors,\n                        ancestry=rule.ancestry,\n                        nested=rule.nested,\n                    )\n                    # TODO supposed to throw an error if there's a slurpy arg\n                    # but keywords() is never called on it\n                    try:\n                        self.manage_children(_rule, scope)\n                    except SassReturn as e:\n                        return e.retval\n                    else:\n                        return Null()\n                __call._pyscss_needs_namespace = True\n                return __call\n            _mixin = _call(mixin)\n            _mixin.mixin = mixin\n            mixin = _mixin\n\n        if block.directive == '@mixin':\n            add = rule.namespace.set_mixin\n        elif block.directive == '@function':\n            add = rule.namespace.set_function\n\n        # Register the mixin for every possible arity it takes\n        if argspec_node.slurp or argspec_node.inject:\n            add(funct, None, mixin)\n        else:\n            while len(new_params):\n                add(funct, len(new_params), mixin)\n                param = new_params.pop()\n                if param not in defaults:\n                    break\n            if not new_params:\n                add(funct, 0, mixin)", "response": "This function is called by _at_function_or_function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimplement the include rule for the current rule.", "response": "def _at_include(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @include, for @mixins\n        \"\"\"\n        caller_namespace = rule.namespace\n        caller_calculator = self._make_calculator(caller_namespace)\n        funct, caller_argspec = self._get_funct_def(rule, caller_calculator, block.argument)\n\n        # Render the passed arguments, using the caller's namespace\n        args, kwargs = caller_argspec.evaluate_call_args(caller_calculator)\n\n        argc = len(args) + len(kwargs)\n        try:\n            mixin = caller_namespace.mixin(funct, argc)\n        except KeyError:\n            try:\n                # TODO maybe? don't do this, once '...' works\n                # Fallback to single parameter:\n                mixin = caller_namespace.mixin(funct, 1)\n            except KeyError:\n                log.error(\"Mixin not found: %s:%d (%s)\", funct, argc, rule.file_and_line, extra={'stack': True})\n                return\n            else:\n                args = [List(args, use_comma=True)]\n                # TODO what happens to kwargs?\n\n        source_file = mixin[0]\n        lineno = mixin[1]\n        m_codestr = mixin[2]\n        pristine_callee_namespace = mixin[3]\n        callee_argspec = mixin[4]\n        if caller_argspec.inject and callee_argspec.inject:\n            # DEVIATION: Pass the ENTIRE local namespace to the mixin (yikes)\n            callee_namespace = Namespace.derive_from(\n                caller_namespace,\n                pristine_callee_namespace)\n        else:\n            callee_namespace = pristine_callee_namespace.derive()\n\n        self._populate_namespace_from_call(\n            \"Mixin {0}\".format(funct),\n            callee_namespace, mixin, args, kwargs)\n\n        _rule = SassRule(\n            # These must be file and line in which the @include occurs\n            source_file=rule.source_file,\n            lineno=rule.lineno,\n\n            # These must be file and line in which the @mixin was defined\n            from_source_file=source_file,\n            from_lineno=lineno,\n\n            unparsed_contents=m_codestr,\n            namespace=callee_namespace,\n\n            # rule\n            import_key=rule.import_key,\n            legacy_compiler_options=rule.legacy_compiler_options,\n            options=rule.options,\n            properties=rule.properties,\n            extends_selectors=rule.extends_selectors,\n            ancestry=rule.ancestry,\n            nested=rule.nested,\n        )\n\n        _rule.options['@content'] = block.unparsed_contents\n        self.manage_children(_rule, scope)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle the content string parsing.", "response": "def _at_content(self, calculator, rule, scope, block):\n        \"\"\"\n        Implements @content\n        \"\"\"\n        if '@content' not in rule.options:\n            log.error(\"Content string not found for @content (%s)\", rule.file_and_line)\n        rule.unparsed_contents = rule.options.pop('@content', '')\n        self.manage_children(rule, scope)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _at_import(self, calculator, rule, scope, block):\n        # TODO it would be neat to opt into warning that you're using\n        # values/functions from a file you didn't explicitly import\n        # TODO base-level directives, like @mixin or @charset, aren't allowed\n        # to be @imported into a nested block\n        # TODO i'm not sure we disallow them nested in the first place\n        # TODO @import is disallowed within mixins, control directives\n        # TODO @import doesn't take a block -- that's probably an issue with a\n        # lot of our directives\n\n        # TODO if there's any #{}-interpolation in the AST, this should become\n        # a CSS import (though in practice Ruby only even evaluates it in url()\n        # -- in a string it's literal!)\n\n        sass_paths = calculator.evaluate_expression(block.argument)\n        css_imports = []\n\n        for sass_path in sass_paths:\n            # These are the rules for when an @import is interpreted as a CSS\n            # import:\n            if (\n                    # If it's a url()\n                    isinstance(sass_path, Url) or\n                    # If it's not a string (including `\"foo\" screen`, a List)\n                    not isinstance(sass_path, String) or\n                    # If the filename begins with an http protocol\n                    sass_path.value.startswith(('http://', 'https://')) or\n                    # If the filename ends with .css\n                    sass_path.value.endswith(self.compiler.static_extensions)):\n                css_imports.append(sass_path.render(compress=False))\n                continue\n\n            # Should be left with a plain String\n            name = sass_path.value\n\n            source = None\n            for extension in self.compiler.extensions:\n                source = extension.handle_import(name, self, rule)\n                if source:\n                    break\n            else:\n                # Didn't find anything!\n                raise SassImportError(name, self.compiler, rule=rule)\n\n            source = self.add_source(source)\n\n            if rule.namespace.has_import(source):\n                # If already imported in this scope, skip\n                # TODO this might not be right -- consider if you @import a\n                # file at top level, then @import it inside a selector block!\n                continue\n\n            _rule = SassRule(\n                source_file=source,\n                lineno=block.lineno,\n                unparsed_contents=source.contents,\n\n                # rule\n                legacy_compiler_options=rule.legacy_compiler_options,\n                options=rule.options,\n                properties=rule.properties,\n                extends_selectors=rule.extends_selectors,\n                ancestry=rule.ancestry,\n                namespace=rule.namespace,\n            )\n            rule.namespace.add_import(source, rule)\n            self.manage_children(_rule, scope)\n\n        # Create a new @import rule for each import determined to be CSS\n        for import_ in css_imports:\n            # TODO this seems extremely janky (surely we should create an\n            # actual new Rule), but the CSS rendering doesn't understand how to\n            # print rules without blocks\n            # TODO if this ever creates a new Rule, shuffle stuff around so\n            # this is still hoisted to the top\n            rule.properties.append(('@import ' + import_, None))", "response": "Handles CSS imports and imports."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _at_if(self, calculator, rule, scope, block):\n        # \"@if\" indicates whether any kind of `if` since the last `@else` has\n        # succeeded, in which case `@else if` should be skipped\n        if block.directive != '@if':\n            if '@if' not in rule.options:\n                raise SyntaxError(\"@else with no @if (%s)\" % (rule.file_and_line,))\n            if rule.options['@if']:\n                # Last @if succeeded; stop here\n                return\n\n        condition = calculator.calculate(block.argument)\n        if condition:\n            inner_rule = rule.copy()\n            inner_rule.unparsed_contents = block.unparsed_contents\n            if not self.should_scope_loop_in_rule(inner_rule):\n                # DEVIATION: Allow not creating a new namespace\n                inner_rule.namespace = rule.namespace\n            self.manage_children(inner_rule, scope)\n        rule.options['@if'] = condition", "response": "Handles the case where a new entry is found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _at_else(self, calculator, rule, scope, block):\n        if '@if' not in rule.options:\n            log.error(\"@else with no @if (%s)\", rule.file_and_line)\n        val = rule.options.pop('@if', True)\n        if not val:\n            inner_rule = rule.copy()\n            inner_rule.unparsed_contents = block.unparsed_contents\n            inner_rule.namespace = rule.namespace  # DEVIATION: Commenting this line gives the Sass bahavior\n            inner_rule.unparsed_contents = block.unparsed_contents\n            self.manage_children(inner_rule, scope)", "response": "Handles the case where the else statement is at the current scope."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _at_for(self, calculator, rule, scope, block):\n        var, _, name = block.argument.partition(' from ')\n        frm, _, through = name.partition(' through ')\n        if through:\n            inclusive = True\n        else:\n            inclusive = False\n            frm, _, through = frm.partition(' to ')\n        frm = calculator.calculate(frm)\n        through = calculator.calculate(through)\n        try:\n            frm = int(float(frm))\n            through = int(float(through))\n        except ValueError:\n            return\n\n        if frm > through:\n            # DEVIATION: allow reversed '@for .. from .. through' (same as enumerate() and range())\n            frm, through = through, frm\n            rev = reversed\n        else:\n            rev = lambda x: x\n        var = var.strip()\n        var = calculator.do_glob_math(var)\n        var = normalize_var(var)\n\n        inner_rule = rule.copy()\n        inner_rule.unparsed_contents = block.unparsed_contents\n        if not self.should_scope_loop_in_rule(inner_rule):\n            # DEVIATION: Allow not creating a new namespace\n            inner_rule.namespace = rule.namespace\n\n        if inclusive:\n            through += 1\n        for i in rev(range(frm, through)):\n            inner_rule.namespace.set_variable(var, Number(i))\n            self.manage_children(inner_rule, scope)", "response": "Implements @for and @for_until"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _at_variables(self, calculator, rule, scope, block):\n        warn_deprecated(\n            rule,\n            \"@variables and @vars are deprecated.  \"\n            \"Just assign variables at top-level.\")\n        _rule = rule.copy()\n        _rule.unparsed_contents = block.unparsed_contents\n        _rule.namespace = rule.namespace\n        _rule.properties = []\n        self.manage_children(_rule, scope)", "response": "Assign variables at top - level."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_properties(self, rule, scope, block):\n        prop, raw_value = (_prop_split_re.split(block.prop, 1) + [None])[:2]\n        if raw_value is not None:\n            raw_value = raw_value.strip()\n\n        try:\n            is_var = (block.prop[len(prop)] == '=')\n        except IndexError:\n            is_var = False\n        if is_var:\n            warn_deprecated(rule, \"Assignment with = is deprecated; use : instead.\")\n        calculator = self._make_calculator(rule.namespace)\n        prop = prop.strip()\n        prop = calculator.do_glob_math(prop)\n        if not prop:\n            return\n\n        _prop = (scope or '') + prop\n        if is_var or prop.startswith('$') and raw_value is not None:\n            # Pop off any flags: !default, !global\n            is_default = False\n            is_global = True  # eventually sass will default this to false\n            while True:\n                splits = raw_value.rsplit(None, 1)\n                if len(splits) < 2 or not splits[1].startswith('!'):\n                    break\n\n                raw_value, flag = splits\n                if flag == '!default':\n                    is_default = True\n                elif flag == '!global':\n                    is_global = True\n                else:\n                    raise ValueError(\"Unrecognized flag: {0}\".format(flag))\n\n            # Variable assignment\n            _prop = normalize_var(_prop)\n            try:\n                existing_value = rule.namespace.variable(_prop)\n            except KeyError:\n                existing_value = None\n\n            is_defined = existing_value is not None and not existing_value.is_null\n            if is_default and is_defined:\n                pass\n            else:\n                if is_defined and prop.startswith('$') and prop[1].isupper():\n                    log.warn(\"Constant %r redefined\", prop)\n\n                # Variable assignment is an expression, so it always performs\n                # real division\n                value = calculator.calculate(raw_value, divide=True)\n                rule.namespace.set_variable(\n                    _prop, value, local_only=not is_global)\n        else:\n            # Regular property destined for output\n            _prop = calculator.apply_vars(_prop)\n            if raw_value is None:\n                value = None\n            else:\n                value = calculator.calculate(raw_value)\n\n            if value is None:\n                pass\n            elif isinstance(value, six.string_types):\n                # TODO kill this branch\n                pass\n            else:\n                if value.is_null:\n                    return\n                style = rule.legacy_compiler_options.get(\n                    'style', self.compiler.output_style)\n                compress = style == 'compressed'\n                value = value.render(compress=compress)\n\n            rule.properties.append((_prop, value))", "response": "Implements properties and variables extraction and assignment of a resource entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nnesting the rules in the current scope.", "response": "def _nest_at_rules(self, rule, scope, block):\n        \"\"\"\n        Implements @-blocks\n        \"\"\"\n        # TODO handle @charset, probably?\n        # Interpolate the current block\n        # TODO this seems like it should be done in the block header.  and more\n        # generally?\n        calculator = self._make_calculator(rule.namespace)\n        if block.header.argument:\n            # TODO is this correct?  do ALL at-rules ALWAYS allow both vars and\n            # interpolation?\n            node = calculator.parse_vars_and_interpolations(\n                block.header.argument)\n            block.header.argument = node.evaluate(calculator).render()\n\n        # TODO merge into RuleAncestry\n        new_ancestry = list(rule.ancestry.headers)\n        if block.directive == '@media' and new_ancestry:\n            for i, header in reversed(list(enumerate(new_ancestry))):\n                if header.is_selector:\n                    continue\n                elif header.directive == '@media':\n                    new_ancestry[i] = BlockAtRuleHeader(\n                        '@media',\n                        \"%s and %s\" % (header.argument, block.argument))\n                    break\n                else:\n                    new_ancestry.insert(i, block.header)\n            else:\n                new_ancestry.insert(0, block.header)\n        else:\n            new_ancestry.append(block.header)\n\n        rule.descendants += 1\n        new_rule = SassRule(\n            source_file=rule.source_file,\n            import_key=rule.import_key,\n            lineno=block.lineno,\n            num_header_lines=block.header.num_lines,\n            unparsed_contents=block.unparsed_contents,\n\n            legacy_compiler_options=rule.legacy_compiler_options,\n            options=rule.options.copy(),\n            #properties\n            #extends_selectors\n            ancestry=RuleAncestry(new_ancestry),\n\n            namespace=rule.namespace.derive(),\n            nested=rule.nested + 1,\n        )\n        self.rules.append(new_rule)\n        rule.namespace.use_import(rule.source_file)\n        self.manage_children(new_rule, scope)\n\n        self._warn_unused_imports(new_rule)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimplements Nested CSS rules", "response": "def _nest_rules(self, rule, scope, block):\n        \"\"\"\n        Implements Nested CSS rules\n        \"\"\"\n        calculator = self._make_calculator(rule.namespace)\n        raw_selectors = calculator.do_glob_math(block.prop)\n        # DEVIATION: ruby sass doesn't support bare variables in selectors\n        raw_selectors = calculator.apply_vars(raw_selectors)\n        c_selectors, c_parents = self.parse_selectors(raw_selectors)\n        if c_parents:\n            warn_deprecated(\n                rule,\n                \"The XCSS 'a extends b' syntax is deprecated.  \"\n                \"Use 'a { @extend b; }' instead.\"\n            )\n\n        new_ancestry = rule.ancestry.with_nested_selectors(c_selectors)\n\n        rule.descendants += 1\n        new_rule = SassRule(\n            source_file=rule.source_file,\n            import_key=rule.import_key,\n            lineno=block.lineno,\n            num_header_lines=block.header.num_lines,\n            unparsed_contents=block.unparsed_contents,\n\n            legacy_compiler_options=rule.legacy_compiler_options,\n            options=rule.options.copy(),\n            #properties\n            extends_selectors=c_parents,\n            ancestry=new_ancestry,\n\n            namespace=rule.namespace.derive(),\n            nested=rule.nested + 1,\n        )\n        self.rules.append(new_rule)\n        rule.namespace.use_import(rule.source_file)\n        self.manage_children(new_rule, scope)\n\n        self._warn_unused_imports(new_rule)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_extends(self, rules):\n        # Game plan: for each rule that has an @extend, add its selectors to\n        # every rule that matches that @extend.\n        # First, rig a way to find arbitrary selectors quickly.  Most selectors\n        # revolve around elements, classes, and IDs, so parse those out and use\n        # them as a rough key.  Ignore order and duplication for now.\n        key_to_selectors = defaultdict(set)\n        selector_to_rules = defaultdict(set)\n        rule_selector_order = {}\n        order = 0\n        for rule in rules:\n            for selector in rule.selectors:\n                for key in selector.lookup_key():\n                    key_to_selectors[key].add(selector)\n                selector_to_rules[selector].add(rule)\n                rule_selector_order[rule, selector] = order\n                order += 1\n\n        # Now go through all the rules with an @extends and find their parent\n        # rules.\n        for rule in rules:\n            for selector in rule.extends_selectors:\n                # This is a little dirty.  intersection isn't a class method.\n                # Don't think about it too much.\n                candidates = set.intersection(*(\n                    key_to_selectors[key] for key in selector.lookup_key()))\n                extendable_selectors = [\n                    candidate for candidate in candidates\n                    if candidate.is_superset_of(selector)]\n\n                if not extendable_selectors:\n                    # TODO implement !optional\n                    warn_deprecated(\n                        rule,\n                        \"Can't find any matching rules to extend {0!r} -- this \"\n                        \"will be fatal in 2.0, unless !optional is specified!\"\n                        .format(selector.render()))\n                    continue\n\n                # Armed with a set of selectors that this rule can extend, do\n                # some substitution and modify the appropriate parent rules.\n                # One tricky bit: it's possible we're extending two selectors\n                # that both exist in the same parent rule, in which case we\n                # want to extend in the order the original selectors appear in\n                # that rule.\n                known_parents = []\n                for extendable_selector in extendable_selectors:\n                    parent_rules = selector_to_rules[extendable_selector]\n                    for parent_rule in parent_rules:\n                        if parent_rule is rule:\n                            # Don't extend oneself\n                            continue\n                        known_parents.append(\n                            (parent_rule, extendable_selector))\n                # This will put our parents back in their original order\n                known_parents.sort(key=rule_selector_order.__getitem__)\n\n                for parent_rule, extendable_selector in known_parents:\n                    more_parent_selectors = []\n\n                    for rule_selector in rule.selectors:\n                        more_parent_selectors.extend(\n                            extendable_selector.substitute(\n                                selector, rule_selector))\n\n                    for parent in more_parent_selectors:\n                        # Update indices, in case later rules try to extend\n                        # this one\n                        for key in parent.lookup_key():\n                            key_to_selectors[key].add(parent)\n                        selector_to_rules[parent].add(parent_rule)\n                        rule_selector_order[parent_rule, parent] = order\n                        order += 1\n\n                    parent_rule.ancestry = (\n                        parent_rule.ancestry.with_more_selectors(\n                            more_parent_selectors))\n\n        # Remove placeholder-only rules\n        return [rule for rule in rules if not rule.is_pure_placeholder]", "response": "Given a list of rules and a set of pending rules and a set of pending rules and a set of rules that can be extended by the current rule."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_css(self, rules):\n        style = rules[0].legacy_compiler_options.get(\n            'style', self.compiler.output_style)\n        debug_info = self.compiler.generate_source_map\n\n        if style == 'legacy':\n            sc, sp, tb, nst, srnl, nl, rnl, lnl, dbg = True, ' ', '  ', False, '', '\\n', '\\n', '\\n', debug_info\n        elif style == 'compressed':\n            sc, sp, tb, nst, srnl, nl, rnl, lnl, dbg = False, '', '', False, '', '', '', '', False\n        elif style == 'compact':\n            sc, sp, tb, nst, srnl, nl, rnl, lnl, dbg = True, ' ', '', False, '\\n', ' ', '\\n', ' ', debug_info\n        elif style == 'expanded':\n            sc, sp, tb, nst, srnl, nl, rnl, lnl, dbg = True, ' ', '  ', False, '\\n', '\\n', '\\n', '\\n', debug_info\n        else:  # if style == 'nested':\n            sc, sp, tb, nst, srnl, nl, rnl, lnl, dbg = True, ' ', '  ', True, '\\n', '\\n', '\\n', ' ', debug_info\n\n        return self._create_css(rules, sc, sp, tb, nst, srnl, nl, rnl, lnl, dbg)", "response": "Generate the final CSS string based on the rules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _constrain(value, lb=0, ub=1):\n    if value < lb:\n        return lb\n    elif value > ub:\n        return ub\n    else:\n        return value", "response": "Helper for Color constructors. Constrains a value to a range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_sub(self, other, op):\n        if not isinstance(other, Number):\n            return NotImplemented\n\n        # If either side is unitless, inherit the other side's units.  Skip all\n        # the rest of the conversion math, too.\n        if self.is_unitless or other.is_unitless:\n            return Number(\n                op(self.value, other.value),\n                unit_numer=self.unit_numer or other.unit_numer,\n                unit_denom=self.unit_denom or other.unit_denom,\n            )\n\n        # Likewise, if either side is zero, it can auto-cast to any units\n        if self.value == 0:\n            return Number(\n                op(self.value, other.value),\n                unit_numer=other.unit_numer,\n                unit_denom=other.unit_denom,\n            )\n        elif other.value == 0:\n            return Number(\n                op(self.value, other.value),\n                unit_numer=self.unit_numer,\n                unit_denom=self.unit_denom,\n            )\n\n        # Reduce both operands to the same units\n        left = self.to_base_units()\n        right = other.to_base_units()\n\n        if left.unit_numer != right.unit_numer or left.unit_denom != right.unit_denom:\n            raise ValueError(\"Can't reconcile units: %r and %r\" % (self, other))\n\n        new_amount = op(left.value, right.value)\n\n        # Convert back to the left side's units\n        if left.value != 0:\n            new_amount = new_amount * self.value / left.value\n\n        return Number(new_amount, unit_numer=self.unit_numer, unit_denom=self.unit_denom)", "response": "Implements both addition and subtraction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the value of the current object to a fixed set of base units.", "response": "def to_base_units(self):\n        \"\"\"Convert to a fixed set of \"base\" units.  The particular units are\n        arbitrary; what's important is that they're consistent.\n\n        Used for addition and comparisons.\n        \"\"\"\n        # Convert to \"standard\" units, as defined by the conversions dict above\n        amount = self.value\n\n        numer_factor, numer_units = convert_units_to_base_units(self.unit_numer)\n        denom_factor, denom_units = convert_units_to_base_units(self.unit_denom)\n\n        return Number(\n            amount * numer_factor / denom_factor,\n            unit_numer=numer_units,\n            unit_denom=denom_units,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wrap_python_function(cls, fn):\n        def wrapped(sass_arg):\n            # TODO enforce no units for trig?\n            python_arg = sass_arg.value\n            python_ret = fn(python_arg)\n            sass_ret = cls(\n                python_ret,\n                unit_numer=sass_arg.unit_numer,\n                unit_denom=sass_arg.unit_denom)\n            return sass_ret\n\n        return wrapped", "response": "Wraps a Python math function that translates the argument from\n            Sass to Python on the way in and vice versa for the return value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a plain Python integer appropriate for indexing a sequence of .", "response": "def to_python_index(self, length, check_bounds=True, circular=False):\n        \"\"\"Return a plain Python integer appropriate for indexing a sequence of\n        the given length.  Raise if this is impossible for any reason\n        whatsoever.\n        \"\"\"\n        if not self.is_unitless:\n            raise ValueError(\"Index cannot have units: {0!r}\".format(self))\n\n        ret = int(self.value)\n        if ret != self.value:\n            raise ValueError(\"Index must be an integer: {0!r}\".format(ret))\n\n        if ret == 0:\n            raise ValueError(\"Index cannot be zero\")\n\n        if check_bounds and not circular and abs(ret) > length:\n            raise ValueError(\"Index {0!r} out of bounds for length {1}\".format(ret, length))\n\n        if ret > 0:\n            ret -= 1\n\n        if circular:\n            ret = ret % length\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_simple_unit(self, unit):\n        if self.unit_denom or len(self.unit_numer) > 1:\n            return False\n\n        if not self.unit_numer:\n            # Empty string historically means no unit\n            return unit == ''\n\n        return self.unit_numer[0] == unit", "response": "Return True iff the unit is simple and matches the given unit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maybe_new(cls, values, use_comma=True):\n        if len(values) == 1:\n            return values[0]\n        else:\n            return cls(values, use_comma=use_comma)", "response": "Returns a new instance of the class from the given list of values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new instance of the class cls with the given list of arguments.", "response": "def from_maybe_starargs(cls, args, use_comma=True):\n        \"\"\"If `args` has one element which appears to be a list, return it.\n        Otherwise, return a list as normal.\n\n        Mainly used by Sass function implementations that predate `...`\n        support, so they can accept both a list of arguments and a single list\n        stored in a variable.\n        \"\"\"\n        if len(args) == 1:\n            if isinstance(args[0], cls):\n                return args[0]\n            elif isinstance(args[0], (list, tuple)):\n                return cls(args[0], use_comma=use_comma)\n\n        return cls(args, use_comma=use_comma)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_name(cls, name):\n        self = cls.__new__(cls)  # TODO\n        self.original_literal = name\n\n        r, g, b, a = COLOR_NAMES[name]\n\n        self.value = r, g, b, a\n        return self", "response": "Build a Color from a CSS color name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a rendered representation of the color.", "response": "def render(self, compress=False):\n        \"\"\"Return a rendered representation of the color.  If `compress` is\n        true, the shortest possible representation is used; otherwise, named\n        colors are rendered as names and all others are rendered as hex (or\n        with the rgba function).\n        \"\"\"\n\n        if not compress and self.original_literal:\n            return self.original_literal\n\n        candidates = []\n\n        # TODO this assumes CSS resolution is 8-bit per channel, but so does\n        # Ruby.\n        r, g, b, a = self.value\n        r, g, b = int(round(r)), int(round(g)), int(round(b))\n\n        # Build a candidate list in order of preference.  If `compress` is\n        # True, the shortest candidate is used; otherwise, the first candidate\n        # is used.\n\n        # Try color name\n        key = r, g, b, a\n        if key in COLOR_LOOKUP:\n            candidates.append(COLOR_LOOKUP[key])\n\n        if a == 1:\n            # Hex is always shorter than function notation\n            if all(ch % 17 == 0 for ch in (r, g, b)):\n                candidates.append(\"#%1x%1x%1x\" % (r // 17, g // 17, b // 17))\n            else:\n                candidates.append(\"#%02x%02x%02x\" % (r, g, b))\n        else:\n            # Can't use hex notation for RGBA\n            if compress:\n                sp = ''\n            else:\n                sp = ' '\n            candidates.append(\"rgba(%d,%s%d,%s%d,%s%.6g)\" % (r, sp, g, sp, b, sp, a))\n\n        if compress:\n            return min(candidates, key=len)\n        else:\n            return candidates[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unquoted(cls, value, literal=False):\n        return cls(value, quotes=None, literal=literal)", "response": "Helper to create a string with no quotes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_combinator_subset_of(specific, general, is_first=True):\n    if is_first and general == ' ':\n        # First selector always has a space to mean \"descendent of root\", which\n        # still holds if any other selector appears above it\n        return True\n\n    if specific == general:\n        return True\n\n    if specific == '>' and general == ' ':\n        return True\n\n    if specific == '+' and general == '~':\n        return True\n\n    return False", "response": "Return whether specific matches a non - strict subset of what general matches."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_selectors(left, right):\n\n    if not left or not right:\n        # At least one is empty, so there are no conflicts; just return\n        # whichever isn't empty.  Remember to return a LIST, though\n        return [left or right]\n\n    lcs = longest_common_subsequence(left, right, _merge_simple_selectors)\n\n    ret = [()]  # start with a dummy empty chain or weaving won't work\n\n    left_last = 0\n    right_last = 0\n    for left_next, right_next, merged in lcs:\n        ret = _weave_conflicting_selectors(\n            ret,\n            left[left_last:left_next],\n            right[right_last:right_next],\n            (merged,))\n\n        left_last = left_next + 1\n        right_last = right_next + 1\n\n    ret = _weave_conflicting_selectors(\n        ret,\n        left[left_last:],\n        right[right_last:])\n\n    return ret", "response": "Given two selector chains representing elements in both lists of simple selectors return a list of\n    containing the elements in left and right."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npart of the selector merge algorithm above. Not useful on its own. Pay no attention to the man behind the curtain.", "response": "def _weave_conflicting_selectors(prefixes, a, b, suffix=()):\n    \"\"\"Part of the selector merge algorithm above.  Not useful on its own.  Pay\n    no attention to the man behind the curtain.\n    \"\"\"\n    # OK, what this actually does: given a list of selector chains, two\n    # \"conflicting\" selector chains, and an optional suffix, return a new list\n    # of chains like this:\n    #   prefix[0] + a + b + suffix,\n    #   prefix[0] + b + a + suffix,\n    #   prefix[1] + a + b + suffix,\n    #   ...\n    # In other words, this just appends a new chain to each of a list of given\n    # chains, except that the new chain might be the superposition of two\n    # other incompatible chains.\n    both = a and b\n    for prefix in prefixes:\n        yield prefix + a + b + suffix\n        if both:\n            # Only use both orderings if there's an actual conflict!\n            yield prefix + b + a + suffix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_simple_selectors(a, b):\n    # TODO what about combinators\n    if a.is_superset_of(b):\n        return b\n    elif b.is_superset_of(a):\n        return a\n    else:\n        return None", "response": "Merge two simple selectors into one."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the longest common subsequence between two iterables.", "response": "def longest_common_subsequence(a, b, mergefunc=None):\n    \"\"\"Find the longest common subsequence between two iterables.\n\n    The longest common subsequence is the core of any diff algorithm: it's the\n    longest sequence of elements that appears in both parent sequences in the\n    same order, but NOT necessarily consecutively.\n\n    Original algorithm borrowed from Wikipedia:\n    http://en.wikipedia.org/wiki/Longest_common_subsequence_problem#Code_for_the_dynamic_programming_solution\n\n    This function is used only to implement @extend, largely because that's\n    what the Ruby implementation does.  Thus it's been extended slightly from\n    the simple diff-friendly algorithm given above.\n\n    What @extend wants to know is whether two simple selectors are compatible,\n    not just equal.  To that end, you must pass in a \"merge\" function to\n    compare a pair of elements manually.  It should return `None` if they are\n    incompatible, and a MERGED element if they are compatible -- in the case of\n    selectors, this is whichever one is more specific.\n\n    Because of this fuzzier notion of equality, the return value is a list of\n    ``(a_index, b_index, value)`` tuples rather than items alone.\n    \"\"\"\n    if mergefunc is None:\n        # Stupid default, just in case\n        def mergefunc(a, b):\n            if a == b:\n                return a\n            return None\n\n    # Precalculate equality, since it can be a tad expensive and every pair is\n    # compared at least once\n    eq = {}\n    for ai, aval in enumerate(a):\n        for bi, bval in enumerate(b):\n            eq[ai, bi] = mergefunc(aval, bval)\n\n    # Build the \"length\" matrix, which provides the length of the LCS for\n    # arbitrary-length prefixes.  -1 exists only to support the base case\n    prefix_lcs_length = {}\n    for ai in range(-1, len(a)):\n        for bi in range(-1, len(b)):\n            if ai == -1 or bi == -1:\n                l = 0\n            elif eq[ai, bi]:\n                l = prefix_lcs_length[ai - 1, bi - 1] + 1\n            else:\n                l = max(\n                    prefix_lcs_length[ai, bi - 1],\n                    prefix_lcs_length[ai - 1, bi])\n\n            prefix_lcs_length[ai, bi] = l\n\n    # The interesting part.  The key insight is that the bottom-right value in\n    # the length matrix must be the length of the LCS because of how the matrix\n    # is defined, so all that's left to do is backtrack from the ends of both\n    # sequences in whatever way keeps the LCS as long as possible, and keep\n    # track of the equal pairs of elements we see along the way.\n    # Wikipedia does this with recursion, but the algorithm is trivial to\n    # rewrite as a loop, as below.\n    ai = len(a) - 1\n    bi = len(b) - 1\n\n    ret = []\n    while ai >= 0 and bi >= 0:\n        merged = eq[ai, bi]\n        if merged is not None:\n            ret.append((ai, bi, merged))\n            ai -= 1\n            bi -= 1\n        elif prefix_lcs_length[ai, bi - 1] > prefix_lcs_length[ai - 1, bi]:\n            bi -= 1\n        else:\n            ai -= 1\n\n    # ret has the latest items first, which is backwards\n    ret.reverse()\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_superset_of(self, other, soft_combinator=False):\n        # Combinators must match, OR be compatible -- space is a superset of >,\n        # ~ is a superset of +\n        if soft_combinator and self.combinator == ' ':\n            combinator_superset = True\n        else:\n            combinator_superset = (\n                self.combinator == other.combinator or\n                (self.combinator == ' ' and other.combinator == '>') or\n                (self.combinator == '~' and other.combinator == '+'))\n\n        return (\n            combinator_superset and\n            set(self.tokens) <= set(other.tokens))", "response": "Return True iff this selector matches the same elements as other and perhaps others."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace this simple selector with the given iterable of parent simple selectors.", "response": "def replace_parent(self, parent_simples):\n        \"\"\"If ``&`` (or the legacy xCSS equivalent ``self``) appears in this\n        selector, replace it with the given iterable of parent selectors.\n\n        Returns a tuple of simple selectors.\n        \"\"\"\n        assert parent_simples\n\n        ancestors = parent_simples[:-1]\n        parent = parent_simples[-1]\n\n        did_replace = False\n        new_tokens = []\n        for token in self.tokens:\n            if not did_replace and token in ('&', 'self'):\n                did_replace = True\n                new_tokens.extend(parent.tokens)\n                if token == 'self':\n                    warn(FutureWarning(\n                        \"The xCSS 'self' selector is deprecated and will be \"\n                        \"removed in 2.0.  Use & instead.  ({0!r})\"\n                        .format(self)\n                    ))\n            else:\n                new_tokens.append(token)\n\n        if not did_replace:\n            # This simple selector doesn't contain a parent reference so just\n            # stick it on the end\n            return parent_simples + (self,)\n\n        # This simple selector was merged into the direct parent.\n        merged_self = type(self)(parent.combinator, new_tokens)\n        selector = ancestors + (merged_self,)\n        # Our combinator goes on the first ancestor, i.e., substituting \"foo\n        # bar baz\" into \"+ &.quux\" produces \"+ foo bar baz.quux\".  This means a\n        # potential conflict with the first ancestor's combinator!\n        root = selector[0]\n        if not _is_combinator_subset_of(self.combinator, root.combinator):\n            raise ValueError(\n                \"Can't sub parent {0!r} into {1!r}: \"\n                \"combinators {2!r} and {3!r} conflict!\"\n                .format(\n                    parent_simples, self, self.combinator, root.combinator))\n\n        root = type(self)(self.combinator, root.tokens)\n        selector = (root,) + selector[1:]\n        return tuple(selector)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_into(self, other):\n        # TODO it shouldn't be possible to merge two elements or two pseudo\n        # elements, /but/ it shouldn't just be a fatal error here -- it\n        # shouldn't even be considered a candidate for extending!\n        # TODO this is slightly inconsistent with ruby, which treats a trailing\n        # set of self tokens like ':before.foo' as a single unit to be stuck at\n        # the end.  but that's completely bogus anyway.\n        element = []\n        middle = []\n        pseudo = []\n        for token in self.tokens + other.tokens:\n            if token in CSS2_PSEUDO_ELEMENTS or token.startswith('::'):\n                pseudo.append(token)\n            elif token[0] in BODY_TOKEN_SIGILS:\n                middle.append(token)\n            else:\n                element.append(token)\n        new_tokens = element + middle + pseudo\n\n        if self.combinator == ' ' or self.combinator == other.combinator:\n            combinator = other.combinator\n        elif other.combinator == ' ':\n            combinator = self.combinator\n        else:\n            raise ValueError(\n                \"Don't know how to merge conflicting combinators: \"\n                \"{0!r} and {1!r}\"\n                .format(self, other))\n        return type(self)(combinator, new_tokens)", "response": "Merge two simple selectors together."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup_key(self):\n        parts = set()\n        for node in self.simple_selectors:\n            for token in node.tokens:\n                if token[0] not in ':[':\n                    parts.add(token)\n\n        if not parts:\n            # Should always have at least ONE key; selectors with no elements,\n            # no classes, and no ids can be indexed as None to avoid a scan of\n            # every selector in the entire document\n            parts.add(None)\n\n        return frozenset(parts)", "response": "Build a key from the important parts of a selector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef substitute(self, target, replacement):\n        # Find the target in the parent selector, and split it into\n        # before/after\n        p_before, p_extras, p_after = self.break_around(target.simple_selectors)\n\n        # The replacement has no hinge; it only has the most specific simple\n        # selector (which is the part that replaces \"self\" in the parent) and\n        # whatever preceding simple selectors there may be\n        r_trail = replacement.simple_selectors[:-1]\n        r_extras = replacement.simple_selectors[-1]\n\n        # TODO what if the prefix doesn't match?  who wins?  should we even get\n        # this far?\n        focal_nodes = (p_extras.merge_into(r_extras),)\n\n        befores = _merge_selectors(p_before, r_trail)\n\n        cls = type(self)\n        return [\n            cls(before + focal_nodes + p_after)\n            for before in befores]", "response": "Return a list of selectors obtained by replacing the target selector with replacement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a simple selector node contained within this one return a parent selector extra specifiers for the parent selector and a child selector.", "response": "def break_around(self, hinge):\n        \"\"\"Given a simple selector node contained within this one (a \"hinge\"),\n        break it in half and return a parent selector, extra specifiers for the\n        hinge, and a child selector.\n\n        That is, given a hinge X, break the selector A + X.y B into A, + .y,\n        and B.\n        \"\"\"\n        hinge_start = hinge[0]\n        for i, node in enumerate(self.simple_selectors):\n            # In this particular case, a ' ' combinator actually means \"no\" (or\n            # any) combinator, so it should be ignored\n            if hinge_start.is_superset_of(node, soft_combinator=True):\n                start_idx = i\n                break\n        else:\n            raise ValueError(\n                \"Couldn't find hinge %r in compound selector %r\" %\n                (hinge_start, self.simple_selectors))\n\n        for i, hinge_node in enumerate(hinge):\n            if i == 0:\n                # We just did this\n                continue\n\n            self_node = self.simple_selectors[start_idx + i]\n            if hinge_node.is_superset_of(self_node):\n                continue\n\n            # TODO this isn't true; consider finding `a b` in `a c a b`\n            raise ValueError(\n                \"Couldn't find hinge %r in compound selector %r\" %\n                (hinge_node, self.simple_selectors))\n\n        end_idx = start_idx + len(hinge) - 1\n\n        focal_node = self.simple_selectors[end_idx]\n        extras = focal_node.difference(hinge[-1])\n\n        return (\n            self.simple_selectors[:start_idx],\n            extras,\n            self.simple_selectors[end_idx + 1:])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting pack by maximum sides", "response": "def MAXSIDE(a, b):\n        \"\"\"maxside: Sort pack by maximum sides\"\"\"\n        return cmp(max(b[0], b[1]), max(a[0], a[1])) or cmp(min(b[0], b[1]), min(a[0], a[1])) or cmp(b[1], a[1]) or cmp(b[0], a[0])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AREA(a, b):\n        return cmp(b[0] * b[1], a[0] * a[1]) or cmp(b[1], a[1]) or cmp(b[0], a[0])", "response": "Area sort by area"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_units_to_base_units(units):\n    total_factor = 1\n    new_units = []\n    for unit in units:\n        if unit not in BASE_UNIT_CONVERSIONS:\n            continue\n\n        factor, new_unit = BASE_UNIT_CONVERSIONS[unit]\n        total_factor *= factor\n        new_units.append(new_unit)\n\n    new_units.sort()\n    return total_factor, tuple(new_units)", "response": "Convert a set of units into a set of base units."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef count_base_units(units):\n    ret = {}\n    for unit in units:\n        factor, base_unit = get_conversion_factor(unit)\n\n        ret.setdefault(base_unit, 0)\n        ret[base_unit] += 1\n\n    return ret", "response": "Returns a dict mapping names of base units to how many times they appear in the given iterable of units."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a list of units remove a specified number of each base unit. Returns a 2 - tuple of total_factor remaining_units.", "response": "def cancel_base_units(units, to_remove):\n    \"\"\"Given a list of units, remove a specified number of each base unit.\n\n    Arguments:\n        units: an iterable of units\n        to_remove: a mapping of base_unit => count, such as that returned from\n            count_base_units\n\n    Returns a 2-tuple of (factor, remaining_units).\n    \"\"\"\n\n    # Copy the dict since we're about to mutate it\n    to_remove = to_remove.copy()\n    remaining_units = []\n    total_factor = Fraction(1)\n\n    for unit in units:\n        factor, base_unit = get_conversion_factor(unit)\n        if not to_remove.get(base_unit, 0):\n            remaining_units.append(unit)\n            continue\n\n        total_factor *= factor\n        to_remove[base_unit] -= 1\n\n    return total_factor, remaining_units"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning whether the given name looks like a builtin CSS function.", "response": "def is_builtin_css_function(name):\n    \"\"\"Returns whether the given `name` looks like the name of a builtin CSS\n    function.\n\n    Unrecognized functions not in this list produce warnings.\n    \"\"\"\n    name = name.replace('_', '-')\n\n    if name in BUILTIN_FUNCTIONS:\n        return True\n\n    # Vendor-specific functions (-foo-bar) are always okay\n    if name[0] == '-' and '-' in name[1:]:\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the encoding for the given CSS source.", "response": "def determine_encoding(buf):\n    \"\"\"Return the appropriate encoding for the given CSS source, according to\n    the CSS charset rules.\n\n    `buf` may be either a string or bytes.\n    \"\"\"\n    # The ultimate default is utf8; bravo, W3C\n    bom_encoding = 'UTF-8'\n\n    if not buf:\n        # What\n        return bom_encoding\n\n    if isinstance(buf, six.text_type):\n        # We got a file that, for whatever reason, produces already-decoded\n        # text.  Check for the BOM (which is useless now) and believe\n        # whatever's in the @charset.\n        if buf[0] == '\\ufeff':\n            buf = buf[0:]\n\n        # This is pretty similar to the code below, but without any encoding\n        # double-checking.\n        charset_start = '@charset \"'\n        charset_end = '\";'\n        if buf.startswith(charset_start):\n            start = len(charset_start)\n            end = buf.index(charset_end, start)\n            return buf[start:end]\n        else:\n            return bom_encoding\n\n    # BOMs\n    if buf[:3] == b'\\xef\\xbb\\xbf':\n        bom_encoding = 'UTF-8'\n        buf = buf[3:]\n    if buf[:4] == b'\\x00\\x00\\xfe\\xff':\n        bom_encoding = 'UTF-32BE'\n        buf = buf[4:]\n    elif buf[:4] == b'\\xff\\xfe\\x00\\x00':\n        bom_encoding = 'UTF-32LE'\n        buf = buf[4:]\n    if buf[:4] == b'\\x00\\x00\\xff\\xfe':\n        raise UnicodeError(\"UTF-32-2143 is not supported\")\n    elif buf[:4] == b'\\xfe\\xff\\x00\\x00':\n        raise UnicodeError(\"UTF-32-2143 is not supported\")\n    elif buf[:2] == b'\\xfe\\xff':\n        bom_encoding = 'UTF-16BE'\n        buf = buf[2:]\n    elif buf[:2] == b'\\xff\\xfe':\n        bom_encoding = 'UTF-16LE'\n        buf = buf[2:]\n\n    # The spec requires exactly this syntax; no escapes or extra spaces or\n    # other shenanigans, thank goodness.\n    charset_start = '@charset \"'.encode(bom_encoding)\n    charset_end = '\";'.encode(bom_encoding)\n    if buf.startswith(charset_start):\n        start = len(charset_start)\n        end = buf.index(charset_end, start)\n        encoded_encoding = buf[start:end]\n        encoding = encoded_encoding.decode(bom_encoding)\n\n        # Ensure that decoding with the specified encoding actually produces\n        # the same @charset rule\n        encoded_charset = buf[:end + len(charset_end)]\n        if (encoded_charset.decode(encoding) !=\n                encoded_charset.decode(bom_encoding)):\n            raise UnicodeError(\n                \"@charset {0} is incompatible with detected encoding {1}\"\n                .format(bom_encoding, encoding))\n    else:\n        # With no @charset, believe the BOM\n        encoding = bom_encoding\n\n    return encoding"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _scan(self, type):\n        tok = self._scanner.token(self._pos, frozenset([type]))\n        self._char_pos = tok[0]\n        if tok[2] != type:\n            raise SyntaxError(\"SyntaxError[@ char %s: %s]\" % (repr(tok[0]), \"Trying to find \" + type))\n        self._pos += 1\n        return tok[3]", "response": "Scan for a specific type of entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an interpolation if there are multiple parts otherwise a plain Literal.", "response": "def maybe(cls, parts, quotes=None, type=String, **kwargs):\n        \"\"\"Returns an interpolation if there are multiple parts, otherwise a\n        plain Literal.  This keeps the AST somewhat simpler, but also is the\n        only way `Literal.from_bareword` gets called.\n        \"\"\"\n        if len(parts) > 1:\n            return cls(parts, quotes=quotes, type=type, **kwargs)\n\n        if quotes is None and type is String:\n            return Literal.from_bareword(parts[0])\n\n        return Literal(type(parts[0], quotes=quotes, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninterpreting this literal as a function definition yields pairs of variable name and default value as an AST node or None.", "response": "def iter_def_argspec(self):\n        \"\"\"Interpreting this literal as a function definition, yields pairs of\n        (variable name as a string, default value as an AST node or None).\n        \"\"\"\n        started_kwargs = False\n        seen_vars = set()\n\n        for var, value in self.argpairs:\n            if var is None:\n                # value is actually the name\n                var = value\n                value = None\n\n                if started_kwargs:\n                    raise SyntaxError(\n                        \"Required argument %r must precede optional arguments\"\n                        % (var.name,))\n\n            else:\n                started_kwargs = True\n\n            if not isinstance(var, Variable):\n                raise SyntaxError(\"Expected variable name, got %r\" % (var,))\n\n            if var.name in seen_vars:\n                raise SyntaxError(\"Duplicate argument %r\" % (var.name,))\n            seen_vars.add(var.name)\n\n            yield var.name, value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninterpreting this literal as a function call return a 2 - tuple of args and kwargs.", "response": "def evaluate_call_args(self, calculator):\n        \"\"\"Interpreting this literal as a function call, return a 2-tuple of\n        ``(args, kwargs)``.\n        \"\"\"\n        args = []\n        kwargs = OrderedDict()  # Sass kwargs preserve order\n        for var_node, value_node in self.argpairs:\n            value = value_node.evaluate(calculator, divide=True)\n            if var_node is None:\n                # Positional\n                args.append(value)\n            else:\n                # Named\n                if not isinstance(var_node, Variable):\n                    raise TypeError(\n                        \"Expected variable name, got {0!r}\".format(var_node))\n                kwargs[var_node.name] = value\n\n        # Slurpy arguments go on the end of the args\n        if self.slurp:\n            args.extend(self.slurp.evaluate(calculator, divide=True))\n\n        return args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a sprite map from a glob pattern.", "response": "def sprite_map(g, **kwargs):\n    \"\"\"\n    Generates a sprite map from the files matching the glob pattern.\n    Uses the keyword-style arguments passed in to control the placement.\n\n    $direction - Sprite map layout. Can be `vertical` (default), `horizontal`, `diagonal` or `smart`.\n\n    $position - For `horizontal` and `vertical` directions, the position of the sprite. (defaults to `0`)\n    $<sprite>-position - Position of a given sprite.\n\n    $padding, $spacing - Adds paddings to sprites (top, right, bottom, left). (defaults to `0, 0, 0, 0`)\n    $<sprite>-padding, $<sprite>-spacing - Padding for a given sprite.\n\n    $dst-color - Together with `$src-color`, forms a map of source colors to be converted to destiny colors (same index of `$src-color` changed to `$dst-color`).\n    $<sprite>-dst-color - Destiny colors for a given sprite. (defaults to `$dst-color`)\n\n    $src-color - Selects source colors to be converted to the corresponding destiny colors. (defaults to `black`)\n    $<sprite>-dst-color - Source colors for a given sprite. (defaults to `$src-color`)\n\n    $collapse - Collapses every image in the sprite map to a fixed size (`x` and `y`).\n    $collapse-x  - Collapses a size for `x`.\n    $collapse-y  - Collapses a size for `y`.\n    \"\"\"\n    if not Image:\n        raise SassMissingDependency('PIL', 'image manipulation')\n\n    sprite_maps = _get_cache('sprite_maps')\n\n    now_time = time.time()\n\n    globs = String(g, quotes=None).value\n    globs = sorted(g.strip() for g in globs.split(','))\n\n    _k_ = ','.join(globs)\n\n    files = None\n    rfiles = None\n    tfiles = None\n    map_name = None\n\n    if _k_ in sprite_maps:\n        sprite_maps[_k_]['*'] = now_time\n    else:\n        files = []\n        rfiles = []\n        tfiles = []\n        for _glob in globs:\n            if '..' not in _glob:  # Protect against going to prohibited places...\n                if callable(config.STATIC_ROOT):\n                    _glob_path = _glob\n                    _rfiles = _files = sorted(config.STATIC_ROOT(_glob))\n                else:\n                    _glob_path = os.path.join(config.STATIC_ROOT, _glob)\n                    _files = glob.glob(_glob_path)\n                    _files = sorted((f, None) for f in _files)\n                    _rfiles = [(rf[len(config.STATIC_ROOT):], s) for rf, s in _files]\n                if _files:\n                    files.extend(_files)\n                    rfiles.extend(_rfiles)\n                    base_name = os.path.normpath(os.path.dirname(_glob)).replace('\\\\', '_').replace('/', '_')\n                    _map_name, _, _map_type = base_name.partition('.')\n                    if _map_type:\n                        _map_type += '-'\n                    if not map_name:\n                        map_name = _map_name\n                    tfiles.extend([_map_type] * len(_files))\n                else:\n                    glob_path = _glob_path\n\n    if files is not None:\n        if not files:\n            log.error(\"Nothing found at '%s'\", glob_path)\n            return String.unquoted('')\n\n        key = [f for (f, s) in files] + [repr(kwargs), config.ASSETS_URL]\n        key = map_name + '-' + make_filename_hash(key)\n        asset_file = key + '.png'\n        ASSETS_ROOT = _assets_root()\n        asset_path = os.path.join(ASSETS_ROOT, asset_file)\n        cache_path = os.path.join(config.CACHE_ROOT or ASSETS_ROOT, asset_file + '.cache')\n\n        inline = Boolean(kwargs.get('inline', False))\n\n        sprite_map = None\n        asset = None\n        file_asset = None\n        inline_asset = None\n        if os.path.exists(asset_path) or inline:\n            try:\n                save_time, file_asset, inline_asset, sprite_map, sizes = pickle.load(open(cache_path))\n                if file_asset:\n                    sprite_maps[file_asset.render()] = sprite_map\n                if inline_asset:\n                    sprite_maps[inline_asset.render()] = sprite_map\n                if inline:\n                    asset = inline_asset\n                else:\n                    asset = file_asset\n            except:\n                pass\n\n            if sprite_map:\n                for file_, storage in files:\n                    _time = getmtime(file_, storage)\n                    if save_time < _time:\n                        if _time > now_time:\n                            log.warning(\"File '%s' has a date in the future (cache ignored)\" % file_)\n                        sprite_map = None  # Invalidate cached sprite map\n                        break\n\n        if sprite_map is None or asset is None:\n            cache_buster = Boolean(kwargs.get('cache_buster', True))\n            direction = String.unquoted(kwargs.get('direction', config.SPRTE_MAP_DIRECTION)).value\n            repeat = String.unquoted(kwargs.get('repeat', 'no-repeat')).value\n            collapse = kwargs.get('collapse', Number(0))\n            if isinstance(collapse, List):\n                collapse_x = int(Number(collapse[0]).value)\n                collapse_y = int(Number(collapse[-1]).value)\n            else:\n                collapse_x = collapse_y = int(Number(collapse).value)\n            if 'collapse_x' in kwargs:\n                collapse_x = int(Number(kwargs['collapse_x']).value)\n            if 'collapse_y' in kwargs:\n                collapse_y = int(Number(kwargs['collapse_y']).value)\n\n            position = Number(kwargs.get('position', 0))\n            if not position.is_simple_unit('%') and position.value > 1:\n                position = position.value / 100.0\n            else:\n                position = position.value\n            if position < 0:\n                position = 0.0\n            elif position > 1:\n                position = 1.0\n\n            padding = kwargs.get('padding', kwargs.get('spacing', Number(0)))\n            padding = [int(Number(v).value) for v in List.from_maybe(padding)]\n            padding = (padding * 4)[:4]\n\n            dst_colors = kwargs.get('dst_color')\n            dst_colors = [list(Color(v).value[:3]) for v in List.from_maybe(dst_colors) if v]\n            src_colors = kwargs.get('src_color', Color.from_name('black'))\n            src_colors = [tuple(Color(v).value[:3]) for v in List.from_maybe(src_colors)]\n            len_colors = max(len(dst_colors), len(src_colors))\n            dst_colors = (dst_colors * len_colors)[:len_colors]\n            src_colors = (src_colors * len_colors)[:len_colors]\n\n            def images(f=lambda x: x):\n                for file_, storage in f(files):\n                    if storage is not None:\n                        _file = storage.open(file_)\n                    else:\n                        _file = file_\n                    _image = Image.open(_file)\n                    yield _image\n\n            names = tuple(os.path.splitext(os.path.basename(file_))[0] for file_, storage in files)\n            tnames = tuple(tfiles[i] + n for i, n in enumerate(names))\n\n            has_dst_colors = False\n            all_dst_colors = []\n            all_src_colors = []\n            all_positions = []\n            all_paddings = []\n\n            for name in names:\n                name = name.replace('-', '_')\n\n                _position = kwargs.get(name + '_position')\n                if _position is None:\n                    _position = position\n                else:\n                    _position = Number(_position)\n                    if not _position.is_simple_unit('%') and _position.value > 1:\n                        _position = _position.value / 100.0\n                    else:\n                        _position = _position.value\n                    if _position < 0:\n                        _position = 0.0\n                    elif _position > 1:\n                        _position = 1.0\n                all_positions.append(_position)\n\n                _padding = kwargs.get(name + '_padding', kwargs.get(name + '_spacing'))\n                if _padding is None:\n                    _padding = padding\n                else:\n                    _padding = [int(Number(v).value) for v in List.from_maybe(_padding)]\n                    _padding = (_padding * 4)[:4]\n                all_paddings.append(_padding)\n\n                _dst_colors = kwargs.get(name + '_dst_color')\n                if _dst_colors is None:\n                    _dst_colors = dst_colors\n                    if dst_colors:\n                        has_dst_colors = True\n                else:\n                    has_dst_colors = True\n                    _dst_colors = [list(Color(v).value[:3]) for v in List.from_maybe(_dst_colors) if v]\n                _src_colors = kwargs.get(name + '_src_color', Color.from_name('black'))\n                if _src_colors is None:\n                    _src_colors = src_colors\n                else:\n                    _src_colors = [tuple(Color(v).value[:3]) for v in List.from_maybe(_src_colors)]\n                _len_colors = max(len(_dst_colors), len(_src_colors))\n                _dst_colors = (_dst_colors * _len_colors)[:_len_colors]\n                _src_colors = (_src_colors * _len_colors)[:_len_colors]\n                all_dst_colors.append(_dst_colors)\n                all_src_colors.append(_src_colors)\n\n            sizes = tuple((collapse_x or i.size[0], collapse_y or i.size[1]) for i in images())\n\n            if direction == 'horizontal':\n                layout = HorizontalSpritesLayout(sizes, all_paddings, position=all_positions)\n            elif direction == 'vertical':\n                layout = VerticalSpritesLayout(sizes, all_paddings, position=all_positions)\n            elif direction == 'diagonal':\n                layout = DiagonalSpritesLayout(sizes, all_paddings)\n            elif direction == 'smart':\n                layout = PackedSpritesLayout(sizes, all_paddings)\n            else:\n                raise Exception(\"Invalid direction %r\" % (direction,))\n            layout_positions = list(layout)\n\n            new_image = Image.new(\n                mode='RGBA',\n                size=(layout.width, layout.height),\n                color=(0, 0, 0, 0)\n            )\n\n            useless_dst_color = has_dst_colors\n\n            offsets_x = []\n            offsets_y = []\n            for i, image in enumerate(images()):\n                x, y, width, height, cssx, cssy, cssw, cssh = layout_positions[i]\n                iwidth, iheight = image.size\n\n                if has_dst_colors:\n                    pixdata = image.load()\n                    for _y in xrange(iheight):\n                        for _x in xrange(iwidth):\n                            pixel = pixdata[_x, _y]\n                            a = pixel[3] if len(pixel) == 4 else 255\n                            if a:\n                                rgb = pixel[:3]\n                                for j, dst_color in enumerate(all_dst_colors[i]):\n                                    if rgb == all_src_colors[i][j]:\n                                        new_color = tuple([int(c) for c in dst_color] + [a])\n                                        if pixel != new_color:\n                                            pixdata[_x, _y] = new_color\n                                            useless_dst_color = False\n                                        break\n\n                if iwidth != width or iheight != height:\n                    cy = 0\n                    while cy < iheight:\n                        cx = 0\n                        while cx < iwidth:\n                            new_image = alpha_composite(new_image, image, (x, y), (cx, cy, cx + width, cy + height))\n                            cx += width\n                        cy += height\n                else:\n                    new_image.paste(image, (x, y))\n                offsets_x.append(cssx)\n                offsets_y.append(cssy)\n\n            if useless_dst_color:\n                log.warning(\"Useless use of $dst-color in sprite map for files at '%s' (never used for)\" % glob_path)\n\n            filetime = int(now_time)\n\n            if not inline:\n                try:\n                    new_image.save(asset_path)\n                    url = '%s%s' % (config.ASSETS_URL, asset_file)\n                    if cache_buster:\n                        url += '?_=%s' % filetime\n                except IOError:\n                    log.exception(\"Error while saving image\")\n                    inline = True\n            if inline:\n                output = six.BytesIO()\n                new_image.save(output, format='PNG')\n                contents = output.getvalue()\n                output.close()\n                mime_type = 'image/png'\n                url = make_data_url(mime_type, contents)\n\n            url = 'url(%s)' % escape(url)\n            if inline:\n                asset = inline_asset = List([String.unquoted(url), String.unquoted(repeat)])\n            else:\n                asset = file_asset = List([String.unquoted(url), String.unquoted(repeat)])\n\n            # Add the new object:\n            sprite_map = dict(zip(tnames, zip(sizes, rfiles, offsets_x, offsets_y)))\n            sprite_map['*'] = now_time\n            sprite_map['*f*'] = asset_file\n            sprite_map['*k*'] = key\n            sprite_map['*n*'] = map_name\n            sprite_map['*t*'] = filetime\n\n            sizes = zip(files, sizes)\n            cache_tmp = tempfile.NamedTemporaryFile(delete=False, dir=ASSETS_ROOT)\n            pickle.dump((now_time, file_asset, inline_asset, sprite_map, sizes), cache_tmp)\n            cache_tmp.close()\n            if sys.platform == 'win32' and os.path.isfile(cache_path):\n                # on windows, cannot rename a file to a path that matches\n                # an existing file, we have to remove it first\n                os.remove(cache_path)\n            os.rename(cache_tmp.name, cache_path)\n\n            # Use the sorted list to remove older elements (keep only 500 objects):\n            if len(sprite_maps) > MAX_SPRITE_MAPS:\n                for a in sorted(sprite_maps, key=lambda a: sprite_maps[a]['*'], reverse=True)[KEEP_SPRITE_MAPS:]:\n                    del sprite_maps[a]\n                log.warning(\"Exceeded maximum number of sprite maps (%s)\" % MAX_SPRITE_MAPS)\n            sprite_maps[asset.render()] = sprite_map\n        image_size_cache = _get_cache('image_size_cache')\n        for file_, size in sizes:\n            image_size_cache[file_] = size\n    # TODO this sometimes returns an empty list, or is never assigned to\n    return asset"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the name of a sprite map", "response": "def sprite_map_name(map):\n    \"\"\"\n    Returns the name of a sprite map The name is derived from the folder than\n    contains the sprites.\n    \"\"\"\n    map = map.render()\n    sprite_maps = _get_cache('sprite_maps')\n    sprite_map = sprite_maps.get(map)\n    if not sprite_map:\n        log.error(\"No sprite map found: %s\", map, extra={'stack': True})\n    if sprite_map:\n        return String.unquoted(sprite_map['*n*'])\n    return String.unquoted('')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the relative path to the original file that is used when construction the sprite.", "response": "def sprite_file(map, sprite):\n    \"\"\"\n    Returns the relative path (from the images directory) to the original file\n    used when construction the sprite. This is suitable for passing to the\n    image_width and image_height helpers.\n    \"\"\"\n    map = map.render()\n    sprite_maps = _get_cache('sprite_maps')\n    sprite_map = sprite_maps.get(map)\n    sprite_name = String.unquoted(sprite).value\n    sprite = sprite_map and sprite_map.get(sprite_name)\n    if not sprite_map:\n        log.error(\"No sprite map found: %s\", map, extra={'stack': True})\n    elif not sprite:\n        log.error(\"No sprite found: %s in %s\", sprite_name, sprite_map['*n*'], extra={'stack': True})\n    if sprite:\n        return String(sprite[1][0])\n    return String.unquoted('')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sprite(map, sprite, offset_x=None, offset_y=None, cache_buster=True):\n    map = map.render()\n    sprite_maps = _get_cache('sprite_maps')\n    sprite_map = sprite_maps.get(map)\n    sprite_name = String.unquoted(sprite).value\n    sprite = sprite_map and sprite_map.get(sprite_name)\n    if not sprite_map:\n        log.error(\"No sprite map found: %s\", map, extra={'stack': True})\n    elif not sprite:\n        log.error(\"No sprite found: %s in %s\", sprite_name, sprite_map['*n*'], extra={'stack': True})\n    if sprite:\n        url = '%s%s' % (config.ASSETS_URL, sprite_map['*f*'])\n        if cache_buster:\n            url += '?_=%s' % sprite_map['*t*']\n        x = Number(offset_x or 0, 'px')\n        y = Number(offset_y or 0, 'px')\n        if not x.value or (x.value <= -1 or x.value >= 1) and not x.is_simple_unit('%'):\n            x -= Number(sprite[2], 'px')\n        if not y.value or (y.value <= -1 or y.value >= 1) and not y.is_simple_unit('%'):\n            y -= Number(sprite[3], 'px')\n        url = \"url(%s)\" % escape(url)\n        return List([String.unquoted(url), x, y])\n    return List([Number(0), Number(0)])", "response": "Returns the image and background position for use in a single shorthand\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sprite_url(map, cache_buster=True):\n    map = map.render()\n    sprite_maps = _get_cache('sprite_maps')\n    sprite_map = sprite_maps.get(map)\n    if not sprite_map:\n        log.error(\"No sprite map found: %s\", map, extra={'stack': True})\n    if sprite_map:\n        url = '%s%s' % (config.ASSETS_URL, sprite_map['*f*'])\n        if cache_buster:\n            url += '?_=%s' % sprite_map['*t*']\n        url = \"url(%s)\" % escape(url)\n        return String.unquoted(url)\n    return String.unquoted('')", "response": "Returns a url to the sprite image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sprite_position(map, sprite, offset_x=None, offset_y=None):\n    map = map.render()\n    sprite_maps = _get_cache('sprite_maps')\n    sprite_map = sprite_maps.get(map)\n    sprite_name = String.unquoted(sprite).value\n    sprite = sprite_map and sprite_map.get(sprite_name)\n    if not sprite_map:\n        log.error(\"No sprite map found: %s\", map, extra={'stack': True})\n    elif not sprite:\n        log.error(\"No sprite found: %s in %s\", sprite_name, sprite_map['*n*'], extra={'stack': True})\n    if sprite:\n        x = None\n        if offset_x is not None and not isinstance(offset_x, Number):\n            x = offset_x\n        if not x or x.value not in ('left', 'right', 'center'):\n            if x:\n                offset_x = None\n            x = Number(offset_x or 0, 'px')\n            if not x.value or (x.value <= -1 or x.value >= 1) and not x.is_simple_unit('%'):\n                x -= Number(sprite[2], 'px')\n        y = None\n        if offset_y is not None and not isinstance(offset_y, Number):\n            y = offset_y\n        if not y or y.value not in ('top', 'bottom', 'center'):\n            if y:\n                offset_y = None\n            y = Number(offset_y or 0, 'px')\n            if not y.value or (y.value <= -1 or y.value >= 1) and not y.is_simple_unit('%'):\n                y -= Number(sprite[3], 'px')\n        return List([x, y])\n    return List([Number(0), Number(0)])", "response": "Returns the position for the original image in the sprite."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the URL for an image.", "response": "def _image_url(path, only_path=False, cache_buster=True, dst_color=None, src_color=None, inline=False, mime_type=None, spacing=None, collapse_x=None, collapse_y=None):\n    \"\"\"\n    src_color - a list of or a single color to be replaced by each corresponding dst_color colors\n    spacing - spaces to be added to the image\n    collapse_x, collapse_y - collapsable (layered) image of the given size (x, y)\n    \"\"\"\n    if inline or dst_color or spacing:\n        if not Image:\n            raise SassMissingDependency('PIL', 'image manipulation')\n\n    filepath = String.unquoted(path).value\n    fileext = os.path.splitext(filepath)[1].lstrip('.').lower()\n    if mime_type:\n        mime_type = String.unquoted(mime_type).value\n    if not mime_type:\n        mime_type = mimetypes.guess_type(filepath)[0]\n    if not mime_type:\n        mime_type = 'image/%s' % fileext\n    path = None\n    IMAGES_ROOT = _images_root()\n    if callable(IMAGES_ROOT):\n        try:\n            _file, _storage = list(IMAGES_ROOT(filepath))[0]\n        except IndexError:\n            filetime = None\n        else:\n            filetime = getmtime(_file, _storage)\n        if filetime is None:\n            filetime = 'NA'\n        elif inline or dst_color or spacing:\n            path = _storage.open(_file)\n    else:\n        _path = os.path.join(IMAGES_ROOT.rstrip(os.sep), filepath.strip('\\\\/'))\n        filetime = getmtime(_path)\n        if filetime is None:\n            filetime = 'NA'\n        elif inline or dst_color or spacing:\n            path = open(_path, 'rb')\n\n    BASE_URL = config.IMAGES_URL or config.STATIC_URL\n    if path:\n        dst_colors = [list(Color(v).value[:3]) for v in List.from_maybe(dst_color) if v]\n\n        src_color = Color.from_name('black') if src_color is None else src_color\n        src_colors = [tuple(Color(v).value[:3]) for v in List.from_maybe(src_color)]\n\n        len_colors = max(len(dst_colors), len(src_colors))\n        dst_colors = (dst_colors * len_colors)[:len_colors]\n        src_colors = (src_colors * len_colors)[:len_colors]\n\n        spacing = Number(0) if spacing is None else spacing\n        spacing = [int(Number(v).value) for v in List.from_maybe(spacing)]\n        spacing = (spacing * 4)[:4]\n\n        file_name, file_ext = os.path.splitext(os.path.normpath(filepath).replace(os.sep, '_'))\n        key = (filetime, src_color, dst_color, spacing)\n        asset_file = file_name + '-' + make_filename_hash(key) + file_ext\n        ASSETS_ROOT = _assets_root()\n        asset_path = os.path.join(ASSETS_ROOT, asset_file)\n\n        if os.path.exists(asset_path):\n            filepath = asset_file\n            BASE_URL = config.ASSETS_URL\n            if inline:\n                path = open(asset_path, 'rb')\n                url = make_data_url(mime_type, path.read())\n            else:\n                url = '%s%s' % (BASE_URL, filepath)\n                if cache_buster:\n                    filetime = getmtime(asset_path)\n                    url = add_cache_buster(url, filetime)\n        else:\n            simply_process = False\n            image = None\n\n            if fileext in ('cur',):\n                simply_process = True\n            else:\n                try:\n                    image = Image.open(path)\n                except IOError:\n                    if not collapse_x and not collapse_y and not dst_colors:\n                        simply_process = True\n\n            if simply_process:\n                if inline:\n                    url = make_data_url(mime_type, path.read())\n                else:\n                    url = '%s%s' % (BASE_URL, filepath)\n                    if cache_buster:\n                        filetime = getmtime(asset_path)\n                        url = add_cache_buster(url, filetime)\n            else:\n                width, height = collapse_x or image.size[0], collapse_y or image.size[1]\n                new_image = Image.new(\n                    mode='RGBA',\n                    size=(width + spacing[1] + spacing[3], height + spacing[0] + spacing[2]),\n                    color=(0, 0, 0, 0)\n                )\n                for i, dst_color in enumerate(dst_colors):\n                    src_color = src_colors[i]\n                    pixdata = image.load()\n                    for _y in xrange(image.size[1]):\n                        for _x in xrange(image.size[0]):\n                            pixel = pixdata[_x, _y]\n                            if pixel[:3] == src_color:\n                                pixdata[_x, _y] = tuple([int(c) for c in dst_color] + [pixel[3] if len(pixel) == 4 else 255])\n                iwidth, iheight = image.size\n                if iwidth != width or iheight != height:\n                    cy = 0\n                    while cy < iheight:\n                        cx = 0\n                        while cx < iwidth:\n                            cropped_image = image.crop((cx, cy, cx + width, cy + height))\n                            new_image.paste(cropped_image, (int(spacing[3]), int(spacing[0])), cropped_image)\n                            cx += width\n                        cy += height\n                else:\n                    new_image.paste(image, (int(spacing[3]), int(spacing[0])))\n\n                if not inline:\n                    try:\n                        new_image.save(asset_path)\n                        filepath = asset_file\n                        BASE_URL = config.ASSETS_URL\n                        if cache_buster:\n                            filetime = getmtime(asset_path)\n                    except IOError:\n                        log.exception(\"Error while saving image\")\n                        inline = True  # Retry inline version\n                    url = os.path.join(config.ASSETS_URL.rstrip(os.sep), asset_file.lstrip(os.sep))\n                    if cache_buster:\n                        url = add_cache_buster(url, filetime)\n                if inline:\n                    output = six.BytesIO()\n                    new_image.save(output, format='PNG')\n                    contents = output.getvalue()\n                    output.close()\n                    url = make_data_url(mime_type, contents)\n    else:\n        url = os.path.join(BASE_URL.rstrip('/'), filepath.lstrip('\\\\/'))\n        if cache_buster and filetime != 'NA':\n            url = add_cache_buster(url, filetime)\n\n    if not os.sep == '/':\n        url = url.replace(os.sep, '/')\n\n    if only_path:\n        return String.unquoted(url)\n    else:\n        return Url.unquoted(url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inline_image(image, mime_type=None, dst_color=None, src_color=None, spacing=None, collapse_x=None, collapse_y=None):\n    return _image_url(image, False, False, dst_color, src_color, True, mime_type, spacing, collapse_x, collapse_y)", "response": "Inline an image into a CSS\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a url to an image found relative to the project s images directory.", "response": "def image_url(path, only_path=False, cache_buster=True, dst_color=None, src_color=None, spacing=None, collapse_x=None, collapse_y=None):\n    \"\"\"\n    Generates a path to an asset found relative to the project's images\n    directory.\n    Passing a true value as the second argument will cause the only the path to\n    be returned instead of a `url()` function\n    \"\"\"\n    return _image_url(path, only_path, cache_buster, dst_color, src_color, False, None, spacing, collapse_x, collapse_y)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef image_height(image):\n    image_size_cache = _get_cache('image_size_cache')\n    if not Image:\n        raise SassMissingDependency('PIL', 'image manipulation')\n    filepath = String.unquoted(image).value\n    path = None\n    try:\n        height = image_size_cache[filepath][1]\n    except KeyError:\n        height = 0\n        IMAGES_ROOT = _images_root()\n        if callable(IMAGES_ROOT):\n            try:\n                _file, _storage = list(IMAGES_ROOT(filepath))[0]\n            except IndexError:\n                pass\n            else:\n                path = _storage.open(_file)\n        else:\n            _path = os.path.join(IMAGES_ROOT, filepath.strip(os.sep))\n            if os.path.exists(_path):\n                path = open(_path, 'rb')\n        if path:\n            image = Image.open(path)\n            size = image.size\n            height = size[1]\n            image_size_cache[filepath] = size\n    return Number(height, 'px')", "response": "Returns the height of the image found at the path supplied by image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(cls, origin, relpath, **kwargs):\n        path = origin / relpath\n        with path.open('rb') as f:\n            return cls.from_file(f, origin, relpath, **kwargs)", "response": "Read a source file from an origin and relpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads Sass source from a path.", "response": "def from_path(cls, path, origin=MISSING, **kwargs):\n        \"\"\"Read Sass source from a :class:`pathlib.Path`.\n\n        If no origin is given, it's assumed to be the file's parent directory.\n        \"\"\"\n        origin, relpath = cls._key_from_path(path, origin)\n\n        # Open in binary mode so we can reliably detect the encoding\n        with path.open('rb') as f:\n            return cls.from_file(f, origin, relpath, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a Sass source from a String specifying the path_string.", "response": "def from_filename(cls, path_string, origin=MISSING, **kwargs):\n        \"\"\" Read Sass source from a String specifying the path\n        \"\"\"\n        path = Path(path_string)\n        return cls.from_path(path, origin, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads Sass source from a file - like object.", "response": "def from_file(cls, f, origin=MISSING, relpath=MISSING, **kwargs):\n        \"\"\"Read Sass source from a file or file-like object.\n\n        If `origin` or `relpath` are missing, they are derived from the file's\n        ``.name`` attribute as with `from_path`.  If it doesn't have one, the\n        origin becomes None and the relpath becomes the file's repr.\n        \"\"\"\n        contents = f.read()\n        encoding = determine_encoding(contents)\n        if isinstance(contents, six.binary_type):\n            contents = contents.decode(encoding)\n\n        if origin is MISSING or relpath is MISSING:\n            filename = getattr(f, 'name', None)\n            if filename is None:\n                origin = None\n                relpath = repr(f)\n            else:\n                origin, relpath = cls._key_from_path(Path(filename), origin)\n\n        return cls(origin, relpath, contents, encoding=encoding, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_string(cls, string, relpath=None, encoding=None, is_sass=None):\n        if isinstance(string, six.text_type):\n            # Already decoded; we don't know what encoding to use for output,\n            # though, so still check for a @charset.\n            # TODO what if the given encoding conflicts with the one in the\n            # file?  do we care?\n            if encoding is None:\n                encoding = determine_encoding(string)\n\n            byte_contents = string.encode(encoding)\n            text_contents = string\n        elif isinstance(string, six.binary_type):\n            encoding = determine_encoding(string)\n            byte_contents = string\n            text_contents = string.decode(encoding)\n        else:\n            raise TypeError(\"Expected text or bytes, got {0!r}\".format(string))\n\n        origin = None\n        if relpath is None:\n            m = hashlib.sha256()\n            m.update(byte_contents)\n            relpath = repr(\"string:{0}:{1}\".format(\n                m.hexdigest()[:16], text_contents[:100]))\n\n        return cls(\n            origin, relpath, text_contents, encoding=encoding,\n            is_sass=is_sass,\n        )", "response": "Read Sass source from the contents of a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the set of items that overlap a bounding rectangle.", "response": "def hit(self, rect):\n        \"\"\"Returns the items that overlap a bounding rectangle.\n\n        Returns the set of all items in the quad-tree that overlap with a\n        bounding rectangle.\n\n        @param rect:\n            The bounding rectangle being tested against the quad-tree. This\n            must possess left, top, right and bottom attributes.\n        \"\"\"\n\n        # Find the hits at the current level.\n        hits = {tuple(self.items[i]) for i in rect.collidelistall(self.items)}\n\n        # Recursively check the lower quadrants.\n        if self.nw and rect.left <= self.cx and rect.top <= self.cy:\n            hits |= self.nw.hit(rect)\n        if self.sw and rect.left <= self.cx and rect.bottom >= self.cy:\n            hits |= self.sw.hit(rect)\n        if self.ne and rect.right >= self.cx and rect.top <= self.cy:\n            hits |= self.ne.hit(rect)\n        if self.se and rect.right >= self.cx and rect.bottom >= self.cy:\n            hits |= self.se.hit(rect)\n\n        return hits"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscroll the background in pixels", "response": "def scroll(self, vector):\n        \"\"\" scroll the background in pixels\n\n        :param vector: (int, int)\n        \"\"\"\n        self.center((vector[0] + self.view_rect.centerx,\n                     vector[1] + self.view_rect.centery))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef center(self, coords):\n        x, y = round(coords[0]), round(coords[1])\n        self.view_rect.center = x, y\n\n        mw, mh = self.data.map_size\n        tw, th = self.data.tile_size\n        vw, vh = self._tile_view.size\n\n        # prevent camera from exposing edges of the map\n        if self.clamp_camera:\n            self._anchored_view = True\n            self.view_rect.clamp_ip(self.map_rect)\n            x, y = self.view_rect.center\n\n        # calc the new position in tiles and pixel offset\n        left, self._x_offset = divmod(x - self._half_width, tw)\n        top, self._y_offset = divmod(y - self._half_height, th)\n        right = left + vw\n        bottom = top + vh\n\n        if not self.clamp_camera:\n            # not anchored, so the rendered map is being offset by values larger\n            # than the tile size.  this occurs when the edges of the map are inside\n            # the screen.  a situation like is shows a background under the map.\n            self._anchored_view = True\n            dx = int(left - self._tile_view.left)\n            dy = int(top - self._tile_view.top)\n\n            if mw < vw or left < 0:\n                left = 0\n                self._x_offset = x - self._half_width\n                self._anchored_view = False\n\n            elif right > mw:\n                left = mw - vw\n                self._x_offset += dx * tw\n                self._anchored_view = False\n\n            if mh < vh or top < 0:\n                top = 0\n                self._y_offset = y - self._half_height\n                self._anchored_view = False\n\n            elif bottom > mh:\n                top = mh - vh\n                self._y_offset += dy * th\n                self._anchored_view = False\n\n        # adjust the view if the view has changed without a redraw\n        dx = int(left - self._tile_view.left)\n        dy = int(top - self._tile_view.top)\n        view_change = max(abs(dx), abs(dy))\n\n        if view_change and (view_change <= self._redraw_cutoff):\n            self._buffer.scroll(-dx * tw, -dy * th)\n            self._tile_view.move_ip(dx, dy)\n            self._queue_edge_tiles(dx, dy)\n            self._flush_tile_queue(self._buffer)\n\n        elif view_change > self._redraw_cutoff:\n            logger.info('scrolling too quickly.  redraw forced')\n            self._tile_view.move_ip(dx, dy)\n            self.redraw_tiles(self._buffer)", "response": "center the map on a pixel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw(self, surface, rect, surfaces=None):\n        if self._zoom_level == 1.0:\n            self._render_map(surface, rect, surfaces)\n        else:\n            self._render_map(self._zoom_buffer, self._zoom_buffer.get_rect(), surfaces)\n            self.scaling_function(self._zoom_buffer, rect.size, surface)\n        return self._previous_blit.copy()", "response": "Draw the map onto a surface."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the size of the map in pixels.", "response": "def set_size(self, size):\n        \"\"\" Set the size of the map in pixels\n\n        This is an expensive operation, do only when absolutely needed.\n\n        :param size: (width, height) pixel size of camera/view of the group\n        \"\"\"\n        buffer_size = self._calculate_zoom_buffer_size(size, self._zoom_level)\n        self._size = size\n        self._initialize_buffers(buffer_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nredrawing the visible portion of the buffer", "response": "def redraw_tiles(self, surface):\n        \"\"\" redraw the visible portion of the buffer -- it is slow.\n        \"\"\"\n        # TODO/BUG: Redraw animated tiles correctly.  They are getting reset here\n        logger.warning('pyscroll buffer redraw')\n        self._clear_surface(self._buffer)\n        self._tile_queue = self.data.get_tile_images_by_rect(self._tile_view)\n        self._flush_tile_queue(surface)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns x y pair that will change world coords to screen coords", "response": "def get_center_offset(self):\n        \"\"\" Return x, y pair that will change world coords to screen coords\n        :return: int, int\n        \"\"\"\n        return (-self.view_rect.centerx + self._half_width,\n                -self.view_rect.centery + self._half_height)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntranslates world coordinates and return screen coordinates. Respects zoom level Will be returned as tuple.", "response": "def translate_point(self, point):\n        \"\"\" Translate world coordinates and return screen coordinates.  Respects zoom level\n\n        Will be returned as tuple.\n\n        :rtype: tuple\n        \"\"\"\n        mx, my = self.get_center_offset()\n        if self._zoom_level == 1.0:\n            return point[0] + mx, point[1] + my\n        else:\n            return int(round((point[0] + mx)) * self._real_ratio_x), int(round((point[1] + my) * self._real_ratio_y))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate rect position and size to screen coordinates. Respects zoom level.", "response": "def translate_rect(self, rect):\n        \"\"\" Translate rect position and size to screen coordinates.  Respects zoom level.\n\n        :rtype: Rect\n        \"\"\"\n        mx, my = self.get_center_offset()\n        rx = self._real_ratio_x\n        ry = self._real_ratio_y\n        x, y, w, h = rect\n        if self._zoom_level == 1.0:\n            return Rect(x + mx, y + my, w, h)\n        else:\n            return Rect(round((x + mx) * rx), round((y + my) * ry), round(w * rx), round(h * ry))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntranslating coordinates and return screen coordinates", "response": "def translate_points(self, points):\n        \"\"\" Translate coordinates and return screen coordinates\n\n        Will be returned in order passed as tuples.\n\n        :return: list\n        \"\"\"\n        retval = list()\n        append = retval.append\n        sx, sy = self.get_center_offset()\n        if self._zoom_level == 1.0:\n            for c in points:\n                append((c[0] + sx, c[1] + sy))\n        else:\n            rx = self._real_ratio_x\n            ry = self._real_ratio_y\n            for c in points:\n                append((int(round((c[0] + sx) * rx)), int(round((c[1] + sy) * ry))))\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef translate_rects(self, rects):\n        retval = list()\n        append = retval.append\n        sx, sy = self.get_center_offset()\n        if self._zoom_level == 1.0:\n            for r in rects:\n                x, y, w, h = r\n                append(Rect(x + sx, y + sy, w, h))\n        else:\n            rx = self._real_ratio_x\n            ry = self._real_ratio_y\n            for r in rects:\n                x, y, w, h = r\n                append(Rect(round((x + sx) * rx), round((y + sy) * ry), round(w * rx), round(h * ry)))\n        return retval", "response": "Translate rect position and size to screen coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders the map and optional surfaces to destination surface.", "response": "def _render_map(self, surface, rect, surfaces):\n        \"\"\" Render the map and optional surfaces to destination surface\n\n        :param surface: pygame surface to draw to\n        :param rect: area to draw to\n        :param surfaces: optional sequence of surfaces to interlace between tiles\n        \"\"\"\n        self._tile_queue = self.data.process_animation_queue(self._tile_view)\n        self._tile_queue and self._flush_tile_queue(self._buffer)\n\n        # TODO: could maybe optimize to remove just the edges, ideally by drawing lines\n        # if not self.anchored_view:\n        #     surface.fill(self._clear_color, self._previous_blit)\n        if not self._anchored_view:\n            self._clear_surface(surface, self._previous_blit)\n\n        offset = -self._x_offset + rect.left, -self._y_offset + rect.top\n\n        with surface_clipping_context(surface, rect):\n            self._previous_blit = surface.blit(self._buffer, offset)\n            if surfaces:\n                surfaces_offset = -offset[0], -offset[1]\n                self._draw_surfaces(surface, surfaces_offset, surfaces)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclearing the buffer on the surface.", "response": "def _clear_surface(self, surface, rect=None):\n        \"\"\" Clear the buffer, taking in account colorkey or alpha\n\n        :return:\n        \"\"\"\n        clear_color = self._rgb_clear_color if self._clear_color is None else self._clear_color\n        surface.fill(clear_color, rect)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw surfaces onto buffer then redraw tiles that cover them", "response": "def _draw_surfaces(self, surface, offset, surfaces):\n        \"\"\" Draw surfaces onto buffer, then redraw tiles that cover them\n\n        :param surface: destination\n        :param offset: offset to compensate for buffer alignment\n        :param surfaces: sequence of surfaces to blit\n        \"\"\"\n        surface_blit = surface.blit\n        ox, oy = offset\n        left, top = self._tile_view.topleft\n        hit = self._layer_quadtree.hit\n        get_tile = self.data.get_tile_image\n        tile_layers = tuple(self.data.visible_tile_layers)\n        dirty = list()\n        dirty_append = dirty.append\n\n        # TODO: check to avoid sorting overhead\n        # sort layers, then the y value\n        def sprite_sort(i):\n            return i[2], i[1][1] + i[0].get_height()\n\n        surfaces.sort(key=sprite_sort)\n\n        layer_getter = itemgetter(2)\n        for layer, group in groupby(surfaces, layer_getter):\n            del dirty[:]\n\n            for i in group:\n                try:\n                    flags = i[3]\n                except IndexError:\n                    dirty_append(surface_blit(i[0], i[1]))\n                else:\n                    dirty_append(surface_blit(i[0], i[1], None, flags))\n\n            # TODO: make set of covered tiles, in the case where a cluster\n            # of sprite surfaces causes excessive over tile overdrawing\n            for dirty_rect in dirty:\n                for r in hit(dirty_rect.move(ox, oy)):\n                    x, y, tw, th = r\n                    for l in [i for i in tile_layers if gt(i, layer)]:\n\n                        if self.tall_sprites and l == layer + 1:\n                            if y - oy + th <= dirty_rect.bottom - self.tall_sprites:\n                                continue\n\n                        tile = get_tile(x // tw + left, y // th + top, l)\n                        tile and surface_blit(tile, (x - ox, y - oy))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _queue_edge_tiles(self, dx, dy):\n        v = self._tile_view\n        tw, th = self.data.tile_size\n        self._tile_queue = iter([])\n\n        def append(rect):\n            self._tile_queue = chain(self._tile_queue, self.data.get_tile_images_by_rect(rect))\n            # TODO: optimize so fill is only used when map is smaller than buffer\n            self._clear_surface(self._buffer, ((rect[0] - v.left) * tw, (rect[1] - v.top) * th,\n                                               rect[2] * tw, rect[3] * th))\n\n        if dx > 0:    # right side\n            append((v.right - 1, v.top, dx, v.height))\n\n        elif dx < 0:  # left side\n            append((v.left, v.top, -dx, v.height))\n\n        if dy > 0:    # bottom side\n            append((v.left, v.bottom - 1, v.width, dy))\n\n        elif dy < 0:  # top side\n            append((v.left, v.top, v.width, -dy))", "response": "Queue edge tiles and clear edge areas on buffer if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_buffers(self, view_size, buffer_size):\n        requires_zoom_buffer = not view_size == buffer_size\n        self._zoom_buffer = None\n\n        if self._clear_color is None:\n            if requires_zoom_buffer:\n                self._zoom_buffer = Surface(view_size)\n            self._buffer = Surface(buffer_size)\n        elif self._clear_color == self._rgba_clear_color:\n            if requires_zoom_buffer:\n                self._zoom_buffer = Surface(view_size, flags=pygame.SRCALPHA)\n            self._buffer = Surface(buffer_size, flags=pygame.SRCALPHA)\n            self.data.convert_surfaces(self._buffer, True)\n        elif self._clear_color is not self._rgb_clear_color:\n            if requires_zoom_buffer:\n                self._zoom_buffer = Surface(view_size, flags=pygame.RLEACCEL)\n                self._zoom_buffer.set_colorkey(self._clear_color)\n            self._buffer = Surface(buffer_size, flags=pygame.RLEACCEL)\n            self._buffer.set_colorkey(self._clear_color)\n            self._buffer.fill(self._clear_color)", "response": "Create the buffers for the current image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _initialize_buffers(self, view_size):\n\n        def make_rect(x, y):\n            return Rect((x * tw, y * th), (tw, th))\n\n        tw, th = self.data.tile_size\n        mw, mh = self.data.map_size\n        buffer_tile_width = int(math.ceil(view_size[0] / tw) + 1)\n        buffer_tile_height = int(math.ceil(view_size[1] / th) + 1)\n        buffer_pixel_size = buffer_tile_width * tw, buffer_tile_height * th\n\n        self.map_rect = Rect(0, 0, mw * tw, mh * th)\n        self.view_rect.size = view_size\n        self._previous_blit = Rect(self.view_rect)\n        self._tile_view = Rect(0, 0, buffer_tile_width, buffer_tile_height)\n        self._redraw_cutoff = 1  # TODO: optimize this value\n        self._create_buffers(view_size, buffer_pixel_size)\n        self._half_width = view_size[0] // 2\n        self._half_height = view_size[1] // 2\n        self._x_offset = 0\n        self._y_offset = 0\n\n        rects = [make_rect(*i) for i in product(range(buffer_tile_width),\n                                                range(buffer_tile_height))]\n\n        # TODO: figure out what depth -actually- does\n        # values <= 8 tend to reduce performance\n        self._layer_quadtree = quadtree.FastQuadTree(rects, 4)\n\n        self.redraw_tiles(self._buffer)", "response": "Create the buffers to draw the cache tile drawing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_animation_queue(self, tile_view):\n\n        # verify that there are tile substitutions ready\n        self._update_time()\n        try:\n            if self._animation_queue[0].next > self._last_time:\n                return\n\n        # raised with the animation queue is empty (no animations at all)\n        except IndexError:\n            return\n\n        new_tiles = list()\n        new_tiles_append = new_tiles.append\n        tile_layers = tuple(self.visible_tile_layers)\n        get_tile_image = self.get_tile_image\n\n        # test if the next scheduled tile change is ready\n        while self._animation_queue[0].next <= self._last_time:\n\n            # get the next tile/frame which is ready to be changed\n            token = heappop(self._animation_queue)\n            next_frame = token.advance(self._last_time)\n            heappush(self._animation_queue, token)\n\n            # following line for when all gid positions are known\n            # for position in self._tracked_tiles & token.positions:\n\n            for position in token.positions.copy():\n                x, y, l = position\n\n                # if this tile is on the buffer (checked by using the tile view)\n                if tile_view.collidepoint(x, y):\n\n                    # record the location of this tile, in case of a screen wipe, or sprite cover\n                    self._animated_tile[position] = next_frame.image\n\n                    # redraw the entire column of tiles\n                    for layer in tile_layers:\n                        if layer == l:\n\n                            # queue the new animated tile\n                            new_tiles_append((x, y, layer, next_frame.image))\n                        else:\n\n                            # queue the normal tile\n                            image = get_tile_image(x, y, layer)\n                            if image:\n                                new_tiles_append((x, y, layer, image))\n\n                # not on screen, but was previously.  clear it.\n                else:\n                    token.positions.remove(position)\n\n        return new_tiles", "response": "Process the animation queue and return the list of tile ids that are ready to be changed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reload_animations(self):\n        self._update_time()\n        self._animation_queue = list()\n        self._tracked_gids = set()\n        self._animation_map = dict()\n\n        for gid, frame_data in self.get_animations():\n            self._tracked_gids.add(gid)\n\n            frames = list()\n            for frame_gid, frame_duration in frame_data:\n                image = self._get_tile_image_by_id(frame_gid)\n                frames.append(AnimationFrame(image, frame_duration))\n\n            # the following line is slow when loading maps, but avoids overhead when rendering\n            # positions = set(self.tmx.get_tile_locations_by_gid(gid))\n\n            # ideally, positions would be populated with all the known\n            # locations of an animation, but searching for their locations\n            # is slow. so it will be updated as the map is drawn.\n\n            positions = set()\n            ani = AnimationToken(positions, frames, self._last_time)\n            self._animation_map[gid] = ani\n            heappush(self._animation_queue, ani)", "response": "Reloads the animation information for all known modules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a tile image from the data.", "response": "def get_tile_image(self, x, y, l):\n        \"\"\" Get a tile image, respecting current animations\n\n        :param x: x coordinate\n        :param y: y coordinate\n        :param l: layer\n\n        :type x: int\n        :type y: int\n        :type l: int\n\n        :rtype: pygame.Surface\n        \"\"\"\n        # disabled for now, re-enable when support for generic maps is restored\n        # # since the tile has been queried, assume it wants to be checked\n        # # for animations sometime in the future\n        # if self._animation_queue:\n        #     self._tracked_tiles.add((x, y, l))\n\n        try:\n            # animated, so return the correct frame\n            return self._animated_tile[(x, y, l)]\n\n        except KeyError:\n\n            # not animated, so return surface from data, if any\n            return self._get_tile_image(x, y, l)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a 2d area return a generator of tile images inside the tile layer.", "response": "def get_tile_images_by_rect(self, rect):\n        \"\"\" Given a 2d area, return generator of tile images inside\n\n        Given the coordinates, yield the following tuple for each tile:\n          X, Y, Layer Number, pygame Surface\n\n        This method also defines render order by re arranging the\n        positions of each tile as it is yielded to the renderer.\n\n        There is an optimization that you can make for your data:\n        If you can provide access to tile information in a batch,\n        then pyscroll can access data faster and render quicker.\n\n        To implement this optimization, override this method.\n\n        Not like python 'Range': should include the end index!\n\n        :param rect: a rect-like object that defines tiles to draw\n        :return: generator\n        \"\"\"\n        x1, y1, x2, y2 = rect_to_bb(rect)\n        for layer in self.visible_tile_layers:\n            for y, x in product(range(y1, y2 + 1),\n                                range(x1, x2 + 1)):\n                tile = self.get_tile_image(x, y, layer)\n                if tile:\n                    yield x, y, layer, tile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_surfaces(self, parent, alpha=False):\n        images = list()\n        for i in self.tmx.images:\n            try:\n                if alpha:\n                    images.append(i.convert_alpha(parent))\n                else:\n                    images.append(i.convert(parent))\n            except AttributeError:\n                images.append(None)\n        self.tmx.images = images", "response": "Convert all images in the data to match the parent s surface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visible_object_layers(self):\n        return (layer for layer in self.tmx.visible_layers\n                if isinstance(layer, pytmx.TiledObjectGroup))", "response": "This must return layer objects\n\n                and pytmx. TiledObjectGroup objects\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tile_images_by_rect(self, rect):\n\n        def rev(seq, start, stop):\n            if start < 0:\n                start = 0\n            return enumerate(seq[start:stop + 1], start)\n\n        x1, y1, x2, y2 = rect_to_bb(rect)\n        images = self.tmx.images\n        layers = self.tmx.layers\n        at = self._animated_tile\n        tracked_gids = self._tracked_gids\n        anim_map = self._animation_map\n        track = bool(self._animation_queue)\n\n        for l in self.tmx.visible_tile_layers:\n            for y, row in rev(layers[l].data, y1, y2):\n                for x, gid in [i for i in rev(row, x1, x2) if i[1]]:\n                    # since the tile has been queried, assume it wants to be checked\n                    # for animations sometime in the future\n                    if track and gid in tracked_gids:\n                        anim_map[gid].positions.add((x, y, l))\n\n                    try:\n                        # animated, so return the correct frame\n                        yield x, y, l, at[(x, y, l)]\n\n                    except KeyError:\n\n                        # not animated, so return surface from data, if any\n                        yield x, y, l, images[gid]", "response": "Yields the tile images that are in the given rect."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef move_back(self, dt):\n        self._position = self._old_position\n        self.rect.topleft = self._position\n        self.feet.midbottom = self.rect.midbottom", "response": "move the sprite back in the specified time frame"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_input(self):\n        poll = pygame.event.poll\n\n        event = poll()\n        while event:\n            if event.type == QUIT:\n                self.running = False\n                break\n\n            elif event.type == KEYDOWN:\n                if event.key == K_ESCAPE:\n                    self.running = False\n                    break\n\n                elif event.key == K_EQUALS:\n                    self.map_layer.zoom += .25\n\n                elif event.key == K_MINUS:\n                    value = self.map_layer.zoom - .25\n                    if value > 0:\n                        self.map_layer.zoom = value\n\n            # this will be handled if the window is resized\n            elif event.type == VIDEORESIZE:\n                init_screen(event.w, event.h)\n                self.map_layer.set_size((event.w, event.h))\n\n            event = poll()\n\n        # using get_pressed is slightly less accurate than testing for events\n        # but is much easier to use.\n        pressed = pygame.key.get_pressed()\n        if pressed[K_UP]:\n            self.hero.velocity[1] = -HERO_MOVE_SPEED\n        elif pressed[K_DOWN]:\n            self.hero.velocity[1] = HERO_MOVE_SPEED\n        else:\n            self.hero.velocity[1] = 0\n\n        if pressed[K_LEFT]:\n            self.hero.velocity[0] = -HERO_MOVE_SPEED\n        elif pressed[K_RIGHT]:\n            self.hero.velocity[0] = HERO_MOVE_SPEED\n        else:\n            self.hero.velocity[0] = 0", "response": "Handle pygame input events in the map layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, dt):\n        self.group.update(dt)\n\n        # check if the sprite's feet are colliding with wall\n        # sprite must have a rect called feet, and move_back method,\n        # otherwise this will fail\n        for sprite in self.group.sprites():\n            if sprite.feet.collidelist(self.walls) > -1:\n                sprite.move_back(dt)", "response": "Update the group and all the sprites that occur over time dt."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the game loop for the current object.", "response": "def run(self):\n        \"\"\" Run the game loop\n        \"\"\"\n        clock = pygame.time.Clock()\n        self.running = True\n\n        from collections import deque\n        times = deque(maxlen=30)\n\n        try:\n            while self.running:\n                dt = clock.tick() / 1000.\n                times.append(clock.get_fps())\n                # print(sum(times)/len(times))\n\n                self.handle_input()\n                self.update(dt)\n                self.draw(screen)\n                pygame.display.flip()\n\n        except KeyboardInterrupt:\n            self.running = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing all sprites and map onto the surface.", "response": "def draw(self, surface):\n        \"\"\" Draw all sprites and map onto the surface\n\n        :param surface: pygame surface to draw to\n        :type surface: pygame.surface.Surface\n        \"\"\"\n        ox, oy = self._map_layer.get_center_offset()\n\n        new_surfaces = list()\n        spritedict = self.spritedict\n        gl = self.get_layer_of_sprite\n        new_surfaces_append = new_surfaces.append\n\n        for spr in self.sprites():\n            new_rect = spr.rect.move(ox, oy)\n            try:\n                new_surfaces_append((spr.image, new_rect, gl(spr), spr.blendmode))\n            except AttributeError:  # generally should only fail when no blendmode available\n                new_surfaces_append((spr.image, new_rect, gl(spr)))\n            spritedict[spr] = new_rect\n\n        self.lostsprites = []\n        return self._map_layer.draw(surface, surface.get_rect(), new_surfaces)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the buffers to draw the cache tile drawing", "response": "def _initialize_buffers(self, view_size):\n        \"\"\" Create the buffers to cache tile drawing\n\n        :param view_size: (int, int): size of the draw area\n        :return: None\n        \"\"\"\n        import math\n        from pygame import Rect\n\n        tw, th = self.data.tile_size\n        mw, mh = self.data.map_size\n        buffer_tile_width = int(math.ceil(view_size[0] / tw) + 2) * 2\n        buffer_tile_height = int(math.ceil(view_size[1] / th) + 2) * 2\n        buffer_pixel_size = buffer_tile_width * tw, buffer_tile_height * th\n\n        self.map_rect = Rect(0, 0, mw * tw, mh * th)\n        self.view_rect.size = view_size\n        self._tile_view = Rect(0, 0, buffer_tile_width, buffer_tile_height)\n        self._redraw_cutoff = 1  # TODO: optimize this value\n        self._create_buffers(view_size, buffer_pixel_size)\n        self._half_width = view_size[0] // 2\n        self._half_height = view_size[1] // 2\n        self._x_offset = 0\n        self._y_offset = 0\n\n        self.redraw_tiles()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nflushes the tile queue to the buffer.", "response": "def _flush_tile_queue(self):\n        \"\"\" Blits (x, y, layer) tuples to buffer from iterator\n        \"\"\"\n        iterator = self._tile_queue\n        surface_blit = self._buffer.blit\n        map_get = self._animation_map.get\n\n        bw, bh = self._buffer.get_size()\n        bw /= 2\n\n        tw, th = self.data.tile_size\n        twh = tw // 2\n        thh = th // 2\n\n        for x, y, l, tile, gid in iterator:\n            tile = map_get(gid, tile)\n            x -= self._tile_view.left\n            y -= self._tile_view.top\n\n            # iso => cart\n            iso_x = ((x - y) * twh) + bw\n            iso_y = ((x + y) * thh)\n            surface_blit(tile, (iso_x, iso_y))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncenter the map on a map pixel", "response": "def center(self, coords):\n        \"\"\" center the map on a \"map pixel\"\n        \"\"\"\n        x, y = [round(i, 0) for i in coords]\n        self.view_rect.center = x, y\n\n        tw, th = self.data.tile_size\n\n        left, ox = divmod(x, tw)\n        top, oy = divmod(y, th)\n\n        vec = int(ox / 2), int(oy)\n\n        iso = vector2_to_iso(vec)\n        self._x_offset = iso[0]\n        self._y_offset = iso[1]\n\n        print(self._tile_view.size)\n        print(self._buffer.get_size())\n\n        # center the buffer on the screen\n        self._x_offset += (self._buffer.get_width() - self.view_rect.width) // 2\n        self._y_offset += (self._buffer.get_height() - self.view_rect.height) // 4\n\n        # adjust the view if the view has changed without a redraw\n        dx = int(left - self._tile_view.left)\n        dy = int(top - self._tile_view.top)\n        view_change = max(abs(dx), abs(dy))\n\n        # force redraw every time: edge queuing not supported yet\n        self._redraw_cutoff = 0\n\n        if view_change and (view_change <= self._redraw_cutoff):\n            self._buffer.scroll(-dx * tw, -dy * th)\n            self._tile_view.move_ip(dx, dy)\n            self._queue_edge_tiles(dx, dy)\n            self._flush_tile_queue()\n\n        elif view_change > self._redraw_cutoff:\n            # logger.info('scrolling too quickly.  redraw forced')\n            self._tile_view.move_ip(dx, dy)\n            self.redraw_tiles()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef advance(self, last_time):\n        # advance the animation frame index, looping by default\n        if self.index == len(self.frames) - 1:\n            self.index = 0\n        else:\n            self.index += 1\n\n        # set the timer for the next advance\n        next_frame = self.frames[self.index]\n        self.next = next_frame.duration + last_time\n        return next_frame", "response": "Advance the frame and set the timer for next frame"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of Odoo command line arguments from the Click context.", "response": "def get_odoo_args(self, ctx):\n        \"\"\"Return a list of Odoo command line arguments from the Click context.\"\"\"\n        config = ctx.params.get(\"config\")\n        addons_path = ctx.params.get(\"addons_path\")\n        database = ctx.params.get(\"database\")\n        log_level = ctx.params.get(\"log_level\")\n        logfile = ctx.params.get(\"logfile\")\n\n        odoo_args = []\n\n        if config:\n            odoo_args.extend([\"--config\", config])\n        if addons_path:\n            odoo_args.extend([\"--addons-path\", addons_path])\n        if database:\n            odoo_args.extend([\"--database\", database])\n        if log_level:\n            odoo_args.extend([\"--log-level\", log_level])\n        if logfile:\n            odoo_args.extend([\"--logfile\", logfile])\n\n        return odoo_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_query(search_term, querytype='AdvancedKeywordQuery'):\n    ''' Repackage strings into a search dictionary\n\n    This function takes a list of search terms and specifications\n    and repackages it as a dictionary object that can be used to conduct a search\n\n    Parameters\n    ----------\n    search_term : str\n\n        The specific term to search in the database. For specific query types,\n        the strings that will yield valid results are limited to:\n\n        'HoldingsQuery' : A Ggeneral search of the metadata associated with PDB IDs\n\n        'ExpTypeQuery' : Experimental Method such as 'X-RAY', 'SOLID-STATE NMR', etc\n\n        'AdvancedKeywordQuery' : Any string that appears in the title or abstract\n\n        'StructureIdQuery' :  Perform a search for a specific Structure ID\n\n        'ModifiedStructuresQuery' : Search for related structures\n\n        'AdvancedAuthorQuery' : Search by the names of authors associated with entries\n\n        'MotifQuery' : Search for a specific motif\n\n        'NoLigandQuery' : Find full list of PDB IDs without free ligrands\n\n    querytype : str\n\n        The type of query to perform, the easiest is an AdvancedKeywordQuery but more\n        specific types of searches may also be performed\n\n    Returns\n    -------\n\n    scan_params : dict\n\n        A dictionary representing the query\n\n\n    Examples\n    --------\n    This method usually gets used in tandem with do_search\n\n    >>> a = make_query('actin network')\n    >>> print (a)\n    {'orgPdbQuery': {'description': 'Text Search for: actin',\n    'keywords': 'actin',\n    'queryType': 'AdvancedKeywordQuery'}}\n\n\n    >>> search_dict = make_query('actin network')\n    >>> found_pdbs = do_search(search_dict)\n    >>> print(found_pdbs)\n    ['1D7M', '3W3D', '4A7H', '4A7L', '4A7N']\n\n    >>> search_dict = make_query('T[AG]AGGY',querytype='MotifQuery')\n    >>> found_pdbs = do_search(search_dict)\n    >>> print(found_pdbs)\n    ['3LEZ', '3SGH', '4F47']\n\n    '''\n    assert querytype in {'HoldingsQuery', 'ExpTypeQuery',\n                         'AdvancedKeywordQuery','StructureIdQuery',\n                         'ModifiedStructuresQuery', 'AdvancedAuthorQuery', 'MotifQuery',\n                         'NoLigandQuery', 'PubmedIdQuery'\n                        }, 'Query type %s not supported yet' % querytype\n\n    query_params = dict()\n    query_params['queryType'] = querytype\n\n    if querytype=='AdvancedKeywordQuery':\n        query_params['description'] = 'Text Search for: '+ search_term\n        query_params['keywords'] = search_term\n\n    elif querytype=='NoLigandQuery':\n        query_params['haveLigands'] = 'yes'\n\n    elif querytype=='AdvancedAuthorQuery':\n        query_params['description'] = 'Author Name: '+ search_term\n        query_params['searchType'] = 'All Authors'\n        query_params['audit_author.name'] = search_term\n        query_params['exactMatch'] = 'false'\n\n    elif querytype=='MotifQuery':\n        query_params['description'] = 'Motif Query For: '+ search_term\n        query_params['motif'] = search_term\n\n    # search for a specific structure\n    elif querytype in ['StructureIdQuery','ModifiedStructuresQuery']:\n        query_params['structureIdList'] = search_term\n\n    elif querytype=='ExpTypeQuery':\n        query_params['experimentalMethod'] = search_term\n        query_params['description'] = 'Experimental Method Search : Experimental Method='+ search_term\n        query_params['mvStructure.expMethod.value']= search_term\n\n    elif querytype=='PubmedIdQuery':\n        query_params['description'] = 'Pubmed Id Search for Pubmed Id '+ search_term\n        query_params['pubMedIdList'] = search_term\n\n\n    scan_params = dict()\n    scan_params['orgPdbQuery'] = query_params\n\n\n    return scan_params", "response": "This function will repackage strings into a search dictionary and return the results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting dict() to XML object an then send query to the RCSB PDB This function takes a valid query dict() object, converts it to XML, and then sends a request to the PDB for a list of IDs corresponding to search results Parameters ---------- scan_params : dict A dictionary of query attributes to use for the search of the PDB Returns ------- idlist : list A list of PDB ids returned by the search Examples -------- This method usually gets used in tandem with make_query >>> a = make_query('actin network') >>> print (a) {'orgPdbQuery': {'description': 'Text Search for: actin', 'keywords': 'actin', 'queryType': 'AdvancedKeywordQuery'}} >>> search_dict = make_query('actin network') >>> found_pdbs = do_search(search_dict) >>> print(found_pdbs) ['1D7M', '3W3D', '4A7H', '4A7L', '4A7N'] >>> search_dict = make_query('T[AG]AGGY',querytype='MotifQuery') >>> found_pdbs = do_search(search_dict) >>> print(found_pdbs) ['3LEZ', '3SGH', '4F47']", "response": "def do_search(scan_params):\n    '''Convert dict() to XML object an then send query to the RCSB PDB\n\n    This function takes a valid query dict() object, converts it to XML,\n    and then sends a request to the PDB for a list of IDs corresponding to search results\n\n    Parameters\n    ----------\n\n    scan_params : dict\n        A dictionary of query attributes to use for\n        the search of the PDB\n\n\n    Returns\n    -------\n\n    idlist : list\n        A list of PDB ids returned by the search\n\n    Examples\n    --------\n    This method usually gets used in tandem with make_query\n\n    >>> a = make_query('actin network')\n    >>> print (a)\n    {'orgPdbQuery': {'description': 'Text Search for: actin',\n    'keywords': 'actin',\n    'queryType': 'AdvancedKeywordQuery'}}\n\n\n    >>> search_dict = make_query('actin network')\n    >>> found_pdbs = do_search(search_dict)\n    >>> print(found_pdbs)\n    ['1D7M', '3W3D', '4A7H', '4A7L', '4A7N']\n\n    >>> search_dict = make_query('T[AG]AGGY',querytype='MotifQuery')\n    >>> found_pdbs = do_search(search_dict)\n    >>> print(found_pdbs)\n    ['3LEZ', '3SGH', '4F47']\n    '''\n\n    url = 'http://www.rcsb.org/pdb/rest/search'\n\n    queryText = xmltodict.unparse(scan_params, pretty=False)\n    queryText = queryText.encode()\n\n    req = urllib.request.Request(url, data=queryText)\n    f = urllib.request.urlopen(req)\n    result = f.read()\n\n    if not result:\n        warnings.warn('No results were obtained for this search')\n\n    idlist = str(result)\n    idlist =idlist.split('\\\\n')\n    idlist[0] = idlist[0][-4:]\n    kk = idlist.pop(-1)\n\n    return idlist"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_protsym_search(point_group, min_rmsd=0.0, max_rmsd=7.0):\n    '''Performs a protein symmetry search of the PDB\n\n    This function can search the Protein Data Bank based on how closely entries\n    match the user-specified symmetry group\n\n    Parameters\n    ----------\n\n    point_group : str\n        The name of the symmetry point group to search. This includes all the standard\n        abbreviations for symmetry point groups (e.g., C1, C2, D2, T, O, I, H, A1)\n\n    min_rmsd : float\n        The smallest allowed total deviation (in Angstroms) for a result to be classified\n        as having a matching symmetry\n\n    max_rmsd : float\n        The largest allowed total deviation (in Angstroms) for a result to be classified\n        as having a matching symmetry\n\n\n    Returns\n    -------\n\n    idlist : list of strings\n        A list of PDB IDs resulting from the search\n\n    Examples\n    --------\n\n    >>> kk = do_protsym_search('C9', min_rmsd=0.0, max_rmsd=1.0)\n    >>> print(kk[:5])\n    ['1KZU', '1NKZ', '2FKW', '3B8M', '3B8N']\n\n    '''\n\n    query_params = dict()\n    query_params['queryType'] = 'PointGroupQuery'\n    query_params['rMSDComparator'] = 'between'\n\n    query_params['pointGroup'] = point_group\n    query_params['rMSDMin'] = min_rmsd\n    query_params['rMSDMax'] = max_rmsd\n\n    scan_params = dict()\n    scan_params['orgPdbQuery'] = query_params\n    idlist =  do_search(scan_params)\n    return idlist", "response": "Performs a protein symmetry search of the PDB file containing the specified point group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all():\n\n    url = 'http://www.rcsb.org/pdb/rest/getCurrent'\n\n    req = urllib.request.Request(url)\n    f = urllib.request.urlopen(req)\n    result = f.read()\n    assert result\n\n    kk = str(result)\n\n    p = re.compile('structureId=\\\"....\"')\n    matches = p.findall(str(result))\n    out = list()\n    for item in matches:\n        out.append(item[-5:-1])\n\n    return out", "response": "Returns a list of all of the PDB entries currently in the RCSB Protein Data Bank"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlook up all information about a given PDB ID and return a dictionary of bare XML structures.", "response": "def get_info(pdb_id, url_root='http://www.rcsb.org/pdb/rest/describeMol?structureId='):\n    '''Look up all information about a given PDB ID\n\n    Parameters\n    ----------\n\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    url_root : string\n        The string root of the specific url for the request type\n\n    Returns\n    -------\n\n    out : OrderedDict\n        An ordered dictionary object corresponding to bare xml\n\n    '''\n\n    url = url_root + pdb_id\n    req = urllib.request.Request(url)\n    f = urllib.request.urlopen(req)\n    result = f.read()\n    assert result\n\n    out = xmltodict.parse(result,process_namespaces=True)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the full PDB file associated with a PDB_ID", "response": "def get_pdb_file(pdb_id, filetype='pdb', compression=False):\n    '''Get the full PDB file associated with a PDB_ID\n\n    Parameters\n    ----------\n\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    filetype: string\n        The file type.\n        'pdb' is the older file format,\n        'cif' is the newer replacement.\n        'xml' an also be obtained and parsed using the various xml tools included in PyPDB\n        'structfact' retrieves structure factors (only available for certain PDB entries)\n\n    compression : bool\n        Retrieve a compressed (gz) version of the file\n\n    Returns\n    -------\n\n    result : string\n        The string representing the full PDB file in the given format\n\n    Examples\n    --------\n    >>> pdb_file = get_pdb_file('4lza', filetype='cif', compression=True)\n    >>> print(pdb_file[:200])\n    data_4LZA\n    #\n    _entry.id   4LZA\n    #\n    _audit_conform.dict_name       mmcif_pdbx.dic\n    _audit_conform.dict_version    4.032\n    _audit_conform.dict_location   http://mmcif.pdb.org/dictionaries/ascii/mmcif_pdbx\n\n    '''\n\n    full_url = \"https://files.rcsb.org/download/\"\n\n    full_url += pdb_id\n\n    if (filetype == 'structfact'):\n        full_url += \"-sf.cif\"\n    else:\n        full_url += \".\" + filetype\n\n    if compression:\n        full_url += \".gz\"\n    else:\n        pass\n\n\n    req = urllib.request.Request(full_url)\n    f = urllib.request.urlopen(req)\n    result = f.read()\n\n    if not compression:\n        result = result.decode('ascii')\n    else:\n        pass\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_info(pdb_id):\n    '''A wrapper for get_info that cleans up the output slighly\n\n    Parameters\n    ----------\n\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    Returns\n    -------\n\n    out : dict\n        A dictionary containing all the information stored in the entry\n\n    Examples\n    --------\n\n    >>> all_info = get_all_info('4lza')\n    >>> print(all_info)\n    {'polymer': {'macroMolecule': {'@name': 'Adenine phosphoribosyltransferase', '\n    accession': {'@id': 'B0K969'}}, '@entityNr': '1', '@type': 'protein',\n    'polymerDescription': {'@description': 'Adenine phosphoribosyltransferase'},\n    'synonym': {'@name': 'APRT'}, '@length': '195', 'enzClass': {'@ec': '2.4.2.7'},\n    'chain': [{'@id': 'A'}, {'@id': 'B'}],\n    'Taxonomy': {'@name': 'Thermoanaerobacter pseudethanolicus ATCC 33223',\n    '@id': '340099'}, '@weight': '22023.9'}, 'id': '4LZA'}\n\n    >>> results = get_all_info('2F5N')\n    >>> first_polymer = results['polymer'][0]\n    >>> first_polymer['polymerDescription']\n    {'@description': \"5'-D(*AP*GP*GP*TP*AP*GP*AP*CP*CP*TP*GP*GP*AP*CP*GP*C)-3'\"}\n\n    '''\n    out = to_dict( get_info(pdb_id) )['molDescription']['structureId']\n    out = remove_at_sign(out)\n    return out", "response": "A wrapper for get_info that cleans up the output slighly\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_raw_blast(pdb_id, output_form='HTML', chain_id='A'):\n    '''Look up full BLAST page for a given PDB ID\n\n    get_blast() uses this function internally\n\n    Parameters\n    ----------\n\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    chain_id : string\n        A single character designating the chain ID of interest\n\n\n    output_form : string\n        TXT, HTML, or XML formatting of the outputs\n\n    Returns\n    -------\n\n    out : OrderedDict\n        An ordered dictionary object corresponding to bare xml\n\n    '''\n\n    url_root = 'http://www.rcsb.org/pdb/rest/getBlastPDB2?structureId='\n    url = url_root + pdb_id + '&chainId='+ chain_id +'&outputFormat=' + output_form\n    req = urllib.request.Request(url)\n    f = urllib.request.urlopen(req)\n    result = f.read()\n    result = result.decode('unicode_escape')\n    assert result\n\n    return result", "response": "This function returns the raw BLAST page for a given PDB ID and chain ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans up HTML BLAST results This function requires BeautifulSoup and the re module It goes throught the complicated output returned by the BLAST search and provides a list of matches, as well as the raw text file showing the alignments for each of the matches. This function works best with HTML formatted Inputs ------ get_blast() uses this function internally Parameters ---------- blast_string : str A complete webpage of standard BLAST results Returns ------- out : 2-tuple A tuple consisting of a list of PDB matches, and a list of their alignment text files (unformatted)", "response": "def parse_blast(blast_string):\n    '''Clean up HTML BLAST results\n\n    This function requires BeautifulSoup and the re module\n    It goes throught the complicated output returned by the BLAST\n    search and provides a list of matches, as well as the raw\n    text file showing the alignments for each of the matches.\n\n    This function works best with HTML formatted Inputs\n    ------\n\n    get_blast() uses this function internally\n\n    Parameters\n    ----------\n\n    blast_string : str\n        A complete webpage of standard BLAST results\n\n    Returns\n    -------\n\n    out : 2-tuple\n        A tuple consisting of a list of PDB matches, and a list\n        of their alignment text files (unformatted)\n\n\n    '''\n\n    soup = BeautifulSoup(str(blast_string), \"html.parser\")\n\n    all_blasts = list()\n    all_blast_ids = list()\n\n    pattern = '></a>....:'\n    prog = re.compile(pattern)\n\n    for item in soup.find_all('pre'):\n        if len(item.find_all('a'))==1:\n            all_blasts.append(item)\n            blast_id = re.findall(pattern, str(item) )[0][-5:-1]\n            all_blast_ids.append(blast_id)\n\n    out = (all_blast_ids, all_blasts)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_blast2(pdb_id, chain_id='A', output_form='HTML'):\n    '''Alternative way to look up BLAST for a given PDB ID. This function is a wrapper\n    for get_raw_blast and parse_blast\n\n    Parameters\n    ----------\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    chain_id : string\n        A single character designating the chain ID of interest\n\n    output_form : string\n        TXT, HTML, or XML formatting of the BLAST page\n\n    Returns\n    -------\n\n    out : 2-tuple\n        A tuple consisting of a list of PDB matches, and a list\n        of their alignment text files (unformatted)\n\n\n    Examples\n    --------\n\n    >>> blast_results = get_blast2('2F5N', chain_id='A', output_form='HTML')\n    >>> print('Total Results: ' + str(len(blast_results[0])) +'\\n')\n    >>> print(blast_results[1][0])\n    Total Results: 84\n    <pre>\n    &gt;<a name=\"45354\"></a>2F5P:3:A|pdbid|entity|chain(s)|sequence\n              Length = 274\n     Score =  545 bits (1404), Expect = e-155,   Method: Composition-based stats.\n     Identities = 274/274 (100%), Positives = 274/274 (100%)\n    Query: 1   MPELPEVETIRRTLLPLIVGKTIEDVRIFWPNIIRHPRDSEAFAARMIGQTVRGLERRGK 60\n               MPELPEVETIRRTLLPLIVGKTIEDVRIFWPNIIRHPRDSEAFAARMIGQTVRGLERRGK\n    Sbjct: 1   MPELPEVETIRRTLLPLIVGKTIEDVRIFWPNIIRHPRDSEAFAARMIGQTVRGLERRGK 60\n    ...\n\n    '''\n\n    raw_results = get_raw_blast(pdb_id, chain_id=chain_id, output_form=output_form)\n    out = parse_blast(raw_results)\n\n    return out", "response": "Alternative way to look up BLAST for a given PDB ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef describe_pdb(pdb_id):\n    out = get_info(pdb_id, url_root = 'http://www.rcsb.org/pdb/rest/describePDB?structureId=')\n    out = to_dict(out)\n    out = remove_at_sign(out['PDBdescription']['PDB'])\n    return out", "response": "Get description and metadata of a PDB entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_entity_info(pdb_id):\n    out = get_info(pdb_id, url_root = 'http://www.rcsb.org/pdb/rest/getEntityInfo?structureId=')\n    out = to_dict(out)\n    return remove_at_sign( out['entityInfo']['PDB'] )", "response": "Return the information about a given PDB ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ligands(pdb_id):\n    out = get_info(pdb_id, url_root = 'http://www.rcsb.org/pdb/rest/ligandInfo?structureId=')\n    out = to_dict(out)\n    return remove_at_sign(out['structureId'])", "response": "Return a list of ligands associated with a given PDB ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_gene_onto(pdb_id):\n    out = get_info(pdb_id, url_root = 'http://www.rcsb.org/pdb/rest/goTerms?structureId=')\n    out = to_dict(out)\n    if not out['goTerms']:\n        return None\n    out = remove_at_sign(out['goTerms'])\n    return out", "response": "Return ligands of given PDB_ID"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the sequence cluster of a PDB ID plus a chain.", "response": "def get_seq_cluster(pdb_id_chain):\n    \"\"\"Get the sequence cluster of a PDB ID plus a pdb_id plus a chain,\n\n    Parameters\n    ----------\n\n    pdb_id_chain : string\n        A string denoting a 4 character PDB ID plus a one character chain\n        offset with a dot: XXXX.X, as in 2F5N.A\n\n    Returns\n    -------\n\n    out : dict\n        A dictionary containing the sequence cluster associated with the PDB\n        entry and chain\n\n    Examples\n    --------\n\n    >>> sclust = get_seq_cluster('2F5N.A')\n    >>> print(sclust['pdbChain'][:10])\n    [{'@name': '4PD2.A', '@rank': '1'},\n     {'@name': '3U6P.A', '@rank': '2'},\n     {'@name': '4PCZ.A', '@rank': '3'},\n     {'@name': '3GPU.A', '@rank': '4'},\n     {'@name': '3JR5.A', '@rank': '5'},\n     {'@name': '3SAU.A', '@rank': '6'},\n     {'@name': '3GQ4.A', '@rank': '7'},\n     {'@name': '1R2Z.A', '@rank': '8'},\n     {'@name': '3U6E.A', '@rank': '9'},\n     {'@name': '2XZF.A', '@rank': '10'}]\n\n    \"\"\"\n\n    url_root = 'http://www.rcsb.org/pdb/rest/sequenceCluster?structureId='\n    out = get_info(pdb_id_chain, url_root = url_root)\n    out = to_dict(out)\n    return remove_at_sign(out['sequenceCluster'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the BLAST search results for a given PDB ID and chain ID.", "response": "def get_blast(pdb_id, chain_id='A'):\n    \"\"\"\n    Return BLAST search results for a given PDB ID\n    The key of the output dict())that outputs the full search results is\n    'BlastOutput_iterations'\n\n    To get a list of just the results without the metadata of the search use:\n    hits = full_results['BlastOutput_iterations']['Iteration']['Iteration_hits']['Hit']\n\n    Parameters\n    ----------\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    chain_id : string\n        A single character designating the chain ID of interest\n\n\n    Returns\n    -------\n\n    out : dict()\n        A nested dict() consisting of the BLAST search results and all associated metadata\n        If you just want the hits, look under four levels of keys:\n        results['BlastOutput_iterations']['Iteration']['Iteration_hits']['Hit']\n\n    Examples\n    --------\n\n    >>> blast_results = get_blast('2F5N', chain_id='A')\n    >>> just_hits = blast_results['BlastOutput_iterations']['Iteration']['Iteration_hits']['Hit']\n    >>> print(just_hits[50]['Hit_hsps']['Hsp']['Hsp_hseq'])\n    PELPEVETVRRELEKRIVGQKIISIEATYPRMVL--TGFEQLKKELTGKTIQGISRRGKYLIFEIGDDFRLISHLRMEGKYRLATLDAPREKHDHL\n    TMKFADG-QLIYADVRKFGTWELISTDQVLPYFLKKKIGPEPTYEDFDEKLFREKLRKSTKKIKPYLLEQTLVAGLGNIYVDEVLWLAKIHPEKET\n    NQLIESSIHLLHDSIIEILQKAIKLGGSSIRTY-SALGSTGKMQNELQVYGKTGEKCSRCGAEIQKIKVAGRGTHFCPVCQQ\n\n\n    \"\"\"\n\n    raw_results = get_raw_blast(pdb_id, output_form='XML', chain_id=chain_id)\n\n    out = xmltodict.parse(raw_results, process_namespaces=True)\n    out = to_dict(out)\n    out = out['BlastOutput']\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns PFAM annotations of the given PDB ID.", "response": "def get_pfam(pdb_id):\n    \"\"\"Return PFAM annotations of given PDB_ID\n\n    Parameters\n    ----------\n\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    Returns\n    -------\n\n    out : dict\n        A dictionary containing the PFAM annotations for the specified PDB ID\n\n    Examples\n    --------\n\n    >>> pfam_info = get_pfam('2LME')\n    >>> print(pfam_info)\n    {'pfamHit': {'@pfamAcc': 'PF03895.10', '@pfamName': 'YadA_anchor',\n    '@structureId': '2LME', '@pdbResNumEnd': '105', '@pdbResNumStart': '28',\n    '@pfamDesc': 'YadA-like C-terminal region', '@eValue': '5.0E-22', '@chainId': 'A'}}\n\n    \"\"\"\n    out = get_info(pdb_id, url_root = 'http://www.rcsb.org/pdb/rest/hmmer?structureId=')\n    out = to_dict(out)\n    if not out['hmmer3']:\n        return dict()\n    return remove_at_sign(out['hmmer3'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning cluster related web services of a given PDB ID.", "response": "def get_clusters(pdb_id):\n    \"\"\"Return cluster related web services of given PDB_ID\n\n    Parameters\n    ----------\n\n    pdb_id : string\n        A 4 character string giving a pdb entry of interest\n\n    Returns\n    -------\n\n    out : dict\n        A dictionary containing the representative clusters for the specified PDB ID\n\n    Examples\n    --------\n\n    >>> clusts = get_clusters('4hhb.A')\n    >>> print(clusts)\n    {'pdbChain': {'@name': '2W72.A'}}\n\n    \"\"\"\n    out = get_info(pdb_id, url_root = 'http://www.rcsb.org/pdb/rest/representatives?structureId=')\n    out = to_dict(out)\n    return remove_at_sign(out['representatives'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a generator of the results returned by a search of the protein data bank. This generator is used internally. Parameters ---------- search_term : str The search keyword field : str The type of information to record about each entry Examples -------- >>> result_gen = find_results_gen('bleb') >>> pprint.pprint([item for item in result_gen][:5]) ['MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456Y BOUND WITH MGADP-BEFX', 'MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456Y BOUND WITH MGADP-ALF4', 'DICTYOSTELIUM DISCOIDEUM MYOSIN II MOTOR DOMAIN S456E WITH BOUND MGADP-BEFX', 'MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456E BOUND WITH MGADP-ALF4', 'The structural basis of blebbistatin inhibition and specificity for myosin ' 'II']", "response": "def find_results_gen(search_term, field='title'):\n    '''\n    Return a generator of the results returned by a search of\n    the protein data bank. This generator is used internally.\n\n    Parameters\n    ----------\n\n    search_term : str\n        The search keyword\n\n    field : str\n        The type of information to record about each entry\n\n    Examples\n    --------\n\n    >>> result_gen = find_results_gen('bleb')\n    >>> pprint.pprint([item for item in result_gen][:5])\n    ['MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456Y BOUND WITH MGADP-BEFX',\n     'MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456Y BOUND WITH MGADP-ALF4',\n     'DICTYOSTELIUM DISCOIDEUM MYOSIN II MOTOR DOMAIN S456E WITH BOUND MGADP-BEFX',\n     'MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456E BOUND WITH MGADP-ALF4',\n     'The structural basis of blebbistatin inhibition and specificity for myosin '\n     'II']\n\n    '''\n    scan_params = make_query(search_term, querytype='AdvancedKeywordQuery')\n    search_result_ids = do_search(scan_params)\n\n    all_titles = []\n    for pdb_result in search_result_ids:\n        result= describe_pdb(pdb_result)\n        if field in result.keys():\n            yield result[field]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the PDB with a search term and field while respecting the query frequency limitations of the API. Parameters ---------- search_term : str The search keyword field : str The type of information to record about each entry max_results : int The maximum number of results to search through when determining the top results sleep_time : float Time (in seconds) to wait between requests. If this number is too small the API will stop working, but it appears to vary among different systems Returns ------- all_data_raw : list of str", "response": "def parse_results_gen(search_term, field='title', max_results = 100, sleep_time=.1):\n    '''\n    Query the PDB with a search term and field while respecting the query frequency\n     limitations of the API.\n\n    Parameters\n    ----------\n\n    search_term : str\n        The search keyword\n\n    field : str\n        The type of information to record about each entry\n\n    max_results : int\n        The maximum number of results to search through when \n        determining the top results\n\n    sleep_time : float\n        Time (in seconds) to wait between requests. If this number is too small\n        the API will stop working, but it appears to vary among different systems\n\n\n    Returns\n    -------\n\n    all_data_raw : list of str\n\n    '''\n\n    if max_results*sleep_time > 30:\n        warnings.warn(\"Because of API limitations, this function\\\n        will take at least \" + str(max_results*sleep_time) + \" seconds to return results.\\\n        If you need greater speed, try modifying the optional argument sleep_time=.1, (although \\\n        this may cause the search to time out)\" )\n\n    all_data_raw = find_results_gen(search_term, field=field)\n    all_data =list()\n    while len(all_data) < max_results:\n        all_data.append(all_data_raw.send(None))\n        time.sleep(sleep_time)\n\n    return all_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_papers(search_term, **kwargs):\n    '''\n    Return an ordered list of the top papers returned by a keyword search of\n    the RCSB PDB\n\n    Parameters\n    ----------\n\n    search_term : str\n        The search keyword\n\n    max_results : int\n        The maximum number of results to return\n\n    Returns\n    -------\n\n    all_papers : list of strings\n        A descending-order list containing the top papers associated with\n        the search term in the PDB\n\n    Examples\n    --------\n\n    >>> matching_papers = find_papers('crispr',max_results=3)\n    >>> print(matching_papers)\n    ['Crystal structure of a CRISPR-associated protein from thermus thermophilus',\n    'CRYSTAL STRUCTURE OF HYPOTHETICAL PROTEIN SSO1404 FROM SULFOLOBUS SOLFATARICUS P2',\n    'NMR solution structure of a CRISPR repeat binding protein']\n\n    '''\n    all_papers = parse_results_gen(search_term, field='title', **kwargs)\n    return remove_dupes(all_papers)", "response": "Returns an ordered list of the top papers associated with the search term in the RCSB PDB and returns them as a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an ordered list of the top authors returned by a keyword search of the RCSB PDB", "response": "def find_authors(search_term, **kwargs):\n    '''Return an ordered list of the top authors returned by a keyword search of\n    the RCSB PDB\n\n    This function is based on the number of unique PDB entries a given author has\n    his or her name associated with, and not author order or the ranking of the\n    entry in the keyword search results. So if an author tends to publish on topics\n    related to the search_term a lot, even if those papers are not the best match for\n    the exact search, he or she will have priority in this function over an author\n    who wrote the one paper that is most relevant to the search term. For the latter\n    option, just do a standard keyword search using do_search.\n\n    Parameters\n    ----------\n\n    search_term : str\n        The search keyword\n\n    max_results : int\n        The maximum number of results to return\n\n    Returns\n    -------\n\n    out : list of str\n\n\n    Examples\n    --------\n\n    >>> top_authors = find_authors('crispr',max_results=100)\n    >>> print(top_authors[:10])\n    ['Doudna, J.A.', 'Jinek, M.', 'Ke, A.', 'Li, H.', 'Nam, K.H.']\n\n    '''\n\n    all_individuals = parse_results_gen(search_term, field='citation_authors', **kwargs)\n\n    full_author_list = []\n    for individual in all_individuals:\n        individual = individual.replace('.,', '.;')\n        author_list_clean = [x.strip() for x in individual.split(';')]\n        full_author_list+=author_list_clean\n\n    out = list(chain.from_iterable(repeat(ii, c) for ii,c in Counter(full_author_list).most_common()))\n\n    return remove_dupes(out)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a list of PDB IDs look up their associated species and return the list of taxonomy names or classifictions associated with each entry.", "response": "def list_taxa(pdb_list, sleep_time=.1):\n    '''Given a list of PDB IDs, look up their associated species\n\n    This function digs through the search results returned\n    by the get_all_info() function and returns any information on\n    taxonomy included within the description.\n\n    The PDB website description of each entry includes the name\n    of the species (and sometimes details of organ or body part)\n    for each protein structure sample.\n\n    Parameters\n    ----------\n\n    pdb_list : list of str\n        List of PDB IDs\n\n    sleep_time : float\n        Time (in seconds) to wait between requests. If this number is too small\n        the API will stop working, but it appears to vary among different systems\n\n    Returns\n    -------\n\n    taxa : list of str\n        A list of the names or classifictions of species\n        associated with entries\n\n    Examples\n    --------\n\n    >>> crispr_query = make_query('crispr')\n    >>> crispr_results = do_search(crispr_query)\n    >>> print(list_taxa(crispr_results[:10]))\n    ['Thermus thermophilus',\n     'Sulfolobus solfataricus P2',\n     'Hyperthermus butylicus DSM 5456',\n     'unidentified phage',\n     'Sulfolobus solfataricus P2',\n     'Pseudomonas aeruginosa UCBPP-PA14',\n     'Pseudomonas aeruginosa UCBPP-PA14',\n     'Pseudomonas aeruginosa UCBPP-PA14',\n     'Sulfolobus solfataricus',\n     'Thermus thermophilus HB8']\n\n\n    '''\n    \n    if len(pdb_list)*sleep_time > 30:\n        warnings.warn(\"Because of API limitations, this function\\\n        will take at least \" + str(len(pdb_list)*sleep_time) + \" seconds to return results.\\\n        If you need greater speed, try modifying the optional argument sleep_time=.1, (although \\\n        this may cause the search to time out)\" )\n    \n    taxa = []\n\n    for pdb_id in pdb_list:\n        all_info = get_all_info(pdb_id)\n        species_results = walk_nested_dict(all_info, 'Taxonomy', maxdepth=25,outputs=[])\n        first_result = walk_nested_dict(species_results,'@name',outputs=[])\n        if first_result:\n            taxa.append(first_result[-1])\n        else:\n            taxa.append('Unknown')\n\n        time.sleep(sleep_time)\n        \n    return taxa"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of PDB IDs look up their associated structure type and return a list of the structure types associated with each entry in the list.", "response": "def list_types(pdb_list, sleep_time=.1):\n    '''Given a list of PDB IDs, look up their associated structure type\n\n\n    Parameters\n    ----------\n\n    pdb_list : list of str\n        List of PDB IDs\n        \n    sleep_time : float\n        Time (in seconds) to wait between requests. If this number is too small\n        the API will stop working, but it appears to vary among different systems\n\n\n    Returns\n    -------\n\n    infotypes : list of str\n        A list of the structure types associated with each PDB\n        in the list. For many entries in the RCSB PDB, this defaults\n        to 'protein'\n\n    Examples\n    --------\n\n    >>> crispr_query = make_query('crispr')\n    >>> crispr_results = do_search(crispr_query)\n    >>> print(list_types(crispr_results[:5]))\n    ['protein', 'protein', 'protein', 'protein', 'protein']\n    '''\n    \n    if len(pdb_list)*sleep_time > 30:\n        warnings.warn(\"Because of API limitations, this function\\\n        will take at least \" + str(len(pdb_list)*sleep_time) + \" seconds to return results.\\\n        If you need greater speed, try modifying the optional argument sleep_time=.1, (although \\\n        this may cause the search to time out)\" )\n    \n    infotypes = []\n    for pdb_id in pdb_list:\n        all_info = get_all_info(pdb_id)\n        type_results = walk_nested_dict(all_info, '@type', maxdepth=25,outputs=[])\n        if type_results:\n            infotypes.append(type_results[-1])\n        else:\n            infotypes.append('Unknown')\n        time.sleep(sleep_time)\n        \n    return infotypes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_at_sign(kk):\n    '''Remove the '@' character from the beginning of key names in a dict()\n\n    Parameters\n    ----------\n\n    kk : dict\n        A dictionary containing keys with the @ character\n        (this pops up a lot in converted XML)\n\n    Returns\n    -------\n\n    kk : dict (modified in place)\n        A dictionary where the @ character has been removed\n\n    '''\n    tagged_keys = [thing for thing in kk.keys() if thing.startswith('@')]\n    for tag_key in tagged_keys:\n        kk[tag_key[1:]] = kk.pop(tag_key)\n\n    return kk", "response": "Removes the '@' character from the beginning of key names in a dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove duplicate entries from a list while preserving order", "response": "def remove_dupes(list_with_dupes):\n    '''Remove duplicate entries from a list while preserving order\n\n    This function uses Python's standard equivalence testing methods in\n    order to determine if two elements of a list are identical. So if in the list [a,b,c]\n    the condition a == b is True, then regardless of whether a and b are strings, ints,\n    or other, then b will be removed from the list: [a, c]\n\n    Parameters\n    ----------\n\n    list_with_dupes : list\n        A list containing duplicate elements\n\n    Returns\n    -------\n    out : list\n        The list with the duplicate entries removed by the order preserved\n\n\n    Examples\n    --------\n    >>> a = [1,3,2,4,2]\n    >>> print(remove_dupes(a))\n    [1,3,2,4]\n\n    '''\n    visited = set()\n    visited_add = visited.add\n    out = [ entry for entry in list_with_dupes if not (entry in visited or visited_add(entry))]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef walk_nested_dict(my_result, term, outputs=[], depth=0, maxdepth=25):\n    '''\n    For a nested dictionary that may itself comprise lists of \n    dictionaries of unknown length, determine if a key is anywhere\n    in any of the dictionaries using a depth-first search\n    \n    Parameters\n    ----------\n    \n    my_result : dict\n        A nested dict containing lists, dicts, and other objects as vals\n        \n    term : str\n        The name of the key stored somewhere in the tree\n    \n    maxdepth : int\n        The maximum depth to search the results tree\n        \n    depth : int\n        The depth of the search so far. \n        Users don't usually access this.\n        \n    outputs : list\n        All of the positive search results collected so far.\n        Users don't usually access this.\n        \n    Returns\n    -------\n    \n    outputs : list\n        All of the search results.\n    \n    '''\n    \n    if depth > maxdepth:\n        warnings.warn('Maximum recursion depth exceeded. Returned None for the search results,'+\n                      ' try increasing the maxdepth keyword argument.')\n        return None\n    \n\n    depth = depth + 1\n    \n    if type(my_result)==dict:\n        if term in my_result.keys():\n            outputs.append(my_result[term])\n\n        else:\n            new_results = list(my_result.values())\n            walk_nested_dict(new_results, term, outputs=outputs, depth=depth,maxdepth=maxdepth)\n    \n    elif type(my_result)==list:\n        for item in my_result:\n            walk_nested_dict(item, term, outputs=outputs, depth=depth,maxdepth=maxdepth)\n            \n    else:\n        pass\n        # dead leaf\n\n    # this conditional may not be necessary    \n    if outputs:\n        return outputs\n    else:\n        return None", "response": "This function walks the nested dictionary and returns a list of the keys that are anywhere in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_file_from_google_drive(file_id, dest_path, overwrite=False, unzip=False, showsize=False):\n\n        destination_directory = dirname(dest_path)\n        if not exists(destination_directory):\n            makedirs(destination_directory)\n\n        if not exists(dest_path) or overwrite:\n\n            session = requests.Session()\n\n            print('Downloading {} into {}... '.format(file_id, dest_path), end='')\n            stdout.flush()\n\n            response = session.get(GoogleDriveDownloader.DOWNLOAD_URL, params={'id': file_id}, stream=True)\n\n            token = GoogleDriveDownloader._get_confirm_token(response)\n            if token:\n                params = {'id': file_id, 'confirm': token}\n                response = session.get(GoogleDriveDownloader.DOWNLOAD_URL, params=params, stream=True)\n\n            if showsize:\n                print()  # Skip to the next line\n\n            current_download_size = [0]\n            GoogleDriveDownloader._save_response_content(response, dest_path, showsize, current_download_size)\n            print('Done.')\n\n            if unzip:\n                try:\n                    print('Unzipping...', end='')\n                    stdout.flush()\n                    with zipfile.ZipFile(dest_path, 'r') as z:\n                        z.extractall(destination_directory)\n                    print('Done.')\n                except zipfile.BadZipfile:\n                    warnings.warn('Ignoring `unzip` since \"{}\" does not look like a valid zip file'.format(file_id))", "response": "Downloads a shared file from Google Drive."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, ba):\n        #\n        # The code here is written for optimal JITting in PyPy, not for best\n        # readability or particular elegance. Do NOT touch!\n        #\n        l = len(ba)\n        i = 0\n        state = self._state\n        while i < l:\n            # optimized version of decode(), since we are not interested in actual code points\n            state = ord(\n                UTF8VALIDATOR_DFA_S[\n                    256 + (state << 4) + ord(UTF8VALIDATOR_DFA_S[ord(ba[i])])\n                ]\n            )\n            if state == UTF8_REJECT:\n                self._state = state\n                self._index += i\n                return False, False, i, self._index\n            i += 1\n        self._state = state\n        self._index += l\n        return True, state == UTF8_ACCEPT, l, self._index", "response": "This function is called by the utf - 8 module to validate a chunk of bytes provided as string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    ''' Run the server. '''\n    try:\n        ip = sys.argv[1]\n        port = int(sys.argv[2])\n    except (IndexError, ValueError):\n        print('Usage: {} <BIND_IP> <PORT>'.format(sys.argv[0]))\n        sys.exit(1)\n\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server.bind((ip, port))\n    server.listen(0)\n\n    try:\n        while True:\n            print('Waiting for connection...')\n            (stream, addr) = server.accept()\n            print('Client connected: {}:{}'.format(addr[0], addr[1]))\n            handle_connection(stream)\n            stream.shutdown(socket.SHUT_WR)\n            stream.close()\n    except KeyboardInterrupt:\n        print('Received SIGINT: shutting down\u2026')", "response": "Main function for the main function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a connection from the server into the internal internal", "response": "def handle_connection(stream):\n    '''\n    Handle a connection.\n\n    The server operates a request/response cycle, so it performs a synchronous\n    loop:\n\n    1) Read data from network into wsproto\n    2) Get next wsproto event\n    3) Handle event\n    4) Send data from wsproto to network\n\n    :param stream: a socket stream\n    '''\n    ws = WSConnection(ConnectionType.SERVER)\n\n    # events is a generator that yields websocket event objects. Usually you\n    # would say `for event in ws.events()`, but the synchronous nature of this\n    # server requires us to use next(event) instead so that we can interleave\n    # the network I/O.\n    events = ws.events()\n    running = True\n\n    while running:\n        # 1) Read data from network\n        in_data = stream.recv(RECEIVE_BYTES)\n        print('Received {} bytes'.format(len(in_data)))\n        ws.receive_data(in_data)\n\n        # 2) Get next wsproto event\n        try:\n            event = next(events)\n        except StopIteration:\n            print('Client connection dropped unexpectedly')\n            return\n\n        # 3) Handle event\n        if isinstance(event, Request):\n            # Negotiate new WebSocket connection\n            print('Accepting WebSocket upgrade')\n            out_data = ws.send(AcceptConnection())\n        elif isinstance(event, CloseConnection):\n            # Print log message and break out\n            print('Connection closed: code={}/{} reason={}'.format(\n                event.code.value, event.code.name, event.reason))\n            out_data = ws.send(event.response())\n            running = False\n        elif isinstance(event, TextMessage):\n            # Reverse text and send it back to wsproto\n            print('Received request and sending response')\n            out_data = ws.send(Message(data=event.data[::-1]))\n        elif isinstance(event, Ping):\n            # wsproto handles ping events for you by placing a pong frame in\n            # the outgoing buffer. You should not call pong() unless you want to\n            # send an unsolicited pong frame.\n            print('Received ping and sending pong')\n            out_data = ws.send(event.response())\n        else:\n            print('Unknown event: {!r}'.format(event))\n\n        # 4) Send data from wsproto to network\n        print('Sending {} bytes'.format(len(out_data)))\n        stream.send(out_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef receive_data(self, data):\n        # type: (bytes) -> None\n        \"\"\"\n        Pass some received data to the connection for handling.\n\n        A list of events that the remote peer triggered by sending this data can\n        be retrieved with :meth:`~wsproto.connection.Connection.events`.\n\n        :param data: The data received from the remote peer on the network.\n        :type data: ``bytes``\n        \"\"\"\n\n        if data is None:\n            # \"If _The WebSocket Connection is Closed_ and no Close control\n            # frame was received by the endpoint (such as could occur if the\n            # underlying transport connection is lost), _The WebSocket\n            # Connection Close Code_ is considered to be 1006.\"\n            self._events.append(CloseConnection(code=CloseReason.ABNORMAL_CLOSURE))\n            self._state = ConnectionState.CLOSED\n            return\n\n        if self.state in (ConnectionState.OPEN, ConnectionState.LOCAL_CLOSING):\n            self._proto.receive_bytes(data)\n        elif self.state is ConnectionState.CLOSED:\n            raise LocalProtocolError(\"Connection already closed.\")", "response": "Pass some received data to the connection for handling."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator that provides any events that have been generated by protocol activity.", "response": "def events(self):\n        # type: () -> Generator[Event, None, None]\n        \"\"\"\n        Return a generator that provides any events that have been generated\n        by protocol activity.\n\n        :returns: generator of :class:`Event <wsproto.events.Event>` subclasses\n        \"\"\"\n        while self._events:\n            yield self._events.popleft()\n\n        try:\n            for frame in self._proto.received_frames():\n                if frame.opcode is Opcode.PING:\n                    assert frame.frame_finished and frame.message_finished\n                    yield Ping(payload=frame.payload)\n\n                elif frame.opcode is Opcode.PONG:\n                    assert frame.frame_finished and frame.message_finished\n                    yield Pong(payload=frame.payload)\n\n                elif frame.opcode is Opcode.CLOSE:\n                    code, reason = frame.payload\n                    if self.state is ConnectionState.LOCAL_CLOSING:\n                        self._state = ConnectionState.CLOSED\n                    else:\n                        self._state = ConnectionState.REMOTE_CLOSING\n                    yield CloseConnection(code=code, reason=reason)\n\n                elif frame.opcode is Opcode.TEXT:\n                    yield TextMessage(\n                        data=frame.payload,\n                        frame_finished=frame.frame_finished,\n                        message_finished=frame.message_finished,\n                    )\n\n                elif frame.opcode is Opcode.BINARY:\n                    yield BytesMessage(\n                        data=frame.payload,\n                        frame_finished=frame.frame_finished,\n                        message_finished=frame.message_finished,\n                    )\n        except ParseFailed as exc:\n            yield CloseConnection(code=exc.code, reason=str(exc))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef server_extensions_handshake(requested, supported):\n    # type: (List[str], List[Extension]) -> Optional[bytes]\n    \"\"\"Agree on the extensions to use returning an appropriate header value.\n\n    This returns None if there are no agreed extensions\n    \"\"\"\n    accepts = {}\n    for offer in requested:\n        name = offer.split(\";\", 1)[0].strip()\n        for extension in supported:\n            if extension.name == name:\n                accept = extension.accept(offer)\n                if accept is True:\n                    accepts[extension.name] = True\n                elif accept is not False and accept is not None:\n                    accepts[extension.name] = accept.encode(\"ascii\")\n\n    if accepts:\n        extensions = []\n        for name, params in accepts.items():\n            if params is True:\n                extensions.append(name.encode(\"ascii\"))\n            else:\n                # py34 annoyance: doesn't support bytestring formatting\n                params = params.decode(\"ascii\")\n                if params == \"\":\n                    extensions.append((\"%s\" % (name)).encode(\"ascii\"))\n                else:\n                    extensions.append((\"%s; %s\" % (name, params)).encode(\"ascii\"))\n        return b\", \".join(extensions)\n\n    return None", "response": "This function returns an appropriate header value for the server extensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitiating an upgrade connection.", "response": "def initiate_upgrade_connection(self, headers, path):\n        # type: (List[Tuple[bytes, bytes]], str) -> None\n        \"\"\"Initiate an upgrade connection.\n\n        This should be used if the request has already be received and\n        parsed.\n\n        \"\"\"\n        if self.client:\n            raise LocalProtocolError(\n                \"Cannot initiate an upgrade connection when acting as the client\"\n            )\n        upgrade_request = h11.Request(method=b\"GET\", target=path, headers=headers)\n        h11_client = h11.Connection(h11.CLIENT)\n        self.receive_data(h11_client.send(upgrade_request))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends an event to the remote.", "response": "def send(self, event):\n        # type(Event) -> bytes\n        \"\"\"Send an event to the remote.\n\n        This will return the bytes to send based on the event or raise\n        a LocalProtocolError if the event is not valid given the\n        state.\n\n        \"\"\"\n        data = b\"\"\n        if isinstance(event, Request):\n            data += self._initiate_connection(event)\n        elif isinstance(event, AcceptConnection):\n            data += self._accept(event)\n        elif isinstance(event, RejectConnection):\n            data += self._reject(event)\n        elif isinstance(event, RejectData):\n            data += self._send_reject_data(event)\n        else:\n            raise LocalProtocolError(\n                \"Event {} cannot be sent during the handshake\".format(event)\n            )\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreceiving data from the remote.", "response": "def receive_data(self, data):\n        # type: (bytes) -> None\n        \"\"\"Receive data from the remote.\n\n        A list of events that the remote peer triggered by sending\n        this data can be retrieved with :meth:`events`.\n\n        \"\"\"\n        self._h11_connection.receive_data(data)\n        while True:\n            try:\n                event = self._h11_connection.next_event()\n            except h11.RemoteProtocolError:\n                raise RemoteProtocolError(\n                    \"Bad HTTP message\", event_hint=RejectConnection()\n                )\n            if (\n                isinstance(event, h11.ConnectionClosed)\n                or event is h11.NEED_DATA\n                or event is h11.PAUSED\n            ):\n                break\n\n            if self.client:\n                if isinstance(event, h11.InformationalResponse):\n                    if event.status_code == 101:\n                        self._events.append(self._establish_client_connection(event))\n                    else:\n                        self._events.append(\n                            RejectConnection(\n                                headers=event.headers,\n                                status_code=event.status_code,\n                                has_body=False,\n                            )\n                        )\n                        self._state = ConnectionState.CLOSED\n                elif isinstance(event, h11.Response):\n                    self._state = ConnectionState.REJECTING\n                    self._events.append(\n                        RejectConnection(\n                            headers=event.headers,\n                            status_code=event.status_code,\n                            has_body=True,\n                        )\n                    )\n                elif isinstance(event, h11.Data):\n                    self._events.append(\n                        RejectData(data=event.data, body_finished=False)\n                    )\n                elif isinstance(event, h11.EndOfMessage):\n                    self._events.append(RejectData(data=b\"\", body_finished=True))\n                    self._state = ConnectionState.CLOSED\n            else:\n                if isinstance(event, h11.Request):\n                    self._events.append(self._process_connection_request(event))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndemonstrates wsproto: 0) Open TCP connection 1) Negotiate WebSocket opening handshake 2) Send a message and display response 3) Send ping and display pong 4) Negotiate WebSocket closing handshake :param stream: a socket stream", "response": "def wsproto_demo(host, port):\n    '''\n    Demonstrate wsproto:\n\n    0) Open TCP connection\n    1) Negotiate WebSocket opening handshake\n    2) Send a message and display response\n    3) Send ping and display pong\n    4) Negotiate WebSocket closing handshake\n\n    :param stream: a socket stream\n    '''\n\n    # 0) Open TCP connection\n    print('Connecting to {}:{}'.format(host, port))\n    conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    conn.connect((host, port))\n\n    # 1) Negotiate WebSocket opening handshake\n    print('Opening WebSocket')\n    ws = WSConnection(ConnectionType.CLIENT)\n    net_send(ws.send(Request(host=host, target='server')), conn)\n    net_recv(ws, conn)\n\n    # events is a generator that yields websocket event objects. Usually you\n    # would say `for event in ws.events()`, but the synchronous nature of this\n    # client requires us to use next(event) instead so that we can interleave\n    # the network I/O. It will raise StopIteration when it runs out of events\n    # (i.e. needs more network data), but since this script is synchronous, we\n    # will explicitly resume the generator whenever we have new network data.\n    events = ws.events()\n\n    # Because this is a client WebSocket, wsproto has automatically queued up\n    # a handshake, and we need to send it and wait for a response.\n    event = next(events)\n    if isinstance(event, AcceptConnection):\n        print('WebSocket negotiation complete')\n    else:\n        raise Exception('Expected AcceptConnection event!')\n\n    # 2) Send a message and display response\n    message = \"wsproto is great\"\n    print('Sending message: {}'.format(message))\n    net_send(ws.send(Message(data=message)), conn)\n    net_recv(ws, conn)\n    event = next(events)\n    if isinstance(event, TextMessage):\n        print('Received message: {}'.format(event.data))\n    else:\n        raise Exception('Expected TextMessage event!')\n\n    # 3) Send ping and display pong\n    payload = b\"table tennis\"\n    print('Sending ping: {}'.format(payload))\n    net_send(ws.send(Ping(payload=payload)), conn)\n    net_recv(ws, conn)\n    event = next(events)\n    if isinstance(event, Pong):\n        print('Received pong: {}'.format(event.payload))\n    else:\n        raise Exception('Expected Pong event!')\n\n    # 4) Negotiate WebSocket closing handshake\n    print('Closing WebSocket')\n    net_send(ws.send(CloseConnection(code=1000, reason='sample reason')), conn)\n    # After sending the closing frame, we won't get any more events. The server\n    # should send a reply and then close the connection, so we need to receive\n    # twice:\n    net_recv(ws, conn)\n    conn.shutdown(socket.SHUT_WR)\n    net_recv(ws, conn)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting pending data from websocket to network.", "response": "def net_send(out_data, conn):\n    ''' Write pending data from websocket to network. '''\n    print('Sending {} bytes'.format(len(out_data)))\n    conn.send(out_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread pending data from network into websocket.", "response": "def net_recv(ws, conn):\n    ''' Read pending data from network into websocket. '''\n    in_data = conn.recv(RECEIVE_BYTES)\n    if not in_data:\n        # A receive of zero bytes indicates the TCP socket has been closed. We\n        # need to pass None to wsproto to update its internal state.\n        print('Received 0 bytes (connection closed)')\n        ws.receive_data(None)\n    else:\n        print('Received {} bytes'.format(len(in_data)))\n        ws.receive_data(in_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the given packet filter string is valid with respect to the filter language.", "response": "def check_filter(filter, layer=Layer.NETWORK):\n        \"\"\"\n        Checks if the given packet filter string is valid with respect to the filter language.\n\n        The remapped function is WinDivertHelperCheckFilter::\n\n            BOOL WinDivertHelperCheckFilter(\n                __in const char *filter,\n                __in WINDIVERT_LAYER layer,\n                __out_opt const char **errorStr,\n                __out_opt UINT *errorPos\n            );\n\n        See: https://reqrypt.org/windivert-doc.html#divert_helper_check_filter\n\n        :return: A tuple (res, pos, msg) with check result in 'res' human readable description of the error in 'msg' and the error's position in 'pos'.\n        \"\"\"\n        res, pos, msg = False, c_uint(), c_char_p()\n        try:\n            res = windivert_dll.WinDivertHelperCheckFilter(filter.encode(), layer, byref(msg), byref(pos))\n        except OSError:\n            pass\n        return res, pos.value, msg.value.decode()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self):\n        if self.is_open:\n            raise RuntimeError(\"WinDivert handle is already open.\")\n        self._handle = windivert_dll.WinDivertOpen(self._filter, self._layer, self._priority,\n                                                   self._flags)", "response": "Opens a WinDivert handle for the given filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncloses the handle opened by open.", "response": "def close(self):\n        \"\"\"\n        Closes the handle opened by open().\n\n        The remapped function is WinDivertClose::\n\n            BOOL WinDivertClose(\n                __in HANDLE handle\n            );\n\n        For more info on the C call visit: http://reqrypt.org/windivert-doc.html#divert_close\n        \"\"\"\n        if not self.is_open:\n            raise RuntimeError(\"WinDivert handle is not open.\")\n        windivert_dll.WinDivertClose(self._handle)\n        self._handle = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreceives a diverted packet that matched the filter.", "response": "def recv(self, bufsize=DEFAULT_PACKET_BUFFER_SIZE):\n        \"\"\"\n        Receives a diverted packet that matched the filter.\n\n        The remapped function is WinDivertRecv::\n\n            BOOL WinDivertRecv(\n                __in HANDLE handle,\n                __out PVOID pPacket,\n                __in UINT packetLen,\n                __out_opt PWINDIVERT_ADDRESS pAddr,\n                __out_opt UINT *recvLen\n            );\n\n        For more info on the C call visit: http://reqrypt.org/windivert-doc.html#divert_recv\n\n        :return: The return value is a `pydivert.Packet`.\n        \"\"\"\n        if self._handle is None:\n            raise RuntimeError(\"WinDivert handle is not open\")\n\n        packet = bytearray(bufsize)\n        packet_ = (c_char * bufsize).from_buffer(packet)\n        address = windivert_dll.WinDivertAddress()\n        recv_len = c_uint(0)\n        windivert_dll.WinDivertRecv(self._handle, packet_, bufsize, byref(address), byref(recv_len))\n        return Packet(\n            memoryview(packet)[:recv_len.value],\n            (address.IfIdx, address.SubIfIdx),\n            Direction(address.Direction)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a packet into the network stack.", "response": "def send(self, packet, recalculate_checksum=True):\n        \"\"\"\n        Injects a packet into the network stack.\n        Recalculates the checksum before sending unless recalculate_checksum=False is passed.\n\n        The injected packet may be one received from recv(), or a modified version, or a completely new packet.\n        Injected packets can be captured and diverted again by other WinDivert handles with lower priorities.\n\n        The remapped function is WinDivertSend::\n\n            BOOL WinDivertSend(\n                __in HANDLE handle,\n                __in PVOID pPacket,\n                __in UINT packetLen,\n                __in PWINDIVERT_ADDRESS pAddr,\n                __out_opt UINT *sendLen\n            );\n\n        For more info on the C call visit: http://reqrypt.org/windivert-doc.html#divert_send\n\n        :return: The return value is the number of bytes actually sent.\n        \"\"\"\n        if recalculate_checksum:\n            packet.recalculate_checksums()\n\n        send_len = c_uint(0)\n        if PY2:\n            # .from_buffer(memoryview) does not work on PY2\n            buff = bytearray(packet.raw)\n        else:\n            buff = packet.raw\n        buff = (c_char * len(packet.raw)).from_buffer(buff)\n        windivert_dll.WinDivertSend(self._handle, buff, len(packet.raw), byref(packet.wd_addr),\n                                    byref(send_len))\n        return send_len"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_param(self, name):\n        value = c_uint64(0)\n        windivert_dll.WinDivertGetParam(self._handle, name, byref(value))\n        return value.value", "response": "Get a WinDivert parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a WinDivert parameter.", "response": "def set_param(self, name, value):\n        \"\"\"\n        Set a WinDivert parameter. See pydivert.Param for the list of parameters.\n\n        The remapped function is DivertSetParam::\n\n            BOOL WinDivertSetParam(\n                __in HANDLE handle,\n                __in WINDIVERT_PARAM param,\n                __in UINT64 value\n            );\n\n        For more info on the C call visit: http://reqrypt.org/windivert-doc.html#divert_set_param\n        \"\"\"\n        return windivert_dll.WinDivertSetParam(self._handle, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raise_on_error(f):\n\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        result = f(*args, **kwargs)\n        retcode = GetLastError()\n        if retcode and retcode != ERROR_IO_PENDING:\n            err = WinError(code=retcode)\n            windll.kernel32.SetLastError(0)  # clear error code so that we don't raise twice.\n            raise err\n        return result\n\n    return wrapper", "response": "A decorator that raises a WinError when GetLastError returns an error."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init():\n    i = instance()\n    for funcname in WINDIVERT_FUNCTIONS:\n        func = getattr(i, funcname)\n        func = raise_on_error(func)\n        setattr(_module, funcname, func)", "response": "Initialize the DLL with the actual functions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking lazy - init proxy function.", "response": "def _mkprox(funcname):\n    \"\"\"\n    Make lazy-init proxy function.\n    \"\"\"\n\n    def prox(*args, **kwargs):\n        _init()\n        return getattr(_module, funcname)(*args, **kwargs)\n\n    return prox"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the protocol and start address of the packet.", "response": "def protocol(self):\n        \"\"\"\n        - | A (ipproto, proto_start) tuple.\n          | ``ipproto`` is the IP protocol in use, e.g. Protocol.TCP or Protocol.UDP.\n          | ``proto_start`` denotes the beginning of the protocol data.\n          | If the packet does not match our expectations, both ipproto and proto_start are None.\n        \"\"\"\n        if self.address_family == socket.AF_INET:\n            proto = i(self.raw[9])\n            start = (i(self.raw[0]) & 0b1111) * 4\n        elif self.address_family == socket.AF_INET6:\n            proto = i(self.raw[6])\n\n            # skip over well-known ipv6 headers\n            start = 40\n            while proto in IPV6_EXT_HEADERS:\n                if start >= len(self.raw):\n                    # less than two bytes left\n                    start = None\n                    proto = None\n                    break\n                if proto == Protocol.FRAGMENT:\n                    hdrlen = 8\n                elif proto == Protocol.AH:\n                    hdrlen = (i(self.raw[start + 1]) + 2) * 4\n                else:\n                    # Protocol.HOPOPT, Protocol.DSTOPTS, Protocol.ROUTING\n                    hdrlen = (i(self.raw[start + 1]) + 1) * 8\n                proto = i(self.raw[start])\n                start += hdrlen\n        else:\n            start = None\n            proto = None\n\n        out_of_bounds = (\n            (proto == Protocol.TCP and start + 20 > len(self.raw)) or\n            (proto == Protocol.UDP and start + 8 > len(self.raw)) or\n            (proto in {Protocol.ICMP, Protocol.ICMPV6} and start + 4 > len(self.raw))\n        )\n        if out_of_bounds:\n            # special-case tcp/udp so that we can rely on .protocol for the port properties.\n            start = None\n            proto = None\n\n        return proto, start"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef icmpv4(self):\n        ipproto, proto_start = self.protocol\n        if ipproto == Protocol.ICMP:\n            return ICMPv4Header(self, proto_start)", "response": "An ICMPv4Header instance if the packet is valid ICMPv4 otherwise None."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tcp(self):\n        ipproto, proto_start = self.protocol\n        if ipproto == Protocol.TCP:\n            return TCPHeader(self, proto_start)", "response": "Returns a TCPHeader instance if the packet is valid TCP."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _payload(self):\n        return self.tcp or self.udp or self.icmpv4 or self.icmpv6", "response": "return whether this is a TCP UDP ICMPv4 or ICMPv6 payload"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the interface and direction as a WINDICE_ADDRESS structure.", "response": "def wd_addr(self):\n        \"\"\"\n        Gets the interface and direction as a `WINDIVERT_ADDRESS` structure.\n        :return: The `WINDIVERT_ADDRESS` structure.\n        \"\"\"\n        address = windivert_dll.WinDivertAddress()\n        address.IfIdx, address.SubIfIdx = self.interface\n        address.Direction = self.direction\n        return address"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matches(self, filter, layer=Layer.NETWORK):\n        buff, buff_ = self.__to_buffers()\n        return windivert_dll.WinDivertHelperEvalFilter(filter.encode(), layer, ctypes.byref(buff_), len(self.raw),\n                                                       ctypes.byref(self.wd_addr))", "response": "Evaluates the packet against the given filter string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize new project at the current path.", "response": "def init(project_name):\n    \"\"\"\n    Initialize new project at the current path.\n\n    After this you can run other FloydHub commands like status and run.\n    \"\"\"\n\n    project_obj = ProjectClient().get_by_name(project_name)\n\n    if not project_obj:\n        namespace, name = get_namespace_from_name(project_name)\n        create_project_base_url = \"{}/projects/create\".format(floyd.floyd_web_host)\n        create_project_url = \"{}?name={}&namespace={}\".format(create_project_base_url, name, namespace)\n        floyd_logger.info(('Project name does not yet exist on floydhub.com. '\n                          'Create your new project on floydhub.com:\\n\\t%s'),\n                          create_project_base_url)\n        webbrowser.open(create_project_url)\n\n        name = click.prompt('Press ENTER to use project name \"%s\" or enter a different name' % project_name, default=project_name, show_default=False)\n\n        project_name = name.strip() or project_name\n        project_obj = ProjectClient().get_by_name(project_name)\n\n        if not project_obj:\n            raise FloydException('Project \"%s\" does not exist on floydhub.com. Ensure it exists before continuing.' % project_name)\n\n    namespace, name = get_namespace_from_name(project_name)\n    experiment_config = ExperimentConfig(name=name,\n                                         namespace=namespace,\n                                         family_id=project_obj.id)\n    ExperimentConfigManager.set_config(experiment_config)\n    FloydIgnoreManager.init()\n\n    yaml_config = read_yaml_config()\n    if not yaml_config:\n        copyfile(os.path.join(os.path.dirname(__file__), 'default_floyd.yml'), 'floyd.yml')\n\n    floyd_logger.info(\"Project \\\"%s\\\" initialized in current directory\", project_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef status(id):\n    if id:\n        try:\n            experiment = ExperimentClient().get(normalize_job_name(id))\n        except FloydException:\n            experiment = ExperimentClient().get(id)\n\n        print_experiments([experiment])\n    else:\n        experiments = ExperimentClient().get_all()\n        print_experiments(experiments)", "response": "Show status of all jobs in a project."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the list of experiments in a table.", "response": "def print_experiments(experiments):\n    \"\"\"\n    Prints job details in a table. Includes urls and mode parameters\n    \"\"\"\n    headers = [\"JOB NAME\", \"CREATED\", \"STATUS\", \"DURATION(s)\", \"INSTANCE\", \"DESCRIPTION\", \"METRICS\"]\n    expt_list = []\n    for experiment in experiments:\n        expt_list.append([normalize_job_name(experiment.name),\n                          experiment.created_pretty, experiment.state,\n                          experiment.duration_rounded, experiment.instance_type_trimmed,\n                          experiment.description, format_metrics(experiment.latest_metrics)])\n    floyd_logger.info(tabulate(expt_list, headers=headers))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone(id, path):\n    try:\n        experiment = ExperimentClient().get(normalize_job_name(id, use_config=False))\n    except FloydException:\n        experiment = ExperimentClient().get(id)\n\n    task_instance_id = get_module_task_instance_id(experiment.task_instances)\n    task_instance = TaskInstanceClient().get(task_instance_id) if task_instance_id else None\n    if not task_instance:\n        sys.exit(\"Cannot clone this version of the job. Try a different version.\")\n    module = ModuleClient().get(task_instance.module_id) if task_instance else None\n\n    if path:\n        # Download a directory from Code\n        code_url = \"{}/api/v1/download/artifacts/code/{}?is_dir=true&path={}\".format(floyd.floyd_host,\n                                                                                     experiment.id,\n                                                                                     path)\n    else:\n        # Download the full Code\n        code_url = \"{}/api/v1/resources/{}?content=true&download=true\".format(floyd.floyd_host,\n                                                                              module.resource_id)\n    ExperimentClient().download_tar(url=code_url,\n                                    untar=True,\n                                    delete_after_untar=True)", "response": "Clone a job into a new version."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows detailed information of a job.", "response": "def info(job_name_or_id):\n    \"\"\"\n    View detailed information of a job.\n    \"\"\"\n    try:\n        experiment = ExperimentClient().get(normalize_job_name(job_name_or_id))\n    except FloydException:\n        experiment = ExperimentClient().get(job_name_or_id)\n\n    task_instance_id = get_module_task_instance_id(experiment.task_instances)\n    task_instance = TaskInstanceClient().get(task_instance_id) if task_instance_id else None\n    normalized_job_name = normalize_job_name(experiment.name)\n    table = [[\"Job name\", normalized_job_name],\n             [\"Created\", experiment.created_pretty],\n             [\"Status\", experiment.state], [\"Duration(s)\", experiment.duration_rounded],\n             [\"Instance\", experiment.instance_type_trimmed],\n             [\"Description\", experiment.description],\n             [\"Metrics\", format_metrics(experiment.latest_metrics)]]\n    if task_instance and task_instance.mode in ['jupyter', 'serving']:\n        table.append([\"Mode\", task_instance.mode])\n        table.append([\"Url\", experiment.service_url])\n    if experiment.tensorboard_url:\n        table.append([\"TensorBoard\", experiment.tensorboard_url])\n    floyd_logger.info(tabulate(table))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef follow_logs(instance_log_id, sleep_duration=1):\n    cur_idx = 0\n    job_terminated = False\n\n    while not job_terminated:\n        # Get the logs in a loop and log the new lines\n        log_file_contents = ResourceClient().get_content(instance_log_id)\n        print_output = log_file_contents[cur_idx:]\n        # Get the status of the Job from the current log line\n        job_terminated = any(terminal_output in print_output for terminal_output in TERMINATION_OUTPUT_LIST)\n        cur_idx += len(print_output)\n        sys.stdout.write(print_output)\n        sleep(sleep_duration)", "response": "Follow the logs until Job termination."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logs(id, url, follow, sleep_duration=1):\n    instance_log_id = get_log_id(id)\n\n    if url:\n        log_url = \"{}/api/v1/resources/{}?content=true\".format(\n            floyd.floyd_host, instance_log_id)\n        floyd_logger.info(log_url)\n        return\n\n    if follow:\n        floyd_logger.info(\"Launching job ...\")\n        follow_logs(instance_log_id, sleep_duration)\n    else:\n        log_file_contents = ResourceClient().get_content(instance_log_id)\n        if len(log_file_contents.strip()):\n            floyd_logger.info(log_file_contents.rstrip())\n        else:\n            floyd_logger.info(\"Launching job now. Try after a few seconds.\")", "response": "View the logs of a job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nview the files from a job.", "response": "def output(id, url):\n    \"\"\"\n    View the files from a job.\n    \"\"\"\n    try:\n        experiment = ExperimentClient().get(normalize_job_name(id))\n    except FloydException:\n        experiment = ExperimentClient().get(id)\n\n    output_dir_url = \"%s/%s/files\" % (floyd.floyd_web_host, experiment.name)\n    if url:\n        floyd_logger.info(output_dir_url)\n    else:\n        floyd_logger.info(\"Opening output path in your browser ...\")\n        webbrowser.open(output_dir_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop(id):\n    try:\n        experiment = ExperimentClient().get(normalize_job_name(id))\n    except FloydException:\n        experiment = ExperimentClient().get(id)\n\n    if experiment.state not in [\"queued\", \"queue_scheduled\", \"running\"]:\n        floyd_logger.info(\"Job in {} state cannot be stopped\".format(experiment.state))\n        sys.exit(1)\n\n    if not ExperimentClient().stop(experiment.id):\n        floyd_logger.error(\"Failed to stop job\")\n        sys.exit(1)\n\n    floyd_logger.info(\"Experiment shutdown request submitted. Check status to confirm shutdown\")", "response": "Stop a running job."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(names, yes):\n    failures = False\n    for name in names:\n        try:\n            experiment = ExperimentClient().get(normalize_job_name(name))\n        except FloydException:\n            experiment = ExperimentClient().get(name)\n\n        if not experiment:\n            failures = True\n            continue\n\n        if not yes and not click.confirm(\"Delete Job: {}?\".format(experiment.name),\n                                         abort=False,\n                                         default=False):\n            floyd_logger.info(\"Job {}: Skipped.\".format(experiment.name))\n            continue\n\n        if not ExperimentClient().delete(experiment.id):\n            failures = True\n        else:\n            floyd_logger.info(\"Job %s Deleted\", experiment.name)\n\n    if failures:\n        sys.exit(1)", "response": "Delete a training job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nviewing the current version of the CLI.", "response": "def version():\n    \"\"\"\n    View the current version of the CLI.\n    \"\"\"\n    import pkg_resources\n    version = pkg_resources.require(PROJECT_NAME)[0].version\n    floyd_logger.info(version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init(dataset_name):\n    dataset_obj = DatasetClient().get_by_name(dataset_name)\n\n    if not dataset_obj:\n        namespace, name = get_namespace_from_name(dataset_name)\n        create_dataset_base_url = \"{}/datasets/create\".format(floyd.floyd_web_host)\n        create_dataset_url = \"{}?name={}&namespace={}\".format(create_dataset_base_url, name, namespace)\n        floyd_logger.info((\"Dataset name does not match your list of datasets. \"\n                           \"Create your new dataset in the web dashboard:\\n\\t%s\"),\n                          create_dataset_base_url)\n        webbrowser.open(create_dataset_url)\n\n        name = click.prompt('Press ENTER to use dataset name \"%s\" or enter a different name' % dataset_name, default=dataset_name, show_default=False)\n\n        dataset_name = name.strip() or dataset_name\n        dataset_obj = DatasetClient().get_by_name(dataset_name)\n\n        if not dataset_obj:\n            raise FloydException('Dataset \"%s\" does not exist on floydhub.com. Ensure it exists before continuing.' % dataset_name)\n\n    namespace, name = get_namespace_from_name(dataset_name)\n    data_config = DataConfig(name=name,\n                             namespace=namespace,\n                             family_id=dataset_obj.id)\n    DataConfigManager.set_config(data_config)\n    floyd_logger.info(\"Data source \\\"{}\\\" initialized in current directory\".format(dataset_name))\n    floyd_logger.info(\"\"\"\n    You can now upload your data to Floyd by:\n        floyd data upload\n    \"\"\")", "response": "Initialize a new dataset in current dir and upload all files in this dir to FloydHub."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuploads files in the current dir to FloydHub.", "response": "def upload(resume, message):\n    \"\"\"\n    Upload files in the current dir to FloydHub.\n    \"\"\"\n    data_config = DataConfigManager.get_config()\n\n    if not upload_is_resumable(data_config) or not opt_to_resume(resume):\n        abort_previous_upload(data_config)\n        access_token = AuthConfigManager.get_access_token()\n        initialize_new_upload(data_config, access_token, message)\n\n    complete_upload(data_config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef status(id):\n    if id:\n        data_source = get_data_object(id, use_data_config=False)\n        print_data([data_source] if data_source else [])\n    else:\n        data_sources = DataClient().get_all()\n        print_data(data_sources)", "response": "Show status of all versions in a dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_data_object(data_id, use_data_config=True):\n    normalized_data_reference = normalize_data_name(data_id, use_data_config=use_data_config)\n    client = DataClient()\n    data_obj = client.get(normalized_data_reference)\n\n    # Try with the raw ID\n    if not data_obj and data_id != normalized_data_reference:\n        data_obj = client.get(data_id)\n\n    return data_obj", "response": "Get the data object for the given data_id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting dataset information in tabular form", "response": "def print_data(data_sources):\n    \"\"\"\n    Print dataset information in tabular form\n    \"\"\"\n    if not data_sources:\n        return\n\n    headers = [\"DATA NAME\", \"CREATED\", \"STATUS\", \"DISK USAGE\"]\n    data_list = []\n    for data_source in data_sources:\n        data_list.append([data_source.name,\n                          data_source.created_pretty,\n                          data_source.state, data_source.size])\n    floyd_logger.info(tabulate(data_list, headers=headers))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone(id, path):\n    data_source = get_data_object(id, use_data_config=False)\n\n    if not data_source:\n        if 'output' in id:\n            floyd_logger.info(\"Note: You cannot clone the output of a running job. You need to wait for it to finish.\")\n        sys.exit()\n\n    if path:\n        # Download a directory from Dataset or Files\n        # Get the type of data resource from the id (foo/projects/bar/ or foo/datasets/bar/)\n        if '/datasets/' in id:\n            resource_type = 'data'\n            resource_id = data_source.id\n        else:\n            resource_type = 'files'\n            try:\n                experiment = ExperimentClient().get(normalize_job_name(id, use_config=False))\n            except FloydException:\n                experiment = ExperimentClient().get(id)\n            resource_id = experiment.id\n\n        data_url = \"{}/api/v1/download/artifacts/{}/{}?is_dir=true&path={}\".format(floyd.floyd_host,\n                                                                                   resource_type,\n                                                                                   resource_id,\n                                                                                   path)\n    else:\n        # Download the full Dataset\n        data_url = \"{}/api/v1/resources/{}?content=true&download=true\".format(floyd.floyd_host,\n                                                                              data_source.resource_id)\n    DataClient().download_tar(url=data_url,\n                              untar=True,\n                              delete_after_untar=True)", "response": "Clone a dataset or files into a directory"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting files in a dataset.", "response": "def listfiles(data_name):\n    \"\"\"\n    List files in a dataset.\n    \"\"\"\n\n    data_source = get_data_object(data_name, use_data_config=False)\n\n    if not data_source:\n        if 'output' in data_name:\n            floyd_logger.info(\"Note: You cannot clone the output of a running job. You need to wait for it to finish.\")\n        sys.exit()\n\n    # Depth-first search\n    dirs = ['']\n    paths = []\n    while dirs:\n        cur_dir = dirs.pop()\n        url = \"/resources/{}/{}?content=true\".format(data_source.resource_id, cur_dir)\n        response = DataClient().request(\"GET\", url).json()\n\n        if response['skipped_files'] > 0:\n            floyd_logger.info(\"Warning: in directory '%s', %s/%s files skipped (too many files)\", cur_dir, response['skipped_files'], response['total_files'])\n\n        files = response['files']\n        files.sort(key=lambda f: f['name'])\n        for f in files:\n            path = os.path.join(cur_dir, f['name'])\n            if f['type'] == 'directory':\n                path += os.sep\n            paths.append(path)\n\n            if f['type'] == 'directory':\n                dirs.append(os.path.join(cur_dir, f['name']))\n    for path in paths:\n        floyd_logger.info(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getfile(data_name, path):\n\n    data_source = get_data_object(data_name, use_data_config=False)\n\n    if not data_source:\n        if 'output' in data_name:\n            floyd_logger.info(\"Note: You cannot clone the output of a running job. You need to wait for it to finish.\")\n        sys.exit()\n\n    url = \"{}/api/v1/resources/{}/{}?content=true\".format(floyd.floyd_host, data_source.resource_id, path)\n    fname = os.path.basename(path)\n    DataClient().download(url, filename=fname)\n    floyd_logger.info(\"Download finished\")", "response": "Download a specific file from a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nviewing the files from a dataset.", "response": "def output(id, url):\n    \"\"\"\n    View the files from a dataset.\n    \"\"\"\n    data_source = get_data_object(id, use_data_config=False)\n\n    if not data_source:\n        sys.exit()\n\n    data_url = \"%s/%s\" % (floyd.floyd_web_host, data_source.name)\n    if url:\n        floyd_logger.info(data_url)\n    else:\n        floyd_logger.info(\"Opening output directory in your browser ...\")\n        webbrowser.open(data_url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new dataset version from the contents of a job.", "response": "def add(source):\n    \"\"\"\n    Create a new dataset version from the contents of a job.\n\n    This will create a new dataset version with the job output.\n    Use the full job name: foo/projects/bar/1/code, foo/projects/bar/1/files or foo/projects/bar/1/output\n    \"\"\"\n    new_data = DatasetClient().add_data(source)\n    print_data([DataClient().get(new_data['data_id'])])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_namespace_from_name(name):\n    if not re.match(NAMESPACE_PATTERN, name):\n        sys.exit((\"Argument '%s' doesn't match any recognized pattern:\\n\"\n                  \"\\tfloyd [data] init <project_or_dataset_name>\\n\"\n                  \"\\tfloyd [data] init <namespace>/<project_or_dataset_name>\\n\"\n                  \"\\tfloyd [data] init <namespace>/[projects|dataset]/<project_or_dataset_name>\\n\"\n                  \"\\n Note: Argument can only contain alphanumeric, hyphen-minus '-' , underscore '_' and dot '.' characters.\"\n                  ) % name)\n\n    name_parts = name.split(\"/\", 2)\n    if len(name_parts) > 1:\n        return name_parts[0], name_parts[-1]\n    else:\n        return current_username(), name", "response": "Returns the namespace and name of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(host, verbose):\n    import raven\n    raven.Client(\n        dsn='https://d8669005bd2b4b1ba6387ec57e1ce660:1d25ce33fcdb4864b9fd4f0c97689a98@sentry.io/226940',\n        release=get_cli_version(),\n        environment='prod',\n        processors=('raven.processors.SanitizePasswordsProcessor',))\n\n    floyd.floyd_host = host\n    configure_logger(verbose)\n    check_cli_version()", "response": "Floyd CLI interacts with FloydHub server and executes your commands."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_cli_version():\n    should_exit = False\n    server_version = VersionClient().get_cli_version()\n    current_version = get_cli_version()\n\n    if LooseVersion(current_version) < LooseVersion(server_version.min_version):\n        print(\"\\nYour version of CLI (%s) is no longer compatible with server.\" % current_version)\n        should_exit = True\n    elif LooseVersion(current_version) < LooseVersion(server_version.latest_version):\n        print(\"\\nNew version of CLI (%s) is now available.\" % server_version.latest_version)\n    else:\n        return\n\n    # new version is ready\n    if should_exit and click.confirm('\\nDo you want to upgrade to version %s now?' % server_version.latest_version):\n        auto_upgrade()\n        sys.exit(0)\n    else:\n        msg_parts = []\n        msg_parts.append(\"\\nTo manually upgrade run:\")\n        msg_parts.append(\"    pip install -U floyd-cli\")\n        if is_conda_env():\n            msg_parts.append(\"Or if you prefer to use conda:\")\n            msg_parts.append(\"    conda install -y -c conda-forge -c floydhub floyd-cli\")\n        print(\"\\n\".join(msg_parts))\n        print(\"\")\n\n    if should_exit:\n        sys.exit(0)", "response": "Check if the current CLI version satisfies the server requirements\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self,\n                method,\n                url,\n                params=None,\n                data=None,\n                files=None,\n                json=None,\n                timeout=5,\n                headers=None,\n                skip_auth=False):\n        \"\"\"\n        Execute the request using requests library\n        \"\"\"\n        request_url = self.base_url + url\n        floyd_logger.debug(\"Starting request to url: %s with params: %s, data: %s\", request_url, params, data)\n\n        request_headers = {'x-floydhub-cli-version': get_cli_version()}\n        # Auth headers if present\n        if self.auth_header:\n            request_headers[\"Authorization\"] = self.auth_header\n        # Add any additional headers\n        if headers:\n            request_headers.update(headers)\n\n        try:\n            response = requests.request(method,\n                                        request_url,\n                                        params=params,\n                                        data=data,\n                                        json=json,\n                                        headers=request_headers,\n                                        files=files,\n                                        timeout=timeout)\n        except requests.exceptions.ConnectionError as exception:\n            floyd_logger.debug(\"Exception: %s\", exception, exc_info=True)\n            sys.exit(\"Cannot connect to the Floyd server. Check your internet connection.\")\n        except requests.exceptions.Timeout as exception:\n            floyd_logger.debug(\"Exception: %s\", exception, exc_info=True)\n            sys.exit(\"Connection to FloydHub server timed out. Please retry or check your internet connection.\")\n\n        floyd_logger.debug(\"Response Content: %s, Headers: %s\" % (response.content, response.headers))\n        self.check_response_status(response)\n        return response", "response": "Execute the request using requests library and check the response status."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload the file from the given url to the given filename.", "response": "def download(self, url, filename, relative=False, headers=None, timeout=5):\n        \"\"\"\n        Download the file from the given url at the current path\n        \"\"\"\n        request_url = self.base_url + url if relative else url\n        floyd_logger.debug(\"Downloading file from url: {}\".format(request_url))\n\n        # Auth headers if present\n        request_headers = {}\n        if self.auth_header:\n            request_headers[\"Authorization\"] = self.auth_header\n        # Add any additional headers\n        if headers:\n            request_headers.update(headers)\n\n        try:\n            response = requests.get(request_url,\n                                    headers=request_headers,\n                                    timeout=timeout,\n                                    stream=True)\n            self.check_response_status(response)\n            with open(filename, 'wb') as f:\n                # chunk mode response doesn't have content-length so we are\n                # using a custom header here\n                content_length = response.headers.get('x-floydhub-content-length')\n                if not content_length:\n                    content_length = response.headers.get('content-length')\n                if content_length:\n                    for chunk in progress.bar(response.iter_content(chunk_size=1024),\n                                              expected_size=(int(content_length) / 1024) + 1):\n                        if chunk:\n                            f.write(chunk)\n                else:\n                    for chunk in response.iter_content(chunk_size=1024):\n                        if chunk:\n                            f.write(chunk)\n            return filename\n        except requests.exceptions.ConnectionError as exception:\n            floyd_logger.debug(\"Exception: {}\".format(exception))\n            sys.exit(\"Cannot connect to the Floyd server. Check your internet connection.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_tar(self, url, untar=True, delete_after_untar=False, destination_dir='.'):\n        try:\n            floyd_logger.info(\"Downloading the tar file to the current directory ...\")\n            filename = self.download(url=url, filename='output.tar')\n            if filename and untar:\n                floyd_logger.info(\"Untarring the contents of the file ...\")\n                tar = tarfile.open(filename)\n                tar.extractall(path=destination_dir)\n                tar.close()\n            if delete_after_untar:\n                floyd_logger.info(\"Cleaning up the tar file ...\")\n                os.remove(filename)\n            return filename\n        except FloydException as e:\n            floyd_logger.info(\"Download URL ERROR! {}\".format(e.message))\n            return False", "response": "Download and optionally untar the tar file from the given url and return the filename."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_response_status(self, response):\n        if not (200 <= response.status_code < 300):\n            try:\n                message = response.json()[\"errors\"]\n            except Exception:\n                message = None\n            floyd_logger.debug(\"Error received : status_code: {}, message: {}\".format(response.status_code,\n                                                                                      message or response.content))\n            if response.status_code == 400:\n                raise BadRequestException(response)\n            elif response.status_code == 401:\n                raise AuthenticationException()\n            elif response.status_code == 403:\n                raise AuthorizationException(response)\n            elif response.status_code == 404:\n                raise NotFoundException()\n            elif response.status_code == 429:\n                raise OverLimitException(response.json().get(\"message\"))\n            elif response.status_code == 502:\n                raise BadGatewayException()\n            elif response.status_code == 504:\n                raise GatewayTimeoutException()\n            elif response.status_code == 423:\n                raise LockedException()\n            elif 500 <= response.status_code < 600:\n                if 'Server under maintenance' in response.content.decode():\n                    raise ServerException('Server under maintenance, please try again later.')\n                else:\n                    raise ServerException()\n            else:\n                msg = \"An error occurred. Server response: {}\".format(response.status_code)\n                raise FloydException(message=msg)", "response": "Check if response is successful. If not raise exception."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(verbose):\n    floyd.floyd_host = floyd.floyd_web_host = \"https://dev.floydhub.com\"\n    floyd.tus_server_endpoint = \"https://upload-v2-dev.floydhub.com/api/v1/upload/\"\n    configure_logger(verbose)\n    check_cli_version()", "response": "A Floyd CLI interacts with FloydHub server and executes your commands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_unignored_file_paths(ignore_list=None, whitelist=None):\n    unignored_files = []\n    if ignore_list is None:\n        ignore_list = []\n    if whitelist is None:\n        whitelist = []\n\n    for root, dirs, files in os.walk(\".\"):\n        floyd_logger.debug(\"Root:%s, Dirs:%s\", root, dirs)\n\n        if ignore_path(unix_style_path(root), ignore_list, whitelist):\n            # Reset dirs to avoid going further down this directory.\n            # Then continue to the next iteration of os.walk, which causes\n            # everything in this directory to be ignored.\n            #\n            # Note that whitelisted files that are within directories that are\n            # ignored will not be whitelisted. This follows the expected\n            # behavior established by .gitignore logic:\n            # \"It is not possible to re-include a file if a parent directory of\n            # that file is excluded.\"\n            # https://git-scm.com/docs/gitignore#_pattern_format\n            dirs[:] = []\n            floyd_logger.debug(\"Ignoring directory : %s\", root)\n            continue\n\n        for file_name in files:\n            file_path = unix_style_path(os.path.join(root, file_name))\n            if ignore_path(file_path, ignore_list, whitelist):\n                floyd_logger.debug(\"Ignoring file : %s\", file_name)\n                continue\n\n            unignored_files.append(os.path.join(root, file_name))\n\n    return unignored_files", "response": "Given an ignore_list and a whitelist of glob patterns returns the list of unignored file paths in the current directory and its subdirectories."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a boolean indicating if a path should be ignored given an ignore_list and a whitelist of glob patterns.", "response": "def ignore_path(path, ignore_list=None, whitelist=None):\n    \"\"\"\n    Returns a boolean indicating if a path should be ignored given an\n    ignore_list and a whitelist of glob patterns.\n    \"\"\"\n    if ignore_list is None:\n        return True\n\n    should_ignore = matches_glob_list(path, ignore_list)\n    if whitelist is None:\n        return should_ignore\n\n    return should_ignore and not matches_glob_list(path, whitelist)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matches_glob_list(path, glob_list):\n    for glob in glob_list:\n        try:\n            if PurePath(path).match(glob):\n                return True\n        except TypeError:\n            pass\n    return False", "response": "Returns a boolean containing if a path matches any glob in a list of glob patterns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the list of files in the current directory and subdirectories.", "response": "def get_files_in_current_directory(file_type):\n    \"\"\"\n    Gets the list of files in the current directory and subdirectories.\n    Respects .floydignore file if present\n    \"\"\"\n    local_files = []\n    total_file_size = 0\n\n    ignore_list, whitelist = FloydIgnoreManager.get_lists()\n\n    floyd_logger.debug(\"Ignoring: %s\", ignore_list)\n    floyd_logger.debug(\"Whitelisting: %s\", whitelist)\n\n    file_paths = get_unignored_file_paths(ignore_list, whitelist)\n\n    for file_path in file_paths:\n        local_files.append((file_type, (unix_style_path(file_path), open(file_path, 'rb'), 'text/plain')))\n        total_file_size += os.path.getsize(file_path)\n\n    return (local_files, total_file_size)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the number of files to compress", "response": "def __get_nfiles_to_compress(self):\n        \"\"\"\n        Return the number of files to compress\n\n        Note: it should take about 0.1s for counting 100k files on a dual core machine\n        \"\"\"\n        floyd_logger.info(\"Get number of files to compress... (this could take a few seconds)\")\n        paths = [self.source_dir]\n        try:\n            # Traverse each subdirs of source_dir and count files/dirs\n            while paths:\n                path = paths.pop()\n                for item in scandir(path):\n                    if item.is_dir():\n                        paths.append(item.path)\n                        self.__files_to_compress += 1\n                    elif item.is_file():\n                        self.__files_to_compress += 1\n        except OSError as e:\n            # OSError: [Errno 13] Permission denied\n            if e.errno == errno.EACCES:\n                self.source_dir = os.getcwd() if self.source_dir == '.' else self.source_dir  # Expand cwd\n                sys.exit((\"Permission denied. Make sure to have read permission \"\n                          \"for all the files and directories in the path: %s\")\n                         % (self.source_dir))\n        floyd_logger.info(\"Compressing %d files\", self.__files_to_compress)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_tarfile(self):\n        floyd_logger.info(\"Compressing data...\")\n        # Show progress bar (file_compressed/file_to_compress)\n        self.__compression_bar = ProgressBar(expected_size=self.__files_to_compress, filled_char='=')\n\n        # Auxiliary functions\n        def dfilter_file_counter(tarinfo):\n            \"\"\"\n            Dummy filter function used to track the progression at file levels.\n            \"\"\"\n            self.__compression_bar.show(self.__files_compressed)\n            self.__files_compressed += 1\n            return tarinfo\n\n        def warn_purge_exit(info_msg, filename, progress_bar, exit_msg):\n            \"\"\"\n            Warn the user that's something went wrong,\n            remove the tarball and provide an exit message.\n            \"\"\"\n            progress_bar.done()\n            floyd_logger.info(info_msg)\n            rmtree(os.path.dirname(filename))\n            sys.exit(exit_msg)\n\n        try:\n            # Define the default signal handler for catching: Ctrl-C\n            signal.signal(signal.SIGINT, signal.default_int_handler)\n            with tarfile.open(self.filename, \"w:gz\") as tar:\n                tar.add(self.source_dir, arcname=os.path.basename(self.source_dir), filter=dfilter_file_counter)\n            self.__compression_bar.done()\n        except (OSError, IOError) as e:\n            # OSError: [Errno 13] Permission denied\n            if e.errno == errno.EACCES:\n                self.source_dir = os.getcwd() if self.source_dir == '.' else self.source_dir  # Expand cwd\n                warn_purge_exit(info_msg=\"Permission denied. Removing compressed data...\",\n                                filename=self.filename,\n                                progress_bar=self.__compression_bar,\n                                exit_msg=(\"Permission denied. Make sure to have read permission \"\n                                          \"for all the files and directories in the path: %s\")\n                                % (self.source_dir))\n            # OSError: [Errno 28] No Space Left on Device (IOError on python2.7)\n            elif e.errno == errno.ENOSPC:\n                dir_path = os.path.dirname(self.filename)\n                warn_purge_exit(info_msg=\"No space left. Removing compressed data...\",\n                                filename=self.filename,\n                                progress_bar=self.__compression_bar,\n                                exit_msg=(\"No space left when compressing your data in: %s.\\n\"\n                                          \"Make sure to have enough space before uploading your data.\")\n                                % (os.path.abspath(dir_path)))\n\n        except KeyboardInterrupt:  # Purge tarball on Ctrl-C\n            warn_purge_exit(info_msg=\"Ctrl-C signal detected: Removing compressed data...\",\n                            filename=self.filename,\n                            progress_bar=self.__compression_bar,\n                            exit_msg=\"Stopped the data upload gracefully.\")", "response": "Create a tar file with the contents of the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pretty_date(time=False):\n    now = datetime.now(PST_TIMEZONE)\n    if type(time) is int:\n        diff = now - datetime.fromtimestamp(time)\n    elif isinstance(time, datetime):\n        diff = now - time\n    elif not time:\n        diff = now - now\n    second_diff = diff.seconds\n    day_diff = diff.days\n\n    if day_diff < 0:\n        return ''\n\n    if day_diff == 0:\n        if second_diff < 10:\n            return \"just now\"\n        if second_diff < 60:\n            return str(second_diff) + \" seconds ago\"\n        if second_diff < 120:\n            return \"a minute ago\"\n        if second_diff < 3600:\n            return str(int(second_diff / 60)) + \" minutes ago\"\n        if second_diff < 7200:\n            return \"an hour ago\"\n        if second_diff < 86400:\n            return str(int(second_diff / 3600)) + \" hours ago\"\n    if day_diff == 1:\n        return \"Yesterday\"\n    if day_diff < 7:\n        return str(day_diff) + \" days ago\"\n    if day_diff < 31:\n        return str(int(day_diff / 7)) + \" weeks ago\"\n    if day_diff < 365:\n        return str(int(day_diff / 30)) + \" months ago\"\n    return str(int(day_diff / 365)) + \" years ago\"", "response": "Returns a pretty string of the current date"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a dataset by name.", "response": "def get_by_name(self, name, namespace=None):\n        \"\"\"\n        name: can be either <namespace>/<dataset_name> or just <dataset_name>\n        namespace: if specified, will skip name parsing, defaults to current user's username\n        \"\"\"\n        if not namespace:\n            namespace, name = get_namespace_from_name(name)\n        if not namespace:\n            namespace = AuthConfigManager.get_access_token().username\n        try:\n            response = self.request('GET', '%s/%s/%s' % (self.url, namespace, name))\n            return Dataset.from_dict(response.json())\n        except NotFoundException:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new object in the database.", "response": "def create(self, data):\n        \"\"\"\n        Create a temporary directory for the tar file that will be removed at\n        the end of the operation.\n        \"\"\"\n        try:\n            floyd_logger.info(\"Making create request to server...\")\n            post_body = data.to_dict()\n            post_body[\"resumable\"] = True\n            response = self.request(\"POST\", self.url, json=post_body)\n            return response.json()\n        except BadRequestException as e:\n            if 'Dataset not found, ID' in e.message:\n                floyd_logger.error(\n                    'Data create: ERROR! Please run \"floyd data init DATASET_NAME\" before upload.')\n            else:\n                floyd_logger.error('Data create: ERROR! %s', e.message)\n            return None\n        except FloydException as e:\n            floyd_logger.error(\"Data create: ERROR! %s\", e.message)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(ctx, cpu, gpu, env, message, data, mode, open_notebook, follow, tensorboard, gpu2, cpu2, max_runtime, task, command):\n    # cli_default is used for any option that has default value\n    cli_default = {'description': '', 'command': ''}\n    # Error early if more than one --env is passed.  Then get the first/only\n    # --env out of the list so all other operations work normally (they don't\n    # expect an iterable). For details on this approach, see the comment above\n    # the --env click option\n    if not env:\n        cli_default['env'] = DEFAULT_ENV\n        env = None\n    elif len(env) > 1:\n        floyd_logger.error(\n            \"You passed more than one environment: {}. Please specify a single environment.\".format(env)\n        )\n        sys.exit(1)\n    else:\n        env = env[0]\n\n    if not mode:\n        cli_default['mode'] = 'command'\n\n    experiment_config = ExperimentConfigManager.get_config()\n    access_token = AuthConfigManager.get_access_token()\n    namespace = experiment_config.namespace or access_token.username\n\n    if not ProjectClient().exists(experiment_config.name, namespace=namespace):\n        floyd_logger.error('Invalid project id, please run '\n                           '\"floyd init PROJECT_NAME\" before scheduling a job.')\n        sys.exit(1)\n\n    experiment_name = \"{}/{}\".format(namespace, experiment_config.name)\n\n    success, data_ids, show_data_info = process_data_ids(data)\n    if not success:\n        sys.exit(2)\n\n    # Create module\n    module_inputs = [{'name': data_str.split(':')[1],\n                      'type': 'dir'} for data_str in data_ids]\n\n    instance_type = None\n    if gpu2:\n        instance_type = G2_INSTANCE_TYPE\n    elif cpu2:\n        instance_type = C2_INSTANCE_TYPE\n    elif gpu:\n        instance_type = G1_INSTANCE_TYPE\n    elif cpu:\n        instance_type = C1_INSTANCE_TYPE\n\n    if not instance_type:\n        cli_default['instance_type'] = C1_INSTANCE_TYPE\n\n    yaml_config = read_yaml_config()\n    arch = INSTANCE_ARCH_MAP[\n        resolve_final_instance_type(instance_type, yaml_config, task, cli_default)\n    ]\n    if not validate_env(env or cli_default['env'], arch):\n        sys.exit(3)\n\n    command_str = ' '.join(command)\n    if command_str and mode in ('jupyter', 'serve'):\n        floyd_logger.error('Command argument \"%s\" cannot be used with mode: %s.\\nSee http://docs.floydhub.com/guides/run_a_job/#mode for more information about run modes.', command_str, mode)  # noqa\n        sys.exit(3)\n    if command_str == '':\n        # set to none so it won't override floyd config\n        command_str = None\n\n    module = Module(name=experiment_name,\n                    description=message or '',\n                    command=command_str,\n                    mode=mode,\n                    family_id=experiment_config.family_id,\n                    inputs=module_inputs,\n                    env=env,\n                    instance_type=instance_type,\n                    yaml_config=yaml_config,\n                    task=task)\n\n    try:\n        module_id = ModuleClient().create(module, cli_default)\n    except BadRequestException as e:\n        if 'Project not found, ID' in e.message:\n            floyd_logger.error(\n                'ERROR: Please run \"floyd init PROJECT_NAME\" before scheduling a job.')\n        else:\n            floyd_logger.error('ERROR: %s', e.message)\n        sys.exit(4)\n    floyd_logger.debug(\"Created module with id : %s\", module_id)\n\n    # Create experiment request\n    # Get the actual command entered in the command line\n    if max_runtime:\n        max_runtime = int(max_runtime)\n    full_command = get_command_line(instance_type, env, message, data, mode, open_notebook, command_str)\n    experiment_request = ExperimentRequest(name=experiment_name,\n                                           description=message,\n                                           full_command=full_command,\n                                           module_id=module_id,\n                                           max_runtime=max_runtime,\n                                           env=env,\n                                           data_ids=data_ids,\n                                           family_id=experiment_config.family_id,\n                                           instance_type=instance_type,\n                                           yaml_config=yaml_config,\n                                           task=task)\n    expt_client = ExperimentClient()\n    expt_info = expt_client.create(experiment_request, cli_default)\n    floyd_logger.debug(\"Created job : %s\", expt_info['id'])\n\n    job_name = expt_info['name']\n\n    show_new_job_info(expt_client, job_name, expt_info, mode, open_notebook, show_data_info)\n\n    if follow:\n        floyd_logger.info(\"\\nFollow flag detected (--follow): Opening logs ...\")\n        instance_log_id = instance_log_id = get_log_id(job_name)\n        follow_logs(instance_log_id)", "response": "This function is called by the FloydHub to run a job on a specific project."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the full floyd command line for the instance type env message data mode open_notebook and command_str", "response": "def get_command_line(instance_type, env, message, data, mode, open_notebook, command_str):\n    \"\"\"\n    Return a string representing the full floyd command entered in the command line\n    \"\"\"\n    floyd_command = [\"floyd\", \"run\"]\n    if instance_type:\n        floyd_command.append('--' + INSTANCE_NAME_MAP[instance_type])\n    if env and not env == DEFAULT_ENV:\n        floyd_command += [\"--env\", env]\n    if message:\n        floyd_command += [\"--message\", shell_quote(message)]\n    if data:\n        for data_item in data:\n            parts = data_item.split(':')\n\n            if len(parts) > 1:\n                data_item = normalize_data_name(parts[0], use_data_config=False) + ':' + parts[1]\n\n            floyd_command += [\"--data\", data_item]\n    if mode and mode != \"job\":\n        floyd_command += [\"--mode\", mode]\n        if mode == 'jupyter':\n            if not open_notebook:\n                floyd_command.append(\"--no-open\")\n    else:\n        if command_str:\n            floyd_command.append(shell_quote(command_str))\n    return ' '.join(floyd_command)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restart(ctx, job_name, data, open_notebook, env, message, gpu, cpu, gpup, cpup, command):\n    # Error early if more than one --env is passed. Then get the first/only\n    # --env out of the list so all other operations work normally (they don't\n    # expect an iterable). For details on this approach, see the comment above\n    # the --env click option\n    if len(env) > 1:\n        floyd_logger.error(\n            \"You passed more than one environment: {}. Please specify a single environment.\".format(env)\n        )\n        sys.exit(1)\n    env = env[0]\n\n    parameters = {}\n\n    expt_client = ExperimentClient()\n\n    try:\n        job = expt_client.get(normalize_job_name(job_name))\n    except FloydException:\n        job = expt_client.get(job_name)\n\n    if gpu:\n        instance_type = G1_INSTANCE_TYPE\n    elif cpu:\n        instance_type = C1_INSTANCE_TYPE\n    else:\n        instance_type = job.instance_type\n\n    if instance_type is not None:\n        parameters['instance_type'] = instance_type\n    else:\n        instance_type = job.instance_type\n\n    if env is not None:\n        arch = INSTANCE_ARCH_MAP[instance_type]\n        if not validate_env(env, arch):\n            sys.exit(1)\n        parameters['env'] = env\n\n    success, data_ids, show_data_info = process_data_ids(data)\n    if not success:\n        sys.exit(1)\n    if data_ids:\n        parameters['data_ids'] = data_ids\n\n    if message:\n        parameters['description'] = message\n\n    if command:\n        parameters['command'] = ' '.join(command)\n\n    floyd_logger.info('Restarting job %s...', job_name)\n\n    new_job_info = expt_client.restart(job.id, parameters=parameters)\n    if not new_job_info:\n        floyd_logger.error(\"Failed to restart job\")\n        sys.exit(1)\n\n    show_new_job_info(expt_client, new_job_info['name'], new_job_info, job.mode, open_notebook, show_data_info)", "response": "Restart a finished job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_user(user, using='records', interaction=None,\n                part_of_week='allweek', part_of_day='allday'):\n    \"\"\"\n    Filter records of a User objects by interaction, part of week and day.\n\n    Parameters\n    ----------\n    user : User\n        a bandicoot User object\n    type : str, default 'records'\n        'records' or 'recharges'\n    part_of_week : {'allweek', 'weekday', 'weekend'}, default 'allweek'\n        * 'weekend': keep only the weekend records\n        * 'weekday': keep only the weekdays records\n        * 'allweek': use all the records\n    part_of_day : {'allday', 'day', 'night'}, default 'allday'\n        * 'day': keep only the records during the day\n        * 'night': keep only the records during the night\n        * 'allday': use all the records\n    interaction : object\n        The interaction to filter records:\n        * \"callandtext\", for only callandtext;\n        * a string, to filter for one type;\n        * None, to use all records.\n    \"\"\"\n\n    if using == 'recharges':\n        records = user.recharges\n    else:\n        records = user.records\n        if interaction == 'callandtext':\n            records = filter(\n                lambda r: r.interaction in ['call', 'text'], records)\n        elif interaction is not None:\n            records = filter(lambda r: r.interaction == interaction, records)\n\n    if part_of_week == 'weekday':\n        records = filter(\n            lambda r: r.datetime.isoweekday() not in user.weekend, records)\n    elif part_of_week == 'weekend':\n        records = filter(\n            lambda r: r.datetime.isoweekday() in user.weekend, records)\n    elif part_of_week != 'allweek':\n        raise KeyError(\n            \"{} is not a valid value for part_of_week. it should be 'weekday', \"\n            \"'weekend' or 'allweek'.\".format(part_of_week))\n\n    if user.night_start < user.night_end:\n        night_filter = lambda r: user.night_end > r.datetime.time(\n        ) > user.night_start\n    else:\n        night_filter = lambda r: not(\n            user.night_end < r.datetime.time() < user.night_start)\n\n    if part_of_day == 'day':\n        records = filter(lambda r: not(night_filter(r)), records)\n    elif part_of_day == 'night':\n        records = filter(night_filter, records)\n    elif part_of_day != 'allday':\n        raise KeyError(\n            \"{} is not a valid value for part_of_day. It should be 'day', 'night' or 'allday'.\".format(part_of_day))\n\n    return list(records)", "response": "Filter a User object by interaction part of week and day."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbin records by chunks of 30 minutes returning the most prevalent position.", "response": "def positions_binning(records):\n    \"\"\"\n    Bin records by chunks of 30 minutes, returning the most prevalent position.\n\n    If multiple positions have the same number of occurrences\n    (during 30 minutes), we select the last one.\n    \"\"\"\n    def get_key(d):\n        return (d.year, d.day, d.hour, d.minute // 30)\n\n    chunks = itertools.groupby(records, key=lambda r: get_key(r.datetime))\n\n    for _, items in chunks:\n        positions = [i.position for i in items]\n        # Given the low number of positions per chunk of 30 minutes, and\n        # the need for a deterministic value, we use max and not Counter\n        yield max(positions, key=positions.count)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _group_range(records, method):\n\n    start_date = records[0].datetime\n    end_date = records[-1].datetime\n    _fun = DATE_GROUPERS[method]\n\n    d = start_date\n\n    # Day and week use timedelta\n    if method not in [\"month\", \"year\"]:\n        def increment(i):\n            return i + timedelta(**{method + 's': 1})\n\n    elif method == \"month\":\n        def increment(i):\n            year, month = divmod(i.month + 1, 12)\n            if month == 0:\n                month = 12\n                year = year - 1\n            return d.replace(year=d.year + year, month=month)\n\n    elif method == \"year\":\n        def increment(i):\n            return d.replace(year=d.year + 1)\n\n    while _fun(d) <= _fun(end_date):\n        yield d\n        d = increment(d)", "response": "Yields the range of all dates between the extrema of\n    a list of records separated by a given time delta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_records(records, groupby='week'):\n\n    def _group_date(records, _fun):\n        for _, chunk in itertools.groupby(records, key=lambda r: _fun(r.datetime)):\n            yield list(chunk)\n\n    return _group_date(records, DATE_GROUPERS[groupby])", "response": "Group records by year month week or day."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_type(data):\n\n    if isinstance(data, (type(None), numbers.Number)):\n        return 'scalar'\n\n    if isinstance(data, SummaryStats):\n        return 'summarystats'\n\n    if hasattr(data, \"__len__\"):  # list or numpy array\n        data = [x for x in data if x is not None]\n        if len(data) == 0 or isinstance(data[0], numbers.Number):\n            return 'distribution_scalar'\n        if isinstance(data[0], SummaryStats):\n            return 'distribution_summarystats'\n\n        raise TypeError(\n            \"{} is not a valid input. It should be a number, a SummaryStats \"\n            \"object, or None\".format(data[0]))\n\n    raise TypeError(\n        \"{} is not a valid input. It should be a number, a SummaryStats \"\n        \"object, or a list\".format(data))", "response": "Infer the type of objects returned by indicators."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns statistics on a single object.", "response": "def statistics(data, summary='default', datatype=None):\n    \"\"\"\n    Return statistics (mean, standard error, standard error and median,\n    min and max) on data metrics.\n\n    Examples\n    --------\n    Given a list of integers or floating point numbers,\n    ``statistics`` computes the mean and standard error of the mean,\n    and the min and max.\n\n    >>> statistics([0, 1, 2, 3])\n    {'mean': 1.5, 'std': 1.2910, 'min': 0, 'max': 3}\n\n    Given a list of ``SummaryStats`` tuples, the function will\n    returns the mean, standard error of the mean, min and max for each attribute\n    of the tuples.\n    \"\"\"\n\n    def _default_stats(agg):\n        if agg is None or len(agg) == 0:\n            return OrderedDict([('mean', None), ('std', None)])\n        else:\n            # Some functions may return None values\n            # It's better to filter them\n            agg = [x for x in agg if x is not None]\n            return OrderedDict([('mean', mean(agg)), ('std', std(agg))])\n\n    def _stats_dict(v):\n        rv = [(key, _default_stats([getattr(s, key, None) for s in data])) for key in v]\n        return OrderedDict(rv)\n\n    summary_keys = {\n        'default': ['mean', 'std'],\n        'extended': ['mean', 'std', 'median', 'skewness', 'kurtosis', 'min', 'max']\n    }\n\n    if datatype is None:\n        datatype = infer_type(data)\n\n    if datatype == 'scalar':\n        return data\n\n    if datatype == 'summarystats':\n        if summary is None:\n            return data.distribution\n        elif summary in ['default', 'extended']:\n            rv = [(key, getattr(data, key, None)) for key in summary_keys[summary]]\n            return OrderedDict(rv)\n        else:\n            raise ValueError(\"{} is not a valid summary type\".format(summary))\n\n    if datatype == 'distribution_scalar':\n        if summary == 'default':\n            return _default_stats(data)\n        elif summary is None:\n            return data\n        else:\n            raise ValueError(\"{} is not a valid summary type\".format(summary))\n\n    if datatype == 'distribution_summarystats':\n        if summary is None:\n            return [item.distribution for item in data]\n        elif summary in ['extended', 'default']:\n            return _stats_dict(summary_keys[summary])\n        else:\n            raise ValueError(\"{} is not a valid summary type\".format(summary))\n\n    raise ValueError(\"{} is not a valid data type.\".format(datatype))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the kurtosis for data.", "response": "def kurtosis(data):\n    \"\"\"\n    Return the kurtosis for ``data``.\n    \"\"\"\n\n    if len(data) == 0:\n        return None\n\n    num = moment(data, 4)\n    denom = moment(data, 2) ** 2.\n\n    return num / denom if denom != 0 else 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the skewness of data.", "response": "def skewness(data):\n    \"\"\"\n    Returns the skewness of ``data``.\n    \"\"\"\n\n    if len(data) == 0:\n        return None\n\n    num = moment(data, 3)\n    denom = moment(data, 2) ** 1.5\n\n    return num / denom if denom != 0 else 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the median of numeric data.", "response": "def median(data):\n    \"\"\"\n    Return the median of numeric data, unsing the \"mean of middle two\" method.\n    If ``data`` is empty, ``0`` is returned.\n\n    Examples\n    --------\n\n    >>> median([1, 3, 5])\n    3.0\n\n    When the number of data points is even, the median is interpolated:\n    >>> median([1, 3, 5, 7])\n    4.0\n    \"\"\"\n\n    if len(data) == 0:\n        return None\n\n    data = sorted(data)\n    return float((data[len(data) // 2] + data[(len(data) - 1) // 2]) / 2.)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a SummaryStats object containing statistics on the given distribution.", "response": "def summary_stats(data):\n    \"\"\"\n    Returns a :class:`~bandicoot.helper.maths.SummaryStats` object\n    containing statistics on the given distribution.\n\n    Examples\n    --------\n    >>> summary_stats([0, 1])\n    SummaryStats(mean=0.5, std=0.5, min=0.0, max=1.0, median=0.5, skewness=0.0, kurtosis=1.0, distribution=[0, 1])\n    \"\"\"\n\n    if data is None:\n        data = []\n    data = sorted(data)\n\n    if len(data) < 1:\n        return SummaryStats(None, None, None, None, None, None, None, [])\n\n    _median = median(data)\n\n    _mean = mean(data)\n    _std = std(data)\n    _minimum = minimum(data)\n    _maximum = maximum(data)\n    _kurtosis = kurtosis(data)\n    _skewness = skewness(data)\n    _distribution = data\n\n    return SummaryStats(_mean, _std, _minimum, _maximum,\n                        _median, _skewness, _kurtosis, _distribution)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the Shannon entropy a measure of uncertainty.", "response": "def entropy(data):\n    \"\"\"\n    Compute the Shannon entropy, a measure of uncertainty.\n    \"\"\"\n\n    if len(data) == 0:\n        return None\n\n    n = sum(data)\n\n    _op = lambda f: f * math.log(f)\n    return - sum(_op(float(i) / n) for i in data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the great - circle distance between two points.", "response": "def great_circle_distance(pt1, pt2):\n    \"\"\"\n    Return the great-circle distance in kilometers between two points,\n    defined by a tuple (lat, lon).\n\n    Examples\n    --------\n    >>> brussels = (50.8503, 4.3517)\n    >>> paris = (48.8566, 2.3522)\n    >>> great_circle_distance(brussels, paris)\n    263.9754164080347\n    \"\"\"\n    r = 6371.\n\n    delta_latitude = math.radians(pt1[0] - pt2[0])\n    delta_longitude = math.radians(pt1[1] - pt2[1])\n    latitude1 = math.radians(pt1[0])\n    latitude2 = math.radians(pt2[0])\n\n    a = math.sin(delta_latitude / 2) ** 2 + math.cos(latitude1) * math.cos(latitude2) * math.sin(delta_longitude / 2) ** 2\n    return r * 2. * math.asin(math.sqrt(a))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap a function f while keeping the same keyword arguments as the last argument of the wrapper.", "response": "def advanced_wrap(f, wrapper):\n    \"\"\"\n    Wrap a decorated function while keeping the same keyword arguments\n    \"\"\"\n    f_sig = list(inspect.getargspec(f))\n    wrap_sig = list(inspect.getargspec(wrapper))\n\n    # Update the keyword arguments of the wrapper\n    if f_sig[3] is None or f_sig[3] == []:\n        f_sig[3], f_kwargs = [], []\n    else:\n        f_kwargs = f_sig[0][-len(f_sig[3]):]\n\n    for key, default in zip(f_kwargs, f_sig[3]):\n        wrap_sig[0].append(key)\n        wrap_sig[3] = wrap_sig[3] + (default, )\n\n    wrap_sig[2] = None  # Remove kwargs\n    src = \"lambda %s: \" % (inspect.formatargspec(*wrap_sig)[1:-1])\n    new_args = inspect.formatargspec(\n        wrap_sig[0], wrap_sig[1], wrap_sig[2], f_kwargs,\n        formatvalue=lambda x: '=' + x)\n    src += 'wrapper%s\\n' % new_args\n\n    decorated = eval(src, locals())\n    decorated.func = f\n    return update_wrapper(decorated, f)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef percent_records_missing_location(user, method=None):\n\n    if len(user.records) == 0:\n        return 0.\n\n    missing_locations = sum([1 for record in user.records if record.position._get_location(user) is None])\n    return float(missing_locations) / len(user.records)", "response": "Returns the percentage of records missing a location parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the percentage of calls that overlap with the next call.", "response": "def percent_overlapping_calls(records, min_gab=300):\n    \"\"\"\n    Return the percentage of calls that overlap with the next call.\n\n    Parameters\n    ----------\n    records : list\n        The records for a single user.\n    min_gab : int\n        Number of seconds that the calls must overlap to be considered an issue.\n        Defaults to 5 minutes.\n    \"\"\"\n\n    calls = [r for r in records if r.interaction == \"call\"]\n\n    if len(calls) == 0:\n        return 0.\n\n    overlapping_calls = 0\n    for i, r in enumerate(calls):\n        if i <= len(calls) - 2:\n            if r.datetime + timedelta(seconds=r.call_duration - min_gab) >= calls[i + 1].datetime:\n                overlapping_calls += 1\n\n    return (float(overlapping_calls) / len(calls))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef antennas_missing_locations(user, Method=None):\n    unique_antennas = set([record.position.antenna for record in user.records\n                           if record.position.antenna is not None])\n    return sum([1 for antenna in unique_antennas if user.antennas.get(antenna) is None])", "response": "Returns the number of antennas missing locations in a given user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a unique hash of the Python source code in the current bandicoot module using the cryptographic hash function SHA - 1.", "response": "def bandicoot_code_signature():\n    \"\"\"\n    Returns a unique hash of the Python source code in the current bandicoot\n    module, using the cryptographic hash function SHA-1.\n    \"\"\"\n    checksum = hashlib.sha1()\n\n    for root, dirs, files in os.walk(MAIN_DIRECTORY):\n        for filename in sorted(files):\n            if not filename.endswith('.py'):\n                continue\n\n            f_path = os.path.join(root, filename)\n            f_size = os.path.getsize(f_path)\n            with open(f_path, 'rb') as f:\n                while f.tell() != f_size:\n                    checksum.update(f.read(0x40000))\n\n    return checksum.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef supported(cls, stream=sys.stdout):\n        if not stream.isatty():\n            return False  # auto color only on TTYs\n        try:\n            import curses\n        except ImportError:\n            return False\n        else:\n            try:\n                try:\n                    return curses.tigetnum(\"colors\") > 2\n                except curses.error:\n                    curses.setupterm()\n                    return curses.tigetnum(\"colors\") > 2\n            except:\n                raise\n                # guess false in case of error\n                return False", "response": "A class method that returns True if the current platform supports coloring terminal output using this method returns False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the given text to the stream in the given color.", "response": "def write(self, text, color):\n        \"\"\"\n        Write the given text to the stream in the given color.\n        \"\"\"\n        color = self._colors[color]\n        self.stream.write('\\x1b[{}m{}\\x1b[0m'.format(color, text))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the percentage of interactions the user had at home.", "response": "def percent_at_home(positions, user):\n    \"\"\"\n    The percentage of interactions the user had while he was at home.\n\n    .. note::\n        The position of the home is computed using\n        :meth:`User.recompute_home <bandicoot.core.User.recompute_home>`.\n        If no home can be found, the percentage of interactions at home\n        will be ``None``.\n    \"\"\"\n\n    if not user.has_home:\n        return None\n\n    total_home = sum(1 for p in positions if p == user.home)\n    return float(total_home) / len(positions) if len(positions) != 0 else 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the radius of gyration for all visited places.", "response": "def radius_of_gyration(positions, user):\n    \"\"\"\n    Returns the radius of gyration, the *equivalent distance* of the mass from\n    the center of gravity, for all visited places. [GON2008]_\n\n    References\n    ----------\n    .. [GON2008] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008).\n        Understanding individual human mobility patterns. Nature, 453(7196),\n        779-782.\n    \"\"\"\n    d = Counter(p._get_location(user) for p in positions\n                if p._get_location(user) is not None)\n    sum_weights = sum(d.values())\n    positions = list(d.keys())  # Unique positions\n\n    if len(positions) == 0:\n        return None\n\n    barycenter = [0, 0]\n    for pos, t in d.items():\n        barycenter[0] += pos[0] * t\n        barycenter[1] += pos[1] * t\n\n    barycenter[0] /= sum_weights\n    barycenter[1] /= sum_weights\n\n    r = 0.\n    for pos, t in d.items():\n        r += float(t) / sum_weights * \\\n            great_circle_distance(barycenter, pos) ** 2\n    return math.sqrt(r)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef entropy_of_antennas(positions, normalize=False):\n    counter = Counter(p for p in positions)\n    raw_entropy = entropy(list(counter.values()))\n    n = len(counter)\n    if normalize and n > 1:\n        return raw_entropy / math.log(n)\n    else:\n        return raw_entropy", "response": "Returns the entropy of the antennas in the order of the given positions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frequent_antennas(positions, percentage=0.8):\n    location_count = Counter(list(map(str, positions)))\n\n    target = math.ceil(sum(location_count.values()) * percentage)\n    location_sort = sorted(list(location_count.keys()),\n                           key=lambda x: location_count[x])\n\n    while target > 0 and len(location_sort) > 0:\n        location_id = location_sort.pop()\n        target -= location_count[location_id]\n\n    return len(location_count) - len(location_sort)", "response": "Returns the number of frequent antennas in a tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the churn rate of a user.", "response": "def churn_rate(user, summary='default', **kwargs):\n    \"\"\"\n    Computes the frequency spent at every towers each week, and returns the\n    distribution of the cosine similarity between two consecutives week.\n\n    .. note:: The churn rate is always computed between pairs of weeks.\n    \"\"\"\n    if len(user.records) == 0:\n        return statistics([], summary=summary)\n\n    query = {\n        'groupby': 'week',\n        'divide_by': OrderedDict([\n            ('part_of_week', ['allweek']),\n            ('part_of_day', ['allday'])\n        ]),\n        'using': 'records',\n        'filter_empty': True,\n        'binning': True\n    }\n\n    rv = grouping_query(user, query)\n    weekly_positions = rv[0][1]\n\n    all_positions = list(set(p for l in weekly_positions for p in l))\n    frequencies = {}\n    cos_dist = []\n\n    for week, week_positions in enumerate(weekly_positions):\n        count = Counter(week_positions)\n        total = sum(count.values())\n        frequencies[week] = [count.get(p, 0) / total for p in all_positions]\n\n    all_indexes = range(len(all_positions))\n    for f_1, f_2 in pairwise(list(frequencies.values())):\n        num = sum(f_1[a] * f_2[a] for a in all_indexes)\n        denom_1 = sum(f ** 2 for f in f_1)\n        denom_2 = sum(f ** 2 for f in f_2)\n        cos_dist.append(1 - num / (denom_1 ** .5 * denom_2 ** .5))\n\n    return statistics(cos_dist, summary=summary)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if two records match.", "response": "def matches(self, other):\n        \"\"\"\n        Return true if two records 'match': if they share the same values for\n        ``interaction``, ``direction``, ``call_duration``, and ``datetime``.\n        \"\"\"\n        return self.interaction == other.interaction and \\\n            self.direction != other.direction and \\\n            self.call_duration == other.call_duration and \\\n            abs((self.datetime - other.datetime).total_seconds()) < 30"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a short description of the object and writes it to the standard output.", "response": "def describe(self):\n        \"\"\"\n        Generates a short description of the object, and writes it to the\n        standard output.\n\n        Examples\n        --------\n        >>> import bandicoot as bc\n        >>> user = bc.User()\n        >>> user.records = bc.tests.generate_user.random_burst(5)\n        >>> user.describe()\n        [x] 5 records from 2014-01-01 10:41:00 to 2014-01-01 11:21:00\n            5 contacts\n        [x] 1 attribute\n        \"\"\"\n        def format_int(name, n):\n            if n == 0 or n == 1:\n                return \"%i %s\" % (n, name[:-1])\n            else:\n                return \"%i %s\" % (n, name)\n\n        empty_box = Colors.OKGREEN + '[ ]' + Colors.ENDC + ' '\n        filled_box = Colors.OKGREEN + '[x]' + Colors.ENDC + ' '\n\n        if self.start_time is None:\n            print(empty_box + \"No records stored\")\n        else:\n            print((filled_box + format_int(\"records\", len(self.records)) +\n                   \" from %s to %s\" % (self.start_time, self.end_time)))\n\n        nb_contacts = bc.individual.number_of_contacts(\n            self, interaction='callandtext', groupby=None)\n        nb_contacts = nb_contacts['allweek']['allday']['callandtext']\n        if nb_contacts:\n            print(filled_box + format_int(\"contacts\", nb_contacts))\n        else:\n            print(empty_box + \"No contacts\")\n\n        if self.has_attributes:\n            print(filled_box + format_int(\"attributes\", len(self.attributes)))\n        else:\n            print(empty_box + \"No attribute stored\")\n\n        if len(self.antennas) == 0:\n            print(empty_box + \"No antenna stored\")\n        else:\n            print(filled_box + format_int(\"antennas\", len(self.antennas)))\n\n        if self.has_recharges:\n            print(filled_box + format_int(\"recharges\", len(self.recharges)))\n        else:\n            print(empty_box + \"No recharges\")\n\n        if self.has_home:\n            print(filled_box + \"Has home\")\n        else:\n            print(empty_box + \"No home\")\n\n        if self.has_text:\n            print(filled_box + \"Has texts\")\n        else:\n            print(empty_box + \"No texts\")\n\n        if self.has_call:\n            print(filled_box + \"Has calls\")\n        else:\n            print(empty_box + \"No calls\")\n\n        if self.has_network:\n            print(filled_box + \"Has network\")\n        else:\n            print(empty_box + \"No network\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recompute_home(self):\n\n        if self.night_start < self.night_end:\n            night_filter = lambda r: self.night_end > r.datetime.time(\n            ) > self.night_start\n        else:\n            night_filter = lambda r: not(\n                self.night_end < r.datetime.time() < self.night_start)\n\n        # Bin positions by chunks of 30 minutes\n        candidates = list(\n            positions_binning(filter(night_filter, self._records)))\n\n        if len(candidates) == 0:\n            self.home = None\n        else:\n            self.home = Counter(candidates).most_common()[0][0]\n\n        self.reset_cache()\n        return self.home", "response": "Return the antenna where the user spends most of his time at night."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_home(self, new_home):\n        if type(new_home) is Position:\n            self.home = new_home\n\n        elif type(new_home) is tuple:\n            self.home = Position(location=new_home)\n\n        else:\n            self.home = Position(antenna=new_home)\n\n        self.reset_cache()", "response": "Sets the user s home."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflatten a nested dictionary.", "response": "def flatten(d, parent_key='', separator='__'):\n    \"\"\"\n    Flatten a nested dictionary.\n\n    Parameters\n    ----------\n    d: dict_like\n        Dictionary to flatten.\n    parent_key: string, optional\n        Concatenated names of the parent keys.\n    separator: string, optional\n        Separator between the names of the each key.\n        The default separator is '_'.\n\n    Examples\n    --------\n\n    >>> d = {'alpha': 1, 'beta': {'a': 10, 'b': 42}}\n    >>> flatten(d) == {'alpha': 1, 'beta_a': 10, 'beta_b': 42}\n    True\n    >>> flatten(d, separator='.') == {'alpha': 1, 'beta.a': 10, 'beta.b': 42}\n    True\n\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + separator + k if parent_key else k\n        if isinstance(v, (dict, OrderedDict)):\n            items.extend(flatten(v, new_key, separator).items())\n        else:\n            items.append((new_key, v))\n    return OrderedDict(items)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef all(user, groupby='week', summary='default', network=False,\n        split_week=False, split_day=False, filter_empty=True, attributes=True,\n        flatten=False):\n    \"\"\"\n    Returns a dictionary containing all bandicoot indicators for the user,\n    as well as reporting variables.\n\n    Relevant indicators are defined in the 'individual', and 'spatial' modules.\n\n    =================================== =======================================================================\n    Reporting variables                 Description\n    =================================== =======================================================================\n    antennas_path                       path of the CSV file containing antennas locations\n    attributes_path                     directory where attributes were loaded\n    version                             bandicoot version\n    groupby                             grouping method ('week' or None)\n    split_week                          whether or not indicators are also computed for weekday and weekend\n    split_day                           whether or not indicators are also computed for day and night\n    start_time                          time of the first record\n    end_time                            time of the last record\n    night_start, night_end              start and end time to define nights\n    weekend                             days used to define the weekend (``[6, 7]`` by default, where 1 is Monday)\n    bins                                number of weeks if the record are grouped\n    has_call                            whether or not records include calls\n    has_text                            whether or not records include texts\n    has_home                            whether or not a :meth:`home location <bandicoot.core.User.recompute_home>` has been found\n    has_network                         whether or not correspondents where loaded\n    percent_records_missing_location    percentage of records without location\n    antennas_missing_locations          number of antennas missing a location\n    percent_outofnetwork_calls          percentage of calls, received or emitted, made with a correspondant not loaded in the network\n    percent_outofnetwork_texts          percentage of texts with contacts not loaded in the network\n    percent_outofnetwork_contacts       percentage of contacts not loaded in the network\n    percent_outofnetwork_call_durations percentage of minutes of calls where the contact was not loaded in the network\n    number_of_records                   total number of records\n    number_of_weeks                     number of weeks with records\n    =================================== =======================================================================\n\n    We also include a last set of reporting variables, for the records ignored\n    at load-time. Values can be ignored due to missing or inconsistent fields\n    (e.g., not including a valid 'datetime' value).\n\n    .. code-block:: python\n\n        {\n            'all': 0,\n            'interaction': 0,\n            'direction': 0,\n            'correspondent_id': 0,\n            'datetime': 0,\n            'call_duration': 0\n        }\n\n    with the total number of records ignored (key ``'all'``), as well as the\n    number of records with faulty values for each columns.\n    \"\"\"\n    scalar_type = 'distribution_scalar' if groupby is not None else 'scalar'\n    summary_type = 'distribution_summarystats' if groupby is not None else 'summarystats'\n\n    number_of_interactions_in = partial(bc.individual.number_of_interactions, direction='in')\n    number_of_interactions_in.__name__ = 'number_of_interaction_in'\n    number_of_interactions_out = partial(bc.individual.number_of_interactions, direction='out')\n    number_of_interactions_out.__name__ = 'number_of_interaction_out'\n\n    functions = [\n        (bc.individual.active_days, scalar_type),\n        (bc.individual.number_of_contacts, scalar_type),\n        (bc.individual.call_duration, summary_type),\n        (bc.individual.percent_nocturnal, scalar_type),\n        (bc.individual.percent_initiated_conversations, scalar_type),\n        (bc.individual.percent_initiated_interactions, scalar_type),\n        (bc.individual.response_delay_text, summary_type),\n        (bc.individual.response_rate_text, scalar_type),\n        (bc.individual.entropy_of_contacts, scalar_type),\n        (bc.individual.balance_of_contacts, summary_type),\n        (bc.individual.interactions_per_contact, summary_type),\n        (bc.individual.interevent_time, summary_type),\n        (bc.individual.percent_pareto_interactions, scalar_type),\n        (bc.individual.percent_pareto_durations, scalar_type),\n        (bc.individual.number_of_interactions, scalar_type),\n        (number_of_interactions_in, scalar_type),\n        (number_of_interactions_out, scalar_type),\n        (bc.spatial.number_of_antennas, scalar_type),\n        (bc.spatial.entropy_of_antennas, scalar_type),\n        (bc.spatial.percent_at_home, scalar_type),\n        (bc.spatial.radius_of_gyration, scalar_type),\n        (bc.spatial.frequent_antennas, scalar_type),\n        (bc.spatial.churn_rate, scalar_type)\n    ]\n\n    if user.has_recharges:\n        functions += [\n            (bc.recharge.amount_recharges, summary_type),\n            (bc.recharge.interevent_time_recharges, summary_type),\n            (bc.recharge.percent_pareto_recharges, scalar_type),\n            (bc.recharge.number_of_recharges, scalar_type),\n            (bc.recharge.average_balance_recharges, scalar_type)\n        ]\n\n    network_functions = [\n        bc.network.clustering_coefficient_unweighted,\n        bc.network.clustering_coefficient_weighted,\n        bc.network.assortativity_attributes,\n        bc.network.assortativity_indicators\n    ]\n\n    groups = list(group_records(user.records, groupby=groupby))\n    bins_with_data = len(groups)\n\n    groups = list(group_records_with_padding(user.records, groupby=groupby))\n    bins = len(groups)\n    bins_without_data = bins - bins_with_data\n\n    reporting = OrderedDict([\n        ('antennas_path', user.antennas_path),\n        ('attributes_path', user.attributes_path),\n        ('recharges_path', user.attributes_path),\n        ('version', bc.__version__),\n        ('code_signature', bc.helper.tools.bandicoot_code_signature()),\n        ('groupby', groupby),\n        ('split_week', split_week),\n        ('split_day', split_day),\n        ('start_time', user.start_time and str(user.start_time)),\n        ('end_time', user.end_time and str(user.end_time)),\n        ('night_start', str(user.night_start)),\n        ('night_end', str(user.night_end)),\n        ('weekend', user.weekend),\n        ('number_of_records', len(user.records)),\n        ('number_of_antennas', len(user.antennas)),\n        ('number_of_recharges', len(user.recharges)),\n        ('bins', bins),\n        ('bins_with_data', bins_with_data),\n        ('bins_without_data', bins_without_data),\n        ('has_call', user.has_call),\n        ('has_text', user.has_text),\n        ('has_home', user.has_home),\n        ('has_recharges', user.has_recharges),\n        ('has_attributes', user.has_attributes),\n        ('has_network', user.has_network),\n        ('percent_records_missing_location', bc.helper.tools.percent_records_missing_location(user)),\n        ('antennas_missing_locations', bc.helper.tools.antennas_missing_locations(user)),\n        ('percent_outofnetwork_calls', user.percent_outofnetwork_calls),\n        ('percent_outofnetwork_texts', user.percent_outofnetwork_texts),\n        ('percent_outofnetwork_contacts', user.percent_outofnetwork_contacts),\n        ('percent_outofnetwork_call_durations', user.percent_outofnetwork_call_durations),\n    ])\n\n    if user.ignored_records is not None:\n        reporting['ignored_records'] = OrderedDict(user.ignored_records)\n\n    returned = OrderedDict([\n        ('name', user.name),\n        ('reporting', reporting)\n    ])\n\n    for fun, datatype in functions:\n        try:\n            metric = fun(user, groupby=groupby, summary=summary,\n                         datatype=datatype, filter_empty=filter_empty,\n                         split_week=split_week, split_day=split_day)\n        except ValueError:\n            metric = fun(user, groupby=groupby, datatype=datatype,\n                         split_week=split_week, filter_empty=filter_empty,\n                         split_day=split_day)\n\n        returned[fun.__name__] = metric\n\n    if network and user.has_network:\n        for fun in network_functions:\n            returned[fun.__name__] = fun(user)\n\n    if attributes and user.attributes != {}:\n        returned['attributes'] = OrderedDict(user.attributes)\n\n    if flatten is True:\n        return globals()['flatten'](returned)\n\n    return returned", "response": "Returns a dictionary containing all bandicoot indicators for a user and optionally all attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the distribution of time between consecutive recharges of the user.", "response": "def interevent_time_recharges(recharges):\n    \"\"\"\n    Return the distribution of time between consecutive recharges\n    of the user.\n    \"\"\"\n    time_pairs = pairwise(r.datetime for r in recharges)\n    times = [(new - old).total_seconds() for old, new in time_pairs]\n    return summary_stats(times)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the percentage of recharges that account for 80% of total recharged amount.", "response": "def percent_pareto_recharges(recharges, percentage=0.8):\n    \"\"\"\n    Percentage of recharges that account for 80% of total recharged amount.\n    \"\"\"\n    amounts = sorted([r.amount for r in recharges], reverse=True)\n    total_sum = sum(amounts)\n    partial_sum = 0\n\n    for count, a in enumerate(amounts):\n        partial_sum += a\n        if partial_sum >= percentage * total_sum:\n            break\n\n    return (count + 1) / len(recharges)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef average_balance_recharges(user, **kwargs):\n\n    balance = 0\n    for r1, r2 in pairwise(user.recharges):\n        # If the range is less than 1 day, cap at 1\n        balance += r1.amount * min(1, (r2.datetime - r1.datetime).days) / 2\n\n    first_recharge = user.recharges[0]\n    last_recharge = user.recharges[-1]\n    duration = (last_recharge.datetime - first_recharge.datetime).days\n    return balance / min(1, duration)", "response": "Return the average daily balance estimated from all recharges."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nround a time DOWN to half nearest half - hour.", "response": "def _round_half_hour(record):\n    \"\"\"\n    Round a time DOWN to half nearest half-hour.\n    \"\"\"\n    k = record.datetime + timedelta(minutes=-(record.datetime.minute % 30))\n    return datetime(k.year, k.month, k.day, k.hour, k.minute, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the keys associated with each axis of the matrices.", "response": "def matrix_index(user):\n    \"\"\"\n    Returns the keys associated with each axis of the matrices.\n\n    The first key is always the name of the current user, followed by the\n    sorted names of all the correspondants.\n    \"\"\"\n\n    other_keys = sorted([k for k in user.network.keys() if k != user.name])\n    return [user.name] + other_keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a directed unweighted matrix where an edge exists if there is at least one call or text.", "response": "def matrix_directed_unweighted(user):\n    \"\"\"\n    Returns a directed, unweighted matrix where an edge exists if there is at\n    least one call or text.\n    \"\"\"\n    matrix = _interaction_matrix(user, interaction=None)\n    for a in range(len(matrix)):\n        for b in range(len(matrix)):\n            if matrix[a][b] is not None and matrix[a][b] > 0:\n                matrix[a][b] = 1\n\n    return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef matrix_undirected_weighted(user, interaction=None):\n    matrix = _interaction_matrix(user, interaction=interaction)\n    result = [[0 for _ in range(len(matrix))] for _ in range(len(matrix))]\n\n    for a in range(len(matrix)):\n        for b in range(len(matrix)):\n            if a != b and matrix[a][b] and matrix[b][a]:\n                result[a][b] = matrix[a][b] + matrix[b][a]\n            elif matrix[a][b] is None or matrix[b][a] is None:\n                result[a][b] = None\n            else:\n                result[a][b] = 0\n\n    return result", "response": "Returns an undirected weighted matrix for call text and call duration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an undirected unweighted matrix where an edge exists if the relationship is reciprocated.", "response": "def matrix_undirected_unweighted(user):\n    \"\"\"\n    Returns an undirected, unweighted matrix where an edge exists if the\n    relationship is reciprocated.\n    \"\"\"\n    matrix = matrix_undirected_weighted(user, interaction=None)\n    for a, b in combinations(range(len(matrix)), 2):\n        if matrix[a][b] is None or matrix[b][a] is None:\n            continue\n\n        if matrix[a][b] > 0 and matrix[b][a] > 0:\n            matrix[a][b], matrix[b][a] = 1, 1\n\n    return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clustering_coefficient_unweighted(user):\n    matrix = matrix_undirected_unweighted(user)\n    closed_triplets = 0\n\n    for a, b in combinations(range(len(matrix)), 2):\n        a_b, a_c, b_c = matrix[a][b], matrix[a][0], matrix[b][0]\n\n        if a_b is None or a_c is None or b_c is None:\n            continue\n\n        if a_b > 0 and a_c > 0 and b_c > 0:\n            closed_triplets += 1.\n\n    d_ego = sum(matrix[0])\n    return 2 * closed_triplets / (d_ego * (d_ego - 1)) if d_ego > 1 else 0", "response": "This function calculates the clustering coefficient of the user in the unweighted undirected ego\n    network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clustering_coefficient_weighted(user, interaction=None):\n    matrix = matrix_undirected_weighted(user, interaction=interaction)\n    weights = [weight for g in matrix for weight in g if weight is not None]\n\n    if len(weights) == 0:\n        return None\n    max_weight = max(weights)\n    triplet_weight = 0\n\n    for a, b in combinations(range(len(matrix)), 2):\n        a_b, a_c, b_c = matrix[a][b], matrix[a][0], matrix[b][0]\n\n        if a_b is None or a_c is None or b_c is None:\n            continue\n\n        if a_b and a_c and b_c:\n            triplet_weight += (a_b * a_c * b_c) ** (1 / 3) / max_weight\n\n    d_ego = sum(1 for i in matrix[0] if i > 0)\n    return 2 * triplet_weight / (d_ego * (d_ego - 1)) if d_ego > 1 else 0", "response": "This method computes the clustering coefficient of the user s weighted undirected network."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the assortativity of indicators for all bandicoot indicators.", "response": "def assortativity_indicators(user):\n    \"\"\"\n    Computes the assortativity of indicators.\n\n    This indicator measures the similarity of the current user with his\n    correspondants, for all bandicoot indicators. For each one, it calculates\n    the variance of the current user's value with the values for all his\n    correspondants:\n\n    .. math::\n\n        \\\\text{assortativity}(J) = \\\\frac{1}{n} \\\\sum_i^n (J_{\\\\text{user}} - J_{\\\\text{i}})^2\n\n    for the indicator :math:`J`, and all the :math:`n` correspondents.\n    \"\"\"\n\n    matrix = matrix_undirected_unweighted(user)\n\n    count_indicator = defaultdict(int)\n    total_indicator = defaultdict(int)\n\n    # Use all indicator except reporting variables and attributes\n    ego_indics = all(user, flatten=True)\n    ego_indics = {a: value for a, value in ego_indics.items()\n                  if a != \"name\" and a[:11] != \"reporting__\" and\n                  a[:10] != \"attributes\"}\n\n    for i, u_name in enumerate(matrix_index(user)):\n        correspondent = user.network.get(u_name, None)\n\n        # Non reciprocated edge\n        if correspondent is None or u_name == user.name or matrix[0][i] == 0:\n            continue\n\n        neighbor_indics = all(correspondent, flatten=True)\n        for a in ego_indics:\n            if ego_indics[a] is not None and neighbor_indics[a] is not None:\n                total_indicator[a] += 1\n                count_indicator[a] += (ego_indics[a] - neighbor_indics[a]) ** 2\n\n    assortativity = {}\n    for i in count_indicator:\n        assortativity[i] = count_indicator[i] / total_indicator[i]\n\n    return assortativity"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assortativity_attributes(user):\n\n    matrix = matrix_undirected_unweighted(user)\n\n    neighbors = [k for k in user.network.keys() if k != user.name]\n    neighbors_attrbs = {}\n    for i, u_name in enumerate(matrix_index(user)):\n        correspondent = user.network.get(u_name, None)\n        if correspondent is None or u_name == user.name or matrix[0][i] == 0:\n            continue\n\n        if correspondent.has_attributes:\n            neighbors_attrbs[correspondent.name] = correspondent.attributes\n\n    assortativity = {}\n    for a in user.attributes:\n        total = sum(1 for n in neighbors if n in neighbors_attrbs and user.attributes[a] == neighbors_attrbs[n][a])\n        den = sum(1 for n in neighbors if n in neighbors_attrbs)\n        assortativity[a] = total / den if den != 0 else None\n\n    return assortativity", "response": "Calculates the assortativity of the nominal attributes of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef network_sampling(n, filename, directory=None, snowball=False, user=None):\n    if snowball:\n        if user is None:\n            raise ValueError(\"Must specify a starting user from whom to initiate the snowball\")\n        else:\n            users, agenda = [user], [user]\n            while len(agenda) > 0:\n                parent = agenda.pop()\n                dealphebetized_network = sorted(parent.network.items(), key=lambda k: random.random())\n                for neighbor in dealphebetized_network:\n                    if neighbor[1] not in users and neighbor[1] is not None and len(users) < n:\n                        users.append(neighbor[1])\n                        if neighbor[1].network:\n                            agenda.push(neighbor[1])\n    else:\n        files = [x for x in os.listdir(directory) if os.path.isfile(os.path.join(directory, x))]\n        shuffled_files = sorted(files, key=lambda k: random.random())\n        user_names = shuffled_files[:n]\n        users = [bc.read_csv(u[:-4], directory) for u in user_names]\n    if len(users) < n:\n        raise ValueError(\"Specified more users than records that exist, only {} records available\".format(len(users)))\n\n    bc.to_csv([bc.utils.all(u) for u in users], filename)", "response": "This function returns a network of n users and exports a CSV of indicators for them."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user_data(user):\n    # For the dasboard, indicators are computed on a daily basis\n    # and by taking into account empty time windows\n    _range = _group_range(user.records, 'day')\n    export = OrderedDict([\n        ('name', 'me'),\n        ('date_range', [x.strftime('%Y-%m-%d') for x in _range]),\n        ('groupby', 'day'),\n        ('indicators', OrderedDict()),\n        ('agg', OrderedDict())\n    ])\n\n    class Indicator(object):\n        def __init__(self, name, function, interaction=None, **args):\n            self.name = name\n            self.function = function\n            self.interaction = interaction\n            self.args = args\n\n    I = Indicator\n    import bandicoot.individual as iv\n\n    indicators_list = [\n        I('nb_out_call', iv.number_of_interactions, 'call', direction='out'),\n        I('nb_out_text', iv.number_of_interactions, 'text', direction='out'),\n        I('nb_inc_call', iv.number_of_interactions, 'call', direction='in'),\n        I('nb_inc_text', iv.number_of_interactions, 'text', direction='in'),\n        I('nb_out_all', iv.number_of_interactions, 'callandtext', direction='out'),\n        I('nb_inc_all', iv.number_of_interactions, 'callandtext', direction='in'),\n        I('nb_all', iv.number_of_interactions, 'callandtext'),\n        I('response_delay', iv.response_delay_text, 'callandtext'),\n        I('response_rate', iv.response_rate_text, 'callandtext'),\n        I('call_duration', iv.call_duration, 'call'),\n        I('percent_initiated_interactions', iv.percent_initiated_interactions, 'call'),\n        I('percent_initiated_conversations', iv.percent_initiated_interactions, 'callandtext'),\n        I('active_day', iv.active_days, 'callandtext'),\n        I('number_of_contacts', iv.number_of_contacts, 'callandtext'),\n        I('percent_nocturnal', iv.percent_nocturnal, 'callandtext'),\n        I('balance_of_contacts', iv.balance_of_contacts, 'callandtext', weighted=False),\n    ]\n\n    ARGS = {'groupby': 'day', 'summary': None, 'filter_empty': False}\n    for i in indicators_list:\n        arguments = i.args\n        arguments.update(ARGS)\n        rv = i.function(user, interaction=i.interaction, **arguments)\n        export['indicators'][i.name] = rv['allweek']['allday'][i.interaction]\n\n    # Format percentages from [0, 1] to [0, 100]\n    with_percentage = ['percent_initiated_interactions', 'percent_nocturnal',\n                       'response_rate', 'balance_of_contacts',\n                       'percent_initiated_conversations']\n\n    def apply_percent(d):\n        if isinstance(d, list):\n            return [apply_percent(dd) for dd in d]\n        elif d is None:\n            return d\n        else:\n            return 100. * d\n\n    for i in with_percentage:\n        export['indicators'][i] = apply_percent(export['indicators'][i])\n\n    # Day by day network\n    def groupby_day_correspondent(r):\n        return (r.datetime.strftime('%Y-%m-%d'), r.correspondent_id)\n    it = itertools.groupby(user.records, groupby_day_correspondent)\n    export['network'] = [list(key) + [len(list(value))] for key, value in it]\n\n    return export", "response": "Compute indicators and statistics used by the visualization\n    and returns a dictionnary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport the visualization to a temporary directory.", "response": "def export(user, directory=None, warnings=True):\n    \"\"\"\n    Build a temporary directory with the visualization.\n    Returns the local path where files have been written.\n\n    Examples\n    --------\n\n        >>> bandicoot.visualization.export(U)\n        Successfully exported the visualization to /tmp/tmpsIyncS\n\n    \"\"\"\n    # Get dashboard directory\n    current_file = os.path.realpath(__file__)\n    current_path = os.path.dirname(current_file)\n    dashboard_path = os.path.join(current_path, 'dashboard_src')\n\n    # Create a temporary directory if needed and copy all files\n    if directory:\n        dirpath = directory\n    else:\n        dirpath = tempfile.mkdtemp()\n\n    # Copy all files except source code\n    copy_tree(dashboard_path + '/public', dirpath, update=1)\n\n    # Export indicators\n    data = user_data(user)\n    bc.io.to_json(data, dirpath + '/data/bc_export.json', warnings=False)\n\n    if warnings:\n        print(\"Successfully exported the visualization to %s\" % dirpath)\n\n    return dirpath"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a temporary directory with a visualization and serve it over HTTP. Examples -------- >>> bandicoot.visualization.run(U) Successfully exported the visualization to /tmp/tmpsIyncS Serving bandicoot visualization at http://0.0.0.0:4242", "response": "def run(user, port=4242):\n    \"\"\"\n    Build a temporary directory with a visualization and serve it over HTTP.\n\n    Examples\n    --------\n\n        >>> bandicoot.visualization.run(U)\n        Successfully exported the visualization to /tmp/tmpsIyncS\n        Serving bandicoot visualization at http://0.0.0.0:4242\n    \"\"\"\n    owd = os.getcwd()\n    dir = export(user)\n    os.chdir(dir)\n\n    Handler = SimpleHTTPServer.SimpleHTTPRequestHandler\n    try:\n        httpd = SocketServer.TCPServer((\"\", port), Handler)\n        print(\"Serving bandicoot visualization at http://0.0.0.0:%i\" % port)\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        print(\"^C received, shutting down the web server\")\n        httpd.server_close()\n    finally:\n        os.chdir(owd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_csv(objects, filename, digits=5, warnings=True):\n\n    if not isinstance(objects, list):\n        objects = [objects]\n\n    data = [flatten(obj) for obj in objects]\n    all_keys = [d for datum in data for d in datum.keys()]\n    field_names = sorted(set(all_keys), key=lambda x: all_keys.index(x))\n\n    with open(filename, 'w') as f:\n        w = csv.writer(f)\n        w.writerow(field_names)\n\n        def make_repr(item):\n            if item is None:\n                return None\n            elif isinstance(item, float):\n                return repr(round(item, digits))\n            else:\n                return str(item)\n\n        for row in data:\n            row = dict((k, make_repr(v)) for k, v in row.items())\n            w.writerow([make_repr(row.get(k, None)) for k in field_names])\n\n    if warnings:\n        print(\"Successfully exported {} object(s) to {}\".format(len(objects),\n              filename))", "response": "Export the flatten indicators of one or several users to CSV."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_json(objects, filename, warnings=True):\n\n    if not isinstance(objects, list):\n        objects = [objects]\n\n    obj_dict = OrderedDict([(obj['name'], obj) for obj in objects])\n\n    with open(filename, 'w') as f:\n        f.write(dumps(obj_dict, indent=4, separators=(',', ': ')))\n\n    if warnings:\n        print(\"Successfully exported {} object(s) to {}\".format(len(objects),\n              filename))", "response": "Exports the indicators of one or several users to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a raw data dictionary and return a Record object.", "response": "def _parse_record(data, duration_format='seconds'):\n    \"\"\"\n    Parse a raw data dictionary and return a Record object.\n    \"\"\"\n\n    def _map_duration(s):\n        if s == '':\n            return None\n        elif duration_format.lower() == 'seconds':\n            return int(s)\n        else:\n            t = time.strptime(s, duration_format)\n            return 3600 * t.tm_hour + 60 * t.tm_min + t.tm_sec\n\n    def _map_position(data):\n        antenna = Position()\n\n        if 'antenna_id' in data and data['antenna_id']:\n            antenna.antenna = data['antenna_id']\n\n        if 'place_id' in data:\n            raise NameError(\"Use field name 'antenna_id' in input files. \"\n                            \"'place_id' is deprecated.\")\n\n        if 'latitude' in data and 'longitude' in data:\n            latitude = data['latitude']\n            longitude = data['longitude']\n            # latitude and longitude should not be empty strings.\n            if latitude and longitude:\n                antenna.location = float(latitude), float(longitude)\n\n        return antenna\n\n    return Record(interaction=data['interaction'] if data['interaction'] else None,\n                  direction=data['direction'],\n                  correspondent_id=data['correspondent_id'],\n                  datetime=_tryto(\n                      lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"),\n                      data['datetime']),\n                  call_duration=_tryto(_map_duration, data['call_duration']),\n                  position=_tryto(_map_position, data))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter records and remove items with missing or inconsistent fields", "response": "def filter_record(records):\n    \"\"\"\n    Filter records and remove items with missing or inconsistent fields\n\n    Parameters\n    ----------\n\n    records : list\n        A list of Record objects\n\n    Returns\n    -------\n    records, ignored : (Record list, dict)\n        A tuple of filtered records, and a dictionary counting the\n        missings fields\n\n    \"\"\"\n\n    def scheme(r):\n        if r.interaction is None:\n            call_duration_ok = True\n        elif r.interaction == 'call':\n            call_duration_ok = isinstance(r.call_duration, (int, float))\n        else:\n            call_duration_ok = True\n\n        callandtext = r.interaction in ['call', 'text']\n        not_callandtext = not callandtext\n\n        return {\n            'interaction': r.interaction in ['call', 'text', 'gps', None],\n            'direction': (not_callandtext and r.direction is None) or r.direction in ['in', 'out'],\n            'correspondent_id': not_callandtext or (r.correspondent_id not in [None, '']),\n            'datetime': isinstance(r.datetime, datetime),\n            'call_duration': call_duration_ok,\n            'location': callandtext or r.position.type() is not None\n        }\n\n    ignored = OrderedDict([\n        ('all', 0),\n        ('interaction', 0),\n        ('direction', 0),\n        ('correspondent_id', 0),\n        ('datetime', 0),\n        ('call_duration', 0),\n        ('location', 0),\n    ])\n\n    bad_records = []\n\n    def _filter(records):\n        for r in records:\n            valid = True\n            for key, valid_key in scheme(r).items():\n                if not valid_key:\n                    ignored[key] += 1\n                    bad_records.append(r)\n                    # Not breaking, to count all fields with errors\n                    valid = False\n\n            if valid:\n                yield r\n            else:\n                ignored['all'] += 1\n\n    return list(_filter(records)), ignored, bad_records"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(name, records, antennas, attributes=None, recharges=None,\n         antennas_path=None, attributes_path=None, recharges_path=None,\n         describe=False, warnings=False, drop_duplicates=False):\n    \"\"\"\n    Low-level function to create a new user. This function is used by\n    read_csv, read_orange, and read_telenor.\n\n    `load` is a low-level function which:\n\n    - assigns records, antennas, and attributes to a new\n      :class:`~bandicoot.core.User` object,\n    - outputs warnings on the standard output if records or\n      antennas are missing fields or have wrong values,\n    - filters records with wrong values and drop duplicates records if\n      asked.\n\n\n    Parameters\n    ----------\n    name : str\n        The name of the user. It is stored in User.name and is useful when\n        exporting metrics about multiple users.\n\n    records : list\n        List of :class:`~bandicoot.core.Record` objects.\n\n    antennas : dict\n        Dictionary of the position for each antenna.\n\n    attributes : dict\n        A (key,value) dictionary of attributes for the current user\n\n    recharges : list\n        List of :class:`~bandicoot.core.Recharge` objects.\n\n    antennas_path : str\n        Path of the antenna file. It will be stored in\n        :attr:`User.antennas_path <bandicoot.core.User.antennas_path>`.\n\n    attributes_path : str\n        Path of the attributes file. It will be stored in\n        :attr:`User.attributes_path <bandicoot.core.User.attributes_path>`.\n\n    recharges_path : str\n        Path of the recharges file. It will be stored in\n        :attr:`User.recharges_path <bandicoot.core.User.recharges_path>`.\n\n    describe : boolean, default: False\n        Print a description of the loaded user to the standard output.\n\n    warnings : boolean, default: True\n        Output warnings on the standard output.\n\n    drop_duplicates : boolean, default False\n        Remove duplicate records, and issue a warning message with the number\n        of removed records.\n\n\n    Examples\n    --------\n\n\n    .. code-block:: python\n\n       >>> records = [Record(...),...]\n       >>> antennas = {'A51': (37.245265, 115.803418),...}\n       >>> attributes = {'age': 60}\n       >>> load(\"Frodo\", records, antennas, attributes)\n\n    will returns a new User object.\n    \"\"\"\n\n    user = User()\n    user.name = name\n    user.antennas_path = antennas_path\n    user.attributes_path = attributes_path\n    user.recharges_path = recharges_path\n\n    user.records, ignored, bad_records = filter_record(records)\n\n    _level = log.getLogger().level\n    if warnings is False:\n        log.getLogger().setLevel(log.ERROR)\n\n    if ignored['all'] != 0:\n        w = \"{} record(s) were removed due to \" \\\n            \"missing or incomplete fields.\".format(ignored['all'])\n        for k in ignored.keys():\n            if k != 'all' and ignored[k] != 0:\n                w += \"\\n\" + \" \" * 9 + \"%s: %i record(s) with \" \\\n                     \"incomplete values\" % (k, ignored[k])\n        log.warn(w)\n\n    user.ignored_records = dict(ignored)\n\n    if antennas is not None:\n        user.antennas = antennas\n    if attributes is not None:\n        user.attributes = attributes\n    if recharges is not None:\n        user.recharges = recharges\n\n    if not user.has_attributes and user.attributes_path is not None:\n        log.warn(\"Attributes path {} is given, but no \"\n                 \"attributes are loaded.\".format(attributes_path))\n\n    if not user.has_recharges and user.recharges_path is not None:\n        log.warn(\"Recharges path {} is given, but no \"\n                 \"recharges are loaded.\".format(recharges_path))\n\n    percent_missing = percent_records_missing_location(user)\n    if percent_missing > 0:\n        w = \"{0:.2%} of the records are missing \" \\\n            \"a location.\".format(percent_missing)\n        if antennas is None:\n            w += \"\\n\" + \" \" * 9 + \"No antennas file was given and \" \\\n                 \"records are using antennas for position.\"\n        log.warn(w)\n\n    msg_loc = antennas_missing_locations(user)\n    if msg_loc > 0:\n        log.warn(\"{} antenna(s) are missing a location.\".format(msg_loc))\n\n    sorted_min_records = sorted(set(user.records), key=lambda r: r.datetime)\n    num_dup = len(user.records) - len(sorted_min_records)\n    if num_dup > 0:\n        if drop_duplicates:\n            user.records = sorted_min_records\n            log.error(\"{0:d} duplicated record(s) were \"\n                      \"removed.\".format(num_dup), extra={'prefix': \"Warning!\"})\n        else:\n            log.warn(\"{0:d} record(s) are duplicated.\".format(num_dup))\n\n    pct_overlap_calls = percent_overlapping_calls(user.records, 300)\n    if pct_overlap_calls > 0:\n        log.warn(\"{0:.2%} of calls overlap the next call by more than \"\n                 \"5 minutes.\".format(pct_overlap_calls))\n\n    if describe:\n        user.describe()\n\n    log.getLogger().setLevel(_level)\n    return user, bad_records", "response": "This function loads a new user into a new User object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a user s record from a CSV file.", "response": "def read_csv(user_id, records_path, antennas_path=None, attributes_path=None,\n             recharges_path=None, network=False, duration_format='seconds',\n             describe=True, warnings=True, errors=False, drop_duplicates=False):\n    \"\"\"\n    Load user records from a CSV file.\n\n    Parameters\n    ----------\n\n    user_id : str\n        ID of the user (filename)\n\n    records_path : str\n        Path of the directory all the user files.\n\n    antennas_path : str, optional\n        Path of the CSV file containing (place_id, latitude, longitude) values.\n        This allows antennas to be mapped to their locations.\n\n    recharges_path : str, optional\n        Path of the directory containing recharges files\n        (``datetime, amount, balance, retailer_id`` CSV file).\n\n    antennas_path : str, optional\n        Path of the CSV file containing (place_id, latitude, longitude) values.\n        This allows antennas to be mapped to their locations.\n\n    network : bool, optional\n        If network is True, bandicoot loads the network of the user's\n        correspondants from the same path. Defaults to False.\n\n    duration_format : str, default is 'seconds'\n        Allows reading records with call duration specified in other formats\n        than seconds. Options are 'seconds' or any format such as '%H:%M:%S',\n        '%M%S', etc.\n\n    describe : boolean\n        If describe is True, it will print a description of the loaded user\n        to the standard output.\n\n    errors : boolean\n        If errors is True, returns a tuple (user, errors), where user is the\n        user object and errors are the records which could not be loaded.\n\n    drop_duplicates : boolean\n        If drop_duplicates, remove \"duplicated records\" (same correspondants,\n        direction, date and time). Not activated by default.\n\n\n    Examples\n    --------\n\n    >>> user = bandicoot.read_csv('sample_records', '.')\n    >>> print len(user.records)\n    10\n\n    >>> user = bandicoot.read_csv('sample_records', 'samples', 'sample_places.csv')\n    >>> print len(user.antennas)\n    5\n\n    >>> user = bandicoot.read_csv('sample_records', '.', None, 'sample_attributes.csv')\n    >>> print user.attributes['age']\n    25\n\n    Notes\n    -----\n    - The csv files can be single, or double quoted if needed.\n    - Empty cells are filled with ``None``. For example, if the column\n      ``call_duration`` is empty for one record, its value will be ``None``.\n      Other values such as ``\"N/A\"``, ``\"None\"``, ``\"null\"`` will be\n      considered as a text.\n    \"\"\"\n\n    antennas = None\n    if antennas_path is not None:\n        try:\n            with open(antennas_path, 'r') as csv_file:\n                reader = csv.DictReader(csv_file)\n                antennas = dict((d['antenna_id'], (float(d['latitude']),\n                                                   float(d['longitude'])))\n                                for d in reader)\n        except IOError:\n            pass\n\n    user_records = os.path.join(records_path, user_id + '.csv')\n    with open(user_records, 'r') as csv_file:\n        reader = csv.DictReader(csv_file)\n        records = [_parse_record(r, duration_format) for r in reader]\n\n    attributes = None\n    if attributes_path is not None:\n        user_attributes = os.path.join(attributes_path, user_id + '.csv')\n        attributes = _load_attributes(user_attributes)\n\n    recharges = None\n    if recharges_path is not None:\n        user_recharges = os.path.join(recharges_path, user_id + '.csv')\n        recharges = _load_recharges(user_recharges)\n\n    user, bad_records = load(user_id, records, antennas, attributes, recharges,\n                             antennas_path, attributes_path, recharges_path,\n                             describe=False, warnings=warnings,\n                             drop_duplicates=drop_duplicates)\n\n    # Loads the network\n    if network is True:\n        user.network = _read_network(user, records_path, attributes_path,\n                                     read_csv, antennas_path, warnings,\n                                     drop_duplicates=drop_duplicates)\n        user.recompute_missing_neighbors()\n\n    if describe:\n        user.describe()\n\n    if errors:\n        return user, bad_records\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a user s antennas from a CSV file.", "response": "def read_orange(user_id, records_path, antennas_path=None,\n                attributes_path=None, recharges_path=None, network=False,\n                describe=True, warnings=True, errors=False):\n    \"\"\"\n    Load user records from a CSV file in *orange* format:\n\n    ``call_record_type;basic_service;user_msisdn;call_partner_identity;datetime;call_duration;longitude;latitude``\n\n    ``basic_service`` takes one of the following values:\n\n    - 11: telephony;\n    - 12: emergency calls;\n    - 21: short message (in)\n    - 22: short message (out)\n\n    Parameters\n    ----------\n    user_id : str\n        ID of the user (filename)\n\n    records_path : str\n        Path of the directory all the user files.\n\n    antennas_path : str, optional\n        Path of the CSV file containing (antenna_id, latitude, longitude)\n        values. This allows antennas to be mapped to their locations.\n\n    attributes_path : str, optional\n        Path of the directory containing attributes files (``key, value`` CSV\n        file). Attributes can for instance be variables such as like, age, or\n        gender. Attributes can be helpful to compute specific metrics.\n\n    network : bool, optional\n        If network is True, bandicoot loads the network of the user's\n        correspondants from the same path. Defaults to False.\n\n    describe : boolean\n        If describe is True, it will print a description of the loaded user to\n        the standard output.\n\n    errors : boolean\n        If errors is True, returns a tuple (user, errors), where user is the\n        user object and errors are the records which could not be loaded.\n    \"\"\"\n\n    def _parse(reader):\n        records = []\n        antennas = dict()\n\n        for row in reader:\n            direction = 'out' if row['call_record_type'] == '1' else 'in'\n            interaction = 'call' if row[\n                'basic_service'] in ['11', '12'] else 'text'\n            contact = row['call_partner_identity']\n            date = datetime.strptime(row['datetime'], \"%Y-%m-%d %H:%M:%S\")\n            call_duration = float(row['call_duration']) if row[\n                'call_duration'] != \"\" else None\n            lon, lat = float(row['longitude']), float(row['latitude'])\n            latlon = (lat, lon)\n\n            antenna = None\n            for key, value in antennas.items():\n                if latlon == value:\n                    antenna = key\n                    break\n            if antenna is None:\n                antenna = len(antennas) + 1\n                antennas[antenna] = latlon\n\n            position = Position(antenna=antenna, location=latlon)\n\n            record = Record(direction=direction,\n                            interaction=interaction,\n                            correspondent_id=contact,\n                            call_duration=call_duration,\n                            datetime=date,\n                            position=position)\n            records.append(record)\n\n        return records, antennas\n\n    user_records = os.path.join(records_path, user_id + \".csv\")\n    fields = ['call_record_type', 'basic_service', 'user_msisdn',\n              'call_partner_identity', 'datetime', 'call_duration',\n              'longitude', 'latitude']\n\n    with open(user_records, 'r') as f:\n        reader = csv.DictReader(f, delimiter=\";\", fieldnames=fields)\n        records, antennas = _parse(reader)\n\n    attributes = None\n    if attributes_path is not None:\n        user_attributes = os.path.join(attributes_path, user_id + '.csv')\n        attributes = _load_attributes(user_attributes)\n\n    recharges = None\n    if recharges_path is not None:\n        user_recharges = os.path.join(recharges_path, user_id + '.csv')\n        recharges = _load_recharges(user_recharges)\n\n    user, bad_records = load(user_id, records, antennas, attributes, recharges,\n                             antennas_path, attributes_path, recharges_path,\n                             describe=False, warnings=warnings)\n\n    if network is True:\n        user.network = _read_network(user, records_path, attributes_path,\n                                     read_orange, antennas_path, warnings)\n        user.recompute_missing_neighbors()\n\n    if describe:\n        user.describe()\n\n    if errors:\n        return user, bad_records\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_telenor(incoming_cdr, outgoing_cdr, cell_towers, describe=True,\n                 warnings=True):\n    \"\"\"\n    Load user records from a CSV file in *telenor* format, which is only\n    applicable for call records.\n\n    .. warning:: ``read_telenor`` has been deprecated in bandicoot 0.4.\n\n    Parameters\n    ----------\n    incoming_cdr : str\n        Path to the CSV file containing incoming records, using the following\n        scheme: ::\n\n             B_PARTY,A_PARTY,DURATION,B_CELL,CALL_DATE,CALL_TIME,CALL_TYPE\n\n    outgoing_cdr : str\n        Path to the CSV file containing outgoing records, using the following\n        scheme: ::\n\n             A_NUMBER,B_NUMBER,DURATION,B_CELL,CALL_DATE,CALL_TIME,CALL_TYPE\n\n    cell_towers : str\n        Path to the CSV file containing the positions of all\n\n    describe : boolean\n        If describe is True, it will print a description of the loaded user to\n        the standard output.\n\n    \"\"\"\n\n    log.warn(\"read_telenor has been deprecated in bandicoot 0.4.\")\n\n    import itertools\n    import csv\n\n    def parse_direction(code):\n        if code == 'MOC':\n            return 'out'\n        elif code == 'MTC':\n            return 'in'\n        else:\n            raise NotImplementedError\n\n    cells = None\n    with open(cell_towers, 'r') as f:\n        cell_towers_list = csv.DictReader(f)\n        cells = {}\n        for line in cell_towers_list:\n            if line['LONGITUDE'] != '' and line['LATITUDE'] != '':\n                latlon = (float(line['LONGITUDE']), float(line['LATITUDE']))\n                cell_id = line['CELLID_HEX']\n                cells[cell_id] = latlon\n\n    def parse_record(raw):\n        direction = parse_direction(raw['CALL_TYPE'].strip())\n\n        if direction == 'in':\n            contact = raw.get('A_PARTY', raw.get('A_NUMBER'))\n            cell_id = raw['B_CELL']\n        else:\n            contact = raw.get('B_PARTY', raw.get('B_NUMBER'))\n            cell_id = raw['A_CELL']\n\n        position = Position(antenna=cell_id, location=cells.get(cell_id))\n\n        _date_str = raw.get('CDATE', raw.get('CALL_DATE'))\n        _time_str = raw.get('CTIME', raw.get('CALL_TIME'))\n        _datetime = datetime.strptime(_date_str + _time_str,\n                                      \"%Y%m%d%H:%M:%S\")\n\n        r = Record(interaction='call',\n                   direction=direction,\n                   correspondent_id=contact,\n                   call_duration=float(raw['DURATION'].strip()),\n                   datetime=_datetime,\n                   position=position)\n\n        return r\n\n    with open(incoming_cdr, 'r') as f_in:\n        incoming_ = list(map(parse_record, csv.DictReader(f_in)))\n\n        with open(outgoing_cdr, 'r') as f:\n            outgoing_ = list(map(parse_record, csv.DictReader(f)))\n\n            records = itertools.chain(incoming_, outgoing_)\n\n    name = incoming_cdr\n\n    user, errors = load(name, records, cells, warnings=None, describe=False)\n\n    if describe:\n        user.describe()\n\n    return user", "response": "Reads the user s call records from a CSV file in telenor format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the interevent time between two records of the user.", "response": "def interevent_time(records):\n    \"\"\"\n    The interevent time between two records of the user.\n    \"\"\"\n    inter_events = pairwise(r.datetime for r in records)\n    inter = [(new - old).total_seconds() for old, new in inter_events]\n\n    return summary_stats(inter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef number_of_contacts(records, direction=None, more=0):\n    if direction is None:\n        counter = Counter(r.correspondent_id for r in records)\n    else:\n        counter = Counter(r.correspondent_id for r in records if r.direction == direction)\n    return sum(1 for d in counter.values() if d > more)", "response": "Returns the number of contacts the user interacted with."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the entropy of the user s contacts.", "response": "def entropy_of_contacts(records, normalize=False):\n    \"\"\"\n    The entropy of the user's contacts.\n\n    Parameters\n    ----------\n    normalize: boolean, default is False\n        Returns a normalized entropy between 0 and 1.\n\n    \"\"\"\n    counter = Counter(r.correspondent_id for r in records)\n\n    raw_entropy = entropy(counter.values())\n    n = len(counter)\n    if normalize and n > 1:\n        return raw_entropy / math.log(n)\n    else:\n        return raw_entropy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interactions_per_contact(records, direction=None):\n    if direction is None:\n        counter = Counter(r.correspondent_id for r in records)\n    else:\n        counter = Counter(r.correspondent_id for r in records\n                          if r.direction == direction)\n    return summary_stats(counter.values())", "response": "Returns the number of interactions a user had with each of its contacts."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the percentage of calls initiated by the user.", "response": "def percent_initiated_interactions(records, user):\n    \"\"\"\n    The percentage of calls initiated by the user.\n    \"\"\"\n    if len(records) == 0:\n        return 0\n\n    initiated = sum(1 for r in records if r.direction == 'out')\n    return initiated / len(records)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the percentage of interactions the user had at night.", "response": "def percent_nocturnal(records, user):\n    \"\"\"\n    The percentage of interactions the user had at night.\n\n    By default, nights are 7pm-7am. Nightimes can be set in\n    ``User.night_start`` and ``User.night_end``.\n    \"\"\"\n    if len(records) == 0:\n        return 0\n\n    if user.night_start < user.night_end:\n        night_filter = lambda d: user.night_end > d.time() > user.night_start\n    else:\n        night_filter = lambda d: not(user.night_end < d.time() < user.night_start)\n\n    return sum(1 for r in records if night_filter(r.datetime)) / len(records)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef call_duration(records, direction=None):\n    if direction is None:\n        call_durations = [r.call_duration for r in records]\n    else:\n        call_durations = [r.call_duration for r in records if r.direction == direction]\n\n    return summary_stats(call_durations)", "response": "Returns a summary of the duration of the user s calls."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _conversations(group, delta=datetime.timedelta(hours=1)):\n    last_time = None\n    results = []\n    for g in group:\n\n        if last_time is None or g.datetime - last_time < delta:\n            if g.interaction == 'text':\n                results.append(g)\n\n            # A call always ends a conversation\n            else:\n                if len(results) != 0:\n                    yield results\n                    results = []\n\n        else:\n            if len(results) != 0:\n                yield results\n\n            if g.interaction == 'call':\n                results = []\n            else:\n                results = [g]\n\n        last_time = g.datetime\n\n    if len(results) != 0:\n        yield results", "response": "Yields the records grouped by conversations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef response_rate_text(records):\n    if len(records) == 0:\n        return None\n\n    interactions = defaultdict(list)\n    for r in records:\n        interactions[r.correspondent_id].append(r)\n\n    def _response_rate(grouped):\n        received, responded = 0, 0\n        conversations = _conversations(grouped)\n\n        for conv in conversations:\n            if len(conv) != 0:\n                first = conv[0]\n                if first.direction == 'in' and first.interaction == 'text':\n                    received += 1\n                    if any((i.direction == 'out' for i in conv)):\n                        responded += 1\n\n        return responded, received\n\n    # Group all records by their correspondent, and compute the response rate\n    # for each\n    all_couples = map(_response_rate, list(interactions.values()))\n    responded, received = map(sum, list(zip(*all_couples)))\n\n    return responded / received if received != 0 else None", "response": "Returns the response rate of a list of text - conversations that began with an incoming text."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a summary of the response delay of the user within a conversation.", "response": "def response_delay_text(records):\n    \"\"\"\n    The response delay of the user within a conversation (in seconds)\n\n    The following sequence of messages defines conversations (``I`` for an\n    incoming text, ``O`` for an outgoing text, ``-`` for a one minute\n    delay): ::\n\n        I-O--I----O, we have a 60 seconds response delay and a 240 seconds response delay\n        O--O---I--O, we have a 1200 seconds response delay\n        I--II---I-I, we don't have a response delay. The user hasn't answered\n\n    For this user, the distribution of response delays will be ``[60, 240, 60]``\n\n    Notes\n    -----\n    See :ref:`Using bandicoot <conversations-label>` for a definition of\n    conversations. Conversation are defined to be a series of text messages each\n    sent no more than an hour after the previous. The response delay can thus\n    not be greater than one hour.\n    \"\"\"\n    interactions = defaultdict(list)\n    for r in records:\n        interactions[r.correspondent_id].append(r)\n\n    def _response_delay(grouped):\n        ts = ((b.datetime - a.datetime).total_seconds()\n              for conv in _conversations(grouped)\n              for a, b in pairwise(conv)\n              if b.direction == 'out' and a.direction == 'in')\n\n        return ts\n\n    delays = [r for i in interactions.values() for r in _response_delay(i)\n              if r > 0]\n\n    return summary_stats(delays)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef percent_initiated_conversations(records):\n    interactions = defaultdict(list)\n    for r in records:\n        interactions[r.correspondent_id].append(r)\n\n    def _percent_initiated(grouped):\n        mapped = [(1 if conv[0].direction == 'out' else 0, 1)\n                  for conv in _conversations(grouped)]\n        return mapped\n\n    all_couples = [sublist for i in interactions.values()\n                   for sublist in _percent_initiated(i)]\n\n    if len(all_couples) == 0:\n        init, total = 0, 0\n    else:\n        init, total = list(map(sum, list(zip(*all_couples))))\n\n    return init / total if total != 0 else 0", "response": "Returns the percentage of conversations that have been initiated by the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef active_days(records):\n    days = set(r.datetime.date() for r in records)\n    return len(days)", "response": "Returns the number of days during which the user is active."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the percentage of contacts that account for 80% of its interactions.", "response": "def percent_pareto_interactions(records, percentage=0.8):\n    \"\"\"\n    The percentage of user's contacts that account for 80% of its interactions.\n    \"\"\"\n    if len(records) == 0:\n        return None\n\n    user_count = Counter(r.correspondent_id for r in records)\n\n    target = int(math.ceil(sum(user_count.values()) * percentage))\n    user_sort = sorted(user_count.keys(), key=lambda x: user_count[x])\n\n    while target > 0 and len(user_sort) > 0:\n        user_id = user_sort.pop()\n        target -= user_count[user_id]\n\n    return (len(user_count) - len(user_sort)) / len(records)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the percentage of time that a user has accounted for 80% of its total time spend on the phone.", "response": "def percent_pareto_durations(records, percentage=0.8):\n    \"\"\"\n    The percentage of user's contacts that account for 80% of its total time\n    spend on the phone. Optionally takes a percentage argument as a decimal\n    (e.g., .8 for 80%).\n    \"\"\"\n    if len(records) == 0:\n        return None\n\n    user_count = defaultdict(int)\n    for r in records:\n        if r.interaction == \"call\":\n            user_count[r.correspondent_id] += r.call_duration\n\n    target = int(math.ceil(sum(user_count.values()) * percentage))\n    user_sort = sorted(user_count.keys(), key=lambda x: user_count[x])\n\n    while target > 0 and len(user_sort) > 0:\n        user_id = user_sort.pop()\n        target -= user_count[user_id]\n\n    return (len(user_count) - len(user_sort)) / len(records)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef balance_of_contacts(records, weighted=True):\n    counter_out = defaultdict(int)\n    counter = defaultdict(int)\n\n    for r in records:\n        if r.direction == 'out':\n            counter_out[r.correspondent_id] += 1\n        counter[r.correspondent_id] += 1\n\n    if not weighted:\n        balance = [counter_out[c] / counter[c] for c in counter]\n    else:\n        balance = [counter_out[c] / sum(counter.values()) for c in counter]\n\n    return summary_stats(balance)", "response": "Calculates the balance of interactions per contact per user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef number_of_interactions(records, direction=None):\n    if direction is None:\n        return len(records)\n    else:\n        return len([r for r in records if r.direction == direction])", "response": "Returns the number of interactions in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_weekmatrices(user, split_interval=60):\n\n    if not float(24 * 60 / split_interval).is_integer():\n        raise ValueError(\n            \"The minute interval set for the week-matrix structure does not evenly divide the day!\")\n\n    contacts_in = partial(bc.individual.number_of_contacts,\n                          direction='in', interaction='callandtext', summary=None)\n    contacts_out = partial(bc.individual.number_of_contacts,\n                           direction='out', interaction='callandtext', summary=None)\n    calls_in = partial(bc.individual.number_of_interactions,\n                       direction='in', interaction='call', summary=None)\n    calls_out = partial(bc.individual.number_of_interactions,\n                        direction='out', interaction='call', summary=None)\n    texts_in = partial(bc.individual.number_of_interactions,\n                       direction='in', interaction='text', summary=None)\n    texts_out = partial(bc.individual.number_of_interactions,\n                        direction='out', interaction='text', summary=None)\n    time_spent_in = partial(bc.individual.call_duration,\n                            direction='in', interaction='call', summary=None)\n    time_spent_out = partial(bc.individual.call_duration,\n                             direction='out', interaction='call', summary=None)\n\n    core_func = [\n        (contacts_in, \"scalar\"),\n        (contacts_out, \"scalar\"),\n        (calls_in, \"scalar\"),\n        (calls_out, \"scalar\"),\n        (texts_in, \"scalar\"),\n        (texts_out, \"scalar\")\n    ]\n\n    time_func = [\n        (time_spent_in, \"summarystats\"),\n        (time_spent_out, \"summarystats\")\n    ]\n\n    wm = []\n    sections = [\n        (i + 1) * split_interval for i in range(7 * 24 * 60 // split_interval)]\n    temp_user = _extract_user_info(user)\n\n    for grouped_records in group_records(user.records, groupby='week'):\n        week_records = list(grouped_records)\n        time_spent_rec = _transform_to_time_spent(\n            week_records, split_interval, sections)\n        wm.extend(_calculate_channels(\n            week_records, sections, split_interval, core_func, temp_user))\n        wm.extend(_calculate_channels(\n            time_spent_rec, sections, split_interval, time_func, temp_user, len(core_func)))\n\n    return wm", "response": "Create week - matrices for a user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_csv(weekmatrices, filename, digits=5):\n\n    with open(filename, 'w') as f:\n        w = csv.writer(f, lineterminator='\\n')\n        w.writerow(['year_week', 'channel', 'weekday', 'section', 'value'])\n\n        def make_repr(item):\n            if item is None:\n                return None\n            elif isinstance(item, float):\n                return repr(round(item, digits))\n            else:\n                return str(item)\n\n        for row in weekmatrices:\n            w.writerow([make_repr(item) for item in row])", "response": "Exports a list of week - matrices to a specified CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a list of week - matrices from a CSV file.", "response": "def read_csv(filename):\n    \"\"\"\n    Read a list of week-matrices from a CSV file.\n    \"\"\"\n\n    with open(filename, 'r') as f:\n        r = csv.reader(f)\n        next(r)  # remove header\n        wm = list(r)\n\n    # remove header and convert to numeric\n    for i, row in enumerate(wm):\n        row[1:4] = map(int, row[1:4])\n        row[4] = float(row[4])\n\n    return wm"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calculate_channels(records, sections, split_interval, channel_funcs, user, c_start=0):\n\n    week_matrix = []\n    if len(records) == 0:\n        return week_matrix\n\n    if not isinstance(records, list):\n        records = [records]\n\n    year_week = str(records[0].datetime.isocalendar()[\n                    0]) + '-' + str(records[0].datetime.isocalendar()[1])\n\n    section_lists, section_id = _weekmatrix_grouping(records, sections, split_interval)\n\n    for c, fun in enumerate(channel_funcs):\n        for b, section_records in enumerate(section_lists):\n            indicator_fun, return_type = fun\n\n            # _records is used to avoid recomputing home\n            user._records = section_records\n            user._cache = {}\n            output = list(indicator_fun(user)['allweek']['allday'].values())[0]\n\n            if return_type == 'scalar':\n                indicator = sum(d for d in output if d is not None)\n            elif return_type == 'summarystats':\n                indicator = sum(d for group in output for d in group if d is not None)\n\n            if indicator != 0:\n                week_matrix.append(\n                    [year_week, c + c_start, section_id[b][0], section_id[b][1], float(indicator)])\n\n    return week_matrix", "response": "Calculates the channels for a user in a week."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse to group a list of records across a week as defined by the supplied sections. Outputs a list containing records in each section and a list with info to identify those sections. Parameters ---------- records : list The week of records to group across the different weekdays/sections. sections : list The list of sections for grouping. Each section will have an integer value stating the minutes away from midnight between Sunday and Monday. split_interval : int The interval in minutes for which each indicator is computed.", "response": "def _weekmatrix_grouping(records, sections, split_interval):\n    \"\"\"\n    Used to group a list of records across a week as defined by the supplied\n    sections. Outputs a list containing records in each section and a list with\n    info to identify those sections.\n\n    Parameters\n    ----------\n    records : list\n        The week of records to group across the different weekdays/sections.\n    sections : list\n        The list of sections for grouping. Each section will have an integer\n        value stating the minutes away from midnight between Sunday and Monday.\n    split_interval : int\n        The interval in minutes for which each indicator is computed.\n    \"\"\"\n\n    def _group_by_weektime(records, sections):\n        for _, group in itertools.groupby(records, key=lambda r: bisect_right(sections, _find_weektime(r.datetime))):\n            yield group\n\n    section_records = _group_by_weektime(records, sections)\n    section_lists = _extract_list_from_generator(section_records)\n    section_indices = [bisect_right(sections, _find_weektime(r_list[0].datetime)) for r_list in section_lists]\n    section_id = _find_day_section_from_indices(\n        section_indices, split_interval)\n    assert(len(section_lists) == len(section_id) and len(\n        section_indices) == len(set(section_indices)))\n\n    return section_lists, section_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform the records to time spent talking in a specific section.", "response": "def _transform_to_time_spent(records, split_interval, sections):\n    \"\"\"\n    Each call that crosses a boundary of the sections in the week-matrix is\n    split. These new records contain the amount of time\n    (in record.call_duration) spent talking in that specific section.\n    \"\"\"\n\n    t_records = []\n    week_nr = records[0].datetime.isocalendar()[1]\n\n    # contrary to the rest of the binning process, this is done with second\n    # precision\n    for r in filter(lambda rec: rec.interaction == 'call' and rec.call_duration > 0, records):\n        t_left = r.call_duration\n        t_to_next_section = _seconds_to_section_split(r, sections)\n        t_spent_total = 0\n\n        while (t_left > 0):\n            t_spent = min(t_to_next_section, t_left)\n            dt_new = r.datetime + dt.timedelta(seconds=t_spent_total)\n\n            if dt_new.isocalendar()[1] > week_nr:\n                dt_new -= dt.timedelta(days=7)\n            t_records.append(\n                Record('call', r.direction, None, dt_new, t_spent, None))\n\n            t_left -= t_spent\n            t_spent_total += t_spent\n            t_to_next_section = split_interval * 60\n\n    return sorted(t_records, key=lambda r: _find_weektime(r.datetime))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new user object with extracted user attributes.", "response": "def _extract_user_info(user):\n    \"\"\"\n    Creates a new user class with extracted user attributes for later use.\n    A new user object is needed to avoid overwritting of e.g. ``user.records``.\n    \"\"\"\n\n    temp_user = User()\n    copy_attributes = [\n        'antennas', 'name', 'night_start', 'night_end', 'weekend', 'home']\n\n    for attr in copy_attributes:\n        setattr(temp_user, attr, getattr(user, attr))\n\n    return temp_user"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_weektime(datetime, time_type='min'):\n\n    if time_type == 'sec':\n        return datetime.weekday() * 24 * 60 * 60 + datetime.hour * 60 * 60 + datetime.minute * 60 + datetime.second\n    elif time_type == 'min':\n        return datetime.weekday() * 24 * 60 + datetime.hour * 60 + datetime.minute\n    else:\n        raise ValueError(\"Invalid time type specified.\")", "response": "Finds the minutes and seconds aways from midnight between Sunday and Monday."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_list_from_generator(generator):\n\n    extracted = []\n    for i in generator:\n        extracted.append(list(i))\n    return extracted", "response": "Extracts all the objects from a generator and adds them to a list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _seconds_to_section_split(record, sections):\n\n    next_section = sections[\n        bisect_right(sections, _find_weektime(record.datetime))] * 60\n    return next_section - _find_weektime(record.datetime, time_type='sec')", "response": "Finds the seconds to the next section from the datetime of a record."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_day_section_from_indices(indices, split_interval):\n\n    cells_day = 24 * 60 // split_interval\n    rv = [[int(math.floor(i / cells_day)), i % cells_day] for i in indices]\n    return rv", "response": "Returns a list with [ weekday section ] identifiers found by a list of indices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the distance matrix between every point in a given list of location tuples.", "response": "def compute_distance_matrix(points):\n    \"\"\"\n    Return a matrix of distance (in meters) between every point in a given list\n    of (lat, lon) location tuples.\n    \"\"\"\n    n = len(points)\n    return [[1000 * great_circle_distance(points[i], points[j])\n             for j in range(n)] for i in range(n)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a matrix of distance between couples of points return the list of every point closer than eps from a certain point.", "response": "def get_neighbors(distance_matrix, source, eps):\n    \"\"\"\n    Given a matrix of distance between couples of points,\n    return the list of every point closer than eps from a certain point.\n    \"\"\"\n\n    return [dest for dest, distance in enumerate(distance_matrix[source]) if distance < eps]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dbscan(points, eps, minpts):\n    next_label = 0\n    n = len(points)\n    labels = [None] * n\n\n    distance_matrix = compute_distance_matrix(points)\n    neighbors = [get_neighbors(distance_matrix, i, eps) for i in range(n)]\n\n    for i in range(n):\n        if labels[i] is not None:\n            continue\n\n        if len(neighbors[i]) < minpts:\n            continue\n\n        labels[i] = next_label\n        candidates = [i]\n        while len(candidates) > 0:\n            c = candidates.pop()\n\n            for j in neighbors[c]:\n                if labels[j] is None:\n                    labels[j] = next_label\n                    if len(neighbors[j]) >= minpts:\n                        candidates.append(j)\n\n        next_label += 1\n\n    return labels", "response": "This function is a simple algorithm that uses DBSCAN to discover cluster labels associated with a list of points. It returns the labels associated with the points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of dict for each stop location and records.", "response": "def get_stops(records, group_dist):\n    \"\"\"\n    Group records arounds stop locations and returns a list of\n    dict(location, records) for each stop.\n\n    Parameters\n    ----------\n    records : list\n        A list of Record objects ordered by non-decreasing datetime\n    group_dist : float\n        Minimum distance (in meters) to switch to a new stop.\n    \"\"\"\n    def traverse(start, next):\n        position_prev = records[next - 1].position.location\n        position_next = records[next].position.location\n        dist = 1000 * great_circle_distance(position_prev, position_next)\n        return dist <= group_dist\n\n    groups = _groupwhile(records, traverse)\n\n    def median(x):\n        return sorted(x)[len(x) // 2]\n\n    stops = []\n    for g in groups:\n        _lat = median([gv.position.location[0] for gv in g])\n        _lon = median([gv.position.location[1] for gv in g])\n        stops.append({\n            'location': (_lat, _lon),\n            'records': g,\n        })\n\n    return stops"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncluster the records into a new antenna tree and update them with the new ones.", "response": "def cluster_and_update(records, group_dist=50, eps=100):\n    \"\"\"\n    Update records, by clustering their positions using the DBSCAN algorithm.\n    Returns a dictionnary associating new antenna identifiers to (lat, lon)\n    location tuples.\n\n    .. note:: Use this function to cluster fine-grained GPS records.\n\n    Parameters\n    ----------\n    records : list\n        A list of Record objects ordered by non-decreasing datetime\n    group_dist : float, default: 50\n        Minimum distance (in meters) to switch to a new stop.\n    eps : float, default: 100\n        The eps parameter for the DBSCAN algorithm.\n    \"\"\"\n    # Run the DBSCAN algorithm with all stop locations and 1 minimal point.\n    stops = get_stops(records, group_dist)\n    labels = dbscan([s['location'] for s in stops], eps, 1)\n\n    # Associate all records with their new antenna\n    antennas = {}\n    for i, stop in enumerate(stops):\n        antenna_id = labels[i]\n        for record in stop['records']:\n            record.position.antenna = antenna_id\n\n        antennas[antenna_id] = stop['location']\n\n    all_records = sum(len(stop['records']) for stop in stops)\n    if all_records != len(records):\n        raise ValueError(\"get_antennas has {} records instead of {} \"\n                         \"initially.\".format(all_records, len(records)))\n\n    return antennas"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fix_location(records, max_elapsed_seconds=300):\n    groups = itertools.groupby(records, lambda r: r.direction)\n    groups = [(interaction, list(g)) for interaction, g in groups]\n\n    def tdist(t1, t2):\n            return abs((t1 - t2).total_seconds())\n\n    for i, (interaction, g) in enumerate(groups):\n        if interaction == 'in':\n            continue\n\n        prev_gps = groups[i-1][1][-1]\n        next_gps = groups[i+1][1][0]\n\n        for r in g:\n            if tdist(r.datetime, prev_gps.datetime) <= max_elapsed_seconds:\n                r.position = prev_gps.position\n            elif tdist(r.datetime, next_gps.datetime) <= max_elapsed_seconds:\n                r.position = next_gps.position", "response": "Update position of all records based on the position of the closest GPS record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch(cert, issuer, hash_algo='sha1', nonce=True, user_agent=None, timeout=10):\n\n    if not isinstance(cert, x509.Certificate):\n        raise TypeError('cert must be an instance of asn1crypto.x509.Certificate, not %s' % type_name(cert))\n\n    if not isinstance(issuer, x509.Certificate):\n        raise TypeError('issuer must be an instance of asn1crypto.x509.Certificate, not %s' % type_name(issuer))\n\n    if hash_algo not in set(['sha1', 'sha256']):\n        raise ValueError('hash_algo must be one of \"sha1\", \"sha256\", not %s' % repr(hash_algo))\n\n    if not isinstance(nonce, bool):\n        raise TypeError('nonce must be a bool, not %s' % type_name(nonce))\n\n    if user_agent is None:\n        user_agent = 'certvalidator %s' % __version__\n    elif not isinstance(user_agent, str_cls):\n        raise TypeError('user_agent must be a unicode string, not %s' % type_name(user_agent))\n\n    cert_id = ocsp.CertId({\n        'hash_algorithm': algos.DigestAlgorithm({'algorithm': hash_algo}),\n        'issuer_name_hash': getattr(cert.issuer, hash_algo),\n        'issuer_key_hash': getattr(issuer.public_key, hash_algo),\n        'serial_number': cert.serial_number,\n    })\n\n    request = ocsp.Request({\n        'req_cert': cert_id,\n    })\n    tbs_request = ocsp.TBSRequest({\n        'request_list': ocsp.Requests([request]),\n    })\n\n    if nonce:\n        nonce_extension = ocsp.TBSRequestExtension({\n            'extn_id': 'nonce',\n            'critical': False,\n            'extn_value': core.OctetString(core.OctetString(os.urandom(16)).dump())\n        })\n        tbs_request['request_extensions'] = ocsp.TBSRequestExtensions([nonce_extension])\n\n    ocsp_request = ocsp.OCSPRequest({\n        'tbs_request': tbs_request,\n    })\n\n    last_e = None\n    for ocsp_url in cert.ocsp_urls:\n        try:\n            request = Request(ocsp_url)\n            request.add_header('Accept', 'application/ocsp-response')\n            request.add_header('Content-Type', 'application/ocsp-request')\n            request.add_header('User-Agent', user_agent)\n            response = urlopen(request, ocsp_request.dump(), timeout)\n            ocsp_response = ocsp.OCSPResponse.load(response.read())\n            request_nonce = ocsp_request.nonce_value\n            response_nonce = ocsp_response.nonce_value\n            if request_nonce and response_nonce and request_nonce.native != response_nonce.native:\n                raise errors.OCSPValidationError(\n                    'Unable to verify OCSP response since the request and response nonces do not match'\n                )\n            return ocsp_response\n\n        except (URLError) as e:\n            last_e = e\n\n    raise last_e", "response": "Fetch an OCSP response for a certificate."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_unarmor(self, certs, var_name):\n\n        output = []\n        for cert in certs:\n            if isinstance(cert, x509.Certificate):\n                output.append(cert)\n            else:\n                if not isinstance(cert, byte_cls):\n                    raise TypeError(pretty_message(\n                        '''\n                        %s must contain only byte strings or\n                        asn1crypto.x509.Certificate objects, not %s\n                        ''',\n                        var_name,\n                        type_name(cert)\n                    ))\n                if pem.detect(cert):\n                    _, _, cert = pem.unarmor(cert)\n                output.append(x509.Certificate.load(cert))\n        return output", "response": "Validate a list of asn1crypto. x509. Certificates objects and unarmor any PEM - encoded contents of them while unarmoring any PEM - encoded contents."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nallowing adding an other certificate that is obtained from doing revocation check via OCSP or CRL or some other method.", "response": "def add_other_cert(self, cert):\n        \"\"\"\n        Allows adding an \"other\" cert that is obtained from doing revocation\n        check via OCSP or CRL, or some other method\n\n        :param cert:\n            An asn1crypto.x509.Certificate object or a byte string of a DER or\n            PEM-encoded certificate\n\n        :return:\n            A boolean indicating if the certificate was added - will return\n            False if the certificate was already present\n        \"\"\"\n\n        if not isinstance(cert, x509.Certificate):\n            if not isinstance(cert, byte_cls):\n                raise TypeError(pretty_message(\n                    '''\n                    cert must be a byte string or an instance of\n                    asn1crypto.x509.Certificate, not %s\n                    ''',\n                    type_name(cert)\n                ))\n            if pem.detect(cert):\n                _, _, cert = pem.unarmor(cert)\n            cert = x509.Certificate.load(cert)\n\n        hashable = cert.subject.hashable\n        if hashable not in self._subject_map:\n            self._subject_map[hashable] = []\n\n        # Don't add the cert if we already have it\n        else:\n            serial_number = cert.serial_number\n            for existing_cert in self._subject_map[hashable]:\n                if existing_cert.serial_number == serial_number:\n                    return False\n\n        self._subject_map[hashable].append(cert)\n        if cert.key_identifier:\n            self._key_identifier_map[cert.key_identifier] = cert\n        else:\n            self._key_identifier_map[cert.public_key.sha1] = cert\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve_by_key_identifier(self, key_identifier):\n\n        if not isinstance(key_identifier, byte_cls):\n            raise TypeError(pretty_message(\n                '''\n                key_identifier must be a byte string, not %s\n                ''',\n                type_name(key_identifier)\n            ))\n\n        return self._key_identifier_map.get(key_identifier)", "response": "Retrieves a certificate via its key identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a list of certificates via their subject name.", "response": "def retrieve_by_name(self, name, first_certificate=None):\n        \"\"\"\n        Retrieves a list certs via their subject name\n\n        :param name:\n            An asn1crypto.x509.Name object\n\n        :param first_certificate:\n            An asn1crypto.x509.Certificate object that if found, should be\n            placed first in the result list\n\n        :return:\n            A list of asn1crypto.x509.Certificate objects\n        \"\"\"\n\n        if not isinstance(name, x509.Name):\n            raise TypeError(pretty_message(\n                '''\n                name must be an instance of asn1crypto.x509.Name, not %s\n                ''',\n                type_name(name)\n            ))\n\n        if first_certificate and not isinstance(first_certificate, x509.Certificate):\n            raise TypeError(pretty_message(\n                '''\n                first_certificate must be an instance of\n                asn1crypto.x509.Certificate, not %s\n                ''',\n                type_name(first_certificate)\n            ))\n\n        hashable = name.hashable\n\n        if hashable not in self._subject_map:\n            return []\n\n        certs = self._subject_map[hashable]\n        first = None\n        output = []\n        for cert in certs:\n            if first_certificate and first_certificate.sha256 == cert.sha256:\n                first = cert\n            else:\n                output.append(cert)\n        if first:\n            output.insert(0, first)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a list of ValidationPath objects from a certificate in the operating system trust store to the end - entity certificate.", "response": "def build_paths(self, end_entity_cert):\n        \"\"\"\n        Builds a list of ValidationPath objects from a certificate in the\n        operating system trust store to the end-entity certificate\n\n        :param end_entity_cert:\n            A byte string of a DER or PEM-encoded X.509 certificate, or an\n            instance of asn1crypto.x509.Certificate\n\n        :return:\n            A list of certvalidator.path.ValidationPath objects that represent\n            the possible paths from the end-entity certificate to one of the CA\n            certs.\n        \"\"\"\n\n        if not isinstance(end_entity_cert, byte_cls) and not isinstance(end_entity_cert, x509.Certificate):\n            raise TypeError(pretty_message(\n                '''\n                end_entity_cert must be a byte string or an instance of\n                asn1crypto.x509.Certificate, not %s\n                ''',\n                type_name(end_entity_cert)\n            ))\n\n        if isinstance(end_entity_cert, byte_cls):\n            if pem.detect(end_entity_cert):\n                _, _, end_entity_cert = pem.unarmor(end_entity_cert)\n            end_entity_cert = x509.Certificate.load(end_entity_cert)\n\n        path = ValidationPath(end_entity_cert)\n        paths = []\n        failed_paths = []\n\n        self._walk_issuers(path, paths, failed_paths)\n\n        if len(paths) == 0:\n            cert_name = end_entity_cert.subject.human_friendly\n            missing_issuer_name = failed_paths[0].first.issuer.human_friendly\n            raise PathBuildingError(pretty_message(\n                '''\n                Unable to build a validation path for the certificate \"%s\" - no\n                issuer matching \"%s\" was found\n                ''',\n                cert_name,\n                missing_issuer_name\n            ))\n\n        return paths"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _walk_issuers(self, path, paths, failed_paths):\n\n        if path.first.signature in self._ca_lookup:\n            paths.append(path)\n            return\n\n        new_branches = 0\n        for issuer in self._possible_issuers(path.first):\n            try:\n                self._walk_issuers(path.copy().prepend(issuer), paths, failed_paths)\n                new_branches += 1\n            except (DuplicateCertificateError):\n                pass\n\n        if not new_branches:\n            failed_paths.append(path)", "response": "Recursively walks through the list of known certificates for the issuer that the certificate is contained within the CA certs list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _possible_issuers(self, cert):\n\n        issuer_hashable = cert.issuer.hashable\n        if issuer_hashable not in self._subject_map:\n            return\n\n        for issuer in self._subject_map[issuer_hashable]:\n            # Info from the authority key identifier extension can be used to\n            # eliminate possible options when multiple keys with the same\n            # subject exist, such as during a transition, or with cross-signing.\n            if cert.authority_key_identifier and issuer.key_identifier:\n                if cert.authority_key_identifier != issuer.key_identifier:\n                    continue\n            elif cert.authority_issuer_serial:\n                if cert.authority_issuer_serial != issuer.issuer_serial:\n                    continue\n\n            yield issuer", "response": "Returns a generator that will list all possible issuers for the cert."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_issuer(self, cert):\n\n        for entry in self:\n            if entry.subject == cert.issuer:\n                if entry.key_identifier and cert.authority_key_identifier:\n                    if entry.key_identifier == cert.authority_key_identifier:\n                        return entry\n                else:\n                    return entry\n\n        raise LookupError('Unable to find the issuer of the certificate specified')", "response": "Return the issuer of the certificate as defined by this path"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves all certificates in the path after the specified certificate.", "response": "def truncate_to(self, cert):\n        \"\"\"\n        Remove all certificates in the path after the cert specified\n\n        :param cert:\n            An asn1crypto.x509.Certificate object to find\n\n        :raises:\n            LookupError - when the certificate could not be found\n\n        :return:\n            The current ValidationPath object, for chaining\n        \"\"\"\n\n        cert_index = None\n        for index, entry in enumerate(self):\n            if entry.issuer_serial == cert.issuer_serial:\n                cert_index = index\n                break\n\n        if cert_index is None:\n            raise LookupError('Unable to find the certificate specified')\n\n        while len(self) > cert_index + 1:\n            self.pop()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef truncate_to_issuer(self, cert):\n\n        issuer_index = None\n        for index, entry in enumerate(self):\n            if entry.subject == cert.issuer:\n                if entry.key_identifier and cert.authority_key_identifier:\n                    if entry.key_identifier == cert.authority_key_identifier:\n                        issuer_index = index\n                        break\n                else:\n                    issuer_index = index\n                    break\n\n        if issuer_index is None:\n            raise LookupError('Unable to find the issuer of the certificate specified')\n\n        while len(self) > issuer_index + 1:\n            self.pop()\n\n        return self", "response": "Removes all certificates in the path after the issuer of the certificate specified."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy(self):\n\n        copy = self.__class__()\n        copy._certs = self._certs[:]\n        copy._cert_hashes = self._cert_hashes.copy()\n        return copy", "response": "Creates a copy of this path and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pop(self):\n\n        last_cert = self._certs.pop()\n        self._cert_hashes.remove(last_cert.issuer_serial)\n\n        return self", "response": "Removes the last certificate from the path and returns the current ValidationPath object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nappending a certificate to the path.", "response": "def append(self, cert):\n        \"\"\"\n        Appends a cert to the path. This should be a cert issued by the last\n        cert in the path.\n\n        :param cert:\n            An asn1crypto.x509.Certificate object\n\n        :return:\n            The current ValidationPath object, for chaining\n        \"\"\"\n\n        if not isinstance(cert, x509.Certificate):\n            if not isinstance(cert, byte_cls):\n                raise TypeError(pretty_message(\n                    '''\n                    cert must be a byte string or an\n                    asn1crypto.x509.Certificate object, not %s\n                    ''',\n                    type_name(cert)\n                ))\n            if pem.detect(cert):\n                _, _, cert = pem.unarmor(cert)\n            cert = x509.Certificate.load(cert)\n\n        if cert.issuer_serial in self._cert_hashes:\n            raise DuplicateCertificateError()\n\n        self._cert_hashes.add(cert.issuer_serial)\n        self._certs.append(cert)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches the CRLs for a certificate and return a list of asn1crypto. crl. CertificateList objects.", "response": "def fetch(cert, use_deltas=True, user_agent=None, timeout=10):\n    \"\"\"\n    Fetches the CRLs for a certificate\n\n    :param cert:\n        An asn1cyrpto.x509.Certificate object to get the CRL for\n\n    :param use_deltas:\n        A boolean indicating if delta CRLs should be fetched\n\n    :param user_agent:\n        The HTTP user agent to use when requesting the CRL. If None,\n        a default is used in the format \"certvalidation 1.0.0\".\n\n    :param timeout:\n        The number of seconds after which an HTTP request should timeout\n\n    :raises:\n        urllib.error.URLError/urllib2.URLError - when a URL/HTTP error occurs\n        socket.error - when a socket error occurs\n\n    :return:\n        A list asn1crypto.crl.CertificateList objects\n    \"\"\"\n\n    if not isinstance(cert, x509.Certificate):\n        raise TypeError('cert must be an instance of asn1crypto.x509.Certificate, not %s' % type_name(cert))\n\n    if user_agent is None:\n        user_agent = 'certvalidator %s' % __version__\n    elif not isinstance(user_agent, str_cls):\n        raise TypeError('user_agent must be a unicode string, not %s' % type_name(user_agent))\n\n    output = []\n\n    sources = cert.crl_distribution_points\n    if use_deltas:\n        sources.extend(cert.delta_crl_distribution_points)\n\n    for distribution_point in sources:\n        url = distribution_point.url\n        output.append(_grab_crl(user_agent, url, timeout))\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches a CRL and parses it into asn1crypto. crl. CertificateList object", "response": "def _grab_crl(user_agent, url, timeout):\n    \"\"\"\n    Fetches a CRL and parses it\n\n    :param user_agent:\n        A unicode string of the user agent to use when fetching the URL\n\n    :param url:\n        A unicode string of the URL to fetch the CRL from\n\n    :param timeout:\n        The number of seconds after which an HTTP request should timeout\n\n    :return:\n        An asn1crypto.crl.CertificateList object\n    \"\"\"\n    request = Request(url)\n    request.add_header('Accept', 'application/pkix-crl')\n    request.add_header('User-Agent', user_agent)\n    response = urlopen(request, None, timeout)\n    data = response.read()\n    if pem.detect(data):\n        _, _, data = pem.unarmor(data)\n    return crl.CertificateList.load(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_certs(certificate_list, user_agent=None, timeout=10):\n\n    output = []\n\n    if user_agent is None:\n        user_agent = 'certvalidator %s' % __version__\n    elif not isinstance(user_agent, str_cls):\n        raise TypeError('user_agent must be a unicode string, not %s' % type_name(user_agent))\n\n    for url in certificate_list.issuer_cert_urls:\n        request = Request(url)\n        request.add_header('Accept', 'application/pkix-cert,application/pkcs7-mime')\n        request.add_header('User-Agent', user_agent)\n        response = urlopen(request, None, timeout)\n\n        content_type = response.headers['Content-Type'].strip()\n        response_data = response.read()\n\n        if content_type == 'application/pkix-cert':\n            output.append(x509.Certificate.load(response_data))\n\n        elif content_type == 'application/pkcs7-mime':\n            signed_data = cms.SignedData.load(response_data)\n            if isinstance(signed_data['certificates'], cms.CertificateSet):\n                for cert_choice in signed_data['certificates']:\n                    if cert_choice.name == 'certificate':\n                        output.append(cert_choice.chosen)\n        else:\n            raise ValueError('Unknown content type of %s when fetching issuer certificate for CRL' % repr(content_type))\n\n    return output", "response": "Fetches certificates from the authority information access extension of the ASN. 1. 0. 0 CRL list and places them into the certificate registry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_path(self):\n\n        if self._path is not None:\n            return\n\n        exceptions = []\n\n        if self._certificate.hash_algo in self._context.weak_hash_algos:\n            raise InvalidCertificateError(pretty_message(\n                '''\n                The X.509 certificate provided has a signature using the weak\n                hash algorithm %s\n                ''',\n                self._certificate.hash_algo\n            ))\n\n        try:\n            paths = self._context.certificate_registry.build_paths(self._certificate)\n        except (PathBuildingError):\n            if self._certificate.self_signed in set(['yes', 'maybe']):\n                raise InvalidCertificateError(pretty_message(\n                    '''\n                    The X.509 certificate provided is self-signed - \"%s\"\n                    ''',\n                    self._certificate.subject.human_friendly\n                ))\n            raise\n\n        for candidate_path in paths:\n            try:\n                validate_path(self._context, candidate_path)\n                self._path = candidate_path\n                return\n            except (ValidationError) as e:\n                exceptions.append(e)\n\n        if len(exceptions) == 1:\n            raise exceptions[0]\n\n        non_signature_exception = None\n        for exception in exceptions:\n            if 'signature' not in str_cls(exception):\n                non_signature_exception = exception\n\n        if non_signature_exception:\n            raise non_signature_exception\n\n        raise exceptions[0]", "response": "Validates the path of the certificate."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates the certificate path and that the certificate is valid for the specified key usage and extended key usage purposes.", "response": "def validate_usage(self, key_usage, extended_key_usage=None, extended_optional=False):\n        \"\"\"\n        Validates the certificate path and that the certificate is valid for\n        the key usage and extended key usage purposes specified.\n\n        :param key_usage:\n            A set of unicode strings of the required key usage purposes. Valid\n            values include:\n\n             - \"digital_signature\"\n             - \"non_repudiation\"\n             - \"key_encipherment\"\n             - \"data_encipherment\"\n             - \"key_agreement\"\n             - \"key_cert_sign\"\n             - \"crl_sign\"\n             - \"encipher_only\"\n             - \"decipher_only\"\n\n        :param extended_key_usage:\n            A set of unicode strings of the required extended key usage\n            purposes. These must be either dotted number OIDs, or one of the\n            following extended key usage purposes:\n\n             - \"server_auth\"\n             - \"client_auth\"\n             - \"code_signing\"\n             - \"email_protection\"\n             - \"ipsec_end_system\"\n             - \"ipsec_tunnel\"\n             - \"ipsec_user\"\n             - \"time_stamping\"\n             - \"ocsp_signing\"\n             - \"wireless_access_points\"\n\n            An example of a dotted number OID:\n\n             - \"1.3.6.1.5.5.7.3.1\"\n\n        :param extended_optional:\n            A bool - if the extended_key_usage extension may be ommited and still\n            considered valid\n\n        :raises:\n            certvalidator.errors.PathValidationError - when an error occurs validating the path\n            certvalidator.errors.RevokedError - when the certificate or another certificate in its path has been revoked\n            certvalidator.errors.InvalidCertificateError - when the certificate is not valid for the usages specified\n\n        :return:\n            A certvalidator.path.ValidationPath object of the validated\n            certificate validation path\n        \"\"\"\n\n        self._validate_path()\n        validate_usage(\n            self._context,\n            self._certificate,\n            key_usage,\n            extended_key_usage,\n            extended_optional\n        )\n        return self._path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the certificate path and certificate for a TLS connection.", "response": "def validate_tls(self, hostname):\n        \"\"\"\n        Validates the certificate path, that the certificate is valid for\n        the hostname provided and that the certificate is valid for the purpose\n        of a TLS connection.\n\n        :param hostname:\n            A unicode string of the TLS server hostname\n\n        :raises:\n            certvalidator.errors.PathValidationError - when an error occurs validating the path\n            certvalidator.errors.RevokedError - when the certificate or another certificate in its path has been revoked\n            certvalidator.errors.InvalidCertificateError - when the certificate is not valid for TLS or the hostname\n\n        :return:\n            A certvalidator.path.ValidationPath object of the validated\n            certificate validation path\n        \"\"\"\n\n        self._validate_path()\n        validate_tls_hostname(self._context, self._certificate, hostname)\n        return self._path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ocsps(self):\n\n        if not self._allow_fetching:\n            return self._ocsps\n\n        output = []\n        for issuer_serial in self._fetched_ocsps:\n            output.extend(self._fetched_ocsps[issuer_serial])\n        return output", "response": "A list of all cached OCSP responses."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retrieve_crls(self, cert):\n\n        if not self._allow_fetching:\n            return self._crls\n\n        if cert.issuer_serial not in self._fetched_crls:\n            try:\n                crls = crl_client.fetch(\n                    cert,\n                    **self._crl_fetch_params\n                )\n                self._fetched_crls[cert.issuer_serial] = crls\n                for crl_ in crls:\n                    try:\n                        certs = crl_client.fetch_certs(\n                            crl_,\n                            user_agent=self._crl_fetch_params.get('user_agent'),\n                            timeout=self._crl_fetch_params.get('timeout')\n                        )\n                        for cert_ in certs:\n                            if self.certificate_registry.add_other_cert(cert_):\n                                self._revocation_certs[cert_.issuer_serial] = cert_\n                    except (URLError, socket.error):\n                        pass\n            except (URLError, socket.error) as e:\n                self._fetched_crls[cert.issuer_serial] = []\n                if self._revocation_mode == \"soft-fail\":\n                    self._soft_fail_exceptions.append(e)\n                    raise SoftFailError()\n                else:\n                    raise\n\n        return self._fetched_crls[cert.issuer_serial]", "response": "Retrieves the CRLs for a certificate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the OCSP response for a certificate and returns the list of OCSPResponse objects.", "response": "def retrieve_ocsps(self, cert, issuer):\n        \"\"\"\n        :param cert:\n            An asn1crypto.x509.Certificate object\n\n        :param issuer:\n            An asn1crypto.x509.Certificate object of cert's issuer\n\n        :return:\n            A list of asn1crypto.ocsp.OCSPResponse objects\n        \"\"\"\n\n        if not self._allow_fetching:\n            return self._ocsps\n\n        if cert.issuer_serial not in self._fetched_ocsps:\n            try:\n                ocsp_response = ocsp_client.fetch(\n                    cert,\n                    issuer,\n                    **self._ocsp_fetch_params\n                )\n\n                self._fetched_ocsps[cert.issuer_serial] = [ocsp_response]\n\n                # Responses can contain certificates that are useful in validating the\n                # response itself. We can use these since they will be validated using\n                # the local trust roots.\n                self._extract_ocsp_certs(ocsp_response)\n            except (URLError, socket.error) as e:\n                self._fetched_ocsps[cert.issuer_serial] = []\n                if self._revocation_mode == \"soft-fail\":\n                    self._soft_fail_exceptions.append(e)\n                    raise SoftFailError()\n                else:\n                    raise\n\n        return self._fetched_ocsps[cert.issuer_serial]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts any certificates included with an OCSP response and adds them to the certificate registry.", "response": "def _extract_ocsp_certs(self, ocsp_response):\n        \"\"\"\n        Extracts any certificates included with an OCSP response and adds them\n        to the certificate registry\n\n        :param ocsp_response:\n            An asn1crypto.ocsp.OCSPResponse object to look for certs inside of\n        \"\"\"\n\n        status = ocsp_response['response_status'].native\n        if status == 'successful':\n            response_bytes = ocsp_response['response_bytes']\n            if response_bytes['response_type'].native == 'basic_ocsp_response':\n                response = response_bytes['response'].parsed\n                if response['certs']:\n                    for other_cert in response['certs']:\n                        if self.certificate_registry.add_other_cert(other_cert):\n                            self._revocation_certs[other_cert.issuer_serial] = other_cert"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_validation(self, cert):\n\n        # CA certs are automatically trusted since they are from the trust list\n        if self.certificate_registry.is_ca(cert) and cert.signature not in self._validate_map:\n            self._validate_map[cert.signature] = ValidationPath(cert)\n\n        return self._validate_map.get(cert.signature)", "response": "Checks to see if a certificate has been validated and returns the ValidationPath used to validate it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear the record that a certificate has been validated.", "response": "def clear_validation(self, cert):\n        \"\"\"\n        Clears the record that a certificate has been validated\n\n        :param cert:\n            An ans1crypto.x509.Certificate object\n        \"\"\"\n\n        if cert.signature in self._validate_map:\n            del self._validate_map[cert.signature]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the TLS server hostname.", "response": "def validate_tls_hostname(validation_context, cert, hostname):\n    \"\"\"\n    Validates the end-entity certificate from a\n    certvalidator.path.ValidationPath object to ensure that the certificate\n    is valid for the hostname provided and that the certificate is valid for\n    the purpose of a TLS connection.\n\n    THE CERTIFICATE PATH MUST BE VALIDATED SEPARATELY VIA validate_path()!\n\n    :param validation_context:\n        A certvalidator.context.ValidationContext object to use for\n        configuring validation behavior\n\n    :param cert:\n        An asn1crypto.x509.Certificate object returned from validate_path()\n\n    :param hostname:\n        A unicode string of the TLS server hostname\n\n    :raises:\n        certvalidator.errors.InvalidCertificateError - when the certificate is not valid for TLS or the hostname\n    \"\"\"\n\n    if not isinstance(validation_context, ValidationContext):\n        raise TypeError(pretty_message(\n            '''\n            validation_context must be an instance of\n            certvalidator.context.ValidationContext, not %s\n            ''',\n            type_name(validation_context)\n        ))\n\n    if validation_context.is_whitelisted(cert):\n        return\n\n    if not cert.is_valid_domain_ip(hostname):\n        raise InvalidCertificateError(pretty_message(\n            '''\n            The X.509 certificate provided is not valid for %s. Valid hostnames\n            include: %s\n            ''',\n            hostname,\n            ', '.join(cert.valid_domains)\n        ))\n\n    bad_key_usage = cert.key_usage_value and 'digital_signature' not in cert.key_usage_value.native\n    bad_ext_key_usage = cert.extended_key_usage_value and 'server_auth' not in cert.extended_key_usage_value.native\n\n    if bad_key_usage or bad_ext_key_usage:\n        raise InvalidCertificateError(pretty_message(\n            '''\n            The X.509 certificate provided is not valid for securing TLS\n            connections\n            '''\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates the end - entity certificate for the specified key usage and extended key usage purposes.", "response": "def validate_usage(validation_context, cert, key_usage, extended_key_usage, extended_optional):\n    \"\"\"\n    Validates the end-entity certificate from a\n    certvalidator.path.ValidationPath object to ensure that the certificate\n    is valid for the key usage and extended key usage purposes specified.\n\n    THE CERTIFICATE PATH MUST BE VALIDATED SEPARATELY VIA validate_path()!\n\n    :param validation_context:\n        A certvalidator.context.ValidationContext object to use for\n        configuring validation behavior\n\n    :param cert:\n        An asn1crypto.x509.Certificate object returned from validate_path()\n\n    :param key_usage:\n        A set of unicode strings of the required key usage purposes\n\n    :param extended_key_usage:\n        A set of unicode strings of the required extended key usage purposes\n\n    :param extended_optional:\n        A bool - if the extended_key_usage extension may be omitted and still\n        considered valid\n\n    :raises:\n        certvalidator.errors.InvalidCertificateError - when the certificate is not valid for the usages specified\n    \"\"\"\n\n    if not isinstance(validation_context, ValidationContext):\n        raise TypeError(pretty_message(\n            '''\n            validation_context must be an instance of\n            certvalidator.context.ValidationContext, not %s\n            ''',\n            type_name(validation_context)\n        ))\n\n    if validation_context.is_whitelisted(cert):\n        return\n\n    if key_usage is None:\n        key_usage = set()\n\n    if extended_key_usage is None:\n        extended_key_usage = set()\n\n    if not isinstance(key_usage, set):\n        raise TypeError(pretty_message(\n            '''\n            key_usage must be a set of unicode strings, not %s\n            ''',\n            type_name(key_usage)\n        ))\n\n    if not isinstance(extended_key_usage, set):\n        raise TypeError(pretty_message(\n            '''\n            extended_key_usage must be a set of unicode strings, not %s\n            ''',\n            type_name(extended_key_usage)\n        ))\n\n    if not isinstance(extended_optional, bool):\n        raise TypeError(pretty_message(\n            '''\n            extended_optional must be a boolean, not %s\n            ''',\n            type_name(extended_optional)\n        ))\n\n    missing_key_usage = key_usage\n    if cert.key_usage_value:\n        missing_key_usage = key_usage - cert.key_usage_value.native\n\n    missing_extended_key_usage = set()\n    if extended_optional is False and not cert.extended_key_usage_value:\n        missing_extended_key_usage = extended_key_usage\n    elif cert.extended_key_usage_value is not None:\n        missing_extended_key_usage = extended_key_usage - set(cert.extended_key_usage_value.native)\n\n    if missing_key_usage or missing_extended_key_usage:\n        plural = 's' if len(missing_key_usage | missing_extended_key_usage) > 1 else ''\n        friendly_purposes = []\n        for purpose in sorted(missing_key_usage | missing_extended_key_usage):\n            friendly_purposes.append(purpose.replace('_', ' '))\n        raise InvalidCertificateError(pretty_message(\n            '''\n            The X.509 certificate provided is not valid for the purpose%s of %s\n            ''',\n            plural,\n            ', '.join(friendly_purposes)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_path(validation_context, path, end_entity_name_override=None):\n\n    if not isinstance(path, ValidationPath):\n        raise TypeError(pretty_message(\n            '''\n            path must be an instance of certvalidator.path.ValidationPath,\n            not %s\n            ''',\n            type_name(path)\n        ))\n\n    if not isinstance(validation_context, ValidationContext):\n        raise TypeError(pretty_message(\n            '''\n            validation_context must be an instance of\n            certvalidator.context.ValidationContext, not %s\n            ''',\n            type_name(validation_context)\n        ))\n\n    moment = validation_context.moment\n\n    if end_entity_name_override is not None and not isinstance(end_entity_name_override, str_cls):\n        raise TypeError(pretty_message(\n            '''\n            end_entity_name_override must be a unicode string, not %s\n            ''',\n            type_name(end_entity_name_override)\n        ))\n\n    # Inputs\n\n    trust_anchor = path.first\n\n    # We skip the trust anchor when measuring the path since technically\n    # the trust anchor is not part of the path\n    path_length = len(path) - 1\n\n    # We don't accept any certificate policy or name constraint values as input\n    # and instead just start allowing everything during initialization\n\n    # Step 1: initialization\n\n    # Step 1 a\n    valid_policy_tree = PolicyTreeRoot('any_policy', set(), set(['any_policy']))\n\n    # Steps 1 b-c skipped since they relate to name constraints\n\n    # Steps 1 d-f\n    # We do not use initial-explicit-policy, initial-any-policy-inhibit or\n    # initial-policy-mapping-inhibit, so they are all set to the path length + 1\n    explicit_policy = path_length + 1\n    inhibit_any_policy = path_length + 1\n    policy_mapping = path_length + 1\n\n    # Steps 1 g-i\n    working_public_key = trust_anchor.public_key\n    # Step 1 j\n    working_issuer_name = trust_anchor.subject\n    # Step 1 k\n    max_path_length = path_length\n    if trust_anchor.max_path_length is not None:\n        max_path_length = trust_anchor.max_path_length\n\n    # Step 2: basic processing\n    index = 1\n    last_index = len(path) - 1\n\n    completed_path = ValidationPath(trust_anchor)\n    validation_context.record_validation(trust_anchor, completed_path)\n\n    cert = trust_anchor\n    while index <= last_index:\n        cert = path[index]\n\n        # Step 2 a 1\n        signature_algo = cert['signature_algorithm'].signature_algo\n        hash_algo = cert['signature_algorithm'].hash_algo\n\n        if hash_algo in validation_context.weak_hash_algos:\n            raise PathValidationError(pretty_message(\n                '''\n                The path could not be validated because the signature of %s\n                uses the weak hash algorithm %s\n                ''',\n                _cert_type(index, last_index, end_entity_name_override, definite=True),\n                hash_algo\n            ))\n\n        if signature_algo == 'rsassa_pkcs1v15':\n            verify_func = asymmetric.rsa_pkcs1v15_verify\n        elif signature_algo == 'dsa':\n            verify_func = asymmetric.dsa_verify\n        elif signature_algo == 'ecdsa':\n            verify_func = asymmetric.ecdsa_verify\n        else:\n            raise PathValidationError(pretty_message(\n                '''\n                The path could not be validated because the signature of %s\n                uses the unsupported algorithm %s\n                ''',\n                _cert_type(index, last_index, end_entity_name_override, definite=True),\n                signature_algo\n            ))\n\n        try:\n            key_object = asymmetric.load_public_key(working_public_key)\n            verify_func(key_object, cert['signature_value'].native, cert['tbs_certificate'].dump(), hash_algo)\n\n        except (oscrypto.errors.SignatureError):\n            raise PathValidationError(pretty_message(\n                '''\n                The path could not be validated because the signature of %s\n                could not be verified\n                ''',\n                _cert_type(index, last_index, end_entity_name_override, definite=True)\n            ))\n\n        # Step 2 a 2\n        if not validation_context.is_whitelisted(cert):\n            validity = cert['tbs_certificate']['validity']\n            if moment < validity['not_before'].native:\n                raise PathValidationError(pretty_message(\n                    '''\n                    The path could not be validated because %s is not valid\n                    until %s\n                    ''',\n                    _cert_type(index, last_index, end_entity_name_override, definite=True),\n                    validity['not_before'].native.strftime('%Y-%m-%d %H:%M:%SZ')\n                ))\n            if moment > validity['not_after'].native:\n                raise PathValidationError(pretty_message(\n                    '''\n                    The path could not be validated because %s expired %s\n                    ''',\n                    _cert_type(index, last_index, end_entity_name_override, definite=True),\n                    validity['not_after'].native.strftime('%Y-%m-%d %H:%M:%SZ')\n                ))\n\n        # Step 2 a 3 - CRL/OCSP\n        if not validation_context._skip_revocation_checks:\n            status_good = False\n            revocation_check_failed = False\n            matched = False\n            soft_fail = False\n            failures = []\n\n            if cert.ocsp_urls or validation_context.revocation_mode == 'require':\n                try:\n                    verify_ocsp_response(\n                        cert,\n                        path,\n                        validation_context,\n                        cert_description=_cert_type(\n                            index,\n                            last_index,\n                            end_entity_name_override,\n                            definite=True\n                        ),\n                        end_entity_name_override=end_entity_name_override\n                    )\n                    status_good = True\n                    matched = True\n                except (OCSPValidationIndeterminateError) as e:\n                    failures.extend([failure[0] for failure in e.failures])\n                    revocation_check_failed = True\n                    matched = True\n                except (SoftFailError):\n                    soft_fail = True\n                except (OCSPNoMatchesError):\n                    pass\n\n            if not status_good and (cert.crl_distribution_points or validation_context.revocation_mode == 'require'):\n                try:\n                    cert_description = _cert_type(index, last_index, end_entity_name_override, definite=True)\n                    verify_crl(\n                        cert,\n                        path,\n                        validation_context,\n                        cert_description=cert_description,\n                        end_entity_name_override=end_entity_name_override\n                    )\n                    revocation_check_failed = False\n                    status_good = True\n                    matched = True\n                except (CRLValidationIndeterminateError) as e:\n                    failures.extend([failure[0] for failure in e.failures])\n                    revocation_check_failed = True\n                    matched = True\n                except (SoftFailError):\n                    soft_fail = True\n                except (CRLNoMatchesError):\n                    pass\n\n            if not soft_fail:\n                if not matched and validation_context.revocation_mode == 'require':\n                    raise PathValidationError(pretty_message(\n                        '''\n                        The path could not be validated because no revocation\n                        information could be found for %s\n                        ''',\n                        _cert_type(index, last_index, end_entity_name_override, definite=True)\n                    ))\n\n                if not status_good and revocation_check_failed:\n                    raise PathValidationError(pretty_message(\n                        '''\n                        The path could not be validated because the %s revocation\n                        checks failed: %s\n                        ''',\n                        _cert_type(index, last_index, end_entity_name_override),\n                        '; '.join(failures)\n                    ))\n\n        # Step 2 a 4\n        if cert.issuer != working_issuer_name:\n            raise PathValidationError(pretty_message(\n                '''\n                The path could not be validated because the %s issuer name\n                could not be matched\n                ''',\n                _cert_type(index, last_index, end_entity_name_override),\n            ))\n\n        # Steps 2 b-c skipped since they relate to name constraints\n\n        # Steps 2 d\n        if cert.certificate_policies_value and valid_policy_tree is not None:\n\n            cert_any_policy = None\n            cert_policy_identifiers = set()\n\n            # Step 2 d 1\n            for policy in cert.certificate_policies_value:\n                policy_identifier = policy['policy_identifier'].native\n\n                if policy_identifier == 'any_policy':\n                    cert_any_policy = policy\n                    continue\n\n                cert_policy_identifiers.add(policy_identifier)\n\n                policy_qualifiers = policy['policy_qualifiers']\n\n                policy_id_match = False\n                parent_any_policy = None\n\n                # Step 2 d 1 i\n                for node in valid_policy_tree.at_depth(index - 1):\n                    if node.valid_policy == 'any_policy':\n                        parent_any_policy = node\n                    if policy_identifier not in node.expected_policy_set:\n                        continue\n                    policy_id_match = True\n                    node.add_child(\n                        policy_identifier,\n                        policy_qualifiers,\n                        set([policy_identifier])\n                    )\n\n                # Step 2 d 1 ii\n                if not policy_id_match and parent_any_policy:\n                    parent_any_policy.add_child(\n                        policy_identifier,\n                        policy_qualifiers,\n                        set([policy_identifier])\n                    )\n\n            # Step 2 d 2\n            if cert_any_policy and (inhibit_any_policy > 0 or (index < path_length and cert.self_issued)):\n                for node in valid_policy_tree.at_depth(index - 1):\n                    for expected_policy_identifier in node.expected_policy_set:\n                        if expected_policy_identifier not in cert_policy_identifiers:\n                            node.add_child(\n                                expected_policy_identifier,\n                                cert_any_policy['policy_qualifiers'],\n                                set([expected_policy_identifier])\n                            )\n\n            # Step 2 d 3\n            for node in valid_policy_tree.walk_up(index - 1):\n                if not node.children:\n                    node.parent.remove_child(node)\n            if len(valid_policy_tree.children) == 0:\n                valid_policy_tree = None\n\n        # Step 2 e\n        if cert.certificate_policies_value is None:\n            valid_policy_tree = None\n\n        # Step 2 f\n        if valid_policy_tree is None and explicit_policy <= 0:\n            raise PathValidationError(pretty_message(\n                '''\n                The path could not be validated because there is no valid set\n                of policies for %s\n                ''',\n                _cert_type(index, last_index, end_entity_name_override, definite=True),\n            ))\n\n        if index != last_index:\n            # Step 3: prepare for certificate index+1\n\n            if cert.policy_mappings_value:\n                policy_map = {}\n                for mapping in cert.policy_mappings_value:\n                    issuer_domain_policy = mapping['issuer_domain_policy'].native\n                    subject_domain_policy = mapping['subject_domain_policy'].native\n\n                    if issuer_domain_policy not in policy_map:\n                        policy_map[issuer_domain_policy] = set()\n                    policy_map[issuer_domain_policy].add(subject_domain_policy)\n\n                    # Step 3 a\n                    if issuer_domain_policy == 'any_policy' or subject_domain_policy == 'any_policy':\n                        raise PathValidationError(pretty_message(\n                            '''\n                            The path could not be validated because %s contains\n                            a policy mapping for the \"any policy\"\n                            ''',\n                            _cert_type(index, last_index, end_entity_name_override, definite=True)\n                        ))\n\n                # Step 3 b\n                if valid_policy_tree is not None:\n                    for mapping in cert.policy_mappings_value:\n                        issuer_domain_policy = mapping['issuer_domain_policy'].native\n\n                        # Step 3 b 1\n                        if policy_mapping > 0:\n                            issuer_domain_policy_match = False\n                            cert_any_policy = None\n\n                            for node in valid_policy_tree.at_depth(index):\n                                if node.valid_policy == 'any_policy':\n                                    cert_any_policy = node\n                                if node.valid_policy == issuer_domain_policy:\n                                    issuer_domain_policy_match = True\n                                    node.expected_policy_set = policy_map[issuer_domain_policy]\n\n                            if not issuer_domain_policy_match and cert_any_policy:\n                                cert_any_policy.parent.add_child(\n                                    issuer_domain_policy,\n                                    cert_any_policy.qualifier_set,\n                                    policy_map[issuer_domain_policy]\n                                )\n\n                        # Step 3 b 2\n                        elif policy_mapping == 0:\n                            for node in valid_policy_tree.at_depth(index):\n                                if node.valid_policy == issuer_domain_policy:\n                                    node.parent.remove_child(node)\n                            for node in valid_policy_tree.walk_up(index - 1):\n                                if not node.children:\n                                    node.parent.remove_child(node)\n                            if len(valid_policy_tree.children) == 0:\n                                valid_policy_tree = None\n\n            # Step 3 c\n            working_issuer_name = cert.subject\n\n            # Steps 3 d-f\n\n            # Handle inheritance of DSA parameters from a signing CA to the\n            # next in the chain\n            copy_params = None\n            if cert.public_key.algorithm == 'dsa' and cert.public_key.hash_algo is None:\n                if working_public_key.algorithm == 'dsa':\n                    copy_params = working_public_key['algorithm']['parameters'].copy()\n\n            working_public_key = cert.public_key\n\n            if copy_params:\n                working_public_key['algorithm']['parameters'] = copy_params\n\n            # Step 3 g skipped since it relates to name constraints\n\n            # Step 3 h\n            if not cert.self_issued:\n                # Step 3 h 1\n                if explicit_policy != 0:\n                    explicit_policy -= 1\n                # Step 3 h 2\n                if policy_mapping != 0:\n                    policy_mapping -= 1\n                # Step 3 h 3\n                if inhibit_any_policy != 0:\n                    inhibit_any_policy -= 1\n\n            # Step 3 i\n            if cert.policy_constraints_value:\n                # Step 3 i 1\n                require_explicit_policy = cert.policy_constraints_value['require_explicit_policy'].native\n                if require_explicit_policy is not None and require_explicit_policy < explicit_policy:\n                    explicit_policy = require_explicit_policy\n                # Step 3 i 2\n                inhibit_policy_mapping = cert.policy_constraints_value['inhibit_policy_mapping'].native\n                if inhibit_policy_mapping is not None and inhibit_policy_mapping < policy_mapping:\n                    policy_mapping = inhibit_policy_mapping\n\n            # Step 3 j\n            if cert.inhibit_any_policy_value:\n                inhibit_any_policy = min(cert.inhibit_any_policy_value.native, inhibit_any_policy)\n\n            # Step 3 k\n            if not cert.ca:\n                raise PathValidationError(pretty_message(\n                    '''\n                    The path could not be validated because %s is not a CA\n                    ''',\n                    _cert_type(index, last_index, end_entity_name_override, definite=True)\n                ))\n\n            # Step 3 l\n            if not cert.self_issued:\n                if max_path_length == 0:\n                    raise PathValidationError(pretty_message(\n                        '''\n                        The path could not be validated because it exceeds the\n                        maximum path length\n                        '''\n                    ))\n                max_path_length -= 1\n\n            # Step 3 m\n            if cert.max_path_length is not None and cert.max_path_length < max_path_length:\n                max_path_length = cert.max_path_length\n\n            # Step 3 n\n            if cert.key_usage_value and 'key_cert_sign' not in cert.key_usage_value.native:\n                raise PathValidationError(pretty_message(\n                    '''\n                    The path could not be validated because %s is not allowed\n                    to sign certificates\n                    ''',\n                    _cert_type(index, last_index, end_entity_name_override, definite=True)\n                ))\n\n        # Step 3 o\n        # Check for critical unsupported extensions\n        supported_extensions = set([\n            'authority_information_access',\n            'authority_key_identifier',\n            'basic_constraints',\n            'crl_distribution_points',\n            'extended_key_usage',\n            'freshest_crl',\n            'key_identifier',\n            'key_usage',\n            'ocsp_no_check',\n            'certificate_policies',\n            'policy_mappings',\n            'policy_constraints',\n            'inhibit_any_policy',\n        ])\n        unsupported_critical_extensions = cert.critical_extensions - supported_extensions\n        if unsupported_critical_extensions:\n            raise PathValidationError(pretty_message(\n                '''\n                The path could not be validated because %s contains the\n                following unsupported critical extension%s: %s\n                ''',\n                _cert_type(index, last_index, end_entity_name_override, definite=True),\n                's' if len(unsupported_critical_extensions) != 1 else '',\n                ', '.join(sorted(unsupported_critical_extensions)),\n            ))\n\n        if validation_context:\n            completed_path = completed_path.copy().append(cert)\n            validation_context.record_validation(cert, completed_path)\n\n        index += 1\n\n    # Step 4: wrap-up procedure\n\n    # Step 4 a\n    if explicit_policy != 0:\n        explicit_policy -= 1\n\n    # Step 4 b\n    if cert.policy_constraints_value:\n        if cert.policy_constraints_value['require_explicit_policy'].native == 0:\n            explicit_policy = 0\n\n    # Steps 4 c-e skipped since this method doesn't output it\n    # Step 4 f skipped since this method defers that to the calling application\n\n    # Step 4 g\n\n    # Step 4 g i\n    if valid_policy_tree is None:\n        intersection = None\n\n    # Step 4 g ii\n    else:\n        intersection = valid_policy_tree\n\n    # Step 4 g iii is skipped since the initial policy set is always any_policy\n\n    if explicit_policy == 0 and intersection is None:\n        raise PathValidationError(pretty_message(\n            '''\n            The path could not be validated because there is no valid set of\n            policies for %s\n            ''',\n            _cert_type(last_index, last_index, end_entity_name_override, definite=True)\n        ))\n\n    return cert", "response": "Validate a path and return a new base - ASN. 1 certificate."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the type of the certificate in the chain.", "response": "def _cert_type(index, last_index, end_entity_name_override, definite=False):\n    \"\"\"\n    :param index:\n        An integer of the index of the certificate in the path\n\n    :param last_index:\n        An integer of the last index in the path\n\n    :param end_entity_name_override:\n        None or a unicode string of the name to use for the final certificate\n        in the path. Used for indirect CRL issuer and OCSP responder\n        certificates.\n\n    :param definite:\n        When returning the string \"end-entity certificate\", passing this flag\n        as True will prepend \"the \" to the result\n\n    :return:\n        A unicode string describing the position of a certificate in the chain\n    \"\"\"\n\n    if index != last_index:\n        return 'intermediate certificate %s' % index\n\n    prefix = 'the ' if definite else ''\n\n    if end_entity_name_override is not None:\n        return prefix + end_entity_name_override\n\n    return prefix + 'end-entity certificate'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _self_signed(cert):\n\n    self_signed = cert.self_signed\n\n    if self_signed == 'yes':\n        return True\n    if self_signed == 'no':\n        return False\n\n    # In the case of \"maybe\", we have to check the signature\n    signature_algo = cert['signature_algorithm'].signature_algo\n    hash_algo = cert['signature_algorithm'].hash_algo\n\n    if signature_algo == 'rsassa_pkcs1v15':\n        verify_func = asymmetric.rsa_pkcs1v15_verify\n    elif signature_algo == 'dsa':\n        verify_func = asymmetric.dsa_verify\n    elif signature_algo == 'ecdsa':\n        verify_func = asymmetric.ecdsa_verify\n    else:\n        raise PathValidationError(pretty_message(\n            '''\n            Unable to verify the signature of the certificate since it uses\n            the unsupported algorithm %s\n            ''',\n            signature_algo\n        ))\n\n    try:\n        key_object = asymmetric.load_certificate(cert)\n        verify_func(key_object, cert['signature_value'].native, cert['tbs_certificate'].dump(), hash_algo)\n        return True\n\n    except (oscrypto.errors.SignatureError):\n        return False", "response": "Determines if a certificate is self - signed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_ocsp_response(cert, path, validation_context, cert_description=None, end_entity_name_override=None):\n\n    if not isinstance(cert, x509.Certificate):\n        raise TypeError(pretty_message(\n            '''\n            cert must be an instance of asn1crypto.x509.Certificate, not %s\n            ''',\n            type_name(cert)\n        ))\n\n    if not isinstance(path, ValidationPath):\n        raise TypeError(pretty_message(\n            '''\n            path must be an instance of certvalidator.path.ValidationPath,\n            not %s\n            ''',\n            type_name(path)\n        ))\n\n    if not isinstance(validation_context, ValidationContext):\n        raise TypeError(pretty_message(\n            '''\n            validation_context must be an instance of\n            certvalidator.context.ValidationContext, not %s\n            ''',\n            type_name(validation_context)\n        ))\n\n    if cert_description is None:\n        cert_description = 'the certificate'\n\n    if not isinstance(cert_description, str_cls):\n        raise TypeError(pretty_message(\n            '''\n            cert_description must be a unicode string, not %s\n            ''',\n            type_name(cert_description)\n        ))\n\n    moment = validation_context.moment\n\n    issuer = path.find_issuer(cert)\n    certificate_registry = validation_context.certificate_registry\n\n    failures = []\n    mismatch_failures = 0\n\n    ocsp_responses = validation_context.retrieve_ocsps(cert, issuer)\n\n    for ocsp_response in ocsp_responses:\n\n        # Make sure that we get a valid response back from the OCSP responder\n        status = ocsp_response['response_status'].native\n        if status != 'successful':\n            mismatch_failures += 1\n            continue\n\n        response_bytes = ocsp_response['response_bytes']\n        if response_bytes['response_type'].native != 'basic_ocsp_response':\n            mismatch_failures += 1\n            continue\n\n        response = response_bytes['response'].parsed\n        tbs_response = response['tbs_response_data']\n\n        # With a valid response, now a check is performed to see if the response is\n        # applicable for the cert and moment requested\n        cert_response = tbs_response['responses'][0]\n\n        response_cert_id = cert_response['cert_id']\n\n        issuer_hash_algo = response_cert_id['hash_algorithm']['algorithm'].native\n        cert_issuer_name_hash = getattr(cert.issuer, issuer_hash_algo)\n        cert_issuer_key_hash = getattr(issuer.public_key, issuer_hash_algo)\n\n        key_hash_mismatch = response_cert_id['issuer_key_hash'].native != cert_issuer_key_hash\n\n        name_mismatch = response_cert_id['issuer_name_hash'].native != cert_issuer_name_hash\n        serial_mismatch = response_cert_id['serial_number'].native != cert.serial_number\n\n        if (name_mismatch or serial_mismatch) and key_hash_mismatch:\n            mismatch_failures += 1\n            continue\n\n        if name_mismatch:\n            failures.append((\n                'OCSP response issuer name hash does not match',\n                ocsp_response\n            ))\n            continue\n\n        if serial_mismatch:\n            failures.append((\n                'OCSP response certificate serial number does not match',\n                ocsp_response\n            ))\n            continue\n\n        if key_hash_mismatch:\n            failures.append((\n                'OCSP response issuer key hash does not match',\n                ocsp_response\n            ))\n            continue\n\n        if moment < cert_response['this_update'].native:\n            failures.append((\n                'OCSP response is from after the validation time',\n                ocsp_response\n            ))\n            continue\n\n        if moment > cert_response['next_update'].native:\n            failures.append((\n                'OCSP response is from before the validation time',\n                ocsp_response\n            ))\n            continue\n\n        # To verify the response as legitimate, the responder cert must be located\n        if tbs_response['responder_id'].name == 'by_key':\n            key_identifier = tbs_response['responder_id'].native\n            signing_cert = certificate_registry.retrieve_by_key_identifier(key_identifier)\n        else:\n            candidate_signing_certs = certificate_registry.retrieve_by_name(\n                tbs_response['responder_id'].chosen,\n                None\n            )\n            signing_cert = candidate_signing_certs[0] if candidate_signing_certs else None\n        if not signing_cert:\n            failures.append((\n                pretty_message(\n                    '''\n                    Unable to verify OCSP response since response signing\n                    certificate could not be located\n                    '''\n                ),\n                ocsp_response\n            ))\n            continue\n\n        # The responder cert has to have a valid path back to one of the trust roots\n        if not certificate_registry.is_ca(signing_cert):\n            signing_cert_paths = certificate_registry.build_paths(signing_cert)\n            for signing_cert_path in signing_cert_paths:\n                try:\n                    # Store the original revocation check value\n                    changed_revocation_flags = False\n                    skip_ocsp = signing_cert.ocsp_no_check_value is not None\n                    skip_ocsp = skip_ocsp or signing_cert_path == path\n                    if skip_ocsp and validation_context._skip_revocation_checks is False:\n                        changed_revocation_flags = True\n\n                        original_revocation_mode = validation_context.revocation_mode\n                        new_revocation_mode = \"soft-fail\" if original_revocation_mode == \"soft-fail\" else \"hard-fail\"\n\n                        validation_context._skip_revocation_checks = True\n                        validation_context._revocation_mode = new_revocation_mode\n\n                    if end_entity_name_override is None and signing_cert.sha256 != issuer.sha256:\n                        end_entity_name_override = cert_description + ' OCSP responder'\n                    _validate_path(\n                        validation_context,\n                        signing_cert_path,\n                        end_entity_name_override=end_entity_name_override\n                    )\n                    signing_cert_issuer = signing_cert_path.find_issuer(signing_cert)\n                    break\n\n                except (PathValidationError):\n                    continue\n\n                finally:\n                    if changed_revocation_flags:\n                        validation_context._skip_revocation_checks = False\n                        validation_context._revocation_mode = original_revocation_mode\n\n            else:\n                failures.append((\n                    pretty_message(\n                        '''\n                        Unable to verify OCSP response since response signing\n                        certificate could not be validated\n                        '''\n                    ),\n                    ocsp_response\n                ))\n                continue\n\n        # If the cert signing the OCSP response is not the issuer, it must be issued\n        # by the cert issuer and be valid for OCSP responses\n        if issuer.issuer_serial != signing_cert.issuer_serial:\n            if signing_cert_issuer.issuer_serial != issuer.issuer_serial:\n                failures.append((\n                    pretty_message(\n                        '''\n                        Unable to verify OCSP response since response was\n                        signed by an unauthorized certificate\n                        '''\n                    ),\n                    ocsp_response\n                ))\n                continue\n            extended_key_usage = signing_cert.extended_key_usage_value\n            if 'ocsp_signing' not in extended_key_usage.native:\n                failures.append((\n                    pretty_message(\n                        '''\n                        Unable to verify OCSP response since response was\n                        signed by an unauthorized certificate\n                        '''\n                    ),\n                    ocsp_response\n                ))\n                continue\n\n        # Determine what algorithm was used to sign the response\n        signature_algo = response['signature_algorithm'].signature_algo\n        hash_algo = response['signature_algorithm'].hash_algo\n\n        if signature_algo == 'rsassa_pkcs1v15':\n            verify_func = asymmetric.rsa_pkcs1v15_verify\n        elif signature_algo == 'dsa':\n            verify_func = asymmetric.dsa_verify\n        elif signature_algo == 'ecdsa':\n            verify_func = asymmetric.ecdsa_verify\n        else:\n            failures.append((\n                pretty_message(\n                    '''\n                    Unable to verify OCSP response since response signature\n                    uses the unsupported algorithm %s\n                    ''',\n                    signature_algo\n                ),\n                ocsp_response\n            ))\n            continue\n\n        # Verify that the response was properly signed by the validated certificate\n        try:\n            key_object = asymmetric.load_certificate(signing_cert)\n            verify_func(key_object, response['signature'].native, tbs_response.dump(), hash_algo)\n        except (oscrypto.errors.SignatureError):\n            failures.append((\n                'Unable to verify OCSP response signature',\n                ocsp_response\n            ))\n            continue\n\n        # Finally check to see if the certificate has been revoked\n        status = cert_response['cert_status'].name\n        if status == 'good':\n            return\n\n        if status == 'revoked':\n            revocation_info = cert_response['cert_status'].chosen\n            if revocation_info['revocation_reason'].native is None:\n                reason = crl.CRLReason('unspecified').human_friendly\n            else:\n                reason = revocation_info['revocation_reason'].human_friendly\n            date = revocation_info['revocation_time'].native.strftime('%Y-%m-%d')\n            time = revocation_info['revocation_time'].native.strftime('%H:%M:%S')\n            raise RevokedError(pretty_message(\n                '''\n                OCSP response indicates %s was revoked at %s on %s, due to %s\n                ''',\n                cert_description,\n                time,\n                date,\n                reason\n            ))\n\n    if mismatch_failures == len(ocsp_responses):\n        raise OCSPNoMatchesError(pretty_message(\n            '''\n            No OCSP responses were issued for %s\n            ''',\n            cert_description\n        ))\n\n    raise OCSPValidationIndeterminateError(\n        pretty_message(\n            '''\n            Unable to determine if %s is revoked due to insufficient\n            information from OCSP responses\n            ''',\n            cert_description\n        ),\n        failures\n    )", "response": "Verifies an OCSP response for a specific key in the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef verify_crl(cert, path, validation_context, use_deltas=True, cert_description=None, end_entity_name_override=None):\n\n    if not isinstance(cert, x509.Certificate):\n        raise TypeError(pretty_message(\n            '''\n            cert must be an instance of asn1crypto.x509.Certificate, not %s\n            ''',\n            type_name(cert)\n        ))\n\n    if not isinstance(path, ValidationPath):\n        raise TypeError(pretty_message(\n            '''\n            path must be an instance of certvalidator.path.ValidationPath,\n            not %s\n            ''',\n            type_name(path)\n        ))\n\n    if not isinstance(validation_context, ValidationContext):\n        raise TypeError(pretty_message(\n            '''\n            validation_context must be an instance of\n            certvalidator.context.ValidationContext, not %s\n            ''',\n            type_name(validation_context)\n        ))\n\n    if cert_description is None:\n        cert_description = 'the certificate'\n\n    if not isinstance(cert_description, str_cls):\n        raise TypeError(pretty_message(\n            '''\n            cert_description must be a unicode string, not %s\n            ''',\n            type_name(cert_description)\n        ))\n\n    moment = validation_context.moment\n    certificate_registry = validation_context.certificate_registry\n\n    certificate_lists = validation_context.retrieve_crls(cert)\n\n    cert_issuer = path.find_issuer(cert)\n\n    complete_lists_by_issuer = {}\n    delta_lists_by_issuer = {}\n    for certificate_list in certificate_lists:\n        issuer_hashable = certificate_list.issuer.hashable\n        if certificate_list.delta_crl_indicator_value is None:\n            if issuer_hashable not in complete_lists_by_issuer:\n                complete_lists_by_issuer[issuer_hashable] = []\n            complete_lists_by_issuer[issuer_hashable].append(certificate_list)\n        else:\n            if issuer_hashable not in delta_lists_by_issuer:\n                delta_lists_by_issuer[issuer_hashable] = []\n            delta_lists_by_issuer[issuer_hashable].append(certificate_list)\n\n    # In the main loop, only complete CRLs are processed, so delta CRLs are\n    # weeded out of the todo list\n    crls_to_process = []\n    for issuer_crls in complete_lists_by_issuer.values():\n        crls_to_process.extend(issuer_crls)\n    total_crls = len(crls_to_process)\n\n    # Build a lookup table for the Distribution point objects associated with\n    # an issuer name hashable\n    distribution_point_map = {}\n    sources = [cert.crl_distribution_points]\n    if use_deltas:\n        sources.extend(cert.delta_crl_distribution_points)\n    for dp_list in sources:\n        for distribution_point in dp_list:\n            if isinstance(distribution_point['crl_issuer'], x509.GeneralNames):\n                dp_name_hashes = []\n                for general_name in distribution_point['crl_issuer']:\n                    if general_name.name == 'directory_name':\n                        dp_name_hashes.append(general_name.chosen.hashable)\n            else:\n                dp_name_hashes = [cert.issuer.hashable]\n            for dp_name_hash in dp_name_hashes:\n                if dp_name_hash not in distribution_point_map:\n                    distribution_point_map[dp_name_hash] = []\n                distribution_point_map[dp_name_hash].append(distribution_point)\n\n    valid_reasons = set([\n        'key_compromise',\n        'ca_compromise',\n        'affiliation_changed',\n        'superseded',\n        'cessation_of_operation',\n        'certificate_hold',\n        'privilege_withdrawn',\n        'aa_compromise',\n    ])\n\n    known_extensions = set([\n        'issuer_alt_name',\n        'crl_number',\n        'delta_crl_indicator',\n        'issuing_distribution_point',\n        'authority_key_identifier',\n        'freshest_crl',\n        'authority_information_access',\n    ])\n\n    checked_reasons = set()\n\n    failures = []\n    issuer_failures = 0\n\n    while len(crls_to_process) > 0:\n        certificate_list = crls_to_process.pop(0)\n        crl_idp = certificate_list.issuing_distribution_point_value\n        delta_certificate_list = None\n        delta_crl_idp = None\n\n        interim_reasons = set()\n\n        crl_issuer = None\n        crl_issuer_name = None\n        is_indirect = False\n\n        if crl_idp and crl_idp['indirect_crl'].native:\n            is_indirect = True\n            crl_idp_name = crl_idp['distribution_point']\n            if crl_idp_name:\n                if crl_idp_name.name == 'full_name':\n                    crl_issuer_name = crl_idp_name.chosen[0].chosen\n                else:\n                    crl_issuer_name = cert_issuer.subject.copy().chosen.append(\n                        crl_idp_name.chosen\n                    )\n            elif certificate_list.authority_key_identifier:\n                tmp_crl_issuer = certificate_registry.retrieve_by_key_identifier(\n                    certificate_list.authority_key_identifier\n                )\n                crl_issuer_name = tmp_crl_issuer.subject\n            else:\n                failures.append((\n                    'CRL is marked as an indirect CRL, but provides no '\n                    'mechanism for locating the CRL issuer certificate',\n                    certificate_list\n                ))\n                continue\n        else:\n            crl_issuer_name = certificate_list.issuer\n\n        if not crl_issuer:\n            crl_issuer = validation_context.check_crl_issuer(certificate_list)\n\n        if not crl_issuer:\n            candidate_crl_issuers = certificate_registry.retrieve_by_name(crl_issuer_name, cert_issuer)\n            candidates_skipped = 0\n            signatures_failed = 0\n            unauthorized_certs = 0\n\n            if not candidate_crl_issuers and crl_issuer_name != certificate_list.issuer:\n                candidate_crl_issuers = certificate_registry.retrieve_by_name(certificate_list.issuer, cert_issuer)\n\n            for candidate_crl_issuer in candidate_crl_issuers:\n                direct_issuer = candidate_crl_issuer.subject == cert_issuer.subject\n\n                # In some cases an indirect CRL issuer is a certificate issued\n                # by the certificate issuer. However, we need to ensure that\n                # the candidate CRL issuer is not the certificate being checked,\n                # otherwise we may be checking an incorrect CRL and produce\n                # incorrect results.\n                indirect_issuer = candidate_crl_issuer.issuer == cert_issuer.subject\n                indirect_issuer = indirect_issuer and candidate_crl_issuer.sha256 != cert.sha256\n\n                if not direct_issuer and not indirect_issuer and not is_indirect:\n                    candidates_skipped += 1\n                    continue\n\n                # Step f\n                candidate_crl_issuer_path = None\n\n                if validation_context:\n                    candidate_crl_issuer_path = validation_context.check_validation(candidate_crl_issuer)\n\n                if candidate_crl_issuer_path is None:\n                    candidate_crl_issuer_path = path.copy().truncate_to_issuer(candidate_crl_issuer)\n                    candidate_crl_issuer_path.append(candidate_crl_issuer)\n                    try:\n                        # Pre-emptively mark a path as validated to prevent recursion\n                        if validation_context:\n                            validation_context.record_validation(candidate_crl_issuer, candidate_crl_issuer_path)\n\n                        temp_override = end_entity_name_override\n                        if temp_override is None and candidate_crl_issuer.sha256 != cert_issuer.sha256:\n                            temp_override = cert_description + ' CRL issuer'\n                        _validate_path(\n                            validation_context,\n                            candidate_crl_issuer_path,\n                            end_entity_name_override=temp_override\n                        )\n\n                    except (PathValidationError) as e:\n                        # If the validation did not work out, clear it\n                        if validation_context:\n                            validation_context.clear_validation(candidate_crl_issuer)\n\n                        # We let a revoked error fall through since step k will catch\n                        # it with a correct error message\n                        if isinstance(e, RevokedError):\n                            raise\n                        raise CRLValidationError('CRL issuer certificate path could not be validated')\n\n                key_usage_value = candidate_crl_issuer.key_usage_value\n                if key_usage_value and 'crl_sign' not in key_usage_value.native:\n                    unauthorized_certs += 1\n                    continue\n\n                try:\n                    # Step g\n                    _verify_signature(certificate_list, candidate_crl_issuer)\n\n                    crl_issuer = candidate_crl_issuer\n                    break\n\n                except (CRLValidationError):\n                    signatures_failed += 1\n                    continue\n\n            if crl_issuer is None:\n                if candidates_skipped == len(candidate_crl_issuers):\n                    issuer_failures += 1\n                else:\n                    if signatures_failed == len(candidate_crl_issuers):\n                        failures.append((\n                            'CRL signature could not be verified',\n                            certificate_list\n                        ))\n                    elif unauthorized_certs == len(candidate_crl_issuers):\n                        failures.append((\n                            'The CRL issuer is not authorized to sign CRLs',\n                            certificate_list\n                        ))\n                    else:\n                        failures.append((\n                            'Unable to locate CRL issuer certificate',\n                            certificate_list\n                        ))\n                continue\n            else:\n                validation_context.record_crl_issuer(certificate_list, crl_issuer)\n\n        # Step b 1\n        has_dp_crl_issuer = False\n        dp_match = False\n\n        dps = cert.crl_distribution_points_value\n        if dps:\n            crl_issuer_general_name = x509.GeneralName(\n                name='directory_name',\n                value=crl_issuer.subject\n            )\n            for dp in dps:\n                if dp['crl_issuer']:\n                    has_dp_crl_issuer = True\n                    if crl_issuer_general_name in dp['crl_issuer']:\n                        dp_match = True\n\n        same_issuer = crl_issuer.subject == cert_issuer.subject\n        indirect_match = has_dp_crl_issuer and dp_match and is_indirect\n        missing_idp = has_dp_crl_issuer and (not dp_match or not is_indirect)\n        indirect_crl_issuer = crl_issuer.issuer == cert_issuer.subject\n\n        if (not same_issuer and not indirect_match and not indirect_crl_issuer) or missing_idp:\n            issuer_failures += 1\n            continue\n\n        # Check to make sure the CRL is valid for the moment specified\n        if moment < certificate_list['tbs_cert_list']['this_update'].native:\n            failures.append((\n                'CRL is from after the validation time',\n                certificate_list\n            ))\n            continue\n        if moment > certificate_list['tbs_cert_list']['next_update'].native:\n            failures.append((\n                'CRL should have been regenerated by the validation time',\n                certificate_list\n            ))\n            continue\n\n        # Step b 2\n\n        if crl_idp is not None:\n            # Step b 2 i\n            has_idp_name = False\n            has_dp_name = False\n            idp_dp_match = False\n\n            idp_general_names = []\n            idp_dp_name = crl_idp['distribution_point']\n            if idp_dp_name:\n                has_idp_name = True\n                if idp_dp_name.name == 'full_name':\n                    for general_name in idp_dp_name.chosen:\n                        idp_general_names.append(general_name)\n                else:\n                    inner_extended_issuer_name = crl_issuer.subject.copy()\n                    inner_extended_issuer_name.chosen.append(idp_dp_name.chosen.untag())\n                    idp_general_names.append(x509.GeneralName(\n                        name='directory_name',\n                        value=inner_extended_issuer_name\n                    ))\n\n            dps = cert.crl_distribution_points_value\n            if dps:\n                for dp in dps:\n                    if idp_dp_match:\n                        break\n                    dp_name = dp['distribution_point']\n                    if dp_name:\n                        has_dp_name = True\n                        if dp_name.name == 'full_name':\n                            for general_name in dp_name.chosen:\n                                if general_name in idp_general_names:\n                                    idp_dp_match = True\n                                    break\n                        else:\n                            inner_extended_issuer_name = crl_issuer.subject.copy()\n                            inner_extended_issuer_name.chosen.append(dp_name.chosen.untag())\n                            dp_extended_issuer_name = x509.GeneralName(\n                                name='directory_name',\n                                value=inner_extended_issuer_name\n                            )\n\n                            if dp_extended_issuer_name in idp_general_names:\n                                idp_dp_match = True\n\n                    elif dp['crl_issuer']:\n                        has_dp_name = True\n                        for dp_crl_issuer_name in dp['crl_issuer']:\n                            if dp_crl_issuer_name in idp_general_names:\n                                idp_dp_match = True\n                                break\n            else:\n                # If there is no DP, we consider the CRL issuer name to be it\n                has_dp_name = True\n                general_name = x509.GeneralName(\n                    name='directory_name',\n                    value=crl_issuer_name\n                )\n                if general_name in idp_general_names:\n                    idp_dp_match = True\n\n            idp_dp_match_failed = has_idp_name and has_dp_name and not idp_dp_match\n\n            if idp_dp_match_failed:\n                failures.append((\n                    pretty_message(\n                        '''\n                        The CRL issuing distribution point extension does not\n                        share any names with the certificate CRL distribution\n                        point extension\n                        '''\n                    ),\n                    certificate_list\n                ))\n                issuer_failures += 1\n                continue\n\n            # Step b 2 ii\n            if crl_idp['only_contains_user_certs'].native:\n                if cert.basic_constraints_value and cert.basic_constraints_value['ca'].native:\n                    failures.append((\n                        pretty_message(\n                            '''\n                            CRL only contains end-entity certificates and\n                            certificate is a CA certificate\n                            '''\n                        ),\n                        certificate_list\n                    ))\n                    continue\n\n            # Step b 2 iii\n            if crl_idp['only_contains_ca_certs'].native:\n                if not cert.basic_constraints_value or cert.basic_constraints_value['ca'].native is False:\n                    failures.append((\n                        pretty_message(\n                            '''\n                            CRL only contains CA certificates and certificate\n                            is an end-entity certificate\n                            '''\n                        ),\n                        certificate_list\n                    ))\n                    continue\n\n            # Step b 2 iv\n            if crl_idp['only_contains_attribute_certs'].native:\n                failures.append((\n                    'CRL only contains attribute certificates',\n                    certificate_list\n                ))\n                continue\n\n        # Step c\n        if use_deltas and certificate_list.freshest_crl_value and len(certificate_list.freshest_crl_value) > 0:\n            for candidate_delta_cl in delta_lists_by_issuer.get(crl_issuer_name.hashable, []):\n\n                # Step c 1\n                if candidate_delta_cl.issuer != crl_issuer_name:\n                    continue\n\n                # Step c 2\n                delta_crl_idp = candidate_delta_cl.issuing_distribution_point_value\n                if (crl_idp is None and delta_crl_idp is not None) or (crl_idp is not None and delta_crl_idp is None):\n                    continue\n\n                if crl_idp and crl_idp.native != delta_crl_idp.native:\n                    continue\n\n                # Step c 3\n                if certificate_list.authority_key_identifier != candidate_delta_cl.authority_key_identifier:\n                    continue\n\n                delta_certificate_list = candidate_delta_cl\n                break\n\n        # Step d\n        idp_reasons = None\n\n        if crl_idp and crl_idp['only_some_reasons'].native is not None:\n            idp_reasons = crl_idp['only_some_reasons'].native\n\n        reason_keys = None\n        if idp_reasons:\n            reason_keys = idp_reasons\n\n        if reason_keys is None:\n            interim_reasons = valid_reasons.copy()\n        else:\n            interim_reasons = reason_keys\n\n        # Step e\n        # We don't skip a CRL if it only contains reasons already checked since\n        # a certificate issuer can self-issue a new cert that is used for CRLs\n\n        if certificate_list.critical_extensions - known_extensions:\n            failures.append((\n                'One or more unrecognized critical extensions are present in '\n                'the CRL',\n                certificate_list\n            ))\n            continue\n\n        if use_deltas and delta_certificate_list and delta_certificate_list.critical_extensions - known_extensions:\n            failures.append((\n                'One or more unrecognized critical extensions are present in '\n                'the delta CRL',\n                delta_certificate_list\n            ))\n            continue\n\n        # Step h\n        if use_deltas and delta_certificate_list:\n            try:\n                _verify_signature(delta_certificate_list, crl_issuer)\n            except (CRLValidationError):\n                failures.append((\n                    'Delta CRL signature could not be verified',\n                    certificate_list,\n                    delta_certificate_list\n                ))\n                continue\n\n            if moment < delta_certificate_list['tbs_cert_list']['this_update'].native:\n                failures.append((\n                    'Delta CRL is from after the validation time',\n                    certificate_list,\n                    delta_certificate_list\n                ))\n                continue\n            if moment > delta_certificate_list['tbs_cert_list']['next_update'].native:\n                failures.append((\n                    'Delta CRL is from before the validation time',\n                    certificate_list,\n                    delta_certificate_list\n                ))\n                continue\n\n        # Step i\n        revoked_reason = None\n        revoked_date = None\n\n        if use_deltas and delta_certificate_list:\n            try:\n                revoked_date, revoked_reason = _find_cert_in_list(cert, cert_issuer, delta_certificate_list, crl_issuer)\n            except (NotImplementedError):\n                failures.append((\n                    'One or more critical extensions are present in the CRL '\n                    'entry for the certificate',\n                    delta_certificate_list\n                ))\n                continue\n\n        # Step j\n        if revoked_reason is None:\n            try:\n                revoked_date, revoked_reason = _find_cert_in_list(cert, cert_issuer, certificate_list, crl_issuer)\n            except (NotImplementedError):\n                failures.append((\n                    'One or more critical extensions are present in the CRL '\n                    'entry for the certificate',\n                    certificate_list\n                ))\n                continue\n\n        # Step k\n        if revoked_reason and revoked_reason.native == 'remove_from_crl':\n            revoked_reason = None\n            revoked_date = None\n\n        if revoked_reason:\n            reason = revoked_reason.human_friendly\n            date = revoked_date.native.strftime('%Y-%m-%d')\n            time = revoked_date.native.strftime('%H:%M:%S')\n            raise RevokedError(pretty_message(\n                '''\n                CRL indicates %s was revoked at %s on %s, due to %s\n                ''',\n                cert_description,\n                time,\n                date,\n                reason\n            ))\n\n        # Step l\n        checked_reasons |= interim_reasons\n\n    # CRLs should not include this value, but at least one of the examples\n    # from the NIST test suite does\n    checked_reasons -= set(['unused'])\n\n    if checked_reasons != valid_reasons:\n        if total_crls == issuer_failures:\n            raise CRLNoMatchesError(pretty_message(\n                '''\n                No CRLs were issued by the issuer of %s, or any indirect CRL\n                issuer\n                ''',\n                cert_description\n            ))\n\n        if not failures:\n            failures.append((\n                'The available CRLs do not cover all revocation reasons',\n            ))\n\n        raise CRLValidationIndeterminateError(\n            pretty_message(\n                '''\n                Unable to determine if %s is revoked due to insufficient\n                information from known CRLs\n                ''',\n                cert_description\n            ),\n            failures\n        )", "response": "Verifies a certificate against a list of CRLs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying the digital signature on an asn1crypto. crl. CertificateList object.", "response": "def _verify_signature(certificate_list, crl_issuer):\n    \"\"\"\n    Verifies the digital signature on an asn1crypto.crl.CertificateList object\n\n    :param certificate_list:\n        An asn1crypto.crl.CertificateList object\n\n    :param crl_issuer:\n        An asn1crypto.x509.Certificate object of the CRL issuer\n\n    :raises:\n        certvalidator.errors.CRLValidationError - when the signature is invalid or uses an unsupported algorithm\n    \"\"\"\n\n    signature_algo = certificate_list['signature_algorithm'].signature_algo\n    hash_algo = certificate_list['signature_algorithm'].hash_algo\n\n    if signature_algo == 'rsassa_pkcs1v15':\n        verify_func = asymmetric.rsa_pkcs1v15_verify\n    elif signature_algo == 'dsa':\n        verify_func = asymmetric.dsa_verify\n    elif signature_algo == 'ecdsa':\n        verify_func = asymmetric.ecdsa_verify\n    else:\n        raise CRLValidationError(pretty_message(\n            '''\n            Unable to verify the CertificateList since the signature uses the\n            unsupported algorithm %s\n            ''',\n            signature_algo\n        ))\n\n    try:\n        key_object = asymmetric.load_certificate(crl_issuer)\n        verify_func(\n            key_object,\n            certificate_list['signature'].native,\n            certificate_list['tbs_cert_list'].dump(),\n            hash_algo\n        )\n    except (oscrypto.errors.SignatureError):\n        raise CRLValidationError('Unable to verify the signature of the CertificateList')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for a cert in the list of revoked certificates and returns the date and time of the cert was revoked and why the cert was revoked and why the certificate was revoked", "response": "def _find_cert_in_list(cert, issuer, certificate_list, crl_issuer):\n    \"\"\"\n    Looks for a cert in the list of revoked certificates\n\n    :param cert:\n        An asn1crypto.x509.Certificate object of the cert being checked\n\n    :param issuer:\n        An asn1crypto.x509.Certificate object of the cert issuer\n\n    :param certificate_list:\n        An ans1crypto.crl.CertificateList object to look in for the cert\n\n    :param crl_issuer:\n        An asn1crypto.x509.Certificate object of the CRL issuer\n\n    :return:\n        A tuple of (None, None) if not present, otherwise a tuple of\n        (asn1crypto.x509.Time object, asn1crypto.crl.CRLReason object)\n        representing the date/time the object was revoked and why\n    \"\"\"\n\n    revoked_certificates = certificate_list['tbs_cert_list']['revoked_certificates']\n\n    cert_serial = cert.serial_number\n    issuer_name = issuer.subject\n\n    known_extensions = set([\n        'crl_reason',\n        'hold_instruction_code',\n        'invalidity_date',\n        'certificate_issuer'\n    ])\n\n    last_issuer_name = crl_issuer.subject\n    for revoked_cert in revoked_certificates:\n        # If any unknown critical extensions, the entry can not be used\n        if revoked_cert.critical_extensions - known_extensions:\n            raise NotImplementedError()\n\n        if revoked_cert.issuer_name and revoked_cert.issuer_name != last_issuer_name:\n            last_issuer_name = revoked_cert.issuer_name\n        if last_issuer_name != issuer_name:\n            continue\n\n        if revoked_cert['user_certificate'].native != cert_serial:\n            continue\n\n        if not revoked_cert.crl_reason_value:\n            crl_reason = crl.CRLReason('unspecified')\n        else:\n            crl_reason = revoked_cert.crl_reason_value\n\n        return (revoked_cert['revocation_date'], crl_reason)\n\n    return (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a child node to the node.", "response": "def add_child(self, valid_policy, qualifier_set, expected_policy_set):\n        \"\"\"\n        Creates a new PolicyTreeNode as a child of this node\n\n        :param valid_policy:\n            A unicode string of a policy name or OID\n\n        :param qualifier_set:\n            An instance of asn1crypto.x509.PolicyQualifierInfos\n\n        :param expected_policy_set:\n            A set of unicode strings containing policy names or OIDs\n        \"\"\"\n\n        child = PolicyTreeNode(valid_policy, qualifier_set, expected_policy_set)\n        child.parent = self\n        self.children.append(child)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a generator yielding all the nodes in the tree at a specific depth", "response": "def at_depth(self, depth):\n        \"\"\"\n        Returns a generator yielding all nodes in the tree at a specific depth\n\n        :param depth:\n            An integer >= 0 of the depth of nodes to yield\n\n        :return:\n            A generator yielding PolicyTreeNode objects\n        \"\"\"\n\n        for child in list(self.children):\n            if depth == 0:\n                yield child\n            else:\n                for grandchild in child.at_depth(depth - 1):\n                    yield grandchild"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator yielding all the nodes starting with leaves and traversing up to the root.", "response": "def walk_up(self, depth):\n        \"\"\"\n        Returns a generator yielding all nodes in the tree at a specific depth,\n        or above. Yields nodes starting with leaves and traversing up to the\n        root.\n\n        :param depth:\n            An integer >= 0 of the depth of nodes to walk up from\n\n        :return:\n            A generator yielding PolicyTreeNode objects\n        \"\"\"\n\n        for child in list(self.children):\n            if depth != 0:\n                for grandchild in child.walk_up(depth - 1):\n                    yield grandchild\n            yield child"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclearing the pool of all active in - memory entries.", "response": "def clear(self):\n        \"\"\"Clear pool connections.\"\"\"\n        while not self._pool.empty():\n            conn = yield from self._pool.get()\n            self._do_close(conn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef acquire(self):\n        while self.size() == 0 or self.size() < self._minsize:\n            _conn = yield from self._create_new_conn()\n            if _conn is None:\n                break\n            self._pool.put_nowait(_conn)\n\n        conn = None\n        while not conn:\n            _conn = yield from self._pool.get()\n            if _conn.reader.at_eof() or _conn.reader.exception():\n                self._do_close(_conn)\n                conn = yield from self._create_new_conn()\n            else:\n                conn = _conn\n\n        self._in_use.add(conn)\n        return conn", "response": "Acquire a connection from the pool or spawn new one if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release(self, conn):\n        self._in_use.remove(conn)\n        if conn.reader.at_eof() or conn.reader.exception():\n            self._do_close(conn)\n        else:\n            self._pool.put_nowait(conn)", "response": "Releases a connection back to the pool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, conn, key):\n        assert self._validate_key(key)\n\n        command = b'delete ' + key + b'\\r\\n'\n        response = yield from self._execute_simple_command(conn, command)\n\n        if response not in (const.DELETED, const.NOT_FOUND):\n            raise ClientException('Memcached delete failed', response)\n        return response == const.DELETED", "response": "Deletes a key value pair from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a single value from the server.", "response": "def get(self, conn, key, default=None):\n        \"\"\"Gets a single value from the server.\n\n        :param key: ``bytes``, is the key for the item being fetched\n        :param default: default value if there is no value.\n        :return: ``bytes``, is the data for this specified key.\n        \"\"\"\n        values, _ = yield from self._multi_get(conn, key)\n        return values.get(key, default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a single value from the server together with the cas token.", "response": "def gets(self, conn, key, default=None):\n        \"\"\"Gets a single value from the server together with the cas token.\n\n        :param key: ``bytes``, is the key for the item being fetched\n        :param default: default value if there is no value.\n        :return: ``bytes``, ``bytes tuple with the value and the cas\n        \"\"\"\n        values, cas_tokens = yield from self._multi_get(\n            conn, key, with_cas=True)\n        return values.get(key, default), cas_tokens.get(key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a list of keys and returns a list of values for the specified keys.", "response": "def multi_get(self, conn, *keys):\n        \"\"\"Takes a list of keys and returns a list of values.\n\n        :param keys: ``list`` keys for the item being fetched.\n        :return: ``list`` of values for the specified keys.\n        :raises:``ValidationException``, ``ClientException``,\n        and socket errors\n        \"\"\"\n        values, _ = yield from self._multi_get(conn, *keys)\n        return tuple(values.get(key) for key in keys)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stats(self, conn, args=None):\n        # req  - stats [additional args]\\r\\n\n        # resp - STAT <name> <value>\\r\\n (one per result)\n        #        END\\r\\n\n        if args is None:\n            args = b''\n\n        conn.writer.write(b''.join((b'stats ', args, b'\\r\\n')))\n\n        result = {}\n\n        resp = yield from conn.reader.readline()\n        while resp != b'END\\r\\n':\n            terms = resp.split()\n\n            if len(terms) == 2 and terms[0] == b'STAT':\n                result[terms[1]] = None\n            elif len(terms) == 3 and terms[0] == b'STAT':\n                result[terms[1]] = terms[2]\n            elif len(terms) >= 3 and terms[0] == b'STAT':\n                result[terms[1]] = b' '.join(terms[2:])\n            else:\n                raise ClientException('stats failed', resp)\n\n            resp = yield from conn.reader.readline()\n\n        return result", "response": "Runs a stats command on the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset a key to a value on the server .", "response": "def set(self, conn, key, value, exptime=0):\n        \"\"\"Sets a key to a value on the server\n        with an optional exptime (0 means don't auto-expire)\n\n        :param key: ``bytes``, is the key of the item.\n        :param value: ``bytes``, data to store.\n        :param exptime: ``int``, is expiration time. If it's 0, the\n        item never expires.\n        :return: ``bool``, True in case of success.\n        \"\"\"\n        flags = 0  # TODO: fix when exception removed\n        resp = yield from self._storage_command(\n            conn, b'set', key, value, flags, exptime)\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a key to a value on the server with an optional cas token", "response": "def cas(self, conn, key, value, cas_token, exptime=0):\n        \"\"\"Sets a key to a value on the server\n        with an optional exptime (0 means don't auto-expire)\n        only if value hasn't change from first retrieval\n\n        :param key: ``bytes``, is the key of the item.\n        :param value: ``bytes``, data to store.\n        :param exptime: ``int``, is expiration time. If it's 0, the\n        item never expires.\n        :param cas_token: ``int``, unique cas token retrieve from previous\n            ``gets``\n        :return: ``bool``, True in case of success.\n        \"\"\"\n        flags = 0  # TODO: fix when exception removed\n        resp = yield from self._storage_command(\n            conn, b'cas', key, value, flags, exptime, cas=cas_token)\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore this data but only if the server doesn t already hold data for this key.", "response": "def add(self, conn, key, value, exptime=0):\n        \"\"\"Store this data, but only if the server *doesn't* already\n        hold data for this key.\n\n        :param key: ``bytes``, is the key of the item.\n        :param value: ``bytes``,  data to store.\n        :param exptime: ``int`` is expiration time. If it's 0, the\n        item never expires.\n        :return: ``bool``, True in case of success.\n        \"\"\"\n        flags = 0  # TODO: fix when exception removed\n        return (yield from self._storage_command(\n            conn, b'add', key, value, flags, exptime))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring this data but only if the server does not already hold it.", "response": "def replace(self, conn, key, value, exptime=0):\n        \"\"\"Store this data, but only if the server *does*\n        already hold data for this key.\n\n        :param key: ``bytes``, is the key of the item.\n        :param value: ``bytes``,  data to store.\n        :param exptime: ``int`` is expiration time. If it's 0, the\n        item never expires.\n        :return: ``bool``, True in case of success.\n        \"\"\"\n        flags = 0  # TODO: fix when exception removed\n        return (yield from self._storage_command(\n            conn, b'replace', key, value, flags, exptime))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd data to an existing key after existing data.", "response": "def append(self, conn, key, value, exptime=0):\n        \"\"\"Add data to an existing key after existing data\n\n        :param key: ``bytes``, is the key of the item.\n        :param value: ``bytes``,  data to store.\n        :param exptime: ``int`` is expiration time. If it's 0, the\n        item never expires.\n        :return: ``bool``, True in case of success.\n        \"\"\"\n        flags = 0  # TODO: fix when exception removed\n        return (yield from self._storage_command(\n            conn, b'append', key, value, flags, exptime))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd data to an existing key before existing data.", "response": "def prepend(self, conn, key, value, exptime=0):\n        \"\"\"Add data to an existing key before existing data\n\n        :param key: ``bytes``, is the key of the item.\n        :param value: ``bytes``, data to store.\n        :param exptime: ``int`` is expiration time. If it's 0, the\n        item never expires.\n        :return: ``bool``, True in case of success.\n        \"\"\"\n        flags = 0  # TODO: fix when exception removed\n        return (yield from self._storage_command(\n            conn, b'prepend', key, value, flags, exptime))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncommand is used to change the data for some item in - place incrementing it.", "response": "def incr(self, conn, key, increment=1):\n        \"\"\"Command is used to change data for some item in-place,\n        incrementing it. The data for the item is treated as decimal\n        representation of a 64-bit unsigned integer.\n\n        :param key: ``bytes``, is the key of the item the client wishes\n        to change\n        :param increment: ``int``, is the amount by which the client\n        wants to increase the item.\n        :return: ``int``, new value of the item's data,\n        after the increment or ``None`` to indicate the item with\n        this value was not found\n        \"\"\"\n        assert self._validate_key(key)\n        resp = yield from self._incr_decr(\n            conn, b'incr', key, increment)\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decr(self, conn, key, decrement=1):\n        assert self._validate_key(key)\n        resp = yield from self._incr_decr(\n            conn, b'decr', key, decrement)\n        return resp", "response": "Command is used to change the data for some item in - place decrementing it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef touch(self, conn, key, exptime):\n        assert self._validate_key(key)\n\n        _cmd = b' '.join([b'touch', key, str(exptime).encode('utf-8')])\n        cmd = _cmd + b'\\r\\n'\n        resp = yield from self._execute_simple_command(conn, cmd)\n        if resp not in (const.TOUCHED, const.NOT_FOUND):\n            raise ClientException('Memcached touch failed', resp)\n        return resp == const.TOUCHED", "response": "This command is used to update the expiration time of an existing item without fetching it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the memcached version of the server.", "response": "def version(self, conn):\n        \"\"\"Current version of the server.\n\n        :return: ``bytes``, memcached version for current the server.\n        \"\"\"\n\n        command = b'version\\r\\n'\n        response = yield from self._execute_simple_command(\n            conn, command)\n        if not response.startswith(const.VERSION):\n            raise ClientException('Memcached version failed', response)\n        version, number = response.split()\n        return number"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_account(self, short_name, author_name=None, author_url=None,\n                       replace_token=True):\n        \"\"\" Create a new Telegraph account\n\n        :param short_name: Account name, helps users with several\n                           accounts remember which they are currently using.\n                           Displayed to the user above the \"Edit/Publish\"\n                           button on Telegra.ph, other users don't see this name\n\n        :param author_name: Default author name used when creating new articles\n\n        :param author_url: Default profile link, opened when users click on the\n                           author's name below the title. Can be any link,\n                           not necessarily to a Telegram profile or channels\n\n        :param replace_token: Replaces current token to a new user's token\n        \"\"\"\n\n        response = self._telegraph.method('createAccount', values={\n            'short_name': short_name,\n            'author_name': author_name,\n            'author_url': author_url\n        })\n\n        if replace_token:\n            self._telegraph.access_token = response.get('access_token')\n\n        return response", "response": "Create a new Telegraph account"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nedit information about a Telegraph account.", "response": "def edit_account_info(self, short_name=None, author_name=None,\n                          author_url=None):\n        \"\"\" Update information about a Telegraph account.\n            Pass only the parameters that you want to edit\n\n        :param short_name: Account name, helps users with several\n                           accounts remember which they are currently using.\n                           Displayed to the user above the \"Edit/Publish\"\n                           button on Telegra.ph, other users don't see this name\n\n        :param author_name: Default author name used when creating new articles\n\n        :param author_url: Default profile link, opened when users click on the\n                           author's name below the title. Can be any link,\n                           not necessarily to a Telegram profile or channels\n        \"\"\"\n\n        return self._telegraph.method('editAccountInfo', values={\n            'short_name': short_name,\n            'author_name': author_name,\n            'author_url': author_url\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrevokes an access token and generate a new one", "response": "def revoke_access_token(self):\n        \"\"\" Revoke access_token and generate a new one, for example,\n            if the user would like to reset all connected sessions, or\n            you have reasons to believe the token was compromised.\n            On success, returns dict with new access_token and auth_url fields\n        \"\"\"\n\n        response = self._telegraph.method('revokeAccessToken')\n\n        self._telegraph.access_token = response.get('access_token')\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_page(self, path, return_content=True, return_html=True):\n\n        response = self._telegraph.method('getPage', path=path, values={\n            'return_content': return_content\n        })\n\n        if return_content and return_html:\n            response['content'] = nodes_to_html(response['content'])\n\n        return response", "response": "Get a Telegraph page by path"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new Telegraph page with the given title content and html_content.", "response": "def create_page(self, title, content=None, html_content=None,\n                    author_name=None, author_url=None, return_content=False):\n        \"\"\" Create a new Telegraph page\n\n        :param title: Page title\n\n        :param content: Content in nodes list format (see doc)\n\n        :param html_content: Content in HTML format\n\n        :param author_name: Author name, displayed below the article's title\n\n        :param author_url: Profile link, opened when users click on\n                           the author's name below the title\n\n        :param return_content: If true, a content field will be returned\n        \"\"\"\n\n        if content is None:\n            content = html_to_nodes(html_content)\n\n        content_json = json.dumps(content)\n\n        return self._telegraph.method('createPage', values={\n            'title': title,\n            'author_name': author_name,\n            'author_url': author_url,\n            'content': content_json,\n            'return_content': return_content\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_account_info(self, fields=None):\n\n        return self._telegraph.method('getAccountInfo', {\n            'fields': json.dumps(fields) if fields else None\n        })", "response": "Get information about a Telegraph account."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the number of views for a Telegraph article.", "response": "def get_views(self, path, year=None, month=None, day=None, hour=None):\n        \"\"\" Get the number of views for a Telegraph article\n\n        :param path: Path to the Telegraph page\n\n        :param year: Required if month is passed. If passed, the number of\n                     page views for the requested year will be returned\n\n        :param month: Required if day is passed. If passed, the number of\n                      page views for the requested month will be returned\n\n        :param day: Required if hour is passed. If passed, the number of\n                    page views for the requested day will be returned\n\n        :param hour: If passed, the number of page views for\n                     the requested hour will be returned\n        \"\"\"\n\n        return self._telegraph.method('getViews', path=path, values={\n            'year': year,\n            'month': month,\n            'day': day,\n            'hour': hour\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upload_file(f):\n    with FilesOpener(f) as files:\n        response = requests.post(\n            'https://telegra.ph/upload',\n            files=files\n        ).json()\n\n    if isinstance(response, list):\n        error = response[0].get('error')\n    else:\n        error = response.get('error')\n\n    if error:\n        raise TelegraphException(error)\n\n    return [i['src'] for i in response]", "response": "Uploads a file to Telegra. ph s servers. Returns a list of links."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_by_natural_key(self, *args):\n\n        kwargs = self.natural_key_kwargs(*args)\n\n        # Since kwargs already has __ lookups in it, we could just do this:\n        # return self.get(**kwargs)\n\n        # But, we should call each related model's get_by_natural_key in case\n        # it's been overridden\n        for name, rel_to in self.model.get_natural_key_info():\n            if not rel_to:\n                continue\n\n            # Extract natural key for related object\n            nested_key = extract_nested_key(kwargs, rel_to, name)\n            if nested_key:\n                # Update kwargs with related object\n                try:\n                    kwargs[name] = rel_to.objects.get_by_natural_key(\n                        *nested_key\n                    )\n                except rel_to.DoesNotExist:\n                    # If related object doesn't exist, assume this one doesn't\n                    raise self.model.DoesNotExist()\n            else:\n                kwargs[name] = None\n\n        return self.get(**kwargs)", "response": "Get the object corresponding to the provided natural key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new object from the provided natural key values.", "response": "def create_by_natural_key(self, *args):\n        \"\"\"\n        Create a new object from the provided natural key values.  If the\n        natural key contains related objects, recursively get or create them by\n        their natural keys.\n        \"\"\"\n\n        kwargs = self.natural_key_kwargs(*args)\n        for name, rel_to in self.model.get_natural_key_info():\n            if not rel_to:\n                continue\n            nested_key = extract_nested_key(kwargs, rel_to, name)\n            # Automatically create any related objects as needed\n            if nested_key:\n                kwargs[name], is_new = (\n                    rel_to.objects.get_or_create_by_natural_key(*nested_key)\n                )\n            else:\n                kwargs[name] = None\n        return self.create(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets or create a new instance of the class by its natural key.", "response": "def get_or_create_by_natural_key(self, *args):\n        \"\"\"\n        get_or_create + get_by_natural_key\n        \"\"\"\n        try:\n            return self.get_by_natural_key(*args), False\n        except self.model.DoesNotExist:\n            return self.create_by_natural_key(*args), True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresolving the list of given keys into objects.", "response": "def resolve_keys(self, keys, auto_create=False):\n        \"\"\"\n        Resolve the list of given keys into objects, if possible.\n        Returns a mapping and a success indicator.\n        \"\"\"\n        resolved = {}\n        success = True\n        for key in keys:\n            if auto_create:\n                resolved[key] = self.find(*key)\n            else:\n                try:\n                    resolved[key] = self.get_by_natural_key(*key)\n                except self.model.DoesNotExist:\n                    success = False\n                    resolved[key] = None\n        return resolved, success"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of tuples of natural key names and their related objects.", "response": "def get_natural_key_info(cls):\n        \"\"\"\n        Derive natural key from first unique_together definition, noting which\n        fields are related objects vs. regular fields.\n        \"\"\"\n        fields = cls.get_natural_key_def()\n        info = []\n        for name in fields:\n            field = cls._meta.get_field(name)\n            rel_to = None\n            if hasattr(field, 'rel'):\n                rel_to = field.rel.to if field.rel else None\n            elif hasattr(field, 'remote_field'):\n                if field.remote_field:\n                    rel_to = field.remote_field.model\n                else:\n                    rel_to = None\n            info.append((name, rel_to))\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_natural_key_fields(cls):\n        natural_key = []\n        for name, rel_to in cls.get_natural_key_info():\n            if not rel_to:\n                natural_key.append(name)\n            else:\n                nested_key = rel_to.get_natural_key_fields()\n                natural_key.extend([\n                    name + '__' + nname\n                    for nname in nested_key\n                ])\n        return natural_key", "response": "Determine actual natural key field list for this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the natural key for this object.", "response": "def natural_key(self):\n        \"\"\"\n        Return the natural key for this object.\n\n        (This is a generic implementation of the standard Django function)\n        \"\"\"\n        # Recursively extract properties from related objects if needed\n        vals = [reduce(getattr, name.split('__'), self)\n                for name in self.get_natural_key_fields()]\n        return vals"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nderives the start and end coordinates from the source file.", "response": "def derive_coordinates(self):\n        \"\"\"\n        Depending on the compilation source, some members of the SourceRef\n        object may be incomplete.\n        Calling this function performs the necessary derivations to complete the\n        object.\n        \"\"\"\n\n        if self._coordinates_resolved:\n            # Coordinates were already resolved. Skip\n            return\n\n        if self.seg_map is not None:\n            # Translate coordinates\n            self.start, self.filename, include_ref = self.seg_map.derive_source_offset(self.start)\n            self.end, end_filename, _ = self.seg_map.derive_source_offset(self.end, is_end=True)\n        else:\n            end_filename = self.filename\n\n        line_start = 0\n        lineno = 1\n        file_pos = 0\n\n        # Skip deriving end coordinate if selection spans multiple files\n        if self.filename != end_filename:\n            get_end = False\n        elif self.end is None:\n            get_end = False\n        else:\n            get_end = True\n\n        if (self.filename is not None) and (self.start is not None):\n            with open(self.filename, 'r', newline='', encoding='utf_8') as fp:\n\n                while True:\n                    line_text = fp.readline()\n\n                    file_pos += len(line_text)\n\n                    if line_text == \"\":\n                        break\n\n                    if (self.start_line is None) and (self.start < file_pos):\n                        self.start_line = lineno\n                        self.start_col = self.start - line_start\n                        self.start_line_text = line_text.rstrip(\"\\n\").rstrip(\"\\r\")\n                        if not get_end:\n                            break\n\n                    if get_end and (self.end_line is None) and (self.end < file_pos):\n                        self.end_line = lineno\n                        self.end_col = self.end - line_start\n                        break\n\n                    lineno += 1\n                    line_start = file_pos\n\n            # If no end coordinate was derived, just do a single char selection\n            if not get_end:\n                self.end_line = self.start_line\n                self.end_col = self.start_col\n                self.end = self.start\n\n        self._coordinates_resolved = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_message(self, severity, text, src_ref):\n        lines = []\n\n        if severity >= Severity.ERROR:\n            color = Fore.RED\n        elif severity >= Severity.WARNING:\n            color = Fore.YELLOW\n        else:\n            color = Fore.GREEN\n\n        if src_ref is None:\n            # No message context available\n            lines.append(\n                color + Style.BRIGHT + severity.name.lower() + \": \" + Style.RESET_ALL + text\n            )\n            return lines\n\n        src_ref.derive_coordinates()\n\n        if (src_ref.start_line is not None) and (src_ref.start_col is not None):\n            # Start line and column is known\n            lines.append(\n                Fore.WHITE + Style.BRIGHT\n                + \"%s:%d:%d: \" % (src_ref.filename, src_ref.start_line, src_ref.start_col)\n                + color + severity.name.lower() + \": \"\n                + Style.RESET_ALL\n                + text\n            )\n        elif src_ref.start_line is not None:\n            # Only line number is known\n            lines.append(\n                Fore.WHITE + Style.BRIGHT\n                + \"%s:%d: \" % (src_ref.filename, src_ref.start_line)\n                + color + severity.name.lower() + \": \"\n                + Style.RESET_ALL\n                + text\n            )\n        else:\n            # Only filename is known\n            lines.append(\n                Fore.WHITE + Style.BRIGHT\n                + \"%s: \" % src_ref.filename\n                + color + severity.name.lower() + \": \"\n                + Style.RESET_ALL\n                + text\n            )\n\n        # If src_ref highlights a span within a single line of text, print it\n        if (src_ref.start_line is not None) and (src_ref.end_line is not None):\n            if src_ref.start_line != src_ref.end_line:\n                # multi-line reference\n                # Select remainder of the line\n                width = len(src_ref.start_line_text) - src_ref.start_col\n\n                lines.append(\n                    src_ref.start_line_text[:src_ref.start_col]\n                    + color + Style.BRIGHT\n                    + src_ref.start_line_text[src_ref.start_col:]\n                    + Style.RESET_ALL\n                )\n\n                lines.append(\n                    \" \"*src_ref.start_col\n                    + color + Style.BRIGHT\n                    + \"^\"*width\n                    + Style.RESET_ALL\n                )\n\n            else:\n                # Single line\n                width = src_ref.end_col - src_ref.start_col + 1\n\n                lines.append(\n                    src_ref.start_line_text[:src_ref.start_col]\n                    + color + Style.BRIGHT\n                    + src_ref.start_line_text[src_ref.start_col : src_ref.end_col+1]\n                    + Style.RESET_ALL\n                    + src_ref.start_line_text[src_ref.end_col+1:]\n                )\n\n                lines.append(\n                    \" \"*src_ref.start_col\n                    + color + Style.BRIGHT\n                    + \"^\"*width\n                    + Style.RESET_ALL\n                )\n\n        return lines", "response": "Formats the message prior to emitting it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nemit a message to stderr", "response": "def emit_message(self, lines):\n        \"\"\"\n        Emit message.\n        Default printer emits messages to stderr\n\n        Parameters\n        ----------\n        lines: list\n            List of strings containing each line of the message\n        \"\"\"\n\n        for line in lines:\n            print(line, file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes an array of strings.", "response": "def normalize_array(value):\n    \"\"\"\n    5.1.1.4 - c.5:\n        Arrays shall be rendered by:\n        1. generating the normalized values of its elements,\n        2. joining these elements with single underscores (_) into a single\n            character sequence, and\n        3. using the first eight characters of the md5 checksum of this\n            character sequence\n\n        ... which can be semi-formalized as:\n            subsequence( md5( join( normalized_values, '_' ), 0, 8 )\n    \"\"\"\n    norm_elements = []\n    for element in value:\n        norm_elements.append(normalize(element))\n\n    norm_str = \"_\".join(norm_elements)\n    md5 = hashlib.md5(norm_str.encode('utf-8')).hexdigest()\n    return md5[:8]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing a struct to be used in the log file.", "response": "def normalize_struct(value):\n    \"\"\"\n    5.1.1.4 - c.6:\n        Structs shall be rendered by:\n        1. generating the normalized value of each member,\n        2. joining each member\u2019s name with its normalized value, separated by\n            a single underscore (_),\n        3. joining the member character sequences with single underscores,\n        4. using the first eight characters of the md5 checksum of this\n            character sequence\n\n        ... which can be semi-formalized as:\n            member_normalization = concat( member_name, '_', normalized_member_value )\n            subsequence( md5( join( apply( struct_members, member_normalization ) ), 0, 8)\n    \"\"\"\n    norm_elements = []\n    for member_name, member_value in value._values.items():\n        norm_elements.append(\"%s_%s\" % (member_name, normalize(member_value)))\n\n    norm_str = \"_\".join(norm_elements)\n    md5 = hashlib.md5(norm_str.encode('utf-8')).hexdigest()\n    return md5[:8]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_value(self):\n        if (self._value is None) and (self.expr is not None):\n            self._value = self.expr.get_value()\n\n        return self._value", "response": "Evaluate self. expr to get the parameter s value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict_type(self):\n        current_comp = self.ref_root\n        for name, array_suffixes, name_src_ref in self.ref_elements:\n\n            # find instance\n            current_comp = current_comp.get_child_by_name(name)\n            if current_comp is None:\n                # Not found!\n                self.msg.fatal(\n                    \"Could not resolve hierarchical reference to '%s'\" % name,\n                    name_src_ref\n                )\n\n            # Do type-check in array suffixes\n            for array_suffix in array_suffixes:\n                array_suffix.predict_type()\n\n            # Check array suffixes\n            if (isinstance(current_comp, comp.AddressableComponent)) and current_comp.is_array:\n                # is an array\n                if len(array_suffixes) != len(current_comp.array_dimensions):\n                    self.msg.fatal(\n                        \"Incompatible number of index dimensions after '%s'. Expected %d, found %d.\"\n                        % (name, len(current_comp.array_dimensions), len(array_suffixes)),\n                        name_src_ref\n                    )\n            elif array_suffixes:\n                # Has array suffixes. Check if compatible with referenced component\n                self.msg.fatal(\n                    \"Unable to index non-array component '%s'\" % name,\n                    name_src_ref\n                )\n\n        return type(current_comp)", "response": "Predict the type of the component being the current version of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a resolved ComponentRef container that describes the relative path", "response": "def get_value(self, eval_width=None):\n        \"\"\"\n        Build a resolved ComponentRef container that describes the relative path\n        \"\"\"\n\n        resolved_ref_elements = []\n\n        for name, array_suffixes, name_src_ref in self.ref_elements:\n            idx_list = [ suffix.get_value() for suffix in array_suffixes ]\n            resolved_ref_elements.append((name, idx_list, name_src_ref))\n\n        # Create container\n        cref = rdltypes.ComponentRef(self.ref_root, resolved_ref_elements)\n\n        return cref"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict_type(self):\n        inst_type = self.inst_ref.predict_type()\n\n        if self.prop_ref_type.allowed_inst_type != inst_type:\n            self.msg.fatal(\n                \"'%s' is not a valid property of instance\" % self.prop_ref_type.get_name(),\n                self.src_ref\n            )\n\n        return self.prop_ref_type", "response": "Predict the type of the property being\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the size of the group node.", "response": "def get_group_node_size(node):\n    \"\"\"\n    Shared getter for AddrmapNode and RegfileNode's \"size\" property\n    \"\"\"\n    # After structural placement, children are sorted\n    if( not node.inst.children\n        or (not isinstance(node.inst.children[-1], comp.AddressableComponent))\n    ):\n        # No addressable child exists.\n        return 0\n\n    # Current node's size is based on last child\n    last_child_node = Node._factory(node.inst.children[-1], node.env, node)\n    return(\n        last_child_node.inst.addr_offset\n        + last_child_node.total_size\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_derived_property(cls, getter_function, name=None):\n\n        if name is None:\n            name = getter_function.__name__\n        mp = property(fget=getter_function)\n        setattr(cls, name, mp)", "response": "Register a user - defined derived property."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef children(self, unroll=False, skip_not_present=True):\n        for child_inst in self.inst.children:\n            if skip_not_present:\n                # Check if property ispresent == False\n                if not child_inst.properties.get('ispresent', True):\n                    # ispresent was explicitly set to False. Skip it\n                    continue\n\n            if unroll and isinstance(child_inst, comp.AddressableComponent) and child_inst.is_array:\n                # Unroll the array\n                range_list = [range(n) for n in child_inst.array_dimensions]\n                for idxs in itertools.product(*range_list):\n                    N = Node._factory(child_inst, self.env, self)\n                    N.current_idx = idxs # pylint: disable=attribute-defined-outside-init\n                    yield N\n            else:\n                yield Node._factory(child_inst, self.env, self)", "response": "Returns an iterator that provides nodes for all immediate children of this component."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef descendants(self, unroll=False, skip_not_present=True, in_post_order=False):\n        for child in self.children(unroll, skip_not_present):\n            if in_post_order:\n                yield from child.descendants(unroll, skip_not_present, in_post_order)\n\n            yield child\n\n            if not in_post_order:\n                yield from child.descendants(unroll, skip_not_present, in_post_order)", "response": "Returns an iterator that provides nodes for all descendant components of this component."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef signals(self, skip_not_present=True):\n        for child in self.children(skip_not_present=skip_not_present):\n            if isinstance(child, SignalNode):\n                yield child", "response": "Returns an iterator that provides nodes for all immediate signals of this component."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fields(self, skip_not_present=True):\n        for child in self.children(skip_not_present=skip_not_present):\n            if isinstance(child, FieldNode):\n                yield child", "response": "Returns an iterator that provides nodes for all immediate fields of this component."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef registers(self, unroll=False, skip_not_present=True):\n        for child in self.children(unroll, skip_not_present):\n            if isinstance(child, RegNode):\n                yield child", "response": "Returns an iterator that provides nodes for all immediate registers of this component."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an immediate child node with the given instance name.", "response": "def get_child_by_name(self, inst_name):\n        \"\"\"\n        Returns an immediate child :class:`~Node` whose instance name matches ``inst_name``\n\n        Returns ``None`` if ``inst_name`` does not match\n\n        Parameters\n        ----------\n        inst_name: str\n            Name of immediate child to get\n\n        Returns\n        -------\n        :class:`~Node` or None\n            Child Node. None if not found.\n        \"\"\"\n        child_inst = self.inst.get_child_by_name(inst_name)\n        if child_inst is None:\n            return None\n        return Node._factory(child_inst, self.env, self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_by_path(self, path):\n        pathparts = path.split('.')\n        current_node = self\n        for pathpart in pathparts:\n            m = re.fullmatch(r'^(\\w+)((?:\\[(?:\\d+|0[xX][\\da-fA-F]+)\\])*)$', pathpart)\n            if not m:\n                raise ValueError(\"Invalid path\")\n            inst_name, array_suffix = m.group(1,2)\n            idx_list = [ int(s,0) for s in re.findall(r'\\[(\\d+|0[xX][\\da-fA-F]+)\\]', array_suffix) ]\n\n            current_node = current_node.get_child_by_name(inst_name)\n            if current_node is None:\n                return None\n\n            if idx_list:\n                if (isinstance(current_node, AddressableNode)) and current_node.inst.is_array:\n                    # is an array\n                    if len(idx_list) != len(current_node.inst.array_dimensions):\n                        raise IndexError(\"Wrong number of array dimensions\")\n\n                    current_node.current_idx = [] # pylint: disable=attribute-defined-outside-init\n                    for i,idx in enumerate(idx_list):\n                        if idx >= current_node.inst.array_dimensions[i]:\n                            raise IndexError(\"Array index out of range\")\n                        current_node.current_idx.append(idx)\n                else:\n                    raise IndexError(\"Index attempted on non-array component\")\n\n        return current_node", "response": "Finds the descendant node that is located at the relative path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the value of a property in the system RDL source.", "response": "def get_property(self, prop_name, **kwargs):\n        \"\"\"\n        Gets the SystemRDL component property\n\n        If a property was not explicitly set in the RDL source, its default\n        value is derived. In some cases, a default value is implied according to\n        other property values.\n\n        Properties values that are a reference to a component instance are\n        converted to a :class:`~Node` overlay object.\n\n        Parameters\n        ----------\n        prop_name: str\n            SystemRDL property name\n        default:\n            Override built-in default value of property.\n            If the property was not explicitly set, return this value rather than\n            the property's intrinsic default value.\n\n        Raises\n        ------\n        LookupError\n            If prop_name is invalid\n        \"\"\"\n\n        ovr_default = False\n        default = None\n        if 'default' in kwargs:\n            ovr_default = True\n            default = kwargs.pop('default')\n\n        # Check for stray kwargs\n        if kwargs:\n            raise TypeError(\"got an unexpected keyword argument '%s'\" % list(kwargs.keys())[0])\n\n        # If its already in the component, then safe to bypass checks\n        if prop_name in self.inst.properties:\n            prop_value = self.inst.properties[prop_name]\n\n            if isinstance(prop_value, rdltypes.ComponentRef):\n                # If this is a hierarchical component reference, convert it to a Node reference\n                prop_value = prop_value.build_node_ref(self, self.env)\n            if isinstance(prop_value, rdltypes.PropertyReference):\n                prop_value._resolve_node(self)\n\n            return prop_value\n\n        if ovr_default:\n            # Default value is being overridden by user. Return their value\n            return default\n\n        # Otherwise, return its default value based on the property's rules\n        rule = self.env.property_rules.lookup_property(prop_name)\n\n        # Is it even a valid property or allowed for this component type?\n        if rule is None:\n            raise LookupError(\"Unknown property '%s'\" % prop_name)\n        if type(self.inst) not in rule.bindable_to:\n            raise LookupError(\"Unknown property '%s'\" % prop_name)\n\n        # Return the default value as specified by the rulebook\n        return rule.get_default(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_properties(self, list_all=False):\n\n        if list_all:\n            props = []\n            for k,v in self.env.property_rules.rdl_properties.items():\n                if type(self.inst) in v.bindable_to:\n                    props.append(k)\n            for k,v in self.env.property_rules.user_properties.items():\n                if type(self.inst) in v.bindable_to:\n                    props.append(k)\n            return props\n        else:\n            return list(self.inst.properties.keys())", "response": "Lists properties associated with this node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_path(self, hier_separator=\".\", array_suffix=\"[{index:d}]\", empty_array_suffix=\"[]\"):\n        if self.parent and not isinstance(self.parent, RootNode):\n            return(\n                self.parent.get_path(hier_separator, array_suffix, empty_array_suffix)\n                + hier_separator\n                + self.get_path_segment(array_suffix, empty_array_suffix)\n            )\n        else:\n            return self.get_path_segment(array_suffix, empty_array_suffix)", "response": "Generates an absolute path string to this node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_html_desc(self, markdown_inst=None):\n        desc_str = self.get_property(\"desc\")\n        if desc_str is None:\n            return None\n        return rdlformatcode.rdlfc_to_html(desc_str, self, md=markdown_inst)", "response": "Returns the string representation of the description of the node as HTML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the byte address offset of this node relative to the parent node s base address.", "response": "def address_offset(self):\n        \"\"\"\n        Byte address offset of this node relative to it's parent\n\n        If this node is an array, it's index must be known\n\n        Raises\n        ------\n        ValueError\n            If this property is referenced on a node whose array index is not\n            fully defined\n        \"\"\"\n        if self.inst.is_array:\n            if self.current_idx is None:\n                raise ValueError(\"Index of array element must be known to derive address\")\n\n            # Calculate the \"flattened\" index of a general multidimensional array\n            # For example, a component array declared as:\n            #   foo[S0][S1][S2]\n            # and referenced as:\n            #   foo[I0][I1][I2]\n            # Is flattened like this:\n            #   idx = I0*S1*S2 + I1*S2 + I2\n            idx = 0\n            for i in range(len(self.current_idx)):\n                sz = 1\n                for j in range(i+1, len(self.inst.array_dimensions)):\n                    sz *= self.inst.array_dimensions[j]\n                idx += sz * self.current_idx[i]\n\n            offset = self.inst.addr_offset + idx * self.inst.array_stride\n\n        else:\n            offset = self.inst.addr_offset\n\n        return offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the absolute byte address of this property.", "response": "def absolute_address(self):\n        \"\"\"\n        Get the absolute byte address of this node.\n\n        Indexes of all arrays in the node's lineage must be known\n\n        Raises\n        ------\n        ValueError\n            If this property is referenced on a node whose array lineage is not\n            fully defined\n\n        \"\"\"\n        if self.parent and not isinstance(self.parent, RootNode):\n            return self.parent.absolute_address + self.address_offset\n        else:\n            return self.address_offset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef total_size(self):\n        if self.inst.is_array:\n            # Total size of arrays is technically supposed to be:\n            #   self.inst.array_stride * (self.inst.n_elements-1) + self.size\n            # However this opens up a whole slew of ugly corner cases that the\n            # spec designers may not have anticipated.\n            # Using a simplified calculation for now until someone actually cares\n            return self.inst.array_stride * self.inst.n_elements\n\n        else:\n            return self.size", "response": "Determines the total size of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the top - level addrmap node.", "response": "def top(self):\n        \"\"\"\n        Returns the top-level addrmap node\n        \"\"\"\n        for child in self.children(skip_not_present=False):\n            if not isinstance(child, AddrmapNode):\n                continue\n            return child\n        raise RuntimeError"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the resource is inherently volatile.", "response": "def is_volatile(self):\n        \"\"\"\n        True if combination of field access properties result in a field that\n        should be interpreted as volatile.\n        (Any hardware-writable field is inherently volatile)\n        \"\"\"\n\n        hw = self.get_property('hw')\n        return (\n            (hw in (rdltypes.AccessType.rw, rdltypes.AccessType.rw1,\n                    rdltypes.AccessType.w, rdltypes.AccessType.w1))\n            or self.get_property('counter')\n            or (self.get_property('next') is not None)\n            or self.get_property('hwset')\n            or self.get_property('hwclr')\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_sw_writable(self):\n        sw = self.get_property('sw')\n\n        return sw in (rdltypes.AccessType.rw, rdltypes.AccessType.rw1,\n                        rdltypes.AccessType.w, rdltypes.AccessType.w1)", "response": "Returns True if the software has the field sw writable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_sw_readable(self):\n        sw = self.get_property('sw')\n\n        return sw in (rdltypes.AccessType.rw, rdltypes.AccessType.rw1,\n                        rdltypes.AccessType.r)", "response": "Returns True if the software is readable by this entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef implements_storage(self):\n\n        # 9.4.1, Table 12\n        sw = self.get_property('sw')\n        hw = self.get_property('hw')\n        if sw in (rdltypes.AccessType.rw, rdltypes.AccessType.rw1):\n            # Software can read and write, implying a storage element\n            return True\n        if hw == rdltypes.AccessType.rw:\n            # Hardware can read and write, implying a storage element\n            return True\n        if (sw in (rdltypes.AccessType.w, rdltypes.AccessType.w1)) and (hw == rdltypes.AccessType.r):\n            # Write-only register visible to hardware is stored\n            return True\n\n\n        onread = self.get_property('onread')\n        if onread is not None:\n            # 9.6.1-c: Onread side-effects imply storage regardless of whether\n            # or not the field is writable by sw\n            return True\n\n\n        if self.get_property('hwset') or self.get_property('hwclr'):\n            # Not in spec, but these imply that a storage element exists\n            return True\n\n        return False", "response": "Returns True if the field contains a storage element."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates and possibly instantiate a component.", "response": "def visitComponent_def(self, ctx:SystemRDLParser.Component_defContext):\n        \"\"\"\n        Create, and possibly instantiate a component\n        \"\"\"\n\n        # Get definition. Returns Component\n        if ctx.component_anon_def() is not None:\n            comp_def = self.visit(ctx.component_anon_def())\n        elif ctx.component_named_def() is not None:\n            comp_def = self.visit(ctx.component_named_def())\n        else:\n            raise RuntimeError\n\n        comp_def.parent_scope = self.component\n\n        if ctx.component_insts() is not None:\n            if isinstance(self, RootVisitor) and isinstance(comp_def, comp.Addrmap):\n                self.msg.warning(\n                    \"Non-standard instantiation of an addrmap in root namespace will be ignored\",\n                    SourceRef.from_antlr(ctx.component_insts().component_inst(0).ID())\n                )\n            else:\n                # Component is instantiated one or more times\n                if ctx.component_inst_type() is not None:\n                    inst_type = self.visit(ctx.component_inst_type())\n                else:\n                    inst_type = None\n\n                # Pass some temporary info to visitComponent_insts\n                self._tmp = (comp_def, inst_type, None)\n                self.visit(ctx.component_insts())\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef define_component(self, body, type_token, def_name, param_defs):\n        for subclass in ComponentVisitor.__subclasses__():\n            if subclass.comp_type == self._CompType_Map[type_token.type]:\n                visitor = subclass(self.compiler, def_name, param_defs)\n                return visitor.visit(body)\n        raise RuntimeError", "response": "Given a component definition recurse to another ComponentVisitor to define a new component"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the integer expression in any of the four instance assignment operators.", "response": "def get_instance_assignment(self, ctx):\n        \"\"\"\n        Gets the integer expression in any of the four instance assignment\n        operators ('=' '@' '+=' '%=')\n        \"\"\"\n        if ctx is None:\n            return None\n\n        visitor = ExprVisitor(self.compiler)\n        expr = visitor.visit(ctx.expr())\n        expr = expressions.AssignmentCast(self.compiler.env, SourceRef.from_antlr(ctx.op), expr, int)\n        expr.predict_type()\n        return expr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvisit a Param_defContext and return a list of Parameter Definitions", "response": "def visitParam_def(self, ctx:SystemRDLParser.Param_defContext):\n        \"\"\"\n        Parameter Definition block\n        \"\"\"\n        self.compiler.namespace.enter_scope()\n\n        param_defs = []\n        for elem in ctx.getTypedRuleContexts(SystemRDLParser.Param_def_elemContext):\n            param_def = self.visit(elem)\n            param_defs.append(param_def)\n\n        self.compiler.namespace.exit_scope()\n        return param_defs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an instance of the Parameter class for the current parameter definition element.", "response": "def visitParam_def_elem(self, ctx:SystemRDLParser.Param_def_elemContext):\n        \"\"\"\n        Individual parameter definition elements\n        \"\"\"\n\n        # Construct parameter type\n        data_type_token = self.visit(ctx.data_type())\n        param_data_type = self.datatype_from_token(data_type_token)\n        if ctx.array_type_suffix() is None:\n            # Non-array type\n            param_type = param_data_type\n        else:\n            # Array-like type\n            param_type = rdltypes.ArrayPlaceholder(param_data_type)\n\n        # Get parameter name\n        param_name = get_ID_text(ctx.ID())\n\n        # Get expression for parameter default, if any\n        if ctx.expr() is not None:\n            visitor = ExprVisitor(self.compiler)\n            default_expr = visitor.visit(ctx.expr())\n            default_expr = expressions.AssignmentCast(self.compiler.env, SourceRef.from_antlr(ctx.ID()), default_expr, param_type)\n            default_expr.predict_type()\n        else:\n            default_expr = None\n\n        # Create Parameter object\n        param = Parameter(param_type, param_name, default_expr)\n\n        # Register it in the parameter def namespace scope\n        self.compiler.namespace.register_element(param_name, param, None, SourceRef.from_antlr(ctx.ID()))\n\n        return param"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef datatype_from_token(self, token):\n\n        if token.type == SystemRDLParser.ID:\n            # Is an identifier for either an enum or struct type\n\n            typ = self.compiler.namespace.lookup_type(get_ID_text(token))\n            if typ is None:\n                self.msg.fatal(\n                    \"Type '%s' is not defined\" % get_ID_text(token),\n                    SourceRef.from_antlr(token)\n                )\n\n            if rdltypes.is_user_enum(typ) or rdltypes.is_user_struct(typ):\n                return typ\n            else:\n                self.msg.fatal(\n                    \"Type '%s' is not a struct or enum\" % get_ID_text(token),\n                    SourceRef.from_antlr(token)\n                )\n\n        else:\n            return self._DataType_Map[token.type]", "response": "Given a SystemRDLParser token lookup the type of the object that represents it"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a value return the type identifier object used within the RDL compiler If the value is not a supported type return None", "response": "def get_rdltype(value):\n    \"\"\"\n    Given a value, return the type identifier object used within the RDL compiler\n    If not a supported type, return None\n    \"\"\"\n\n    if isinstance(value, (int, bool, str)):\n        # Pass canonical types as-is\n        return type(value)\n    elif is_user_enum(type(value)):\n        return type(value)\n    elif is_user_struct(type(value)):\n        return type(value)\n    elif isinstance(value, enum.Enum):\n        return type(value)\n    elif isinstance(value, list):\n        # Create ArrayPlaceholder representation\n        # Determine element type and make sure it is uniform\n        array_el_type = None\n        for el in value:\n            el_type = get_rdltype(el)\n            if el_type is None:\n                return None\n\n            if (array_el_type is not None) and (el_type != array_el_type):\n                return None\n            array_el_type = el_type\n        return ArrayPlaceholder(array_el_type)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an HTML formatted string of the enum s description property.", "response": "def get_html_desc(self, markdown_inst=None):\n        \"\"\"\n        Translates the enum's 'desc' property into HTML.\n\n        Any RDLFormatCode tags used in the description are converted to HTML.\n        The text is also fed through a Markdown processor.\n\n        The additional Markdown processing allows designers the choice to use a\n        more modern lightweight markup language as an alternative to SystemRDL's\n        \"RDLFormatCode\".\n\n        Parameters\n        ----------\n        markdown_inst: ``markdown.Markdown``\n            Override the class instance of the Markdown processor.\n            See the `Markdown module <https://python-markdown.github.io/reference/#Markdown>`_\n            for more details.\n\n        Returns\n        -------\n        str or None\n            HTML formatted string.\n            If node does not have a description, returns ``None``\n        \"\"\"\n        desc_str = self._rdl_desc_\n        if desc_str is None:\n            return None\n        return rdlformatcode.rdlfc_to_html(desc_str, md=markdown_inst)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a string that represents this enum s declaration namespace namespaceama.", "response": "def get_scope_path(cls, scope_separator=\"::\"):\n        \"\"\"\n        Generate a string that represents this enum's declaration namespace\n        scope.\n\n        Parameters\n        ----------\n        scope_separator: str\n            Override the separator between namespace scopes\n        \"\"\"\n        if cls.get_parent_scope() is None:\n            return \"\"\n        elif isinstance(cls.get_parent_scope(), comp.Root):\n            return \"\"\n        else:\n            parent_path = cls.get_parent_scope().get_scope_path(scope_separator)\n            if parent_path:\n                return(\n                    parent_path\n                    + scope_separator\n                    + cls.get_parent_scope().type_name\n                )\n            else:\n                return cls.get_parent_scope().type_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefines a new struct type derived from the current type.", "response": "def define_new(cls, name, members, is_abstract=False):\n        \"\"\"\n        Define a new struct type derived from the current type.\n\n        Parameters\n        ----------\n        name: str\n            Name of the struct type\n        members: {member_name : type}\n            Dictionary of struct member types.\n        is_abstract: bool\n            If set, marks the struct as abstract.\n        \"\"\"\n        m = OrderedDict(cls._members)\n\n        # Make sure derivation does not have any overlapping keys with its parent\n        if set(m.keys()) & set(members.keys()):\n            raise ValueError(\"'members' contains keys that overlap with parent\")\n\n        m.update(members)\n\n        dct = {\n            '_members' : m,\n            '_is_abstract': is_abstract,\n        }\n        newcls = type(name, (cls,), dct)\n        return newcls"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefine a user - defined property.", "response": "def define_udp(self, name, valid_type, valid_components=None, default=None):\n        \"\"\"\n        Pre-define a user-defined property.\n\n        This is the equivalent to the following RDL:\n\n        .. code-block:: none\n\n            property <name> {\n                type = <valid_type>;\n                component = <valid_components>;\n                default = <default>\n            };\n\n        Parameters\n        ----------\n        name: str\n            Property name\n        valid_components: list\n            List of :class:`~systemrdl.component.Component` types the UDP can be bound to.\n            If None, then UDP can be bound to all components.\n        valid_type: type\n            Assignment type that this UDP will enforce\n        default:\n            Default if a value is not specified when the UDP is bound to a component.\n            Value must be compatible with ``valid_type``\n\n        \"\"\"\n        if valid_components is None:\n            valid_components = [\n                comp.Field,\n                comp.Reg,\n                comp.Regfile,\n                comp.Addrmap,\n                comp.Mem,\n                comp.Signal,\n                #TODO constraint,\n            ]\n\n        if name in self.env.property_rules.rdl_properties:\n            raise ValueError(\"name '%s' conflicts with existing built-in RDL property\")\n\n        udp = UserProperty(self.env, name, valid_components, [valid_type], default)\n\n        self.env.property_rules.user_properties[udp.name] = udp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a single file and append it to the RDLCompiler s root_namespace namespace.", "response": "def compile_file(self, path, incl_search_paths=None):\n        \"\"\"\n        Parse & compile a single file and append it to RDLCompiler's root\n        namespace.\n\n        If any exceptions (:class:`~systemrdl.RDLCompileError` or other)\n        occur during compilation, then the RDLCompiler object should be discarded.\n\n        Parameters\n        ----------\n        path:str\n            Path to an RDL source file\n\n        incl_search_paths:list\n            List of additional paths to search to resolve includes.\n            If unset, defaults to an empty list.\n\n            Relative include paths are resolved in the following order:\n\n            1. Search each path specified in ``incl_search_paths``.\n            2. Path relative to the source file performing the include.\n\n        Raises\n        ------\n        :class:`~systemrdl.RDLCompileError`\n            If any fatal compile error is encountered.\n        \"\"\"\n\n        if incl_search_paths is None:\n            incl_search_paths = []\n\n        fpp = preprocessor.FilePreprocessor(self.env, path, incl_search_paths)\n        preprocessed_text, seg_map = fpp.preprocess()\n        input_stream = preprocessor.PreprocessedInputStream(preprocessed_text, seg_map)\n\n        lexer = SystemRDLLexer(input_stream)\n        lexer.removeErrorListeners()\n        lexer.addErrorListener(messages.RDLAntlrErrorListener(self.msg))\n\n        token_stream = CommonTokenStream(lexer)\n\n        parser = SystemRDLParser(token_stream)\n        parser.removeErrorListeners()\n        parser.addErrorListener(messages.RDLAntlrErrorListener(self.msg))\n\n        # Run Antlr parser on input\n        parsed_tree = parser.root()\n        if self.msg.had_error:\n            self.msg.fatal(\"Parse aborted due to previous errors\")\n\n        # Traverse parse tree with RootVisitor\n        self.visitor.visit(parsed_tree)\n\n        # Reset default property assignments from namespace.\n        # They should not be shared between files since that would be confusing.\n        self.namespace.default_property_ns_stack = [{}]\n\n        if self.msg.had_error:\n            self.msg.fatal(\"Compile aborted due to previous errors\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new instance of the addrmap component with the given top - level addrmap component definition and parameters.", "response": "def elaborate(self, top_def_name=None, inst_name=None, parameters=None):\n        \"\"\"\n        Elaborates the design for the given top-level addrmap component.\n\n        During elaboration, the following occurs:\n\n        - An instance of the ``$root`` meta-component is created.\n        - The addrmap component specified by ``top_def_name`` is instantiated as a\n          child of ``$root``.\n        - Expressions, parameters, and inferred address/field placements are elaborated.\n        - Validation checks are performed.\n\n        If a design contains multiple root-level addrmaps, ``elaborate()`` can be\n        called multiple times in order to elaborate each individually.\n\n        If any exceptions (:class:`~systemrdl.RDLCompileError` or other)\n        occur during elaboration, then the RDLCompiler object should be discarded.\n\n        Parameters\n        ----------\n        top_def_name: str\n            Explicitly choose which addrmap  in the root namespace will be the\n            top-level component.\n\n            If unset, The last addrmap defined will be chosen.\n\n        inst_name: str\n            Overrides the top-component's instantiated name.\n            By default, instantiated name is the same as ``top_def_name``\n\n        parameters: dict\n            Dictionary of parameter overrides for the top component instance.\n\n        Raises\n        ------\n        :class:`~systemrdl.RDLCompileError`\n            If any fatal elaboration error is encountered\n\n        Returns\n        -------\n        :class:`~systemrdl.node.RootNode`\n            Elaborated root meta-component's Node object.\n        \"\"\"\n        if parameters is None:\n            parameters = {}\n\n        # Get top-level component definition to elaborate\n        if top_def_name is not None:\n            # Lookup top_def_name\n            if top_def_name not in self.root.comp_defs:\n                self.msg.fatal(\"Elaboration target '%s' not found\" % top_def_name)\n            top_def = self.root.comp_defs[top_def_name]\n\n            if not isinstance(top_def, comp.Addrmap):\n                self.msg.fatal(\"Elaboration target '%s' is not an 'addrmap' component\" % top_def_name)\n        else:\n            # Not specified. Find the last addrmap defined\n            for comp_def in reversed(list(self.root.comp_defs.values())):\n                if isinstance(comp_def, comp.Addrmap):\n                    top_def = comp_def\n                    top_def_name = comp_def.type_name\n                    break\n            else:\n                self.msg.fatal(\"Could not find any 'addrmap' components to elaborate\")\n\n        # Create an instance of the root component\n        root_inst = deepcopy(self.root)\n        root_inst.is_instance = True\n        root_inst.original_def = self.root\n        root_inst.inst_name = \"$root\"\n\n        # Create a top-level instance\n        top_inst = deepcopy(top_def)\n        top_inst.is_instance = True\n        top_inst.original_def = top_def\n        top_inst.addr_offset = 0\n        top_inst.external = True # addrmap is always implied as external\n        if inst_name is not None:\n            top_inst.inst_name = inst_name\n        else:\n            top_inst.inst_name = top_def_name\n\n        # Override parameters as needed\n        for param_name, value in parameters.items():\n            # Find the parameter to override\n            parameter = None\n            for p in top_inst.parameters:\n                if p.name == param_name:\n                    parameter = p\n                    break\n            else:\n                raise ValueError(\"Parameter '%s' is not available for override\" % param_name)\n\n\n            value_expr = expr.ExternalLiteral(self.env, value)\n            value_type = value_expr.predict_type()\n            if value_type is None:\n                raise TypeError(\"Override value for parameter '%s' is an unrecognized type\" % param_name)\n\n            if value_type != parameter.param_type:\n                raise TypeError(\"Incorrect type for parameter '%s'\" % param_name)\n\n            parameter.expr = value_expr\n\n\n        # instantiate top_inst into the root component instance\n        root_inst.children.append(top_inst)\n\n        root_node = RootNode(root_inst, self.env, None)\n\n        # Resolve all expressions\n        walker.RDLWalker(skip_not_present=False).walk(\n            root_node,\n            ElabExpressionsListener(self.msg)\n        )\n\n        # Resolve address and field placement\n        walker.RDLWalker(skip_not_present=False).walk(\n            root_node,\n            PrePlacementValidateListener(self.msg),\n            StructuralPlacementListener(self.msg),\n            LateElabListener(self.msg)\n        )\n\n        # Validate design\n        # Only need to validate nodes that are present\n        walker.RDLWalker(skip_not_present=True).walk(root_node, ValidateListener(self.env))\n\n        if self.msg.had_error:\n            self.msg.fatal(\"Elaborate aborted due to previous errors\")\n\n        return root_node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assign_value(self, comp_def, value, src_ref):\n\n        # Check if property is allowed in this component\n        if type(comp_def) not in self.bindable_to:\n            self.env.msg.fatal(\n                \"The property '%s' is not valid for '%s' components\"\n                % (self.get_name(), type(comp_def).__name__.lower()),\n                src_ref\n            )\n\n        # Property assignments with no rhs show up as None here\n        # For built-in properties, this implies a True value\n        if value is None:\n            value = True\n\n        # unpack true type of value\n        # Contents of value can be:\n        #   - An expression (instance of an Expr subclass)\n        #   - Direct assignment of a type-compatible value\n        if isinstance(value, expressions.Expr):\n            # Predict expected type after value would get evaluated\n            assign_type = value.predict_type()\n        elif rdltypes.is_user_enum(value):\n            # Value is a user enum type derived from UserEnum.\n            # Mark it as such\n            assign_type = rdltypes.UserEnum\n        else:\n            # Value is already evaluated\n            assign_type = type(value)\n\n        # Check if value's type is compatible\n        for valid_type in self.valid_types:\n            if expressions.is_castable(assign_type, valid_type):\n                break\n        else:\n            self.env.msg.fatal(\n                \"Incompatible assignment to property '%s'\" % self.get_name(),\n                src_ref\n            )\n\n        # Store the property\n        comp_def.properties[self.get_name()] = value", "response": "Assign the value to the property in the specified component definition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassign the value to the opposite of the current entry in the component definition.", "response": "def assign_value(self, comp_def, value, src_ref):\n        \"\"\"\n        Side effect: Ensure assignment of the opposite is cleared since it is being\n        overridden\n        \"\"\"\n        super().assign_value(comp_def, value, src_ref)\n        if self.opposite_property in comp_def.properties:\n            del comp_def.properties[self.opposite_property]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default value of the attribute if it is not explicitly set.", "response": "def get_default(self, node):\n        \"\"\"\n        If not explicitly set, check if the opposite was set first before returning\n        default\n        \"\"\"\n        if self.opposite_property in node.inst.properties:\n            return not node.inst.properties[self.opposite_property]\n        else:\n            return self.default"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default(self, node):\n        if node.inst.properties.get(\"onread\", None) == rdltypes.OnReadType.rset:\n            return True\n        else:\n            return self.default", "response": "Returns the default value for the node if it is not explicitly set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assign_value(self, comp_def, value, src_ref):\n        super().assign_value(comp_def, value, src_ref)\n        if \"rclr\" in comp_def.properties:\n            del comp_def.properties[\"rclr\"]\n        if \"rset\" in comp_def.properties:\n            del comp_def.properties[\"rset\"]", "response": "Override assign_value to override the value in the component definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default(self, node):\n        if node.inst.properties.get(\"rset\", False):\n            return rdltypes.OnReadType.rset\n        elif node.inst.properties.get(\"rclr\", False):\n            return rdltypes.OnReadType.rclr\n        else:\n            return self.default", "response": "Returns the default value for the object based on the properties of the node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default value for the node.", "response": "def get_default(self, node):\n        \"\"\"\n        If not explicitly set, check if onwrite sets the equivalent\n        \"\"\"\n        if node.inst.properties.get(\"onwrite\", None) == rdltypes.OnWriteType.woclr:\n            return True\n        else:\n            return self.default"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default(self, node):\n        if node.inst.properties.get(\"woset\", False):\n            return rdltypes.OnWriteType.woset\n        elif node.inst.properties.get(\"woclr\", False):\n            return rdltypes.OnWriteType.woclr\n        else:\n            return self.default", "response": "Returns the default value for the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassign value to the entry in the comp_def.", "response": "def assign_value(self, comp_def, value, src_ref):\n        \"\"\"\n        Set both alias and actual value\n        \"\"\"\n        super().assign_value(comp_def, value, src_ref)\n        comp_def.properties['incrthreshold'] = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the default value is set for the node.", "response": "def get_default(self, node):\n        \"\"\"\n        Unless specified otherwise, intr fields are implicitly stickybit\n        \"\"\"\n        if node.inst.properties.get(\"intr\", False):\n            # Interrupt is set!\n            # Default is implicitly stickybit, unless the mutually-exclusive\n            # sticky property was set instead\n            return not node.inst.properties.get(\"sticky\", False)\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve addresses of children of Addrmap and Regfile components of a given node.", "response": "def resolve_addresses(self, node):\n        \"\"\"\n        Resolve addresses of children of Addrmap and Regfile components\n        \"\"\"\n\n        # Get alignment based on 'alignment' property\n        # This remains constant for all children\n        prop_alignment = self.alignment_stack[-1]\n        if prop_alignment is None:\n            # was not specified. Does not contribute to alignment\n            prop_alignment = 1\n\n        prev_node = None\n        for child_node in node.children(skip_not_present=False):\n            if not isinstance(child_node, AddressableNode):\n                continue\n\n            if child_node.inst.addr_offset is not None:\n                # Address is already known. Do not need to infer\n                prev_node = child_node\n                continue\n\n            if node.env.chk_implicit_addr:\n                node.env.msg.message(\n                    node.env.chk_implicit_addr,\n                    \"Address offset of component '%s' is not explicitly set\" % child_node.inst.inst_name,\n                    child_node.inst.inst_src_ref\n                )\n\n            # Get alignment specified by '%=' allocator, if any\n            alloc_alignment = child_node.inst.addr_align\n            if alloc_alignment is None:\n                # was not specified. Does not contribute to alignment\n                alloc_alignment = 1\n\n            # Calculate alignment based on current addressing mode\n            if self.addressing_mode_stack[-1] == rdltypes.AddressingType.compact:\n                if isinstance(child_node, RegNode):\n                    # Regs are aligned based on their accesswidth\n                    mode_alignment = child_node.get_property('accesswidth') // 8\n                else:\n                    # Spec does not specify for other components\n                    # Assuming absolutely compact packing\n                    mode_alignment = 1\n\n            elif self.addressing_mode_stack[-1] == rdltypes.AddressingType.regalign:\n                # Components are aligned to a multiple of their size\n                # Spec vaguely suggests that alignment is also a power of 2\n                mode_alignment = child_node.size\n                mode_alignment = roundup_pow2(mode_alignment)\n\n            elif self.addressing_mode_stack[-1] == rdltypes.AddressingType.fullalign:\n                # Same as regalign except for arrays\n                # Arrays are aligned to their total size\n                # Both are rounded to power of 2\n                mode_alignment = child_node.total_size\n                mode_alignment = roundup_pow2(mode_alignment)\n\n            else:\n                raise RuntimeError\n\n            # Calculate resulting address offset\n            alignment = max(prop_alignment, alloc_alignment, mode_alignment)\n            if prev_node is None:\n                next_offset = 0\n            else:\n                next_offset = prev_node.inst.addr_offset + prev_node.total_size\n\n            # round next_offset up to alignment\n            child_node.inst.addr_offset = roundup_to(next_offset, alignment)\n\n            prev_node = child_node\n\n        # Sort children by address offset\n        # Non-addressable child components are sorted to be first (signals)\n        def get_child_sort_key(inst):\n            if not isinstance(inst, comp.AddressableComponent):\n                return -1\n            else:\n                return inst.addr_offset\n        node.inst.children.sort(key=get_child_sort_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ID_text(token):\n    if isinstance(token, CommonToken):\n        text = token.text\n    else:\n        text = token.getText()\n\n    text = text.lstrip('\\\\')\n    return text", "response": "Get the text from the ID token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef derive_source_offset(self, offset, is_end=False):\n        for segment in self.segments:\n            if offset <= segment.end:\n                if isinstance(segment, MacroSegment):\n                    if is_end:\n                        return (\n                            segment.src_end,\n                            segment.src,\n                            segment.incl_ref\n                        )\n                    else:\n                        return (\n                            segment.src_start,\n                            segment.src,\n                            segment.incl_ref\n                        )\n                else:\n                    return (\n                        segment.src_start + (offset - segment.start),\n                        segment.src,\n                        segment.incl_ref\n                    )\n\n        # Reached end. Assume end of last segment\n        return (\n            self.segments[-1].src_end,\n            self.segments[-1].src,\n            self.segments[-1].incl_ref\n        )", "response": "Derives the corresponding source offset from the original source file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an RDLFormatCode string to an HTML node.", "response": "def rdlfc_to_html(text, node=None, md=None):\n    \"\"\"\n    Convert an RDLFormatCode string to HTML\n    \"\"\"\n    if md is None:\n        md = markdown.Markdown()\n\n    # --------------------------------------------------------------------------\n    # Remove any common indentation\n    # --------------------------------------------------------------------------\n    linelist = text.splitlines()\n    if linelist:\n        text = (\n            linelist[0].lstrip()\n            + \"\\n\"\n            + textwrap.dedent(\"\\n\".join(linelist[1:]))\n        )\n    else:\n        text = text.strip()\n\n    # --------------------------------------------------------------------------\n    # Parse and replace RDLFormatCode Tags\n    # --------------------------------------------------------------------------\n    token_spec = [\n        ('b', r'\\[b\\]'),\n        ('xb', r'\\[/b\\]'),\n        ('i', r'\\[i\\]'),\n        ('xi', r'\\[/i\\]'),\n        ('u', r'\\[u\\]'),\n        ('xu', r'\\[/u\\]'),\n        ('color', r'\\[color=[^\\]]+\\]'),\n        ('xcolor', r'\\[/color\\]'),\n        ('size', r'\\[size=[^\\]]+\\]'),\n        ('xsize', r'\\[/size\\]'),\n        ('plainurl', r'\\[url\\].*?\\[/url\\]'),\n        ('url', r'\\[url=[^\\]]+\\]'),\n        ('xurl', r'\\[/url\\]'),\n        ('email', r'\\[email\\].*?\\[/email\\]'),\n        ('img', r'\\[img\\].*?\\[/img\\]'),\n        ('code', r'\\[code\\].*?\\[/code\\]'),\n        ('list', r'\\[list(?:=[^\\]]+)?\\]'),\n        ('bullet', r'\\[\\*\\]'),\n        ('xlist', r'\\[/list\\]'),\n        ('quote', r'\\[quote\\]'),\n        ('xquote', r'\\[/quote\\]'),\n        ('br', r'\\[br\\]'),\n        ('lb', r'\\[lb\\]'),\n        ('rb', r'\\[rb\\]'),\n        ('p', r'\\[p\\]'),\n        ('xp', r'\\[/p\\]'),\n        ('sp', r'\\[sp\\]'),\n        ('index', r'\\[index\\]'),\n        ('index_parent', r'\\[index_parent\\]'),\n        ('name', r'\\[name\\]'),\n        ('instname', r'\\[instname\\]'),\n    ]\n    tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_spec)\n    pos = 0\n    text_segs = []\n    is_first_bullet = []\n    list_end_tag = []\n    for m in re.finditer(tok_regex, text, re.DOTALL):\n        start = m.start()\n        end = m.end()\n\n        # Emit prior text\n        if start != pos:\n            text_segs.append(text[pos:start])\n        pos = end\n\n        if m.lastgroup == 'b':\n            text_segs.append(\"<b>\")\n        elif m.lastgroup == 'xb':\n            text_segs.append(\"</b>\")\n        elif m.lastgroup == 'i':\n            text_segs.append(\"<i>\")\n        elif m.lastgroup == 'xi':\n            text_segs.append(\"</i>\")\n        elif m.lastgroup == 'u':\n            text_segs.append(\"<u>\")\n        elif m.lastgroup == 'xu':\n            text_segs.append(\"</u>\")\n        elif m.lastgroup == 'color':\n            m2 = re.match(r'\\[color=([^\\]]+)\\]', m.group(0))\n            text_segs.append('<span style=\"color:%s\">' % m2.group(1))\n        elif m.lastgroup == 'xcolor':\n            text_segs.append('</span>')\n        elif m.lastgroup == 'size':\n            m2 = re.match(r'\\[size=([^\\]]+)\\]', m.group(0))\n            text_segs.append('<span style=\"font-size:%s\">' % m2.group(1))\n        elif m.lastgroup == 'xsize':\n            text_segs.append('</span>')\n        elif m.lastgroup == 'plainurl':\n            m2 = re.match(r'\\[url\\](.*?)\\[/url\\]', m.group(0), re.DOTALL)\n            text_segs.append('<a href=\"%s\">%s</a>' % (m2.group(1).strip(), m2.group(1).strip()))\n        elif m.lastgroup == 'url':\n            m2 = re.match(r'\\[url=([^\\]]+)\\]', m.group(0))\n            text_segs.append('<a href=\"%s\">' % m2.group(1).strip())\n        elif m.lastgroup == 'xurl':\n            text_segs.append('</a>')\n        elif m.lastgroup == 'email':\n            m2 = re.match(r'\\[email\\](.*?)\\[/email\\]', m.group(0), re.DOTALL)\n            text_segs.append('<a href=\"mailto:%s\">%s</a>' % (m2.group(1).strip(), m2.group(1).strip()))\n        elif m.lastgroup == 'img':\n            m2 = re.match(r'\\[img\\](.*?)\\[/img\\]', m.group(0), re.DOTALL)\n            text_segs.append('<img src=\"%s\">' % m2.group(1))\n        elif m.lastgroup == 'code':\n            m2 = re.match(r'\\[code\\](.*?)\\s*\\[/code\\]', m.group(0), re.DOTALL)\n            text_segs.append('<code>%s</code>' % m2.group(1))\n\n        elif m.lastgroup == 'list':\n            # List start tag\n            m2 = re.match(r'\\[list(?:=([^\\]]+))?\\]', m.group(0))\n            ltype = m2.group(1)\n            if ltype is None:\n                text_segs.append('<ul>')\n                is_first_bullet.append(True)\n                list_end_tag.append('</ul>')\n            elif ltype.strip() in (\"1\", \"A\", \"a\", \"I\", \"i\"):\n                text_segs.append('<ol type=\"%s\">' % ltype.strip())\n                is_first_bullet.append(True)\n                list_end_tag.append('</ol>')\n            else:\n                # Bad type. re-emit erronous list tag\n                text_segs.append(m.group(0))\n\n        elif m.lastgroup == 'bullet':\n            if len(is_first_bullet) == 0: #pylint: disable=len-as-condition\n                # Not inside a list tag. Re-emit erronous tag\n                text_segs.append(\"\\\\[\\\\*\\\\]\")\n            else:\n                if not is_first_bullet[-1]:\n                    text_segs.append(\"</li>\")\n                is_first_bullet[-1] = False\n                text_segs.append(\"<li>\")\n\n        elif m.lastgroup == 'xlist':\n            if len(list_end_tag) == 0: #pylint: disable=len-as-condition\n                # Not inside a list tag. Re-emit erronous tag\n                text_segs.append(m.group(0))\n            else:\n                if not is_first_bullet[-1]:\n                    text_segs.append(\"</li>\")\n                text_segs.append(list_end_tag[-1])\n                is_first_bullet.pop()\n                list_end_tag.pop()\n\n        elif m.lastgroup == 'quote':\n            text_segs.append('\"')\n        elif m.lastgroup == 'xquote':\n            text_segs.append('\"')\n        elif m.lastgroup == 'br':\n            text_segs.append(\"<br>\")\n        elif m.lastgroup == 'lb':\n            text_segs.append(\"\\\\[\")\n        elif m.lastgroup == 'rb':\n            text_segs.append(\"\\\\]\")\n        elif m.lastgroup == 'p':\n            text_segs.append(\"\\n\\n<p>\")\n        elif m.lastgroup == 'xp':\n            text_segs.append(\"</p>\")\n        elif m.lastgroup == 'sp':\n            text_segs.append(\"&nbsp\")\n        elif m.lastgroup == 'index':\n            if (node is not None) and node.inst.is_array:\n                subscripts = []\n                if node.current_idx is None:\n                    # Index is not known. Use range\n                    for dim in node.inst.array_dimensions:\n                        subscripts.append(\"[0:%d]\" % dim)\n                else:\n                    # Index is known\n                    for idx in node.current_idx:\n                        subscripts.append(\"[%d]\" % idx)\n                range_str = \"\".join(subscripts)\n                text_segs.append(\"<span class='rdlfc-index'>%s</span>\" % range_str)\n            else:\n                text_segs.append(m.group(0))\n        elif m.lastgroup == 'index_parent':\n            if (node is not None) and (node.parent is not None) and node.parent.inst.is_array:\n                subscripts = []\n                if node.parent.current_idx is None:\n                    # Index is not known. Use range\n                    for dim in node.parent.inst.array_dimensions:\n                        subscripts.append(\"[0:%d]\" % dim)\n                else:\n                    # Index is known\n                    for idx in node.parent.current_idx:\n                        subscripts.append(\"[%d]\" % idx)\n                range_str = \"\".join(subscripts)\n                text_segs.append(\"<span class='rdlfc-index_parent'>%s</span>\" % range_str)\n            else:\n                text_segs.append(m.group(0))\n        elif m.lastgroup == 'name':\n            if node is not None:\n                text_segs.append(node.get_property(\"name\"))\n            else:\n                text_segs.append(m.group(0))\n        elif m.lastgroup == 'instname':\n            if node is not None:\n                text_segs.append(node.inst.inst_name)\n            else:\n                text_segs.append(m.group(0))\n\n    # Emit trailing text\n    text_segs.append(text[pos:])\n\n    text_out = \"\".join(text_segs)\n\n    #---------------------------------------------------------------------------\n    # Pass through markdown processor\n    #---------------------------------------------------------------------------\n    text_out = md.reset().convert(text_out)\n\n    return text_out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef preprocess(self):\n        tokens = self.tokenize()\n        pl_segments, has_perl_tags = self.get_perl_segments(tokens)\n\n        # Generate flattened output\n        str_parts = []\n        smap = segment_map.SegmentMap()\n        offset = 0\n\n        if has_perl_tags:\n            # Needs to be processed through perl interpreter\n            emit_list = self.run_perl_miniscript(pl_segments)\n\n            for entry in emit_list:\n                if entry['type'] == \"ref\":\n                    pl_seg = pl_segments[entry['ref']]\n                    emit_text = pl_seg.get_text()\n\n                    map_seg = segment_map.UnalteredSegment(\n                        offset, offset + len(emit_text) - 1,\n                        pl_seg.start, pl_seg.end, pl_seg.file_pp.path,\n                        pl_seg.file_pp.incl_ref\n                    )\n                    offset += len(emit_text)\n                    smap.segments.append(map_seg)\n                    str_parts.append(emit_text)\n\n                elif entry['type'] == \"text\":\n                    pl_seg = pl_segments[entry['ref']]\n                    emit_text = entry['text']\n\n                    map_seg = segment_map.MacroSegment(\n                        offset, offset + len(emit_text) - 1,\n                        pl_seg.start, pl_seg.end, pl_seg.file_pp.path,\n                        pl_seg.file_pp.incl_ref\n                    )\n                    offset += len(emit_text)\n                    smap.segments.append(map_seg)\n                    str_parts.append(emit_text)\n        else:\n            # OK to bypass perl interpreter\n            for pl_seg in pl_segments:\n                emit_text = pl_seg.get_text()\n                map_seg = segment_map.UnalteredSegment(\n                    offset, offset + len(emit_text) - 1,\n                    pl_seg.start, pl_seg.end, pl_seg.file_pp.path,\n                    pl_seg.file_pp.incl_ref\n                )\n                offset += len(emit_text)\n                smap.segments.append(map_seg)\n                str_parts.append(emit_text)\n\n        #segment_map.print_segment_debug(\"\".join(str_parts), smap)\n\n        return (\"\".join(str_parts), smap)", "response": "Run preprocessor on a top - level file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tokenize(self):\n        tokens = []\n        token_spec = [\n            ('mlc', r'/\\*.*?\\*/'),\n            ('slc', r'//[^\\r\\n]*?\\r?\\n'),\n            ('perl', r'<%.*?%>'),\n            ('incl', r'`include'),\n        ]\n        tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_spec)\n        for m in re.finditer(tok_regex, self.text, re.DOTALL):\n            if m.lastgroup in (\"incl\", \"perl\"):\n                tokens.append((m.lastgroup, m.start(0), m.end(0)-1))\n        return tokens", "response": "Tokenize the input text returning a list of tuples each containing the type start and end of the token."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_include(self, start):\n        # Seek back to start of line\n        i = start\n        while i:\n            if self.text[i] == '\\n':\n                i += 1\n                break\n            i -= 1\n        line_start = i\n\n        # check that there is no unexpected text before the include\n        if not (self.text[line_start:start] == \"\" or self.text[line_start:start].isspace()):\n            self.env.msg.fatal(\n                \"Unexpected text before include\",\n                messages.SourceRef(line_start, start-1, filename=self.path)\n            )\n\n        # Capture include contents\n        inc_regex = re.compile(r'`include\\s+(\"([^\\r\\n]+)\"|<([^\\r\\n]+)>)')\n        m_inc = inc_regex.match(self.text, start)\n        if m_inc is None:\n            self.env.msg.fatal(\n                \"Invalid usage of include directive\",\n                messages.SourceRef(start, start+7, filename=self.path)\n            )\n        incl_path_raw = m_inc.group(2) or m_inc.group(3)\n        end = m_inc.end(0)-1\n        path_start = m_inc.start(1)\n        #[^\\r\\n]*?\\r?\\n\n        # Check that only comments follow\n        tail_regex = re.compile(r'(?:[ \\t]*/\\*[^\\r\\n]*?\\*/)*[ \\t]*(?://[^\\r\\n]*?|/\\*[^\\r\\n]*?)?\\r?\\n')\n        if not tail_regex.match(self.text, end+1):\n            tail_capture_regex = re.compile(r'[^\\r\\n]*?\\r?\\n')\n            m = tail_capture_regex.match(self.text, end+1)\n            self.env.msg.fatal(\n                \"Unexpected text after include\",\n                messages.SourceRef(end+1, m.end(0)-1, filename=self.path)\n            )\n\n        # Resolve include path.\n        if os.path.isabs(incl_path_raw):\n            incl_path = incl_path_raw\n        else:\n            # Search include paths first.\n            for search_path in self.search_paths:\n                incl_path = os.path.join(search_path, incl_path_raw)\n                if os.path.isfile(incl_path):\n                    # found match!\n                    break\n            else:\n                # Otherwise, assume it is relative to the current file\n                incl_path = os.path.join(os.path.dirname(self.path), incl_path_raw)\n        if not os.path.isfile(incl_path):\n            self.env.msg.fatal(\n                \"Could not find '%s' in include search paths\" % incl_path_raw,\n                messages.SourceRef(path_start, end, filename=self.path)\n            )\n\n        # Check if path has already been referenced before\n        incl_ref = self.incl_ref\n        while incl_ref:\n            if os.path.samefile(incl_path, incl_ref.path):\n                self.env.msg.fatal(\n                    \"Include of '%s' results in a circular reference\" % incl_path_raw,\n                    messages.SourceRef(path_start, end, filename=self.path)\n                )\n            incl_ref = incl_ref.parent\n\n        return(end, incl_path)", "response": "Extract include from text based on start position of token\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a list of perl preprocessor segments and their corresponding has_perl_tags flag.", "response": "def get_perl_segments(self, tokens):\n        \"\"\"\n        Build a list of perl preprocessor segments:\n            PPPUnalteredSegment\n            PPPPerlSegment\n            PPPMacroSegment\n        returns:\n            (pl_segments, has_perl_tags)\n        \"\"\"\n        pl_segments = []\n        has_perl_tags = False\n        pos = 0\n\n        for typ, start, end in tokens:\n\n            # Capture any leading text\n            if start != pos:\n                pl_seg = PPPUnalteredSegment(self, pos, start-1)\n                pl_segments.append(pl_seg)\n\n            if typ == \"incl\":\n                # Got an `include ...\n\n                # Extract the path and actual end position\n                end, incl_path = self.parse_include(start)\n\n                # create a reference that captures the location where the include occurred\n                incl_ref = segment_map.IncludeRef(start, end, self.path, self.incl_ref)\n\n                # Recurse and extract perl segments from included file\n                incl_file_pp = FilePreprocessor(self.env, incl_path, self.search_paths, incl_ref)\n                incl_tokens = incl_file_pp.tokenize()\n                incl_pl_segments, incl_has_pl_tags = incl_file_pp.get_perl_segments(incl_tokens)\n\n                pl_segments.extend(incl_pl_segments)\n                has_perl_tags = has_perl_tags or incl_has_pl_tags\n\n            else:\n                # Got a Perl tag <% ... %>\n                if self.text[start+2] == \"=\":\n                    pl_seg = PPPMacroSegment(self, start, end)\n                else:\n                    pl_seg = PPPPerlSegment(self, start, end)\n                pl_segments.append(pl_seg)\n                has_perl_tags = True\n\n            pos = end+1\n\n        # Capture any trailing text\n        text_len = len(self.text)\n        if text_len > pos:\n            pl_seg = PPPUnalteredSegment(self, pos, text_len-1)\n            pl_segments.append(pl_seg)\n\n        return (pl_segments, has_perl_tags)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate and runs a minimal perl miniscript that derives the text that will be emitted from the preprocessor and returns the list of the text that will be emitted from the preprocessor.", "response": "def run_perl_miniscript(self, segments):\n        \"\"\"\n        Generates and runs a perl miniscript that derives the text that will be\n        emitted from the preprocessor\n\n        returns the resulting emit list\n        \"\"\"\n\n        # Check if perl is installed\n        if shutil.which(\"perl\") is None:\n            self.env.msg.fatal(\n                \"Input contains Perl preprocessor tags, but an installation of Perl could not be found\"\n            )\n\n        # Generate minimal perl script that captures activities described in the source file\n        lines = []\n        for i,pp_seg in enumerate(segments):\n            if isinstance(pp_seg, PPPUnalteredSegment):\n                # Text outside preprocessor tags that should remain unaltered\n                # Insert command to emit reference to this text segment\n                lines.append(\"rdlppp_utils::emit_ref(%d);\" % i)\n\n            elif isinstance(pp_seg, PPPPerlSegment):\n                # Perl code snippet. Insert directly\n                lines.append(pp_seg.get_text())\n\n            elif isinstance(pp_seg, PPPMacroSegment):\n                # Preprocessor macro print tag\n                # Insert command to store resulting text\n                var = pp_seg.get_text()\n\n                # Check for any illegal characters\n                if re.match(r'[\\s;]', var):\n                    self.env.msg.fatal(\n                        \"Invalid text found in Perl macro expansion\",\n                        messages.SourceRef(pp_seg.start, pp_seg.end, filename=self.path)\n                    )\n\n                lines.append(\"rdlppp_utils::emit_text(%d, %s);\" % (i, var))\n        miniscript = '\\n'.join(lines)\n\n        # Run miniscript\n        result = subprocess_run(\n            [\"perl\", os.path.join(os.path.dirname(__file__), \"ppp_runner.pl\")],\n            input=miniscript.encode(\"utf-8\"),\n            stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n            timeout=5\n        )\n        if result.returncode:\n            self.env.msg.fatal(\n                \"Encountered a Perl syntax error while executing embedded Perl preprocessor commands:\\n\"\n                + result.stderr.decode(\"utf-8\"),\n                # TODO: Fix useless context somehow\n                messages.SourceRef(filename=self.path)\n            )\n\n        # miniscript returns the emit list in JSON format. Convert it\n        emit_list = json.loads(result.stdout.decode('utf-8'))\n\n        return emit_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subprocess_run(*popenargs, input=None, timeout=None, check=False, **kwargs):\n    #pylint: disable=redefined-builtin\n    if input is not None:\n        if 'stdin' in kwargs:\n            raise ValueError('stdin and input arguments may not both be used.')\n        kwargs['stdin'] = subprocess.PIPE\n\n    with subprocess.Popen(*popenargs, **kwargs) as process:\n        try:\n            stdout, stderr = process.communicate(input, timeout=timeout)\n        except subprocess.TimeoutExpired:\n            process.kill()\n            stdout, stderr = process.communicate()\n            raise subprocess.TimeoutExpired(process.args, timeout, output=stdout,\n                                 stderr=stderr)\n        except:\n            process.kill()\n            process.wait()\n            raise\n        retcode = process.poll()\n        if check and retcode:\n            raise subprocess.CalledProcessError(retcode, process.args,\n                                     output=stdout, stderr=stderr)\n    return CompletedProcess(process.args, retcode, stdout, stderr)", "response": "Runs a subprocess with arguments and returns a CompletedProcess instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_properties(self, comp_type):\n        # Flatten out all the default assignments that apply to the current scope\n        # This does not include any default assignments made within the current\n        # scope, so exclude those.\n        props = {}\n        for scope in self.default_property_ns_stack[:-1]:\n            props.update(scope)\n\n        # filter out properties that are not relevant\n        prop_names = list(props.keys())\n        for prop_name in prop_names:\n            rule = self.env.property_rules.lookup_property(prop_name)\n            if rule is None:\n                self.msg.fatal(\n                    \"Unrecognized property '%s'\" % prop_name,\n                    props[prop_name][0]\n                )\n            if comp_type not in rule.bindable_to:\n                del props[prop_name]\n\n        return props", "response": "Returns a flattened dictionary of all default properties that apply to the current component type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_scope_path(self, scope_separator=\"::\"):\n        if self.parent_scope is None:\n            return \"\"\n        elif isinstance(self.parent_scope, Root):\n            return \"\"\n        else:\n            parent_path = self.parent_scope.get_scope_path(scope_separator)\n            if parent_path:\n                return(\n                    parent_path\n                    + scope_separator\n                    + self.parent_scope.type_name\n                )\n            else:\n                return self.parent_scope.type_name", "response": "Generate a string that represents this component s declaration namespace namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of elements in the array.", "response": "def n_elements(self):\n        \"\"\"\n        Total number of array elements.\n        If array is multidimensional, array is flattened.\n        Returns 1 if not an array.\n        \"\"\"\n        if self.is_array:\n            return functools.reduce(operator.mul, self.array_dimensions)\n        else:\n            return 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitiates the walker to traverse the current ``node`` and its children. Calls the corresponding callback for each of the ``listeners`` provided in the order that they are listed. Parameters ---------- node : :class:`~systemrdl.node.Node` Node to start traversing. Listener traversal includes this node. listeners : list List of :class:`~RDLListener` that are invoked during node traversal. Listener callbacks are executed in the same order as provided by this parameter.", "response": "def walk(self, node, *listeners:RDLListener):\n        \"\"\"\n        Initiates the walker to traverse the current ``node`` and its children.\n        Calls the corresponding callback for each of the ``listeners`` provided in\n        the order that they are listed.\n\n        Parameters\n        ----------\n        node : :class:`~systemrdl.node.Node`\n            Node to start traversing.\n            Listener traversal includes this node.\n\n        listeners : list\n            List of :class:`~RDLListener` that are invoked during\n            node traversal.\n            Listener callbacks are executed in the same order as provided by this\n            parameter.\n        \"\"\"\n\n\n        for listener in listeners:\n            self.do_enter(node, listener)\n        for child in node.children(unroll=self.unroll, skip_not_present=self.skip_not_present):\n            self.walk(child, *listeners)\n        for listener in listeners:\n            self.do_exit(node, listener)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the function defined by the function_name.", "response": "def get_function(fn_name):\n    \"\"\"Retrieve the function defined by the function_name.\n\n    Arguments:\n      fn_name: specification of the type module:function_name.\n\n    \"\"\"\n    module_name, callable_name = fn_name.split(':')\n    current = globals()\n    if not callable_name:\n        callable_name = module_name\n    else:\n        import importlib\n        try:\n            module = importlib.import_module(module_name)\n        except ImportError:\n            log.error(\"failed to import %s\", module_name)\n            raise\n        current = module\n    for level in callable_name.split('.'):\n        current = getattr(current, level)\n    code = current.__code__\n    if code.co_argcount != 2:\n        raise ValueError('function should take 2 arguments: lines, file_name')\n    return current"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses command line arguments.", "response": "def parse_command_line(argv):\n    \"\"\"Parse command line argument. See -h option.\n\n    Arguments:\n      argv: arguments on the command line must include caller file name.\n\n    \"\"\"\n    import textwrap\n\n    example = textwrap.dedent(\"\"\"\n    Examples:\n    # Simple string substitution (-e). Will show a diff. No changes applied.\n    {0} -e \"re.sub('failIf', 'assertFalse', line)\" *.py\n\n    # File level modifications (-f). Overwrites the files in place (-w).\n    {0} -w -f fixer:fixit *.py\n\n    # Will change all test*.py in subdirectories of tests.\n    {0} -e \"re.sub('failIf', 'assertFalse', line)\" -s tests test*.py\n    \"\"\").format(os.path.basename(argv[0]))\n    formatter_class = argparse.RawDescriptionHelpFormatter\n    parser = argparse.ArgumentParser(description=\"Python mass editor\",\n                                     epilog=example,\n                                     formatter_class=formatter_class)\n    parser.add_argument(\"-V\", \"--version\", action=\"version\",\n                        version=\"%(prog)s {}\".format(__version__))\n    parser.add_argument(\"-w\", \"--write\", dest=\"dry_run\",\n                        action=\"store_false\", default=True,\n                        help=\"modify target file(s) in place. \"\n                        \"Shows diff otherwise.\")\n    parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose_count\",\n                        action=\"count\", default=0,\n                        help=\"increases log verbosity (can be specified \"\n                        \"multiple times)\")\n    parser.add_argument(\"-e\", \"--expression\", dest=\"expressions\", nargs=1,\n                        help=\"Python expressions applied to target files. \"\n                        \"Use the line variable to reference the current line.\")\n    parser.add_argument(\"-f\", \"--function\", dest=\"functions\", nargs=1,\n                        help=\"Python function to apply to target file. \"\n                        \"Takes file content as input and yield lines. \"\n                        \"Specify function as [module]:?<function name>.\")\n    parser.add_argument(\"-x\", \"--executable\", dest=\"executables\", nargs=1,\n                        help=\"Python executable to apply to target file.\")\n    parser.add_argument(\"-s\", \"--start\", dest=\"start_dirs\",\n                        help=\"Directory(ies) from which to look for targets.\")\n    parser.add_argument(\"-m\", \"--max-depth-level\", type=int, dest=\"max_depth\",\n                        help=\"Maximum depth when walking subdirectories.\")\n    parser.add_argument(\"-o\", \"--output\", metavar=\"FILE\",\n                        type=argparse.FileType(\"w\"), default=sys.stdout,\n                        help=\"redirect output to a file\")\n    parser.add_argument(\"-g\", \"--generate\", metavar=\"FILE\", type=str,\n                        help=\"generate input file suitable for -f option\")\n    parser.add_argument(\"--encoding\", dest=\"encoding\",\n                        help=\"Encoding of input and output files\")\n    parser.add_argument(\"--newline\", dest=\"newline\",\n                        help=\"Newline character for output files\")\n    parser.add_argument(\"patterns\", metavar=\"pattern\",\n                        nargs=\"*\",  # argparse.REMAINDER,\n                        help=\"shell-like file name patterns to process.\")\n    arguments = parser.parse_args(argv[1:])\n\n    if not (arguments.expressions or\n            arguments.functions or\n            arguments.generate or\n            arguments.executables):\n        parser.error(\n            '--expression, --function, --generate or --executable missing')\n\n    # Sets log level to WARN going more verbose for each new -V.\n    log.setLevel(max(3 - arguments.verbose_count, 0) * 10)\n    return arguments"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve files that match any of the patterns.", "response": "def get_paths(patterns, start_dirs=None, max_depth=1):\n    \"\"\"Retrieve files that match any of the patterns.\"\"\"\n    # Shortcut: if there is only one pattern, make sure we process just that.\n    if len(patterns) == 1 and not start_dirs:\n        pattern = patterns[0]\n        directory = os.path.dirname(pattern)\n        if directory:\n            patterns = [os.path.basename(pattern)]\n            start_dirs = directory\n            max_depth = 1\n\n    if not start_dirs or start_dirs == '.':\n        start_dirs = os.getcwd()\n    for start_dir in start_dirs.split(','):\n        for root, dirs, files in os.walk(start_dir):  # pylint: disable=W0612\n            if max_depth is not None:\n                relpath = os.path.relpath(root, start=start_dir)\n                depth = len(relpath.split(os.sep))\n                if depth > max_depth:\n                    continue\n            names = []\n            for pattern in patterns:\n                names += fnmatch.filter(files, pattern)\n            for name in names:\n                path = os.path.join(root, name)\n                yield path"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a list of files and returns a list of files that match the input.", "response": "def edit_files(patterns, expressions=None,\n               functions=None, executables=None,\n               start_dirs=None, max_depth=1, dry_run=True,\n               output=sys.stdout, encoding=None, newline=None):\n    \"\"\"Process patterns with MassEdit.\n\n    Arguments:\n      patterns: file pattern to identify the files to be processed.\n      expressions: single python expression to be applied line by line.\n      functions: functions to process files contents.\n      executables: os executables to execute on the argument files.\n\n    Keyword arguments:\n      max_depth: maximum recursion level when looking for file matches.\n      start_dirs: workspace(ies) where to start the file search.\n      dry_run: only display differences if True. Save modified file otherwise.\n      output: handle where the output should be redirected.\n\n    Return:\n      list of files processed.\n\n    \"\"\"\n    if not is_list(patterns):\n        raise TypeError(\"patterns should be a list\")\n    if expressions and not is_list(expressions):\n        raise TypeError(\"expressions should be a list of exec expressions\")\n    if functions and not is_list(functions):\n        raise TypeError(\"functions should be a list of functions\")\n    if executables and not is_list(executables):\n        raise TypeError(\"executables should be a list of program names\")\n\n    editor = MassEdit(dry_run=dry_run, encoding=encoding, newline=newline)\n    if expressions:\n        editor.set_code_exprs(expressions)\n    if functions:\n        editor.set_functions(functions)\n    if executables:\n        editor.set_executables(executables)\n\n    processed_paths = []\n    for path in get_paths(patterns, start_dirs=start_dirs,\n                          max_depth=max_depth):\n        try:\n            diffs = list(editor.edit_file(path))\n            if dry_run:\n                # At this point, encoding is the input encoding.\n                diff = \"\".join(diffs)\n                if not diff:\n                    continue\n                # The encoding of the target output may not match the input\n                # encoding. If it's defined, we round trip the diff text\n                # to bytes and back to silence any conversion errors.\n                encoding = output.encoding\n                if encoding:\n                    bytes_diff = diff.encode(encoding=encoding, errors='ignore')\n                    diff = bytes_diff.decode(encoding=output.encoding)\n                output.write(diff)\n        except UnicodeDecodeError as err:\n            log.error(\"failed to process %s: %s\", path, err)\n            continue\n        processed_paths.append(os.path.abspath(path))\n    return processed_paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef command_line(argv):\n    arguments = parse_command_line(argv)\n    if arguments.generate:\n        generate_fixer_file(arguments.generate)\n    paths = edit_files(arguments.patterns,\n                       expressions=arguments.expressions,\n                       functions=arguments.functions,\n                       executables=arguments.executables,\n                       start_dirs=arguments.start_dirs,\n                       max_depth=arguments.max_depth,\n                       dry_run=arguments.dry_run,\n                       output=arguments.output,\n                       encoding=arguments.encoding,\n                       newline=arguments.newline)\n    # If the output is not sys.stdout, we need to close it because\n    # argparse.FileType does not do it for us.\n    is_sys = arguments.output in [sys.stdout, sys.stderr]\n    if not is_sys and isinstance(arguments.output, io.IOBase):\n        arguments.output.close()\n    return paths", "response": "Instantiate an editor and process arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport module that are needed for the code expr to compile.", "response": "def import_module(module):  # pylint: disable=R0201\n        \"\"\"Import module that are needed for the code expr to compile.\n\n        Argument:\n          module (str or list): module(s) to import.\n\n        \"\"\"\n        if isinstance(module, list):\n            all_modules = module\n        else:\n            all_modules = [module]\n        for mod in all_modules:\n            globals()[mod] = __import__(mod.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __edit_line(line, code, code_obj):  # pylint: disable=R0201\n        try:\n            # pylint: disable=eval-used\n            result = eval(code_obj, globals(), locals())\n        except TypeError as ex:\n            log.error(\"failed to execute %s: %s\", code, ex)\n            raise\n        if result is None:\n            log.error(\"cannot process line '%s' with %s\", line, code)\n            raise RuntimeError('failed to process line')\n        elif isinstance(result, list) or isinstance(result, tuple):\n            line = unicode(' '.join([unicode(res_element)\n                                     for res_element in result]))\n        else:\n            line = unicode(result)\n        return line", "response": "Edit a line with one code object built in the ctor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nedits a single line using the code expression.", "response": "def edit_line(self, line):\n        \"\"\"Edit a single line using the code expression.\"\"\"\n        for code, code_obj in self.code_objs.items():\n            line = self.__edit_line(line, code, code_obj)\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a file contents.", "response": "def edit_content(self, original_lines, file_name):\n        \"\"\"Processes a file contents.\n\n        First processes the contents line by line applying the registered\n        expressions, then process the resulting contents using the\n        registered functions.\n\n        Arguments:\n          original_lines (list of str): file content.\n          file_name (str): name of the file.\n\n        \"\"\"\n        lines = [self.edit_line(line) for line in original_lines]\n        for function in self._functions:\n            try:\n                lines = list(function(lines, file_name))\n            except UnicodeDecodeError as err:\n                log.error('failed to process %s: %s', file_name, err)\n                return lines\n            except Exception as err:\n                log.error(\"failed to process %s with code %s: %s\",\n                          file_name, function, err)\n                raise  # Let the exception be handled at a higher level.\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nedit a file in place returns a list of modifications.", "response": "def edit_file(self, file_name):\n        \"\"\"Edit file in place, returns a list of modifications (unified diff).\n\n        Arguments:\n          file_name (str, unicode): The name of the file.\n\n        \"\"\"\n        with io.open(file_name, \"r\", encoding=self.encoding) as from_file:\n            try:\n                from_lines = from_file.readlines()\n            except UnicodeDecodeError as err:\n                log.error(\"encoding error (see --encoding): %s\", err)\n                raise\n\n        if self._executables:\n            nb_execs = len(self._executables)\n            if nb_execs > 1:\n                log.warn(\"found %d executables. Will use first one\", nb_execs)\n            exec_list = self._executables[0].split()\n            exec_list.append(file_name)\n            try:\n                log.info(\"running %s...\", \" \".join(exec_list))\n                output = subprocess.check_output(exec_list,\n                                                 universal_newlines=True)\n            except Exception as err:\n                log.error(\"failed to execute %s: %s\", \" \".join(exec_list), err)\n                raise  # Let the exception be handled at a higher level.\n            to_lines = output.split(unicode(\"\\n\"))\n        else:\n            to_lines = from_lines\n\n        # unified_diff wants structure of known length. Convert to a list.\n        to_lines = list(self.edit_content(to_lines, file_name))\n        diffs = difflib.unified_diff(from_lines, to_lines,\n                                     fromfile=file_name, tofile='<new>')\n        if not self.dry_run:\n            bak_file_name = file_name + \".bak\"\n            if os.path.exists(bak_file_name):\n                msg = \"{} already exists\".format(bak_file_name)\n                if sys.version_info < (3, 3):\n                    raise OSError(msg)\n                else:\n                    # noinspection PyCompatibility\n                    # pylint: disable=undefined-variable\n                    raise FileExistsError(msg)\n            try:\n                os.rename(file_name, bak_file_name)\n                with io.open(file_name, 'w', encoding=self.encoding, newline=self.newline) as new:\n                    new.writelines(to_lines)\n                # Keeps mode of original file.\n                shutil.copymode(bak_file_name, file_name)\n            except Exception as err:\n                log.error(\"failed to write output to %s: %s\", file_name, err)\n                # Try to recover...\n                try:\n                    os.rename(bak_file_name, file_name)\n                except OSError as err:\n                    log.error(\"failed to restore %s from %s: %s\",\n                              file_name, bak_file_name, err)\n                raise\n            try:\n                os.unlink(bak_file_name)\n            except OSError as err:\n                log.warning(\"failed to remove backup %s: %s\",\n                            bak_file_name, err)\n        return list(diffs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompiling argument and adds it to the list of code objects.", "response": "def append_code_expr(self, code):\n        \"\"\"Compile argument and adds it to the list of code objects.\"\"\"\n        # expects a string.\n        if isinstance(code, str) and not isinstance(code, unicode):\n            code = unicode(code)\n        if not isinstance(code, unicode):\n            raise TypeError(\"string expected\")\n        log.debug(\"compiling code %s...\", code)\n        try:\n            code_obj = compile(code, '<string>', 'eval')\n            self.code_objs[code] = code_obj\n        except SyntaxError as syntax_err:\n            log.error(\"cannot compile %s: %s\", code, syntax_err)\n            raise\n        log.debug(\"compiled code %s\", code)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending the function to the list of functions to be called.", "response": "def append_function(self, function):\n        \"\"\"Append the function to the list of functions to be called.\n\n        If the function is already a callable, use it. If it's a type str\n        try to interpret it as [module]:?<callable>, load the module\n        if there is one and retrieve the callable.\n\n        Argument:\n          function (str or callable): function to call on input.\n\n        \"\"\"\n        if not hasattr(function, '__call__'):\n            function = get_function(function)\n            if not hasattr(function, '__call__'):\n                raise ValueError(\"function is expected to be callable\")\n        self._functions.append(function)\n        log.debug(\"registered %s\", function.__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append_executable(self, executable):\n        if isinstance(executable, str) and not isinstance(executable, unicode):\n            executable = unicode(executable)\n        if not isinstance(executable, unicode):\n            raise TypeError(\"expected executable name as str, not {}\".\n                            format(executable.__class__.__name__))\n        self._executables.append(executable)", "response": "Append san executable os command to the list to be called."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_functions(self, functions):\n        for func in functions:\n            try:\n                self.append_function(func)\n            except (ValueError, AttributeError) as ex:\n                log.error(\"'%s' is not a callable function: %s\", func, ex)\n                raise", "response": "Check functions passed as argument and set them to be used."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting data to MNE using FIFF format.", "response": "def write_mnefiff(data, filename):\n    \"\"\"Export data to MNE using FIFF format.\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data with only one trial\n    filename : path to file\n        file to export to (include '.mat')\n\n    Notes\n    -----\n    It cannot store data larger than 2 GB.\n    The data is assumed to have only EEG electrodes.\n    It overwrites a file if it exists.\n    \"\"\"\n    from mne import create_info, set_log_level\n    from mne.io import RawArray\n\n    set_log_level(WARNING)\n\n    TRIAL = 0\n    info = create_info(list(data.axis['chan'][TRIAL]), data.s_freq, ['eeg', ] *\n                       data.number_of('chan')[TRIAL])\n\n    UNITS = 1e-6  # mne wants data in uV\n    fiff = RawArray(data.data[0] * UNITS, info)\n\n    if data.attr['chan']:\n        fiff.set_channel_positions(data.attr['chan'].return_xyz(),\n                                   data.attr['chan'].return_label())\n\n    fiff.save(filename, overwrite=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_Lacourse2018(dat_orig, s_freq, time, opts):\n    # Downsample z-score parameters, tolerance\n    step = opts.windowing['step']\n    if step:\n        ds_freq = int(1 / step) # downsampled sampling frequency\n        opts.zscore['dur'] *= opts.windowing['step']\n        opts.tolerance *= opts.windowing['step']\n        if opts.zscore['step']:\n            opts.zscore['step'] *= opts.windowing['step']\n    else:\n        ds_freq = s_freq\n\n    \n    # Absolute sigma power\n    dat_sigma = transform_signal(dat_orig, s_freq, 'double_sosbutter', \n                                 opts.det_butter)\n    dat_det = transform_signal(dat_sigma, s_freq, 'moving_ms', opts.moving_ms)\n    dat_det[dat_det <= 0] = 0.000000001\n    abs_sig_pow = log10(dat_det)\n    # Option to adapt the absolute threshold, for low-amplitude recordings\n    if opts.abs_pow_thresh < 0:\n        opts.abs_pow_thresh = (mean(abs_sig_pow) - \n                               opts.abs_pow_thresh * std(abs_sig_pow))\n    abs_sig_pow = transform_signal(abs_sig_pow, ds_freq, 'smooth', opts.smooth)\n    \n    # Relative sigma power\n    dat_det = transform_signal(dat_orig, s_freq, 'moving_power_ratio', \n                               opts.moving_power_ratio)\n    dat_det[dat_det <= 0] = 0.000000001\n    dat_det = log10(dat_det)\n    rel_sig_pow = transform_signal(dat_det, s_freq, 'moving_zscore', \n                                   opts.zscore)\n    rel_sig_pow = transform_signal(rel_sig_pow, ds_freq, 'smooth', opts.smooth)\n    \n    # Sigma covariance\n    dat_broad = transform_signal(dat_orig, s_freq, 'double_sosbutter', \n                                 opts.det_butter2)\n    dat_covar = transform_signal(dat_sigma, s_freq, 'moving_covar', \n                                 opts.moving_covar, dat2=dat_broad)\n    dat_det = dat_covar.copy()\n    dat_det[dat_det < 0] = 0 # negative covariances are discarded\n    dat_det = log10(dat_det + 1) # add 1 to avoid -inf\n    sigma_covar = transform_signal(dat_det, s_freq, 'moving_zscore', \n                                   opts.zscore)\n    sigma_covar = transform_signal(sigma_covar, ds_freq, 'smooth', opts.smooth)\n    \n    # Sigma correlation\n    dat_sd_broad = transform_signal(dat_broad, s_freq, 'moving_sd', \n                                    opts.moving_sd)\n    dat_sd_sigma = transform_signal(dat_sigma, s_freq, 'moving_sd', \n                                    opts.moving_sd)\n    dat_sd_broad[dat_sd_broad == 0] = 0.000000001\n    dat_sd_sigma[dat_sd_sigma == 0] = 0.000000001\n    sigma_corr = dat_covar / (dat_sd_broad * dat_sd_sigma)\n    sigma_corr = transform_signal(sigma_corr, ds_freq, 'smooth', opts.smooth)\n\n    # Thresholding\n    abs_and_cov = logical_and(abs_sig_pow >= opts.abs_pow_thresh,\n                              sigma_covar >= opts.covar_thresh)\n    concensus = logical_and.reduce((rel_sig_pow >= opts.rel_pow_thresh,\n                                    sigma_corr >= opts.corr_thresh,\n                                    abs_and_cov))                                    \n    events = detect_events(concensus, 'custom') # at s_freq * 0.1\n    \n    if events is not None:\n        events = _merge_close(dat_sigma, events, time, opts.tolerance)\n        events = _select_period(events, abs_and_cov)\n        \n        if opts.windowing['step']:\n            events = events * (s_freq * opts.windowing['step']) # upsample\n            events = asarray(around(events), dtype=int)\n        \n        events = within_duration(events, time, opts.duration)\n        events = _merge_close(dat_sigma, events, time, opts.min_interval)\n        events = remove_straddlers(events, time, s_freq)\n\n        power_peaks = peak_in_power(events, dat_orig, s_freq, opts.power_peaks)\n        powers = power_in_band(events, dat_orig, s_freq, opts.frequency)\n        sp_in_chan = make_spindles(events, power_peaks, powers, dat_sigma,\n                                   dat_orig, time, s_freq)\n\n    else:\n        lg.info('No spindle found')\n        sp_in_chan = []\n\n    values = {'abs_pow_thresh': opts.abs_pow_thresh, \n              'rel_pow_thresh': opts.rel_pow_thresh, \n              'covar_thresh': opts.covar_thresh,\n              'corr_thresh': opts.corr_thresh}\n\n    density = len(sp_in_chan) * s_freq * 30 / len(dat_orig)\n\n    return sp_in_chan, values, density", "response": "Spindle detection based on Lacourse et al. 2018"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nspindle detection based on Martin et al. 2013", "response": "def detect_Martin2013(dat_orig, s_freq, time, opts):\n    \"\"\"Spindle detection based on Martin et al. 2013\n    \n    Parameters\n    ----------\n    dat_orig : ndarray (dtype='float')\n        vector with the data for one channel\n    s_freq : float\n        sampling frequency\n    time : ndarray (dtype='float')\n        vector with the time points for each sample\n    opts : instance of 'DetectSpindle'\n        'remez' : dict\n             parameters for 'remez' filter\n        'moving_rms' : dict\n             parameters for 'moving_rms'\n        'percentile' : float\n            percentile for detection threshold\n    \n    Returns\n    -------\n    list of dict\n        list of detected spindles\n    dict\n        'det_value_lo' with detection value, 'det_value_hi' is nan,\n        'sel_value' is nan (for consistency with other methods)\n    float\n        spindle density, per 30-s epoch\n\n    References\n    ----------\n    Martin, N. et al. Neurobio Aging 34(2), 468-76 (2013).\n    \"\"\"\n    dat_filt = transform_signal(dat_orig, s_freq, 'remez', opts.det_remez)\n    dat_det = transform_signal(dat_filt, s_freq, 'moving_rms', opts.moving_rms)\n        # downsampled\n    \n    det_value = percentile(dat_det, opts.det_thresh)\n    \n    events = detect_events(dat_det, 'above_thresh', det_value)\n    \n    if events is not None:\n        events *= int(around(s_freq * opts.moving_rms['step'])) # upsample\n        events = _merge_close(dat_filt, events, time, opts.tolerance)\n        events = within_duration(events, time, opts.duration)\n        events = _merge_close(dat_filt, events, time, opts.min_interval)\n        events = remove_straddlers(events, time, s_freq)\n\n        power_peaks = peak_in_power(events, dat_orig, s_freq, opts.power_peaks)\n        powers = power_in_band(events, dat_orig, s_freq, opts.frequency)\n        sp_in_chan = make_spindles(events, power_peaks, powers, dat_filt,\n                                   dat_orig, time, s_freq)\n\n    else:\n        lg.info('No spindle found')\n        sp_in_chan = []\n\n    values = {'det_value_lo': det_value, 'sel_value': nan}\n\n    density = len(sp_in_chan) * s_freq * 30 / len(dat_orig)\n\n    return sp_in_chan, values, density"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef detect_Ferrarelli2007(dat_orig, s_freq, time, opts):\n    dat_det = transform_signal(dat_orig, s_freq, 'remez', opts.det_remez)\n    dat_det = transform_signal(dat_det, s_freq, 'abs')\n    \n    idx_env = peaks_in_time(dat_det)\n    idx_peak = idx_env[peaks_in_time(dat_det[idx_env])]\n\n    det_value = define_threshold(dat_det, s_freq, 'mean', opts.det_thresh)\n    sel_value = define_threshold(dat_det[idx_peak], s_freq, 'histmax', \n                                 opts.sel_thresh, nbins=120)\n    \n    events_env = detect_events(dat_det[idx_env], 'above_thresh', det_value)\n    \n    if events_env is not None:\n        events_env = _merge_close(dat_det[idx_env], events_env, time[idx_env], \n                                  opts.tolerance)\n        events_env = select_events(dat_det[idx_env], events_env, \n                                   'above_thresh', sel_value)  \n        events = idx_env[events_env]\n\n        # merging is necessary, because detected spindles may overlap if the\n        # signal envelope does not dip below sel_thresh between two peaks above \n        # det_thresh\n        events = _merge_close(dat_det, events, time, opts.min_interval)\n        events = within_duration(events, time, opts.duration)        \n        events = remove_straddlers(events, time, s_freq)\n\n        power_peaks = peak_in_power(events, dat_orig, s_freq, opts.power_peaks)\n        powers = power_in_band(events, dat_orig, s_freq, opts.frequency)\n        sp_in_chan = make_spindles(events, power_peaks, powers, dat_det,\n                                   dat_orig, time, s_freq)\n        lg.info('Spindles in chan: ' + str(len(sp_in_chan)))\n\n    else:\n        lg.info('No spindle found')\n        sp_in_chan = []\n\n    values = {'det_value_lo': det_value, 'sel_value': sel_value}\n\n    density = len(sp_in_chan) * s_freq * 30 / len(dat_orig)\n\n    return sp_in_chan, values, density", "response": "Spindle detection based on Ferrarelli et al. 2007."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nspindling detection based on FASST method, itself based on Moelle et al. (2002). Parameters ---------- dat_orig : ndarray (dtype='float') vector with the data for one channel s_freq : float sampling frequency time : ndarray (dtype='float') vector with the time points for each sample opts : instance of 'DetectSpindle' 'det_remez' : dict parameters for 'remez', 'moving_rms' : dict parameters for 'moving_rms' 'smooth' : dict parameters for 'smooth' 'det_thresh' : float detection threshold 'sel_thresh' : nan not used, but keep it for consistency with the other methods 'duration' : tuple of float min and max duration of spindles submethod : str 'abs' (rectified) or 'rms' (root-mean-square) Returns ------- list of dict list of detected spindles dict 'det_value_lo' with detection value, 'det_value_hi' with nan, 'sel_value' with nan float spindle density, per 30-s epoch References ---------- Leclercq, Y. et al. Compu. Intel. and Neurosci. (2011).", "response": "def detect_FASST(dat_orig, s_freq, time, opts, submethod='rms'):\n    \"\"\"Spindle detection based on FASST method, itself based on Moelle et al. \n    (2002).\n    \n    Parameters\n    ----------\n    dat_orig : ndarray (dtype='float')\n        vector with the data for one channel\n    s_freq : float\n        sampling frequency\n    time : ndarray (dtype='float')\n        vector with the time points for each sample\n    opts : instance of 'DetectSpindle'\n        'det_remez' : dict\n            parameters for 'remez',\n        'moving_rms' : dict\n            parameters for 'moving_rms'\n        'smooth' : dict\n            parameters for 'smooth'\n        'det_thresh' : float\n            detection threshold\n        'sel_thresh' : nan\n            not used, but keep it for consistency with the other methods\n        'duration' : tuple of float\n            min and max duration of spindles\n    submethod : str\n        'abs' (rectified) or 'rms' (root-mean-square)\n\n    Returns\n    -------\n    list of dict\n        list of detected spindles\n    dict\n        'det_value_lo' with detection value, 'det_value_hi' with nan,\n        'sel_value' with nan\n    float\n        spindle density, per 30-s epoch\n\n    References\n    ----------\n    Leclercq, Y. et al. Compu. Intel. and Neurosci. (2011).\n    \"\"\"\n    dat_det = transform_signal(dat_orig, s_freq, 'butter', opts.det_butter)\n    \n    det_value = percentile(dat_det, opts.det_thresh)\n    \n    if submethod == 'abs':\n        dat_det = transform_signal(dat_det, s_freq, 'abs')\n    elif submethod == 'rms':\n        dat_det = transform_signal(dat_det, s_freq, 'moving_rms', \n                                   opts.moving_rms)\n        \n    dat_det = transform_signal(dat_det, s_freq, 'smooth', opts.smooth)\n    \n    events = detect_events(dat_det, 'above_thresh', det_value)\n\n    if events is not None:\n        events = _merge_close(dat_det, events, time, opts.tolerance)\n        events = within_duration(events, time, opts.duration)\n        events = _merge_close(dat_det, events, time, opts.min_interval)\n        events = remove_straddlers(events, time, s_freq)\n\n        power_peaks = peak_in_power(events, dat_orig, s_freq, opts.power_peaks)\n        powers = power_in_band(events, dat_orig, s_freq, opts.frequency)\n        sp_in_chan = make_spindles(events, power_peaks, powers, dat_det,\n                               dat_orig, time, s_freq)\n\n    else:\n        lg.info('No spindle found')\n        sp_in_chan = []\n\n    values = {'det_value_lo': det_value, 'sel_value': nan}\n\n    density = len(sp_in_chan) * s_freq * 30 / len(dat_orig)\n\n    return sp_in_chan, values, density"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nspindling detection based on the UCSD method Parameters ---------- dat_orig : ndarray (dtype='float') vector with the data for one channel s_freq : float sampling frequency time : ndarray (dtype='float') vector with the time points for each sample opts : instance of 'DetectSpindle' det_wavelet : dict parameters for 'wavelet_real', det_thres' : float detection threshold sel_thresh : float selection threshold duration : tuple of float min and max duration of spindles frequency : tuple of float low and high frequency of spindle band (for power ratio) ratio_thresh : float ratio between power inside and outside spindle band to accept them Returns ------- list of dict list of detected spindles dict 'det_value_lo' with detection value, 'det_value_hi' with nan, 'sel_value' with selection value float spindle density, per 30-s epoch", "response": "def detect_UCSD(dat_orig, s_freq, time, opts):\n    \"\"\"Spindle detection based on the UCSD method\n\n    Parameters\n    ----------\n    dat_orig : ndarray (dtype='float')\n        vector with the data for one channel\n    s_freq : float\n        sampling frequency\n    time : ndarray (dtype='float')\n        vector with the time points for each sample\n    opts : instance of 'DetectSpindle'\n        det_wavelet : dict\n            parameters for 'wavelet_real',\n        det_thres' : float\n            detection threshold\n        sel_thresh : float\n            selection threshold\n        duration : tuple of float\n            min and max duration of spindles\n        frequency : tuple of float\n            low and high frequency of spindle band (for power ratio)\n        ratio_thresh : float\n            ratio between power inside and outside spindle band to accept them\n\n    Returns\n    -------\n    list of dict\n        list of detected spindles\n    dict\n        'det_value_lo' with detection value, 'det_value_hi' with nan,\n        'sel_value' with selection value\n    float\n        spindle density, per 30-s epoch\n\n    \"\"\"\n    dat_det = transform_signal(dat_orig, s_freq, 'wavelet_real',\n                               opts.det_wavelet)\n\n    det_value = define_threshold(dat_det, s_freq, 'median+std',\n                                 opts.det_thresh)\n\n    events = detect_events(dat_det, 'maxima', det_value)\n\n    dat_sel = transform_signal(dat_orig, s_freq, 'wavelet_real',\n                               opts.sel_wavelet)\n    sel_value = define_threshold(dat_sel, s_freq, 'median+std',\n                                 opts.sel_thresh)\n    events = select_events(dat_sel, events, 'above_thresh', sel_value)\n\n    events = _merge_close(dat_det, events, time, opts.tolerance)\n    events = within_duration(events, time, opts.duration)\n    events = _merge_close(dat_det, events, time, opts.min_interval)\n    events = remove_straddlers(events, time, s_freq)\n\n    events = power_ratio(events, dat_orig, s_freq, opts.frequency,\n                         opts.ratio_thresh)\n\n    power_peaks = peak_in_power(events, dat_orig, s_freq, opts.power_peaks)\n    powers = power_in_band(events, dat_orig, s_freq, opts.frequency)\n    sp_in_chan = make_spindles(events, power_peaks, powers, dat_det,\n                               dat_orig, time, s_freq)\n\n    values = {'det_value_lo': det_value, 'sel_value': sel_value}\n\n    density = len(sp_in_chan) * s_freq * 30 / len(dat_orig)\n\n    return sp_in_chan, values, density"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nspindle detection for one channel.", "response": "def detect_Concordia(dat_orig, s_freq, time, opts):\n    \"\"\"Spindle detection, experimental Concordia method. Similar to Moelle 2011\n    and Nir2011.\n\n    Parameters\n    ----------\n    dat_orig : ndarray (dtype='float')\n        vector with the data for one channel\n    s_freq : float\n        sampling frequency\n    opts : instance of 'DetectSpindle'\n        'det_butter' : dict\n            parameters for 'butter',\n        'moving_rms' : dict\n            parameters for 'moving_rms'\n        'smooth' : dict\n            parameters for 'smooth'\n        'det_thresh' : float\n            low detection threshold\n        'det_thresh_hi' : float\n            high detection threshold\n        'sel_thresh' : float\n            selection threshold\n        'duration' : tuple of float\n            min and max duration of spindles\n\n    Returns\n    -------\n    list of dict\n        list of detected spindles\n    dict\n        'det_value_lo', 'det_value_hi' with detection values, 'sel_value' with\n        selection value\n    float\n        spindle density, per 30-s epoch\n    \"\"\"\n    dat_det = transform_signal(dat_orig, s_freq, 'butter', opts.det_butter)\n    dat_det = transform_signal(dat_det, s_freq, 'moving_rms', opts.moving_rms)\n    dat_det = transform_signal(dat_det, s_freq, 'smooth', opts.smooth)\n\n    det_value_lo = define_threshold(dat_det, s_freq, 'mean+std',\n                                    opts.det_thresh)\n    det_value_hi = define_threshold(dat_det, s_freq, 'mean+std',\n                                    opts.det_thresh_hi)\n    sel_value = define_threshold(dat_det, s_freq, 'mean+std', opts.sel_thresh)\n\n    events = detect_events(dat_det, 'between_thresh',\n                           value=(det_value_lo, det_value_hi))\n\n    if events is not None:\n        events = _merge_close(dat_det, events, time, opts.tolerance)\n\n        events = select_events(dat_det, events, 'above_thresh', sel_value)\n\n        events = within_duration(events, time, opts.duration)\n        events = _merge_close(dat_det, events, time, opts.min_interval)\n        events = remove_straddlers(events, time, s_freq)\n\n        power_peaks = peak_in_power(events, dat_orig, s_freq, opts.power_peaks)\n        powers = power_in_band(events, dat_orig, s_freq, opts.frequency)\n        sp_in_chan = make_spindles(events, power_peaks, powers, dat_det,\n                                   dat_orig, time, s_freq)\n\n    else:\n        lg.info('No spindle found')\n        sp_in_chan = []\n\n    values = {'det_value_lo': det_value_lo, 'sel_value': sel_value}\n\n    density = len(sp_in_chan) * s_freq * 30 / len(dat_orig)\n\n    return sp_in_chan, values, density"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform the data using different methods.", "response": "def transform_signal(dat, s_freq, method, method_opt=None, dat2=None):\n    \"\"\"Transform the data using different methods.\n\n    Parameters\n    ----------\n    dat : ndarray (dtype='float')\n        vector with all the data for one channel\n    s_freq : float\n        sampling frequency\n    method : str\n        one of 'butter', 'cheby2', 'double_butter', 'morlet', 'morlet_real', \n        'hilbert', 'abs', 'smooth', 'gaussian', 'remez', 'wavelet_real',\n        'low_butter', 'zscore', 'moving_rms', 'moving_ms'\n    method_opt : dict\n        depends on methods\n    dat2 : ndarray(dtype='float')\n        second vector with data\n\n    Returns\n    -------\n    ndarray (dtype='float')\n        vector with all the data for one channel\n\n    Notes\n    -----\n    double_butter implements an effective bandpass by applying a highpass, \n    followed by a lowpass. This method reduces filter instability, due to \n    underlying numerical instability arising from nyquist / freq, at low freq.\n    Wavelets pass only absolute values already, it does not make sense to store\n    the complex values.\n\n    Methods\n    -------    \n    butter has parameters:\n        freq : tuple of float\n            low and high values for bandpass\n        order : int\n            filter order (will be effecively doubled by filtfilt)\n\n    cdemod has parameters:\n        freq : float\n            carrier frequency for complex demodulation\n    \n    cheby2 has parameters:\n        freq : tuple of float\n            low and high values for bandpass\n        order : int\n            filter order (will be effecively doubled by filtfilt)\n            \n    double_butter has parameters:\n        freq : tuple of float\n            low and high values for highpass, then lowpass\n        order : int\n            filter order (will be effecively doubled by filtfilt)\n\n    gaussian has parameters:\n        dur : float\n            standard deviation of the Gaussian kernel, aka sigma (sec)\n            \n    low_butter has parameters:\n        freq : float\n            Highcut frequency, in Hz\n        order : int\n            filter order (will be effecively doubled by filtfilt)\n            \n    morlet has parameters:\n        f0 : float\n            center frequency in Hz\n        sd : float\n            standard deviation of frequency\n        dur : float\n            window length in number of standard deviations\n\n    wavelet_real has parameters:\n        freqs : ndarray\n            vector of wavelet frequencies for spindle detection\n        dur : float\n            duration of the wavelet (sec)\n        width : float\n            wavelet width\n        win : float\n            moving average window length (sec) of wavelet convolution\n\n    smooth has parameters:\n        dur : float\n            duration of the window (sec)\n\n    moving_rms has parameters:\n        dur : float\n            duration of the window (sec)\n            \n    moving_zscore has parameters:\n        dur : float\n            duration of the z-score sliding window (sec)\n            \n    remez has parameters:\n        freq : tuple of float\n            low and high values for bandpass\n        rolloff : float\n            bandwidth, in hertz, between stop and pass frequencies\n        dur : float\n            dur * s_freq = N, where N is the filter order, a.k.a number of taps\n                \n    tri_smooth has parameters:\n        dur : float\n            length of convolution window, base of isosceles triangle\n    \"\"\"\n    if 'abs' == method:\n        dat = absolute(dat)\n        \n    if 'abs2' == method:\n        dat = dat.real**2 + dat.imag**2\n\n    if 'butter' == method:\n        freq = method_opt['freq']\n        N = method_opt['order']\n\n        nyquist = s_freq / 2\n        Wn = asarray(freq) / nyquist\n        b, a = butter(N, Wn, btype='bandpass')\n        dat = filtfilt(b, a, dat)\n        \n    if 'cdemod' == method:\n        carr_freq = method_opt['freq']\n        carr_sig = exp(-1j * 2 * pi * carr_freq * arange(0, len(dat)) / s_freq)\n        \n        dat = dat * carr_sig        \n    \n    if 'cheby2' == method:\n        freq = method_opt['freq']\n        N = method_opt['order']\n\n        Rs = 40\n        nyquist = s_freq / 2\n        Wn = asarray(freq) / nyquist\n        b, a = cheby2(N, Rs, Wn, btype='bandpass')\n        dat = filtfilt(b, a, dat)\n\n    if 'double_butter' == method:\n        freq = method_opt['freq']\n        N = method_opt['order']\n        nyquist = s_freq / 2\n        \n        # Highpass\n        Wn = freq[0] / nyquist\n        b, a = butter(N, Wn, btype='highpass')\n        dat = filtfilt(b, a, dat)\n        \n        # Lowpass\n        Wn = freq[1] / nyquist\n        b, a = butter(N, Wn, btype='lowpass')\n        dat = filtfilt(b, a, dat)\n\n    if 'double_sosbutter' == method:\n        freq = method_opt['freq']\n        N = method_opt['order']\n        nyquist = s_freq / 2\n        \n        # Highpass\n        Wn = freq[0] / nyquist\n        sos = butter(N, Wn, btype='highpass', output='sos')\n        dat = sosfiltfilt(sos, dat)\n        \n        # Lowpass\n        Wn = freq[1] / nyquist\n        sos = butter(N, Wn, btype='lowpass', output='sos')\n        dat = sosfiltfilt(sos, dat)\n    \n    if 'gaussian' == method:\n        sigma = method_opt['dur']\n\n        dat = gaussian_filter(dat, sigma)        \n\n    if 'hilbert' == method:\n        N = len(dat)\n        dat = hilbert(dat, N=next_fast_len(N)) # much faster this way\n        dat = dat[:N] # truncate away zero-padding\n        \n    if 'low_butter' == method:\n        freq = method_opt['freq']\n        N = method_opt['order']\n        nyquist = s_freq / 2\n        \n        Wn = freq / nyquist\n        b, a = butter(N, Wn, btype='lowpass')\n        dat = filtfilt(b, a, dat)\n    \n    if 'high_butter' == method:\n        freq = method_opt['freq']\n        N = method_opt['order']\n        nyquist = s_freq / 2\n        \n        Wn = freq / nyquist\n        b, a = butter(N, Wn, btype='highpass')\n        dat = filtfilt(b, a, dat)\n    \n    if 'morlet' == method:\n        f0 = method_opt['f0']\n        sd = method_opt['sd']\n        dur = method_opt['dur']\n\n        wm = _wmorlet(f0, sd, s_freq, dur)\n        dat = absolute(fftconvolve(dat, wm, mode='same'))\n\n    if 'moving' in method:\n        dur = method_opt['dur']\n        halfdur = dur / 2\n        total_dur = len(dat) / s_freq\n        last = len(dat) - 1\n        \n        if method_opt['step']:\n            step = method_opt['step']\n            len_out = int(len(dat) / (step * s_freq))\n        else:\n            step = 1 / s_freq\n            len_out = len(dat)\n            \n        out = zeros((len_out))\n        \n        if 'moving_covar' == method:            \n            for i, j in enumerate(arange(0, total_dur, step)[:-2]):\n                beg = max(0, int((j - halfdur) * s_freq))\n                end = min(last, int((j + halfdur) * s_freq))\n                win1 = dat[beg:end]\n                win2 = dat2[beg:end]\n                out[i] = mean((win1 - mean(win1)) * (win2 - mean(win2)))\n            dat = out\n    \n        if 'moving_power_ratio' == method:\n            freq1 = method_opt['freq_narrow']\n            freq2 = method_opt['freq_broad']\n            fft_dur = method_opt['fft_dur']\n            nfft = int(s_freq * fft_dur)\n            \n            for i, j in enumerate(arange(0, total_dur, step)[:-2]):\n                beg = max(0, int((j - halfdur) * s_freq))\n                end = min(last, int((j + halfdur) * s_freq))\n                windat = dat[beg:end]\n                \n                sf, psd = periodogram(windat, s_freq, 'hann', nfft=nfft,\n                                       detrend='constant')\n                f0 = asarray([abs(x - freq1[0]) for x in sf]).argmin()\n                f1 = asarray([abs(x - freq1[1]) for x in sf]).argmin()\n                pow1 = sum(psd[f0:f1])\n                \n                f0 = asarray([abs(x - freq2[0]) for x in sf]).argmin()\n                f1 = asarray([abs(x - freq2[1]) for x in sf]).argmin()\n                pow2 = sum(psd[f0:f1])\n                \n                out[i] = pow1 / pow2\n                \n            dat = out\n        \n        if 'moving_sd' == method:\n            for i, j in enumerate(arange(0, total_dur, step)[:-2]):\n                beg = max(0, int((j - halfdur) * s_freq))\n                end = min(last, int((j + halfdur) * s_freq))\n                win = dat[beg:end]\n                out[i] = std(win)\n            dat = out\n        \n        if 'moving_zscore' == method:        \n            pcl_range = method_opt['pcl_range']\n            \n            for i, j in enumerate(arange(0, total_dur, step)[:-2]):\n                beg = max(0, int((j - halfdur) * s_freq))\n                end = min(last, int((j + halfdur) * s_freq))\n                windat = stddat = dat[beg:end]\n                \n                if pcl_range is not None:\n                    lo, hi = pcl_range\n                    stddat = windat[logical_and(\n                                            windat > percentile(windat, lo),\n                                            windat < percentile(windat, hi))]\n                out[i] = (dat[i] - mean(windat)) / std(stddat)\n            dat = out\n        \n        if method in ['moving_rms', 'moving_ms']:\n            for i, j in enumerate(arange(0, total_dur, step)[:-2]):\n                beg = max(0, int((j - halfdur) * s_freq))\n                end = min(last, int((j + halfdur) * s_freq))\n                out[i] = mean(square(dat[beg:end]))   \n            if method == 'moving_rms':\n                out = sqrt(out)\n            dat = out\n    \n    if 'remez' == method:\n        Fp1, Fp2 = method_opt['freq']\n        rolloff = method_opt['rolloff']\n        dur = method_opt['dur']\n        \n        N = int(s_freq * dur)\n        nyquist = s_freq / 2\n        Fs1, Fs2 = Fp1 - rolloff, Fp2 + rolloff\n        dens = 20\n        \n        bpass = remez(N, [0, Fs1, Fp1, Fp2, Fs2, nyquist], [0, 1, 0], \n                      grid_density=dens, fs=s_freq)\n        dat = filtfilt(bpass, 1, dat)\n\n    if 'smooth' == method:   \n        dur = method_opt['dur']\n        win = method_opt['win']\n        \n        if 'flat' in win:\n            flat = ones(int(dur * s_freq))\n            H = flat / sum(flat)\n            \n            if 'flat_left' == win:\n                H = concatenate((H, zeros(len(H))))\n            elif 'flat_right' == win:\n                H = concatenate((zeros(len(H) - 1), H))\n            \n        elif 'triangle' == win:\n            T = int(dur * s_freq / 2)\n            a = arange(T, 0, -1)\n            \n            H = hstack([a[-1:0:-1], a])\n            H = H / sum(H)\n            \n        dat = fftconvolve(dat, H, mode='same')\n    \n    if 'sosbutter' == method:\n        freq = method_opt['freq']\n        N = method_opt['order']\n\n        nyquist = s_freq / 2\n        Wn = asarray(freq) / nyquist\n        sos = butter(N, Wn, btype='bandpass', output='sos')\n        dat = sosfiltfilt(sos, dat)\n        \n    if 'spectrogram' == method:\n        nperseg = method_opt['dur'] * s_freq\n        noverlap = method_opt['step'] * s_freq\n        detrend = method_opt['detrend']\n        \n        dat = spectrogram(dat, fs=s_freq, nperseg=nperseg, noverlap=noverlap, \n                          detrend=detrend)\n    \n    if 'wavelet_real' == method:\n        freqs = method_opt['freqs']\n        dur = method_opt['dur']\n        width = method_opt['width']\n        win = int(method_opt['win'] * s_freq)\n\n        wm = _realwavelets(s_freq, freqs, dur, width)\n        tfr = empty((dat.shape[0], wm.shape[0]))\n        for i, one_wm in enumerate(wm):\n            x = abs(fftconvolve(dat, one_wm, mode='same'))\n            tfr[:, i] = fftconvolve(x, tukey(win), mode='same')\n        dat = mean(tfr, axis=1)\n\n    return dat"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefines the threshold based on relative values.", "response": "def define_threshold(dat, s_freq, method, value, nbins=120):\n    \"\"\"Return the value of the threshold based on relative values.\n\n    Parameters\n    ----------\n    dat : ndarray (dtype='float')\n        vector with the data after selection-transformation\n    s_freq : float\n        sampling frequency\n    method : str\n        one of 'mean', 'median', 'std', 'mean+std', 'median+std', 'histmax'\n    value : float\n        value to multiply the values for\n    nbins : int\n        for histmax method, number of bins in the histogram\n\n    Returns\n    -------\n    float\n        threshold in useful units.\n\n    \"\"\"\n    if method == 'mean':\n        value = value * mean(dat)\n    elif method == 'median':\n        value = value * median(dat)\n    elif method == 'std':\n        value = value * std(dat)\n    elif method == 'mean+std':\n        value = mean(dat) + value * std(dat)\n    elif method == 'median+std':\n        value = median(dat) + value * std(dat)\n    elif method == 'histmax':\n        hist = histogram(dat, bins=nbins)\n        idx_maxbin = argmax(hist[0])\n        maxamp = mean((hist[1][idx_maxbin], hist[1][idx_maxbin + 1]))\n        value = value * maxamp\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind indices of peaks or troughs in a given time vector.", "response": "def peaks_in_time(dat, troughs=False):\n    \"\"\"Find indices of peaks or troughs in data.\n    \n    Parameters\n    ----------\n    dat : ndarray (dtype='float')\n        vector with the data\n    troughs : bool\n        if True, will return indices of troughs instead of peaks\n        \n    Returns\n    -------\n    nadarray of int\n        indices of peaks (or troughs) in dat\n        \n    Note\n    ----\n    This function does not deal well with flat signal; when the signal is not \n    increasing, it is assumed to be descreasing. As a result, this function\n    finds troughs where the signal begins to increase after either decreasing \n    or remaining constant\n    \"\"\"\n    diff_dat = diff(dat)\n    increasing = zeros(len(diff_dat))\n    increasing[diff_dat > 0] = 1 # mask for all points where dat is increasing\n    flipping = diff(increasing) # peaks are -1, troughs are 1, the rest is zero\n    \n    target = -1 if not troughs else 1\n        \n    return where(flipping == target)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detect_events(dat, method, value=None):\n    if 'thresh' in method or 'custom' == method:\n\n        if method == 'above_thresh':\n            above_det = dat >= value\n            detected = _detect_start_end(above_det)\n\n        if method == 'below_thresh':\n            below_det = dat < value\n            detected = _detect_start_end(below_det)\n\n        if method == 'between_thresh':\n            above_det = dat >= value[0]\n            below_det = dat < value[1]\n            between_det = logical_and(above_det, below_det)\n            detected = _detect_start_end(between_det)\n            \n        if method == 'custom':\n            detected = _detect_start_end(dat)\n\n        if detected is None:\n            return None\n        \n        if method in ['above_thresh', 'custom']:    \n            # add the location of the peak in the middle\n            detected = insert(detected, 1, 0, axis=1)\n            for i in detected:\n                i[1] = i[0] + argmax(dat[i[0]:i[2]])\n\n        if method in ['below_thresh', 'between_thresh']:\n            # add the location of the trough in the middle\n            detected = insert(detected, 1, 0, axis=1)\n            for i in detected:\n                i[1] = i[0] + argmin(dat[i[0]:i[2]])\n\n    if method == 'maxima':\n        peaks = argrelmax(dat)[0]\n        detected = vstack((peaks, peaks, peaks)).T\n\n        if value is not None:\n            detected = detected[dat[peaks] > value, :]\n\n    return detected", "response": "Detect events using the method specified in method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_events(dat, detected, method, value):\n    if method == 'above_thresh':\n        above_sel = dat >= value\n        detected = _select_period(detected, above_sel)\n    elif method == 'below_thresh':\n        below_sel = dat <= value\n        detected = _select_period(detected, below_sel)\n\n    return detected", "response": "Select start and end samples of the events."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_close(events, min_interval, merge_to_longer=False):\n    half_iv = min_interval / 2\n    merged = []\n\n    for higher in events:\n\n        if not merged:\n            merged.append(higher)\n\n        else:\n            lower = merged[-1]\n\n            if higher['start'] - half_iv <= lower['end'] + half_iv:\n\n                if merge_to_longer and (higher['end'] - higher['start'] >\n                lower['end'] - lower['start']):\n                    start = min(lower['start'], higher['start'])\n                    higher.update({'start': start})\n                    merged[-1] = higher\n\n                else:\n                    end = max(lower['end'], higher['end'])\n                    merged[-1].update({'end': end})\n\n            else:\n                merged.append(higher)\n\n    return merged", "response": "Merge events that are separated by a less than a minimum interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether an event is within time limits.", "response": "def within_duration(events, time, limits):\n    \"\"\"Check whether event is within time limits.\n\n    Parameters\n    ----------\n    events : ndarray (dtype='int')\n        N x M matrix with start sample first and end samples last on M\n    time : ndarray (dtype='float')\n        vector with time points\n    limits : tuple of float\n        low and high limit for spindle duration\n\n    Returns\n    -------\n    ndarray (dtype='int')\n        N x M matrix with start sample first and end samples last on M\n    \"\"\"\n    min_dur = max_dur = ones(events.shape[0], dtype=bool)\n    \n    if limits[0] is not None:\n        min_dur = time[events[:, -1] - 1] - time[events[:, 0]] >= limits[0]\n    \n    if limits[1] is not None:\n        max_dur = time[events[:, -1] - 1] - time[events[:, 0]] <= limits[1]\n\n    return events[min_dur & max_dur, :]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrejects an event if it straddles a stitch by comparing its duration to its timespan.", "response": "def remove_straddlers(events, time, s_freq, toler=0.1):\n    \"\"\"Reject an event if it straddles a stitch, by comparing its \n    duration to its timespan.\n\n    Parameters\n    ----------\n    events : ndarray (dtype='int')\n        N x M matrix with start, ..., end samples\n    time : ndarray (dtype='float')\n        vector with time points\n    s_freq : float\n        sampling frequency\n    toler : float, def=0.1\n        maximum tolerated difference between event duration and timespan\n\n    Returns\n    -------\n    ndarray (dtype='int')\n        N x M matrix with start , ..., end samples\n    \"\"\"\n    dur = (events[:, -1] - 1 - events[:, 0]) / s_freq\n    continuous = time[events[:, -1] - 1] - time[events[:, 0]] - dur < toler\n    \n    return events[continuous, :]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nestimating the ratio in power between spindle band and lower frequencies.", "response": "def power_ratio(events, dat, s_freq, limits, ratio_thresh):\n    \"\"\"Estimate the ratio in power between spindle band and lower frequencies.\n\n    Parameters\n    ----------\n    events : ndarray (dtype='int')\n        N x 3 matrix with start, peak, end samples\n    dat : ndarray (dtype='float')\n        vector with the original data\n    s_freq : float\n        sampling frequency\n    limits : tuple of float\n        high and low frequencies for spindle band\n    ratio_thresh : float\n        ratio between spindle vs non-spindle amplitude\n\n    Returns\n    -------\n    ndarray (dtype='int')\n        N x 3 matrix with start, peak, end samples\n\n    Notes\n    -----\n    In the original matlab script, it uses amplitude, not power.\n\n    \"\"\"\n    ratio = empty(events.shape[0])\n    for i, one_event in enumerate(events):\n\n        x0 = one_event[0]\n        x1 = one_event[2]\n\n        if x0 < 0 or x1 >= len(dat):\n            ratio[i] = 0\n\n        else:\n            f, Pxx = periodogram(dat[x0:x1], s_freq, scaling='spectrum')\n            Pxx = sqrt(Pxx)  # use amplitude\n\n            freq_sp = (f >= limits[0]) & (f <= limits[1])\n            freq_nonsp = (f <= limits[1])\n\n            ratio[i] = mean(Pxx[freq_sp]) / mean(Pxx[freq_nonsp])\n\n    events = events[ratio > ratio_thresh, :]\n\n    return events"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef peak_in_power(events, dat, s_freq, method, value=None):\n    dat = diff(dat)  # remove 1/f\n\n    peak = empty(events.shape[0])\n    peak.fill(nan)\n\n    if method is not None:\n        for i, one_event in enumerate(events):\n\n            if method == 'peak':\n                x0 = one_event[1] - value / 2 * s_freq\n                x1 = one_event[1] + value / 2 * s_freq\n\n            elif method == 'interval':\n                x0 = one_event[0]\n                x1 = one_event[2]\n\n            if x0 < 0 or x1 >= len(dat):\n                peak[i] = nan\n            else:\n                f, Pxx = periodogram(dat[x0:x1], s_freq)\n                idx_peak = Pxx[f < MAX_FREQUENCY_OF_INTEREST].argmax()\n                peak[i] = f[idx_peak]\n\n    return peak", "response": "Define peak in power of the signal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef power_in_band(events, dat, s_freq, frequency):\n    dat = diff(dat)  # remove 1/f\n\n    pw = empty(events.shape[0])\n    pw.fill(nan)\n\n    for i, one_event in enumerate(events):\n\n        x0 = one_event[0]\n        x1 = one_event[2]\n\n        if x0 < 0 or x1 >= len(dat):\n            pw[i] = nan\n        else:\n            sf, Pxx = periodogram(dat[x0:x1], s_freq)\n            # find nearest frequencies in sf\n            b0 = asarray([abs(x - frequency[0]) for x in sf]).argmin()\n            b1 = asarray([abs(x - frequency[1]) for x in sf]).argmin()\n            pw[i] = mean(Pxx[b0:b1])\n\n    return pw", "response": "Define the power of the signal within frequency band."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_spindles(events, power_peaks, powers, dat_det, dat_orig, time,\n                  s_freq):\n    \"\"\"Create dict for each spindle, based on events of time points.\n\n    Parameters\n    ----------\n    events : ndarray (dtype='int')\n        N x 3 matrix with start, peak, end samples, and peak frequency\n    power_peaks : ndarray (dtype='float')\n        peak in power spectrum for each event\n    powers : ndarray (dtype='float')\n        average power in power spectrum for each event\n    dat_det : ndarray (dtype='float')\n        vector with the data after detection-transformation (to compute peak)\n    dat_orig : ndarray (dtype='float')\n        vector with the raw data on which detection was performed\n    time : ndarray (dtype='float')\n        vector with time points\n    s_freq : float\n        sampling frequency\n\n    Returns\n    -------\n    list of dict\n        list of all the spindles, with information about start_time, peak_time,\n        end_time (s), peak_val (signal units), area_under_curve\n        (signal units * s), peak_freq (Hz)\n    \"\"\"\n    i, events = _remove_duplicate(events, dat_det)\n    power_peaks = power_peaks[i]\n\n    spindles = []\n    for i, one_peak, one_pwr in zip(events, power_peaks, powers):\n        one_spindle = {'start': time[i[0]],\n                       'end': time[i[2] - 1],\n                       'peak_time': time[i[1]],\n                       'peak_val_det': dat_det[i[1]],\n                       'peak_val_orig': dat_orig[i[1]],\n                       'dur': (i[2] - i[0]) / s_freq,\n                       'auc_det': sum(dat_det[i[0]:i[2]]) / s_freq,\n                       'auc_orig': sum(dat_orig[i[0]:i[2]]) / s_freq,\n                       'rms_det': sqrt(mean(square(dat_det[i[0]:i[2]]))),\n                       'rms_orig': sqrt(mean(square(dat_orig[i[0]:i[2]]))),\n                       'power_orig': one_pwr,\n                       'peak_freq': one_peak,\n                       'ptp_det': ptp(dat_det[i[0]:i[2]]),\n                       'ptp_orig': ptp(dat_orig[i[0]:i[2]])\n                       }\n        spindles.append(one_spindle)\n\n    return spindles", "response": "Create a list of dicts for each spindle based on events of time points power_peaks and power_val."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove duplicates from the events.", "response": "def _remove_duplicate(old_events, dat):\n    \"\"\"Remove duplicates from the events.\n\n    Parameters\n    ----------\n    old_events : ndarray (dtype='int')\n        N x 3 matrix with start, peak, end samples\n    dat : ndarray (dtype='float')\n        vector with the data after detection-transformation (to compute peak)\n\n    Returns\n    -------\n    ndarray (dtype='int')\n        vector of indices of the events to keep\n    ndarray (dtype='int')\n        N x 3 matrix with start, peak, end samples\n\n    Notes\n    -----\n    old_events is assumed to be sorted. It only checks for the start time and\n    end time. When two (or more) events have the same start time and the same\n    end time, then it takes the largest peak.\n\n    There is no tolerance, indices need to be identical.\n    \"\"\"\n    diff_events = diff(old_events, axis=0)\n    dupl = where((diff_events[:, 0] == 0) & (diff_events[:, 2] == 0))[0]\n    dupl += 1  # more convenient, it copies old_event first and then compares\n\n    n_nondupl_events = old_events.shape[0] - len(dupl)\n    new_events = zeros((n_nondupl_events, old_events.shape[1]), dtype='int')\n    if len(dupl):\n        lg.debug('Removing ' + str(len(dupl)) + ' duplicate events')\n\n    i = 0\n    indices = []\n    for i_old, one_old_event in enumerate(old_events):\n        if i_old not in dupl:\n            new_events[i, :] = one_old_event\n            i += 1\n            indices.append(i_old)\n        else:\n            peak_0 = new_events[i - 1, 1]\n            peak_1 = one_old_event[1]\n            if dat[peak_0] >= dat[peak_1]:\n                new_events[i - 1, 1] = peak_0\n            else:\n                new_events[i - 1, 1] = peak_1\n\n    return indices, new_events"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _detect_start_end(true_values):\n    neg = zeros((1), dtype='bool')\n    int_values = asarray(concatenate((neg, true_values[:-1], neg)), \n                         dtype='int')\n    # must discard last value to avoid axis out of bounds\n    cross_threshold = diff(int_values)\n\n    event_starts = where(cross_threshold == 1)[0]\n    event_ends = where(cross_threshold == -1)[0]\n\n    if len(event_starts):\n        events = vstack((event_starts, event_ends)).T\n\n    else:\n        events = None\n\n    return events", "response": "This function detects the start and end times of a single event tree tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _select_period(detected, true_values):\n    true_values = invert(true_values)\n\n    for one_spindle in detected:\n        # get the first time point when it goes above/below selection thres\n        start_sel = where(true_values[:one_spindle[0]])[0]\n        if start_sel.any():\n            one_spindle[0] = start_sel[-1]\n\n        # get the last time point when it stays above/below selection thres\n        end_sel = where(true_values[one_spindle[2]:])[0] - 1\n        if end_sel.any():\n            one_spindle[2] += end_sel[0]\n\n    return detected", "response": "This function checks whether the detected values are in the time range of the last time point of the last time point of the first time point."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _merge_close(dat, events, time, min_interval):\n    if not events.any():\n        return events\n    \n    no_merge = time[events[1:, 0] - 1] - time[events[:-1, 2]] >= min_interval\n\n    if no_merge.any():\n        begs = concatenate([[events[0, 0]], events[1:, 0][no_merge]])\n        ends = concatenate([events[:-1, 2][no_merge], [events[-1, 2]]])\n\n        new_events = vstack((begs, ends)).T\n    else:\n        new_events = asarray([[events[0, 0], events[-1, 2]]])\n\n    # add the location of the peak in the middle\n    new_events = insert(new_events, 1, 0, axis=1)\n    for i in new_events:\n        if i[2] - i[0] >= 1:\n            i[1] = i[0] + argmax(dat[i[0]:i[2]])\n\n    return new_events", "response": "Merge together events separated by less than a minimum interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wmorlet(f0, sd, sampling_rate, ns=5):\n    st = 1. / (2. * pi * sd)\n    w_sz = float(int(ns * st * sampling_rate))  # half time window size\n    t = arange(-w_sz, w_sz + 1, dtype=float) / sampling_rate\n    w = (exp(-t ** 2 / (2. * st ** 2)) * exp(2j * pi * f0 * t) /\n         sqrt(sqrt(pi) * st * sampling_rate))\n    return w", "response": "This function returns a complex morlet wavelet in the time domain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _realwavelets(s_freq, freqs, dur, width):\n    x = arange(-dur / 2, dur / 2, 1 / s_freq)\n    wavelets = empty((len(freqs), len(x)))\n\n    g = exp(-(pi * x ** 2) / width ** 2)\n\n    for i, one_freq in enumerate(freqs):\n        y = cos(2 * pi * x * one_freq)\n        wavelets[i, :] = y * g\n\n    return wavelets", "response": "Create real wavelets for UCSD."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nactioning when button was clicked.", "response": "def button_clicked(self, button):\n        \"\"\"Action when button was clicked.\n\n        Parameters\n        ----------\n        button : instance of QPushButton\n            which button was pressed\n        \"\"\"\n        if button is self.idx_ok:\n\n            chans = self.get_channels()\n            group = self.one_grp\n            cycle = self.get_cycles()\n            stage = self.idx_stage.selectedItems()\n            params = {k: v.get_value() for k, v in self.index.items()}\n            name = self.name.get_value()\n\n            if None in [params['f1'], params['f2']]:\n                self.parent.statusBar().showMessage(\n                        'Specify bandpass frequencies')\n                return\n\n            if params['max_dur'] is None:\n                self.parent.statusBar().showMessage('Specify maximum duration')\n                return\n            elif params['max_dur'] >= 30:\n                self.parent.statusBar().showMessage(\n                        'Maximum duration must be below 30 seconds.')\n                return\n\n            if stage == []:\n                stage = None\n            else:\n                stage = [x.text() for x in self.idx_stage.selectedItems()]\n\n            chan_full = None\n            reject_artf = False\n            if params['excl_event'] == 'channel-specific':\n                chan_full = [i + ' (' + self.idx_group.currentText() + ''\n                           ')' for i in chans]\n                chans = None\n                reject_artf = True\n            elif params['excl_event'] == 'from any channel':\n                reject_artf = True\n            \n            data = fetch(self.parent.info.dataset, \n                          self.parent.notes.annot, cat=(1, 1, 1, 0),\n                          stage=stage, cycle=cycle, \n                          chan_full=chan_full, min_dur=params['min_seg_dur'], \n                          reject_epoch=params['excl_epoch'], \n                          reject_artf=reject_artf)\n            \n            if not data.segments:\n                msg = 'No valid signal found.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('Error fetching data')\n                error_dialog.showMessage(msg)\n                return\n            \n            ding = data.read_data(chans, group['ref_chan'], group['name'],\n                                  parent=self)\n            \n            if not ding:\n                self.parent.statusBar().showMessage('Process interrupted.')\n                return\n\n            data = data[0]['data']\n            \n# =============================================================================\n#             if params['resample']:                \n#                 rs_freq = params['rs_freq']\n#                 ratio = rs_freq / data.s_freq\n#                 up = Fraction(ratio).numerator\n#                 dn = Fraction(ratio).denominator\n#                 \n#                 rs_dat = zeros((data.number_of('chan')[0], \n#                                 int(data.number_of('time')[0] * ratio)))\n#                 \n#                 for i in range(data.number_of('chan')[0]):                    \n#                     rs_dat[i, :] = resample_poly(data.data[0][i], up, dn)\n#                          \n#                 data.data[0] = rs_dat\n#                 data.s_freq = rs_freq\n#                 start_time = data.axis['time'][0][0]\n#                 end_time = data.axis['time'][0][-1]\n#                 data.axis['time'][0] = arange(start_time, end_time, \n#                          1 / rs_freq)\n#                 lg.info(str(data.data.shape))\n#                 lg.info(str(data.data[0].shape))\n#                 lg.info(str(data.data[0][0].shape))\n#                 lg.info(str(data.axis['time'].shape))\n#                 lg.info(str(data.s_freq))\n# =============================================================================\n            \n            if params['prep_filt']:\n                low_cut = params['prep_lc']\n                high_cut = params['prep_hc']\n                data = filter_(data, axis='time', low_cut=low_cut, \n                               high_cut=high_cut)\n            \n            if params['detrend']:\n                data = math(data, operator_name='detrend', axis='time')            \n            \n            self.parent.notes.detect_events(data, self.method, \n                                            params, label=name)\n\n            self.accept()\n\n        if button is self.idx_cancel:\n            self.reject()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the values of the current language field.", "response": "def update_values(self):\n        \"\"\"Update form values when detection method is selected.\"\"\"\n        self.method = self.idx_method.currentText()\n        spin_det = DetectSpindle(method=self.method)\n        \n        self.index['f1'].set_value(spin_det.frequency[0])\n        self.index['f2'].set_value(spin_det.frequency[1])\n        self.index['tolerance'].set_value(spin_det.tolerance)\n        self.index['min_dur'].set_value(spin_det.duration[0])\n        self.index['max_dur'].set_value(spin_det.duration[1])        \n        self.index['interval'].set_value(spin_det.min_interval)\n\n        if spin_det.rolloff:\n            self.index['rolloff'].set_value(spin_det.rolloff)\n            self.index['rolloff'].setEnabled(True)\n        else:\n            self.index['rolloff'].set_value('N/A')\n            self.index['rolloff'].setEnabled(False)\n\n        if self.method == 'Ferrarelli2007':\n            self.label[0].setText('Detection threshold')\n            self.label[1].setText('Selection threshold')\n            self.label[2].setText('')\n            self.label[3].setText('')\n            self.label[4].setText('')\n            self.label[5].setText('')\n\n            self.index['0'].set_value(spin_det.det_thresh)\n            self.index['1'].set_value(spin_det.sel_thresh)        \n        \n        if self.method == 'Nir2011':\n            self.label[0].setText('Gaussian smoothing sigma (sec)')\n            self.label[1].setText('Detection threshold (SD)')\n            self.label[2].setText('Selection threshold (SD)')\n            self.label[3].setText('')\n            self.label[4].setText('')\n            self.label[5].setText('')\n\n            self.index['0'].set_value(spin_det.smooth['dur'])\n            self.index['1'].set_value(spin_det.det_thresh)\n            self.index['2'].set_value(spin_det.sel_thresh)            \n        \n        if self.method == 'Moelle2011':\n            self.label[0].setText('RMS window length (sec)')\n            self.label[1].setText('Smoothing window length (sec)')\n            self.label[2].setText('Detection threshold (SD)')\n            self.label[3].setText('')\n            self.label[4].setText('')\n            self.label[5].setText('')\n\n            self.index['0'].set_value(spin_det.moving_rms['dur'])\n            self.index['1'].set_value(spin_det.smooth['dur'])\n            self.index['2'].set_value(spin_det.det_thresh)\n\n        if self.method == 'Wamsley2012':\n            self.label[0].setText('Wavelet window length (sec)')\n            self.label[1].setText('Wavelet sigma (sec)')\n            self.label[2].setText('Smoothing window length (sec)')\n            self.label[3].setText('Detection threshold')\n            self.label[4].setText('')\n            self.label[5].setText('')\n            \n            self.index['0'].set_value(spin_det.det_wavelet['dur'])\n            self.index['1'].set_value(spin_det.det_wavelet['sd'])\n            self.index['2'].set_value(spin_det.smooth['dur'])\n            self.index['3'].set_value(spin_det.det_thresh)\n\n        if self.method == 'Martin2013':\n            self.label[0].setText('RMS window length (sec)')\n            self.label[1].setText('RMS window step (sec)')\n            self.label[2].setText('Detection threshold (percentile)')\n            \n            self.index['0'].set_value(spin_det.moving_rms['dur'])\n            self.index['1'].set_value(spin_det.moving_rms['step'])\n            self.index['2'].set_value(spin_det.det_thresh)\n        \n        if self.method == 'Ray2015':\n            self.label[0].setText('Smoothing window length (sec)')\n            self.label[1].setText('z-score window length (sec)')\n            self.label[2].setText('Detection threshold (z)')\n            self.label[3].setText('Selection threshold (z)')\n            self.label[4].setText('')\n            self.label[5].setText('')\n            \n            self.index['0'].set_value(spin_det.smooth['dur'])\n            self.index['1'].set_value(spin_det.zscore['step'])\n            self.index['2'].set_value(spin_det.det_thresh)\n            self.index['3'].set_value(spin_det.sel_thresh)\n\n        if self.method == 'Lacourse2018':\n            self.label[0].setText('Window length (sec)')\n            self.label[1].setText('Window step (sec)')\n            self.label[2].setText('Absolute power threshold')\n            self.label[3].setText('Relative power threshold')\n            self.label[4].setText('Covariance threshold')\n            self.label[5].setText('Correlation threshold')\n            \n            self.index['0'].set_value(spin_det.windowing['dur'])\n            self.index['1'].set_value(spin_det.windowing['step'])\n            self.index['2'].set_value(spin_det.abs_pow_thresh)\n            self.index['3'].set_value(spin_det.rel_pow_thresh)\n            self.index['4'].set_value(spin_det.covar_thresh)\n            self.index['5'].set_value(spin_det.corr_thresh)\n\n        if self.method == 'FASST':\n            self.label[0].setText('Detection threshold (percentile)')\n            self.label[1].setText('Smoothing window length (sec)')\n            self.label[2].setText('')\n            self.label[3].setText('')\n            self.label[4].setText('')\n            self.label[5].setText('')\n\n            self.index['0'].set_value(spin_det.det_thresh)\n            self.index['1'].set_value(spin_det.smooth['dur'])\n            \n        if self.method == 'FASST2':\n            self.label[0].setText('Detection threshold (percentile)')\n            self.label[1].setText('RMS window length (sec)')\n            self.label[2].setText('Smoothing window length (sec)')\n            self.label[3].setText('')\n            self.label[4].setText('')\n            self.label[5].setText('')\n\n            self.index['0'].set_value(spin_det.det_thresh)\n            self.index['1'].set_value(spin_det.moving_rms['dur'])\n            self.index['2'].set_value(spin_det.smooth['dur'])\n            \n        if 'UCSD' in self.method:\n            self.label[0].setText('Wavelet duration (sec)')\n            self.label[1].setText('Wavelet width (sec)')\n            self.label[2].setText('Smoothing window length (sec)')\n            self.label[3].setText('Detection threshold (SD)')\n            self.label[4].setText('Selection threshold (SD)')\n            self.label[5].setText('')\n\n            self.index['0'].set_value(spin_det.det_wavelet['dur'])\n            self.index['1'].set_value(spin_det.det_wavelet['width'])\n            self.index['2'].set_value(spin_det.det_wavelet['win'])\n            self.index['3'].set_value(spin_det.det_thresh)\n            self.index['4'].set_value(spin_det.sel_thresh)\n\n        if 'Concordia' in self.method:\n            self.label[0].setText('RMS window length (sec)')\n            self.label[1].setText('Smoothing window length (sec)')\n            self.label[2].setText('Detection threshold, low (SD)')\n            self.label[3].setText('Detection threshold, high (SD)')\n            self.label[4].setText('Tolerance (sec)')\n            self.label[5].setText('Selection threshold (SD)')\n\n            self.index['0'].set_value(spin_det.moving_rms['dur'])\n            self.index['1'].set_value(spin_det.smooth['dur'])\n            self.index['2'].set_value(spin_det.det_thresh)\n            self.index['3'].set_value(spin_det.det_thresh_hi)\n            self.index['4'].set_value(spin_det.tolerance)\n            self.index['5'].set_value(spin_det.sel_thresh)\n        \n        for i, j in enumerate(self.label):\n            one_param = self.index[str(i)]\n            if j.text() == '':\n                one_param.set_value('')\n                one_param.setEnabled(False)\n            else:\n                one_param.setEnabled(True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_values(self):\n        self.method = self.idx_method.currentText()\n        sw_det = DetectSlowWave(method=self.method)\n\n        self.index['f1'].set_value(sw_det.det_filt['freq'][0])\n        self.index['f2'].set_value(sw_det.det_filt['freq'][1])\n        self.index['min_trough_dur'].set_value(sw_det.trough_duration[0])\n        self.index['max_trough_dur'].set_value(sw_det.trough_duration[1])\n        self.index['max_trough_amp'].set_value(sw_det.max_trough_amp)\n        self.index['min_ptp'].set_value(sw_det.min_ptp)\n        self.index['min_dur'].set_value(sw_det.min_dur)\n        self.index['max_dur'].set_value(sw_det.max_dur)", "response": "Update the values in the index when detection method is selected."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef math(data, operator=None, operator_name=None, axis=None):\n    if operator is not None and operator_name is not None:\n        raise TypeError('Parameters \"operator\" and \"operator_name\" are '\n                        'mutually exclusive')\n\n    # turn input into a tuple of functions in operators\n    if operator_name is not None:\n        if isinstance(operator_name, str):\n            operator_name = (operator_name, )\n\n        operators = []\n        for one_operator_name in operator_name:\n            operators.append(eval(one_operator_name))\n        operator = tuple(operators)\n\n    # make it an iterable\n    if callable(operator):\n        operator = (operator, )\n\n    operations = []\n    for one_operator in operator:\n        on_axis = False\n        keepdims = True\n\n        try:\n            args = getfullargspec(one_operator).args\n        except TypeError:\n            lg.debug('func ' + str(one_operator) + ' is not a Python '\n                     'function')\n        else:\n            if 'axis' in args:\n                on_axis = True\n\n                if axis is None:\n                    raise TypeError('You need to specify an axis if you '\n                                    'use ' + one_operator.__name__ +\n                                    ' (which applies to an axis)')\n\n            if 'keepdims' in args or one_operator in NOKEEPDIM:\n                keepdims = False\n\n        operations.append({'name': one_operator.__name__,\n                           'func': one_operator,\n                           'on_axis': on_axis,\n                           'keepdims': keepdims,\n                           })\n\n    output = data._copy()\n\n    if axis is not None:\n        idx_axis = data.index_of(axis)\n\n    first_op = True\n    for op in operations:\n        #lg.info('running operator: ' + op['name'])\n        func = op['func']\n\n        if func == mode:\n            func = lambda x, axis: mode(x, axis=axis)[0]\n\n        for i in range(output.number_of('trial')):\n\n            # don't copy original data, but use data if it's the first operation\n            if first_op:\n                x = data(trial=i)\n            else:\n                x = output(trial=i)\n\n            if op['on_axis']:\n                lg.debug('running ' + op['name'] + ' on ' + str(idx_axis))\n\n                try:\n                    if func == diff:\n                        lg.debug('Diff has one-point of zero padding')\n                        x = _pad_one_axis_one_value(x, idx_axis)\n                    output.data[i] = func(x, axis=idx_axis)\n\n                except IndexError:\n                    raise ValueError('The axis ' + axis + ' does not '\n                                     'exist in [' +\n                                     ', '.join(list(data.axis.keys())) + ']')\n\n            else:\n                lg.debug('running ' + op['name'] + ' on each datapoint')\n                output.data[i] = func(x)\n\n        first_op = False\n\n        if op['on_axis'] and not op['keepdims']:\n            del output.axis[axis]\n\n    return output", "response": "Returns a new object that can be used to perform mathematical operations on each trial and channel individually."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets mean SD and mean of log values.", "response": "def get_descriptives(data):\n    \"\"\"Get mean, SD, and mean and SD of log values.\n\n    Parameters\n    ----------\n    data : ndarray\n        Data with segment as first dimension\n        and all other dimensions raveled into second dimension.\n\n    Returns\n    -------\n    dict of ndarray\n        each entry is a 1-D vector of descriptives over segment dimension\n    \"\"\"\n    output = {}\n    dat_log = log(abs(data))\n    output['mean'] = nanmean(data, axis=0)\n    output['sd'] = nanstd(data, axis=0)\n    output['mean_log'] = nanmean(dat_log, axis=0)\n    output['sd_log'] = nanstd(dat_log, axis=0)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the header of a neo file.", "response": "def _read_header(fid):\n    \"\"\"Based on neo/rawio/axonrawio.py, but I only kept of data with no-gaps\n    and in one segment.\n    \"\"\"\n    fid.seek(0, SEEK_SET)\n    fFileSignature = fid.read(4)\n    assert fFileSignature == b'ABF2', 'only format ABF2 is currently supported'\n\n    header = {}\n    for key, offset, fmt in headerDescriptionV2:\n        fid.seek(0 + offset, SEEK_SET)\n        val = unpack(fmt, fid.read(calcsize(fmt)))\n        if len(val) == 1:\n            header[key] = val[0]\n        else:\n            header[key] = val\n\n    # sections\n    sections = {}\n    for s, sectionName in enumerate(sectionNames):\n        fid.seek(76 + s * 16)\n        uBlockIndex, uBytes, llNumEntries = unpack('IIl', fid.read(calcsize('IIl')))\n        sections[sectionName] = {}\n        sections[sectionName]['uBlockIndex'] = uBlockIndex\n        sections[sectionName]['uBytes'] = uBytes\n        sections[sectionName]['llNumEntries'] = llNumEntries\n    header['sections'] = sections\n\n    # strings sections\n    # hack for reading channels names and units\n    fid.seek(sections['StringsSection']['uBlockIndex'] * BLOCKSIZE)\n    big_string = fid.read(sections['StringsSection']['uBytes'])\n    goodstart = -1\n    for key in [b'AXENGN', b'clampex', b'Clampex', b'CLAMPEX', b'axoscope', b'Clampfit']:\n        goodstart = big_string.find(key)\n        if goodstart != -1:\n            break\n    assert goodstart != -1, 'This file does not contain clampex, axoscope or clampfit in the header'\n    big_string = big_string[goodstart:]\n    strings = big_string.split(b'\\x00')\n\n    # ADC sections\n    header['listADCInfo'] = []\n    for i in range(sections['ADCSection']['llNumEntries']):\n        # read ADCInfo\n        fid.seek(sections['ADCSection']['uBlockIndex'] *\n                 BLOCKSIZE + sections['ADCSection']['uBytes'] * i)\n        ADCInfo = _read_info_as_dict(fid, ADCInfoDescription)\n        ADCInfo['ADCChNames'] = strings[ADCInfo['lADCChannelNameIndex'] - 1]\n        ADCInfo['ADCChUnits'] = strings[ADCInfo['lADCUnitsIndex'] - 1]\n        header['listADCInfo'].append(ADCInfo)\n\n    # protocol sections\n    fid.seek(sections['ProtocolSection']['uBlockIndex'] * BLOCKSIZE)\n    header['protocol'] = _read_info_as_dict(fid, protocolInfoDescription)\n    header['sProtocolPath'] = strings[header['uProtocolPathIndex'] - 1]\n\n    # DAC sections\n    header['listDACInfo'] = []\n    for i in range(sections['DACSection']['llNumEntries']):\n        # read DACInfo\n        fid.seek(sections['DACSection']['uBlockIndex'] *\n                 BLOCKSIZE + sections['DACSection']['uBytes'] * i)\n        DACInfo = _read_info_as_dict(fid, DACInfoDescription)\n        DACInfo['DACChNames'] = strings[DACInfo['lDACChannelNameIndex'] - 1]\n        DACInfo['DACChUnits'] = strings[\n            DACInfo['lDACChannelUnitsIndex'] - 1]\n\n        header['listDACInfo'].append(DACInfo)\n\n    \"\"\" Not present in test file. No tests, no code.\n    # tags\n    listTag = []\n    for i in range(sections['TagSection']['llNumEntries']):\n        fid.seek(sections['TagSection']['uBlockIndex'] *\n                 BLOCKSIZE + sections['TagSection']['uBytes'] * i)\n        tag = _read_info_as_dict(fid, TagInfoDescription)\n        listTag.append(tag)\n\n    header['listTag'] = listTag\n\n    # EpochPerDAC  sections\n    # header['dictEpochInfoPerDAC'] is dict of dicts:\n    #  - the first index is the DAC number\n    #  - the second index is the epoch number\n    # It has to be done like that because data may not exist\n    # and may not be in sorted order\n    header['dictEpochInfoPerDAC'] = {}\n    for i in range(sections['EpochPerDACSection']['llNumEntries']):\n        #  read DACInfo\n        fid.seek(sections['EpochPerDACSection']['uBlockIndex'] *\n                 BLOCKSIZE +\n                 sections['EpochPerDACSection']['uBytes'] * i)\n        EpochInfoPerDAC = _read_info_as_dict(fid, EpochInfoPerDACDescription)\n        DACNum = EpochInfoPerDAC['nDACNum']\n        EpochNum = EpochInfoPerDAC['nEpochNum']\n        # Checking if the key exists, if not, the value is empty\n        # so we have to create empty dict to populate\n        if DACNum not in header['dictEpochInfoPerDAC']:\n            header['dictEpochInfoPerDAC'][DACNum] = {}\n\n        header['dictEpochInfoPerDAC'][DACNum][EpochNum] =\\\n            EpochInfoPerDAC\n    \"\"\"\n\n    return header"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef return_hdr(self):\n        with self.filename.open('br') as f:\n            orig = _read_header(f)\n\n        assert orig['protocol']['nOperationMode'] == 3, 'Only continuous no-gap recordings are supported'\n        assert orig['sections']['SynchArraySection']['llNumEntries'] == 0\n        assert orig['sections']['SynchArraySection']['uBlockIndex'] == 0\n\n        # file format\n        if orig['nDataFormat'] == 0:\n            self.dtype = dtype('i2')\n        elif orig['nDataFormat'] == 1:\n            self.dtype = dtype('f4')\n\n        chan_name = []\n        offset = []\n        gain = []\n        for ch in orig['listADCInfo']:\n            chan_name.append(ch['ADCChNames'].decode('utf-8').strip())\n\n            # compute the gain and offset\n            ch_gain = orig['protocol']['lADCResolution'] / orig['protocol']['fADCRange']\n            ch_gain *= (ch['fInstrumentScaleFactor'] *\n                        ch['fSignalGain'] *\n                        ch['fADCProgrammableGain'])\n            if ch['nTelegraphEnable'] == 1:\n                ch_gain *= ch['fTelegraphAdditGain']\n            gain.append(1 / ch_gain)\n\n            offset.append(ch['fInstrumentOffset'] -\n                          ch['fSignalOffset'])\n\n        self.offset = array(offset)[:, newaxis]\n        self.gain = array(gain)[:, newaxis]\n        self.head = orig['sections']['DataSection']['uBlockIndex'] * BLOCKSIZE\n        self.n_chan = orig['sections']['ADCSection']['llNumEntries']\n        assert self.n_chan == len(chan_name)\n        self.n_samples = int(orig['sections']['DataSection']['llNumEntries'] /\n                             self.n_chan)\n\n        subj_id = self.filename.stem\n        try:\n            start_time = (datetime.strptime(str(orig['uFileStartDate']), '%Y%m%d') +\n                          timedelta(seconds=orig['uFileStartTimeMS'] / 1000))\n        except ValueError:  # no time given, use placeholder\n            start_time = DEFAULT_DATETIME\n\n        s_freq = 1.e6 / orig['protocol']['fADCSequenceInterval']\n\n        return subj_id, start_time, s_freq, chan_name, self.n_samples, orig", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the data as 2D numpy. ndarray.", "response": "def return_dat(self, chan, begsam, endsam):\n        \"\"\"Return the data as 2D numpy.ndarray.\n\n        Parameters\n        ----------\n        chan : int or list\n            index (indices) of the channels to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A 2d matrix, with dimension chan X samples. To save memory, the\n            data are memory-mapped, and you cannot change the values on disk.\n\n        Notes\n        -----\n        When asking for an interval outside the data boundaries, it returns NaN\n        for those values.\n        \"\"\"\n        data = memmap(self.filename, dtype=self.dtype, mode='r', order='F',\n                      shape=(self.n_chan, self.n_samples), offset=self.head)\n\n        dat = data[chan, max((begsam, 0)):min((endsam, self.n_samples))].astype(float64)\n        dat = (dat + self.offset[chan, :]) * self.gain[chan, :]\n\n        if begsam < 0:\n\n            pad = empty((dat.shape[0], 0 - begsam))\n            pad.fill(NaN)\n            dat = c_[pad, dat]\n\n        if endsam >= self.n_samples:\n\n            pad = empty((dat.shape[0], endsam - self.n_samples))\n            pad.fill(NaN)\n            dat = c_[dat, pad]\n\n        return dat"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread Settings information either from INI or from default values.", "response": "def read_settings(widget, value_name):\n    \"\"\"Read Settings information, either from INI or from default values.\n\n    Parameters\n    ----------\n    widget : str\n        name of the widget\n    value_name : str\n        name of the value of interest.\n\n    Returns\n    -------\n    multiple types\n        type depends on the type in the default values.\n\n    \"\"\"\n    setting_name = widget + '/' + value_name\n    default_value = DEFAULTS[widget][value_name]\n\n    default_type = type(default_value)\n    if default_type is list:\n        default_type = type(default_value[0])\n\n    val = settings.value(setting_name, default_value, type=default_type)\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_settings(self):\n        bbox = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Apply |\n                                QDialogButtonBox.Cancel)\n        self.idx_ok = bbox.button(QDialogButtonBox.Ok)\n        self.idx_apply = bbox.button(QDialogButtonBox.Apply)\n        self.idx_cancel = bbox.button(QDialogButtonBox.Cancel)\n        bbox.clicked.connect(self.button_clicked)\n\n        page_list = QListWidget()\n        page_list.setSpacing(1)\n        page_list.currentRowChanged.connect(self.change_widget)\n\n        pages = ['General', 'Overview', 'Signals', 'Channels', 'Spectrum',\n                 'Notes', 'Video']\n        for one_page in pages:\n            page_list.addItem(one_page)\n\n        self.stacked = QStackedWidget()\n        self.stacked.addWidget(self.config)\n        self.stacked.addWidget(self.parent.overview.config)\n        self.stacked.addWidget(self.parent.traces.config)\n        self.stacked.addWidget(self.parent.channels.config)\n        self.stacked.addWidget(self.parent.spectrum.config)\n        self.stacked.addWidget(self.parent.notes.config)\n        self.stacked.addWidget(self.parent.video.config)\n\n        hsplitter = QSplitter()\n        hsplitter.addWidget(page_list)\n        hsplitter.addWidget(self.stacked)\n\n        btnlayout = QHBoxLayout()\n        btnlayout.addStretch(1)\n        btnlayout.addWidget(bbox)\n\n        vlayout = QVBoxLayout()\n        vlayout.addWidget(hsplitter)\n        vlayout.addLayout(btnlayout)\n\n        self.setLayout(vlayout)", "response": "Create the widgets for the current version of the current version of the current version of the current version of the version of the current version of the version of the version of the version of the version of the version."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef button_clicked(self, button):\n        if button in (self.idx_ok, self.idx_apply):\n\n            # loop over widgets, to see if they were modified\n            for i_config in range(self.stacked.count()):\n                one_config = self.stacked.widget(i_config)\n\n                if one_config.modified:\n                    lg.debug('Settings for ' + one_config.widget +\n                             ' were modified')\n                    one_config.get_values()\n\n                    if self.parent.info.dataset is not None:\n                        one_config.update_widget()\n                    one_config.modified = False\n\n            if button == self.idx_ok:\n                self.accept()\n\n        if button == self.idx_cancel:\n            self.reject()", "response": "Action when a button was clicked."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_values(self, value_names):\n        output = {}\n        for value_name in value_names:\n            output[value_name] = read_settings(self.widget, value_name)\n\n        return output", "response": "Create a dictionary with the original values from the settings or the defaults."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget values from the GUI and save them in preference file.", "response": "def get_values(self):\n        \"\"\"Get values from the GUI and save them in preference file.\"\"\"\n        for value_name, widget in self.index.items():\n            self.value[value_name] = widget.get_value(self.value[value_name])\n\n            setting_name = self.widget + '/' + value_name\n            settings.setValue(setting_name, self.value[value_name])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs values to the GUI.", "response": "def put_values(self):\n        \"\"\"Put values to the GUI.\n\n        Notes\n        -----\n        In addition, when one small widget has been changed, it calls\n        set_modified, so that we know that the preference widget was modified.\n\n        \"\"\"\n        for value_name, widget in self.index.items():\n            widget.set_value(self.value[value_name])\n            widget.connect(self.set_modified)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget indices based on user - selected values.", "response": "def _get_indices(values, selected, tolerance):\n    \"\"\"Get indices based on user-selected values.\n\n    Parameters\n    ----------\n    values : ndarray (any dtype)\n        values present in the axis.\n    selected : ndarray (any dtype) or tuple or list\n        values selected by the user\n    tolerance : float\n        avoid rounding errors.\n\n    Returns\n    -------\n    idx_data : list of int\n        indices of row/column to select the data\n    idx_output : list of int\n        indices of row/column to copy into output\n\n    Notes\n    -----\n    This function is probably not very fast, but it's pretty robust. It keeps\n    the order, which is extremely important.\n\n    If you use values in the self.axis, you don't need to specify tolerance.\n    However, if you specify arbitrary points, floating point errors might\n    affect the actual values. Of course, using tolerance is much slower.\n\n    Maybe tolerance should be part of Select instead of here.\n\n    \"\"\"\n    idx_data = []\n    idx_output = []\n    for idx_of_selected, one_selected in enumerate(selected):\n\n        if tolerance is None or values.dtype.kind == 'U':\n            idx_of_data = where(values == one_selected)[0]\n        else:\n            idx_of_data = where(abs(values - one_selected) <= tolerance)[0] # actual use min\n\n        if len(idx_of_data) > 0:\n            idx_data.append(idx_of_data[0])\n            idx_output.append(idx_of_selected)\n\n    return idx_data, idx_output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef number_of(self, axis):\n        if axis == 'trial':\n            return len(self.data)\n        else:\n            n_trial = self.number_of('trial')\n            output = empty(n_trial, dtype='int')\n            for i in range(n_trial):\n                output[i] = len(self.axis[axis][i])\n\n            return output", "response": "Return the number of in one axis as generally as possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new instance of Data but does not copy the data necessarily.", "response": "def _copy(self, axis=True, attr=True, data=False):\n        \"\"\"Create a new instance of Data, but does not copy the data\n        necessarily.\n\n        Parameters\n        ----------\n        axis : bool, optional\n            deep copy the axes (default: True)\n        attr : bool, optional\n            deep copy the attributes (default: True)\n        data : bool, optional\n            deep copy the data (default: False)\n\n        Returns\n        -------\n        instance of Data (or ChanTime, ChanFreq, ChanTimeFreq)\n            copy of the data, but without the actual data\n\n        Notes\n        -----\n        It's important that we copy all the relevant information here. If you\n        add new attributes, you should add them here.\n\n        Remember that it deep-copies all the information, so if you copy data\n        the size might become really large.\n        \"\"\"\n        cdata = type(self)()  # create instance of the same class\n\n        cdata.s_freq = self.s_freq\n        cdata.start_time = self.start_time\n\n        if axis:\n            cdata.axis = deepcopy(self.axis)\n        else:\n            cdata_axis = OrderedDict()\n            for axis_name in self.axis:\n                cdata_axis[axis_name] = array([], dtype='O')\n            cdata.axis = cdata_axis\n\n        if attr:\n            cdata.attr = deepcopy(self.attr)\n\n        if data:\n            cdata.data = deepcopy(self.data)\n\n        else:\n            # empty data with the correct number of trials\n            cdata.data = empty(self.number_of('trial'), dtype='O')\n\n        return cdata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport data in other formats.", "response": "def export(self, filename, export_format='FieldTrip', **options):\n        \"\"\"Export data in other formats.\n\n        Parameters\n        ----------\n        filename : path to file\n            file to write\n        export_format : str, optional\n            supported export format is currently FieldTrip, EDF, FIFF, Wonambi,\n            BrainVision\n\n        Notes\n        -----\n        'edf' takes an optional argument \"physical_max\", see write_edf.\n\n        'wonambi' takes an optional argument \"subj_id\", see write_wonambi.\n        wonambi format creates two files, one .won with the dataset info as json\n        file and one .dat with the memmap recordings.\n\n        'brainvision' takes an additional argument (\"markers\") which is a list\n        of dictionaries with fields:\n            \"name\" : str (name of the marker),\n            \"start\" : float (start time in seconds)\n            \"end\" : float (end time in seconds)\n\n        'bids' has an optional argument \"markers\", like in 'brainvision'\n        \"\"\"\n        filename = Path(filename)\n        filename.parent.mkdir(parents=True, exist_ok=True)\n\n        export_format = export_format.lower()\n        if export_format == 'edf':\n            from .ioeeg import write_edf  # avoid circular import\n            write_edf(self, filename, **options)\n\n        elif export_format == 'fieldtrip':\n            from .ioeeg import write_fieldtrip  # avoid circular import\n            write_fieldtrip(self, filename)\n\n        elif export_format == 'mnefiff':\n\n            from .ioeeg import write_mnefiff\n            write_mnefiff(self, filename)\n\n        elif export_format == 'wonambi':\n            from .ioeeg import write_wonambi\n            write_wonambi(self, filename, **options)\n\n        elif export_format == 'brainvision':\n            from .ioeeg import write_brainvision\n            write_brainvision(self, filename, **options)\n\n        elif export_format == 'bids':\n            from .ioeeg import write_bids\n            write_bids(self, filename, **options)\n\n        else:\n            raise ValueError('Cannot export to ' + export_format)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding channels to list l.", "response": "def add_channels_to_list(self, l, add_ref=False):\n        \"\"\"Create list of channels (one for those to plot, one for ref).\n\n        Parameters\n        ----------\n        l : instance of QListWidget\n            one of the two lists (chan_to_plot or ref_chan)\n        \"\"\"\n        l.clear()\n\n        l.setSelectionMode(QAbstractItemView.ExtendedSelection)\n        for chan in self.chan_name:\n            item = QListWidgetItem(chan)\n            l.addItem(item)\n            \n        if add_ref:\n            item = QListWidgetItem('_REF')\n            l.addItem(item)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef highlight_channels(self, l, selected_chan):\n        for row in range(l.count()):\n            item = l.item(row)\n            if item.text() in selected_chan:\n                item.setSelected(True)\n            else:\n                item.setSelected(False)", "response": "Highlight channels in the list of channels."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rereference(self):\n        selectedItems = self.idx_l0.selectedItems()\n\n        chan_to_plot = []\n        for selected in selectedItems:\n            chan_to_plot.append(selected.text())\n        self.highlight_channels(self.idx_l1, chan_to_plot)", "response": "Automatically highlight channels to use as reference based on selected channels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the information about the channel groups.", "response": "def get_info(self):\n        \"\"\"Get the information about the channel groups.\n\n        Returns\n        -------\n        dict\n            information about this channel group\n\n        Notes\n        -----\n        The items in selectedItems() are ordered based on the user's selection\n        (which appears pretty random). It's more consistent to use the same\n        order of the main channel list. That's why the additional for-loop\n        is necessary. We don't care about the order of the reference channels.\n        \"\"\"\n        selectedItems = self.idx_l0.selectedItems()\n        selected_chan = [x.text() for x in selectedItems]\n        chan_to_plot = []\n        for chan in self.chan_name + ['_REF']:\n            if chan in selected_chan:\n                chan_to_plot.append(chan)\n\n        selectedItems = self.idx_l1.selectedItems()\n        ref_chan = []\n        for selected in selectedItems:\n            ref_chan.append(selected.text())\n\n        hp = self.idx_hp.value()\n        if hp == 0:\n            low_cut = None\n        else:\n            low_cut = hp\n\n        lp = self.idx_lp.value()\n        if lp == 0:\n            high_cut = None\n        else:\n            high_cut = lp\n\n        scale = self.idx_scale.value()\n\n        group_info = {'name': self.group_name,\n                      'chan_to_plot': chan_to_plot,\n                      'ref_chan': ref_chan,\n                      'hp': low_cut,\n                      'lp': high_cut,\n                      'scale': float(scale),\n                      'color': self.idx_color\n                      }\n\n        return group_info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_action(self):\n        actions = {}\n\n        act = QAction('Load Montage...', self)\n        act.triggered.connect(self.load_channels)\n        act.setEnabled(False)\n        actions['load_channels'] = act\n\n        act = QAction('Save Montage...', self)\n        act.triggered.connect(self.save_channels)\n        act.setEnabled(False)\n        actions['save_channels'] = act\n\n        self.action = actions", "response": "Create actions related to channel selection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new channel group.", "response": "def new_group(self, checked=False, test_name=None):\n        \"\"\"Create a new channel group.\n\n        Parameters\n        ----------\n        checked : bool\n            comes from QAbstractButton.clicked\n        test_name : str\n            used for testing purposes to avoid modal window\n\n        Notes\n        -----\n        Don't call self.apply() just yet, only if the user wants it.\n        \"\"\"\n        chan_name = self.parent.labels.chan_name\n        if chan_name is None:\n            msg = 'No dataset loaded'\n            self.parent.statusBar().showMessage(msg)\n            lg.debug(msg)\n\n        else:\n            if test_name is None:\n                new_name = QInputDialog.getText(self, 'New Channel Group',\n                                                'Enter Name')\n            else:\n                new_name = [test_name, True]  # like output of getText\n\n            if new_name[1]:\n                s_freq = self.parent.info.dataset.header['s_freq']\n                group = ChannelsGroup(chan_name, new_name[0],\n                                      self.config.value, s_freq)\n                self.tabs.addTab(group, new_name[0])\n                self.tabs.setCurrentIndex(self.tabs.currentIndex() + 1)\n\n                # activate buttons\n                self.button_color.setEnabled(True)\n                self.button_del.setEnabled(True)\n                self.button_apply.setEnabled(True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef color_group(self, checked=False, test_color=None):\n        group = self.tabs.currentWidget()\n        if test_color is None:\n            newcolor = QColorDialog.getColor(group.idx_color)\n        else:\n            newcolor = test_color\n        group.idx_color = newcolor\n\n        self.apply()", "response": "Change the color of the group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply(self):\n        self.read_group_info()\n\n        if self.tabs.count() == 0:\n            # disactivate buttons\n            self.button_color.setEnabled(False)\n            self.button_del.setEnabled(False)\n            self.button_apply.setEnabled(False)\n        else:\n            # activate buttons\n            self.button_color.setEnabled(True)\n            self.button_del.setEnabled(True)\n            self.button_apply.setEnabled(True)\n\n        if self.groups:\n            self.parent.overview.update_position()\n            self.parent.spectrum.update()\n            self.parent.notes.enable_events()\n        else:\n            self.parent.traces.reset()\n            self.parent.spectrum.reset()\n            self.parent.notes.enable_events()", "response": "Apply changes to the plots."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_group_info(self):\n        self.groups = []\n        for i in range(self.tabs.count()):\n            one_group = self.tabs.widget(i).get_info()\n            # one_group['name'] = self.tabs.tabText(i)\n            self.groups.append(one_group)", "response": "Get information about groups directly from the widget."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_channels(self, checked=False, test_name=None):\n        chan_name = self.parent.labels.chan_name\n\n        if self.filename is not None:\n            filename = self.filename\n        elif self.parent.info.filename is not None:\n            filename = (splitext(self.parent.info.filename)[0] +\n                        '_channels.json')\n        else:\n            filename = None\n\n        if test_name is None:\n            filename, _ = QFileDialog.getOpenFileName(self,\n                                                      'Open Channels Montage',\n                                                      filename,\n                                                      'Channels File (*.json)')\n        else:\n            filename = test_name\n\n        if filename == '':\n            return\n\n        self.filename = filename\n        with open(filename, 'r') as outfile:\n            groups = load(outfile)\n\n        s_freq = self.parent.info.dataset.header['s_freq']\n        no_in_dataset = []\n        for one_grp in groups:\n            no_in_dataset.extend(set(one_grp['chan_to_plot']) -\n                                 set(chan_name))\n            chan_to_plot = set(chan_name) & set(one_grp['chan_to_plot'])\n            ref_chan = set(chan_name) & set(one_grp['ref_chan'])\n\n            group = ChannelsGroup(chan_name, one_grp['name'], one_grp, s_freq)\n            group.highlight_channels(group.idx_l0, chan_to_plot)\n            group.highlight_channels(group.idx_l1, ref_chan)\n            self.tabs.addTab(group, one_grp['name'])\n\n        if no_in_dataset:\n            msg = 'Channels not present in the dataset: ' + ', '.join(no_in_dataset)\n            self.parent.statusBar().showMessage(msg)\n            lg.debug(msg)\n\n        self.apply()", "response": "Load channel groups from file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_channels(self, checked=False, test_name=None):\n        self.read_group_info()\n\n        if self.filename is not None:\n            filename = self.filename\n        elif self.parent.info.filename is not None:\n            filename = (splitext(self.parent.info.filename)[0] +\n                        '_channels.json')\n        else:\n            filename = None\n\n        if test_name is None:\n            filename, _ = QFileDialog.getSaveFileName(self,\n                                                      'Save Channels Montage',\n                                                      filename,\n                                                      'Channels File (*.json)')\n        else:\n            filename = test_name\n\n        if filename == '':\n            return\n\n        self.filename = filename\n\n        groups = deepcopy(self.groups)\n        for one_grp in groups:\n            one_grp['color'] = one_grp['color'].rgba()\n\n        with open(filename, 'w') as outfile:\n            dump(groups, outfile, indent=' ')", "response": "Save channel groups to file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset(self):\n        self.filename = None\n        self.groups = []\n\n        self.tabs.clear()\n\n        self.setEnabled(False)\n        self.button_color.setEnabled(False)\n        self.button_del.setEnabled(False)\n        self.button_apply.setEnabled(False)\n        self.action['load_channels'].setEnabled(False)\n        self.action['save_channels'].setEnabled(False)", "response": "Reset all the information of this widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self):\n        self.idx_chan = QComboBox()\n        self.idx_chan.activated.connect(self.display_window)\n\n        self.idx_fig = QGraphicsView(self)\n        self.idx_fig.scale(1, -1)\n\n        layout = QVBoxLayout()\n        layout.addWidget(self.idx_chan)\n        layout.addWidget(self.idx_fig)\n        self.setLayout(layout)\n\n        self.resizeEvent(None)", "response": "Create empty scene for power spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self):\n        self.idx_chan.clear()\n        for chan_name in self.parent.traces.chan:\n            self.idx_chan.addItem(chan_name)\n\n        if self.selected_chan is not None:\n            self.idx_chan.setCurrentIndex(self.selected_chan)\n            self.selected_chan = None", "response": "Update the combobox with the current channel names."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display_window(self):\n        if self.idx_chan.count() == 0:\n            self.update()\n\n        chan_name = self.idx_chan.currentText()\n        lg.debug('Power spectrum for channel ' + chan_name)\n\n        if chan_name:\n            trial = 0\n            data = self.parent.traces.data(trial=trial, chan=chan_name)\n            self.display(data)\n        else:\n            self.scene.clear()", "response": "Read the channel name from QComboBox and plot its spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying the data in the current channel and the current level of the spectrum figure.", "response": "def display(self, data):\n        \"\"\"Make graphicsitem for spectrum figure.\n\n        Parameters\n        ----------\n        data : ndarray\n            1D vector containing the data only\n\n        This function can be called by self.display_window (which reads the\n        data for the selected channel) or by the mouse-events functions in\n        traces (which read chunks of data from the user-made selection).\n        \"\"\"\n        value = self.config.value\n        self.scene = QGraphicsScene(value['x_min'], value['y_min'],\n                                    value['x_max'] - value['x_min'],\n                                    value['y_max'] - value['y_min'])\n        self.idx_fig.setScene(self.scene)\n\n        self.add_grid()\n        self.resizeEvent(None)\n\n        s_freq = self.parent.traces.data.s_freq\n        f, Pxx = welch(data, fs=s_freq,\n                       nperseg=int(min((s_freq, len(data)))))  # force int\n\n        freq_limit = (value['x_min'] <= f) & (f <= value['x_max'])\n\n        if self.config.value['log']:\n            Pxx_to_plot = log(Pxx[freq_limit])\n        else:\n            Pxx_to_plot = Pxx[freq_limit]\n\n        self.scene.addPath(Path(f[freq_limit], Pxx_to_plot),\n                           QPen(QColor(LINE_COLOR), LINE_WIDTH))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding axis and ticks to the figure.", "response": "def add_grid(self):\n        \"\"\"Add axis and ticks to figure.\n\n        Notes\n        -----\n        I know that visvis and pyqtgraphs can do this in much simpler way, but\n        those packages create too large a padding around the figure and this is\n        pretty fast.\n\n        \"\"\"\n        value = self.config.value\n\n        # X-AXIS\n        # x-bottom\n        self.scene.addLine(value['x_min'], value['y_min'],\n                           value['x_min'], value['y_max'],\n                           QPen(QColor(LINE_COLOR), LINE_WIDTH))\n        # at y = 0, dashed\n        self.scene.addLine(value['x_min'], 0,\n                           value['x_max'], 0,\n                           QPen(QColor(LINE_COLOR), LINE_WIDTH, Qt.DashLine))\n        # ticks on y-axis\n        y_high = int(floor(value['y_max']))\n        y_low = int(ceil(value['y_min']))\n        x_length = (value['x_max'] - value['x_min']) / value['x_tick']\n        for y in range(y_low, y_high):\n            self.scene.addLine(value['x_min'], y,\n                               value['x_min'] + x_length, y,\n                               QPen(QColor(LINE_COLOR), LINE_WIDTH))\n        # Y-AXIS\n        # left axis\n        self.scene.addLine(value['x_min'], value['y_min'],\n                           value['x_max'], value['y_min'],\n                           QPen(QColor(LINE_COLOR), LINE_WIDTH))\n        # larger ticks on x-axis every 10 Hz\n        x_high = int(floor(value['x_max']))\n        x_low = int(ceil(value['x_min']))\n        y_length = (value['y_max'] - value['y_min']) / value['y_tick']\n        for x in range(x_low, x_high, 10):\n            self.scene.addLine(x, value['y_min'],\n                               x, value['y_min'] + y_length,\n                               QPen(QColor(LINE_COLOR), LINE_WIDTH))\n        # smaller ticks on x-axis every 10 Hz\n        y_length = (value['y_max'] - value['y_min']) / value['y_tick'] / 2\n        for x in range(x_low, x_high, 5):\n            self.scene.addLine(x, value['y_min'],\n                               x, value['y_min'] + y_length,\n                               QPen(QColor(LINE_COLOR), LINE_WIDTH))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfitting the whole scene in view.", "response": "def resizeEvent(self, event):\n        \"\"\"Fit the whole scene in view.\n\n        Parameters\n        ----------\n        event : instance of Qt.Event\n            not important\n\n        \"\"\"\n        value = self.config.value\n        self.idx_fig.fitInView(value['x_min'],\n                               value['y_min'],\n                               value['x_max'] - value['x_min'],\n                               value['y_max'] - value['y_min'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset widget as new", "response": "def reset(self):\n        \"\"\"Reset widget as new\"\"\"\n        self.idx_chan.clear()\n        if self.scene is not None:\n            self.scene.clear()\n        self.scene = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nslows wave detection based on Massimini et al. 2004.", "response": "def detect_Massimini2004(dat_orig, s_freq, time, opts):\n    \"\"\"Slow wave detection based on Massimini et al., 2004.\n\n    Parameters\n    ----------\n    dat_orig : ndarray (dtype='float')\n        vector with the data for one channel\n    s_freq : float\n        sampling frequency\n    time : ndarray (dtype='float')\n        vector with the time points for each sample\n    opts : instance of 'DetectSlowWave'\n        'det_filt' : dict\n            parameters for 'butter',\n        'duration' : tuple of float\n            min and max duration of SW\n        'min_ptp' : float\n            min peak-to-peak amplitude\n        'trough_duration' : tuple of float\n            min and max duration of first half-wave (trough)\n\n    Returns\n    -------\n    list of dict\n        list of detected SWs\n    float\n        SW density, per 30-s epoch\n\n    References\n    ----------\n    Massimini, M. et al. J Neurosci 24(31) 6862-70 (2004).\n\n    \"\"\"\n    if opts.invert:\n        dat_orig = -dat_orig\n\n    dat_det = transform_signal(dat_orig, s_freq, 'double_butter', \n                               opts.det_filt)\n    above_zero = detect_events(dat_det, 'above_thresh', value=0.)\n\n    sw_in_chan = []\n    if above_zero is not None:\n        troughs = within_duration(above_zero, time, opts.trough_duration)\n        #lg.info('troughs within duration: ' + str(troughs.shape))\n\n        if troughs is not None:\n            troughs = select_peaks(dat_det, troughs, opts.max_trough_amp)\n            #lg.info('troughs deep enough: ' + str(troughs.shape))\n\n            if troughs is not None:\n                events = _add_halfwave(dat_det, troughs, s_freq, opts)\n                #lg.info('SWs high enough: ' + str(events.shape))\n\n                if len(events):\n                    events = within_duration(events, time, opts.duration)\n                    events = remove_straddlers(events, time, s_freq)\n                    #lg.info('SWs within duration: ' + str(events.shape))\n\n                    sw_in_chan = make_slow_waves(events, dat_det, time, s_freq)\n\n    if len(sw_in_chan) == 0:\n        lg.info('No slow wave found')\n\n    return sw_in_chan"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_peaks(data, events, limit):\n    selected = abs(data[events[:, 1]]) >= abs(limit)\n\n    return events[selected, :]", "response": "Select peaks in a single event vector."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a list of dicts for each slow wave based on events of time points and data.", "response": "def make_slow_waves(events, data, time, s_freq):\n    \"\"\"Create dict for each slow wave, based on events of time points.\n\n    Parameters\n    ----------\n    events : ndarray (dtype='int')\n        N x 5 matrix with start, trough, zero, peak, end samples\n    data : ndarray (dtype='float')\n        vector with the data\n    time : ndarray (dtype='float')\n        vector with time points\n    s_freq : float\n        sampling frequency\n\n    Returns\n    -------\n    list of dict\n        list of all the SWs, with information about start,\n        trough_time, zero_time, peak_time, end, duration (s), trough_val,\n        peak_val, peak-to-peak amplitude (signal units), area_under_curve\n        (signal units * s)\n    \"\"\"\n    slow_waves = []\n    for ev in events:\n        one_sw = {'start': time[ev[0]],\n                  'trough_time': time[ev[1]],\n                  'zero_time': time[ev[2]],\n                  'peak_time': time[ev[3]],\n                  'end': time[ev[4] - 1],\n                  'trough_val': data[ev[1]],\n                  'peak_val': data[ev[3]],\n                  'dur': (ev[4] - ev[0]) / s_freq,\n                  'ptp': abs(ev[3] - ev[1])\n                  }\n        slow_waves.append(one_sw)\n\n    return slow_waves"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_halfwave(data, events, s_freq, opts):\n    max_dur = opts.duration[1]\n    if max_dur is None:\n        max_dur = MAXIMUM_DURATION\n    \n    window = int(s_freq * max_dur)\n\n    peak_and_end = zeros((events.shape[0], 2), dtype='int')\n    events = concatenate((events, peak_and_end), axis=1)\n    selected = []\n\n    for ev in events:\n        zero_crossings = where(diff(sign(data[ev[2]:ev[0] + window])))[0]\n        \n        if zero_crossings.any():\n            ev[4] = ev[2] + zero_crossings[0] + 1\n            #lg.info('0cross is at ' + str(ev[4]))\n            \n        else:\n            selected.append(False)\n            #lg.info('no 0cross, rejected')\n            continue\n\n        ev[3] = ev[2] + argmin(data[ev[2]:ev[4]])\n\n        if abs(data[ev[1]] - data[ev[3]]) < opts.min_ptp:\n            selected.append(False)\n            #lg.info('ptp too low, rejected: ' + str(abs(data[ev[1]] - data[ev[3]])))\n            continue\n\n        selected.append(True)\n        #lg.info('SW checks out, accepted! ptp is ' + str(abs(data[ev[1]] - data[ev[3]])))\n\n    return events[selected, :]", "response": "Add a halfwave to a set of events."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self):\n\n        \"\"\" ------ MARKERS ------ \"\"\"\n        tab0 = QTableWidget()\n        self.idx_marker = tab0\n\n        tab0.setColumnCount(3)\n        tab0.horizontalHeader().setStretchLastSection(True)\n        tab0.setSelectionBehavior(QAbstractItemView.SelectRows)\n        tab0.setEditTriggers(QAbstractItemView.NoEditTriggers)\n        go_to_marker = lambda r, c: self.go_to_marker(r, c, 'dataset')\n        tab0.cellDoubleClicked.connect(go_to_marker)\n        tab0.setHorizontalHeaderLabels(['Start', 'Duration', 'Text'])\n\n        \"\"\" ------ SUMMARY ------ \"\"\"\n        tab1 = QWidget()\n        self.idx_eventtype = QComboBox(self)\n        self.idx_stage = QComboBox(self)\n        self.idx_stage.activated.connect(self.get_sleepstage)\n        self.idx_quality = QComboBox(self)\n        self.idx_quality.activated.connect(self.get_quality)\n\n        self.idx_annotations = QPushButton('Load Annotation File...')\n        self.idx_annotations.clicked.connect(self.load_annot)\n        self.idx_rater = QLabel('')\n\n        b0 = QGroupBox('Info')\n        form = QFormLayout()\n        b0.setLayout(form)\n\n        form.addRow('File:', self.idx_annotations)\n        form.addRow('Rater:', self.idx_rater)\n\n        b1 = QGroupBox('Staging')\n        b2 = QGroupBox('Signal quality')\n\n        layout = QVBoxLayout()\n        layout.addWidget(b0)\n        layout.addWidget(b1)\n        layout.addWidget(b2)\n        self.idx_summary = layout\n\n        tab1.setLayout(layout)\n\n        \"\"\" ------ ANNOTATIONS ------ \"\"\"\n        tab2 = QWidget()\n        tab_annot = QTableWidget()\n        self.idx_annot_list = tab_annot\n        delete_row = QPushButton('Delete')\n        delete_row.clicked.connect(self.delete_row)\n\n        scroll = QScrollArea(tab2)\n        scroll.setWidgetResizable(True)\n\n        evttype_group = QGroupBox('Event Types')\n        scroll.setWidget(evttype_group)\n        self.idx_eventtype_scroll = scroll\n\n        tab_annot.setColumnCount(5)\n        tab_annot.setHorizontalHeaderLabels(['Start', 'Duration', 'Text',\n                                             'Type', 'Channel'])\n        tab_annot.horizontalHeader().setStretchLastSection(True)\n        tab_annot.setSelectionBehavior(QAbstractItemView.SelectRows)\n        tab_annot.setEditTriggers(QAbstractItemView.NoEditTriggers)\n        go_to_annot = lambda r, c: self.go_to_marker(r, c, 'annot')\n        tab_annot.cellDoubleClicked.connect(go_to_annot)\n        tab_annot.cellDoubleClicked.connect(self.reset_current_row)\n\n        layout = QVBoxLayout()\n        layout.addWidget(self.idx_eventtype_scroll, stretch=1)\n        layout.addWidget(self.idx_annot_list)\n        layout.addWidget(delete_row)\n        tab2.setLayout(layout)\n\n        \"\"\" ------ TABS ------ \"\"\"\n        self.addTab(tab0, 'Markers')\n        self.addTab(tab1, 'Summary')  # disable\n        self.addTab(tab2, 'Annotations')", "response": "Create the widget layout with all the annotations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate actions associated with Annotations.", "response": "def create_action(self):\n        \"\"\"Create actions associated with Annotations.\"\"\"\n        actions = {}\n\n        act = QAction('New Annotations', self)\n        act.triggered.connect(self.new_annot)\n        actions['new_annot'] = act\n\n        act = QAction('Load Annotations', self)\n        act.triggered.connect(self.load_annot)\n        actions['load_annot'] = act\n\n        act = QAction('Clear Annotations...', self)\n        act.triggered.connect(self.clear_annot)\n        actions['clear_annot'] = act\n\n        act = QAction('New...', self)\n        act.triggered.connect(self.new_rater)\n        actions['new_rater'] = act\n        \n        act = QAction('Rename...', self)\n        act.triggered.connect(self.rename_rater)\n        actions['rename_rater'] = act\n\n        act = QAction('Delete...', self)\n        act.triggered.connect(self.delete_rater)\n        actions['del_rater'] = act\n\n        act = QAction(QIcon(ICON['bookmark']), 'New Bookmark', self)\n        act.setCheckable(True)\n        actions['new_bookmark'] = act\n\n        act = QAction(QIcon(ICON['new_eventtype']), 'New Event Type', self)\n        act.triggered.connect(self.new_eventtype)\n        actions['new_eventtype'] = act\n\n        act = QAction(QIcon(ICON['del_eventtype']), 'Delete Event Type', self)\n        act.triggered.connect(self.delete_eventtype)\n        actions['del_eventtype'] = act\n        \n        act = QAction('Rename Event Type', self)\n        act.triggered.connect(self.rename_eventtype)\n        actions['rename_eventtype'] = act\n\n        act = QAction('New Name', self)\n        act.triggered.connect(self.markers_to_events)\n        actions['m2e_newname'] = act\n        \n        act = QAction('Keep Marker Names', self)\n        act.triggered.connect(partial(self.markers_to_events, True))\n        actions['m2e_keepname'] = act\n        \n        act = QAction('Merge Events...', self)\n        act.triggered.connect(self.parent.show_merge_dialog)\n        actions['merge_events'] = act\n\n        act = QAction(QIcon(ICON['event']), 'Event Mode', self)\n        act.setCheckable(True)\n        actions['new_event'] = act\n\n        uncheck_new_event = lambda: actions['new_event'].setChecked(False)\n        uncheck_new_bookmark = lambda: actions['new_bookmark'].setChecked(False)\n        actions['new_event'].triggered.connect(uncheck_new_bookmark)\n        actions['new_bookmark'].triggered.connect(uncheck_new_event)\n\n        act = {}\n        for one_stage, one_shortcut in zip(STAGE_NAME, STAGE_SHORTCUT):\n            act[one_stage] = QAction('Score as ' + one_stage, self.parent)\n            act[one_stage].setShortcut(one_shortcut)\n            stage_idx = STAGE_NAME.index(one_stage)\n            act[one_stage].triggered.connect(partial(self.get_sleepstage,\n                                                     stage_idx))\n            self.addAction(act[one_stage])\n\n        actions['stages'] = act\n\n        act = {}\n        for one_qual, one_shortcut in zip(QUALIFIERS, QUALITY_SHORTCUT):\n            act[one_qual] = QAction('Score as ' + one_qual, self.parent)\n            act[one_qual].setShortcut(one_shortcut)\n            qual_idx = QUALIFIERS.index(one_qual)\n            act[one_qual].triggered.connect(partial(self.get_quality,\n                                                    qual_idx))\n            self.addAction(act[one_qual])\n\n        actions['quality'] = act\n\n        act = QAction('Set Cycle Start', self)\n        act.setShortcut('Ctrl+[')\n        act.triggered.connect(self.get_cycle_mrkr)\n        actions['cyc_start'] = act\n\n        act = QAction('Set Cycle End', self)\n        act.setShortcut('Ctrl+]')\n        act.triggered.connect(partial(self.get_cycle_mrkr, end=True))\n        actions['cyc_end'] = act\n\n        act = QAction('Remove Cycle Marker', self)\n        act.triggered.connect(self.remove_cycle_mrkr)\n        actions['remove_cyc'] = act\n\n        act = QAction('Clear Cycle Markers', self)\n        act.triggered.connect(self.clear_cycle_mrkrs)\n        actions['clear_cyc'] = act\n\n        act = QAction('Domino', self)\n        act.triggered.connect(partial(self.import_staging, 'domino'))\n        actions['import_domino'] = act\n\n        act = QAction('Alice', self)\n        act.triggered.connect(partial(self.import_staging, 'alice'))\n        actions['import_alice'] = act\n\n        act = QAction('Sandman', self)\n        act.triggered.connect(partial(self.import_staging, 'sandman'))\n        actions['import_sandman'] = act\n\n        act = QAction('RemLogic', self)\n        act.triggered.connect(partial(self.import_staging, 'remlogic'))\n        actions['import_remlogic'] = act\n\n        act = QAction('Compumedics', self)\n        act.triggered.connect(partial(self.import_staging, 'compumedics'))\n        actions['import_compumedics'] = act\n        \n        act = QAction('PRANA', self)\n        act.triggered.connect(partial(self.import_staging, 'prana'))\n        actions['import_prana'] = act\n        \n        act = QAction('DeltaMed', self)\n        act.triggered.connect(partial(self.import_staging, 'deltamed'))\n        actions['import_deltamed'] = act\n\n        act = QAction('FASST', self)\n        act.triggered.connect(self.import_fasst)\n        actions['import_fasst'] = act\n\n        act = QAction('Domino', self)\n        act.triggered.connect(partial(self.import_staging, 'domino', \n                                      as_qual=True))\n        actions['import_domino_qual'] = act\n\n        act = QAction('Alice', self)\n        act.triggered.connect(partial(self.import_staging, 'alice', \n                                      as_qual=True))\n        actions['import_alice_qual'] = act\n\n        act = QAction('Sandman', self)\n        act.triggered.connect(partial(self.import_staging, 'sandman', \n                                      as_qual=True))\n        actions['import_sandman_qual'] = act\n\n        act = QAction('RemLogic', self)\n        act.triggered.connect(partial(self.import_staging, 'remlogic', \n                                      as_qual=True))\n        actions['import_remlogic_qual'] = act\n\n        act = QAction('Compumedics', self)\n        act.triggered.connect(partial(self.import_staging, 'compumedics', \n                                      as_qual=True))\n        actions['import_compumedics_qual'] = act\n        \n        act = QAction('PRANA', self)\n        act.triggered.connect(partial(self.import_staging, 'prana', \n                                      as_qual=True))\n        actions['import_prana_qual'] = act\n        \n        act = QAction('DeltaMed', self)\n        act.triggered.connect(partial(self.import_staging, 'deltamed', \n                                      as_qual=True))\n        actions['import_deltamed_qual'] = act\n        \n        act = QAction('Wonambi', self)\n        act.triggered.connect(partial(self.import_events, 'wonambi'))\n        actions['import_events_wonambi'] = act\n        \n        act = QAction('RemLogic', self)\n        act.triggered.connect(partial(self.import_events, 'remlogic'))\n        actions['import_events_remlogic'] = act\n\n        act = QAction('CSV', self)\n        act.triggered.connect(partial(self.export, xformat='csv'))\n        actions['export_to_csv'] = act\n        \n        act = QAction('RemLogic', self)\n        act.triggered.connect(partial(self.export, xformat='remlogic'))\n        actions['export_to_remlogic'] = act\n        \n        act = QAction('RemLogic FR', self)\n        act.triggered.connect(partial(self.export, xformat='remlogic_fr'))\n        actions['export_to_remlogic_fr'] = act\n\n        act = QAction('Export Events...', self)\n        act.triggered.connect(self.parent.show_export_events_dialog)\n        actions['export_events'] = act\n\n        act = QAction('Spindle...', self)\n        act.triggered.connect(self.parent.show_spindle_dialog)\n        act.setShortcut('Ctrl+Shift+s')\n        act.setEnabled(False)\n        actions['spindle'] = act\n\n        act = QAction('Slow Wave...', self)\n        act.triggered.connect(self.parent.show_slow_wave_dialog)\n        act.setShortcut('Ctrl+Shift+w')\n        act.setEnabled(False)\n        actions['slow_wave'] = act\n\n        act = QAction('Analysis Console', self)\n        act.triggered.connect(self.parent.show_analysis_dialog)\n        act.setShortcut('Ctrl+Shift+a')\n        act.setEnabled(False)\n        actions['analyze'] = act\n\n        act = QAction('Sleep Statistics', self)\n        act.triggered.connect(self.export_sleeps_stats)\n        actions['export_sleepstats'] = act\n\n        self.action = actions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate information about the sleep scoring.", "response": "def update_notes(self, xml_file, new=False):\n        \"\"\"Update information about the sleep scoring.\n\n        Parameters\n        ----------\n        xml_file : str\n            file of the new or existing .xml file\n        new : bool\n            if the xml_file should be a new file or an existing one\n        \"\"\"\n        if new:\n            create_empty_annotations(xml_file, self.parent.info.dataset)\n            self.annot = Annotations(xml_file)\n        else:\n            self.annot = Annotations(xml_file)\n\n        self.enable_events()\n\n        self.parent.create_menubar()\n        self.idx_stage.clear()\n        for one_stage in STAGE_NAME:\n            self.idx_stage.addItem(one_stage)\n        self.idx_stage.setCurrentIndex(-1)\n        self.idx_quality.clear()\n        for one_qual in QUALIFIERS:\n            self.idx_quality.addItem(one_qual)\n        self.idx_quality.setCurrentIndex(-1)\n\n        w1 = self.idx_summary.takeAt(1).widget()\n        w2 = self.idx_summary.takeAt(1).widget()\n        self.idx_summary.removeWidget(w1)\n        self.idx_summary.removeWidget(w2)\n        w1.deleteLater()\n        w2.deleteLater()\n\n        b1 = QGroupBox('Staging')\n\n        layout = QFormLayout()\n        for one_stage in STAGE_NAME:\n            layout.addRow(one_stage, QLabel(''))\n        b1.setLayout(layout)\n        self.idx_summary.addWidget(b1)\n        self.idx_stage_stats = layout\n\n        b2 = QGroupBox('Signal quality')\n\n        layout = QFormLayout()\n        for one_qual in QUALIFIERS:\n            layout.addRow(one_qual, QLabel(''))\n        b2.setLayout(layout)\n        self.idx_summary.addWidget(b2)\n        self.idx_qual_stats = layout\n\n        self.display_notes()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling both slow wave and spindle detection if both annotations and channels are active.", "response": "def enable_events(self):\n        \"\"\"enable slow wave and spindle detection if both\n        annotations and channels are active.\n        \"\"\"\n        if self.annot is not None and self.parent.channels.groups:\n            self.action['spindle'].setEnabled(True)\n            self.action['slow_wave'].setEnabled(True)\n            self.action['analyze'].setEnabled(True)\n        else:\n            self.action['spindle'].setEnabled(False)\n            self.action['slow_wave'].setEnabled(False)\n            self.action['analyze'].setEnabled(False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef display_notes(self):\n        if self.annot is not None:\n            short_xml_file = short_strings(basename(self.annot.xml_file))\n            self.idx_annotations.setText(short_xml_file)\n            # if annotations were loaded without dataset\n            if self.parent.overview.scene is None:\n                self.parent.overview.update()\n\n            if not self.annot.raters:\n                self.new_rater()\n\n            self.idx_rater.setText(self.annot.current_rater)\n            self.display_eventtype()\n            self.update_annotations()\n            self.display_stats()\n            self.epoch_length = self.annot.epoch_length", "response": "Display information about scores and raters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef display_stats(self):\n        for i, one_stage in enumerate(STAGE_NAME):\n            second_in_stage = self.annot.time_in_stage(one_stage)\n            time_in_stage = str(timedelta(seconds=second_in_stage))\n\n            label = self.idx_stage_stats.itemAt(i,\n                                                QFormLayout.FieldRole).widget()\n            label.setText(time_in_stage)\n\n        for i, one_qual in enumerate(QUALIFIERS):\n            second_in_qual = self.annot.time_in_stage(one_qual, attr='quality')\n            time_in_qual = str(timedelta(seconds=second_in_qual))\n\n            label = self.idx_qual_stats.itemAt(i,\n                                               QFormLayout.FieldRole).widget()\n            label.setText(time_in_qual)", "response": "Display summary statistics about duration in each stage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_bookmark(self, time):\n        if self.annot is None:  # remove if buttons are disabled\n            msg = 'No score file loaded'\n            lg.debug(msg)\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error adding bookmark')\n            error_dialog.showMessage(msg)\n            error_dialog.exec()\n            return\n\n        answer = QInputDialog.getText(self, 'New Bookmark',\n                                      'Enter bookmark\\'s name')\n        if answer[1]:\n            name = answer[0]\n            self.annot.add_bookmark(name, time)\n            lg.info('Added Bookmark ' + name + 'at ' + str(time))\n\n        self.update_annotations()", "response": "Run this function when user adds a new bookmark."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_bookmark(self, time):\n        self.annot.remove_bookmark(time=time)\n        self.update_annotations()", "response": "User removes a bookmark from the set of annotations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the markers which are in the dataset.", "response": "def update_dataset_marker(self):\n        \"\"\"Update markers which are in the dataset. It always updates the list\n        of events. Depending on the settings, it might add the markers to\n        overview and traces.\n        \"\"\"\n        start_time = self.parent.overview.start_time\n\n        markers = []\n        if self.parent.info.markers is not None:\n            markers = self.parent.info.markers\n\n        self.idx_marker.clearContents()\n        self.idx_marker.setRowCount(len(markers))\n\n        for i, mrk in enumerate(markers):\n            abs_time = (start_time +\n                        timedelta(seconds=mrk['start'])).strftime('%H:%M:%S')\n            dur = timedelta(seconds=mrk['end'] - mrk['start'])\n            duration = '{0:02d}.{1:03d}'.format(dur.seconds,\n                                                round(dur.microseconds / 1000))\n\n            item_time = QTableWidgetItem(abs_time)\n            item_duration = QTableWidgetItem(duration)\n            item_name = QTableWidgetItem(mrk['name'])\n\n            color = self.parent.value('marker_color')\n            item_time.setForeground(QColor(color))\n            item_duration.setForeground(QColor(color))\n            item_name.setForeground(QColor(color))\n\n            self.idx_marker.setItem(i, 0, item_time)\n            self.idx_marker.setItem(i, 1, item_duration)\n            self.idx_marker.setItem(i, 2, item_name)\n\n        # store information about the time as list (easy to access)\n        marker_start = [mrk['start'] for mrk in markers]\n        marker_end = [mrk['end'] for mrk in markers]\n        self.idx_marker.setProperty('start', marker_start)\n        self.idx_marker.setProperty('end', marker_end)\n\n        if self.parent.traces.data is not None:\n            self.parent.traces.display()\n        self.parent.overview.display_markers()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display_eventtype(self):\n        if self.annot is not None:\n            event_types = sorted(self.annot.event_types, key=str.lower)\n        else:\n            event_types = []\n\n        self.idx_eventtype.clear()\n\n        evttype_group = QGroupBox('Event Types')\n        layout = QVBoxLayout()\n        evttype_group.setLayout(layout)\n        \n        self.check_all_eventtype = check_all = QCheckBox('All event types')\n        check_all.setCheckState(Qt.Checked)\n        check_all.clicked.connect(self.toggle_eventtype)\n        layout.addWidget(check_all)\n\n        self.idx_eventtype_list = []\n        for one_eventtype in event_types:\n            self.idx_eventtype.addItem(one_eventtype)\n            item = QCheckBox(one_eventtype)\n            layout.addWidget(item)\n            item.setCheckState(Qt.Checked)\n            item.stateChanged.connect(self.update_annotations)\n            item.stateChanged.connect(self.toggle_check_all_eventtype)\n            self.idx_eventtype_list.append(item)\n\n        self.idx_eventtype_scroll.setWidget(evttype_group)", "response": "Display the list of event types in the annotations and update widgets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toggle_eventtype(self):\n        check = self.check_all_eventtype.isChecked()\n        \n        for btn in self.idx_eventtype_list:\n            btn.setChecked(check)", "response": "Toggle all event types in event type scroll."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks All event types scroll.", "response": "def toggle_check_all_eventtype(self):\n        \"\"\"Check 'All' if all event types are checked in event type scroll.\"\"\"\n        checklist = asarray([btn.isChecked for btn in self.idx_eventtype_list])\n        \n        if not checklist.all():\n            self.check_all_eventtype.setChecked(False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns which events are present in one time window.", "response": "def get_selected_events(self, time_selection=None):\n        \"\"\"Returns which events are present in one time window.\n\n        Parameters\n        ----------\n        time_selection : tuple of float\n            start and end of the window of interest\n\n        Returns\n        -------\n        list of dict\n            list of events in the window of interest\n        \"\"\"\n        events = []\n        for checkbox in self.idx_eventtype_list:\n            if checkbox.checkState() == Qt.Checked:\n                events.extend(self.annot.get_events(name=checkbox.text(),\n                                                    time=time_selection))\n\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the annotations of the current page.", "response": "def update_annotations(self):\n        \"\"\"Update annotations made by the user, including bookmarks and events.\n        Depending on the settings, it might add the bookmarks to overview and\n        traces.\n        \"\"\"\n        start_time = self.parent.overview.start_time\n\n        if self.parent.notes.annot is None:\n            all_annot = []\n        else:\n            bookmarks = self.parent.notes.annot.get_bookmarks()\n            events = self.get_selected_events()\n\n            all_annot = bookmarks + events\n            all_annot = sorted(all_annot, key=lambda x: x['start'])\n\n        self.idx_annot_list.clearContents()\n        self.idx_annot_list.setRowCount(len(all_annot))\n\n        for i, mrk in enumerate(all_annot):\n            abs_time = (start_time +\n                        timedelta(seconds=mrk['start'])).strftime('%H:%M:%S')\n            dur = timedelta(seconds=mrk['end'] - mrk['start'])\n            duration = '{0:02d}.{1:03d}'.format(dur.seconds,\n                                                round(dur.microseconds / 1000))\n\n            item_time = QTableWidgetItem(abs_time)\n            item_duration = QTableWidgetItem(duration)\n            item_name = QTableWidgetItem(mrk['name'])\n            if mrk in bookmarks:\n                item_type = QTableWidgetItem('bookmark')\n                color = self.parent.value('annot_bookmark_color')\n            else:\n                item_type = QTableWidgetItem('event')\n                color = convert_name_to_color(mrk['name'])\n            chan = mrk['chan']\n            if isinstance(chan, (tuple, list)):\n                chan = ', '.join(chan)\n            item_chan = QTableWidgetItem(chan)\n\n            item_time.setForeground(QColor(color))\n            item_duration.setForeground(QColor(color))\n            item_name.setForeground(QColor(color))\n            item_type.setForeground(QColor(color))\n            item_chan.setForeground(QColor(color))\n\n            self.idx_annot_list.setItem(i, 0, item_time)\n            self.idx_annot_list.setItem(i, 1, item_duration)\n            self.idx_annot_list.setItem(i, 2, item_name)\n            self.idx_annot_list.setItem(i, 3, item_type)\n            self.idx_annot_list.setItem(i, 4, item_chan)\n\n        # store information about the time as list (easy to access)\n        annot_start = [ann['start'] for ann in all_annot]\n        annot_end = [ann['end'] for ann in all_annot]\n        annot_name = [ann['name'] for ann in all_annot]\n        self.idx_annot_list.setProperty('start', annot_start)\n        self.idx_annot_list.setProperty('end', annot_end)\n        self.idx_annot_list.setProperty('name', annot_name)\n\n        if self.parent.traces.data is not None:\n            self.parent.traces.display_annotations()\n        self.parent.overview.display_annotations()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting bookmarks or event from annotations based on row.", "response": "def delete_row(self):\n        \"\"\"Delete bookmarks or event from annotations, based on row.\"\"\"\n        sel_model = self.idx_annot_list.selectionModel()\n        for row in sel_model.selectedRows():\n            i = row.row()\n            start = self.idx_annot_list.property('start')[i]\n            end = self.idx_annot_list.property('end')[i]\n            name = self.idx_annot_list.item(i, 2).text()\n            marker_event = self.idx_annot_list.item(i, 3).text()\n            if marker_event == 'bookmark':\n                self.annot.remove_bookmark(name=name, time=(start, end))\n            else:\n                self.annot.remove_event(name=name, time=(start, end))\n                highlight = self.parent.traces.highlight\n                if highlight:\n                    self.parent.traces.scene.removeItem(highlight)\n                    highlight = None\n                    self.parent.traces.event_sel = None\n\n        self.update_annotations()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef go_to_marker(self, row, col, table_type):\n        if table_type == 'dataset':\n            marker_time = self.idx_marker.property('start')[row]\n            marker_end_time = self.idx_marker.property('end')[row]\n        else:\n            marker_time = self.idx_annot_list.property('start')[row]\n            marker_end_time = self.idx_annot_list.property('end')[row]\n\n        window_length = self.parent.value('window_length')\n        \n        if self.parent.traces.action['centre_event'].isChecked():\n            window_start = (marker_time + marker_end_time - window_length) / 2\n        else:\n            window_start = floor(marker_time / window_length) * window_length\n            \n        self.parent.overview.update_position(window_start)\n        \n        if table_type == 'annot':\n            for annot in self.parent.traces.idx_annot:\n                if annot.marker.x() == marker_time:\n                    self.parent.traces.highlight_event(annot)\n                    break", "response": "Move to point in time marked by the marker."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_row(self, ev_start, ev_end):\n        all_starts = self.idx_annot_list.property('start')\n        all_ends = self.idx_annot_list.property('end')\n        \n        for i, (start, end) in enumerate(zip(all_starts, all_ends)):\n            if start == ev_start and end == ev_end:\n                return i\n        \n        for i, start in enumerate(all_starts):\n            if start == ev_start:\n                return i\n            \n        for i, end in enumerate(all_ends):\n            if end == ev_end:\n                return i\n        \n        raise ValueError", "response": "Highlights an event row in the table from start and end time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sleepstage(self, stage_idx=None):\n        if self.annot is None:  # remove if buttons are disabled\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting sleep stage')\n            error_dialog.showMessage('No score file loaded')\n            error_dialog.exec()\n            return\n\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n\n        if window_length != self.epoch_length:\n            msg = ('Zoom to ' + str(self.epoch_length) + ' (epoch length) ' +\n                   'for sleep scoring.')\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting sleep stage')\n            error_dialog.showMessage(msg)\n            error_dialog.exec()\n            lg.debug(msg)\n            return\n\n        try:\n            self.annot.set_stage_for_epoch(window_start,\n                                           STAGE_NAME[stage_idx])\n\n        except KeyError:\n            msg = ('The start of the window does not correspond to any epoch ' +\n                   'in sleep scoring file.\\n\\n'\n                   'Switch to the appropriate window length in View, then use '\n                   'Navigation --> Line Up with Epoch to line up the window.')\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting sleep stage')\n            error_dialog.showMessage(msg)\n            error_dialog.exec()\n            lg.debug(msg)\n\n        else:\n            lg.debug('User staged ' + str(window_start) + ' as ' +\n                     STAGE_NAME[stage_idx])\n\n            self.set_stage_index()\n            self.parent.overview.mark_stages(window_start, window_length,\n                                             STAGE_NAME[stage_idx])\n            self.display_stats()\n            self.parent.traces.page_next()", "response": "Score the sleep stage using shortcuts or combobox."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the signal qualifier using shortcuts or combobox.", "response": "def get_quality(self, qual_idx=None):\n        \"\"\"Get the signal qualifier, using shortcuts or combobox.\"\"\"\n        if self.annot is None:  # remove if buttons are disabled\n            msg = 'No score file loaded'\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting quality')\n            error_dialog.showMessage(msg)\n            error_dialog.exec()\n            lg.debug(msg)\n            return\n\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n\n        try:\n            self.annot.set_stage_for_epoch(window_start,\n                                           QUALIFIERS[qual_idx],\n                                           attr='quality')\n\n        except KeyError:\n            msg = ('The start of the window does not correspond to any epoch ' +\n                   'in sleep scoring file')\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting quality')\n            error_dialog.showMessage(msg)\n            error_dialog.exec()\n            lg.debug(msg)\n\n        else:\n            lg.debug('User staged ' + str(window_start) + ' as ' +\n                     QUALIFIERS[qual_idx])\n\n            self.set_quality_index()\n            self.parent.overview.mark_quality(window_start, window_length,\n                                              QUALIFIERS[qual_idx])\n            self.display_stats()\n            self.parent.traces.page_next()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmarking cycle start or end.", "response": "def get_cycle_mrkr(self, end=False):\n        \"\"\"Mark cycle start or end.\n\n        Parameters\n        ----------\n        end : bool\n            If True, marks a cycle end; otherwise, it's a cycle start\n        \"\"\"\n        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n\n        try:\n            self.annot.set_cycle_mrkr(window_start, end=end)\n\n        except KeyError:\n            msg = ('The start of the window does not correspond to any epoch '\n                   'in sleep scoring file')\n            self.parent.statusBar().showMessage(msg)\n            lg.debug(msg)\n\n        else:\n            bound = 'start'\n            if end:\n                bound = 'end'\n            lg.info('User marked ' + str(window_start) + ' as cycle ' +\n                    bound)\n\n            self.parent.overview.mark_cycles(window_start, window_length,\n                                             end=end)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving all cycle markers.", "response": "def clear_cycle_mrkrs(self, test=False):\n        \"\"\"Remove all cycle markers.\"\"\"\n        if not test:\n            msgBox = QMessageBox(QMessageBox.Question, 'Clear Cycle Markers',\n                                 'Are you sure you want to remove all cycle '\n                                 'markers for this rater?')\n            msgBox.setStandardButtons(QMessageBox.Yes | QMessageBox.No)\n            msgBox.setDefaultButton(QMessageBox.Yes)\n            response = msgBox.exec_()\n\n            if response == QMessageBox.No:\n                return\n\n        self.annot.clear_cycles()\n\n        self.parent.overview.display()\n        self.parent.overview.display_annotations()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the current stage in combobox.", "response": "def set_stage_index(self):\n        \"\"\"Set the current stage in combobox.\"\"\"\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        stage = self.annot.get_stage_for_epoch(window_start, window_length)\n        #lg.info('winstart: ' + str(window_start) + ', stage: ' + str(stage))\n\n        if stage is None:\n            self.idx_stage.setCurrentIndex(-1)\n        else:\n            self.idx_stage.setCurrentIndex(STAGE_NAME.index(stage))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the current signal quality in combobox.", "response": "def set_quality_index(self):\n        \"\"\"Set the current signal quality in combobox.\"\"\"\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        qual = self.annot.get_stage_for_epoch(window_start, window_length,\n                                              attr='quality')\n        #lg.info('winstart: ' + str(window_start) + ', quality: ' + str(qual))\n\n        if qual is None:\n            self.idx_quality.setCurrentIndex(-1)\n        else:\n            self.idx_quality.setCurrentIndex(QUALIFIERS.index(qual))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naction : create a new file for annotations.", "response": "def new_annot(self):\n        \"\"\"Action: create a new file for annotations.\"\"\"\n        if self.parent.info.filename is None:\n            msg = 'No dataset loaded'\n            self.parent.statusBar().showMessage(msg)\n            lg.debug(msg)\n            return\n\n        filename = splitext(self.parent.info.filename)[0] + '_scores.xml'\n        filename, _ = QFileDialog.getSaveFileName(self, 'Create annotation file',\n                                                  filename,\n                                                  'Annotation File (*.xml)')\n        if filename == '':\n            return\n\n        self.update_notes(filename, True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naction load a file for annotations.", "response": "def load_annot(self):\n        \"\"\"Action: load a file for annotations.\"\"\"\n        if self.parent.info.filename is not None:\n            filename = splitext(self.parent.info.filename)[0] + '_scores.xml'\n        else:\n            filename = None\n\n        filename, _ = QFileDialog.getOpenFileName(self, 'Load annotation file',\n                                                  filename,\n                                                  'Annotation File (*.xml)')\n\n        if filename == '':\n            return\n\n        try:\n            self.update_notes(filename, False)\n        except FileNotFoundError:\n            msg = 'Annotation file not found'\n            self.parent.statusBar().showMessage(msg)\n            lg.info(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\naction clear all the annotations", "response": "def clear_annot(self):\n        \"\"\"Action: clear all the annotations (ask for confirmation first).\"\"\"\n        msgBox = QMessageBox(QMessageBox.Question, 'Clear Annotations',\n                             'Do you want to remove all the annotations?')\n        msgBox.setStandardButtons(QMessageBox.Yes | QMessageBox.No)\n        msgBox.setDefaultButton(QMessageBox.Yes)\n        response = msgBox.exec_()\n\n        if response == QMessageBox.No:\n            return\n\n        self.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_fasst(self, checked=False, test_fasst=None, test_annot=None):\n\n        if self.parent.info.filename is not None:\n            fasst_file = splitext(self.parent.info.filename)[0] + '.mat'\n            annot_file = splitext(self.parent.info.filename)[0] + '_scores.xml'\n        else:\n            fasst_file = annot_file = ''\n\n        if test_fasst is None:\n            fasst_file, _ = QFileDialog.getOpenFileName(self, 'Load FASST score file',\n                                                        fasst_file,\n                                                        'FASST score file (*.mat)')\n        else:\n            fasst_file = test_fasst\n\n        if fasst_file == '':\n            return\n\n        if test_annot is None:\n            annot_file, _ = QFileDialog.getSaveFileName(self, 'Create annotation file',\n                                                        annot_file,\n                                                        'Annotation File (*.xml)')\n        else:\n            annot_file = test_annot\n\n        if annot_file == '':\n            return\n\n        try:\n            create_annotation(annot_file, from_fasst=fasst_file)\n        except BaseException as err:\n            self.parent.statusBar().showMessage(str(err))\n            lg.info(str(err))\n            return\n\n        try:\n            self.update_notes(annot_file, False)\n        except FileNotFoundError:\n            msg = 'Annotation file not found'\n            self.parent.statusBar().showMessage(msg)\n            lg.info(msg)", "response": "Action: import from FASST. mat file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_staging(self, source, staging_start=None, as_qual=False,\n                       test_filename=None, test_rater=None):\n        \"\"\"Action: import an external sleep staging file.\n\n        Parameters\n        ----------\n        source : str\n            Name of program where staging was exported. One of 'alice',\n            'compumedics', 'domino', 'remlogic', 'sandman'.\n        staging_start : datetime, optional\n            Absolute time when staging begins.\n        as_qual : bool\n            if True, scores will be imported as quality\n        \"\"\"\n        if self.annot is None:  # remove if buttons are disabled\n            msg = 'No score file loaded'\n            self.parent.statusBar().showMessage(msg)\n            lg.info(msg)\n            return\n\n        if self.parent.info.dataset is None:\n            msg = 'No dataset loaded'\n            self.parent.statusBar().showMessage(msg)\n            lg.info(msg)\n            return\n\n        record_start = self.parent.info.dataset.header['start_time']\n\n        if test_filename is None:\n            filename, _ = QFileDialog.getOpenFileName(self,\n                                                      'Load staging file',\n                                                      None,\n                                                      'Text File (*.txt)')\n        else:\n            filename = test_filename\n\n        if filename == '':\n            return\n\n        if test_rater is None:\n            rater, ok = QInputDialog.getText(self, 'Import staging',\n                                            'Enter rater name')\n            if not ok:\n                return\n\n            if rater in self.annot.raters and not as_qual:\n                msgBox = QMessageBox(QMessageBox.Question, 'Overwrite staging',\n                                     'Rater %s already exists. \\n \\n'\n                                     'Overwrite %s\\'s sleep staging '\n                                     'with imported staging? Events '\n                                     'and bookmarks will be preserved.'\n                                     % (rater, rater))\n                msgBox.setStandardButtons(QMessageBox.Yes | QMessageBox.No)\n                msgBox.setDefaultButton(QMessageBox.No)\n                response = msgBox.exec_()\n\n                if response == QMessageBox.No:\n                    return\n        else:\n            rater = test_rater\n\n        if source in ['deltamed', 'compumedics']:\n            time_str, ok = QInputDialog.getText(self, 'Staging start time',\n                                                'Enter date and time when '\n                                                'staging \\nbegins, using '\n                                                '24-hour clock. \\n\\nFormat: '\n                                                'YYYY,MM,DD HH:mm:SS')\n            if not ok:\n                return\n\n            try:\n                staging_start = datetime.strptime(time_str,\n                                                  '%Y,%m,%d %H:%M:%S')\n            except (ValueError, TypeError) as e:\n                msg = 'Incorrect formatting for date and time.'\n                self.parent.statusBar().showMessage(msg)\n                lg.info(msg)\n                return\n\n        poor = ['Artefact', 'Artifact']\n        if as_qual:\n            query = 'Which epoch label should be read as Poor quality signal?'\n            poor, ok = QInputDialog.getText(self, 'Import quality', query)\n            poor = [poor]\n            \n            if not ok:\n                return\n        \n        try:\n            unaligned = self.annot.import_staging(filename, source, rater,\n                                                  record_start,\n                                                  staging_start=staging_start,\n                                                  poor=poor, \n                                                  as_qual=as_qual)\n        except FileNotFoundError:\n            msg = 'File not found'\n            self.parent.statusBar().showMessage(msg)\n            lg.info(msg)\n            \n        if unaligned:\n            msg = 'Imported scores are not aligned with existing scores.'\n            self.parent.statusBar().showMessage(msg)\n            lg.info(msg)\n\n        self.display_notes()\n        self.parent.create_menubar()", "response": "Action: Import an external sleep staging file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_rater(self):\n        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        newuser = NewUserDialog(self.parent.value('scoring_window'))\n        answer = newuser.exec_()\n\n        if answer == QDialog.Rejected:\n            return\n\n        rater_name = newuser.rater_name.text()\n\n        if rater_name != '':\n            self.annot.add_rater(rater_name, newuser.epoch_length.value())\n            self.display_notes()\n            self.parent.create_menubar()", "response": "Action: add a new rater."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactioning to select one rater.", "response": "def select_rater(self, rater=False):\n        \"\"\"Action: select one rater.\n\n        Parameters\n        ----------\n        rater : str\n            name of the rater\n        \"\"\"\n        self.annot.get_rater(rater)\n        self.display_notes()\n        self.set_stage_index()\n        self.set_quality_index()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_rater(self):\n        answer = QInputDialog.getText(self, 'Delete Rater',\n                                      'Enter rater\\'s name')\n        if answer[1]:\n            self.annot.remove_rater(answer[0])\n            self.display_notes()\n            self.parent.create_menubar()", "response": "Action: create dialog to delete rater."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_rater(self, test_name=None, test_new_name=None):\n        if test_name and test_new_name:\n            name = test_name, True\n            new_name = test_new_name, True\n        else:\n            name = QInputDialog.getText(self, 'Rename Rater',\n                                        'Enter name of rater to rename.')\n        \n        if name[1]:\n            new_name = QInputDialog.getText(self, 'Rename Rater',\n                                            'Enter new name for rater.')\n            \n            if new_name[1]:\n                self.annot.rename_rater(name[0], new_name[0])\n                self.display_notes()\n                self.parent.create_menubar()", "response": "action: create dialog to rename rater event type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactioning create dialog to add new event type.", "response": "def new_eventtype(self, test_type_str=None):\n        \"\"\"Action: create dialog to add new event type.\"\"\"\n        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        if test_type_str:\n            answer = test_type_str, True\n        else:\n            answer = QInputDialog.getText(self, 'New Event Type',\n                                          'Enter new event\\'s name')\n        if answer[1]:\n            self.annot.add_event_type(answer[0])\n            self.display_eventtype()\n            n_eventtype = self.idx_eventtype.count()\n            self.idx_eventtype.setCurrentIndex(n_eventtype - 1)\n            \n        return answer"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\naction create dialog to delete event type.", "response": "def delete_eventtype(self, test_type_str=None):\n        \"\"\"Action: create dialog to delete event type.\"\"\"\n        if test_type_str:\n            answer = test_type_str, True\n        else:\n            answer = QInputDialog.getText(self, 'Delete Event Type',\n                                          'Enter event\\'s name to delete')\n        if answer[1]:\n            self.annot.remove_event_type(answer[0])\n            self.display_eventtype()\n            self.update_annotations()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rename_eventtype(self, test_name=None, test_new_name=None):\n        if test_name and test_new_name:\n            name = test_name, True\n            new_name = test_new_name, True\n        else:\n            name = QInputDialog.getText(self, 'Rename Event Type',\n                                        'Enter name of event type to rename.')\n        \n        if name[1]:\n            new_name = QInputDialog.getText(self, 'Rename Event Type',\n                                            'Enter new name for event type.')\n            \n            if new_name[1]:\n                self.annot.rename_event_type(name[0], new_name[0])\n                self.display_eventtype()\n                self.update_annotations()", "response": "action: create dialog to rename event type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nactions : add a single event.", "response": "def add_event(self, name, time, chan):\n        \"\"\"Action: add a single event.\"\"\"\n        self.annot.add_event(name, time, chan=chan)\n        self.update_annotations()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nactions : remove single event.", "response": "def remove_event(self, name=None, time=None, chan=None):\n        \"\"\"Action: remove single event.\"\"\"\n        self.annot.remove_event(name=name, time=time, chan=chan)\n        self.update_annotations()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_event_type(self, new_name=None, name=None, time=None, \n                          chan=None):\n        \"\"\"Action: change an event's type.\"\"\"\n        if new_name is None:\n            event_types = self.annot.event_types\n            \n            if name is None:\n                event = self.annot.get_events(time=time, chan=chan)[0]\n                name = event['name']\n                \n            idx_name = event_types.index(name)\n            if idx_name == len(event_types) - 1:\n                new_name = event_types[0]\n            else:\n                new_name = event_types[idx_name + 1]\n                    \n        self.annot.add_event(new_name, time, chan=chan)\n        self.annot.remove_event(name=name, time=time, chan=chan)\n        self.update_annotations()\n        \n        return new_name", "response": "Action : change an event s type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies all markers in dataset to events.", "response": "def markers_to_events(self, keep_name=False):\n        \"\"\"Copy all markers in dataset to event type. \"\"\"\n        markers = self.parent.info.markers\n        \n        if markers is None:\n            self.parent.statusBar.showMessage('No markers in dataset.')\n            return\n        \n        if not keep_name:\n            name, ok = self.new_eventtype()            \n            if not ok:\n                return            \n        else:\n            name = None\n                \n        self.annot.add_events(markers, name=name, chan='')\n        \n        if keep_name:\n            self.display_eventtype()\n            n_eventtype = self.idx_eventtype.count()\n            self.idx_eventtype.setCurrentIndex(n_eventtype - 1)\n        \n        self.update_annotations()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detect_events(self, data, method, params, label):\n        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        lg.info('Adding event type ' + label)\n        self.annot.add_event_type(label)\n        self.display_eventtype()\n        n_eventtype = self.idx_eventtype.count()\n        self.idx_eventtype.setCurrentIndex(n_eventtype - 1)\n\n        if params['max_dur'] in [0, 'None']:\n            params['max_dur'] = None\n\n        freq = (float(params['f1']), float(params['f2']))\n        duration = (params['min_dur'], params['max_dur'])\n\n        if method in SPINDLE_METHODS:\n            detector = DetectSpindle(method=method, frequency=freq,\n                                     duration=duration, merge=params['merge'])\n            detector.rolloff = params['rolloff']\n            detector.min_interval = params['interval']\n\n            if 'Ferrarelli2007' == method:\n                detector.det_thresh = params['0']\n                detector.sel_thresh = params['1']\n                \n            if 'Nir2011' == method:\n                detector.smooth['dur'] = params['0']\n                detector.det_thresh = params['1']\n                detector.sel_thresh = params['2']\n                \n            if 'Moelle2011' == method:\n                detector.moving_rms['dur'] = params['0']\n                detector.smooth['dur'] = params['1']\n                detector.det_thresh = params['2']            \n            \n            if 'Wamsley2012' == method:\n                detector.det_wavelet['dur'] = params['0']\n                detector.det_wavelet['sd'] = params['1']\n                detector.smooth['dur'] = params['2']  \n                detector.det_thresh = params['3']  \n            \n            if 'Martin2013' == method:\n                detector.moving_rms['dur'] = params['0']\n                detector.moving_rms['step'] = params['1']\n                detector.det_thresh = params['2']\n            \n            if 'Ray2015' == method:\n                detector.smooth['dur'] = params['0']\n                detector.zscore['step'] = params['1']\n                detector.det_thresh = params['2']  \n                detector.sel_thresh = params['3']             \n            \n            if 'Lacourse2018' == method:\n                detector.windowing['dur'] = params['0']\n                detector.windowing['step'] = params['1']\n                detector.abs_pow_thresh = params['2']\n                detector.rel_pow_thresh = params['3']\n                detector.covar_thresh = params['4']\n                detector.corr_thresh = params['5']\n                \n            if 'FASST' == method:\n                detector.det_thresh = params['0']\n                detector.smooth['dur'] = params['1']\n                \n            if 'FASST2' == method:\n                detector.det_thresh = params['0']\n                detector.moving_rms['dur'] = params['1']\n                detector.smooth['dur'] = params['2']\n            \n            if 'UCSD' == method:\n                detector.det_wavelet['dur'] = params['0']\n                detector.det_wavelet['width'] = params['1']\n                detector.det_wavelet['win'] = params['2']\n                detector.det_thresh = params['3']  \n                detector.sel_thresh = params['4'] \n                \n            if 'Concordia' == method:\n                detector.moving_rms['dur'] = params['0']\n                detector.smooth['dur'] = params['1']\n                detector.det_thresh = params['2']            \n                detector.det_thresh_hi = params['3']            \n                detector.tolerance = params['4']            \n                detector.sel_thresh = params['5']            \n                \n        elif method in SLOW_WAVE_METHODS:\n            detector = DetectSlowWave(method=method, duration=duration)\n\n            detector.det_filt['freq'] = freq\n            detector.trough_duration = (params['min_trough_dur'],\n                                        params['max_trough_dur'])\n            detector.max_trough_amp = params['max_trough_amp']\n            detector.min_ptp = params['min_ptp']\n            detector.invert = params['invert']\n\n        else:\n            lg.info('Method not recognized: ' + method)\n            return\n\n        events = detector(data, parent=self)\n\n        if events:\n            self.annot.add_events(events, name=label)\n\n        self.update_annotations()", "response": "Detect events and display on signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nactions : export annotations to CSV", "response": "def export(self, xformat='csv'):\n        \"\"\"action: export annotations to CSV.\"\"\"\n        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        if xformat == 'csv':\n            filename = splitext(self.annot.xml_file)[0] + '.csv'\n            filename, _ = QFileDialog.getSaveFileName(self, 'Export stages',\n                                                      filename,\n                                                      'Sleep stages (*.csv)')\n        if 'remlogic' in xformat:\n            filename = splitext(self.annot.xml_file)[0] + '.txt'\n            filename, _ = QFileDialog.getSaveFileName(self, 'Export stages',\n                                                      filename,\n                                                      'Sleep stages (*.txt)')\n        if filename == '':\n            return\n\n        self.annot.export(filename, xformat=xformat)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_events(self, source='wonambi'):\n        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        if 'wonambi' == source:\n            format_str = 'CSV File (*.csv)'\n            rec_start = None\n        \n        elif 'remlogic' == source:\n            format_str = 'Text file (*.txt)'\n            rec_start = self.parent.info.dataset.header['start_time']\n            \n        \n        fn, _ = QFileDialog.getOpenFileName(self, 'Import events',\n                                            None, format_str)\n        \n        if fn == '':\n            return\n        \n        fn = Path(fn).resolve()\n\n        self.annot.import_events(fn, source=source, rec_start=rec_start,\n                                 parent=self.parent)\n        self.display_notes()", "response": "action import events from text file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nactioning : export sleep statistics CSV.", "response": "def export_sleeps_stats(self):\n        \"\"\"action: export sleep statistics CSV.\"\"\"\n        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        fn = splitext(self.annot.xml_file)[0] + '_sleepstats.csv'\n        fn, _ = QFileDialog.getSaveFileName(self, 'Export sleep statistics',\n                                            fn, 'Sleep stats (*.csv)')\n        if fn == '':\n            return\n\n        dt_dialog = DateTimeDialog('Lights OFF', self.annot.start_time,\n                                   self.annot.last_second)\n        if not dt_dialog.exec():\n            return                        \n        lights_off = float(dt_dialog.idx_seconds.value())\n\n        dt_dialog = DateTimeDialog('Lights ON', self.annot.start_time,\n                                   self.annot.last_second)\n        if not dt_dialog.exec():\n            return\n        lights_on = float(dt_dialog.idx_seconds.value())\n\n        if self.annot.export_sleep_stats(fn, lights_off, lights_on) is None:\n            self.parent.statusBar().showMessage('No epochs scored as sleep.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n        self.idx_annotations.setText('Load Annotation File...')\n        self.idx_rater.setText('')\n\n        self.annot = None\n        self.dataset_markers = None\n\n        # remove dataset marker\n        self.idx_marker.clearContents()\n        self.idx_marker.setRowCount(0)\n\n        # remove summary statistics\n        w1 = self.idx_summary.takeAt(1).widget()\n        w2 = self.idx_summary.takeAt(1).widget()\n        self.idx_summary.removeWidget(w1)\n        self.idx_summary.removeWidget(w2)\n        w1.deleteLater()\n        w2.deleteLater()\n        b1 = QGroupBox('Staging')\n        b2 = QGroupBox('Signal quality')\n        self.idx_summary.addWidget(b1)\n        self.idx_summary.addWidget(b2)\n\n        # remove annotations\n        self.display_eventtype()\n        self.update_annotations()\n\n        self.parent.create_menubar()", "response": "Remove all annotations from window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactioning when the user clicks the button.", "response": "def button_clicked(self, button):\n        \"\"\"Action when button was clicked.\n\n        Parameters\n        ----------\n        button : instance of QPushButton\n            which button was pressed\n        \"\"\"\n        if button is self.idx_ok:\n\n            evt_types = [x.text() for x in self.idx_evt_type.selectedItems()]\n            self.merge_to = self.idx_merge_to.currentText()\n            min_interval = self.min_interval.get_value()\n            events = []\n            merge_to_longer = False\n\n            if not evt_types:\n                QMessageBox.warning(self, 'Missing information',\n                                     'Choose at least one event type.')\n                return\n\n            min_interval = 0 if not min_interval else min_interval\n\n            if self.merge_to == 'longer duration event':\n                merge_to_longer = True\n\n            if len(evt_types) > 1:\n                answer = QInputDialog.getText(self, 'New Event Type',\n                                      'Enter new event\\'s name')\n\n                if answer[1]:\n                    name = answer[0]\n\n                else:\n                    return\n\n            else:\n                name = evt_types[0]\n\n            for etype in evt_types:\n                events.extend(self.parent.notes.annot.get_events(name=etype,\n                                                                 qual='Good'))\n\n            if self.cross_chan.get_value():\n                events = merge_close(events, min_interval,\n                                     merge_to_longer=merge_to_longer)\n\n            else:\n                channels = sorted(set([y for x in events for y in x['chan']]))\n                events = []\n                chan_events = []\n\n                for chan in channels:\n                    chan_events = []\n\n                    for etype in evt_types:\n\n                        chan_events.extend(self.parent.notes.annot.get_events(\n                                name=etype, chan=chan, qual='Good'))\n                        events.extend(merge_close(chan_events, min_interval,\n                                              merge_to_longer=merge_to_longer))\n\n            for etype in evt_types:\n                self.parent.notes.annot.remove_event_type(etype)\n\n            self.parent.notes.add_events(events, name=name, chan=None)\n\n            self.parent.notes.display_eventtype()\n            n_eventtype = self.parent.notes.idx_eventtype.count()\n            self.parent.notes.idx_eventtype.setCurrentIndex(n_eventtype - 1)\n\n            self.accept()\n\n        if button is self.idx_cancel:\n            self.reject()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_event_types(self):\n        self.idx_evt_type.clear()\n        self.idx_evt_type.setSelectionMode(QAbstractItemView.ExtendedSelection)\n        event_types = sorted(self.parent.notes.annot.event_types,\n                             key=str.lower)\n\n        for ty in event_types:\n            item = QListWidgetItem(ty)\n            self.idx_evt_type.addItem(item)", "response": "Update event types in event type box."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactioning when button was clicked.", "response": "def button_clicked(self, button):\n        \"\"\"Action when button was clicked.\n\n        Parameters\n        ----------\n        button : instance of QPushButton\n            which button was pressed\n        \"\"\"\n        if button is self.idx_ok:            \n            fn = Path(self.filename)\n            xp_format = self.xp_format.get_value()\n            \n            if self.all_types.get_value():\n                evt_type = self.event_types\n            else:\n                evt_type = [\n                        x.text() for x in self.idx_evt_type.selectedItems()]\n                \n            if 'CSV' == xp_format:\n                                \n                self.parent.notes.annot.export_events(fn, evt_type)\n            \n            elif 'Brain Vision' == xp_format:\n                \n                events = []\n                for et in evt_type:                \n                    events.extend(self.parent.notes.annot.get_events(name=et))\n        \n                if not events:\n                    self.parent.statusBar.showMessage('No events found.')\n                    return\n                \n                events = sorted(events, key=lambda x: x['start'])\n                dataset = self.parent.info.dataset\n                data = ChanTime()\n                data.start_time = dataset.header['start_time']\n                data.s_freq = int(dataset.header['s_freq'])\n                \n                with fn.with_suffix('.vmrk').open('w') as f:\n                    lg.info('Writing to ' + str(fn) + '.vmrk')\n                    f.write(_write_vmrk(data, fn, events))\n                \n            self.accept()\n            \n        if button is self.idx_cancel:\n            self.reject()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the event types list info when dialog is opened.", "response": "def update(self):\n        \"\"\"Update the event types list, info, when dialog is opened.\"\"\"\n        self.filename = self.parent.notes.annot.xml_file\n        \n        self.event_types = self.parent.notes.annot.event_types\n        self.idx_evt_type.clear()\n        for ev in self.event_types:\n            self.idx_evt_type.addItem(ev)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning buttons on and off.", "response": "def toggle_buttons(self):\n        \"\"\"Turn buttons on and off.\"\"\"\n        all_types = self.all_types.get_value()\n        self.idx_evt_type.setEnabled(not all_types)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving as dialog for getting name location of dataset export.", "response": "def save_as(self):\n        \"\"\"Dialog for getting name, location of dataset export.\"\"\"\n        filename = splitext(self.filename)[0]\n        filename, _ = QFileDialog.getSaveFileName(self, 'Export events',\n                                                  filename)\n        if filename == '':\n            return\n\n        self.filename = filename\n        short_filename = short_strings(basename(self.filename))\n        self.idx_filename.setText(short_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport data to FieldTrip.", "response": "def write_fieldtrip(data, filename):\n    \"\"\"Export data to FieldTrip.\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data with only one trial\n    filename : path to file\n        file to export to (include '.mat')\n\n    Notes\n    -----\n    It saves mat file using Version 6 ('-v7') because it relies on scipy.io\n    functions. Therefore it cannot store data larger than 2 GB.\n    \"\"\"\n    n_trl = data.number_of('trial')\n    trial = empty(n_trl, dtype='O')\n    time = empty(n_trl, dtype='O')\n\n    for trl in range(n_trl):\n        trial[trl] = data.data[trl]\n        time[trl] = data.axis['time'][trl]\n\n    ft_data = {'fsample': float(data.s_freq),\n               'label': data.axis['chan'][0].astype('O'),\n               'trial': trial,\n               'time': time,\n               'cfg': 'Converted from wonambi on ' + str(datetime.now()),\n               }\n\n    savemat(filename, {VAR: ft_data})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_hdr(self):\n        # fieldtrip does not have this information\n        orig = dict()\n        subj_id = str()\n        start_time = datetime.fromordinal(1)  # fake\n\n        try:\n            ft_data = loadmat(self.filename, struct_as_record=True,\n                              squeeze_me=True)\n            if VAR not in ft_data:\n                raise KeyError('Save the FieldTrip variable as ''{}'''\n                               ''.format(VAR))\n            ft_data = ft_data[VAR]\n\n            s_freq = ft_data['fsample'].astype('float64').item()\n            n_samples = ft_data['trial'].item().shape[1]\n            chan_name = list(ft_data['label'].item())\n\n        except NotImplementedError:\n\n            with File(self.filename) as f:\n\n                if VAR not in f.keys():\n                    raise KeyError('Save the FieldTrip variable as ''{}'''\n                                   ''.format(VAR))\n\n                s_freq = int(f[VAR]['fsample'].value.squeeze())\n                chan_name = read_hdf5_chan_name(f, f[VAR]['label'])\n\n                n_samples = int(around(f[f[VAR]['trial'][0].item()].shape[0]))\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the data as 2D numpy. ndarray.", "response": "def return_dat(self, chan, begsam, endsam):\n        \"\"\"Return the data as 2D numpy.ndarray.\n\n        Parameters\n        ----------\n        chan : int or list\n            index (indices) of the channels to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A 2d matrix, with dimension chan X samples\n\n        \"\"\"\n        TRL = 0\n        try:\n            ft_data = loadmat(self.filename, struct_as_record=True,\n                              squeeze_me=True)\n            ft_data = ft_data[VAR]\n\n            data = ft_data['trial'].item(TRL)\n\n        except NotImplementedError:\n            from h5py import File\n\n            with File(self.filename) as f:\n                data = f[f[VAR]['trial'][TRL].item()].value.T\n\n        return data[chan, begsam:endsam]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts different names into SI units.", "response": "def _convert_unit(unit):\n    \"\"\"Convert different names into SI units.\n\n    Parameters\n    ----------\n    unit : str\n        unit to convert to SI\n\n    Returns\n    -------\n    str\n        unit in SI format.\n\n    Notes\n    -----\n    SI unit such as mV (milliVolt, mVolt), \u03bcV (microVolt, muV).\n\n    \"\"\"\n    if unit is None:\n        return ''\n\n    prefix = None\n    suffix = None\n    if unit[:5].lower() == 'milli':\n        prefix = 'm'\n        unit = unit[5:]\n    elif unit[:5].lower() == 'micro':\n        prefix = mu\n        unit = unit[5:]\n    elif unit[:2].lower() == 'mu':\n        prefix = mu\n        unit = unit[2:]\n\n    if unit[-4:].lower() == 'volt':\n        suffix = 'V'\n        unit = unit[:-4]\n\n    if prefix is None and suffix is None:\n        unit = unit\n    elif prefix is None and suffix is not None:\n        unit = unit + suffix\n    elif prefix is not None and suffix is None:\n        unit = prefix + unit\n    else:\n        unit = prefix + suffix\n\n    return unit"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetects file format of the channels based on extension.", "response": "def detect_format(filename):\n    \"\"\"Detect file format of the channels based on extension.\n\n    Parameters\n    ----------\n    filename : Path\n        name of the filename\n\n    Returns\n    -------\n    str\n        file format\n    \"\"\"\n    filename = Path(filename)\n\n    if filename.suffix == '.csv':\n        recformat = 'csv'\n    elif filename.suffix == '.sfp':\n        recformat = 'sfp'\n\n    else:\n        recformat = 'unknown'\n\n    return recformat"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assign_region_to_channels(channels, anat, parc_type='aparc', max_approx=3,\n                              exclude_regions=None):\n    \"\"\"Assign a brain region based on the channel location.\n\n    Parameters\n    ----------\n    channels : instance of wonambi.attr.chan.Channels\n        channels to assign regions to\n    anat : instance of wonambi.attr.anat.Freesurfer\n        anatomical information taken from freesurfer.\n    parc_type : str\n        'aparc', 'aparc.a2009s', 'BA', 'BA.thresh', or 'aparc.DKTatlas40'\n        'aparc.DKTatlas40' is only for recent freesurfer versions\n    max_approx : int, optional\n        approximation to define position of the electrode.\n    exclude_regions : list of str or empty list\n        do not report regions if they contain these substrings. None means\n        that it does not exclude any region. For example, to exclude white\n        matter regions and unknown regions you can use\n        exclude_regions=('White', 'WM', 'Unknown')\n\n    Returns\n    -------\n    instance of wonambi.attr.chan.Channels\n        same instance as before, now Chan have attr 'region'\n    \"\"\"\n    for one_chan in channels.chan:\n        one_region, approx = anat.find_brain_region(one_chan.xyz,\n                                                    parc_type,\n                                                    max_approx,\n                                                    exclude_regions)\n        one_chan.attr.update({'region': one_region, 'approx': approx})\n\n    return channels", "response": "Assign a brain region to a set of channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_chan_in_region(channels, anat, region_name):\n    if 'region' not in channels.chan[0].attr.keys():\n        lg.info('Computing region for each channel.')\n        channels = assign_region_to_channels(channels, anat)\n\n    chan_in_region = []\n    for one_chan in channels.chan:\n        if region_name in one_chan.attr['region']:\n            chan_in_region.append(one_chan.label)\n\n    return chan_in_region", "response": "Find which channels are in a specific region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_channel_groups(chan):\n    labels = chan.return_label()\n    group_names = {match('([A-Za-z ]+)\\d+', label).group(1) for label in labels}\n\n    groups = {}\n    for group_name in group_names:\n        groups[group_name] = [label for label in labels if label.startswith(group_name)]\n\n    return groups", "response": "Find all groups of the given channel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an MRI mask around an electrode location.", "response": "def create_sphere_around_elec(xyz, template_mri, distance=8, freesurfer=None):\n    \"\"\"Create an MRI mask around an electrode location,\n\n    Parameters\n    ----------\n    xyz : ndarray\n        3x0 array\n    template_mri : path or str (as path) or nibabel.Nifti\n        (path to) MRI to be used as template\n    distance : float\n        distance in mm between electrode and selected voxels\n    freesurfer : instance of Freesurfer\n        to adjust RAS coordinates, see Notes\n\n    Returns\n    -------\n    3d bool ndarray\n        mask where True voxels are within selected distance to the electrode\n\n    Notes\n    -----\n    Freesurfer uses two coordinate systems: one for volumes (\"RAS\") and one for\n    surfaces (\"tkReg\", \"tkRAS\", and \"Surface RAS\"), so the electrodes might be\n    stored in one of the two systems. If the electrodes are in surface\n    coordinates (f.e. if you can plot surface and electrodes in the same space),\n    then you need to convert the coordinate system. This is done by passing an\n    instance of Freesurfer.\n    \"\"\"\n    if freesurfer is None:\n        shift = 0\n    else:\n        shift = freesurfer.surface_ras_shift\n\n    if isinstance(template_mri, str) or isinstance(template_mri, Path):\n        template_mri = nload(str(template_mri))\n\n    mask = zeros(template_mri.shape, dtype='bool')\n    for vox in ndindex(template_mri.shape):\n        vox_ras = apply_affine(template_mri.affine, vox) - shift\n        if norm(xyz - vox_ras) <= distance:\n            mask[vox] = True\n\n    return mask"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the location in xy for some channels.", "response": "def return_xy(self, labels=None):\n        \"\"\"Returns the location in xy for some channels.\n\n        Parameters\n        ----------\n        labels : list of str, optional\n            the names of the channels.\n\n        Returns\n        -------\n        numpy.ndarray\n            a 2xn vector with the position of a channel.\n\n        Notes\n        -----\n        Simplest implementation. We should at least use project onto a 2D plane\n        \"\"\"\n        xyz = self.return_xyz(labels=labels)\n        xy = asarray(xyz)[:, 1:]\n        return xy"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the location in xy for some channels.", "response": "def return_xyz(self, labels=None):\n        \"\"\"Returns the location in xy for some channels.\n\n        Parameters\n        ----------\n        labels : list of str, optional\n            the names of the channels.\n\n        Returns\n        -------\n        numpy.ndarray\n            a 3xn vector with the position of a channel.\n        \"\"\"\n        all_labels = self.return_label()\n\n        if labels is None:\n            labels = all_labels\n\n        xyz = []\n        for one_label in labels:\n            idx = all_labels.index(one_label)\n            xyz.append(self.chan[idx].xyz)\n\n        return asarray(xyz)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef return_attr(self, attr, labels=None):\n        all_labels = self.return_label()\n\n        if labels is None:\n            labels = all_labels\n\n        all_attr = []\n        for one_label in labels:\n            idx = all_labels.index(one_label)\n            try:\n                all_attr.append(self.chan[idx].attr[attr])\n            except KeyError:\n                possible_attr = ', '.join(self.chan[idx].attr.keys())\n                lg.debug('key \"{}\" not found, '.format(attr) +\n                         'possible keys are {}'.format(possible_attr))\n                all_attr.append(None)\n\n        return all_attr", "response": "returns the attributes for each channel in the channel list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export(self, elec_file):\n        elec_file = Path(elec_file)\n        if elec_file.suffix == '.csv':\n            sep = ', '\n        elif elec_file.suffix == '.sfp':\n            sep = ' '\n\n        with elec_file.open('w') as f:\n            for one_chan in self.chan:\n                values = ([one_chan.label, ] +\n                          ['{:.3f}'.format(x) for x in one_chan.xyz])\n                line = sep.join(values) + '\\n'\n                f.write(line)", "response": "Export channel name and location to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndesigns filter and apply it.", "response": "def filter_(data, axis='time', low_cut=None, high_cut=None, order=4,\n            ftype='butter', Rs=None, notchfreq=50, notchquality=25):\n    \"\"\"Design filter and apply it.\n\n    Parameters\n    ----------\n    ftype : str\n        'butter', 'cheby1', 'cheby2', 'ellip', 'bessel', 'diff', or 'notch'\n    axis : str, optional\n        axis to apply the filter on.\n    low_cut : float, optional\n        (not for notch) low cutoff for high-pass filter\n    high_cut : float, optional\n        (not for notch) high cutoff for low-pass filter\n    order : int, optional\n        (not for notch) filter order\n    data : instance of Data\n        (not for notch) the data to filter.\n    notchfreq : float\n        (only for notch) frequency to apply notch filter to (+ harmonics)\n    notchquality : int\n        (only for notch) Quality factor (see scipy.signal.iirnotch)\n\n    Returns\n    -------\n    filtered_data : instance of DataRaw\n        filtered data\n\n    Notes\n    -----\n    You can specify any filter type as defined by iirfilter.\n\n    If you specify low_cut only, it generates a high-pass filter.\n    If you specify high_cut only, it generates a low-pass filter.\n    If you specify both, it generates a band-pass filter.\n\n    low_cut and high_cut should be given as ratio of the Nyquist. But if you\n    specify s_freq, then the ratio will be computed automatically.\n\n    Raises\n    ------\n    ValueError\n        if the cutoff frequency is larger than the Nyquist frequency.\n    \"\"\"\n    nyquist = data.s_freq / 2.\n\n    btype = None\n    if low_cut is not None and high_cut is not None:\n        if low_cut > nyquist or high_cut > nyquist:\n            raise ValueError('cutoff has to be less than Nyquist '\n                             'frequency')\n        btype = 'bandpass'\n        Wn = (low_cut / nyquist,\n              high_cut / nyquist)\n\n    elif low_cut is not None:\n        if low_cut > nyquist:\n            raise ValueError('cutoff has to be less than Nyquist '\n                             'frequency')\n        btype = 'highpass'\n        Wn = low_cut / nyquist\n\n    elif high_cut is not None:\n        if high_cut > nyquist:\n            raise ValueError('cutoff has to be less than Nyquist '\n                             'frequency')\n        btype = 'lowpass'\n        Wn = high_cut / nyquist\n\n    if btype is None and ftype != 'notch':\n        raise TypeError('You should specify at least low_cut or high_cut')\n\n    if Rs is None:\n        Rs = 40\n\n    if ftype == 'notch':\n        b_a = [iirnotch(w0 / nyquist, notchquality) for w0 in arange(notchfreq, nyquist, notchfreq)]\n\n    else:\n        lg.debug('order {0: 2}, Wn {1}, btype {2}, ftype {3}'\n                 ''.format(order, str(Wn), btype, ftype))\n        b_a = [iirfilter(order, Wn, btype=btype, ftype=ftype, rs=Rs), ]\n\n    fdata = data._copy()\n    for i in range(data.number_of('trial')):\n        x = data.data[i]\n        for b, a in b_a:\n            x = filtfilt(b, a, x, axis=data.index_of(axis))\n        fdata.data[i] = x\n\n    return fdata"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndesign taper and convolve it with the signal.", "response": "def convolve(data, window, axis='time', length=1):\n    \"\"\"Design taper and convolve it with the signal.\n\n    Parameters\n    ----------\n    data : instance of Data\n        the data to filter.\n    window : str\n        one of the windows in scipy, using get_window\n    length : float, optional\n        length of the window\n    axis : str, optional\n        axis to apply the filter on.\n\n    Returns\n    -------\n    instance of DataRaw\n        data after convolution\n\n    Notes\n    -----\n    Most of the code is identical to fftconvolve(axis=data.index_of(axis))\n    but unfortunately fftconvolve in scipy 0.13 doesn't take that argument\n    so we need to redefine it here. It's pretty slow too.\n\n    Taper is normalized such that the integral of the function remains the\n    same even after convolution.\n\n    See Also\n    --------\n    scipy.signal.get_window : function used to create windows\n    \"\"\"\n    taper = get_window(window, int(length * data.s_freq))\n    taper = taper / sum(taper)\n\n    fdata = data._copy()\n    idx_axis = data.index_of(axis)\n\n    for i in range(data.number_of('trial')):\n        orig_dat = data.data[i]\n\n        sel_dim = []\n        i_dim = []\n        dat = empty(orig_dat.shape, dtype=orig_dat.dtype)\n        for i_axis, one_axis in enumerate(data.list_of_axes):\n            if one_axis != axis:\n                i_dim.append(i_axis)\n                sel_dim.append(range(data.number_of(one_axis)[i]))\n\n        for one_iter in product(*sel_dim):\n            # create the numpy indices for one value per dimension,\n            # except for the dimension of interest\n            idx = [[x] for x in one_iter]\n            idx.insert(idx_axis, range(data.number_of(axis)[i]))\n            indices = ix_(*idx)\n\n            d_1dim = squeeze(orig_dat[indices], axis=i_dim)\n\n            d_1dim = fftconvolve(d_1dim, taper, 'same')\n\n            for to_squeeze in i_dim:\n                d_1dim = expand_dims(d_1dim, axis=to_squeeze)\n                dat[indices] = d_1dim\n        fdata.data[0] = dat\n\n    return fdata"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes value between min and max values.", "response": "def normalize(x, min_value, max_value):\n    \"\"\"Normalize value between min and max values.\n    It also clips the values, so that you cannot have values higher or lower\n    than 0 - 1.\"\"\"\n    x = (x - min_value) / (max_value - min_value)\n    return clip(x, 0, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, png_file):\n        with open(png_file, 'wb') as f:\n            f.write(self._repr_png_())", "response": "Save png to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_hdr(self):\n        foldername = Path(self.filename)\n        chan_files = self.chan_files = []\n        hdr = {}\n        hdr['chan_name'] = []\n        hdr['start_time'] = DEFAULT_DATETIME\n        \n        for file in listdir(self.filename):\n            base, suffix = splitext(file)\n            if suffix == '.txt':\n                if base[-3:] == 'hyp':\n                    self.hypno_file = file\n                else:\n                    chan_files.append(foldername / file)\n                    chan_name = base[base.index('_') + 1:]\n                    hdr['chan_name'].append(chan_name)\n                    hdr['subj_id'] = base[:base.index('_')]\n                    \n        if not chan_files:\n            raise FileNotFoundError('No channel found.')\n            return\n        \n        with open(chan_files[0], 'rt') as f:\n            line0 = f.readline()\n            hdr['s_freq'] = int(\n                    line0[line0.index('Rate:') + 5:line0.index('Hz')])\n            \n            for i, _ in enumerate(f):\n                pass\n            hdr['n_samples'] = i\n        \n        output = (hdr['subj_id'], hdr['start_time'], hdr['s_freq'], \n                  hdr['chan_name'], hdr['n_samples'], hdr)\n        \n        return output", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef return_dat(self, chan, begsam, endsam):\n        #n_sam = self.hdr[4]\n        interval = endsam - begsam\n        dat = empty((len(chan), interval))\n        \n        #beg_block = floor((begsam / n_sam) * n_block)\n        #end_block = floor((endsam / n_sam) * n_block)\n                \n        for i, chan in enumerate(chan):\n            k = 0\n            \n            with open(self.chan_files[chan], 'rt') as f:\n                f.readline()\n                \n                for j, datum in enumerate(f):\n                    \n                    if begsam <= j + 1 < endsam:\n                        dat[i, k] = float64(datum)\n                        k += 1\n                        if k == interval:\n                            break\n        \n        # calibration\n        phys_range = self.phys_max - self.phys_min\n        dig_range = self.dig_max - self.dig_min\n        gain = phys_range / dig_range\n        dat *= gain\n        \n        return dat", "response": "Return the data as 2D numpy. ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate timestamps on x - axis every so often.", "response": "def _make_timestamps(start_time, minimum, maximum, steps):\n    \"\"\"Create timestamps on x-axis, every so often.\n\n    Parameters\n    ----------\n    start_time : instance of datetime\n        actual start time of the dataset\n    minimum : int\n        start time of the recording from start_time, in s\n    maximum : int\n        end time of the recording from start_time, in s\n    steps : int\n        how often you want a label, in s\n\n    Returns\n    -------\n    dict\n        where the key is the label and the value is the time point where the\n        label should be placed.\n\n    Notes\n    -----\n    This function takes care that labels are placed at the meaningful time, not\n    at random values.\n    \"\"\"\n    t0 = start_time + timedelta(seconds=minimum)\n    t1 = start_time + timedelta(seconds=maximum)\n\n    t0_midnight = t0.replace(hour=0, minute=0, second=0, microsecond=0)\n\n    d0 = t0 - t0_midnight\n    d1 = t1 - t0_midnight\n\n    first_stamp = ceil(d0.total_seconds() / steps) * steps\n    last_stamp = ceil(d1.total_seconds() / steps) * steps\n\n    stamp_label = []\n    stamp_time = []\n    for stamp in range(first_stamp, last_stamp, steps):\n        stamp_as_datetime = t0_midnight + timedelta(seconds=stamp)\n        stamp_label.append(stamp_as_datetime.strftime('%H:%M'))\n        stamp_time.append(stamp - d0.total_seconds())\n\n    return stamp_label, stamp_time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading full duration and update maximum.", "response": "def update(self, reset=True):\n        \"\"\"Read full duration and update maximum.\n\n        Parameters\n        ----------\n        reset: bool\n            If True, current window start time is reset to 0.\n        \"\"\"\n        if self.parent.info.dataset is not None:\n            # read from the dataset, if available\n            header = self.parent.info.dataset.header\n            maximum = header['n_samples'] / header['s_freq']  # in s\n            self.minimum = 0\n            self.maximum = maximum\n            self.start_time = self.parent.info.dataset.header['start_time']\n\n        elif self.parent.notes.annot is not None:\n            # read from annotations\n            annot = self.parent.notes.annot\n            self.minimum = annot.first_second\n            self.maximum = annot.last_second\n            self.start_time = annot.start_time\n\n        # make it time-zone unaware\n        self.start_time = self.start_time.replace(tzinfo=None)\n\n        if reset:\n            self.parent.value('window_start', 0)  # the only value that is reset\n\n        self.display()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display(self):\n        lg.debug('GraphicsScene is between {}s and {}s'.format(self.minimum,\n                                                               self.maximum))\n\n        x_scale = 1 / self.parent.value('overview_scale')\n        lg.debug('Set scene x-scaling to {}'.format(x_scale))\n\n        self.scale(1 / self.transform().m11(), 1)  # reset to 1\n        self.scale(x_scale, 1)\n\n        self.scene = QGraphicsScene(self.minimum, 0,\n                                    self.maximum,\n                                    TOTAL_HEIGHT)\n        self.setScene(self.scene)\n\n        # reset annotations\n        self.idx_markers = []\n        self.idx_annot = []\n\n        self.display_current()\n\n        for name, pos in BARS.items():\n            item = QGraphicsRectItem(self.minimum, pos['pos0'],\n                                     self.maximum, pos['pos1'])\n            item.setToolTip(pos['tip'])\n            self.scene.addItem(item)\n\n        self.add_timestamps()", "response": "Updates the widgets especially based on length of recordings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd timestamps at the bottom of the overview.", "response": "def add_timestamps(self):\n        \"\"\"Add timestamps at the bottom of the overview.\"\"\"\n        transform, _ = self.transform().inverted()\n\n        stamps = _make_timestamps(self.start_time, self.minimum, self.maximum,\n                                  self.parent.value('timestamp_steps'))\n\n        for stamp, xpos in zip(*stamps):\n            text = self.scene.addSimpleText(stamp)\n            text.setFlag(QGraphicsItem.ItemIgnoresTransformations)\n\n            # set xpos and adjust for text width\n            text_width = text.boundingRect().width() * transform.m11()\n            text.setPos(xpos - text_width / 2, TIME_HEIGHT)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_settings(self):\n        self.display()\n        self.display_markers()\n        if self.parent.notes.annot is not None:\n            self.parent.notes.display_notes()", "response": "After changing the settings we need to recreate the whole image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_position(self, new_position=None):\n        if new_position is not None:\n            lg.debug('Updating position to {}'.format(new_position))\n            self.parent.value('window_start', new_position)\n            self.idx_current.setPos(new_position, 0)\n\n            current_time = (self.start_time +\n                            timedelta(seconds=new_position))\n            msg = 'Current time: ' + current_time.strftime('%H:%M:%S')\n            msg2 = f' ({new_position} seconds from start)'\n            self.parent.statusBar().showMessage(msg + msg2)\n            lg.debug(msg)\n        else:\n            lg.debug('Updating position at {}'\n                     ''.format(self.parent.value('window_start')))\n\n        if self.parent.info.dataset is not None:\n            self.parent.traces.read_data()\n            if self.parent.traces.data is not None:\n                self.parent.traces.display()\n                self.parent.spectrum.display_window()\n\n        if self.parent.notes.annot is not None:\n            self.parent.notes.set_stage_index()\n            self.parent.notes.set_quality_index()\n\n        self.display_current()", "response": "Update the cursor position and much more."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef display_current(self):\n        if self.idx_current in self.scene.items():\n            self.scene.removeItem(self.idx_current)\n\n        item = QGraphicsRectItem(0,\n                                 CURR['pos0'],\n                                 self.parent.value('window_length'),\n                                 CURR['pos1'])\n        # it's necessary to create rect first, and then move it\n        item.setPos(self.parent.value('window_start'), 0)\n        item.setPen(QPen(Qt.lightGray))\n        item.setBrush(QBrush(Qt.lightGray))\n        item.setZValue(-10)\n        self.scene.addItem(item)\n        self.idx_current = item", "response": "Create a rectangle showing the current window."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display_markers(self):\n        for rect in self.idx_markers:\n            self.scene.removeItem(rect)\n        self.idx_markers = []\n\n        markers = []\n        if self.parent.info.markers is not None:\n            if self.parent.value('marker_show'):\n                markers = self.parent.info.markers\n\n        for mrk in markers:\n            rect = QGraphicsRectItem(mrk['start'],\n                                     BARS['markers']['pos0'],\n                                     mrk['end'] - mrk['start'],\n                                     BARS['markers']['pos1'])\n            self.scene.addItem(rect)\n\n            color = self.parent.value('marker_color')\n            rect.setPen(QPen(QColor(color)))\n            rect.setBrush(QBrush(QColor(color)))\n            rect.setZValue(-5)\n            self.idx_markers.append(rect)", "response": "Mark all the markers from the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef display_annotations(self):\n        for rect in self.idx_annot:\n            self.scene.removeItem(rect)\n        self.idx_annot = []\n\n        if self.parent.notes.annot is None:\n            return\n\n        bookmarks = []\n        events = []\n        if self.parent.value('annot_show'):\n            bookmarks = self.parent.notes.annot.get_bookmarks()\n            events = self.parent.notes.get_selected_events()\n\n        annotations = bookmarks + events\n\n        for annot in annotations:\n            rect = QGraphicsRectItem(annot['start'],\n                                     BARS['annot']['pos0'],\n                                     annot['end'] - annot['start'],\n                                     BARS['annot']['pos1'])\n            self.scene.addItem(rect)\n\n            if annot in bookmarks:\n                color = self.parent.value('annot_bookmark_color')\n            if annot in events:\n                color = convert_name_to_color(annot['name'])\n\n            rect.setPen(QPen(QColor(color), LINE_WIDTH))\n            rect.setBrush(QBrush(QColor(color)))\n            rect.setZValue(-5)\n            self.idx_annot.append(rect)\n\n        for epoch in self.parent.notes.annot.epochs:\n            self.mark_stages(epoch['start'],\n                             epoch['end'] - epoch['start'],\n                             epoch['stage'])\n            self.mark_quality(epoch['start'],\n                              epoch['end'] - epoch['start'],\n                              epoch['quality'])\n\n        cycles = self.parent.notes.annot.rater.find('cycles')\n        cyc_starts = [float(mrkr.text) for mrkr in cycles.findall('cyc_start')]\n        cyc_ends = [float(mrkr.text) for mrkr in cycles.findall('cyc_end')]\n\n        for mrkr in cyc_starts:\n            self.mark_cycles(mrkr, 30) # TODO: better width solution\n        for mrkr in cyc_ends:\n            self.mark_cycles(mrkr, 30, end=True)", "response": "Display all the bookmarks and events from annotations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmarking stages only add the new ones.", "response": "def mark_stages(self, start_time, length, stage_name):\n        \"\"\"Mark stages, only add the new ones.\n\n        Parameters\n        ----------\n        start_time : int\n            start time in s of the epoch being scored.\n        length : int\n           duration in s of the epoch being scored.\n        stage_name : str\n            one of the stages defined in global stages.\n        \"\"\"\n        y_pos = BARS['stage']['pos0']\n        current_stage = STAGES.get(stage_name, STAGES['Unknown'])\n\n        # the -1 is really important, otherwise we stay on the edge of the rect\n        old_score = self.scene.itemAt(start_time + length / 2,\n                                      y_pos +\n                                      current_stage['pos0'] +\n                                      current_stage['pos1'] - 1,\n                                      self.transform())\n\n        # check we are not removing the black border\n        if old_score is not None and old_score.pen() == NoPen:\n            lg.debug('Removing old score at {}'.format(start_time))\n            self.scene.removeItem(old_score)\n            self.idx_annot.remove(old_score)\n\n        rect = QGraphicsRectItem(start_time,\n                                 y_pos + current_stage['pos0'],\n                                 length,\n                                 current_stage['pos1'])\n        rect.setPen(NoPen)\n        rect.setBrush(current_stage['color'])\n        self.scene.addItem(rect)\n        self.idx_annot.append(rect)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mark_quality(self, start_time, length, qual_name):\n        y_pos = BARS['quality']['pos0']\n        height = 10\n\n        # the -1 is really important, otherwise we stay on the edge of the rect\n        old_score = self.scene.itemAt(start_time + length / 2,\n                                      y_pos + height - 1,\n                                      self.transform())\n\n        # check we are not removing the black border\n        if old_score is not None and old_score.pen() == NoPen:\n            lg.debug('Removing old score at {}'.format(start_time))\n            self.scene.removeItem(old_score)\n            self.idx_annot.remove(old_score)\n\n        if qual_name == 'Poor':\n            rect = QGraphicsRectItem(start_time, y_pos, length, height)\n            rect.setPen(NoPen)\n            rect.setBrush(Qt.black)\n            self.scene.addItem(rect)\n            self.idx_annot.append(rect)", "response": "Mark signal quality only add the new ones."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mark_cycles(self, start_time, length, end=False):\n        y_pos = STAGES['cycle']['pos0']\n        height = STAGES['cycle']['pos1']\n        color = STAGES['cycle']['color']\n\n        # the -1 is really important, otherwise we stay on the edge of the rect\n        old_rect = self.scene.itemAt(start_time + length / 2,\n                                     y_pos + height - 1,\n                                     self.transform())\n\n        # check we are not removing the black border\n        if old_rect is not None and old_rect.pen() == NoPen:\n            lg.debug('Removing old score at {}'.format(start_time))\n            self.scene.removeItem(old_rect)\n            self.idx_annot.remove(old_rect)\n\n        rect = QGraphicsRectItem(start_time, y_pos, 30, height)\n        rect.setPen(NoPen)\n        rect.setBrush(color)\n        self.scene.addItem(rect)\n        self.idx_annot.append(rect)\n\n        if end:\n            start_time -= 120\n\n        kink_hi = QGraphicsRectItem(start_time, y_pos, 150, 1)\n        kink_hi.setPen(NoPen)\n        kink_hi.setBrush(color)\n        self.scene.addItem(kink_hi)\n        self.idx_annot.append(kink_hi)\n\n        kink_lo = QGraphicsRectItem(start_time, y_pos + height, 150, 1)\n        kink_lo.setPen(NoPen)\n        kink_lo.setBrush(color)\n        self.scene.addItem(kink_lo)\n        self.idx_annot.append(kink_lo)", "response": "Mark a cycle bound only add the new one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmark selected signal from list of start and end times.", "response": "def mark_poi(self, times=None):\n        \"\"\"Mark selected signal, from list of start and end times.\n        \n        Parameters\n        ----------\n        times : list of tuple of float\n            start and end times, in sec form rec start\n        \"\"\"\n        y_pos = BARS['quality']['pos0']\n        height = 5\n        \n        for rect in self.idx_poi:\n            self.scene.removeItem(rect)\n        self.idx_poi = []\n        \n        if not times:\n            return\n        \n        for beg, end in times:\n            rect = QGraphicsRectItem(beg, y_pos, end - beg, height)\n            rect.setPen(NoPen)\n            rect.setBrush(Qt.darkRed)\n            self.scene.addItem(rect)\n            self.idx_poi.append(rect)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\njumping to window when user clicks on overview.", "response": "def mousePressEvent(self, event):\n        \"\"\"Jump to window when user clicks on overview.\n\n        Parameters\n        ----------\n        event : instance of QtCore.QEvent\n            it contains the position that was clicked.\n        \"\"\"\n        if self.scene is not None:\n            x_in_scene = self.mapToScene(event.pos()).x()\n            window_length = self.parent.value('window_length')\n            window_start = int(floor(x_in_scene / window_length) *\n                               window_length)\n            if self.parent.notes.annot is not None:\n                window_start = self.parent.notes.annot.get_epoch_start(\n                        window_start)\n            self.update_position(window_start)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the widget and clear the scene.", "response": "def reset(self):\n        \"\"\"Reset the widget, and clear the scene.\"\"\"\n        self.minimum = None\n        self.maximum = None\n        self.start_time = None  # datetime, absolute start time\n\n        self.idx_current = None\n        self.idx_markers = []\n        self.idx_annot = []\n\n        if self.scene is not None:\n            self.scene.clear()\n        self.scene = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare_colors(color, values, limits_c, colormap, alpha, chan=None):\n    if values is not None:\n        if limits_c is None:\n            limits_c = array([-1, 1]) * nanmax(abs(values))\n\n        norm_values = normalize(values, *limits_c)\n\n        cm = get_colormap(colormap)\n        colors = cm[norm_values]\n\n    elif color is not None:\n        colors = ColorArray(color)\n\n    else:\n        cm = get_colormap('hsl')\n        group_idx = _chan_groups_to_index(chan)\n        colors = cm[group_idx]\n\n    if alpha is not None:\n        colors.alpha = alpha\n\n    return colors, limits_c", "response": "Prepare the colors for all the channels based on various inputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new surface to the visualization.", "response": "def add_surf(self, surf, color=SKIN_COLOR, vertex_colors=None,\n                 values=None, limits_c=None, colormap=COLORMAP, alpha=1,\n                 colorbar=False):\n        \"\"\"Add surfaces to the visualization.\n\n        Parameters\n        ----------\n        surf : instance of wonambi.attr.anat.Surf\n            surface to be plotted\n        color : tuple or ndarray, optional\n            4-element tuple, representing RGB and alpha, between 0 and 1\n        vertex_colors : ndarray\n            ndarray with n vertices x 4 to specify color of each vertex\n        values : ndarray, optional\n            vector with values for each vertex\n        limits_c : tuple of 2 floats, optional\n            min and max values to normalize the color\n        colormap : str\n            one of the colormaps in vispy\n        alpha : float\n            transparency (1 = opaque)\n        colorbar : bool\n            add a colorbar at the back of the surface\n        \"\"\"\n        colors, limits = _prepare_colors(color=color, values=values,\n                                         limits_c=limits_c, colormap=colormap,\n                                         alpha=alpha)\n\n        # meshdata uses numpy array, in the correct dimension\n        vertex_colors = colors.rgba\n        if vertex_colors.shape[0] == 1:\n            vertex_colors = tile(vertex_colors, (surf.n_vert, 1))\n\n        meshdata = MeshData(vertices=surf.vert, faces=surf.tri,\n                            vertex_colors=vertex_colors)\n        mesh = SurfaceMesh(meshdata)\n\n        self._add_mesh(mesh)\n\n        # adjust camera\n        surf_center = mean(surf.vert, axis=0)\n        if surf_center[0] < 0:\n            azimuth = 270\n        else:\n            azimuth = 90\n        self._view.camera.azimuth = azimuth\n        self._view.camera.center = surf_center\n\n        self._surf.append(mesh)\n\n        if colorbar:\n            self._view.add(_colorbar_for_surf(colormap, limits))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_chan(self, chan, color=None, values=None, limits_c=None,\n                 colormap=CHAN_COLORMAP, alpha=None, colorbar=False):\n        \"\"\"Add channels to visualization\n\n        Parameters\n        ----------\n        chan : instance of Channels\n            channels to plot\n        color : tuple\n            3-, 4-element tuple, representing RGB and alpha, between 0 and 1\n        values : ndarray\n            array with values for each channel\n        limits_c : tuple of 2 floats, optional\n            min and max values to normalize the color\n        colormap : str\n            one of the colormaps in vispy\n        alpha : float\n            transparency (0 = transparent, 1 = opaque)\n        colorbar : bool\n            add a colorbar at the back of the surface\n        \"\"\"\n        # reuse previous limits\n        if limits_c is None and self._chan_limits is not None:\n            limits_c = self._chan_limits\n\n        chan_colors, limits = _prepare_colors(color=color, values=values,\n                                              limits_c=limits_c,\n                                              colormap=colormap, alpha=alpha,\n                                              chan=chan)\n\n        self._chan_limits = limits\n\n        xyz = chan.return_xyz()\n        marker = Markers()\n        marker.set_data(pos=xyz, size=CHAN_SIZE, face_color=chan_colors)\n        self._add_mesh(marker)\n\n        if colorbar:\n            self._view.add(_colorbar_for_surf(colormap, limits))", "response": "Add a channel to the visualization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining the selection of the items in the base base.", "response": "def select(data, trial=None, invert=False, **axes_to_select):\n    \"\"\"Define the selection of trials, using ranges or actual values.\n\n    Parameters\n    ----------\n    data : instance of Data\n        data to select from.\n    trial : list of int or ndarray (dtype='i'), optional\n        index of trials of interest\n    **axes_to_select, optional\n        Values need to be tuple or list. If the values in one axis are string,\n        then you need to specify all the strings that you want. If the values\n        are numeric, then you should specify the range (you cannot specify\n        single values, nor multiple values). To select only up to one point,\n        you can use (None, value_of_interest)\n    invert : bool\n        take the opposite selection\n\n    Returns\n    -------\n    instance, same class as input\n        data where selection has been applied.\n    \"\"\"\n    if trial is not None and not isinstance(trial, Iterable):\n        raise TypeError('Trial needs to be iterable.')\n\n    for axis_to_select, values_to_select in axes_to_select.items():\n        if (not isinstance(values_to_select, Iterable) or\n           isinstance(values_to_select, str)):\n            raise TypeError(axis_to_select + ' needs to be iterable.')\n\n    if trial is None:\n        trial = range(data.number_of('trial'))\n    else:\n        trial = trial\n        if invert:\n            trial = setdiff1d(range(data.number_of('trial')), trial)\n\n    # create empty axis\n    output = data._copy(axis=False)\n    for one_axis in output.axis:\n        output.axis[one_axis] = empty(len(trial), dtype='O')\n    output.data = empty(len(trial), dtype='O')\n\n    to_select = {}\n    for cnt, i in enumerate(trial):\n        lg.debug('Selection on trial {0: 6}'.format(i))\n        for one_axis in output.axis:\n            values = data.axis[one_axis][i]\n\n            if one_axis in axes_to_select.keys():\n                values_to_select = axes_to_select[one_axis]\n\n                if len(values_to_select) == 0:\n                    selected_values = ()\n\n                elif isinstance(values_to_select[0], str):\n                    selected_values = asarray(values_to_select, dtype='U')\n\n                else:\n                    if (values_to_select[0] is None and\n                       values_to_select[1] is None):\n                        bool_values = ones(len(values), dtype=bool)\n                    elif values_to_select[0] is None:\n                        bool_values = values < values_to_select[1]\n                    elif values_to_select[1] is None:\n                        bool_values = values_to_select[0] <= values\n                    else:\n                        bool_values = ((values_to_select[0] <= values) &\n                                       (values < values_to_select[1]))\n                    selected_values = values[bool_values]\n\n                if invert:\n                    selected_values = setdiff1d(values, selected_values)\n\n                lg.debug('In axis {0}, selecting {1: 6} '\n                         'values'.format(one_axis,\n                                         len(selected_values)))\n                to_select[one_axis] = selected_values\n            else:\n                lg.debug('In axis ' + one_axis + ', selecting all the '\n                         'values')\n                selected_values = data.axis[one_axis][i]\n\n            output.axis[one_axis][cnt] = selected_values\n\n        output.data[cnt] = data(trial=i, **to_select)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resample(data, s_freq=None, axis='time', ftype='fir', n=None):\n    output = data._copy()\n    ratio = int(data.s_freq / s_freq)\n\n    for i in range(data.number_of('trial')):\n        output.data[i] = decimate(data.data[i], ratio,\n                                  axis=data.index_of(axis),\n                                  zero_phase=True)\n\n        n_samples = output.data[i].shape[data.index_of(axis)]\n        output.axis[axis][i] = linspace(data.axis[axis][i][0],\n                                        data.axis[axis][i][-1] +\n                                        1 / data.s_freq,\n                                        n_samples)\n\n    output.s_freq = s_freq\n\n    return output", "response": "Downsample the data after applying a filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new segment list for analysis.", "response": "def fetch(dataset, annot, cat=(0, 0, 0, 0), evt_type=None, stage=None,\n          cycle=None, chan_full=None, epoch=None, epoch_dur=30,\n          epoch_overlap=0, epoch_step=None, reject_epoch=False,\n          reject_artf=False, min_dur=0, buffer=0):\n    \"\"\"Create instance of Segments for analysis, complete with info about\n    stage, cycle, channel, event type. Segments contains only metadata until\n    .read_data is called.\n\n    Parameters\n    ----------\n    dataset : instance of Dataset\n        info about record\n    annot : instance of Annotations\n        scoring info\n    cat : tuple of int\n        Determines where the signal is concatenated.\n        If cat[0] is 1, cycles will be concatenated.\n        If cat[1] is 1, different stages will be concatenated.\n        If cat[2] is 1, discontinuous signal within a same condition\n        (stage, cycle, event type) will be concatenated.\n        If cat[3] is 1, events of different types will be concatenated.\n        0 in any position indicates no concatenation.\n    evt_type: list of str, optional\n        Enter a list of event types to get events; otherwise, epochs will\n        be returned.\n    stage: list of str, optional\n        Stage(s) of interest. If None, stage is ignored.\n    cycle: list of tuple of two float, optional\n        Cycle(s) of interest, as start and end times in seconds from record\n        start. If None, cycles are ignored.\n    chan_full: list of str or None\n        Channel(s) of interest, only used for events (epochs have no\n        channel). Channel format is 'chan_name (group_name)'.\n        If used for epochs, separate segments will be returned for each \n        channel; this is necessary for channel-specific artefact removal (see\n        reject_artf below). If None, channel is ignored.\n    epoch : str, optional\n        If 'locked', returns epochs locked to staging. If 'unlocked', divides\n        signal (with specified concatenation) into epochs of duration epoch_dur\n        starting at first sample of every segment and discarding any remainder.\n        If None, longest run of signal is returned.\n    epoch_dur : float\n        only for epoch='unlocked'. Duration of epochs returned, in seconds.\n    epoch_overlap : float\n        only for epoch='unlocked'. Ratio of overlap between two consecutive\n        segments. Value between 0 and 1. Overriden by step.\n    epoch_step : float\n        only for epoch='unlocked'. Time between consecutive epoch starts, in\n        seconds. Overrides epoch_overlap/\n    reject_epoch: bool\n        If True, epochs marked as 'Poor' quality or staged as 'Artefact' will\n        be rejected (and the signal segmented in consequence). Has no effect on\n        event selection.\n    reject_artf : bool\n        If True, excludes events marked as 'Artefact'. If chan_full is \n        specified, only artefacts marked on a given channel are removed from \n        that channel. Signal is segmented in consequence. \n        If None, Artefact events are ignored.\n    min_dur : float\n        Minimum duration of segments returned, in seconds.\n    buffer : float\n        adds this many seconds of signal before and after each segment\n\n    Returns\n    -------\n    instance of Segments\n        metadata for all analysis segments\n    \"\"\"\n    bundles = get_times(annot, evt_type=evt_type, stage=stage, cycle=cycle,\n                        chan=chan_full, exclude=reject_epoch, buffer=buffer)\n\n    # Remove artefacts\n    if reject_artf and bundles:\n        for bund in bundles:\n            bund['times'] = remove_artf_evts(bund['times'], annot, \n                bund['chan'], min_dur=0)\n\n    # Divide bundles into segments to be concatenated\n    if bundles:\n\n        if 'locked' == epoch:\n            bundles = _divide_bundles(bundles)\n\n        elif 'unlocked' == epoch:\n\n            if epoch_step is not None:\n                step = epoch_step\n            else:\n                step = epoch_dur - (epoch_dur * epoch_overlap)\n\n            bundles = _concat(bundles, cat)\n            bundles = _find_intervals(bundles, epoch_dur, step)\n\n        elif not epoch:\n            bundles = _concat(bundles, cat)\n\n        # Minimum duration\n        bundles = _longer_than(bundles, min_dur)\n    \n    segments = Segments(dataset)\n    segments.segments = bundles\n\n    return segments"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_times(annot, evt_type=None, stage=None, cycle=None, chan=None,\n              exclude=False, buffer=0):\n    \"\"\"Get start and end times for selected segments of data, bundled\n    together with info.\n\n    Parameters\n    ----------\n    annot: instance of Annotations\n        The annotation file containing events and epochs\n    evt_type: list of str, optional\n        Enter a list of event types to get events; otherwise, epochs will\n        be returned.\n    stage: list of str, optional\n        Stage(s) of interest. If None, stage is ignored.\n    cycle: list of tuple of two float, optional\n        Cycle(s) of interest, as start and end times in seconds from record\n        start. If None, cycles are ignored.\n    chan: list of str or tuple of None\n        Channel(s) of interest. Channel format is 'chan_name (group_name)'.\n        If None, channel is ignored.\n    exclude: bool\n        Exclude epochs by quality. If True, epochs marked as 'Poor' quality\n        or staged as 'Artefact' will be rejected (and the signal segmented\n        in consequence). Has no effect on event selection.\n    buffer : float\n        adds this many seconds of signal before and after each segment\n\n    Returns\n    -------\n    list of dict\n        Each dict has times (the start and end times of each segment, as\n        list of tuple of float), stage, cycle, chan, name (event type,\n        if applicable)\n\n    Notes\n    -----\n    This function returns epoch or event start and end times, bundled\n    together according to the specified parameters.\n    Presently, setting exclude to True does not exclude events found in Poor\n    signal epochs. The rationale is that events would never be marked in Poor\n    signal epochs. If they were automatically detected, these epochs would\n    have been left out during detection. If they were manually marked, then\n    it must have been Good signal. At the moment, in the GUI, the exclude epoch\n    option is disabled when analyzing events, but we could fix the code if we\n    find a use case for rejecting events based on the quality of the epoch\n    signal.\n    \"\"\"\n    getter = annot.get_epochs\n    last = annot.last_second\n\n    if stage is None:\n        stage = (None,)\n    if cycle is None:\n        cycle = (None,)\n    if chan is None:\n        chan = (None,)\n    if evt_type is None:\n        evt_type = (None,)\n    elif isinstance(evt_type[0], str):\n        getter = annot.get_events\n        if chan != (None,):\n            chan.append('') # also retrieve events marked on all channels\n    else:\n        lg.error('Event type must be list/tuple of str or None')\n\n    qual = None\n    if exclude:\n        qual = 'Good'\n\n    bundles = []\n    for et in evt_type:\n\n        for ch in chan:\n\n            for cyc in cycle:\n\n                for ss in stage:\n\n                    st_input = ss\n                    if ss is not None:\n                        st_input = (ss,)\n\n                    evochs = getter(name=et, time=cyc, chan=(ch,),\n                                    stage=st_input, qual=qual)\n                    if evochs:\n                        times = [(\n                                max(e['start'] - buffer, 0), \n                                min(e['end'] + buffer, last)) for e in evochs]\n                        times = sorted(times, key=lambda x: x[0])\n                        one_bundle = {'times': times,\n                                      'stage': ss,\n                                      'cycle': cyc,\n                                      'chan': ch,\n                                      'name': et}\n                        bundles.append(one_bundle)\n\n    return bundles", "response": "Get start and end times for selected segments of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _longer_than(segments, min_dur):\n    if min_dur <= 0.:\n        return segments\n\n    long_enough = []\n    for seg in segments:\n\n        if sum([t[1] - t[0] for t in seg['times']]) >= min_dur:\n            long_enough.append(seg)\n\n    return long_enough", "response": "Remove segments longer than min_dur."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _concat(bundles, cat=(0, 0, 0, 0)):\n    chan = sorted(set([x['chan'] for x in bundles]))\n    cycle = sorted(set([x['cycle'] for x in bundles]))\n    stage = sorted(set([x['stage'] for x in bundles]))\n    evt_type = sorted(set([x['name'] for x in bundles]))\n\n    all_cycle = None\n    all_stage = None\n    all_evt_type = None\n\n    if cycle[0] is not None:\n        all_cycle = ', '.join([str(c) for c in cycle])\n    if stage[0] is not None:\n        all_stage = ', '.join(stage)\n    if evt_type[0] is not None:\n        all_evt_type = ', '.join(evt_type)\n\n    if cat[0]:\n        cycle = [all_cycle]\n\n    if cat[1]:\n        stage = [all_stage]\n\n    if cat[3]:\n        evt_type = [all_evt_type]\n\n    to_concat = []\n    for ch in chan:\n\n        for cyc in cycle:\n\n            for st in stage:\n\n                for et in evt_type:\n                    new_times = []\n\n                    for bund in bundles:\n                        chan_cond = ch == bund['chan']\n                        cyc_cond = cyc in (bund['cycle'], all_cycle)\n                        st_cond = st in (bund['stage'], all_stage)\n                        et_cond = et in (bund['name'], all_evt_type)\n\n                        if chan_cond and cyc_cond and st_cond and et_cond:\n                            new_times.extend(bund['times'])\n\n                    new_times = sorted(new_times, key=lambda x: x[0])\n                    new_bund = {'times': new_times,\n                              'chan': ch,\n                              'cycle': cyc,\n                              'stage': st,\n                              'name': et\n                              }\n                    to_concat.append(new_bund)\n\n    if not cat[2]:\n        to_concat_new = []\n\n        for bund in to_concat:\n            last = None\n            bund['times'].append((inf,inf))\n            start = 0\n\n            for i, j in enumerate(bund['times']):\n\n                if last is not None:\n                    if not isclose(j[0], last, abs_tol=0.01):\n                        new_times = bund['times'][start:i]\n                        new_bund = bund.copy()\n                        new_bund['times'] = new_times\n                        to_concat_new.append(new_bund)\n                        start = i\n                last = j[1]\n\n        to_concat = to_concat_new\n\n    to_concat = [x for x in to_concat if x['times']]\n\n    return to_concat", "response": "Prepare event or epoch start and end times for concatenation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _divide_bundles(bundles):\n    divided = []\n\n    for bund in bundles:\n        for t in bund['times']:\n            new_bund = bund.copy()\n            new_bund['times'] = [t]\n            divided.append(new_bund)\n\n    return divided", "response": "Take each subsegment inside a bundle and put it in its own bundle and copying the bundle metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndivides bundles into segments of a certain duration and a certain step.", "response": "def _find_intervals(bundles, duration, step):\n    \"\"\"Divide bundles into segments of a certain duration and a certain step,\n    discarding any remainder.\"\"\"\n    segments = []\n    for bund in bundles:\n        beg, end = bund['times'][0][0], bund['times'][-1][1]\n\n        if end - beg >= duration:\n            new_begs = arange(beg, end - duration, step)\n\n            for t in new_begs:\n                seg = bund.copy()\n                seg['times'] = [(t, t + duration)]\n                segments.append(seg)\n\n    return segments"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_data(data, active_chan, ref_chan=[], grp_name=None):\n    output = ChanTime()\n    output.s_freq = data.s_freq\n    output.start_time = data.start_time\n    output.axis['time'] = data.axis['time']\n    output.axis['chan'] = empty(1, dtype='O')\n    output.data = empty(1, dtype='O')\n    output.data[0] = empty((len(active_chan), data.number_of('time')[0]),\n                           dtype='f')\n\n    sel_data = _select_channels(data, active_chan + ref_chan)\n    data1 = montage(sel_data, ref_chan=ref_chan)\n    data1.data[0] = nan_to_num(data1.data[0])\n\n    all_chan_grp_name = []\n\n    for i, chan in enumerate(active_chan):\n        chan_grp_name = chan\n        if grp_name:\n            chan_grp_name = chan + ' (' + grp_name + ')'\n        all_chan_grp_name.append(chan_grp_name)\n\n        dat = data1(chan=chan, trial=0)\n        output.data[0][i, :] = dat\n\n    output.axis['chan'][0] = asarray(all_chan_grp_name, dtype='U')\n\n    return output", "response": "Create data after montage."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a sub - segment matrix for easy manipulation of the original data.", "response": "def _create_subepochs(x, nperseg, step):\n    \"\"\"Transform the data into a matrix for easy manipulation\n\n    Parameters\n    ----------\n    x : 1d ndarray\n        actual data values\n    nperseg : int\n        number of samples in each row to create\n    step : int\n        distance in samples between rows\n    Returns\n    -------\n    2d ndarray\n        a view (i.e. doesn't copy data) of the original x, with shape\n        determined by nperseg and step. You should use the last dimension\n    \"\"\"\n    axis = x.ndim - 1  # last dim\n    nsmp = x.shape[axis]\n    stride = x.strides[axis]\n    noverlap = nperseg - step\n    v_shape = *x.shape[:axis], (nsmp - noverlap) // step, nperseg\n    v_strides = *x.strides[:axis], stride * step, stride\n    v = as_strided(x, shape=v_shape, strides=v_strides,\n                   writeable=False)  # much safer\n    return v"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _select_channels(data, channels):\n    output = data._copy()\n    chan_list = list(data.axis['chan'][0])\n    idx_chan = [chan_list.index(i_chan) for i_chan in channels]\n    output.data[0] = data.data[0][idx_chan, :]\n    output.axis['chan'][0] = asarray(channels)\n\n    return output", "response": "Select channels.\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data with all the channels\n    channels : list\n        channels of interest\n\n    Returns\n    -------\n    instance of ChanTime\n        data with only channels of interest\n\n    Notes\n    -----\n    This function does the same as wonambi.trans.select, but it's much faster.\n    wonambi.trans.Select needs to flexible for any data type, here we assume\n    that we have one trial, and that channel is the first dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_data(self, chan=[], ref_chan=[], grp_name=None, concat_chan=False, \n                  max_s_freq=30000, parent=None):\n        \"\"\"Read data for analysis. Adds data as 'data' in each dict.\n\n        Parameters\n        ----------\n        chan : list of str\n            active channel names as they appear in record, without ref or group\n            If given an empty list, the channel specified in seg['chan'] will\n            be read for each segment\n        ref_chan : list of str\n            reference channel names as they appear in record, without group\n        grp_name : str\n            name of the channel group, required in GUI\n        concat_chan : bool\n            if True, data from all channels will be concatenated\n        max_s_freq: : int\n            maximum sampling frequency\n        parent : QWidget\n            for GUI only. Identifies parent widget for display of progress\n            dialog.\n        \"\"\"\n        output = []\n\n        # Set up Progress Bar\n        if parent:\n            n_subseg = sum([len(x['times']) for x in self.segments])\n            progress = QProgressDialog('Fetching signal', 'Abort', 0, n_subseg,\n                                       parent)\n            progress.setWindowModality(Qt.ApplicationModal)\n            counter = 0\n\n        # Begin bundle loop; will yield one segment per loop\n        for i, seg in enumerate(self.segments):\n            one_segment = ChanTime()\n            one_segment.axis['chan'] = empty(1, dtype='O')\n            one_segment.axis['time'] = empty(1, dtype='O')\n            one_segment.data = empty(1, dtype='O')\n            subseg = []\n\n            # Subsegment loop; subsegments will be concatenated\n            for t0, t1 in seg['times']:\n                if parent:\n                    progress.setValue(counter)\n                    counter += 1\n\n                # if channel not specified, use segment channel\n                if chan:\n                    active_chan = chan\n                elif seg['chan']:\n                    active_chan = [seg['chan'].split(' (')[0]]\n                else:\n                    raise ValueError('No channel was specified and the '\n                                     'segment at {}-{} has no channel.'.format(\n                                             t0, t1))\n                active_chan = chan if chan else [seg['chan'].split(' (')[0]]\n                chan_to_read = active_chan + ref_chan\n\n                data = self.dataset.read_data(chan=chan_to_read, begtime=t0,\n                                         endtime=t1)\n\n                # Downsample if necessary\n                if data.s_freq > max_s_freq:\n                    q = int(data.s_freq / max_s_freq)\n                    lg.debug('Decimate (no low-pass filter) at ' + str(q))\n\n                    data.data[0] = data.data[0][:, slice(None, None, q)]\n                    data.axis['time'][0] = data.axis['time'][0][slice(\n                            None, None, q)]\n                    data.s_freq = int(data.s_freq / q)\n\n                # read data from disk\n                subseg.append(_create_data(\n                    data, active_chan, ref_chan=ref_chan, grp_name=grp_name))\n\n            one_segment.s_freq = s_freq = data.s_freq\n            one_segment.axis['chan'][0] = chs = subseg[0].axis['chan'][0]\n            one_segment.axis['time'][0] = timeline = hstack(\n                    [x.axis['time'][0] for x in subseg])\n            one_segment.data[0] = empty((len(active_chan), len(timeline)),\n                                        dtype='f')\n            n_stitch = sum(asarray(diff(timeline) > 2/s_freq, dtype=bool))\n\n            for i, ch in enumerate(subseg[0].axis['chan'][0]):\n                    one_segment.data[0][i, :] = hstack(\n                            [x(chan=ch)[0] for x in subseg])\n\n            # For channel concatenation\n            if concat_chan and len(chs) > 1:\n                one_segment.data[0] = ravel(one_segment.data[0])\n                one_segment.axis['chan'][0] = asarray([(', ').join(chs)],\n                                dtype='U')\n                # axis['time'] should not be used in this case\n\n            output.append({'data': one_segment,\n                           'chan': active_chan,\n                           'stage': seg['stage'],\n                           'cycle': seg['cycle'],\n                           'name': seg['name'],\n                           'n_stitch': n_stitch\n                           })\n\n            if parent:\n                if progress.wasCanceled():\n                    parent.parent.statusBar().showMessage('Process canceled by'\n                                           ' user.')\n                    return\n\n        if parent:\n            progress.setValue(counter)\n\n        self.segments = output\n\n        return 1", "response": "Read data for analysis. Adds data as data in each dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating all the widgets and dockwidgets in the main main", "response": "def create_widgets(MAIN):\n    \"\"\"Create all the widgets and dockwidgets. It also creates actions to\n    toggle views of dockwidgets in dockwidgets.\n    \"\"\"\n\n    \"\"\" ------ CREATE WIDGETS ------ \"\"\"\n    MAIN.labels = Labels(MAIN)\n    MAIN.channels = Channels(MAIN)\n    MAIN.notes = Notes(MAIN)\n    MAIN.merge_dialog = MergeDialog(MAIN)\n    MAIN.export_events_dialog = ExportEventsDialog(MAIN)\n    MAIN.export_dataset_dialog = ExportDatasetDialog(MAIN)\n    MAIN.spindle_dialog = SpindleDialog(MAIN)\n    MAIN.slow_wave_dialog = SWDialog(MAIN)\n    MAIN.analysis_dialog = AnalysisDialog(MAIN)\n    #MAIN.plot_dialog = PlotDialog(MAIN)\n    MAIN.overview = Overview(MAIN)\n    MAIN.spectrum = Spectrum(MAIN)\n    MAIN.traces = Traces(MAIN)\n    MAIN.video = Video(MAIN)\n    MAIN.settings = Settings(MAIN)  # depends on all widgets apart from Info\n    MAIN.info = Info(MAIN)  # this has to be the last, it depends on settings\n\n    MAIN.setCentralWidget(MAIN.traces)\n\n    \"\"\" ------ LIST DOCKWIDGETS ------ \"\"\"\n    new_docks = [{'name': 'Information',\n                  'widget': MAIN.info,\n                  'main_area': Qt.LeftDockWidgetArea,\n                  'extra_area': Qt.RightDockWidgetArea,\n                  },\n                 {'name': 'Labels',\n                  'widget': MAIN.labels,\n                  'main_area': Qt.RightDockWidgetArea,\n                  'extra_area': Qt.LeftDockWidgetArea,\n                  },\n                 {'name': 'Channels',\n                  'widget': MAIN.channels,\n                  'main_area': Qt.RightDockWidgetArea,\n                  'extra_area': Qt.LeftDockWidgetArea,\n                  },\n                 {'name': 'Spectrum',\n                  'widget': MAIN.spectrum,\n                  'main_area': Qt.RightDockWidgetArea,\n                  'extra_area': Qt.LeftDockWidgetArea,\n                  },\n                 {'name': 'Annotations',\n                  'widget': MAIN.notes,\n                  'main_area': Qt.LeftDockWidgetArea,\n                  'extra_area': Qt.RightDockWidgetArea,\n                  },\n                 {'name': 'Video',\n                  'widget': MAIN.video,\n                  'main_area': Qt.LeftDockWidgetArea,\n                  'extra_area': Qt.RightDockWidgetArea,\n                  },\n                 {'name': 'Overview',\n                  'widget': MAIN.overview,\n                  'main_area': Qt.BottomDockWidgetArea,\n                  'extra_area': Qt.TopDockWidgetArea,\n                  },\n                 ]\n\n    \"\"\" ------ CREATE DOCKWIDGETS ------ \"\"\"\n    idx_docks = {}\n    actions = MAIN.action\n\n    actions['dockwidgets'] = []\n    for dock in new_docks:\n        dockwidget = QDockWidget(dock['name'], MAIN)\n        dockwidget.setWidget(dock['widget'])\n        dockwidget.setAllowedAreas(dock['main_area'] | dock['extra_area'])\n        dockwidget.setObjectName(dock['name'])  # savestate\n\n        idx_docks[dock['name']] = dockwidget\n        MAIN.addDockWidget(dock['main_area'], dockwidget)\n\n        dockwidget_action = dockwidget.toggleViewAction()\n        dockwidget_action.setIcon(QIcon(ICON['widget']))\n\n        actions['dockwidgets'].append(dockwidget_action)\n\n    \"\"\" ------ ORGANIZE DOCKWIDGETS ------ \"\"\"\n    MAIN.tabifyDockWidget(idx_docks['Information'],\n                          idx_docks['Video'])\n    MAIN.tabifyDockWidget(idx_docks['Channels'],\n                          idx_docks['Labels'])\n    idx_docks['Information'].raise_()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating all the possible actions.", "response": "def create_actions(MAIN):\n    \"\"\"Create all the possible actions.\"\"\"\n    actions = MAIN.action  # actions was already taken\n\n    \"\"\" ------ OPEN SETTINGS ------ \"\"\"\n    actions['open_settings'] = QAction(QIcon(ICON['settings']), 'Settings',\n                                       MAIN)\n    actions['open_settings'].triggered.connect(MAIN.show_settings)\n\n    \"\"\" ------ CLOSE WINDOW ------ \"\"\"\n    actions['close_wndw'] = QAction(QIcon(ICON['quit']), 'Quit', MAIN)\n    actions['close_wndw'].triggered.connect(MAIN.close)\n\n    \"\"\" ------ ABOUT ------ \"\"\"\n    actions['about'] = QAction('About WONAMBI', MAIN)\n    actions['about'].triggered.connect(MAIN.about)\n\n    actions['aboutqt'] = QAction('About Qt', MAIN)\n    actions['aboutqt'].triggered.connect(lambda: QMessageBox.aboutQt(MAIN))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_menubar(MAIN):\n    actions = MAIN.action\n    menubar = MAIN.menuBar()\n    menubar.clear()\n\n    \"\"\" ------ FILE ------ \"\"\"\n    menu_file = menubar.addMenu('File')\n    menu_file.addAction(MAIN.info.action['open_dataset'])\n    submenu_recent = menu_file.addMenu('Recent Datasets')\n    submenu_recent.addActions(MAIN.info.action['open_recent'])\n    menu_file.addAction(MAIN.info.action['export'])\n\n    menu_file.addSeparator()\n    menu_file.addAction(actions['open_settings'])\n    menu_file.addSeparator()\n    menu_file.addAction(actions['close_wndw'])\n\n    \"\"\" ------ CHANNELS ------ \"\"\"\n    actions = MAIN.channels.action\n    menu_time = menubar.addMenu('Channels')\n    menu_time.addAction(actions['load_channels'])\n    menu_time.addAction(actions['save_channels'])\n\n    \"\"\" ------ NAVIGATION ------ \"\"\"\n    actions = MAIN.traces.action\n\n    menu_time = menubar.addMenu('Navigation')\n    menu_time.addAction(actions['step_prev'])\n    menu_time.addAction(actions['step_next'])\n    menu_time.addAction(actions['page_prev'])\n    menu_time.addAction(actions['page_next'])\n    menu_time.addSeparator()\n    menu_time.addAction(actions['addtime_-6h'])\n    menu_time.addAction(actions['addtime_-1h'])\n    menu_time.addAction(actions['addtime_-10min'])\n    menu_time.addAction(actions['addtime_10min'])\n    menu_time.addAction(actions['addtime_1h'])\n    menu_time.addAction(actions['addtime_6h'])\n    menu_time.addSeparator()\n    menu_time.addAction(actions['next_event'])\n    menu_time.addAction(actions['del_and_next_event'])\n    menu_time.addAction(actions['next_of_same_type'])\n    menu_time.addAction(actions['centre_event'])\n    menu_time.addSeparator()\n    menu_time.addAction(actions['go_to_epoch'])\n    menu_time.addAction(actions['line_up_with_epoch'])\n\n    \"\"\" ------ VIEW ------ \"\"\"\n    actions = MAIN.traces.action\n\n    menu_view = menubar.addMenu('View')\n    submenu_ampl = menu_view.addMenu('Global Scaling')\n    submenu_ampl.addAction(actions['Y_less'])\n    submenu_ampl.addAction(actions['Y_more'])\n    submenu_ampl.addSeparator()\n    for x in sorted(MAIN.value('y_scale_presets'), reverse=True):\n        act = submenu_ampl.addAction('Set to ' + str(x))\n        act.triggered.connect(partial(MAIN.traces.Y_ampl, x))\n\n    submenu_dist = menu_view.addMenu('Distance Between Traces')\n    submenu_dist.addAction(actions['Y_wider'])\n    submenu_dist.addAction(actions['Y_tighter'])\n    submenu_dist.addSeparator()\n    for x in sorted(MAIN.value('y_distance_presets'),\n                    reverse=True):\n        act = submenu_dist.addAction('Set to ' + str(x))\n        act.triggered.connect(partial(MAIN.traces.Y_dist, x))\n\n    submenu_length = menu_view.addMenu('Window Length')\n    submenu_length.addAction(actions['X_more'])\n    submenu_length.addAction(actions['X_less'])\n    submenu_length.addSeparator()\n    for x in sorted(MAIN.value('window_length_presets'),\n                    reverse=True):\n        act = submenu_length.addAction('Set to ' + str(x))\n        act.triggered.connect(partial(MAIN.traces.X_length, x))\n\n    menu_view.addAction(actions['cross_chan_mrk'])\n\n    menu_view.addSeparator()\n    menu_view.addAction(actions['export_svg'])\n\n    \"\"\" ------ ANNOTATIONS ------ \"\"\"\n    actions = MAIN.notes.action\n\n    menu_annot = menubar.addMenu('Annotations')\n    menu_annot.addAction(actions['new_annot'])\n    menu_annot.addAction(actions['load_annot'])\n    menu_annot.addAction(actions['clear_annot'])\n    menu_annot.addSeparator()\n\n    submenu_rater = menu_annot.addMenu('Rater')\n    submenu_rater.addAction(actions['new_rater'])\n    submenu_rater.addAction(actions['rename_rater'])\n    submenu_rater.addAction(actions['del_rater'])\n    submenu_rater.addSeparator()\n    if MAIN.notes.annot is not None:\n        for rater in sorted(MAIN.notes.annot.raters):\n            act = submenu_rater.addAction(rater)\n            act.triggered.connect(partial(MAIN.notes.select_rater, rater))\n    menu_annot.addSeparator()\n\n    submenu_marker = menu_annot.addMenu('Bookmark')\n    submenu_marker.addAction(actions['new_bookmark'])\n\n    submenu_event = menu_annot.addMenu('Event')\n    submenu_event.addAction(actions['new_eventtype'])\n    submenu_event.addAction(actions['del_eventtype'])\n    submenu_event.addAction(actions['rename_eventtype'])\n    submenu_mrk2evt = submenu_event.addMenu('Markers into Events')\n    submenu_mrk2evt.addAction(actions['m2e_newname'])\n    submenu_mrk2evt.addAction(actions['m2e_keepname'])\n    submenu_event.addAction(actions['merge_events'])\n    submenu_event.addAction(MAIN.traces.action['change_event_type'])\n\n    # these are the real QActions attached to notes\n    submenu_stage = menu_annot.addMenu('Stage')\n    submenu_stage.addActions(MAIN.notes.actions())\n\n    submenu_mrkr = menu_annot.addMenu('Cycle')\n    submenu_mrkr.addAction(actions['cyc_start'])\n    submenu_mrkr.addAction(actions['cyc_end'])\n    submenu_mrkr.addAction(actions['remove_cyc'])\n    submenu_mrkr.addAction(actions['clear_cyc'])\n    menu_annot.addSeparator()\n\n    submenu_import = menu_annot.addMenu('Import Staging')\n    submenu_import.addAction(actions['import_alice'])\n    submenu_import.addAction(actions['import_compumedics'])\n    submenu_import.addAction(actions['import_deltamed'])\n    submenu_import.addAction(actions['import_domino'])\n    submenu_import.addAction(actions['import_prana'])\n    submenu_import.addAction(actions['import_remlogic'])\n    submenu_import.addAction(actions['import_sandman'])\n    submenu_import.addAction(actions['import_fasst'])\n\n    submenu_import_qual = menu_annot.addMenu('Import Signal Quality')\n    submenu_import_qual.addAction(actions['import_alice_qual'])\n    submenu_import_qual.addAction(actions['import_compumedics_qual'])\n    submenu_import_qual.addAction(actions['import_deltamed_qual'])\n    submenu_import_qual.addAction(actions['import_domino_qual'])\n    submenu_import_qual.addAction(actions['import_prana_qual'])\n    submenu_import_qual.addAction(actions['import_remlogic_qual'])\n    submenu_import_qual.addAction(actions['import_sandman_qual'])\n    \n    submenu_import_event = menu_annot.addMenu('Import Events')\n    submenu_import_event.addAction(actions['import_events_wonambi'])\n    submenu_import_event.addAction(actions['import_events_remlogic'])\n    menu_annot.addSeparator()\n    \n    submenu_export_staging = menu_annot.addMenu('Export Staging')\n    submenu_export_staging.addAction(actions['export_to_csv'])\n    submenu_export_staging.addAction(actions['export_to_remlogic'])\n    submenu_export_staging.addAction(actions['export_to_remlogic_fr'])\n\n    menu_annot.addAction(actions['export_events'])    \n    menu_annot.addSeparator()\n    \n    menu_annot.addAction(actions['export_sleepstats'])\n\n    \"\"\" ------ ANALYSIS ------ \"\"\"\n    actions = MAIN.notes.action\n\n    menu_analysis = menubar.addMenu('Analysis')\n\n    submenu_detect = menu_analysis.addMenu('Detection')\n    submenu_detect.addAction(actions['spindle'])\n    submenu_detect.addAction(actions['slow_wave'])\n\n    menu_analysis.addAction(actions['analyze'])\n\n\n    \"\"\" ------ WINDOWS ------ \"\"\"\n    actions = MAIN.action\n\n    menu_window = menubar.addMenu('Windows')\n    for dockwidget_act in actions['dockwidgets']:\n        menu_window.addAction(dockwidget_act)\n    MAIN.menu_window = menu_window\n\n    menu_about = menubar.addMenu('About')\n    menu_about.addAction(actions['about'])\n    menu_about.addAction(actions['aboutqt'])", "response": "Create the whole menubar based on actions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_toolbar(MAIN):\n    actions = MAIN.action\n\n    toolbar = MAIN.addToolBar('File Management')\n    toolbar.setObjectName('File Management')  # for savestate\n    toolbar.addAction(MAIN.info.action['open_dataset'])\n    toolbar.addSeparator()\n    toolbar.addAction(MAIN.channels.action['load_channels'])\n    toolbar.addAction(MAIN.channels.action['save_channels'])\n    toolbar.addSeparator()\n    toolbar.addAction(MAIN.notes.action['new_annot'])\n    toolbar.addAction(MAIN.notes.action['load_annot'])\n\n    \"\"\" ------ SCROLL ------ \"\"\"\n    actions = MAIN.traces.action\n\n    toolbar = MAIN.addToolBar('Scroll')\n    toolbar.setObjectName('Scroll')  # for savestate\n    toolbar.addAction(actions['step_prev'])\n    toolbar.addAction(actions['step_next'])\n    toolbar.addAction(actions['page_prev'])\n    toolbar.addAction(actions['page_next'])\n    toolbar.addSeparator()\n    toolbar.addAction(actions['X_more'])\n    toolbar.addAction(actions['X_less'])\n    toolbar.addSeparator()\n    toolbar.addAction(actions['Y_less'])\n    toolbar.addAction(actions['Y_more'])\n    toolbar.addAction(actions['Y_wider'])\n    toolbar.addAction(actions['Y_tighter'])\n\n    \"\"\" ------ ANNOTATIONS ------ \"\"\"\n    actions = MAIN.notes.action\n\n    toolbar = MAIN.addToolBar('Annotations')\n    toolbar.setObjectName('Annotations')\n\n    toolbar.addAction(actions['new_bookmark'])\n    toolbar.addSeparator()\n    toolbar.addAction(actions['new_event'])\n    toolbar.addWidget(MAIN.notes.idx_eventtype)\n    toolbar.addSeparator()\n    toolbar.addWidget(MAIN.notes.idx_stage)\n    toolbar.addWidget(MAIN.notes.idx_quality)", "response": "Create the various toolbars."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_evt_types(self):\n        self.event_types = self.parent.notes.annot.event_types\n        self.idx_evt_type.clear()\n        self.frequency['norm_evt_type'].clear()\n        for ev in self.event_types:\n            self.idx_evt_type.addItem(ev)\n            self.frequency['norm_evt_type'].addItem(ev)", "response": "Update the event types list when dialog is opened."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nenabling and disable buttons according to options selected.", "response": "def toggle_buttons(self):\n        \"\"\"Enable and disable buttons, according to options selected.\"\"\"\n        event_on = self.chunk['event'].isChecked()\n        epoch_on = self.chunk['epoch'].isChecked()\n        #segment_on = self.chunk['segment'].isChecked()\n        self.lock_to_staging.setEnabled(epoch_on)\n        lock_on = self.lock_to_staging.get_value()\n        lock_enabled = self.lock_to_staging.isEnabled()\n\n        self.evt_chan_only.setEnabled(event_on)\n        self.idx_evt_type.setEnabled(event_on)\n        self.reject_epoch.setEnabled(not event_on)\n        self.reject_event.setEnabled(logical_or((lock_enabled and not lock_on),\n                                                not lock_enabled))\n        self.cat['evt_type'].setEnabled(event_on)\n\n        epop = self.epoch_param\n        for wgt in epop.values():\n            wgt.setEnabled(epoch_on and not lock_on)\n\n        if epoch_on and not lock_on:\n            overlap_on = epop['overlap'].isChecked()\n            epop['overlap_val'].setEnabled(overlap_on)\n            epop['step_val'].setEnabled(not overlap_on)\n\n        if Pac is not None:\n            surro = self.pac['surro_method']\n            surro.model().item(1).setEnabled(epoch_on)\n            if surro.get_value() == 'Swap phase/amplitude across trials':\n                surro.set_value('No surrogates')\n        # \"Swap phase/amplitude across trials\" only available if using epochs\n        # because trials need to be of equal length\n\n        if event_on:\n            self.reject_epoch.setChecked(False)\n        elif self.cat['evt_type'].get_value():\n            self.cat['evt_type'].setChecked(False)\n\n        if epoch_on and not lock_on:\n            self.reject_event.set_value('none')\n            for i in self.cat.values():\n                i.setChecked(False)\n                i.setEnabled(False)\n        self.cat['discontinuous'].setEnabled(not epoch_on)\n\n        bandpass_on = self.trans['bandpass'].get_value() != 'none'\n        for w in self.trans['bp'].values():\n            w[0].setEnabled(bandpass_on)\n            w[1].setEnabled(bandpass_on)\n\n        notch1_on = self.trans['notch1'].get_value() != 'none'\n        for w in self.trans['n1'].values():\n            w[0].setEnabled(notch1_on)\n            w[1].setEnabled(notch1_on)\n\n        notch2_on = self.trans['notch2'].get_value() != 'none'\n        for w in self.trans['n2'].values():\n            w[0].setEnabled(notch2_on)\n            w[1].setEnabled(notch2_on)\n\n        density_on = self.event['global']['density'].isChecked()\n        self.event['global']['density_per'].setEnabled(density_on)\n\n        for buttons in self.event['local'].values():\n            checked = buttons[0].isChecked()\n            buttons[1].setEnabled(checked)\n            if not checked:\n                buttons[1].setChecked(False)\n\n        el = self.event['local']\n        ev_psd_on = asarray([x.get_value() for x in [el['power'][0],\n                             el['energy'][0], el['peakpf'][0],\n                             el['peakef'][0]]]).any()\n        self.event['band_box'].setEnabled(ev_psd_on)\n\n        sw = self.event['sw']\n        slope_on = sw['avg_slope'].get_value() or sw['max_slope'].get_value()\n        sw['prep'].setEnabled(slope_on)\n        sw['invert'].setEnabled(slope_on)\n\n        self.update_nseg()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables and disable concatenation options.", "response": "def toggle_concatenate(self):\n        \"\"\"Enable and disable concatenation options.\"\"\"\n        if not (self.chunk['epoch'].isChecked() and\n                self.lock_to_staging.get_value()):\n            for i,j in zip([self.idx_chan, self.idx_cycle, self.idx_stage,\n                            self.idx_evt_type],\n                   [self.cat['chan'], self.cat['cycle'],\n                    self.cat['stage'], self.cat['evt_type']]):\n                if len(i.selectedItems()) > 1:\n                    j.setEnabled(True)\n                else:\n                    j.setEnabled(False)\n                    j.setChecked(False)\n\n        if not self.chunk['event'].isChecked():\n            self.cat['evt_type'].setEnabled(False)\n\n        if not self.cat['discontinuous'].get_value():\n            self.cat['chan'].setEnabled(False)\n            self.cat['chan'].setChecked(False)\n\n        self.update_nseg()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling and disable frequency domain options.", "response": "def toggle_freq(self):\n        \"\"\"Enable and disable frequency domain options.\"\"\"\n        freq = self.frequency\n\n        export_full_on = freq['export_full'].get_value()\n        export_band_on = freq['export_band'].get_value()\n        freq_on = asarray([export_full_on,\n                           export_band_on,\n                           freq['plot_on'].get_value(),\n                           freq['fooof_on'].get_value()]).any()\n        freq['box_param'].setEnabled(freq_on)\n        freq['box_output'].setEnabled(freq_on)\n        freq['box_nfft'].setEnabled(freq_on)\n        freq['box_band'].setEnabled(export_band_on)\n\n        welch_on = freq['welch_on'].get_value() and freq_on\n        freq['box_welch'].setEnabled(welch_on)\n\n        if welch_on:\n            overlap_on = freq['overlap'].isChecked()\n            freq['overlap_val'].setEnabled(overlap_on)\n            freq['step_val'].setEnabled(not overlap_on)\n            freq['box_output'].setEnabled(not welch_on)\n            freq['box_nfft'].setEnabled(not welch_on)\n            freq['spectrald'].setChecked(True)\n\n        nfft_fixed_on = freq['nfft_fixed'].isChecked()\n        zeropad_on = freq['nfft_zeropad'].isChecked()\n        freq['nfft_fixed_val'].setEnabled(nfft_fixed_on)\n\n        epoch_on = self.chunk['epoch'].isChecked()\n        rectangular = welch_on or epoch_on or zeropad_on or nfft_fixed_on or \\\n                        (self.nseg == 1)\n        freq['prep'].setEnabled(freq_on)\n        if not freq_on:\n            freq['prep'].set_value(False)\n        freq['export_full'].setEnabled(rectangular)\n        freq['plot_on'].setEnabled(rectangular)\n        freq['fooof_on'].setEnabled(rectangular)\n        freq['box_norm'].setEnabled(freq_on and \\\n            ((welch_on or nfft_fixed_on or zeropad_on) or \\\n             (export_band_on and not export_full_on)))\n        if not freq['plot_on'].isEnabled():\n            freq['plot_on'].set_value(False)\n        if not freq['box_norm'].isEnabled():\n            freq['norm'].set_value('none')\n\n        dpss_on = freq['taper'].get_value() == 'dpss'\n        freq['box_mtap'].setEnabled(dpss_on)\n\n        if dpss_on:\n            nhbw_on = freq['nhbw'].get_value()\n            freq['nhbw_val'].setEnabled(nhbw_on)\n\n        complex_on = freq['complex'].isChecked()\n        freq['sides'].setEnabled(complex_on)\n        if complex_on:\n            freq['welch_on'].setEnabled(False)\n            freq['welch_on'].set_value(False)\n        else:\n            freq['welch_on'].setEnabled(True)\n\n        norm_evt = freq['norm'].get_value() == 'by mean of event type(s)'\n        norm_stage = freq['norm'].get_value() == 'by mean of stage(s)'\n        freq['norm_evt_type'].setEnabled(norm_evt)\n        freq['norm_stage'].setEnabled(norm_stage)\n        freq['norm_concat'].setEnabled(norm_evt or norm_stage)\n\n        nchan = len(self.idx_chan.selectedItems())\n        #s2 = (self.nseg == 1 and nchan == 2) or (self.nseg == 2 and nchan == 1)\n        freq['box_cross'].setEnabled(nchan == 2 and freq_on)\n        if not nchan == 2:\n            freq['csd'].set_value(False)\n            freq['gainphase'].set_value(False)\n            #freq['phaseshift'].set_value(False)\n            freq['coh'].set_value(False)\n\n        if True in [freq[x].get_value() for x in ['csd', 'gainphase', 'coh']]:\n            freq['spectrald'].setChecked(True)\n            freq['box_output'].setEnabled(False)\n            #freq['norm'].set_value('none')\n            #freq['box_norm'].setEnabled(False)\n\n        fooof_on = freq['fooof_on'].get_value()\n        freq['box_fooof'].setEnabled(fooof_on)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef toggle_pac(self):\n        if Pac is not None:\n            pac_on = self.pac['pac_on'].get_value()\n            self.pac['prep'].setEnabled(pac_on)\n            self.pac['box_metric'].setEnabled(pac_on)\n            self.pac['box_complex'].setEnabled(pac_on)\n            self.pac['box_surro'].setEnabled(pac_on)\n            self.pac['box_opts'].setEnabled(pac_on)\n\n            if not pac_on:\n                self.pac['prep'].set_value(False)\n\n        if Pac is not None and pac_on:\n\n            pac = self.pac\n            hilb_on = pac['hilbert_on'].isChecked()\n            wav_on = pac['wavelet_on'].isChecked()\n            for button in pac['hilbert'].values():\n                button[0].setEnabled(hilb_on)\n                if button[1] is not None:\n                    button[1].setEnabled(hilb_on)\n            pac['wav_width'][0].setEnabled(wav_on)\n            pac['wav_width'][1].setEnabled(wav_on)\n\n            if pac['metric'].get_value() in [\n                    'Kullback-Leibler Distance',\n                    'Heights ratio']:\n                pac['nbin'][0].setEnabled(True)\n                pac['nbin'][1].setEnabled(True)\n            else:\n                pac['nbin'][0].setEnabled(False)\n                pac['nbin'][1].setEnabled(False)\n\n            if pac['metric'] == 'ndPac':\n                for button in pac['surro'].values():\n                    button[0].setEnabled(False)\n                    if button[1] is not None:\n                        button[1].setEnabled(False)\n                pac['surro']['pval'][0].setEnabled(True)\n\n            ndpac_on = pac['metric'].get_value() == 'ndPac'\n            surro_on = logical_and(pac['surro_method'].get_value() != ''\n                                       'No surrogates', not ndpac_on)\n            norm_on = pac['surro_norm'].get_value() != 'No normalization'\n            blocks_on = 'across time' in pac['surro_method'].get_value()\n            pac['surro_method'].setEnabled(not ndpac_on)\n            for button in pac['surro'].values():\n                button[0].setEnabled(surro_on and norm_on)\n                if button[1] is not None:\n                    button[1].setEnabled(surro_on and norm_on)\n            pac['surro']['nblocks'][0].setEnabled(blocks_on)\n            pac['surro']['nblocks'][1].setEnabled(blocks_on)\n            if ndpac_on:\n                pac['surro_method'].set_value('No surrogates')\n                pac['surro']['pval'][0].setEnabled(True)", "response": "Enable and disable PAC options."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the number of segments displayed in the dialog.", "response": "def update_nseg(self):\n        \"\"\"Update the number of segments, displayed in the dialog.\"\"\"\n        self.nseg = 0\n        if self.one_grp:\n            segments = self.get_segments()\n\n            if segments is not None:\n                self.nseg = len(segments)\n                self.show_nseg.setText('Number of segments: ' + str(self.nseg))\n                times = [t for seg in segments for t in seg['times']]\n                self.parent.overview.mark_poi(times)\n\n            else:\n                self.show_nseg.setText('No valid segments')\n\n        self.toggle_freq()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks or uncheck all local event parameters.", "response": "def check_all_local(self):\n        \"\"\"Check or uncheck all local event parameters.\"\"\"\n        all_local_chk = self.event['global']['all_local'].isChecked()\n        for buttons in self.event['local'].values():\n            buttons[0].setChecked(all_local_chk)\n            buttons[1].setEnabled(buttons[0].isChecked())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_all_local_prep(self):\n        all_local_pp_chk = self.event['global']['all_local_prep'].isChecked()\n        for buttons in self.event['local'].values():\n            if buttons[1].isEnabled():\n                buttons[1].setChecked(all_local_pp_chk)", "response": "Check or uncheck all enabled event pre - processing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uncheck_all_local(self):\n        for buttons in self.event['local'].values():\n            if not buttons[0].get_value():\n                self.event['global']['all_local'].setChecked(False)\n            if buttons[1].isEnabled() and not buttons[1].get_value():\n                self.event['global']['all_local_prep'].setChecked(False)", "response": "Uncheck all local box when a local event is unchecked."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nactioning when button was clicked.", "response": "def button_clicked(self, button):\n        \"\"\"Action when button was clicked.\n\n        Parameters\n        ----------\n        button : instance of QPushButton\n            which button was pressed\n        \"\"\"\n        if button is self.idx_ok:\n\n            # File location\n            if not self.filename:\n                msg = 'Select location for data export file.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('File path error')\n                error_dialog.showMessage(msg)\n                return\n\n            # Check for signal\n            self.update_nseg\n            if self.nseg <= 0:\n                msg = 'No valid signal found.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('Error fetching data')\n                error_dialog.showMessage(msg)\n                return\n\n            # Which analyses?\n            freq = self.frequency\n            freq_full = freq['export_full'].get_value()\n            freq_band = freq['export_band'].get_value()\n            freq_plot = freq['plot_on'].get_value()\n            freq_fooof = freq['fooof_on'].get_value()\n            freq_prep = freq['prep'].get_value()\n            freq_on = freq_full or freq_band or freq_plot or freq_fooof\n\n            if Pac is not None:\n                pac_on = self.pac['pac_on'].get_value()\n                pac_prep = self.pac['prep'].get_value()\n            else:\n                pac_on = False\n                pac_prep = False\n\n            ev = self.event\n            glob = asarray(\n                    [v.get_value() for v in ev['global'].values()]).any()\n            loc = asarray(\n                    [v[0].get_value() for v in ev['local'].values()]).any()\n            avg_sl = ev['sw']['avg_slope'].get_value()\n            max_sl = ev['sw']['max_slope'].get_value()\n            loc_prep = asarray(\n                    [v[1].get_value() for v in ev['local'].values()]).any()\n            slope_prep = ev['sw']['prep'].get_value()\n\n            if not (freq_on or pac_on or glob or loc or avg_sl or max_sl):\n                return\n\n            if freq['export_band'].get_value():\n                bands = freq_from_str(freq['band'].get_value())\n                if bands is None:\n                    msg = ('Invalid input for Define bands. Click the '\n                    \"'i' button for instructions.\")\n                    error_dialog = QErrorMessage(self)\n                    error_dialog.setWindowTitle('Error reading bands')\n                    error_dialog.showMessage(msg)\n                    return\n\n            if (freq['norm'].get_value() == 'by mean of event type(s)' and\n                not freq['norm_evt_type'].selectedItems()):\n                msg = 'Select event type(s) for normalization.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('Error fetching data')\n                error_dialog.showMessage(msg)\n                return\n\n            if (freq['norm'].get_value() == 'by mean of stage(s)' and\n                not freq['norm_stage'].selectedItems()):\n                msg = 'Select stage(s) for normalization.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('Error fetching data')\n                error_dialog.showMessage(msg)\n                return\n\n            # Fetch signal\n            eco = self.evt_chan_only\n            chan = [] if (eco.get_value() and eco.isEnabled()) else self.chan\n            concat_chan = self.cat['chan'].get_value()\n\n            self.data = self.get_segments()\n\n            if not self.data.segments:\n                msg = 'No valid signal found.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('Error fetching data')\n                error_dialog.showMessage(msg)\n                return\n\n            ding = self.data.read_data(chan,\n                               ref_chan=self.one_grp['ref_chan'],\n                               grp_name=self.one_grp['name'],\n                               concat_chan=concat_chan,\n                               max_s_freq=self.parent.value('max_s_freq'),\n                               parent=self)\n\n            if not ding:\n                self.parent.statusBar().showMessage('Process interrupted.')\n                return\n\n            # Transform signal\n            if freq_prep or pac_prep or loc_prep or slope_prep:\n                lg.info('Pre-processing data')\n                self.data = self.transform_data(self.data)\n\n            \"\"\" ------ FREQUENCY ------ \"\"\"\n\n            if freq_on:\n\n                csd_on = freq['csd'].get_value()\n                gainphase_on = freq['gainphase'].get_value()\n                coh_on = freq['coh'].get_value()\n\n                # don't need autospectrum if all we want is CSD\n                if not (csd_on and not (gainphase_on or coh_on)):\n                    asd = self.compute_freq() # autospectral density\n                    if not asd:\n                        return\n\n                if csd_on or gainphase_on or coh_on:\n                    csd = self.compute_freq(csd=True) # cross-spectral density\n                    chancombo = str(csd[0]['data'].axis['chan'][0][0])\n                    freq_out = []\n\n                    if csd_on:\n                        freq_out.append((csd, 'csd',\n                                         ('Cross-spectral density, '\n                                          + chancombo + ', '),\n                                          None, 'semilogy'))\n\n                    if gainphase_on:\n                        xg, yg, ph = self.compute_freq_cross(csd, asd,\n                                                            output='gainphase')\n                        xchancombo = str(xg[0]['data'].axis['chan'][0][0])\n                        ychancombo = str(yg[0]['data'].axis['chan'][0][0])\n                        freq_out.append((xg, 'xgain',\n                                         ('Gain, ' + xchancombo + ', '),\n                                         'Gain', 'linear'))\n                        freq_out.append((yg, 'ygain',\n                                         ('Gain, ' + ychancombo + ', '),\n                                         'Gain', 'linear'))\n                        freq_out.append((ph, 'phase',\n                                         ('Phase shift, ' + xchancombo + ', '),\n                                         'Phase shift (degrees)', 'linear'))\n\n                    if coh_on:\n                        coh, = self.compute_freq_cross(csd, asd,\n                                                       output='coherence')\n                        freq_out.append((coh, 'coh',\n                                         ('Coherence, ' + chancombo + ', '),\n                                         'Coherence', 'linear'))\n\n                else:\n                    freq_out = [(asd, 'freq', '', None, 'semilogy')]\n\n                for one_xf, suffix, prefix, ylabel, scale in freq_out:\n\n                    if freq_band:\n                        filename = (splitext(self.filename)[0] + '_' + suffix +\n                                    '_band.csv')\n                        export_freq_band(one_xf, bands, filename)\n\n                    if freq_full or freq_plot or freq_fooof:\n                        n_freq_bins = [x['data']()[0].shape for x in one_xf]\n\n                        if all(x == n_freq_bins[0] for x in n_freq_bins):\n                            x = list(one_xf[0]['data'].axis['freq'][0])\n\n                            if len(one_xf) == 1:\n                                desc = None\n                                y = abs(one_xf[0]['data'].data[0][0])\n                            else:\n                                as_matrix = asarray(\n                                   [y for x in one_xf for y in x['data']()[0]])\n                                desc = get_descriptives(as_matrix)\n                                y = desc['mean']\n\n                            if freq_full:\n                                filename = (splitext(self.filename)[0] + '_' +\n                                            suffix + '_full.csv')\n                                export_freq(one_xf, filename, desc=desc)\n\n                            if freq_plot:\n                                self.plot_freq(x, y,\n                                               title=(prefix + self.title),\n                                               ylabel=ylabel, scale=scale)\n\n                            if freq_fooof:\n                                self.report_fooof(asarray(x), y, suffix)\n\n            \"\"\" ------ PAC ------ \"\"\"\n\n            if pac_on:\n                pac_output = self.compute_pac()\n\n                if pac_output is not None:\n                    xpac, fpha, famp = pac_output\n                else:\n                    return\n\n                as_matrix = asarray(\n                        [ravel(chan['data'][x,:,:]) for chan in xpac.values() \\\n                         for x in range(chan['data'].shape[0])])\n                desc = get_descriptives(as_matrix)\n                self.export_pac(xpac, fpha, famp, desc)\n\n            \"\"\" ------ EVENTS ------ \"\"\"\n\n            evt_dat, count, density = self.compute_evt_params()\n\n            if (evt_dat or count or density):\n                fn = splitext(self.filename)[0] + '_params.csv'\n                export_event_params(fn, evt_dat, count=count, density=density)\n\n            self.parent.overview.mark_poi() # remove poi\n            self.accept()\n\n        if button is self.idx_cancel:\n            self.parent.overview.mark_poi() # remove poi\n            self.reject()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning instance of trans. Segments.", "response": "def get_segments(self):\n        \"\"\"Get segments for analysis. Creates instance of trans.Segments.\"\"\"\n        # Chunking\n        chunk = {k: v.isChecked() for k, v in self.chunk.items()}\n        lock_to_staging = self.lock_to_staging.get_value()\n        epoch_dur = self.epoch_param['dur'].get_value()\n        epoch_overlap = self.epoch_param['overlap_val'].value()\n        epoch_step = None\n        epoch = None\n\n        if chunk['epoch']:\n            if lock_to_staging:\n                epoch = 'locked'\n            else:\n                epoch = 'unlocked'\n\n                if self.epoch_param['step'].isChecked():\n                    epoch_step = self.epoch_param['step_val'].get_value()\n\n                    if epoch_step <= 0:\n                        epoch_step = 0.1\n\n        # Which channel(s)\n        self.chan = self.get_channels() # chan name without group\n        if not self.chan:\n            return\n\n        # Which event type(s)\n        chan_full = None\n        evt_type = None\n\n        if chunk['event']:\n\n            if self.evt_chan_only.get_value():\n                chan_full = [i + ' (' + self.idx_group.currentText() + ''\n                           ')' for i in self.chan]\n\n            evt_type = self.idx_evt_type.selectedItems()\n            if not evt_type:\n                return\n            else:\n                evt_type = [x.text() for x in evt_type]\n\n        # Which cycle(s)\n        cycle = self.cycle = self.get_cycles()\n\n        # Which stage(s)\n        stage = self.idx_stage.selectedItems()\n        if not stage:\n            stage = self.stage = None\n        else:\n            stage = self.stage = [\n                    x.text() for x in self.idx_stage.selectedItems()]\n\n        # Concatenation\n        cat = {k: v.get_value() for k, v in self.cat.items()}\n        cat = (int(cat['cycle']), int(cat['stage']),\n               int(cat['discontinuous']), int(cat['evt_type']))\n\n        # Artefact event rejection\n        reject_event = self.reject_event.get_value()\n        if reject_event == 'channel-specific':\n            chan_full = [i + ' (' + self.idx_group.currentText() + ''\n                           ')' for i in self.chan]\n            reject_artf = True\n        elif reject_event == 'from any channel':\n            reject_artf = True\n        else:\n            reject_artf = False\n\n        # Other options\n        min_dur = self.min_dur.get_value()\n        reject_epoch = self.reject_epoch.get_value()\n\n        # Generate title for summary plot\n        self.title = self.make_title(chan_full, cycle, stage, evt_type)\n\n        segments = fetch(self.parent.info.dataset,\n                         self.parent.notes.annot, cat=cat,\n                         evt_type=evt_type, stage=stage, cycle=cycle,\n                         chan_full=chan_full, epoch=epoch,\n                         epoch_dur=epoch_dur, epoch_overlap=epoch_overlap,\n                         epoch_step=epoch_step, reject_epoch=reject_epoch,\n                         reject_artf=reject_artf, min_dur=min_dur)\n\n        return segments"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying pre - processing transformation to data and add it to data dict.", "response": "def transform_data(self, data):\n        \"\"\"Apply pre-processing transformation to data, and add it to data\n        dict.\n\n        Parameters\n        ---------\n        data : instance of Segments\n            segments including 'data' (ChanTime)\n\n        Returns\n        -------\n        instance of Segments\n            same object with transformed data as 'trans_data' (ChanTime)\n        \"\"\"\n        trans = self.trans\n        differ = trans['diff'].get_value()\n        bandpass = trans['bandpass'].get_value()\n        notch1 = trans['notch1'].get_value()\n        notch2 = trans['notch2'].get_value()\n\n        for seg in data:\n            dat = seg['data']\n\n            if differ:\n                dat = math(dat, operator=diff, axis='time')\n\n            if bandpass != 'none':\n                order = trans['bp']['order'][1].get_value()\n                f1 = trans['bp']['f1'][1].get_value()\n                f2 = trans['bp']['f2'][1].get_value()\n\n                if f1 == '':\n                    f1 = None\n                if f2 == '':\n                    f2 = None\n\n                dat = filter_(dat, low_cut=f1, high_cut=f2, order=order,\n                              ftype=bandpass)\n\n            if notch1 != 'none':\n                order = trans['n1']['order'][1].get_value()\n                cf = trans['n1']['cf'][1].get_value()\n                hbw = trans['n1']['bw'][1].get_value() / 2.0\n                lo_pass = cf - hbw\n                hi_pass = cf + hbw\n                dat = filter_(dat, low_cut=hi_pass, order=order, ftype=notch1)\n                dat = filter_(dat, high_cut=lo_pass, order=order, ftype=notch1)\n\n            if notch2 != 'none':\n                order = trans['n2']['order'][1].get_value()\n                cf = trans['n2']['cf'][1].get_value()\n                hbw = trans['n2']['bw'][1].get_value() / 2.0\n                lo_pass = cf - hbw\n                hi_pass = cf + hbw\n                dat = filter_(dat, low_cut=hi_pass, order=order, ftype=notch1)\n                dat = filter_(dat, high_cut=lo_pass, order=order, ftype=notch1)\n\n            seg['trans_data'] = dat\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_as(self):\n        filename = splitext(\n                self.parent.notes.annot.xml_file)[0] + '_data'\n        filename, _ = QFileDialog.getSaveFileName(self, 'Export analysis data',\n                                                  filename,\n                                                  'CSV (*.csv)')\n        if filename == '':\n            return\n\n        self.filename = filename\n        short_filename = short_strings(basename(self.filename))\n        self.idx_filename.setText(short_filename)", "response": "Save the name of the data export file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute frequency domain analysis.", "response": "def compute_freq(self, csd=False):\n        \"\"\"Compute frequency domain analysis.\n\n        Returns\n        -------\n        list of dict\n            each item is a dict where 'data' is an instance of ChanFreq for a\n            single segment of signal, 'name' is the event type, if applicable,\n            'times' is a tuple of the start and end times in sec, 'duration' is\n            the actual duration of the segment, in seconds (can be dissociated\n            from 'times' if the signal was concatenated)\n            and with 'chan' (str), 'stage' (str) and 'cycle' (int)\n        \"\"\"\n        progress = QProgressDialog('Computing frequency', 'Abort',\n                                   0, len(self.data) - 1, self)\n        progress.setWindowModality(Qt.ApplicationModal)\n\n        freq = self.frequency\n        prep = freq['prep'].get_value()\n        scaling = freq['scaling'].get_value()\n        log_trans = freq['log_trans'].get_value()\n        #sides = freq['sides'].get_value()\n        taper = freq['taper'].get_value()\n        halfbandwidth = freq['hbw'].get_value()\n        NW = freq['nhbw_val'].get_value()\n        duration = freq['duration'].get_value()\n        overlap = freq['overlap_val'].value()\n        step = freq['step_val'].get_value()\n        centend = freq['centend'].get_value()\n        detrend = freq['detrend'].get_value()\n        norm = freq['norm'].get_value()\n        norm_concat = freq['norm_concat'].get_value()\n\n        if csd:\n            output = 'csd'\n        elif freq['spectrald'].isChecked():\n            output = 'spectraldensity'\n        else:\n            output = 'complex'\n\n        sides = 'one'\n        #if sides == 1:\n        #    sides = 'one'\n        #elif sides == 2:\n        #    sides = 'two'\n\n        if freq['overlap'].isChecked():\n            step = None\n        else:\n            overlap = None\n\n        if NW == 0 or not freq['nhbw'].get_value():\n            NW = None\n        if duration == 0 or not freq['welch_on'].get_value():\n            duration = None\n        if step == 0:\n            step = None\n        if detrend == 'none':\n            detrend = None\n\n        if freq['nfft_fixed'].isChecked():\n            n_fft = int(freq['nfft_fixed_val'].get_value())\n        elif freq['nfft_zeropad'].isChecked():\n            n_fft = max([x['data'].number_of('time')[0] for x in self.data])\n            lg.info('n_fft is zero-padded to: ' + str(n_fft))\n        elif freq['nfft_seg'].isChecked():\n            n_fft = None\n\n        # Normalization data preparation\n        if norm not in ['none', 'by integral of each segment']:\n            norm_evt_type = None\n            norm_stage = None\n            norm_chan = None\n            ncat = (0, 0, 0, 0)\n\n            if norm == 'by mean of event type(s)':\n                norm_chan = [x + ' (' + self.idx_group.currentText() + ''\n                                    ')'for x in self.one_grp['chan_to_plot']]\n                norm_evt_type = [x.text() for x in \\\n                                 freq['norm_evt_type'].selectedItems()]\n\n            if norm == 'by mean of stage(s)':\n                norm_stage = [x.text() for x in \\\n                              freq['norm_stage'].selectedItems()]\n\n            if norm_concat:\n                ncat = (1, 1, 1, 1)\n\n            lg.info(' '.join(['Getting segments for norm. cat: ', str(ncat),\n                              'evt_type', str(norm_evt_type), 'stage',\n                              str(norm_stage), 'chan', str(norm_chan)]))\n            norm_seg = fetch(self.parent.info.dataset,\n                                    self.parent.notes.annot, ncat,\n                                    evt_type=norm_evt_type, stage=norm_stage,\n                                    chan_full=norm_chan)\n\n            if not norm_seg.segments:\n                msg = 'No valid normalization signal found.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('Error fetching data')\n                error_dialog.showMessage(msg)\n                progress.cancel()\n                return\n\n            norm_seg.read_data(self.chan, ref_chan=self.one_grp['ref_chan'],\n                               grp_name=self.one_grp['name'], parent=None)\n\n            if prep:\n                norm_seg = self.transform_data(norm_seg)\n\n            all_Sxx = []\n            for seg in norm_seg:\n                dat = seg['data']\n                if prep:\n                    dat = seg['trans_data']\n\n                try:\n                    Sxx = frequency(dat, output=output, scaling=scaling,\n                                sides=sides, taper=taper,\n                                halfbandwidth=halfbandwidth, NW=NW,\n                                duration=duration, overlap=overlap, step=step,\n                                detrend=detrend, n_fft=n_fft, \n                                log_trans=log_trans, centend=centend)\n                except ValueError:\n                    msg = ('Value error encountered in frequency '\n                           'transformation for normalization reference data.'\n                           '\\nIf using time-averaging, make sure the '\n                           'normalization data segments are at least as long '\n                           'as the time window.')\n                    error_dialog = QErrorMessage(self)\n                    error_dialog.setWindowTitle('Error transforming data')\n                    error_dialog.showMessage(msg)\n                    progress.cancel()\n                    return\n\n                all_Sxx.append(Sxx)\n\n            nSxx = ChanFreq()\n            nSxx.s_freq = Sxx.s_freq\n            nSxx.axis['freq'] = Sxx.axis['freq']\n            nSxx.axis['chan'] = Sxx.axis['chan']\n            nSxx.data = empty(1, dtype='O')\n            nSxx.data[0] = empty((Sxx.number_of('chan')[0],\n                     Sxx.number_of('freq')[0]), dtype='f')\n            nSxx.data[0] = mean(\n                    stack([x()[0] for x in all_Sxx], axis=2), axis=2)\n\n        # end of normalization data prep\n\n        lg.info(' '.join(['Freq settings:', output, scaling, 'sides:',\n                         str(sides), taper, 'hbw:', str(halfbandwidth), 'NW:',\n                         str(NW), 'dur:', str(duration), 'overlap:',\n                         str(overlap), 'step:', str(step), 'detrend:',\n                         str(detrend), 'n_fft:', str(n_fft), 'norm',\n                         str(norm), 'log:', str(log_trans), 'central tendency',\n                         str(centend)]))\n\n        # Main frequency analysis\n        xfreq = []\n        for i, seg in enumerate(self.data):\n            new_seg = dict(seg)\n            data = seg['data']\n\n            if prep:\n                data = seg['trans_data']\n\n            timeline = seg['data'].axis['time'][0]\n            new_seg['start'] = timeline[0]\n            new_seg['end'] = timeline[-1]\n            new_seg['duration'] = len(timeline) / data.s_freq\n\n            try:\n                Sxx = frequency(data, output=output, scaling=scaling,\n                                sides=sides, taper=taper,\n                                halfbandwidth=halfbandwidth, NW=NW,\n                                duration=duration, overlap=overlap, step=step,\n                                detrend=detrend, n_fft=n_fft, \n                                log_trans=log_trans, centend=centend)\n            except SyntaxError:\n                msg = 'Value error encountered in frequency transformation.'\n                error_dialog = QErrorMessage(self)\n                error_dialog.setWindowTitle('Error transforming data')\n                error_dialog.showMessage(msg)\n                progress.cancel()\n                return\n\n            if norm != 'none':\n\n                for j, chan in enumerate(Sxx.axis['chan'][0]):\n\n                    dat = Sxx.data[0][j,:]\n                    sf = Sxx.axis['freq'][0]\n                    f_res = sf[1] - sf[0] # frequency resolution\n\n                    if norm == 'by integral of each segment':\n                        norm_dat = sum(dat) * f_res # integral by midpoint rule\n                    else:\n                        norm_dat = nSxx(chan=chan)[0]\n\n                    Sxx.data[0][j,:] = dat / norm_dat\n\n            new_seg['data'] = Sxx\n            xfreq.append(new_seg)\n\n            progress.setValue(i)\n            if progress.wasCanceled():\n                msg = 'Analysis canceled by user.'\n                self.parent.statusBar().showMessage(msg)\n                return\n\n        progress.close()\n\n        return xfreq"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_freq_cross(self, csd, asd, output='coherence'):\n        if output == 'coherence':\n            coh_list = []\n\n            for i in range(len(csd)):\n                dat = ChanFreq()\n                dat.data = empty(1, dtype='O')\n                dat.data[0] = empty((1, csd[i]['data'].number_of('freq')[0]),\n                        dtype='f')\n                dat.axis['freq'] = empty(1, dtype='O')\n                dat.axis['freq'][0] = csd[i]['data'].axis['freq'][0]\n                dat.axis['chan'] = csd[i]['data'].axis['chan']\n\n                newdict = dict(csd[i])\n                newdict['data'] = dat\n\n                Pxy = csd[i]['data'].data[0][0]\n                Pxx = asd[i]['data'].data[0][0]\n                Pyy = asd[i]['data'].data[0][1]\n\n                Cxy = abs(Pxy)**2 / Pxx / Pyy # ms coherence\n\n                dat.data[0][0, :] = Cxy\n                coh_list.append(newdict)\n\n            out = (coh_list,)\n\n        elif output == 'gainphase':\n            xg_list = []\n            yg_list = []\n            ph_list = []\n\n            for i in range(len(csd)):\n                xgain = ChanFreq()\n                xgain.data = empty(1, dtype='O')\n                xgain.data[0] = empty((1, csd[i]['data'].number_of('freq')[0]),\n                        dtype='f')\n                xgain.axis['freq'] = empty(1, dtype='O')\n                xgain.axis['freq'][0] = csd[i]['data'].axis['freq'][0]\n                xgain.axis['chan'] = empty(1, dtype='O')\n\n                ygain = ChanFreq()\n                ygain.data = empty(1, dtype='O')\n                ygain.data[0] = empty((1, csd[i]['data'].number_of('freq')[0]),\n                        dtype='f')\n                ygain.axis['freq'] = empty(1, dtype='O')\n                ygain.axis['freq'][0] = csd[i]['data'].axis['freq'][0]\n                ygain.axis['chan'] = empty(1, dtype='O')\n\n                phase = ChanFreq()\n                phase.data = empty(1, dtype='O')\n                phase.data[0] = empty((1, csd[i]['data'].number_of('freq')[0]),\n                        dtype='f')\n                phase.axis['freq'] = empty(1, dtype='O')\n                phase.axis['freq'][0] = csd[i]['data'].axis['freq'][0]\n                phase.axis['chan'] = empty(1, dtype='O')\n\n                xchan = asd[i]['data'].axis['chan'][0][0]\n                ychan = asd[i]['data'].axis['chan'][0][1]\n                xgain.axis['chan'][0] = asarray(['-->'.join((xchan, ychan))],\n                          dtype='U')\n                ygain.axis['chan'][0] = asarray(['-->'.join((ychan, xchan))],\n                          dtype='U')\n                phase.axis['chan'][0] = asarray(['-->'.join((xchan, ychan))],\n                          dtype='U')\n\n                Pxy = csd[i]['data'].data[0][0]\n                Pxx = asd[i]['data'].data[0][0]\n                Pyy = asd[i]['data'].data[0][1]\n\n                Hx = Pxy / Pxx\n                Hy = Pxy / Pyy\n\n                xgain.data[0][0, :] = abs(Hx)\n                ygain.data[0][0, :] = abs(Hy)\n\n                phase.data[0][0, :] = angle(Hx, deg=True)\n                # phase is same in both directions, since Pxx and Pyy are real\n\n                xg_dict = dict(csd[i])\n                xg_dict['data'] = xgain\n                xg_list.append(xg_dict)\n\n                yg_dict = dict(csd[i])\n                yg_dict['data'] = ygain\n                yg_list.append(yg_dict)\n\n                ph_dict = dict(csd[i])\n                ph_dict['data'] = phase\n                ph_list.append(ph_dict)\n\n            out = (xg_list, yg_list, ph_list)\n\n        return out", "response": "Compute cross - spectrum gain phase shift and or coherence."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots mean frequency spectrum and display in dialog.", "response": "def plot_freq(self, x, y, title='', ylabel=None, scale='semilogy'):\n        \"\"\"Plot mean frequency spectrum and display in dialog.\n\n        Parameters\n        ----------\n        x : list\n            vector with frequencies\n        y : ndarray\n            vector with amplitudes\n        title : str\n            plot title\n        ylabel : str\n            plot y label\n        scale : str\n            semilogy, loglog or linear\n        \"\"\"\n        freq = self.frequency\n        scaling = freq['scaling'].get_value()\n\n        if ylabel is None:\n            if freq['complex'].get_value():\n                ylabel = 'Amplitude (uV)'\n            elif 'power' == scaling:\n                ylabel = 'Power spectral density (uV ** 2 / Hz)'\n            elif 'energy' == scaling:\n                ylabel = 'Energy spectral density (uV ** 2)'\n\n        self.parent.plot_dialog = PlotDialog(self.parent)\n        self.parent.plot_dialog.canvas.plot(x, y, title, ylabel, scale=scale)\n        self.parent.show_plot_dialog()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a report of the FOOOF model.", "response": "def report_fooof(self, x, y, suffix):\n        \"\"\"Create FOOOF (fitting oscillations and 1/f) report.\n\n        Parameters\n        ----------\n        x : ndarray\n            vector with frequencies\n        y : ndarray\n            vector with amplitudes\n        \"\"\"\n        filename = splitext(self.filename)[0] + '_' + suffix + '_fooof.csv'\n\n        freq = self.frequency\n        freq_range = [freq['fo_min_freq'].get_value(),\n                      freq['fo_max_freq'].get_value()]\n        pk_thresh = freq['fo_pk_thresh'].get_value()\n        pk_width = [freq['fo_pk_width_min'].get_value(),\n                    freq['fo_pk_width_max'].get_value()]\n        max_n_pk = freq['fo_max_n_pk'].get_value()\n        min_pk_amp = freq['fo_min_pk_amp'].get_value()\n        bg_mode = freq['fo_bg_mode'].get_value()\n\n        if max_n_pk == 0:\n            max_n_pk = inf\n\n        if FOOOF is None:\n            lg.warning('\"fooof\" package is required for this function, run \"pip install fooof\"')\n            return\n\n        fm = FOOOF(peak_width_limits=pk_width, max_n_peaks=max_n_pk,\n                   min_peak_amplitude=min_pk_amp, peak_threshold=pk_thresh,\n                   background_mode=bg_mode)\n        fm.fit(x, y, freq_range)\n\n        with open(filename, 'w', newline='') as f:\n            lg.info('Writing to ' + str(filename))\n            csv_file = writer(f)\n            csv_file.writerow(['Wonambi v{}'.format(__version__)])\n            csv_file.writerow(['FOOOF - POWER SPECTRUM MODEL'])\n            csv_file.writerow('')\n            csv_file.writerow(['The model was run on the frequency range '\n                              '{} - {} Hz'.format(int(floor(fm.freq_range[0])),\n                               int(ceil(fm.freq_range[1])))])\n            csv_file.writerow(['Frequency Resolution is {:1.2f} Hz'.format(\n                    fm.freq_res)])\n            csv_file.writerow('')\n            csv_file.writerow(['Background Parameters (offset, ' + \\\n                    ('knee, ' if fm.background_mode == 'knee' else '') + \\\n                    'slope): ' + ', '.join(['{:2.4f}'] * \\\n                    len(fm.background_params_)).format(\n                            *fm.background_params_)])\n            csv_file.writerow('')\n            csv_file.writerow(['{} peaks were found:'.format(\n                    len(fm.peak_params_))])\n            csv_file.writerow('')\n            csv_file.writerow(['Index', 'CF', 'Amp', 'BW'])\n\n            for i, op in enumerate(fm.peak_params_):\n                csv_file.writerow([i, op[0], op[1], op[2]])\n\n            csv_file.writerow('')\n            csv_file.writerow(['Goodness of fit metrics:'])\n            csv_file.writerow(['R^2 of model fit is {:5.4f}'.format(\n                    fm.r_squared_)])\n            csv_file.writerow(['Root mean squared error is {:5.4f}'.format(\n                    fm.error_)])\n            csv_file.writerow('')\n            csv_file.writerow(['Haller M, Donoghue T, Peterson E, Varma P, '\n                               'Sebastian P, Gao R, Noto T, Knight RT, '\n                               'Shestyuk A, Voytek B (2018) Parameterizing '\n                               'Neural Power Spectra. bioRxiv, 299859. doi: '\n                               'https://doi.org/10.1101/299859'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_pac(self):\n        n_segments = sum([len(x['data'].axis['chan'][0]) for x in self.data])\n        progress = QProgressDialog('Computing PAC', 'Abort',\n                                   0, n_segments - 1, self)\n        progress.setWindowModality(Qt.ApplicationModal)\n\n        pac = self.pac\n        idpac = (pac['metric'].currentIndex() + 1,\n                 pac['surro_method'].currentIndex(),\n                 pac['surro_norm'].currentIndex())\n        fpha = freq_from_str(self.pac['fpha'].get_value())\n        famp = freq_from_str(self.pac['famp'].get_value())\n        nbins = self.pac['nbin'][1].get_value()\n        nblocks = self.pac['surro']['nblocks'][1].get_value()\n\n        if pac['hilbert_on'].isChecked():\n            dcomplex = 'hilbert'\n            filt = self.pac['hilbert']['filt'][1].get_value()\n            cycle = (self.pac['hilbert']['cycle_pha'][1].get_value(),\n                     self.pac['hilbert']['cycle_amp'][1].get_value())\n            filtorder = self.pac['hilbert']['order'][1].get_value()\n            width = 7 # not used\n        elif pac['wavelet_on'].isChecked():\n            dcomplex = 'wavelet'\n            filt = 'fir1' # not used\n            cycle = (3, 6) # not used\n            filtorder = 3 # not used\n            width = self.pac['wav_width'][1].get_value()\n\n        lg.info(' '.join([str(x) for x in ['Instantiating PAC:', 'idpac:',\n                          idpac, 'fpha:', fpha, 'famp:', famp,  'dcomplex:',\n                          dcomplex, 'filt:', filt, 'cycle:', cycle,\n                          'filtorder:', filtorder, 'width:', width,\n                          'nbins:', nbins, 'nblocks:', nblocks]]))\n        p = Pac(idpac=idpac, fpha=fpha, famp=famp, dcomplex=dcomplex,\n                filt=filt, cycle=cycle, filtorder=filtorder, width=width,\n                nbins=nbins, nblocks=nblocks)\n\n        nperm = self.pac['surro']['nperm'][1].get_value()\n        optimize = self.pac['optimize'].get_value()\n        get_pval = self.pac['surro']['pval'][0].get_value()\n        get_surro = self.pac['surro']['save_surro'][0].get_value()\n        njobs = self.pac['njobs'].get_value()\n\n        if optimize == 'True':\n            optimize = True\n        elif optimize == 'False':\n            optimize = False\n\n        xpac = {}\n\n        all_chan = sorted(set(\n                [x for y in self.data for x in y['data'].axis['chan'][0]]))\n\n        counter = 0\n        for chan in all_chan:\n            batch = []\n            batch_dat = []\n\n            for i, j in enumerate(self.data):\n\n                if self.pac['prep'].get_value():\n                    data = j['trans_data']\n                else:\n                    data = j['data']\n\n                if chan in data.axis['chan'][0]:\n                    batch.append(j)\n\n                    if idpac[1] == 1:\n                        batch_dat.append(data(chan=chan)[0])\n\n            xpac[chan] = {}\n            xpac[chan]['data'] = zeros((len(batch), len(famp), len(fpha)))\n            xpac[chan]['times'] = []\n            xpac[chan]['duration'] = []\n            xpac[chan]['stage'] = []\n            xpac[chan]['cycle'] = []\n            xpac[chan]['name'] = []\n            xpac[chan]['n_stitch'] = []\n\n            if get_pval:\n                xpac[chan]['pval'] = zeros((len(batch), len(famp), len(fpha)))\n\n            if idpac[2] > 0:\n                xpac[chan]['surro'] = zeros((len(batch), nperm,\n                                            len(famp), len(fpha)))\n\n            for i, j in enumerate(batch):\n                progress.setValue(counter)\n                counter += 1\n\n                if self.pac['prep'].get_value():\n                    data = j['trans_data']\n                else:\n                    data = j['data']\n\n                sf = data.s_freq\n\n                if idpac[1] == 1:\n                    new_batch_dat = list(batch_dat)\n                    new_batch_dat.insert(0, new_batch_dat.pop(i))\n                    dat = asarray(new_batch_dat)\n                else:\n                    dat = data(chan=chan)[0]\n\n                timeline = data.axis['time'][0]\n                xpac[chan]['times'].append((timeline[0], timeline[-1]))\n                duration = len(timeline) / sf\n                xpac[chan]['duration'].append(duration)\n                xpac[chan]['stage'].append(j['stage'])\n                xpac[chan]['cycle'].append(j['cycle'])\n                xpac[chan]['name'].append(j['name'])\n                xpac[chan]['n_stitch'].append(j['n_stitch'])\n\n                out = p.filterfit(sf=sf, xpha=dat, xamp=None, axis=1, traxis=0,\n                                  nperm=nperm, optimize=optimize,\n                                  get_pval=get_pval, get_surro=get_surro,\n                                  njobs=njobs)\n\n                if get_pval:\n\n                    if get_surro:\n                        (xpac[chan]['data'][i, :, :],\n                         xpac[chan]['pval'][i, :, :],\n                         xpac[chan]['surro'][i, :, :, :]) = (out[0][:, :, 0],\n                             out[1][:, :, 0], out[2][:, :, :, 0])\n                    else:\n                        (xpac[chan]['data'][i, :, :],\n                         xpac[chan]['pval'][i, :, :]) = (out[0][:, :, 0],\n                             out[1][:, :, 0])\n\n                elif get_surro:\n                    (xpac[chan]['data'][i, :, :],\n                     xpac[chan]['surro'][i, :, :, :]) = (out[0][:, :, 0],\n                         out[1][:, :, :, 0])\n\n                else:\n                    xpac[chan]['data'][i, :, :] = out[:, :, 0]\n\n                if progress.wasCanceled():\n                    msg = 'Analysis canceled by user.'\n                    self.parent.statusBar().showMessage(msg)\n                    return\n\n        #progress.setValue(counter)\n\n        return xpac, fpha, famp", "response": "Compute phase - amplitude coupling values from data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_pac(self, xpac, fpha, famp, desc):\n        filename = splitext(self.filename)[0] + '_pac.csv'\n\n        heading_row_1 = ['Segment index',\n                       'Start time',\n                       'End time',\n                       'Duration',\n                       'Stitch',\n                       'Stage',\n                       'Cycle',\n                       'Event type',\n                       'Channel',\n                       ]\n        spacer = [''] * (len(heading_row_1) - 1)\n        heading_row_2 = []\n\n        for fp in fpha:\n            fp_str = str(fp[0]) + '-' + str(fp[1])\n\n            for fa in famp:\n                fa_str = str(fa[0]) + '-' + str(fa[1])\n                heading_row_2.append(fp_str + '_' + fa_str + '_pac')\n\n        if 'pval' in xpac[list(xpac.keys())[0]].keys():\n            heading_row_3 = [x[:-4] + '_pval' for x in heading_row_2]\n            heading_row_2.extend(heading_row_3)\n\n        with open(filename, 'w', newline='') as f:\n            lg.info('Writing to ' + str(filename))\n            csv_file = writer(f)\n            csv_file.writerow(['Wonambi v{}'.format(__version__)])\n            csv_file.writerow(heading_row_1 + heading_row_2)\n            csv_file.writerow(['Mean'] + spacer + list(desc['mean']))\n            csv_file.writerow(['SD'] + spacer + list(desc['sd']))\n            csv_file.writerow(['Mean of ln'] + spacer + list(desc['mean_log']))\n            csv_file.writerow(['SD of ln'] + spacer + list(desc['sd_log']))\n            idx = 0\n\n            for chan in xpac.keys():\n\n                for i, j in enumerate(xpac[chan]['times']):\n                    idx += 1\n\n                    cyc = None\n                    if xpac[chan]['cycle'][i] is not None:\n                        cyc = xpac[chan]['cycle'][i][2]\n\n                    data_row = list(ravel(xpac[chan]['data'][i, :, :]))\n\n                    pval_row = []\n                    if 'pval' in xpac[chan]:\n                        pval_row = list(ravel(xpac[chan]['pval'][i, :, :]))\n\n                    csv_file.writerow([idx,\n                                       j[0],\n                                       j[1],\n                                       xpac[chan]['duration'][i],\n                                       xpac[chan]['n_stitch'][i],\n                                       xpac[chan]['stage'][i],\n                                       cyc,\n                                       xpac[chan]['name'][i],\n                                       chan,\n                                       ] + data_row + pval_row)", "response": "Write PAC analysis data to CSV."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_title(self, chan, cycle, stage, evt_type):\n        cyc_str = None\n        if cycle is not None:\n            cyc_str = [str(c[2]) for c in cycle]\n            cyc_str[0] = 'cycle ' + cyc_str[0]\n\n        title = [' + '.join([str(x) for x in y]) for y in [chan, cyc_str,\n                 stage, evt_type] if y is not None]\n\n        return ', '.join(title)", "response": "Make a title for plots etc."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot(self, x, y, title, ylabel, scale='semilogy', idx_lim=(1, -1)):\n        x = x[slice(*idx_lim)]\n        y = y[slice(*idx_lim)]\n        ax = self.figure.add_subplot(111)\n        ax.set_title(title)\n        ax.set_xlabel('Frequency (Hz)')\n        ax.set_ylabel(ylabel)\n\n        if 'semilogy' == scale:\n            ax.semilogy(x, y, 'r-')\n        elif 'loglog' == scale:\n            ax.loglog(x, y, 'r-')\n        elif 'linear' == scale:\n            ax.plot(x, y, 'r-')", "response": "Plot the data.\n\n        Parameters\n        ----------\n        x : ndarray\n            vector with frequencies\n        y : ndarray\n            vector with amplitudes\n        title : str\n            title of the plot, to appear above it\n        ylabel : str\n            label for the y-axis\n        scale : str\n            'log y-axis', 'log both axes' or 'linear', to set axis scaling\n        idx_lim : tuple of (int or None)\n            indices of the data to plot. by default, the first value is left\n            out, because of assymptotic tendencies near 0 Hz."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the basic dialog.", "response": "def create_dialog(self):\n        \"\"\"Create the basic dialog.\"\"\"\n        self.bbox = QDialogButtonBox(QDialogButtonBox.Close)\n        self.idx_close = self.bbox.button(QDialogButtonBox.Close)\n        self.idx_close.pressed.connect(self.reject)\n\n        btnlayout = QHBoxLayout()\n        btnlayout.addStretch(1)\n        btnlayout.addWidget(self.bbox)\n\n        layout = QVBoxLayout()\n        layout.addWidget(self.toolbar)\n        layout.addWidget(self.canvas)\n        layout.addLayout(btnlayout)\n        layout.addStretch(1)\n        self.setLayout(layout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detect_HouseDetector(dat_orig, s_freq, time, opts):\n    nperseg = int(opts.spectrogram['dur'] * s_freq)\n    overlap = opts.spectrogram['overlap']\n    noverlap = int(overlap * nperseg)\n    detrend = opts.spectrogram['detrend']\n    min_interval = int(opts.min_interval * s_freq)\n    \n    sf, t, dat_det = spectrogram(dat_orig, \n                                 fs=s_freq, \n                                 nperseg=nperseg, \n                                 noverlap=noverlap, \n                                 detrend=detrend)\n    freq1 = opts.freq_band1\n    freq2 = opts.freq_band2\n    f0 = asarray([abs(freq1[0] - x) for x in sf]).argmin() if freq1[0] else None\n    f1 = asarray([abs(freq1[1] - x) for x in sf]).argmin() if freq1[1] else None\n    f2 = asarray([abs(freq2[1] - x) for x in sf]).argmin() if freq2[1] else None\n    f3 = asarray([abs(freq2[1] - x) for x in sf]).argmin() if freq2[1] else None\n    \n    dat_eq1 = zeros(dat_det.shape[1])\n    dat_eq2 = zeros(dat_det.shape[1])\n    for i in range(dat_det.shape[1]):\n        dat_eq1[i] = splitpoint(dat_det[f0:f1, i], sf[f0:f1])\n        dat_eq2[i] = splitpoint(dat_det[f2:f3, i], sf[f2:f3])\n        \n    dat_acc = dat_eq1[1:] / dat_eq1[:-1]\n    starts = dat_acc >= opts.det_thresh\n    print(f'starts: {sum(starts)}')\n    print(f'1.01: {sum(dat_acc >= 1.01)}')\n    print(f'1.02: {sum(dat_acc >= 1.02)}')\n    print(f'1.05: {sum(dat_acc >= 1.05)}')\n    print(f'1.1: {sum(dat_acc >= 1.1)}')\n    print(f'1.2: {sum(dat_acc >= 1.2)}')\n    print(f'1.3: {sum(dat_acc >= 1.3)}')\n    print(f'1.4: {sum(dat_acc >= 1.4)}')\n    print(f'1.5: {sum(dat_acc >= 1.5)}')\n    print(f'1.75: {sum(dat_acc >= 1.75)}')\n    print(f'2: {sum(dat_acc >= 2)}')\n    print(f'2.5: {sum(dat_acc >= 2.5)}')\n    print(f'3: {sum(dat_acc >= 3)}')\n    print(f'5: {sum(dat_acc >= 5)}')\n    print(f'10: {sum(dat_acc >= 10)}')\n    \n    if starts.any():\n        new_starts = asarray(zeros(len(starts)), dtype=bool)\n        ends = asarray(zeros(len(starts) - 1), dtype=bool)\n        iter_len = len(starts) - 2\n        i = 0\n        while i <= iter_len:\n            if starts[i]:\n                for j, k in enumerate(dat_eq2[i + 2:-1]):\n                    if k < dat_eq2[i] * opts.det_thresh_end:\n                        new_starts[i] = True\n                        ends[i + j + 1] = True\n                        break\n                i += j + min_interval\n            else:\n                i += 1\n        \n        if sum(new_starts) > sum(ends): # a start without an end\n            ends[-1] = True\n        \n        events = vstack((where(new_starts == True)[0] + 1,\n                         where(ends == True)[0] + 2)).T\n        if overlap: \n            events = events - int(1 / 2 / overlap) # from win centre to win start\n        events = events * (nperseg - noverlap) # upsample\n        print(f'n_events before dur = {events.shape}')\n        events = within_duration(events, time, opts.duration)\n        print(f'n_events after dur = {events.shape}')\n        events = remove_straddlers(events, time, s_freq)\n        print(f'n_events after strad = {events.shape}')\n    \n        ar_in_chan = make_arousals(events, time, s_freq)\n        \n    else:\n        lg.info('No arousals found')\n        ar_in_chan = []\n\n    return ar_in_chan", "response": "House arousal detection.\n\n    Parameters\n    ----------\n    dat_orig : ndarray (dtype='float')\n        vector with the data for one channel\n    s_freq : float\n        sampling frequency\n    time : ndarray (dtype='float')\n        vector with the time points for each sample\n    opts : instance of 'DetectSlowWave'\n        'duration' : tuple of float\n            min and max duration of arousal\n\n    Returns\n    -------\n    list of dict\n        list of detected arousals\n    float\n        arousal density, per 30-s epoch"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_arousals(events, time, s_freq):\n    arousals = []\n    for ev in events:\n        one_ar = {'start': time[ev[0]],\n                  'end': time[ev[1] - 1],\n                  'dur': (ev[1] - ev[0]) / s_freq,\n                  }\n        arousals.append(one_ar)\n\n    return arousals", "response": "Create arousal dict for each arousal based on events of time points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert absolute time into samples.", "response": "def _convert_time_to_sample(abs_time, dataset):\n    \"\"\"Convert absolute time into samples.\n\n    Parameters\n    ----------\n    abs_time : dat\n        if it's int or float, it's assumed it's s;\n        if it's timedelta, it's assumed from the start of the recording;\n        if it's datetime, it's assumed it's absolute time.\n    dataset : instance of wonambi.Dataset\n        dataset to get sampling frequency and start time\n\n    Returns\n    -------\n    int\n        sample (from the starting of the recording).\n    \"\"\"\n    if isinstance(abs_time, datetime):\n        abs_time = abs_time - dataset.header['start_time']\n\n    if not isinstance(abs_time, timedelta):\n        try:\n            abs_time = timedelta(seconds=float(abs_time))\n        except TypeError as err:\n            if isinstance(abs_time, int64):\n                # timedelta and int64: http://bugs.python.org/issue5476\n                abs_time = timedelta(seconds=int(abs_time))\n            else:\n                raise err\n\n    sample = int(ceil(abs_time.total_seconds() * dataset.header['s_freq']))\n    return sample"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_format(filename):\n    filename = Path(filename)\n\n    if filename.is_dir():\n        if list(filename.glob('*.stc')) and list(filename.glob('*.erd')):\n            return Ktlx\n        elif (filename / 'patient.info').exists():\n            return Moberg\n        elif (filename / 'info.xml').exists():\n            return EgiMff\n        elif list(filename.glob('*.openephys')):\n            return OpenEphys\n        elif list(filename.glob('*.txt')):\n            return Text\n        else:\n            raise UnrecognizedFormat('Unrecognized format for directory ' +\n                                     str(filename))\n    else:\n        if filename.suffix == '.won':\n            return Wonambi\n\n        if filename.suffix.lower() == '.trc':\n            return Micromed\n\n        if filename.suffix == '.set':\n            return EEGLAB\n\n        if filename.suffix == '.edf':\n            return Edf\n\n        if filename.suffix == '.abf':\n            return Abf\n\n        if filename.suffix == '.vhdr' or filename.suffix == '.eeg':\n            return BrainVision\n\n        if filename.suffix == '.dat':  # very general\n            try:\n                _read_header_length(filename)\n\n            except (AttributeError, ValueError):  # there is no HeaderLen\n                pass\n\n            else:\n                return BCI2000\n\n        with filename.open('rb') as f:\n            file_header = f.read(8)\n            if file_header in (b'NEURALCD', b'NEURALSG', b'NEURALEV'):\n                return BlackRock\n            elif file_header[:6] == b'MATLAB':  # we might need to read more\n                return FieldTrip\n        \n        if filename.suffix.lower() == '.txt':\n            with filename.open('rt') as f:\n                first_line = f.readline()\n                if '.rr' in first_line[-4:]:\n                    return LyonRRI\n        \n        else:\n            raise UnrecognizedFormat('Unrecognized format for file ' +\n                                     str(filename))", "response": "Detects the format of a file and returns the appropriate object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_videos(self, begtime=None, endtime=None):\n        if isinstance(begtime, datetime):\n            begtime = begtime - self.header['start_time']\n        if isinstance(begtime, timedelta):\n            begtime = begtime.total_seconds()\n        if isinstance(endtime, datetime):\n            endtime = endtime - self.header['start_time']\n        if isinstance(endtime, timedelta):\n            endtime = endtime.total_seconds()\n\n        videos = self.dataset.return_videos(begtime, endtime)\n        \"\"\"\n        try\n        except AttributeError:\n            lg.debug('This format does not have video')\n            videos = None\n        \"\"\"\n        return videos", "response": "Return a list of videos with start and end times for a period."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_data(self, chan=None, begtime=None, endtime=None, begsam=None,\n                  endsam=None, s_freq=None):\n        \"\"\"Read the data and creates a ChanTime instance\n\n        Parameters\n        ----------\n        chan : list of strings\n            names of the channels to read\n        begtime : int or datedelta or datetime or list\n            start of the data to read;\n            if it's int or float, it's assumed it's s;\n            if it's timedelta, it's assumed from the start of the recording;\n            if it's datetime, it's assumed it's absolute time.\n            It can also be a list of any of the above type.\n        endtime : int or datedelta or datetime\n            end of the data to read;\n            if it's int or float, it's assumed it's s;\n            if it's timedelta, it's assumed from the start of the recording;\n            if it's datetime, it's assumed it's absolute time.\n            It can also be a list of any of the above type.\n        begsam : int\n            first sample (this sample will be included)\n        endsam : int\n            last sample (this sample will NOT be included)\n        s_freq : int\n            sampling frequency of the data\n\n        Returns\n        -------\n        An instance of ChanTime\n\n        Notes\n        -----\n        begsam and endsam follow Python convention, which starts at zero,\n        includes begsam but DOES NOT include endsam.\n\n        If begtime and endtime are a list, they both need the exact same\n        length and the data will be stored in trials.\n\n        If neither begtime or begsam are specified, it starts from the first\n        sample. If neither endtime or endsam are specified, it reads until the\n        end.\n        \"\"\"\n        data = ChanTime()\n        data.start_time = self.header['start_time']\n        data.s_freq = s_freq = s_freq if s_freq else self.header['s_freq']\n\n        if chan is None:\n            chan = self.header['chan_name']\n        if not (isinstance(chan, list) or isinstance(chan, tuple)):\n            raise TypeError('Parameter \"chan\" should be a list')\n        add_ref = False\n        if '_REF' in chan:\n            add_ref = True\n            chan[:] = [x for x in chan if x != '_REF']\n        idx_chan = [self.header['chan_name'].index(x) for x in chan]\n\n        if begtime is None and begsam is None:\n            begsam = 0\n        if endtime is None and endsam is None:\n            endsam = self.header['n_samples']\n\n        if begtime is not None:\n            if not isinstance(begtime, list):\n                begtime = [begtime]\n            begsam = []\n            for one_begtime in begtime:\n                begsam.append(_convert_time_to_sample(one_begtime, self))\n        if endtime is not None:\n            if not isinstance(endtime, list):\n                endtime = [endtime]\n            endsam = []\n            for one_endtime in endtime:\n                endsam.append(_convert_time_to_sample(one_endtime, self))\n\n        if not isinstance(begsam, list):\n            begsam = [begsam]\n        if not isinstance(endsam, list):\n            endsam = [endsam]\n\n        if len(begsam) != len(endsam):\n            raise ValueError('There should be the same number of start and ' +\n                             'end point')\n        n_trl = len(begsam)\n\n        data.axis['chan'] = empty(n_trl, dtype='O')\n        data.axis['time'] = empty(n_trl, dtype='O')\n        data.data = empty(n_trl, dtype='O')\n\n        for i, one_begsam, one_endsam in zip(range(n_trl), begsam, endsam):            \n            dataset = self.dataset\n            lg.debug('begsam {0: 6}, endsam {1: 6}'.format(one_begsam,\n                     one_endsam))\n            dat = dataset.return_dat(idx_chan, one_begsam, one_endsam)\n            chan_in_dat = chan\n            \n            if add_ref:\n                zero_ref = zeros((1, one_endsam - one_begsam))\n                dat = concatenate((dat, zero_ref), axis=0)\n                chan_in_dat.append('_REF')\n            \n            data.data[i] = dat\n            data.axis['chan'][i] = asarray(chan_in_dat, dtype='U')\n            data.axis['time'][i] = (arange(one_begsam, one_endsam) / s_freq)\n\n        return data", "response": "Read the data and creates a ChanTime instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_dat(x):\n    n_smp = int(len(x) / DATA_PRECISION)\n    dat = zeros(n_smp)\n\n    for i in range(n_smp):\n        i0 = i * DATA_PRECISION\n        i1 = i0 + DATA_PRECISION\n        dat[i] = int.from_bytes(x[i0:i1], byteorder='little', signed=True)\n\n    return dat", "response": "read 24bit binary data and convert them to numpy."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the header for further use.", "response": "def return_hdr(self):\n        \"\"\"Return the header for further use.\n\n        Returns\n        -------\n        subj_id : str\n            subject identification code\n        start_time : datetime\n            start time of the dataset\n        s_freq : float\n            sampling frequency\n        chan_name : list of str\n            list of all the channels\n        n_samples : int\n            number of samples in the dataset\n        orig : dict\n            additional information taken directly from the header\n\n        Notes\n        -----\n        the time is probably in \"local\" Unix Time, which is in the local time\n        zone, so we read it as \"UTC\" (meaning, do not apply timezone\n        transformation) and then remove timezone info.\n        The only doubt I have is how to interpret the \"SystemOffset\" time.\n        I assume it's in s, and that would fix most of the time zone problems,\n        but it does not take into account DST. Or maybe \"SystemOffset\" is in\n        micros and we need to apply the correct time zone to TimeStamp Unix\n        time. This needs to be tested with a Moberg system.\n        \"\"\"\n        subj_id = str()\n        patient = parse(join(self.filename, 'patient.info'))\n        for patientname in ['PatientFirstName', 'PatientLastName']:\n            subj_id += patient.findall(patientname)[0].text.strip()\n\n        unix_time = int(patient.findall('TimeStamp')[0].text.strip()) / 1e6\n        system_offset = int(patient.findall('SystemOffset')[0].text.strip())\n        start_time = (datetime.fromtimestamp(unix_time, TIMEZONE) +\n                      timedelta(seconds=system_offset)).replace(tzinfo=None)\n\n        s_freq = 256  # could not find it in the text files\n\n        montage = parse(join(self.filename, 'Montage.xml'))\n        mont = montage.find('Montage')\n        chan_name = [chan.get('lead') for chan in mont.findall('Channel')\n                     if chan.get('role') == 'REFERENTIAL_INPUT']\n\n        data_size = getsize(join(self.filename, EEG_FILE))\n        n_samples = int(data_size / DATA_PRECISION / len(chan_name))\n        self.n_smp = n_samples\n\n        self.n_chan = len(chan_name)\n        settings = parse(join(self.filename, SETTINGS_FILE))\n        conversion = settings.findall('SampleConversion')[0].text.strip()\n\n        dig_min, dig_max, anl_min, anl_max = [int(x) for x in\n                                              conversion.split(',')]\n\n        if dig_max == -dig_min and anl_max == -anl_min:\n            self.convertion = lambda dat: dat / dig_max * anl_max\n        else:  # pragma: no cover\n            self.convertion = lambda dat: ((dat + dig_min) /\n                                           (dig_max - dig_min) *\n                                           (anl_max - anl_min) + anl_min)\n\n        orig = {'patient': patient,\n                'montage': montage,\n                'settings': settings,\n                }\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef return_dat(self, chan, begsam, endsam):\n        if begsam < 0:\n            begpad = -1 * begsam\n            begsam = 0\n        else:\n            begpad = 0\n\n        if endsam > self.n_smp:\n            endpad = endsam - self.n_smp\n            endsam = self.n_smp\n        else:\n            endpad = 0\n\n        first_sam = DATA_PRECISION * self.n_chan * begsam\n        toread_sam = DATA_PRECISION * self.n_chan * (endsam - begsam)\n\n        with open(join(self.filename, EEG_FILE), 'rb') as f:\n            f.seek(first_sam)\n            x = f.read(toread_sam)\n\n        dat = _read_dat(x)\n        dat = reshape(dat, (self.n_chan, -1), 'F')\n        dat = self.convertion(dat[chan, :])\n        dat = pad(dat, ((0, 0), (begpad, endpad)),\n                  mode='constant', constant_values=NaN)\n\n        return dat", "response": "Return the data as 2D numpy. ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _release(level):\n    version, comment = _new_version(level)\n\n    if version is not None:\n\n        run(['git',\n             'commit',\n             str(VER_PATH.relative_to(BASE_PATH)),\n             str(CHANGES_PATH.relative_to(BASE_PATH)),\n             '--amend',\n             '--no-edit',\n             ])\n        run(['git',\n             'tag',\n             '-a',\n             'v' + version,\n             '-m',\n             '\"' + comment + '\"',\n             ])\n        run(['git',\n             'push',\n             'origin',\n             '--tags',\n             ])\n        run(['git',\n             'push',\n             'origin',\n             'master',\n             '-f',\n             ])", "response": "This function is used to create a new release."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a URL to a file.", "response": "def _urlretrieve(url, filename):\n    \"\"\"urlretrive, but it ignores ssl errors (due to https://portal.g-node.org)\n    \"\"\"\n    ctx = create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = CERT_NONE\n\n    with urlopen(url, context=ctx) as u, filename.open('wb') as f:\n            f.write(u.read())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xml2dict(root):\n    output = {}\n    if root.items():\n        output.update(dict(root.items()))\n\n    for element in root:\n        if element:\n            if len(element) == 1 or element[0].tag != element[1].tag:\n                one_dict = xml2dict(element)\n            else:\n                one_dict = {ns(element[0].tag): xml2list(element)}\n\n            if element.items():\n                one_dict.update(dict(element.items()))\n            output.update({ns(element.tag): one_dict})\n\n        elif element.items():\n            output.update({ns(element.tag): dict(element.items())})\n\n        else:\n            output.update({ns(element.tag): element.text})\n\n    return output", "response": "Use functions instead of Class and remove namespace based on :\n    http://stackoverflow. com/questions / 2148119"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_chan_name(orig):\n    sensors = orig['sensorLayout'][1]\n\n    eeg_chan = []\n    for one_sensor in sensors:\n        if one_sensor['type'] in ('0', '1'):\n            if one_sensor['name'] is not None:\n                eeg_chan.append(one_sensor['name'])\n            else:\n                eeg_chan.append(one_sensor['number'])\n\n    pns_chan = []\n    if 'pnsSet' in orig:\n        pnsSet = orig['pnsSet'][1]\n        for one_sensor in pnsSet:\n            pns_chan.append(one_sensor['name'])\n\n    return eeg_chan + pns_chan, len(eeg_chan)", "response": "Read channel labels which can be across xml files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef return_hdr(self):\n        orig = {}\n        for xml_file in self.filename.glob('*.xml'):\n            if xml_file.stem[0] != '.':\n                orig[xml_file.stem] = parse_xml(str(xml_file))\n\n        signals = sorted(self.filename.glob('signal*.bin'))\n\n        for signal in signals:\n            block_hdr, i_data = read_all_block_hdr(signal)\n            self._signal.append(signal)\n            self._block_hdr.append(block_hdr)\n            self._i_data.append(i_data)\n            n_samples = asarray([x['n_samples'][0] for x in block_hdr], 'q')\n            self._n_samples.append(n_samples)\n\n        try:\n            subj_id = orig['subject'][0][0]['name']\n        except KeyError:\n            subj_id = ''\n        try:\n            start_time = datetime.strptime(orig['info'][0]['recordTime'][:26],\n                                           '%Y-%m-%dT%H:%M:%S.%f')\n        except KeyError:\n            start_time = DEFAULT_DATETIME\n        self.start_time = start_time\n\n        videos = (list(self.filename.glob('*.mp4')) +  # as described in specs\n                  list(self.filename.glob('*.mov')))  # actual example\n        videos = [x for x in videos if x.stem[0] != '.']  # remove hidden files\n\n        if len(videos) > 1:\n            lg.warning('More than one video present: ' + ', '.join(videos))\n        self._videos = videos\n\n        # it only works if they have all the same sampling frequency\n        s_freq = [x[0]['freq'][0] for x in self._block_hdr]\n        assert all([x == s_freq[0] for x in s_freq])\n        SIGNAL = 0\n        s_freq = self._block_hdr[SIGNAL][0]['freq'][0]\n        n_samples = sum(self._n_samples[SIGNAL])\n\n        chan_name, self._nchan_signal1 = _read_chan_name(orig)\n        self._orig = orig\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the data as 2D numpy. ndarray.", "response": "def return_dat(self, chan, begsam, endsam):\n        \"\"\"Return the data as 2D numpy.ndarray.\n\n        Parameters\n        ----------\n        chan : list\n            indices of the channels to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A 2d matrix, with dimension chan X samples\n\n        Notes\n        -----\n        This format is tricky for both channels and samples. For the samples,\n        we just use the boundaries in the block header. For the channels, we\n        assume that there are max two signals, one EEG and one PIB box. We\n        just use the boundary between them to define if a channel belongs to\n        the first group or to the second.\n\n        TODO\n        ----\n        use wonambi.ioeeg.utils._select_blocks here, but you need to test it\n        with a PIB box.\n        \"\"\"\n        assert begsam < endsam\n\n        data = empty((len(chan), endsam - begsam))\n        data.fill(NaN)\n\n        chan = asarray(chan)\n\n        # we assume there are only two signals\n        signals_to_read = []\n        if (chan < self._nchan_signal1).any():\n            signals_to_read.append(0)\n        if (chan >= self._nchan_signal1).any():\n            signals_to_read.append(1)\n\n        for one_signal in signals_to_read:\n            if one_signal == 0:\n                i_chan_data = chan < self._nchan_signal1\n                i_chan_rec = chan[i_chan_data]\n\n            if one_signal == 1:\n                i_chan_data = chan >= self._nchan_signal1\n                i_chan_rec = chan[i_chan_data] - self._nchan_signal1\n\n            x = self._n_samples[one_signal]\n            x1 = cumsum(append(0, x))\n\n            # begrec is -1 when begsam is before start of the recordings\n            begrec = where(begsam < x1)[0][0] - 1\n            try:\n                endrec = where(endsam < x1)[0][0] - 1\n            except IndexError:\n                endrec = len(x)\n\n            f = self._signal[one_signal].open('rb')\n\n            i0 = 0\n            for rec in range(begrec, endrec + 1):\n\n                # if begsam is before start of the recordings, we just shift the baseline\n                if rec == -1:\n                    i0 = - begsam\n                    continue\n\n                # if endsam is after end of the recordings, we just stop here\n                if rec == len(self._n_samples[one_signal]):\n                    break\n\n                if rec == begrec:\n                    begpos_rec = begsam - x1[rec]\n                else:\n                    begpos_rec = 0\n\n                if rec == endrec:\n                    endpos_rec = endsam - x1[rec]\n                else:\n                    endpos_rec = x[rec]\n\n                i1 = i0 + endpos_rec - begpos_rec\n\n                lg.debug('data {: 8d}-{: 8d}, rec ({}) {: 5d} - {: 5d}'.format(i0, i1, rec, begpos_rec, endpos_rec))\n\n                rec_dat = _read_block(f,\n                                      self._block_hdr[one_signal][rec],\n                                      self._i_data[one_signal][rec])\n\n                data[i_chan_data, i0:i1] = rec_dat[i_chan_rec,\n                                                   begpos_rec:endpos_rec]\n                i0 = i1\n\n            f.close()\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef return_videos(self, begtime, endtime):\n        try:\n            self._orig['po_videoSyncups']\n        except KeyError:\n            raise OSError('No po_videoSyncups.xml in folder to sync videos')\n\n        if not self._videos:\n            raise OSError('No mp4 video files')\n\n        mp4_file = self._videos[:1]  # make clear we only use the first video\n\n        return mp4_file, begtime, endtime", "response": "This function returns the videos and beginning and end time of the video in the specified time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites data to a Wonambi file.", "response": "def write_wonambi(data, filename, subj_id='', dtype='float64'):\n    \"\"\"Write file in simple Wonambi format.\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data with only one trial\n    filename : path to file\n        file to export to (the extensions .won and .dat will be added)\n    subj_id : str\n        subject id\n    dtype : str\n        numpy dtype in which you want to save the data\n\n    Notes\n    -----\n    Wonambi format creates two files, one .won with the dataset info as json\n    file and one .dat with the memmap recordings.\n\n    It will happily overwrite any existing file with the same name.\n\n    Memory-mapped matrices are column-major, Fortran-style, to be compatible\n    with Matlab.\n    \"\"\"\n    filename = Path(filename)\n\n    json_file = filename.with_suffix('.won')\n    memmap_file = filename.with_suffix('.dat')\n\n    start_time = data.start_time + timedelta(seconds=data.axis['time'][0][0])\n\n    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S.%f')\n    dataset = {'subj_id': subj_id,\n               'start_time': start_time_str,\n               's_freq': data.s_freq,\n               'chan_name': list(data.axis['chan'][0]),\n               'n_samples': int(data.number_of('time')[0]),\n               'dtype': dtype,\n               }\n\n    with json_file.open('w') as f:\n        dump(dataset, f, sort_keys=True, indent=4)\n\n    memshape = (len(dataset['chan_name']),\n                dataset['n_samples'])\n\n    mem = memmap(str(memmap_file), dtype, mode='w+', shape=memshape, order='F')\n    mem[:, :] = data.data[0]\n    mem.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef return_hdr(self):\n        with open(self.filename, 'r') as f:\n            orig = load(f)\n\n        start_time = datetime.strptime(orig['start_time'],\n                                       '%Y-%m-%d %H:%M:%S.%f')\n        self.memshape = (len(orig['chan_name']),\n                         orig['n_samples'])\n        self.dtype = orig.get('dtype', 'float64')\n\n        return (orig['subj_id'], start_time, orig['s_freq'], orig['chan_name'],\n                orig['n_samples'], orig)", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the data as 2D numpy. ndarray.", "response": "def return_dat(self, chan, begsam, endsam):\n        \"\"\"Return the data as 2D numpy.ndarray.\n\n        Parameters\n        ----------\n        chan : int or list\n            index (indices) of the channels to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A 2d matrix, with dimension chan X samples. To save memory, the\n            data are memory-mapped, and you cannot change the values on disk.\n\n        Raises\n        ------\n        FileNotFoundError\n            if .dat file is not in the same directory, with the same name.\n\n        Notes\n        -----\n        When asking for an interval outside the data boundaries, it returns NaN\n        for those values. It then converts the memmap to a normal numpy array,\n        I think, and so it reads the data into memory. However, I'm not 100%\n        sure that this is what happens.\n        \"\"\"\n        memmap_file = Path(self.filename).with_suffix('.dat')\n        if not memmap_file.exists():\n            raise FileNotFoundError('Could not find ' + str(memmap_file))\n\n        data = memmap(str(memmap_file), self.dtype, mode='c',\n                      shape=self.memshape, order='F')\n\n        n_smp = self.memshape[1]\n        dat = data[chan, max((begsam, 0)):min((endsam, n_smp))].astype(float64)\n\n        if begsam < 0:\n\n            pad = empty((dat.shape[0], 0 - begsam))\n            pad.fill(NaN)\n            dat = c_[pad, dat]\n\n        if endsam >= n_smp:\n\n            pad = empty((dat.shape[0], endsam - n_smp))\n            pad.fill(NaN)\n            dat = c_[dat, pad]\n\n        return dat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_geometry(surf_file):\n    with open(surf_file, 'rb') as f:\n        filebytes = f.read()\n\n    assert filebytes[:3] == b'\\xff\\xff\\xfe'\n    i0 = filebytes.index(b'\\x0A\\x0A') + 2\n    i1 = i0 + 4\n    vnum = unpack('>i', filebytes[i0:i1])[0]\n    i0 = i1\n    i1 += 4\n    fnum = unpack('>i', filebytes[i0:i1])[0]\n    i0 = i1\n    i1 += 4 * vnum * 3\n    verts = unpack('>' + 'f' * vnum * 3, filebytes[i0:i1])\n    i0 = i1\n    i1 += 4 * fnum * 3\n    faces = unpack('>' + 'i' * fnum * 3, filebytes[i0:i1])\n\n    verts = asarray(verts).reshape(vnum, 3)\n    faces = asarray(faces).reshape(fnum, 3)\n    return verts, faces", "response": "Read a triangular format Freesurfer surface mesh."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_freesurfer_LUT(fs_lut=None):\n    if fs_lut is not None:\n        lg.info('Reading user-specified lookuptable {}'.format(fs_lut))\n        fs_lut = Path(fs_lut)\n    else:\n        try:\n            fs_home = environ['FREESURFER_HOME']\n        except KeyError:\n            raise OSError('Freesurfer is not installed or FREESURFER_HOME is '\n                          'not defined as environmental variable')\n        else:\n            fs_lut = Path(fs_home) / 'FreeSurferColorLUT.txt'\n            lg.info('Reading lookuptable in FREESURFER_HOME {}'.format(fs_lut))\n\n    idx = []\n    label = []\n    rgba = empty((0, 4))\n    with fs_lut.open('r') as f:\n        for l in f:\n            if len(l) <= 1 or l[0] == '#' or l[0] == '\\r':\n                continue\n            (t0, t1, t2, t3, t4, t5) = [t(s) for t, s in\n                                        zip((int, str, int, int, int, int),\n                                        l.split())]\n            idx.append(t0)\n            label.append(t1)\n            rgba = vstack((rgba, array([t2, t3, t4, t5])))\n\n    return idx, label, rgba", "response": "Imports the Look - up Table with colors and labels for anatomical regions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_brain_region(self, abs_pos, parc_type='aparc', max_approx=None,\n                          exclude_regions=None):\n        \"\"\"Find the name of the brain region in which an electrode is located.\n\n        Parameters\n        ----------\n        abs_pos : numpy.ndarray\n            3x0 vector with the position of interest.\n        parc_type : str\n            'aparc', 'aparc.a2009s', 'BA', 'BA.thresh', or 'aparc.DKTatlas40'\n            'aparc.DKTatlas40' is only for recent freesurfer versions\n        max_approx : int, optional\n            max approximation to define position of the electrode.\n        exclude_regions : list of str or empty list\n            do not report regions if they contain these substrings. None means\n            that it does not exclude any region.\n\n        Notes\n        -----\n        It determines the possible brain region in which one electrode is\n        present, based on Freesurfer segmentation. You can imagine that\n        sometimes one electrode is not perfectly located within one region,\n        but it's a few mm away. The parameter \"approx\" specifies this tolerance\n        where each value is one mm. It keeps on searching in larger and larger\n        spots until it finds at least one region which is not white matter. If\n        there are multiple regions, it returns the region with the most\n        detection.\n        Minimal value is 0, which means only if the electrode is in the\n        precise location.\n\n        If you want to exclude white matter regions with 'aparc', use\n            exclude_regions = ('White', 'WM', 'Unknown')\n        and with 'aparc.a2009s', use:\n            exclude_regions = ('White-Matter')\n        \"\"\"\n        # convert to freesurfer coordinates of the MRI\n        pos = around(dot(FS_AFFINE, append(abs_pos, 1)))[:3].astype(int)\n        lg.debug('Position in the MRI matrix: {}'.format(pos))\n\n        mri_dat, _ = self.read_seg(parc_type)\n\n        if max_approx is None:\n            max_approx = 3\n\n        for approx in range(max_approx + 1):\n            lg.debug('Trying approx {} out of {}'.format(approx, max_approx))\n            regions = _find_neighboring_regions(pos, mri_dat,\n                                                self.lookuptable, approx,\n                                                exclude_regions)\n            if regions:\n                break\n\n        if regions:\n            c_regions = Counter(regions)\n            return c_regions.most_common(1)[0][0], approx\n        else:\n            return '--not found--', approx", "response": "This function returns the name of the brain region in which an electrode is located."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_label(self, hemi, parc_type='aparc'):\n        parc_file = self.dir / 'label' / (hemi + '.' + parc_type + '.annot')\n        vert_val, region_color, region_name = read_annot(parc_file)\n        region_name = [x.decode('utf-8') for x in region_name]\n        return vert_val, region_color, region_name", "response": "Read the labels for each hemisphere and return the corresponding value color and region name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the MRI segmentation.", "response": "def read_seg(self, parc_type='aparc'):\n        \"\"\"Read the MRI segmentation.\n\n        Parameters\n        ----------\n        parc_type : str\n            'aparc' or 'aparc.a2009s'\n\n        Returns\n        -------\n        numpy.ndarray\n            3d matrix with values\n        numpy.ndarray\n            4x4 affine matrix\n        \"\"\"\n        seg_file = self.dir / 'mri' / (parc_type + '+aseg.mgz')\n        seg_mri = load(seg_file)\n        seg_aff = seg_mri.affine\n        seg_dat = seg_mri.get_data()\n        return seg_dat, seg_aff"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef concatenate(data, axis):\n    output = data._copy(axis=False)\n\n    for dataaxis in data.axis:\n        output.axis[dataaxis] = empty(1, dtype='O')\n\n        if dataaxis == axis:\n            output.axis[dataaxis][0] = cat(data.axis[dataaxis])\n        else:\n            output.axis[dataaxis][0] = data.axis[dataaxis][0]\n\n        if len(unique(output.axis[dataaxis][0])) != len(output.axis[dataaxis][0]):\n            lg.warning('Axis ' + dataaxis + ' does not have unique values')\n\n    output.data = empty(1, dtype='O')\n    if axis == 'trial':\n\n        # create new axis\n        new_axis = empty(1, dtype='O')\n        n_trial = data.number_of('trial')\n        trial_name = ['trial{0:06}'.format(x) for x in range(n_trial)]\n        new_axis[0] = asarray(trial_name, dtype='U')\n        output.axis['trial_axis'] = new_axis\n\n        # concatenate along the extra dimension\n        all_trial = []\n        for one_trial in data.data:\n            all_trial.append(expand_dims(one_trial, -1))\n        output.data[0] = cat(all_trial, axis=-1)\n\n    else:\n        output.data[0] = cat(data.data, axis=output.index_of(axis))\n\n    return output", "response": "Concatenate multiple trials into one trials according to any dimension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the header for further use.", "response": "def return_hdr(self):\n        \"\"\"Return the header for further use.\n\n        Returns\n        -------\n        subj_id : str\n            subject identification code\n        start_time : datetime\n            start time of the dataset\n        s_freq : float\n            sampling frequency\n        chan_name : list of str\n            list of all the channels\n        n_samples : int\n            number of samples in the dataset\n        orig : dict\n            the full header\n        \"\"\"\n        hdr = {}            \n        hdr['s_freq'] = self.s_freq\n        hdr['chan_name'] = ['RRi']\n        \n        with open(self.filename, 'rt') as f:\n            head = [next(f) for x in range(12)]\n            hdr['subj_id'] = head[0][11:-3]\n            hdr['start_time'] = DEFAULT_DATETIME\n            hdr['recorder'] = head[2][10:]\n            hdr['s_freq_ecg'] = int(head[3][4:]) # ECG sampling frequency\n            t = datetime.strptime(head[4][16:24], '%H:%M:%S')\n            hdr['total_dur'] = timedelta(hours=t.hour, minutes=t.minute, \n               seconds=t.second)\n            hdr['export_date'] = DEFAULT_DATETIME\n            hdr['data_type'] = head[10][11:]\n            \n            for i, _ in enumerate(f):\n                pass\n            hdr['n_samples'] = i\n        \n        output = (hdr['subj_id'], hdr['start_time'], hdr['s_freq'], \n                  hdr['chan_name'], hdr['n_samples'], hdr)\n        \n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns raw irregularly - timed RRI.", "response": "def return_rri(self, begsam, endsam):\n        \"\"\"Return raw, irregularly-timed RRI.\"\"\"\n        interval = endsam - begsam\n        dat = empty(interval)\n        k = 0\n            \n        with open(self.filename, 'rt') as f:\n            [next(f) for x in range(12)]\n            \n            for j, datum in enumerate(f):\n                \n                if begsam <= j < endsam:\n                    dat[k] = float64(datum[:datum.index('\\t')])\n                    k += 1\n                    if k == interval:\n                        break\n                    \n        return dat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the table with the new ones.", "response": "def update(self, checked=False, labels=None, custom_labels=None):\n        \"\"\"Use this function when we make changes to the list of labels or when\n        we load a new dataset.\n\n        Parameters\n        ----------\n        checked : bool\n            argument from clicked.connect\n        labels : list of str\n            list of labels in the dataset (default)\n        custom_labels : list of str\n            list of labels from a file\n        \"\"\"\n        if labels is not None:\n            self.setEnabled(True)\n            self.chan_name = labels\n\n        self.table.blockSignals(True)\n        self.table.clearContents()\n        self.table.setRowCount(len(self.chan_name))\n\n        for i, label in enumerate(self.chan_name):\n            old_label = QTableWidgetItem(label)\n            old_label.setFlags(Qt.ItemIsSelectable | Qt.ItemIsEnabled)\n\n            if custom_labels is not None and i < len(custom_labels) and custom_labels[i]:  # it's not empty string or None\n                label_txt = custom_labels[i]\n            else:\n                label_txt = label\n            new_label = QTableWidgetItem(label_txt)\n\n            self.table.setItem(i, 0, old_label)\n            self.table.setItem(i, 1, new_label)\n\n        self.table.blockSignals(False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the values of the peaks in the input data.", "response": "def peaks(data, method='max', axis='time', limits=None):\n    \"\"\"Return the values of an index where the data is at max or min\n\n    Parameters\n    ----------\n    method : str, optional\n        'max' or 'min'\n    axis : str, optional\n        the axis where you want to detect the peaks\n    limits : tuple of two values, optional\n        the lowest and highest limits where to search for the peaks\n    data : instance of Data\n        one of the datatypes\n\n    Returns\n    -------\n    instance of Data\n        with one dimension less that the input data. The actual values in\n        the data can be not-numberic, for example, if you look for the\n        max value across electrodes\n\n    Notes\n    -----\n    This function is useful when you want to find the frequency value at which\n    the power is the largest, or to find the time point at which the signal is\n    largest, or the channel at which the activity is largest.\n    \"\"\"\n    idx_axis = data.index_of(axis)\n    output = data._copy()\n    output.axis.pop(axis)\n\n    for trl in range(data.number_of('trial')):\n        values = data.axis[axis][trl]\n        dat = data(trial=trl)\n\n        if limits is not None:\n            limits = (values < limits[0]) | (values > limits[1])\n\n            idx = [slice(None)] * len(data.list_of_axes)\n            idx[idx_axis] = limits\n            dat[idx] = nan\n\n        if method == 'max':\n            peak_val = nanargmax(dat, axis=idx_axis)\n        elif method == 'min':\n            peak_val = nanargmin(dat, axis=idx_axis)\n\n        output.data[trl] = values[peak_val]\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the average and maximum slopes for each wave in a slow taxonomy.", "response": "def get_slopes(data, s_freq, level='all', smooth=0.05):\n    \"\"\"Get the slopes (average and/or maximum) for each quadrant of a slow\n    wave, as well as the combination of quadrants 2 and 3.\n\n    Parameters\n    ----------\n    data : ndarray\n        raw data as vector\n    s_freq : int\n        sampling frequency\n    level : str\n        if 'average', returns average slopes (uV / s). if 'maximum', returns\n        the maximum of the slope derivative (uV / s**2). if 'all', returns all.\n    smooth : float or None\n        if not None, signal will be smoothed by moving average, with a window\n        of this duration\n\n    Returns\n    -------\n    tuple of ndarray\n        each array is len 5, with q1, q2, q3, q4 and q23. First array is\n        average slopes and second is maximum slopes.\n\n    Notes\n    -----\n    This function is made to take automatically detected start and end\n    times AS WELL AS manually delimited ones. In the latter case, the first\n    and last zero has to be detected within this function.\n    \"\"\"\n    data = negative(data) # legacy code\n    \n    nan_array = empty((5,))\n    nan_array[:] = nan\n    idx_trough = data.argmin()\n    idx_peak = data.argmax()\n    if idx_trough >= idx_peak:\n        return nan_array, nan_array\n\n    zero_crossings_0 = where(diff(sign(data[:idx_trough])))[0]\n    zero_crossings_1 = where(diff(sign(data[idx_trough:idx_peak])))[0]\n    zero_crossings_2 = where(diff(sign(data[idx_peak:])))[0]\n    if zero_crossings_1.any():\n        idx_zero_1 = idx_trough + zero_crossings_1[0]\n    else:\n        return nan_array, nan_array\n\n    if zero_crossings_0.any():\n        idx_zero_0 = zero_crossings_0[-1]\n    else:\n        idx_zero_0 = 0\n\n    if zero_crossings_2.any():\n        idx_zero_2 = idx_peak + zero_crossings_2[0]\n    else:\n        idx_zero_2 = len(data) - 1\n\n    avgsl = nan_array\n    if level in ['average', 'all']:\n        q1 = data[idx_trough] / ((idx_trough - idx_zero_0) / s_freq)\n        q2 = data[idx_trough] / ((idx_zero_1 - idx_trough) / s_freq)\n        q3 = data[idx_peak] / ((idx_peak - idx_zero_1) / s_freq)\n        q4 = data[idx_peak] / ((idx_zero_2 - idx_peak) / s_freq)\n        q23 = (data[idx_peak] - data[idx_trough]) \\\n                / ((idx_peak - idx_trough) / s_freq)\n        avgsl = asarray([q1, q2, q3, q4, q23])\n        avgsl[isinf(avgsl)] = nan\n\n    maxsl = nan_array\n    if level in ['maximum', 'all']:\n\n        if smooth is not None:\n            win = int(smooth * s_freq)\n            flat = ones(win)\n            data = fftconvolve(data, flat / sum(flat), mode='same')\n\n        if idx_trough - idx_zero_0 >= win:\n            maxsl[0] = min(diff(data[idx_zero_0:idx_trough]))\n\n        if idx_zero_1 - idx_trough >= win:\n            maxsl[1] = max(diff(data[idx_trough:idx_zero_1]))\n\n        if idx_peak - idx_zero_1 >= win:\n            maxsl[2] = max(diff(data[idx_zero_1:idx_peak]))\n\n        if idx_zero_2 - idx_peak >= win:\n            maxsl[3] = min(diff(data[idx_peak:idx_zero_2]))\n\n        if idx_peak - idx_trough >= win:\n            maxsl[4] = max(diff(data[idx_trough:idx_peak]))\n\n        maxsl[isinf(maxsl)] = nan\n\n    return avgsl, maxsl"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute event parameters. Parameters ---------- segments : instance of wonambi.trans.select.Segments list of segments, with time series and metadata params : dict of bool, or str 'dur', 'minamp', 'maxamp', 'ptp', 'rms', 'power', 'peakf', 'energy', 'peakef'. If 'all', a dict will be created with these keys and all values as True, so that all parameters are returned. band : tuple of float band of interest for power and energy n_fft : int length of FFT. if shorter than input signal, signal is truncated; if longer, signal is zero-padded to length slopes : dict of bool 'avg_slope', 'max_slope', 'prep', 'invert' prep : dict of bool same keys as params. if True, segment['trans_data'] will be used as dat parent : QMainWindow for use with GUI only Returns ------- list of dict list of segments, with time series, metadata and parameters", "response": "def event_params(segments, params, band=None, n_fft=None, slopes=None, \n                 prep=None, parent=None):\n    \"\"\"Compute event parameters.\n    \n    Parameters\n    ----------\n    segments : instance of wonambi.trans.select.Segments\n        list of segments, with time series and metadata\n    params : dict of bool, or str\n        'dur', 'minamp', 'maxamp', 'ptp', 'rms', 'power', 'peakf', 'energy', \n        'peakef'. If 'all', a dict will be created with these keys and all \n        values as True, so that all parameters are returned.\n    band : tuple of float\n        band of interest for power and energy\n    n_fft : int\n        length of FFT. if shorter than input signal, signal is truncated; if \n        longer, signal is zero-padded to length\n    slopes : dict of bool\n        'avg_slope', 'max_slope', 'prep', 'invert'\n    prep : dict of bool\n        same keys as params. if True, segment['trans_data'] will be used as dat\n    parent : QMainWindow\n        for use with GUI only\n        \n    Returns\n    -------\n    list of dict\n        list of segments, with time series, metadata and parameters\n    \"\"\"\n    if parent is not None:\n        progress = QProgressDialog('Computing parameters', 'Abort',\n                                   0, len(segments) - 1, parent)\n        progress.setWindowModality(Qt.ApplicationModal)\n\n    param_keys = ['dur', 'minamp', 'maxamp', 'ptp', 'rms', 'power', 'peakpf', \n                  'energy', 'peakef']\n    \n    if params == 'all':\n        params = {k: 1 for k in param_keys}\n    if prep is None:\n        prep = {k: 0 for k in param_keys}    \n    if band is None:\n        band = (None, None)\n\n    params_out = []\n    evt_output = False\n\n    for i, seg in enumerate(segments):\n        out = dict(seg)\n        dat = seg['data']            \n\n        if params['dur']:\n            out['dur'] = float(dat.number_of('time')) / dat.s_freq\n            evt_output = True\n\n        if params['minamp']:\n            dat1 = dat\n            if prep['minamp']:\n                dat1 = seg['trans_data']\n            out['minamp'] = math(dat1, operator=_amin, axis='time')\n            evt_output = True\n\n        if params['maxamp']:\n            dat1 = dat\n            if prep['maxamp']:\n                dat1 = seg['trans_data']\n            out['maxamp'] = math(dat1, operator=_amax, axis='time')\n            evt_output = True\n\n        if params['ptp']:\n            dat1 = dat\n            if prep['ptp']:\n                dat1 = seg['trans_data']\n            out['ptp'] = math(dat1, operator=_ptp, axis='time')\n            evt_output = True\n\n        if params['rms']:\n            dat1 = dat\n            if prep['rms']:\n                dat1 = seg['trans_data']\n            out['rms'] = math(dat1, operator=(square, _mean, sqrt),\n               axis='time')\n            evt_output = True\n\n        for pw, pk in [('power', 'peakpf'), ('energy', 'peakef')]:\n\n            if params[pw] or params[pk]:\n                evt_output = True\n\n                if prep[pw] or prep[pk]:\n                    prep_pw, prep_pk = band_power(seg['trans_data'], band,\n                                                 scaling=pw, n_fft=n_fft)\n                if not (prep[pw] and prep[pk]):\n                    raw_pw, raw_pk = band_power(dat, band, \n                                                scaling=pw, n_fft=n_fft)\n\n                if prep[pw]:\n                    out[pw] = prep_pw\n                else:\n                    out[pw] = raw_pw\n\n                if prep[pk]:\n                    out[pk] = prep_pk\n                else:\n                    out[pk] = raw_pk\n\n        if slopes:\n            evt_output = True\n            out['slope'] = {}\n            dat1 = dat\n            if slopes['prep']:\n                dat1 = seg['trans_data']\n            if slopes['invert']:\n                dat1 = math(dat1, operator=negative, axis='time')\n\n            if slopes['avg_slope'] and slopes['max_slope']:\n                level = 'all'\n            elif slopes['avg_slope']:\n                level = 'average'\n            else:\n                level = 'maximum'\n\n            for chan in dat1.axis['chan'][0]:\n                d = dat1(chan=chan)[0]\n                out['slope'][chan] = get_slopes(d, dat.s_freq, level=level)\n                \n        if evt_output:\n            timeline = dat.axis['time'][0]\n            out['start'] = timeline[0]\n            out['end'] = timeline[-1]\n            params_out.append(out)\n\n        if parent:\n            progress.setValue(i)\n            if progress.wasCanceled():\n                msg = 'Analysis canceled by user.'\n                parent.statusBar().showMessage(msg)\n                return\n\n    if parent:\n        progress.close()\n\n    return params_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_event_params(filename, params, count=None, density=None):\n    heading_row_1 = ['Segment index',\n                   'Start time',\n                   'End time',\n                   'Stitches',\n                   'Stage',\n                   'Cycle',\n                   'Event type',\n                   'Channel']\n    spacer = [''] * (len(heading_row_1) - 1)\n\n    param_headings_1 = ['Min. amplitude (uV)',\n                        'Max. amplitude (uV)',\n                        'Peak-to-peak amplitude (uV)',\n                        'RMS (uV)']\n    param_headings_2 = ['Power (uV^2)',\n                        'Peak power frequency (Hz)',\n                        'Energy (uV^2s)',\n                        'Peak energy frequency (Hz)']\n    slope_headings =   ['Q1 average slope (uV/s)',\n                        'Q2 average slope (uV/s)',\n                        'Q3 average slope (uV/s)',\n                        'Q4 average slope (uV/s)',\n                        'Q23 average slope (uV/s)',\n                        'Q1 max. slope (uV/s^2)',\n                        'Q2 max. slope (uV/s^2)',\n                        'Q3 max. slope (uV/s^2)',\n                        'Q4 max. slope (uV/s^2)',\n                        'Q23 max. slope (uV/s^2)']\n    ordered_params_1 = ['minamp', 'maxamp', 'ptp', 'rms']\n    ordered_params_2 = ['power', 'peakpf', 'energy', 'peakef']\n\n    idx_params_1 = in1d(ordered_params_1, list(params[0].keys()))\n    sel_params_1 = list(compress(ordered_params_1, idx_params_1))\n    heading_row_2 = list(compress(param_headings_1, idx_params_1))\n\n    if 'dur' in params[0].keys():\n        heading_row_2 = ['Duration (s)'] + heading_row_2\n\n    idx_params_2 = in1d(ordered_params_2, list(params[0].keys()))\n    sel_params_2 = list(compress(ordered_params_2, idx_params_2))\n    heading_row_3 = list(compress(param_headings_2, idx_params_2))\n\n    heading_row_4 = []\n    if 'slope' in params[0].keys():\n        if next(iter(params[0]['slope']))[0]:\n            heading_row_4.extend(slope_headings[:5])\n        if next(iter(params[0]['slope']))[1]:\n            heading_row_4.extend(slope_headings[5:])\n\n    # Get data as matrix and compute descriptives\n    dat = []\n    if 'dur' in params[0].keys():\n        one_mat = asarray([seg['dur'] for seg in params \\\n                           for chan in seg['data'].axis['chan'][0]])\n        one_mat = reshape(one_mat, (len(one_mat), 1))\n        dat.append(one_mat)\n\n    if sel_params_1:\n        one_mat = asarray([[seg[x](chan=chan)[0] for x in sel_params_1] \\\n                for seg in params for chan in seg['data'].axis['chan'][0]])\n        dat.append(one_mat)\n\n    if sel_params_2:\n        one_mat = asarray([[seg[x][chan] for x in sel_params_2] \\\n                for seg in params for chan in seg['data'].axis['chan'][0]])\n        dat.append(one_mat)\n\n    if 'slope' in params[0].keys():\n        one_mat = asarray([[x for y in seg['slope'][chan] for x in y] \\\n                for seg in params for chan in seg['data'].axis['chan'][0]])\n        dat.append(one_mat)\n\n    if dat:\n        dat = concatenate(dat, axis=1)\n        desc = get_descriptives(dat)\n\n    with open(filename, 'w', newline='') as f:\n        lg.info('Writing to ' + str(filename))\n        csv_file = writer(f)\n        csv_file.writerow(['Wonambi v{}'.format(__version__)])\n\n        if count:\n            csv_file.writerow(['Count', count])\n        if density:\n            csv_file.writerow(['Density', density])\n\n        if dat == []:\n            return\n\n        csv_file.writerow(heading_row_1 + heading_row_2 + heading_row_3 \\\n                          + heading_row_4)\n        csv_file.writerow(['Mean'] + spacer + list(desc['mean']))\n        csv_file.writerow(['SD'] + spacer + list(desc['sd']))\n        csv_file.writerow(['Mean of ln'] + spacer + list(desc['mean_log']))\n        csv_file.writerow(['SD of ln'] + spacer + list(desc['sd_log']))\n        idx = 0\n\n        for seg in params:\n            if seg['cycle'] is not None:\n                seg['cycle'] = seg['cycle'][2]\n\n            for chan in seg['data'].axis['chan'][0]:\n                idx += 1\n                data_row_1 = [seg[x](chan=chan)[0] for x in sel_params_1]\n                data_row_2 = [seg[x][chan] for x in sel_params_2]\n\n                if 'dur' in seg.keys():\n                    data_row_1 = [seg['dur']] + data_row_1\n\n                if 'slope' in seg.keys():\n                    data_row_3 = [x for y in seg['slope'][chan] for x in y]\n                    data_row_2 = data_row_2 + data_row_3\n\n                csv_file.writerow([idx,\n                                   seg['start'],\n                                   seg['end'],\n                                   seg['n_stitch'],\n                                   seg['stage'],\n                                   seg['cycle'],\n                                   seg['name'],\n                                   chan,\n                                   ] + data_row_1 + data_row_2)", "response": "Write event analysis data to CSV."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_freq(xfreq, filename, desc=None):\n    heading_row_1 = ['Segment index',\n                   'Start time',\n                   'End time',\n                   'Duration',\n                   'Stitches',\n                   'Stage',\n                   'Cycle',\n                   'Event type',\n                   'Channel',\n                   ]\n    spacer = [''] * (len(heading_row_1) - 1)\n    freq = list(xfreq[0]['data'].axis['freq'][0])\n\n    with open(filename, 'w', newline='') as f:\n        lg.info('Writing to ' + str(filename))\n        csv_file = writer(f)\n        csv_file.writerow(['Wonambi v{}'.format(__version__)])\n        csv_file.writerow(heading_row_1 + freq)\n\n        if desc:\n            csv_file.writerow(['Mean'] + spacer + list(desc['mean']))\n            csv_file.writerow(['SD'] + spacer + list(desc['sd']))\n            csv_file.writerow(['Mean of ln'] + spacer + list(\n                    desc['mean_log']))\n            csv_file.writerow(['SD of ln'] + spacer + list(desc['sd_log']))\n\n        idx = 0\n        for seg in xfreq:\n\n            for chan in seg['data'].axis['chan'][0]:\n                idx += 1\n\n                cyc = None\n                if seg['cycle'] is not None:\n                    cyc = seg['cycle'][2]\n\n                data_row = list(seg['data'](chan=chan)[0])\n                csv_file.writerow([idx,\n                                   seg['start'],\n                                   seg['end'],\n                                   seg['duration'],\n                                   seg['n_stitch'],\n                                   seg['stage'],\n                                   cyc,\n                                   seg['name'],\n                                   chan,\n                                   ] + data_row)", "response": "Write frequency analysis data to CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_freq_band(xfreq, bands, filename):\n    heading_row_1 = ['Segment index',\n                   'Start time',\n                   'End time',\n                   'Duration',\n                   'Stitches',\n                   'Stage',\n                   'Cycle',\n                   'Event type',\n                   'Channel',\n                   ]\n    spacer = [''] * (len(heading_row_1) - 1)\n    band_hdr = [str(b1) + '-' + str(b2) for b1, b2 in bands]\n    xband = xfreq.copy()\n\n    for seg in xband:\n        bandlist = []\n\n        for i, b in enumerate(bands):\n            pwr, _ = band_power(seg['data'], b)\n            bandlist.append(pwr)\n\n        seg['band'] = bandlist\n\n    as_matrix = asarray([\n            [x['band'][y][chan] for y in range(len(x['band']))] \\\n            for x in xband for chan in x['band'][0].keys()])\n    desc = get_descriptives(as_matrix)\n\n    with open(filename, 'w', newline='') as f:\n        lg.info('Writing to ' + str(filename))\n        csv_file = writer(f)\n        csv_file.writerow(['Wonambi v{}'.format(__version__)])\n        csv_file.writerow(heading_row_1 + band_hdr)\n        csv_file.writerow(['Mean'] + spacer + list(desc['mean']))\n        csv_file.writerow(['SD'] + spacer + list(desc['sd']))\n        csv_file.writerow(['Mean of ln'] + spacer + list(desc['mean_log']))\n        csv_file.writerow(['SD of ln'] + spacer + list(desc['sd_log']))\n        idx = 0\n\n        for seg in xband:\n\n            for chan in seg['band'][0].keys():\n                idx += 1\n\n                cyc = None\n                if seg['cycle'] is not None:\n                    cyc = seg['cycle'][2]\n\n                data_row = list(\n                        [seg['band'][x][chan] for x in range(\n                                len(seg['band']))])\n                csv_file.writerow([idx,\n                                   seg['start'],\n                                   seg['end'],\n                                   seg['duration'],\n                                   seg['n_stitch'],\n                                   seg['stage'],\n                                   cyc,\n                                   seg['name'],\n                                   chan,\n                                   ] + data_row)", "response": "Write frequency analysis data to CSV by pre - defined band."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport events from Wonambi event CSV file.", "response": "def events_from_csv(source_file, stage=[]):\n    \"\"\"Import events from Wonambi event CSV.\n    \n    Parameters\n    ----------\n    source_file : str\n        path to file CSV file\n    stage : list of str (optional)\n        target stage, eg ['NREM2', 'NREM3'] or ['REM']\n        \n    Returns\n    -------\n    instance of wonambi.Graphoelement\n        class with events list\n    \"\"\"\n    events = []    \n    stage_cond = True\n    \n    with open(source_file, 'r', encoding='utf-8') as csvfile:\n        csv_reader = reader(csvfile, delimiter=',')\n        \n        for row in csv_reader:\n            try:\n                int(row[0])\n                if stage:\n                    stage_cond = row[4] in stage\n                if stage_cond:\n                    one_ev = {'name': row[6],\n                              'start': float(row[1]),\n                              'end': float(row[2]),\n                              'chan': row[7].split(', '),  # always a list\n                              'stage': row[4],\n                              'quality': 'Good'\n                              }\n                    events.append(one_ev)\n                \n            except ValueError:\n                continue\n    \n    grapho = Graphoelement()\n    grapho.events = events\n    grapho.chan_name = list(set([chan for e in events for chan in e['chan']]))\n    \n    return grapho"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_annot(self, annot, name=None, chan=None):\n        annot.add_events(self.events, name=name, chan=chan)", "response": "Write events to the Annotations file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_empty_annotations(xml_file, dataset):\n    xml_file = Path(xml_file)\n    root = Element('annotations')\n    root.set('version', VERSION)\n\n    info = SubElement(root, 'dataset')\n    x = SubElement(info, 'filename')\n    x.text = str(dataset.filename)\n    x = SubElement(info, 'path')  # not to be relied on\n    x.text = str(dataset.filename)\n    x = SubElement(info, 'start_time')\n    start_time = dataset.header['start_time'].replace(tzinfo=None)\n    x.text = start_time.isoformat()\n\n    first_sec = 0\n    last_sec = int(dataset.header['n_samples'] /\n                   dataset.header['s_freq'])  # in s\n\n    x = SubElement(info, 'first_second')\n    x.text = str(first_sec)\n    x = SubElement(info, 'last_second')\n    x.text = str(last_sec)\n\n    xml = parseString(tostring(root))\n    with xml_file.open('w') as f:\n        f.write(xml.toxml())", "response": "Create an empty annotation file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_annotation(xml_file, from_fasst):\n    xml_file = Path(xml_file)\n    try:\n        mat = loadmat(str(from_fasst), variable_names='D', struct_as_record=False,\n                      squeeze_me=True)\n    except ValueError:\n        raise UnrecognizedFormat(str(from_fasst) + ' does not look like a FASST .mat file')\n\n    D = mat['D']\n    info = D.other.info\n    score = D.other.CRC.score\n\n    microsecond, second = modf(info.hour[2])\n    start_time = datetime(*info.date, int(info.hour[0]), int(info.hour[1]),\n                          int(second), int(microsecond * 1e6))\n    first_sec = score[3, 0][0]\n    last_sec = score[0, 0].shape[0] * score[2, 0]\n\n    root = Element('annotations')\n    root.set('version', VERSION)\n\n    info = SubElement(root, 'dataset')\n    x = SubElement(info, 'filename')\n    x.text = D.other.info.fname\n    x = SubElement(info, 'path')  # not to be relied on\n    x.text = D.other.info.fname\n    x = SubElement(info, 'start_time')\n    x.text = start_time.isoformat()\n\n    x = SubElement(info, 'first_second')\n    x.text = str(int(first_sec))\n    x = SubElement(info, 'last_second')\n    x.text = str(int(last_sec))\n\n    xml = parseString(tostring(root))\n    with xml_file.open('w') as f:\n        f.write(xml.toxml())\n\n    annot = Annotations(xml_file)\n\n    n_raters = score.shape[1]\n    for i_rater in range(n_raters):\n        rater_name = score[1, i_rater]\n        epoch_length = int(score[2, i_rater])\n        annot.add_rater(rater_name, epoch_length=epoch_length)\n\n        for epoch_start, epoch in enumerate(score[0, i_rater]):\n            if isnan(epoch):\n                continue\n            annot.set_stage_for_epoch(epoch_start * epoch_length,\n                                      FASST_STAGE_KEY[int(epoch)], save=False)\n\n    annot.save()\n\n    return annot", "response": "Create an annotation file from an FASST sleep scoring file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_annotation_version(xml_file):\n    with open(xml_file, 'r') as f:\n        s = f.read()\n\n    m = search('<annotations version=\"([0-9]*)\">', s)\n    current = int(m.groups()[0])\n\n    if current < 4:\n        s = sub('<marker><name>(.*?)</name><time>(.*?)</time></marker>',\n                 '<marker><marker_name>\\g<1></marker_name><marker_start>\\g<2></marker_start><marker_end>\\g<2></marker_end><marker_chan/></marker>',\n                 s)\n\n    if current < 5:\n        s = s.replace('marker', 'bookmark')\n\n        # note indentation\n        s = sub('<annotations version=\"[0-9]*\">',\n                '<annotations version=\"5\">', s)\n        with open(xml_file, 'w') as f:\n            f.write(s)", "response": "Update the fields that have changed over different versions of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads RemLogic time string to datetime", "response": "def _remlogic_time(time_cell, date):\n    \"\"\"Reads RemLogic time string to datetime\n    \n    Parameters\n    ----------\n    time_cell : str\n        entire time cell from text file\n    date : datetime\n        start date from text file\n        \n    Returns\n    -------\n    datetime\n        date and time\n    \"\"\"\n    stage_start_time = datetime.strptime(time_cell[-8:], '%I:%M:%S')\n    start = datetime.combine(date.date(), stage_start_time.time())\n    \n    if time_cell[1] == 'U':\n        start = start + timedelta(hours=12)\n    elif time_cell[-8:-10] == '12':\n        start = start + timedelta(hours=12)\n    else:\n        start = start + timedelta(hours=24)\n        \n    return start"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading xml from file.", "response": "def load(self):\n        \"\"\"Load xml from file.\"\"\"\n        lg.info('Loading ' + str(self.xml_file))\n        update_annotation_version(self.xml_file)\n\n        xml = parse(self.xml_file)\n        return xml.getroot()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving xml to file.", "response": "def save(self):\n        \"\"\"Save xml to file.\"\"\"\n        if self.rater is not None:\n            self.rater.set('modified', datetime.now().isoformat())\n\n        xml = parseString(tostring(self.root))\n        with open(self.xml_file, 'w') as f:\n            f.write(xml.toxml())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_staging(self, filename, source, rater_name, rec_start,\n                       staging_start=None, epoch_length=None,\n                       poor=['Artefact'], as_qual=False):\n        \"\"\"Import staging from an external staging text file.\n\n        Parameters\n        ----------\n        filename : str\n            Staging file name.\n        source : str\n            Name of program where staging was made. One of 'domino', 'alice',\n            'compumedics', 'sandman', 'remlogic'\n        rater_name : str\n            Rater name for imported staging.\n        rec_start : datetime\n            Date and time (year, month, day, hour, minute, second) of recording\n            start. Year is ignored (New Year's Eve celebratory recordings\n            unsupported.)\n        staging_start : datetime (default: None)\n            Date and time of staging start. For use when not provided in\n            staging file.\n        epoch_length : int\n            duration in s of a scoring epoch\n        poor : list of str\n            epochs with stage names in this list will be marked as Poor quality\n        as_qual : bool\n            if True, the staging only be used to mark quality, as per poor\n        \"\"\"            \n        if as_qual and rater_name not in self.raters:            \n            self.parent.statusBar.showMessage('Rater not found.')\n            return\n        clue = None # used in some instances to pick out epochs from other evts\n        idx_clue = None\n            \n        if source in ['remlogic', 'sandman']:\n            encoding = 'ISO-8859-1'\n        else:\n            encoding = 'utf-8'\n\n        with open(filename, 'r', encoding=encoding) as f:\n            lines = f.readlines()\n\n        if source == 'domino':\n            \n            for i, line in enumerate(lines):\n                if line[0].isdigit():\n                    idx_first_line = i\n                    break\n            \n            if lines[idx_first_line].index(';') > 15:\n                idx_time = (11, 19)\n                idx_stage = slice(25, 26)\n                stage_key = PHYSIP_STAGE_KEY\n            else:\n                idx_time = (0, 8)\n                idx_stage = slice(14, 16)\n                stage_key = DOMINO_STAGE_KEY\n            \n            stage_start = datetime.strptime(\n                    lines[idx_first_line][idx_time[0]:idx_time[1]], '%H:%M:%S')\n            stage_day = int(lines[1][12:14])\n            stage_month = int(lines[1][15:17])\n            stage_start_for_delta = stage_start.replace(year=1999,\n                                                        month=stage_month,\n                                                        day=stage_day)\n            rec_start_for_delta = rec_start.replace(year=1999)\n            first_second = int((stage_start_for_delta -\n                                rec_start_for_delta).total_seconds())\n            \n            if epoch_length is None:\n                epoch_length = int(lines[5][6:8])\n\n        elif source == 'remlogic':\n            clue = 'SLEEP-' # signifies an epoch (as opposed to an event)\n            idx_clue = slice(-18, -6)            \n            idx_head = lines.index(\n                    next(l for l in lines if 'Time [hh:mm:ss]' in l))\n            first_line = next(l for l in lines[idx_head:] if clue in l)\n            idx_first_line = lines.index(first_line)\n            \n            stage_start_date = _try_parse_datetime(\n                    lines[3][16:lines[3].index('\\n')], \n                    ('%Y/%m/%d', '%d/%m/%Y'))\n            stage_start_time = None\n            try:\n                stage_start_time = datetime.strptime(\n                        first_line[:19], '%Y-%m-%dT%H:%M:%S')\n            except ValueError:\n                cells = first_line.split('\\t')\n                for cell in cells:\n                    try:\n                        stage_start_time = datetime.strptime(cell[-8:], \n                                                             '%I:%M:%S')\n                        if cell[1] == 'U':\n                            stage_start_time = stage_start_time + timedelta(\n                                    hours=12)\n                    except ValueError:\n                        continue\n                if stage_start_time == None:\n                    raise ValueError('No valid start time found.')\n                    \n            stage_start = datetime.combine(stage_start_date.date(), \n                                           stage_start_time.time())\n            \n            first_second = int((stage_start - rec_start).total_seconds())\n\n            stage_key = {k[-2:]: v for k, v in REMLOGIC_STAGE_KEY.items()}\n            idx_stage = slice(-6, -4)\n            \n            if epoch_length is None:\n                epoch_length = int(first_line[-3:-1])                            \n\n        elif source == 'alice':\n            stage_start = datetime.strptime(lines[1][2:13], '%I:%M:%S %p')\n            dt = rec_start\n\n            # best guess in absence of date\n            if lines[1][11:13] == 'pm' and rec_start.hour < 12:\n                dt = rec_start - timedelta(days=1)\n            elif lines[1][11:13] == 'am' and rec_start.hour > 12:\n                dt = rec_start + timedelta\n\n            stage_start = stage_start.replace(year=dt.year,\n                                              month=dt.month,\n                                              day=dt.day)\n            first_second = int((stage_start - rec_start).total_seconds())\n\n            idx_first_line = 1\n\n            lines[-1] += '_' # to fill newline position\n            stage_key = ALICE_STAGE_KEY\n            idx_stage = slice(-3, -1)\n            \n            if epoch_length is None:            \n                epoch_length = 30\n\n        elif source == 'sandman':\n            stage_start = datetime.strptime(lines[4][12:33],\n                                            '%d/%m/%Y %I:%M:%S %p')\n            first_second = int((stage_start - rec_start).total_seconds())\n\n            idx_first_line = 14\n\n            stage_key = SANDMAN_STAGE_KEY\n            idx_stage = slice(-14, -12)\n            \n            if epoch_length is None: \n                epoch_length = 30\n\n        elif source == 'compumedics':\n            if staging_start is None:\n                first_second = 0\n            else:\n                first_second = int((\n                        staging_start - rec_start).total_seconds())\n\n            idx_first_line = 0\n            stage_key = COMPUMEDICS_STAGE_KEY\n            idx_stage = slice(0, 1)\n            \n            if epoch_length is None: \n                epoch_length = 30\n            \n        elif source == 'deltamed':\n            if staging_start is None:\n                first_second = 0\n            else:\n                first_second = int((\n                        staging_start - rec_start).total_seconds())\n\n            idx_first_line = 0\n            stage_key = DELTAMED_STAGE_KEY\n            idx_stage = slice(-2, -1)\n            \n            if epoch_length is None: \n                epoch_length = int(lines[0][:lines[0].index('\\t')])\n        \n        elif source == 'prana':\n            stage_start = datetime.strptime(lines[5][:11], '%d %H:%M:%S')\n            \n            # best guess in absence of date\n            dt = rec_start\n            if stage_start.hour > 12 and rec_start.hour < 12:\n                dt = rec_start - timedelta(days=1)\n            elif stage_start.hour < 12 and rec_start.hour > 12:\n                dt = rec_start + timedelta(days=1)                                \n            stage_start = stage_start.replace(year=dt.year,\n                                              month=dt.month,\n                                              day=dt.day)                \n            first_second = int((stage_start - rec_start).total_seconds())\n            \n            idx_first_line = 5\n            \n            stage_key = PRANA_STAGE_KEY\n            \n            spacer = next(i for i, j in enumerate(lines[5][30:]) \\\n                          if j.strip())\n            idx_stage = slice(30 + spacer, 30 + spacer + 1)\n            \n            if epoch_length is None:\n                idx_epoch_length = None\n                for i,j in enumerate(lines[3]):\n                    if j.isdigit():\n                        idx_epoch_length = i, i + lines[3][i:].index(' ')\n                        epoch_length = int(lines[3][slice(*idx_epoch_length)])\n                        break\n                if idx_epoch_length is None:\n                    epoch_length = 30\n\n        else:\n            raise ValueError('Unknown source program for staging file')\n\n        offset = first_second % epoch_length\n        lg.info('Time offset: ' + str(offset) + ' sec')\n        \n        if rater_name not in self.raters:\n            self.add_rater(rater_name)\n\n        self.get_rater(rater_name)\n        stages = self.rater.find('stages')\n        \n        if as_qual:\n            \n            for i, one_line in enumerate(lines[idx_first_line:]):                     \n                \n                if one_line[idx_stage] in poor:\n                    epoch_beg = first_second + (i * epoch_length)\n                    \n                    try:\n                        self.set_stage_for_epoch(epoch_beg, 'Poor', \n                                                 attr='quality', \n                                                 save=False)\n                    except KeyError:\n                        return 1\n        \n        else:\n            # list is necessary so that it does not remove in place\n            for s in list(stages):\n                stages.remove(s)\n        \n            for i in arange(offset, first_second - epoch_length, epoch_length):\n                epoch = SubElement(stages, 'epoch')\n                \n                start_time = SubElement(epoch, 'epoch_start')\n                epoch_beg = i\n                start_time.text = str(epoch_beg)\n\n                end_time = SubElement(epoch, 'epoch_end')\n                end_time.text = str(epoch_beg + epoch_length)\n\n                epoch_stage = SubElement(epoch, 'stage')\n                epoch_stage.text = 'Unknown'\n                quality = SubElement(epoch, 'quality')\n                quality.text = 'Good'\n            \n            idx_epoch = 0\n            for i, one_line in enumerate(lines[idx_first_line:]):\n                if clue is not None:\n                    if clue not in one_line[idx_clue]:\n                        continue\n                \n                epoch = SubElement(stages, 'epoch')\n\n                start_time = SubElement(epoch, 'epoch_start')\n                epoch_beg = first_second + (idx_epoch * epoch_length)\n                start_time.text = str(epoch_beg)\n\n                end_time = SubElement(epoch, 'epoch_end')\n                end_time.text = str(epoch_beg + epoch_length)\n\n                epoch_stage = SubElement(epoch, 'stage')\n\n                try:\n                    key = one_line[idx_stage]\n                    one_stage = stage_key[key]\n\n                except KeyError:\n                    one_stage = 'Unknown'\n                    lg.info('Stage not recognized: ' + key)\n\n                epoch_stage.text = one_stage\n\n                quality = SubElement(epoch, 'quality')\n                if one_stage in poor:\n                    quality.text = 'Poor'\n                else:\n                    quality.text = 'Good'\n                    \n                idx_epoch += 1\n                    \n        self.save()", "response": "Import staging from an external staging text file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new bookmark to the rater.", "response": "def add_bookmark(self, name, time, chan=''):\n        \"\"\"Add a new bookmark\n\n        Parameters\n        ----------\n        name : str\n            name of the bookmark\n        time : (float, float)\n            float with start and end time in s\n\n        Raises\n        ------\n        IndexError\n            When there is no selected rater\n        \"\"\"\n        try:\n            bookmarks = self.rater.find('bookmarks')\n        except AttributeError:\n            raise IndexError('You need to have at least one rater')\n        new_bookmark = SubElement(bookmarks, 'bookmark')\n        bookmark_name = SubElement(new_bookmark, 'bookmark_name')\n        bookmark_name.text = name\n        bookmark_time = SubElement(new_bookmark, 'bookmark_start')\n        bookmark_time.text = str(time[0])\n        bookmark_time = SubElement(new_bookmark, 'bookmark_end')\n        bookmark_time.text = str(time[1])\n\n        if isinstance(chan, (tuple, list)):\n            chan = ', '.join(chan)\n        event_chan = SubElement(new_bookmark, 'bookmark_chan')\n        event_chan.text = chan\n\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_bookmark(self, name=None, time=None, chan=None):\n        bookmarks = self.rater.find('bookmarks')\n\n        for m in bookmarks:\n\n            bookmark_name = m.find('bookmark_name').text\n            bookmark_start = float(m.find('bookmark_start').text)\n            bookmark_end = float(m.find('bookmark_end').text)\n            bookmark_chan = m.find('bookmark_chan').text\n            if bookmark_chan is None:  # xml doesn't store empty string\n                bookmark_chan = ''\n\n            if name is None:\n                name_cond = True\n            else:\n                name_cond = bookmark_name == name\n\n            if time is None:\n                time_cond = True\n            else:\n                time_cond = (time[0] <= bookmark_end and\n                             time[1] >= bookmark_start)\n\n            if chan is None:\n                chan_cond = True\n            else:\n                chan_cond = bookmark_chan == chan\n\n            if name_cond and time_cond and chan_cond:\n                bookmarks.remove(m)\n\n        self.save()", "response": "This method removes all the bookmarks from the bookmark table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all the bookmarks in the window.", "response": "def get_bookmarks(self, time=None, chan=None):\n        \"\"\"\n        Raises\n        ------\n        IndexError\n            When there is no selected rater\n        \"\"\"\n        # get bookmarks inside window\n        try:\n            bookmarks = self.rater.find('bookmarks')\n        except AttributeError:\n            raise IndexError('You need to have at least one rater')\n\n        mrks = []\n        for m in bookmarks:\n\n            bookmark_start = float(m.find('bookmark_start').text)\n            bookmark_end = float(m.find('bookmark_end').text)\n            bookmark_chan = m.find('bookmark_chan').text\n            if bookmark_chan is None:  # xml doesn't store empty string\n                bookmark_chan = ''\n\n            if time is None:\n                time_cond = True\n            else:\n                time_cond = (time[0] <= bookmark_end and\n                             time[1] >= bookmark_start)\n\n            if chan is None:\n                chan_cond = True\n            else:\n                chan_cond = bookmark_chan == chan\n\n            if time_cond and chan_cond:\n                one_mrk = {'name': m.find('bookmark_name').text,\n                           'start': bookmark_start,\n                           'end': bookmark_end,\n                           'chan': bookmark_chan.split(', '),  # always a list\n                           }\n                mrks.append(one_mrk)\n\n        return mrks"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef event_types(self):\n        try:\n            events = self.rater.find('events')\n        except AttributeError:\n            raise IndexError('You need to have at least one rater')\n\n        return [x.get('type') for x in events]", "response": "Return a list of event types in the rater."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an event type to the event type list.", "response": "def add_event_type(self, name):\n        \"\"\"\n        Raises\n        ------\n        IndexError\n            When there is no selected rater\n        \"\"\"\n        if name in self.event_types:\n            lg.info('Event type ' + name + ' exists already.')\n            return\n\n        events = self.rater.find('events')\n        new_event_type = SubElement(events, 'event_type')\n        new_event_type.set('type', name)\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves event type based on name.", "response": "def remove_event_type(self, name):\n        \"\"\"Remove event type based on name.\"\"\"\n\n        if name not in self.event_types:\n            lg.info('Event type ' + name + ' was not found.')\n\n        events = self.rater.find('events')\n\n        # list is necessary so that it does not remove in place\n        for e in list(events):\n            if e.get('type') == name:\n                events.remove(e)\n\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rename_event_type(self, name, new_name):\n\n        if name not in self.event_types:\n            lg.info('Event type ' + name + ' was not found.')\n\n        events = self.rater.find('events')\n\n        for e in list(events):\n            if e.get('type') == name:\n                e.set('type', new_name)\n\n        self.save()", "response": "Rename an event type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_event(self, name, time, chan=''):\n        if name not in self.event_types:\n            self.add_event_type(name)\n\n        events = self.rater.find('events')\n        pattern = \"event_type[@type='\" + name + \"']\"\n        event_type = events.find(pattern)\n\n        new_event = SubElement(event_type, 'event')\n        event_start = SubElement(new_event, 'event_start')\n        event_start.text = str(time[0])\n        event_end = SubElement(new_event, 'event_end')\n        event_end.text = str(time[1])\n\n        if isinstance(chan, (tuple, list)):\n            chan = ', '.join(chan)\n        event_chan = SubElement(new_event, 'event_chan')\n        event_chan.text = chan\n\n        event_qual = SubElement(new_event, 'event_qual')\n        event_qual.text = 'Good' \n\n        self.save()", "response": "Adds an event to the annotations file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_events(self, event_list, name=None, chan=None, parent=None):\n        if name not in self.event_types:\n            self.add_event_type(name)\n\n        events = self.rater.find('events')\n        pattern = \"event_type[@type='\" + name + \"']\"\n        event_type = events.find(pattern)\n        \n        if parent is not None:\n            progress = QProgressDialog('Saving events', 'Abort',\n                               0, len(events) - 1, parent)\n            progress.setWindowModality(Qt.ApplicationModal)\n\n        for i, evt in enumerate(event_list):\n            new_event = SubElement(event_type, 'event')\n            event_start = SubElement(new_event, 'event_start')\n            event_start.text = str(evt['start'])\n            event_end = SubElement(new_event, 'event_end')\n            event_end.text = str(evt['end'])\n    \n            one_chan = chan\n            if chan is None:\n                one_chan = evt['chan']\n            if isinstance(one_chan, (tuple, list)):\n                one_chan = ', '.join(chan)\n            event_chan = SubElement(new_event, 'event_chan')\n            event_chan.text = one_chan\n    \n            event_qual = SubElement(new_event, 'event_qual')\n            event_qual.text = 'Good' \n            \n            if parent is not None:\n                progress.setValue(i)                    \n                if progress.wasCanceled():\n                    return\n\n        self.save()\n        \n        if parent is not None:\n            progress.close()", "response": "Add series of events to the event list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving event from the window.", "response": "def remove_event(self, name=None, time=None, chan=None):\n        \"\"\"get events inside window.\"\"\"\n        events = self.rater.find('events')\n        if name is not None:\n            pattern = \"event_type[@type='\" + name + \"']\"\n        else:\n            pattern = \"event_type\"\n\n        if chan is not None:\n            if isinstance(chan, (tuple, list)):\n                chan = ', '.join(chan)\n\n        for e_type in list(events.iterfind(pattern)):\n\n            for e in e_type:\n\n                event_start = float(e.find('event_start').text)\n                event_end = float(e.find('event_end').text)\n                event_chan = e.find('event_chan').text\n\n                if time is None:\n                    time_cond = True\n                else:\n                    time_cond = allclose(time[0], event_start) and allclose(\n                            time[1], event_end)\n\n                if chan is None:\n                    chan_cond = True\n                else:\n                    chan_cond = event_chan == chan\n\n                if time_cond and chan_cond:\n                    e_type.remove(e)\n\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_events(self, name=None, time=None, chan=None, stage=None,\n                   qual=None):\n        \"\"\"Get list of events in the file.\n\n        Parameters\n        ----------\n        name : str, optional\n            name of the event of interest\n        time : tuple of two float, optional\n            start and end time of the period of interest\n        chan : tuple of str, optional\n            list of channels of interests\n        stage : tuple of str, optional\n            list of stages of interest\n        qual : str, optional\n            epoch signal qualifier (Good or Poor)\n        Returns\n        -------\n        list of dict\n            where each dict has 'name' (name of the event), 'start' (start\n            time), 'end' (end time), 'chan' (channels of interest, can be\n            empty), 'stage', 'quality' (signal quality)\n\n        Raises\n        ------\n        IndexError\n            When there is no rater / epochs at all\n        \"\"\"\n        # get events inside window\n        events = self.rater.find('events')\n        if name is not None:\n            pattern = \"event_type[@type='\" + name + \"']\"\n        else:\n            pattern = \"event_type\"\n\n        if chan is not None:\n            if isinstance(chan, (tuple, list)):\n                if chan[0] is not None:\n                    chan = ', '.join(chan)\n                else:\n                    chan = None\n\n        if stage or qual:\n            ep_starts = [x['start'] for x in self.epochs]\n            if stage:\n                ep_stages = [x['stage'] for x in self.epochs]\n            if qual:\n                ep_quality = [x['quality'] for x in self.epochs]\n\n        ev = []\n        for e_type in events.iterfind(pattern):\n\n            event_name = e_type.get('type')\n\n            for e in e_type:\n\n                event_start = float(e.find('event_start').text)\n                event_end = float(e.find('event_end').text)\n                event_chan = e.find('event_chan').text\n                event_qual = e.find('event_qual').text\n                if event_chan is None:  # xml doesn't store empty string\n                    event_chan = ''\n\n                if stage or qual:\n                    pos = bisect_left(ep_starts, event_start)\n                    if pos == len(ep_starts):\n                        pos -= 1\n                    elif event_start != ep_starts[pos]:\n                        pos -= 1\n\n                if stage is None:\n                    stage_cond = True\n                else:\n                    ev_stage = ep_stages[pos]\n                    stage_cond = ev_stage in stage\n\n                if qual is None:\n                    qual_cond = True\n                else:\n                    ev_qual = ep_quality[pos]\n                    qual_cond = ev_qual == qual\n\n                if time is None:\n                    time_cond = True\n                else:\n                    time_cond = time[0] <= event_end and time[1] >= event_start\n\n                if chan is None:\n                    chan_cond = True\n                else:\n                    chan_cond = event_chan == chan\n\n                if time_cond and chan_cond and stage_cond and qual_cond:\n                    one_ev = {'name': event_name,\n                              'start': event_start,\n                              'end': event_end,\n                              'chan': event_chan.split(', '),  # always a list\n                              'stage': '',\n                              'quality': event_qual\n                              }\n                    if stage is not None:\n                        one_ev['stage'] = ev_stage\n                    ev.append(one_ev)\n\n        return ev", "response": "Get list of events in the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate epochs in annotation file.", "response": "def create_epochs(self, epoch_length=30, first_second=None):\n        \"\"\"Create epochs in annotation file.\n        Parameters\n        ----------\n        epoch_length : int\n            duration in seconds of each epoch\n        first_second : int, optional\n            Time, in seconds from record start, at which the epochs begin\n        \"\"\"\n        lg.info('creating epochs of length ' + str(epoch_length))\n        if first_second is None:\n            first_second = self.first_second\n        last_sec = ceil((self.last_second - first_second) /\n                        epoch_length) * epoch_length\n\n        stages = self.rater.find('stages')\n        for epoch_beg in range(first_second, last_sec, epoch_length):\n            epoch = SubElement(stages, 'epoch')\n\n            start_time = SubElement(epoch, 'epoch_start')\n            start_time.text = str(epoch_beg)\n\n            end_time = SubElement(epoch, 'epoch_end')\n            end_time.text = str(epoch_beg + epoch_length)\n\n            stage = SubElement(epoch, 'stage')\n            stage.text = 'Unknown'\n\n            quality = SubElement(epoch, 'quality')\n            quality.text = 'Good'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets epochs as generator", "response": "def epochs(self):\n        \"\"\"Get epochs as generator\n\n        Returns\n        -------\n        list of dict\n            each epoch is defined by start_time and end_time (in s in reference\n            to the start of the recordings) and a string of the sleep stage,\n            and a string of the signal quality.\n            If you specify stages_of_interest, only epochs belonging to those\n            stages will be included (can be an empty list).\n\n        Raises\n        ------\n        IndexError\n            When there is no rater / epochs at all\n        \"\"\"\n        if self.rater is None:\n            raise IndexError('You need to have at least one rater')\n\n        for one_epoch in self.rater.iterfind('stages/epoch'):\n            epoch = {'start': int(one_epoch.find('epoch_start').text),\n                     'end': int(one_epoch.find('epoch_end').text),\n                     'stage': one_epoch.find('stage').text,\n                     'quality': one_epoch.find('quality').text\n                     }\n            yield epoch"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_epochs(self, time=None, stage=None, qual=None,\n                   chan=None, name=None):\n        \"\"\"Get list of events in the file.\n\n        Parameters\n        ----------\n        time : tuple of two float, optional\n            start and end time of the period of interest\n        stage : tuple of str, optional\n            list of stages of interest\n        qual : str, optional\n            epoch signal qualifier (Good or Poor)\n        chan : None\n            placeholder, to maintain format similar to get_events\n        name : None\n            placeholder, to maintain format similar to get_events\n        Returns\n        -------\n        list of dict\n            where each dict has 'start' (start time), 'end' (end time),\n            'stage', 'qual' (signal quality)\n        \"\"\"\n        time_cond = True\n        stage_cond = True\n        qual_cond = True\n        valid = []\n\n        for ep in self.epochs:\n            if stage:\n                stage_cond = ep['stage'] in stage\n            if qual:\n                qual_cond = ep['quality'] == qual\n            if time:\n                time_cond = time[0] <= ep['start'] and time[1] >= ep['end']\n            if stage_cond and qual_cond and time_cond:\n                valid.append(ep)\n\n        return valid", "response": "Get list of events in the file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_epoch_start(self, window_start):\n        epoch_starts = [x['start'] for x in self.epochs]\n        idx = asarray([abs(window_start - x) for x in epoch_starts]).argmin()\n\n        return epoch_starts[idx]", "response": "Get the start of the nearest epoch."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the stage for one specific epoch.", "response": "def get_stage_for_epoch(self, epoch_start, window_length=None,\n                            attr='stage'):\n        \"\"\"Return stage for one specific epoch.\n\n        Parameters\n        ----------\n        id_epoch : str\n            index of the epoch\n        attr : str, optional\n            'stage' or 'quality'\n\n        Returns\n        -------\n        stage : str\n            description of the stage.\n        \"\"\"\n        for epoch in self.epochs:\n            if epoch['start'] == epoch_start:\n                return epoch[attr]\n\n            if window_length is not None:\n                epoch_length = epoch['end'] - epoch['start']\n                if logical_and(window_length < epoch_length,\n                               0 <= \\\n                               (epoch_start - epoch['start']) < epoch_length):\n                    return epoch[attr]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn time spent in one stage or qualifier.", "response": "def time_in_stage(self, name, attr='stage'):\n        \"\"\"Return time (in seconds) in the selected stage.\n\n        Parameters\n        ----------\n        name : str\n            one of the sleep stages, or qualifiers\n        attr : str, optional\n            either 'stage' or 'quality'\n\n        Returns\n        -------\n        int\n            time spent in one stage/qualifier, in seconds.\n\n        \"\"\"\n        return sum(x['end'] - x['start'] for x in self.epochs\n                   if x[attr] == name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange the stage for one specific epoch.", "response": "def set_stage_for_epoch(self, epoch_start, name, attr='stage', save=True):\n        \"\"\"Change the stage for one specific epoch.\n\n        Parameters\n        ----------\n        epoch_start : int\n            start time of the epoch, in seconds\n        name : str\n            description of the stage or qualifier.\n        attr : str, optional\n            either 'stage' or 'quality'\n        save : bool\n            whether to save every time one epoch is scored\n\n        Raises\n        ------\n        KeyError\n            When the epoch_start is not in the list of epochs.\n        IndexError\n            When there is no rater / epochs at all\n\n        Notes\n        -----\n        In the GUI, you want to save as often as possible, even if it slows\n        down the program, but it's the safer option. But if you're converting\n        a dataset, you want to save at the end. Do not forget to save!\n        \"\"\"\n        if self.rater is None:\n            raise IndexError('You need to have at least one rater')\n\n        for one_epoch in self.rater.iterfind('stages/epoch'):\n            if int(one_epoch.find('epoch_start').text) == epoch_start:\n                one_epoch.find(attr).text = name\n                if save:\n                    self.save()\n                return\n\n        raise KeyError('epoch starting at ' + str(epoch_start) + ' not found')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmarks the epoch start as cycle start or end.", "response": "def set_cycle_mrkr(self, epoch_start, end=False):\n        \"\"\"Mark epoch start as cycle start or end.\n\n        Parameters\n        ----------\n        epoch_start: int\n            start time of the epoch, in seconds\n        end : bool\n            If True, marked as cycle end; otherwise, marks cycle start\n        \"\"\"\n        if self.rater is None:\n            raise IndexError('You need to have at least one rater')\n\n        bound = 'start'\n        if end:\n            bound = 'end'\n\n        for one_epoch in self.rater.iterfind('stages/epoch'):\n            if int(one_epoch.find('epoch_start').text) == epoch_start:\n                cycles = self.rater.find('cycles')\n                name = 'cyc_' + bound\n                new_bound = SubElement(cycles, name)\n                new_bound.text = str(int(epoch_start))\n                self.save()\n                return\n\n        raise KeyError('epoch starting at ' + str(epoch_start) + ' not found')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves cycle marker at epoch_start.", "response": "def remove_cycle_mrkr(self, epoch_start):\n        \"\"\"Remove cycle marker at epoch_start.\n\n        Parameters\n        ----------\n        epoch_start: int\n            start time of epoch, in seconds\n        \"\"\"\n        if self.rater is None:\n            raise IndexError('You need to have at least one rater')\n        cycles = self.rater.find('cycles')\n        for one_mrkr in cycles.iterfind('cyc_start'):\n            if int(one_mrkr.text) == epoch_start:\n                cycles.remove(one_mrkr)\n                self.save()\n                return\n\n        for one_mrkr in cycles.iterfind('cyc_end'):\n            if int(one_mrkr.text) == epoch_start:\n                cycles.remove(one_mrkr)\n                self.save()\n                return\n\n        raise KeyError('cycle marker at ' + str(epoch_start) + ' not found')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove all cycles in current rater.", "response": "def clear_cycles(self):\n        \"\"\"Remove all cycle markers in current rater.\"\"\"\n        if self.rater is None:\n            raise IndexError('You need to have at least one rater')\n\n        cycles = self.rater.find('cycles')\n        for cyc in list(cycles):\n            cycles.remove(cyc)\n\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cycles(self):\n        cycles = self.rater.find('cycles')\n\n        if not cycles:\n            return None\n\n        starts = sorted(\n                [float(mrkr.text) for mrkr in cycles.findall('cyc_start')])\n        ends = sorted(\n                [float(mrkr.text) for mrkr in cycles.findall('cyc_end')])\n        cyc_list = []\n\n        if not starts or not ends:\n            return None\n\n        if all(i < starts[0] for i in ends):\n            raise ValueError('First cycle has no start.')\n\n        for (this_start, next_start) in zip(starts, starts[1:] + [inf]):\n            # if an end is smaller than the next start, make it the end\n            # otherwise, the next_start is the end\n            end_between_starts = [end for end in ends \\\n                                  if this_start < end <= next_start]\n\n            if len(end_between_starts) > 1:\n                raise ValueError('Found more than one cycle end for same '\n                                 'cycle')\n\n            if end_between_starts:\n                one_cycle = (this_start, end_between_starts[0])\n            else:\n                one_cycle = (this_start, next_start)\n\n            if one_cycle[1] == inf:\n                raise ValueError('Last cycle has no end.')\n\n            cyc_list.append(one_cycle)\n\n        output = []\n        for i, j in enumerate(cyc_list):\n            cyc = j[0], j[1], i + 1\n            output.append(cyc)\n\n        return output", "response": "Return the cycle start and end times for each cycle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef switch(self, time=None):\n        stag_to_int = {'NREM1': 1, 'NREM2': 2, 'NREM3': 3, 'REM': 5, 'Wake': 0}\n        hypno = [stag_to_int[x['stage']] for x in self.get_epochs(time=time) \\\n                 if x['stage'] in stag_to_int.keys()]\n        \n        return sum(asarray(diff(hypno), dtype=bool))", "response": "Obtain switch parameter ie number of times the stage shifts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nobtaining sleep fragmentation parameter ie number of stage shifts to a lighter stage.", "response": "def slp_frag(self, time=None):\n        \"\"\"Obtain sleep fragmentation parameter, ie number of stage shifts to \n        a lighter stage.\"\"\"\n        epochs = self.get_epochs(time=time)\n        stage_int = {'Wake': 0, 'NREM1': 1, 'NREM2': 2, 'NREM3': 3, 'REM': 2}\n        \n        hypno_str = [x['stage'] for x in epochs \\\n                     if x['stage'] in stage_int.keys()]\n        hypno_int = [stage_int[x] for x in hypno_str]\n        frag = sum(asarray(clip(diff(hypno_int), a_min=None, a_max=0), \n                           dtype=bool))\n            \n        # N3 to REM doesn't count\n        n3_to_rem = 0\n        for i, j in enumerate(hypno_str[:-1]):\n            if j == 'NREM3':\n                if hypno_str[i + 1] == 'REM':\n                    n3_to_rem += 1\n        \n        return frag - n3_to_rem"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef latency_to_consolidated(self, lights_off, duration=5, \n                                stage=['NREM2', 'NREM3']):\n        \"\"\"Find latency to the first period of uninterrupted 'stage'.\n        \n        Parameters\n        ----------\n        lights_off : float\n            lights off time, in seconds form recording start\n        duration : float\n            duration of uninterrupted period, in minutes\n        stage : list of str\n            target stage(s)\n            \n        Returns\n        -------\n        float\n            latency to the start of the consolidated period, in minutes\n        \"\"\"\n        epochs = self.get_epochs()\n        \n        if len(stage) > 1:\n            for ep in epochs:\n                if ep['stage'] in stage:\n                    ep['stage'] = 'target'\n            stage = ['target']\n            \n        hypno = [x['stage'] for x in epochs]        \n        groups = groupby(hypno)\n        runs = [(stag, sum(1 for _ in group)) for stag, group in groups]\n        \n        idx_start = 0\n        for one_stage, n in runs:\n            if (one_stage in stage) and n >= duration * 60 / self.epoch_length:\n                break\n            idx_start += n\n            \n        if idx_start < len(hypno):\n            latency = (epochs[idx_start]['start'] - lights_off) / 60 \n        else:\n            latency = nan\n        \n        return latency", "response": "Find the latency to the start of the consolidated period."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport the current state of the object to CSV file.", "response": "def export(self, file_to_export, xformat='csv'):\n        \"\"\"Export epochwise annotations to csv file.\n\n        Parameters\n        ----------\n        file_to_export : path to file\n            file to write to\n        \"\"\"\n        if 'csv' == xformat:\n        \n            with open(file_to_export, 'w', newline='') as f:\n                csv_file = writer(f)\n                csv_file.writerow(['Wonambi v{}'.format(__version__)])\n                csv_file.writerow(('clock start time', 'start', 'end',\n                                   'stage'))\n    \n                for epoch in self.epochs:\n                    epoch_time = (self.start_time +\n                                  timedelta(seconds=epoch['start']))\n                    csv_file.writerow((epoch_time.strftime('%H:%M:%S'),\n                                       epoch['start'],\n                                       epoch['end'],\n                                       epoch['stage']))\n                    \n        if 'remlogic' in xformat:\n            \n            columns = 'Time [hh:mm:ss]\\tEvent\\tDuration[s]\\n'\n            if 'remlogic_fr' == xformat:\n                columns = 'Heure [hh:mm:ss]\\tEv\u00e9nement\\tDur\u00e9e[s]\\n'\n                \n            patient_id = splitext(basename(self.dataset))[0]\n            rec_date = self.start_time.strftime('%d/%m/%Y')\n            stkey = {v:k for k, v in REMLOGIC_STAGE_KEY.items()}\n            stkey['Artefact'] = 'SLEEP-UNSCORED'\n            stkey['Unknown'] = 'SLEEP-UNSCORED'\n            stkey['Movement'] = 'SLEEP-UNSCORED'\n            \n            with open(file_to_export, 'w') as f:\n                f.write('RemLogic Event Export\\n')\n                f.write('Patient:\\t' + patient_id + '\\n')\n                f.write('Patient ID:\\t' + patient_id + '\\n')\n                f.write('Recording Date:\\t' + rec_date + '\\n')\n                f.write('\\n')\n                f.write('Events Included:\\n')\n                \n                for i in sorted(set([stkey[x['stage']] for x in self.epochs])):\n                    f.write(i + '\\n')\n                \n                f.write('\\n')\n                f.write(columns)\n                \n                for epoch in self.epochs:\n                    epoch_time = (self.start_time +\n                                  timedelta(seconds=epoch['start']))\n                    f.write((epoch_time.strftime('%Y-%m-%dT%H:%M:%S.000000') + \n                             '\\t' + \n                             stkey[epoch['stage']] + \n                             '\\t' + \n                             str(self.epoch_length) + \n                             '\\n'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate CSV with sleep statistics.", "response": "def export_sleep_stats(self, filename, lights_off, lights_on):\n        \"\"\"Create CSV with sleep statistics.\n\n        Parameters\n        ----------\n        filename: str\n            Filename for csv export\n        lights_off: float\n            Initial time when sleeper turns off the light (or their phone) to\n            go to sleep, in seconds from recording start\n        lights_on: float\n            Final time when sleeper rises from bed after sleep, in seconds from\n            recording start\n\n        Returns\n        -------\n        float or None\n            If there are no epochs scored as sleep, returns None. Otherwise,\n            returns the sleep onset latency, for testing purposes.\n            \n        Note\n        ----\n        Total dark time and sleep efficiency does NOT subtract epochs marked as\n        Undefined or Unknown.\n        \"\"\"\n        epochs = self.get_epochs()\n        ep_starts = [i['start'] for i in epochs]\n        hypno = [i['stage'] for i in epochs]\n        n_ep_per_min = 60 / self.epoch_length\n\n        first = {}\n        latency = {}\n        for stage in ['NREM1', 'NREM2', 'NREM3', 'REM']:\n            first[stage] = next(((i, j) for i, j in enumerate(epochs) if \\\n                                j['stage'] == stage), None)\n            if first[stage] is not None:\n                latency[stage] = (first[stage][1]['start'] - \n                       lights_off) / 60\n            else:\n                first[stage] = nan\n                latency[stage] = nan\n\n        idx_loff = asarray([abs(x - lights_off) for x in ep_starts]).argmin()\n        idx_lon = asarray([abs(x - lights_on) for x in ep_starts]).argmin()\n        duration = {}\n        for stage in ['NREM1', 'NREM2', 'NREM3', 'REM', 'Wake', 'Movement',\n                      'Artefact']:\n            duration[stage] = hypno[idx_loff:idx_lon].count(\n                    stage) / n_ep_per_min\n\n        slp_onset = sorted(first.values(), key=lambda x: x[1]['start'])[0]\n        wake_up = next((len(epochs) - i, j) for i, j in enumerate(\n                epochs[::-1]) if j['stage'] in ['NREM1', 'NREM2', 'NREM3',\n                                                'REM'])\n        total_dark_time = (lights_on - lights_off) / 60\n        #slp_period_time = (wake_up[1]['start'] - slp_onset[1]['start']) / 60\n        slp_onset_lat = (slp_onset[1]['start'] - lights_off) / 60\n        waso = hypno[slp_onset[0]:wake_up[0]].count('Wake') / n_ep_per_min\n        wake = waso + slp_onset_lat\n        total_slp_period = sum((waso, duration['NREM1'], duration['NREM2'],\n                                  duration['NREM3'], duration['REM']))\n        total_slp_time = total_slp_period - waso\n        slp_eff = total_slp_time / total_dark_time\n        switch = self.switch()\n        slp_frag = self.slp_frag()\n        \n        dt_format = '%d/%m/%Y %H:%M:%S'\n        loff_str = (self.start_time + timedelta(seconds=lights_off)).strftime(\n                dt_format)\n        lon_str = (self.start_time + timedelta(seconds=lights_on)).strftime(\n                dt_format)\n        slp_onset_str = (self.start_time + timedelta(\n                seconds=slp_onset[1]['start'])).strftime(dt_format)\n        wake_up_str = (self.start_time + timedelta(\n                seconds=wake_up[1]['start'])).strftime(dt_format)\n        \n        slcnrem5 = self.latency_to_consolidated(lights_off, duration=5, \n                                                stage=['NREM2', 'NREM3'])\n        slcnrem10 = self.latency_to_consolidated(lights_off, duration=10, \n                                                 stage=['NREM2', 'NREM3'])\n        slcn35 = self.latency_to_consolidated(lights_off, duration=5, \n                                              stage=['NREM3'])\n        slcn310 = self.latency_to_consolidated(lights_off, duration=10, \n                                               stage=['NREM3'])\n        \n        cycles = self.get_cycles() if self.get_cycles() else []\n        cyc_stats = []\n        \n        for i, cyc in enumerate(cycles):\n            one_cyc = {}\n            cyc_hypno = [x['stage'] for x in self.get_epochs(time=cyc)]\n            one_cyc['duration'] = {}\n            \n            for stage in ['NREM1', 'NREM2', 'NREM3', 'REM', 'Wake', 'Movement',\n                      'Artefact']:\n                one_cyc['duration'][stage] = cyc_hypno.count(stage) # in epochs\n                        \n            one_cyc['tst'] = sum([one_cyc['duration'][stage] for stage in [\n                    'NREM1', 'NREM2', 'NREM3', 'REM']])\n            one_cyc['tsp'] = one_cyc['tst'] + one_cyc['duration']['Wake']\n            one_cyc['slp_eff'] = one_cyc['tst'] / one_cyc['tsp']\n            one_cyc['switch'] = self.switch(time=cyc)\n            one_cyc['slp_frag'] = self.slp_frag(time=cyc)\n            \n            cyc_stats.append(one_cyc)\n\n        \n        with open(filename, 'w', newline='') as f:\n            lg.info('Writing to ' + str(filename))\n            cf = writer(f)\n            cf.writerow(['Wonambi v{}'.format(__version__)])            \n            cf.writerow(['Variable', 'Acronym', \n                         'Unit 1', 'Value 1', \n                         'Unit 2', 'Value 2', \n                         'Formula'])\n            cf.writerow(['Lights off', 'LOFF', \n                         'dd/mm/yyyy HH:MM:SS', loff_str, \n                         'seconds from recording start', lights_off, \n                         'marker'])\n            cf.writerow(['Lights on', 'LON', \n                         'dd/mm/yyyy HH:MM:SS', lon_str, \n                         'seconds from recording start', lights_on, \n                         'marker'])\n            cf.writerow(['Sleep onset', 'SO', \n                         'dd/mm/yyyy HH:MM:SS', slp_onset_str, \n                         'seconds from recording start', slp_onset[1]['start'], \n                         'first sleep epoch (N1 or N2) - LOFF'])\n            cf.writerow(['Time of last awakening', '',\n                         'dd/mm/yyyy HH:MM:SS', wake_up_str,\n                         'seconds from recording start', wake_up[1]['start'],\n                         'end time of last epoch of N1, N2, N3 or REM'])\n            cf.writerow(['Total dark time (Time in bed)', 'TDT (TIB)', \n                         'Epochs', total_dark_time * n_ep_per_min, \n                         'Minutes', total_dark_time, \n                         'LON - LOFF'])\n            cf.writerow(['Sleep latency', 'SL', \n                         'Epochs', slp_onset_lat * n_ep_per_min, \n                         'Minutes', slp_onset_lat, \n                         'LON - SO'])\n            cf.writerow(['Wake', 'W', \n                         'Epochs', wake * n_ep_per_min, \n                         'Minutes', wake,\n                         'total wake duration between LOFF and LON'])\n            cf.writerow(['Wake after sleep onset', 'WASO', \n                         'Epochs', waso * n_ep_per_min, \n                         'Minutes', waso, \n                         'W - SL'])\n            cf.writerow(['N1 duration', '',\n                         'Epochs', duration['NREM1'] * n_ep_per_min,\n                         'Minutes', duration['NREM1'],\n                         'total N1 duration between LOFF and LON'])\n            cf.writerow(['N2 duration', '',\n                         'Epochs', duration['NREM2'] * n_ep_per_min,\n                         'Minutes', duration['NREM2'],\n                         'total N2 duration between LOFF and LON'])\n            cf.writerow(['N3 duration', '',\n                         'Epochs', duration['NREM3'] * n_ep_per_min, \n                         'Minutes', duration['NREM3'],\n                         'total N3 duration between LOFF and LON'])\n            cf.writerow(['REM duration', '', \n                         'Epochs', duration['REM'] * n_ep_per_min,\n                         'Minutes', duration['REM'],\n                         'total REM duration between LOFF and LON'])            \n            cf.writerow(['Artefact duration', '', \n                         'Epochs', \n                         duration['Artefact'] * n_ep_per_min,\n                         'Minutes', duration['Artefact'],\n                         'total Artefact duration between LOFF and LON'])\n            cf.writerow(['Movement duration', '', \n                         'Epochs', \n                         duration['Movement'] * n_ep_per_min,\n                         'Minutes', duration['Movement'],\n                         'total Movement duration between LOFF and LON'])\n            cf.writerow(['Total sleep period', 'TSP', \n                         'Epochs', total_slp_period * n_ep_per_min, \n                         'Minutes', total_slp_period,\n                         'WASO + N1 + N2 + N3 + REM'])\n            cf.writerow(['Total sleep time', 'TST', \n                         'Epochs', total_slp_time * n_ep_per_min, \n                         'Minutes', total_slp_time, \n                         'N1 + N2 + N3 + REM'])\n            cf.writerow(['Sleep efficiency', 'SE', \n                         '%', slp_eff * 100, \n                         '', '',\n                         'TST / TDT'])\n            cf.writerow(['W % TSP', '',\n                         '%', waso * 100 / total_slp_period,\n                         '', '',\n                         'WASO / TSP'])\n            cf.writerow(['N1 % TSP', '',\n                         '%', duration['NREM1'] * 100 / total_slp_period,\n                         '', '',\n                         'N1 / TSP'])\n            cf.writerow(['N2 % TSP', '',\n                         '%', duration['NREM2'] * 100 / total_slp_period,\n                         '', '',\n                         'N2 / TSP'])\n            cf.writerow(['N3 % TSP', '',\n                         '%', duration['NREM3'] * 100 / total_slp_period,\n                         '', '',\n                         'N3 / TSP'])\n            cf.writerow(['REM % TSP', '',\n                         '%', duration['REM'] * 100 / total_slp_period,\n                         '', '',\n                         'REM / TSP'])\n            cf.writerow(['N1 % TST', '',\n                         '%', duration['NREM1'] * 100 / total_slp_time,\n                         '', '',\n                         'N1 / TST'])\n            cf.writerow(['N2 % TST', '',\n                         '%', duration['NREM2'] * 100 / total_slp_time,\n                         '', '',\n                         'N2 / TST'])\n            cf.writerow(['N3 % TST', '',\n                         '%', duration['NREM3'] * 100 / total_slp_time,\n                         '', '',\n                         'N3 / TST'])\n            cf.writerow(['REM % TST', '',\n                         '%', duration['REM'] * 100 / total_slp_time,\n                         '', '',\n                         'REM / TST'])\n            cf.writerow(['Switch', '',\n                         'N', switch,\n                         '', '', \n                         'number of stage shifts'])\n            cf.writerow(['Switch %', '',\n                         '% epochs', \n                         switch * 100 / total_slp_period / n_ep_per_min,\n                         '% minutes', switch * 100 / total_slp_period,\n                         'switch / TSP'])\n            cf.writerow(['Sleep fragmentation', '',\n                         'N', slp_frag,\n                         '', '', \n                         ('number of shifts to a lighter stage '\n                          '(W > N1 > N2 > N3; W > N1 > REM)')])\n            cf.writerow(['Sleep fragmentation index', 'SFI', \n                         '% epochs', \n                         slp_frag * 100 / total_slp_time / n_ep_per_min, \n                         '% minutes', slp_frag * 100 / total_slp_time,\n                         'sleep fragmentation / TST'])\n            cf.writerow(['Sleep latency to N1', 'SLN1', \n                         'Epochs', latency['NREM1'] * n_ep_per_min, \n                         'Minutes', latency['NREM1'],\n                         'first N1 epoch - LOFF'])\n            cf.writerow(['Sleep latency to N2', 'SLN2', \n                         'Epochs', latency['NREM2'] * n_ep_per_min, \n                         'Minutes', latency['NREM2'],\n                         'first N2 epoch - LOFF'])\n            cf.writerow(['Sleep latency to N3', 'SLN3', \n                         'Epochs', latency['NREM3'] * n_ep_per_min, \n                         'Minutes', latency['NREM3'],\n                         'first N3 epoch - LOFF'])\n            cf.writerow(['Sleep latency to REM', 'SLREM', \n                         'Epochs', latency['REM'] * n_ep_per_min, \n                         'Minutes', latency['REM'],\n                         'first REM epoch - LOFF'])\n            cf.writerow(['Sleep latency to consolidated NREM, 5 min', \n                         'SLCNREM5', \n                         'Epochs', slcnrem5 * n_ep_per_min, \n                         'Minutes', slcnrem5,\n                         ('start of first uninterrupted 5-minute period of '\n                          'N2 and/or N3 - LOFF')])\n            cf.writerow(['Sleep latency to consolidated NREM, 10 min', \n                         'SLCNREM10', \n                         'Epochs', slcnrem10 * n_ep_per_min, \n                         'Minutes', slcnrem10,\n                         ('start of first uninterrupted 10-minute period of '\n                          'N2 and/or N3 - LOFF')])\n            cf.writerow(['Sleep latency to consolidated N3, 5 min', 'SLCN35', \n                         'Epochs', slcn35 * n_ep_per_min, \n                         'Minutes', slcn35,\n                         ('start of first uninterrupted 5-minute period of '\n                          'N3 - LOFF')])\n            cf.writerow(['Sleep latency to consolidated N3, 10 min', 'SLCN310', \n                         'Epochs', slcn310 * n_ep_per_min, \n                         'Minutes', slcn310,\n                         ('start of first uninterrupted 10-minute period of '\n                          'N3 - LOFF')])\n                \n            for i in range(len(cycles)):\n                one_cyc = cyc_stats[i]\n                \n                cf.writerow([''])\n                cf.writerow([f'Cycle {i + 1}'])\n                cf.writerow(['Cycle % duration', '',\n                             '%', (one_cyc['tsp'] * 100 / \n                                   total_slp_period / n_ep_per_min),\n                             '', '', \n                             'cycle TSP / night TSP'])\n                \n                for stage in ['Wake', 'NREM1', 'NREM2', 'NREM3', 'REM', \n                              'Artefact', 'Movement']:\n                    cf.writerow([f'{stage} (c{i + 1})', '',\n                             'Epochs', one_cyc['duration'][stage],\n                             'Minutes', \n                             one_cyc['duration'][stage] / n_ep_per_min,\n                             f'total {stage} duration in cycle {i + 1}'])\n                    \n                cf.writerow([f'Total sleep period (c{i + 1})', \n                             f'TSP (c{i + 1})',\n                             'Epochs', one_cyc['tsp'],\n                             'Minutes', one_cyc['tsp'] / n_ep_per_min,\n                             f'Wake + N1 + N2 + N3 + REM in cycle {i + 1}'])\n                cf.writerow([f'Total sleep time (c{i + 1})', f'TST (c{i + 1})',\n                             'Epochs', one_cyc['tst'],\n                             'Minutes', one_cyc['tst'] / n_ep_per_min,\n                             f'N1 + N2 + N3 + REM in cycle {i + 1}'])\n                cf.writerow([f'Sleep efficiency (c{i + 1})', f'SE (c{i + 1})',\n                             '%', one_cyc['slp_eff'] * 100,\n                             '', '',\n                             f'TST / TSP in cycle {i + 1}'])\n                    \n                for denom in ['TSP', 'TST']:\n                    for stage in ['Wake', 'NREM1', 'NREM2', 'NREM3', 'REM']:\n                        cf.writerow([f'{stage} % {denom} (c{i + 1})', '', \n                                     '%', (one_cyc['duration'][stage] / \n                                           one_cyc[denom.lower()]) * 100, \n                                     '', '', \n                                     f'{stage} / {denom} in cycle {i + 1}'])\n                                     \n                cf.writerow([f'Switch (c{i + 1})', '', \n                             'N', one_cyc['switch'], '', '', \n                             f'number of stage shifts in cycle {i + 1}'])\n                cf.writerow([f'Switch % (c{i + 1})', '', \n                             '% epochs', (one_cyc['switch'] * 100 / \n                                          one_cyc['tsp']), \n                             '% minutes', (one_cyc['switch'] * 100 * \n                                           n_ep_per_min / one_cyc['tsp']), \n                             f'switch / TSP in cycle {i + 1}'])\n                cf.writerow([f'Sleep fragmentation (c{i + 1})', '', \n                             'N', one_cyc['slp_frag'], '', '', \n                             'number of shifts to a lighter stage in cycle '\n                             f'{i + 1}'])\n                cf.writerow([f'Sleep fragmentation index (c{i + 1})', \n                             f'SFI (c{i + 1})', \n                             '% epochs', (one_cyc['slp_frag'] * 100 / \n                                          one_cyc['tsp']), \n                             '% minutes', (one_cyc['slp_frag'] * 100 * \n                                           n_ep_per_min / one_cyc['tsp']), \n                             f'sleep fragmentation / TSP in cycle {i + 1}'])\n\n        return slp_onset_lat, waso, total_slp_time"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_events(self, filename, evt_type):\n        filename = splitext(filename)[0] + '.csv'\n        headings_row = ['Index',\n                       'Start time',\n                       'End time',\n                       'Stitches',\n                       'Stage',\n                       'Cycle',\n                       'Event type',\n                       'Channel']\n        \n        events = []\n        for et in evt_type:\n            events.extend(self.get_events(name=et))\n            \n        events = sorted(events, key=lambda evt: evt['start'])\n        \n        if events is None:\n            lg.info('No events found.')\n            return\n        \n        with open(filename, 'w', newline='') as f:\n            lg.info('Writing to ' + str(filename))\n            csv_file = writer(f)\n            csv_file.writerow(['Wonambi v{}'.format(__version__)])\n            csv_file.writerow(headings_row)\n            \n            for i, ev in enumerate(events):\n                csv_file.writerow([i + 1,\n                                   ev['start'],\n                                   ev['end'],\n                                   0,\n                                   ev['stage'],\n                                   '',\n                                   ev['name'],\n                                   ', '.join(ev['chan']),\n                                   ])", "response": "Export events to CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting events from Wonambi CSV file and write to annot.", "response": "def import_events(self, filename, source='wonambi', rec_start=None, \n                      parent=None):\n        \"\"\"Import events from Wonambi CSV event export and write to annot.\n        \n        Parameters\n        ----------\n        filename : str\n            path to file\n        source : str\n            source program: 'wonambi' or 'remlogic'\n        rec_start : datetime\n            Date and time (year, month, day, hour, minute, second) of recording\n            start. Year is ignored (New Year's Eve celebratory recordings\n            unsupported.) Only required for remlogic.\n        parent : QWidget\n            for GUI progress bar\n        \"\"\"\n        events = []\n        \n        if 'wonambi' == source:\n        \n            with open(filename, 'r', encoding='utf-8') as csvfile:\n                csv_reader = reader(csvfile, delimiter=',')\n                \n                for row in csv_reader:\n                    try:\n                        int(row[0])\n                        one_ev = {'name': row[6],\n                                  'start': float(row[1]),\n                                  'end': float(row[2]),\n                                  'chan': row[7].split(', '),  # always a list\n                                  'stage': row[4],\n                                  'quality': 'Good'\n                                  }\n                        events.append(one_ev)\n                        \n                    except ValueError:\n                        continue\n                \n        elif 'remlogic' == source:\n    \n            with open(filename, 'r', encoding='ISO-8859-1') as f:\n                lines = f.readlines()\n                \n                idx_header = lines.index(next(\n                        l for l in lines if 'Time [hh:mm:ss]' in l))\n                header = lines[idx_header].split('\\t')\n                header = [s.strip() for s in header] # remove trailing newline\n                idx_time = header.index('Time [hh:mm:ss]')\n                idx_evt = header.index('Event')\n                idx_dur = header.index('Duration[s]')\n                \n                # Find staging start date\n                stage_start_date = _try_parse_datetime(\n                        lines[3][16:lines[3].index('\\n')], \n                        ('%Y/%m/%d', '%d/%m/%Y'))\n                \n                # Events loop\n                for l in lines[idx_header + 1:]:\n                    cells = l.split('\\t')\n                    one_evttype = cells[idx_evt]\n                    \n                    # skip epoch staging\n                    if 'SLEEP-' in one_evttype:\n                        continue\n                    \n                    clock_start = _remlogic_time(cells[idx_time], \n                                                 stage_start_date)\n                    start = float((clock_start - rec_start).total_seconds())\n                    \n                    one_ev = {'name': one_evttype,\n                              'start': start,\n                              'end': start + float(cells[idx_dur]),\n                              'chan': '',\n                              'stage': '',\n                              'quality': 'Good'\n                                  }\n                    events.append(one_ev)\n                \n        else:\n            raise ValueError('Unknown source program for events file')\n            \n        self.add_events(events, parent=parent)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the widget layout with all the information.", "response": "def create(self):\n        \"\"\"Create the widget layout with all the information.\"\"\"\n        b0 = QGroupBox('Dataset')\n        form = QFormLayout()\n        b0.setLayout(form)\n\n        open_rec = QPushButton('Open Dataset...')\n        open_rec.clicked.connect(self.open_dataset)\n        open_rec.setToolTip('Click here to open a new recording')\n        self.idx_filename = open_rec\n        self.idx_s_freq = QLabel('')\n        self.idx_n_chan = QLabel('')\n        self.idx_start_time = QLabel('')\n        self.idx_end_time = QLabel('')\n\n        form.addRow('Filename:', self.idx_filename)\n        form.addRow('Sampl. Freq:', self.idx_s_freq)\n        form.addRow('N. Channels:', self.idx_n_chan)\n        form.addRow('Start Time: ', self.idx_start_time)\n        form.addRow('End Time: ', self.idx_end_time)\n\n        b1 = QGroupBox('View')\n        form = QFormLayout()\n        b1.setLayout(form)\n\n        self.idx_start = QLabel('')\n        self.idx_start.setToolTip('Start time in seconds from the beginning of'\n                                  ' the recordings')\n        self.idx_length = QLabel('')\n        self.idx_length.setToolTip('Duration of the time window in seconds')\n        self.idx_scaling = QLabel('')\n        self.idx_scaling.setToolTip('Global scaling for all the channels')\n        self.idx_distance = QLabel('')\n        self.idx_distance.setToolTip('Visual distances between the traces of '\n                                     'individual channels')\n\n        form.addRow('Start Time:', self.idx_start)\n        form.addRow('Length:', self.idx_length)\n        form.addRow('Scaling:', self.idx_scaling)\n        form.addRow('Distance:', self.idx_distance)\n\n        layout = QVBoxLayout()\n        layout.addWidget(b0)\n        layout.addWidget(b1)\n\n        self.setLayout(layout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating actions associated with this widget.", "response": "def create_action(self):\n        \"\"\"Create actions associated with this widget.\n\n        Notes\n        -----\n        I think that this should be a function or a property.\n\n        The good thing about the property is that it is updated every time you\n        run it (for example, if you change some parameters in the settings).\n        The main drawback is that you cannot reference back to the QAction, as\n        it creates new ones every time.\n        \"\"\"\n        output = {}\n\n        act = QAction(QIcon(ICON['open_rec']), 'Open Dataset...', self)\n        act.setShortcut(QKeySequence.Open)\n        act.triggered.connect(self.open_dataset)\n        output['open_dataset'] = act\n\n        max_dataset_history = self.parent.value('max_dataset_history')\n        recent_recs = keep_recent_datasets(max_dataset_history)\n\n        act = []\n        for one_recent_rec in recent_recs:\n            act_recent = QAction(one_recent_rec, self)\n            act_recent.triggered.connect(partial(self.open_dataset,\n                                                 one_recent_rec))\n            act.append(act_recent)\n        output['open_recent'] = act\n\n        act = QAction('Export dataset...', self)\n        act.triggered.connect(self.parent.show_export_dataset_dialog)\n        act.setEnabled(False)\n        output['export'] = act\n\n        self.action = output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open_dataset(self, recent=None, debug_filename=None, bids=False):\n        if recent:\n            filename = recent\n\n        elif debug_filename is not None:\n            filename = debug_filename\n\n        else:\n            try:\n                dir_name = dirname(self.filename)\n            except (AttributeError, TypeError):\n                dir_name = self.parent.value('recording_dir')\n\n            file_or_dir = choose_file_or_dir()\n            if file_or_dir == 'dir':\n                filename = QFileDialog.getExistingDirectory(self,\n                                                            'Open directory',\n                                                            dir_name)\n            elif file_or_dir == 'file':\n                filename, _ = QFileDialog.getOpenFileName(self, 'Open file',\n                                                          dir_name)\n\n            elif file_or_dir == 'abort':\n                return\n\n        if filename == '':\n            return\n\n        # clear previous dataset once the user opens another dataset\n        if self.dataset is not None:\n            self.parent.reset()\n\n        self.parent.statusBar().showMessage('Reading dataset: ' +\n                                            basename(filename))\n        lg.info('Reading dataset: ' + str(filename))\n        self.filename = filename # temp\n        self.dataset = Dataset(filename) #temp\n#==============================================================================\n#         try:\n#             self.filename = filename\n#             self.dataset = Dataset(filename)\n#         except FileNotFoundError:\n#             msg = 'File ' + basename(filename) + ' cannot be read'\n#             self.parent.statusBar().showMessage(msg)\n#             lg.info(msg)\n#             error_dialog = QErrorMessage()\n#             error_dialog.setWindowTitle('Error opening dataset')\n#             error_dialog.showMessage(msg)\n#             if debug_filename is None:\n#                 error_dialog.exec()\n#             return\n#\n#         except BaseException as err:\n#             self.parent.statusBar().showMessage(str(err))\n#             lg.info('Error ' + str(err))\n#             error_dialog = QErrorMessage()\n#             error_dialog.setWindowTitle('Error opening dataset')\n#             error_dialog.showMessage(str(err))\n#             if debug_filename is None:\n#                 error_dialog.exec()\n#             return\n#==============================================================================\n\n        self.action['export'].setEnabled(True)\n\n        self.parent.statusBar().showMessage('')\n\n        self.parent.update()", "response": "Open a new dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef display_dataset(self):\n        header = self.dataset.header\n\n        self.parent.setWindowTitle(basename(self.filename))\n        short_filename = short_strings(basename(self.filename))\n        self.idx_filename.setText(short_filename)\n        self.idx_s_freq.setText(str(header['s_freq']))\n        self.idx_n_chan.setText(str(len(header['chan_name'])))\n        start_time = header['start_time'].strftime('%b-%d %H:%M:%S')\n        self.idx_start_time.setText(start_time)\n        end_time = (header['start_time'] +\n                    timedelta(seconds=header['n_samples'] / header['s_freq']))\n        self.idx_end_time.setText(end_time.strftime('%b-%d %H:%M:%S'))", "response": "Update the widget with information about the dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates information about the size of the traces.", "response": "def display_view(self):\n        \"\"\"Update information about the size of the traces.\"\"\"\n        self.idx_start.setText(str(self.parent.value('window_start')))\n        self.idx_length.setText(str(self.parent.value('window_length')))\n        self.idx_scaling.setText(str(self.parent.value('y_scale')))\n        self.idx_distance.setText(str(self.parent.value('y_distance')))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self.filename = None\n        self.dataset = None\n\n        # about the recordings\n        self.idx_filename.setText('Open Recordings...')\n        self.idx_s_freq.setText('')\n        self.idx_n_chan.setText('')\n        self.idx_start_time.setText('')\n        self.idx_end_time.setText('')\n\n        # about the visualization\n        self.idx_scaling.setText('')\n        self.idx_distance.setText('')\n        self.idx_length.setText('')\n        self.idx_start.setText('')", "response": "Reset the widget to original state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export(self, new_format, filename=None, chan=None, begtime=None,\n               endtime=None):\n        \"\"\"Export current dataset to wonambi format (.won).\n\n        Parameters\n        ----------\n        new_format : str\n            Format for exported record: 'edf' or 'wonambi'\n        filename : str or PosixPath\n            filename to export to\n        chan : list of str, opt\n            list of original channel names to export. if None, all channels are\n            exported\n        begtime : int or datedelta or datetime\n            start of the data to read;\n            if it's int or float, it's assumed it's s;\n            if it's timedelta, it's assumed from the start of the recording;\n            if it's datetime, it's assumed it's absolute time.\n        endtime : int or datedelta or datetime\n            end of the data to read;\n            if it's int or float, it's assumed it's s;\n            if it's timedelta, it's assumed from the start of the recording;\n            if it's datetime, it's assumed it's absolute time.\n        \"\"\"\n        dataset = self.dataset\n        subj_id = dataset.header['subj_id']\n        if filename is None:\n            filename = dataset.filename\n\n        data = dataset.read_data(chan=chan, begtime=begtime, endtime=endtime)\n\n        if 'wonambi' == new_format:\n            write_wonambi(data, filename, subj_id=subj_id)\n\n        elif 'edf' == new_format:\n            write_edf(data, filename, subj_id=subj_id)\n\n        else:\n            self.parent.statusBar().showMessage('Format unrecognized.')", "response": "Export current dataset to new format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef button_clicked(self, button):\n        if button is self.idx_ok:\n\n            #new_format = self.new_format.get_value().lower()\n            new_format = 'edf'\n            chan = None\n            beg = None\n            end = None\n\n            if not self.all_time.get_value():\n                beg = self.times['beg'].get_value()\n                end = self.times['end'].get_value()\n\n            if not self.all_chan.get_value():\n                chan = self.get_channels()\n\n            self.parent.info.export(new_format, filename=self.filename,\n                                    chan=chan, begtime=beg, endtime=end)\n\n            self.accept()\n\n        if button is self.idx_cancel:\n            self.reject()", "response": "Action when a button was clicked."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning buttons on and off.", "response": "def toggle_buttons(self):\n        \"\"\"Turn buttons on and off.\"\"\"\n        all_time_on = self.all_time.get_value()\n        all_chan_on = self.all_chan.get_value()\n\n        self.times['beg'].setEnabled(not all_time_on)\n        self.times['end'].setEnabled(not all_time_on)\n        self.idx_chan.setEnabled(not all_chan_on)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_channels(self):\n        selectedItems = self.idx_chan.selectedItems()\n        selected_chan = [x.text() for x in selectedItems]\n        chan_in_order = []\n        for chan in self.chan:\n            if chan in selected_chan:\n                chan_in_order.append(chan)\n\n        return chan_in_order", "response": "Get the selected channel s in order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self):\n        self.filename = self.parent.info.dataset.filename\n\n        self.chan = self.parent.info.dataset.header['chan_name']\n        for chan in self.chan:\n            self.idx_chan.addItem(chan)", "response": "Update the internal list of items from the info file before opening dialog."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_data(datatype='ChanTime', n_trial=1, s_freq=256,\n                chan_name=None, n_chan=8,\n                time=None, freq=None, start_time=None,\n                signal='random', amplitude=1, color=0, sine_freq=10,\n                attr=None):\n    \"\"\"Create data of different datatype from scratch.\n\n    Parameters\n    ----------\n    datatype : str\n        one of 'ChanTime', 'ChanFreq', 'ChanTimeFreq'\n    n_trial : int\n        number of trials\n    s_freq : int\n        sampling frequency\n    chan_name : list of str\n        names of the channels\n    n_chan : int\n        if chan_name is not specified, this defines the number of channels\n    time : numpy.ndarray or tuple of two numbers\n        if tuple, the first and second numbers indicate beginning and end\n    freq : numpy.ndarray or tuple of two numbers\n        if tuple, the first and second numbers indicate beginning and end\n    start_time : datetime.datetime, optional\n        starting time of the recordings\n    attr : list of str\n        list of possible attributes (currently only 'channels')\n\n    Only for datatype == 'ChanTime'\n    signal : str\n        'random', 'sine'\n    amplitude : float\n        amplitude (peak-to-peak) of the signal\n    color : float\n        noise color to generate (white noise is 0, pink is 1, brown is 2).\n        This is only appropriate if signal == 'random'\n    sine_freq : float\n        frequency of the sine wave (only if signal == 'sine'), where phase\n        is random for each channel\n\n    Returns\n    -------\n    data : instance of specified datatype\n\n    Notes\n    -----\n    ChanTime uses randn (to have normally distributed noise), while when you\n    have freq, it uses random (which gives always positive values).\n    You can only color noise for ChanTime, not for the other datatypes.\n    \"\"\"\n    possible_datatypes = ('ChanTime', 'ChanFreq', 'ChanTimeFreq')\n    if datatype not in possible_datatypes:\n        raise ValueError('Datatype should be one of ' +\n                         ', '.join(possible_datatypes))\n\n    if time is not None:\n        if isinstance(time, tuple) and len(time) == 2:\n            time = arange(time[0], time[1], 1. / s_freq)\n    else:\n        time = arange(0, 1, 1. / s_freq)\n\n    if freq is not None:\n        if isinstance(freq, tuple) and len(freq) == 2:\n            freq = arange(freq[0], freq[1])\n    else:\n        freq = arange(0, s_freq / 2. + 1)\n\n    if chan_name is None:\n        chan_name = _make_chan_name(n_chan)\n    else:\n        n_chan = len(chan_name)\n\n    if start_time is None:\n        start_time = datetime.now()\n\n    if datatype == 'ChanTime':\n        data = ChanTime()\n        data.data = empty(n_trial, dtype='O')\n        for i in range(n_trial):\n\n            if signal == 'random':\n                values = random.randn(*(len(chan_name), len(time)))\n                for i_ch, x in enumerate(values):\n                    values[i_ch, :] = _color_noise(x, s_freq, color)\n\n            elif signal == 'sine':\n                values = empty((n_chan, time.shape[0]))\n                for i_ch in range(n_chan):\n                    values[i_ch, :] = sin(2 * pi * sine_freq * time +\n                                          random.randn())\n\n            data.data[i] = values / ptp(values, axis=1)[:, None] * amplitude\n\n    if datatype == 'ChanFreq':\n        data = ChanFreq()\n        data.data = empty(n_trial, dtype='O')\n        for i in range(n_trial):\n            data.data[i] = random.random((len(chan_name), len(freq)))\n\n    if datatype == 'ChanTimeFreq':\n        data = ChanTimeFreq()\n        data.data = empty(n_trial, dtype='O')\n        for i in range(n_trial):\n            data.data[i] = random.random((len(chan_name), len(time), len(freq)))\n\n    data.start_time = start_time\n    data.s_freq = s_freq\n    data.axis['chan'] = empty(n_trial, dtype='O')\n    for i in range(n_trial):\n        data.axis['chan'][i] = asarray(chan_name, dtype='U')\n\n    if datatype in ('ChanTime', 'ChanTimeFreq'):\n        data.axis['time'] = empty(n_trial, dtype='O')\n        for i in range(n_trial):\n            data.axis['time'][i] = time\n\n    if datatype in ('ChanFreq', 'ChanTimeFreq'):\n        data.axis['freq'] = empty(n_trial, dtype='O')\n        for i in range(n_trial):\n            data.axis['freq'][i] = freq\n\n    if attr is not None:\n        if 'chan' in attr:\n            data.attr['chan'] = create_channels(data.chan[0])\n\n    return data", "response": "Create data of different datatype from scratch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_channels(chan_name=None, n_chan=None):\n    if chan_name is not None:\n        n_chan = len(chan_name)\n\n    elif n_chan is not None:\n        chan_name = _make_chan_name(n_chan)\n\n    else:\n        raise TypeError('You need to specify either the channel names (chan_name) or the number of channels (n_chan)')\n\n    xyz = round(random.randn(n_chan, 3) * 10, decimals=2)\n    return Channels(chan_name, xyz)", "response": "Create a new random Channels object with random xyz coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _color_noise(x, s_freq, coef=0):\n    # convert to freq domain\n    y = fft(x)\n    ph = angle(y)\n    m = abs(y)\n\n    # frequencies for each fft value\n    freq = linspace(0, s_freq / 2, int(len(m) / 2) + 1)\n    freq = freq[1:-1]\n\n    # create new power spectrum\n    m1 = zeros(len(m))\n    # leave zero alone, and multiply the rest by the function\n    m1[1:int(len(m) / 2)] = m[1:int(len(m) / 2)] * f(freq, coef)\n    # simmetric around nyquist freq\n    m1[int(len(m1) / 2 + 1):] = m1[1:int(len(m1) / 2)][::-1]\n\n    # reconstruct the signal\n    y1 = m1 * exp(1j * ph)\n    return real(ifft(y1))", "response": "Add some color to the noise by changing the power spectrum."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a single block of data from a file.", "response": "def _read_block_continuous(f, i_block):\n    \"\"\"Read a single block / record completely\n\n    Parameters\n    ----------\n    f : file handle\n        handle to a file opened with 'rb'\n    i_block : int\n        index of the block to read\n\n    Returns\n    -------\n    1D array\n        data inside a block for one channel\n\n    Notes\n    -----\n    It skips the timestamp information (it's assumed to be continuous) and the\n    control characters. Maybe it might be useful to check the control\n    characters but it will slow down the execution.\n    \"\"\"\n    f.seek(HDR_LENGTH + i_block * BLK_SIZE + BEG_BLK_SIZE)\n    v = unpack(DAT_FMT, f.read(DAT_FMT_SIZE))\n\n    return array(v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the channel labels and their respective files from the Continuous_Data. openephys folder and return the frequency and the list of channels", "response": "def _read_openephys(openephys_file):\n    \"\"\"Read the channel labels and their respective files from the\n    'Continuous_Data.openephys' file\n\n    Parameters\n    ----------\n    openephys_file : Path\n        path to Continuous_Data.openephys inside the open-ephys folder\n\n    Returns\n    -------\n    int\n        sampling frequency\n    list of dict\n        list of channels containing the label, the filename and the gain\n    \"\"\"\n    root = ElementTree.parse(openephys_file).getroot()\n\n    channels = []\n    for recording in root:\n        s_freq = float(recording.attrib['samplerate'])\n        for processor in recording:\n            for channel in processor:\n                channels.append(channel.attrib)\n\n    return s_freq, channels"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the date from the settings. xml file.", "response": "def _read_date(settings_file):\n    \"\"\"Get the data from the settings.xml file\n\n    Parameters\n    ----------\n    settings_file : Path\n        path to settings.xml inside open-ephys folder\n\n    Returns\n    -------\n    datetime\n        start time of the recordings\n\n    Notes\n    -----\n    The start time is present in the header of each file. This might be useful\n    if 'settings.xml' is not present.\n    \"\"\"\n    root = ElementTree.parse(settings_file).getroot()\n    for e0 in root:\n        if e0.tag == 'INFO':\n            for e1 in e0:\n                if e1.tag == 'DATE':\n                    break\n\n    return datetime.strptime(e1.text, '%d %b %Y %H:%M:%S')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_n_samples(channel_file):\n    n_blocks = int((channel_file.stat().st_size - HDR_LENGTH) / BLK_SIZE)\n    n_samples = n_blocks * BLK_LENGTH\n    return n_blocks, n_samples", "response": "Calculate the number of samples based on the file size"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_header(filename):\n    with filename.open('rb') as f:\n        h = f.read(HDR_LENGTH).decode()\n\n        header = {}\n        for line in h.split('\\n'):\n            if '=' in line:\n                key, value = line.split(' = ')\n                key = key.strip()[7:]\n                value = value.strip()[:-1]\n                header[key] = value\n\n    return header", "response": "Read the text header for each file in the channel"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_header(channel_file, s_freq):\n    hdr = _read_header(channel_file)\n\n    assert int(hdr['header_bytes']) == HDR_LENGTH\n    assert int(hdr['blockLength']) == BLK_LENGTH\n    assert int(hdr['sampleRate']) == s_freq\n\n    return float(hdr['bitVolts'])", "response": "Check that the header is consistent with the data in the text file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef return_hdr(self):\n        subj_id = self.filename.stem  # use directory name as subject name\n\n        start_time = _read_date(self.settings_xml)\n\n        s_freq, channels = _read_openephys(self.openephys_file)\n\n        # only use channels that are actually in the folder\n        chan_name = []\n        self.channels = []\n        gain = []\n        for chan in channels:\n            channel_filename = (self.filename / chan['filename'])\n            if channel_filename.exists():\n                chan_name.append(chan['name'])\n                self.channels.append(channel_filename)\n                gain.append(_check_header(channel_filename, s_freq))\n\n            else:\n                lg.warning(f'could not find {chan[\"filename\"]} in {self.filename}')\n\n        self.gain = array(gain)\n        n_blocks, n_samples = _read_n_samples(self.channels[0])\n\n        self.blocks = ones(n_blocks, dtype='int') * BLK_LENGTH\n\n        orig = {}\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef return_dat(self, chan, begsam, endsam):\n        dat = empty((len(chan), endsam - begsam))\n        dat.fill(NaN)\n\n        for i_chan, sel_chan in enumerate(chan):\n            with self.channels[sel_chan].open('rb') as f:\n                for i_dat, blk, i_blk in _select_blocks(self.blocks, begsam, endsam):\n                    dat_in_rec = _read_block_continuous(f, blk)\n                    dat[i_chan, i_dat[0]:i_dat[1]] = dat_in_rec[i_blk[0]:i_blk[1]]\n\n        return dat * self.gain[chan, None]", "response": "Read the data for some channel in the channel and return it in the recordings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consensus(events, threshold, s_freq, min_duration=None):\n    chan = events[0][0]['chan']\n    beg = min([one_rater[0]['start'] for one_rater in events])\n    end = max([one_rater[-1]['end'] for one_rater in events])\n    n_samples = int((end - beg) * s_freq)\n    times = arange(beg, end + 1/s_freq, 1/s_freq)\n    \n    positives = zeros((len(events), n_samples))\n    for i, one_rater in enumerate(events):\n        for ev in one_rater:\n            n_start = int((ev['start'] - beg) * s_freq)\n            n_end = int((ev['end'] - beg) * s_freq)\n            positives[i, n_start:n_end].fill(1)\n                \n    consensus = mean(positives, axis=0)\n    consensus[consensus >= threshold] = 1\n    consensus[consensus < 1] = 0\n    consensus = concatenate(([0], consensus, [0]))\n    on_off = diff(consensus)\n    onsets = where(on_off == 1)\n    offsets = where(on_off == -1)\n    start_times = times[onsets]\n    end_times = times[offsets]\n    merged = vstack((start_times, end_times))\n    \n    if min_duration:\n        merged = merged[:, merged[1, :] - merged[0, :] >= min_duration]\n        \n    out = Graphoelement()\n    out.events = [{'start': merged[0, i], \n                   'end': merged[1, i],\n                   'chan': chan} for i in range(merged.shape[1])]\n\n    return out", "response": "Takes two or more event lists and outputs a merged list based on consensus."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_events(detection, standard, threshold):\n    # Vectorize start and end times and set up for broadcasting\n    det_beg = asarray([x['start'] for x in detection])[:, newaxis]\n    det_end = asarray([x['end'] for x in detection])[:, newaxis]\n    std_beg = asarray([x['start'] for x in standard])[newaxis, :]\n    std_end = asarray([x['end'] for x in standard])[newaxis, :]\n\n    # Get durations and broadcast them\n    det_dur = repeat(det_end - det_beg, len(standard), axis=1)\n    std_dur = repeat(std_end - std_beg, len(detection), axis=0)\n    \n    # Subtract every end by every start and find overlaps\n    det_minus_std = det_end - std_beg # array of shape (len(det), len(std))\n    std_minus_det = std_end - det_beg    \n    overlapping = logical_and(det_minus_std > 0, std_minus_det > 0)\n    \n    # Find intersection and union\n    shorter_diff = minimum(det_minus_std, std_minus_det)\n    longer_diff = maximum(det_minus_std, std_minus_det)\n    \n    shorter_dur = minimum(det_dur, std_dur)\n    longer_dur = maximum(det_dur, std_dur)\n    \n    interx = minimum(shorter_diff, shorter_dur)\n    union = maximum(longer_diff, longer_dur)\n        \n    # Compute intersection-union score and set non-overlapping pairs to 0\n    iu = interx / union\n    iu[invert(overlapping)] = 0\n    \n    # Threshold IU score to yield  True Positive candidates\n    iu[iu <= threshold] = 0\n    \n    # If no events, tp and fp are empty, fn is all events\n    if iu.size == 0:\n        tp = fp = asarray([])\n        fn = arange(len(standard))\n    else:\n    \n        # Find partial matches, round 1\n        det_match1 = argmax(iu, axis=1)\n        std_match1 = argmax(iu, axis=0)\n        \n        # Find full matches, round 1, then remove them from IU\n        tp = zeros(iu.shape, dtype=bool)\n        for i, j in enumerate(std_match1):\n            if det_match1[j] == i:\n                tp[j, i] = True\n                iu[j, :].fill(0)\n                iu[:, i].fill(0)\n        \n        # Round 2\n        det_match2 = argmax(iu, axis=1)\n        std_match2 = argmax(iu, axis=0)\n        \n        for i, j in enumerate(std_match2):\n            if det_match2[j] == i:\n                tp[j, i] = True\n    \n        # Find false positives and false negatives\n        fp = where(logical_and(det_match1 == 0, det_match2 == 0))[0]\n        fn = where(logical_and(std_match1 == 0, std_match2 == 0))[0]\n    \n    # Store in MatchedEvents class, which computes statistics\n    match = MatchedEvents(tp, fp, fn, detection, standard, threshold)\n    \n    return match", "response": "Find best matches between detected and standard events by a threshold."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite matched events to Wonambi XML file for visualization.", "response": "def to_annot(self, annot, category, name, s_freq=512):\n        \"\"\"Write matched events to Wonambi XML file for visualization.\n        \n        Parameters\n        ----------\n        annot : instance of Annotations\n            Annotations file\n        category : str\n            'tp_cons', 'tp_det', 'tp_std', 'fp' or 'fn'\n        name : str\n            name for the event type\n        s_freq : int\n            sampling frequency, in Hz, only required for 'tp_cons' category\n        \"\"\"\n        if 'tp_cons' == category:\n            cons = consensus((self.detection, self.standard), 1, s_freq)\n            events = cons.events\n        \n        elif 'tp_det' == category:\n            events = asarray(self.detection)[self.tp.any(axis=1)]\n            \n        elif 'tp_std' == category:\n            events = asarray(self.standard)[self.tp.any(axis=0)] \n            \n        elif 'fp' == category:\n            events = asarray(self.detection)[self.fp]\n        \n        elif 'fn' == category:\n            events = asarray(self.standard)[self.fn]\n        \n        else:\n            raise ValueError(\"Invalid category.\")\n        \n        for one_ev in events:\n            annot.add_event(name,\n                            (one_ev['start'], one_ev['end']),\n                            chan=one_ev['chan'])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all_to_annot(self, annot, names=['TPd', 'TPs', 'FP', 'FN']):\n        self.to_annot(annot, 'tp_det', names[0])\n        self.to_annot(annot, 'tp_std', names[1])\n        self.to_annot(annot, 'fp', names[2])\n        self.to_annot(annot, 'fn', names[3])", "response": "Convenience function to write all events to XML by category showing TP detection TP standard and TP function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _select_blocks(blocks, begsam, endsam):\n    intervals = cumsum(append(0, blocks))\n\n    try:\n        begblk = max(where(begsam < intervals)[0][0] - 1, 0)\n        endblk = min(where(endsam > intervals)[0][-1], len(blocks) - 1)\n    except IndexError:\n        raise StopIteration\n\n    for blk in range(begblk, endblk + 1):\n\n        beg_in_blk = max(begsam - intervals[blk], 0)\n        end_in_blk = min(endsam - intervals[blk], blocks[blk])\n\n        beg_in_dat = beg_in_blk - begsam + intervals[blk]\n        end_in_dat = end_in_blk - begsam + intervals[blk]\n\n        yield (beg_in_dat, end_in_dat), blk, (beg_in_blk, end_in_blk)", "response": "Returns a generator that yields the data stored in the specified blocks."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_sample_to_video_time(sample, orig_s_freq, sampleStamp,\n                                 sampleTime):\n    \"\"\"Convert sample number to video time, using snc information.\n\n    Parameters\n    ----------\n    sample : int\n        sample that you want to convert in time\n    orig_s_freq : int\n        sampling frequency (used as backup)\n    sampleStamp : list of int\n        Sample number from start of study\n    sampleTime : list of datetime.datetime\n        File time representation of sampleStamp\n\n    Returns\n    -------\n    instance of datetime\n        absolute time of the sample.\n\n    Notes\n    -----\n    Note that there is a discrepancy of 4 or 5 hours between the time in\n    snc and the time in the header. I'm pretty sure that the time in the\n    header is accurate, so we use that. I think that the time in snc does\n    not take into account the time zone (that'd explain the 4 or 5\n    depending on summertime). This time is only used to get the right video\n    so we call this \"video time\".\n    \"\"\"\n    if sample < sampleStamp[0]:\n        s_freq = orig_s_freq\n        id0 = 0\n    elif sample > sampleStamp[-1]:\n        s_freq = orig_s_freq\n        id0 = len(sampleStamp) - 1\n    else:\n        id0 = where(asarray(sampleStamp) <= sample)[0][-1]\n        id1 = where(asarray(sampleStamp) >= sample)[0][0]\n\n        if id0 == id1:\n            return sampleTime[id0]\n        s_freq = ((sampleStamp[id1] - sampleStamp[id0]) /\n                  (sampleTime[id1] - sampleTime[id0]).total_seconds())\n    time_diff = timedelta(seconds=(sample - sampleStamp[id0]) / s_freq)\n    return sampleTime[id0] + time_diff", "response": "Convert sample number to video time using snc information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_conversion(hdr):\n    discardbits = hdr['discardbits']\n    n_chan = hdr['num_channels']\n\n    if hdr['headbox_type'][0] in (1, 3):\n        # all channels\n        factor = ones((n_chan)) * (8711. / (2 ** 21 - 0.5)) * 2 ** discardbits\n\n    elif hdr['headbox_type'][0] == 4:\n        # 0 - 23\n        ch1 = ones((24)) * (8711. / (2 ** 21 - 0.5)) * 2 ** discardbits\n        # 24 - 27\n        ch2 = ones((4)) * ((5000000. / (2 ** 10 - 0.5)) / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2))\n\n    elif hdr['headbox_type'][0] == 6:\n        # 0 - 31\n        ch1 = ones((32)) * (8711. / (2 ** 21 - 0.5)) * 2 ** discardbits\n        # 32 - 35\n        ch2 = ones((4)) * ((5000000. / (2 ** 10 - 0.5)) / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2))\n\n    elif hdr['headbox_type'][0] == 8:\n        # 0 - 24\n        ch1 = ones((25)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 25 - 26\n        ch2 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2))\n\n    elif hdr['headbox_type'][0] == 9:\n        # 0 - 32\n        ch1 = ones((33)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 33 - 34\n        ch2 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2))\n\n    elif hdr['headbox_type'][0] == 14:\n        # 0 - 37\n        ch1 = ones((38)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 38 - 47\n        ch2 = ones((10)) * ((10800000 / 65536) / (2 ** 6)) * 2 ** discardbits\n        # 48-49\n        ch3 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2, ch3))\n\n    elif hdr['headbox_type'][0] == 15:\n        # 0 - 23\n        ch1 = ones((24)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 24 - 27 (as above)\n        ch2 = ones((4)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 28 - 31 (note 10000000 instead of 10800000)\n        ch3 = ones((4)) * ((10000000 / 65536) / (2 ** 6)) * 2 ** discardbits\n        # 32-33\n        ch4 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2, ch3, ch4))\n\n    elif hdr['headbox_type'][0] == 17:\n        # 0 - 39\n        ch1 = ones((40)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 40 - 43\n        ch2 = ones((4)) * ((10800000 / 65536) / (2 ** 6)) * 2 ** discardbits\n        # 44 - 45\n        ch3 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2, ch3))\n\n    elif hdr['headbox_type'][0] == 19:\n        # all channels\n        factor = ones((n_chan)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n\n    elif hdr['headbox_type'][0] == 21:\n        # 0 - 127\n        ch1 = ones((128)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 128 - 129\n        ch2 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n        # 130 - 255\n        ch3 = ones((126)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2, ch3))\n\n    elif hdr['headbox_type'][0] == 22:\n        # 0 - 31\n        ch1 = ones((32)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 32 - 39\n        ch2 = ones((8)) * ((10800000. / 65536.) / (2 ** 6)) * 2 ** discardbits\n        # 40 - 41\n        ch3 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n        # 42\n        ch4 = ones((1)) * ((10800000. / 65536.) / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2, ch3, ch4))\n\n    elif hdr['headbox_type'][0] == 23:\n        # 0 - 31\n        ch1 = ones((32)) * (8711. / ((2 ** 21) - 0.5)) * 2 ** discardbits\n        # 32 - 35\n        ch2 = ones((4)) * ((10800000. / 65536.) / (2 ** 6)) * 2 ** discardbits\n        # 36 - 37\n        ch3 = ones((2)) * (1 / (2 ** 6)) * 2 ** discardbits\n        # 38\n        ch4 = ones((1)) * ((10800000. / 65536.) / (2 ** 6)) * 2 ** discardbits\n\n        factor = concatenate((ch1, ch2, ch3, ch4))\n\n    else:\n        raise NotImplementedError('Implement conversion factor for headbox ' +\n                                  str(hdr['headbox_type'][0]))\n\n    return factor[:n_chan]", "response": "Calculate the conversion factor for the current headbox version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the channel names within a string.", "response": "def _find_channels(note):\n    \"\"\"Find the channel names within a string.\n\n    The channel names are stored in the .ent file. We can read the file with\n    _read_ent and we can parse most of the notes (comments) with _read_notes\n    however the note containing the montage cannot be read because it's too\n    complex. So, instead of parsing it, we just pass the string of the note\n    around. This function takes the string and finds where the channel\n    definition is.\n\n    Parameters\n    ----------\n    note : str\n        string read from .ent file, it's the note which contains montage.\n\n    Returns\n    -------\n    chan_name : list of str\n        the names of the channels.\n\n    \"\"\"\n    id_ch = note.index('ChanNames')\n    chan_beg = note.index('(', id_ch)\n    chan_end = note.index(')', chan_beg)\n    note_with_chan = note[chan_beg + 1:chan_end]\n    return [x.strip('\" ') for x in note_with_chan.split(',')]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the start time of the next segment in the base class of the base class of the base class.", "response": "def _find_start_time(hdr, s_freq):\n    \"\"\"Find the start time, usually in STC, but if that's not correct, use ERD\n\n    Parameters\n    ----------\n    hdr : dict\n        header with stc (and stamps) and erd\n    s_freq : int\n        sampling frequency\n\n    Returns\n    -------\n    datetime\n        either from stc or from erd\n\n    Notes\n    -----\n    Sometimes, but rather rarely, there is a mismatch between the time in the\n    stc and the time in the erd. For some reason, the time in the stc is way\n    off (by hours), which is clearly not correct.\n\n    We can try to reconstruct the actual time, but looking at the ERD time\n    (of any file apart from the first one) and compute the original time back\n    based on the offset of the number of samples in stc. For some reason, this\n    is not the same for all the ERD, but the jitter is in the order of 1-2s\n    which is acceptable for our purposes (probably, but be careful about the\n    notes).\n    \"\"\"\n    start_time = hdr['stc']['creation_time']\n\n    for one_stamp in hdr['stamps']:\n        if one_stamp['segment_name'].decode() == hdr['erd']['filename']:\n            offset = one_stamp['start_stamp']\n            break\n\n    erd_time = (hdr['erd']['creation_time'] -\n                timedelta(seconds=offset / s_freq)).replace(microsecond=0)\n\n    stc_erd_diff = (start_time - erd_time).total_seconds()\n    if stc_erd_diff > START_TIME_TOL:\n        lg.warn('Time difference between ERD and STC is {} s so using ERD time'\n                ' at {}'.format(stc_erd_diff, erd_time))\n        start_time = erd_time\n\n    return start_time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread notes stored in. ent file.", "response": "def _read_ent(ent_file):\n    \"\"\"Read notes stored in .ent file.\n\n    This is a basic implementation, that relies on turning the information in\n    the string in the dict format, and then evaluate it. It's not very flexible\n    and it might not read some notes, but it's fast. I could not implement a\n    nice, recursive approach.\n\n    Returns\n    -------\n    allnote : a list of dict\n        where each dict contains keys such as:\n          - type\n          - length : length of the note in B,\n          - prev_length : length of the previous note in B,\n          - unused,\n          - value : the actual content of the note.\n\n    Notes\n    -----\n    The notes are stored in a format called 'Excel list' but could not find\n    more information. It's based on \"(\" and \"(.\", and I found it very hard to\n    parse. With some basic regex and substitution, it can be evaluated into\n    a dict, with sub dictionaries. However, the note containing the name of the\n    electrodes (I think they called it \"montage\") cannot be parsed, because\n    it's too complicated. If it cannot be converted into a dict, the whole\n    string is passed as value.\n    \"\"\"\n    with ent_file.open('rb') as f:\n        f.seek(352)  # end of header\n\n        note_hdr_length = 16\n\n        allnote = []\n        while True:\n            note = {}\n            note['type'], = unpack('<i', f.read(4))\n            note['length'], = unpack('<i', f.read(4))\n            note['prev_length'], = unpack('<i', f.read(4))\n            note['unused'], = unpack('<i', f.read(4))\n            if not note['type']:\n                break\n            s = f.read(note['length'] - note_hdr_length)\n            s = s[:-2]  # it ends with one empty byte\n            s = s.decode('utf-8', errors='replace')\n            s1 = s.replace('\\n', ' ')\n            s1 = s1.replace('\\\\xd ', '')\n            s1 = s1.replace('(.', '{')\n            s1 = sub(r'\\(([A-Za-z0-9,\" ]*)\\)', r'[\\1]', s1)\n            s1 = s1.replace(')', '}')\n            # s1 = s1.replace('\",', '\" :')\n            s1 = sub(r'(\\{[\\w\"]*),', r'\\1 :', s1)\n            s1 = s1.replace('{\"', '\"')\n            s1 = s1.replace('},', ',')\n            s1 = s1.replace('}}', '}')\n            s1 = sub(r'\\(([0-9 ,-\\.]*)\\}', r'[\\1]', s1)\n            try:\n                note['value'] = eval(s1)\n            except:\n                note['value'] = s\n            allnote.append(note)\n    return allnote"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a packet of compressed data from the file and return it as ndarray.", "response": "def _read_packet(f, pos, n_smp, n_allchan, abs_delta):\n    \"\"\"\n    Read a packet of compressed data\n\n    Parameters\n    ----------\n    f : instance of opened file\n        erd file\n    pos : int\n        index of the start of the packet in the file (in bytes from beginning\n        of the file)\n    n_smp : int\n        number of samples to read\n    n_allchan : int\n        number of channels (we should specify if shorted or not)\n    abs_delta: byte\n        if the delta has this value, it means that you should read the absolute\n        value at the end of packet. If schema is 7, the length is 1; if schema\n        is 8 or 9, the length is 2.\n\n    Returns\n    -------\n    ndarray\n        data read in the packet up to n_smp.\n\n    Notes\n    -----\n    TODO: shorted chan. If I remember correctly, deltamask includes all the\n    channels, but the absolute values are only used for not-shorted channels\n\n    TODO: implement schema 7, which is slightly different, but I don't remember\n    where exactly.\n    \"\"\"\n    if len(abs_delta) == 1:  # schema 7\n        abs_delta = unpack('b', abs_delta)[0]\n    else:  # schema 8, 9\n        abs_delta = unpack('h', abs_delta)[0]\n\n    l_deltamask = int(ceil(n_allchan / BITS_IN_BYTE))\n    dat = empty((n_allchan, n_smp), dtype=int32)\n    f.seek(pos)\n\n    for i_smp in range(n_smp):\n        eventbite = f.read(1)\n\n        try:\n            assert eventbite in (b'\\x00', b'\\x01')\n        except:\n            raise Exception('at pos ' + str(i_smp) +\n                            ', eventbite (should be x00 or x01): ' +\n                            str(eventbite))\n\n        byte_deltamask = unpack('<' + 'B' * l_deltamask, f.read(l_deltamask))\n        deltamask = unpackbits(array(byte_deltamask[::-1], dtype ='uint8'))\n        deltamask = deltamask[:-n_allchan-1:-1]\n\n        n_bytes = int(deltamask.sum()) + deltamask.shape[0]\n\n        deltamask = deltamask.astype('bool')\n        # numpy has a weird way of handling string/bytes.\n        # We create a byte representation, because then tostring() works fine\n        delta_dtype = empty(n_allchan, dtype='a1')\n        delta_dtype[deltamask] = 'h'\n        delta_dtype[~deltamask] = 'b'\n        relval = array(unpack('<' + delta_dtype.tostring().decode(),\n                              f.read(n_bytes)))\n\n        read_abs = (delta_dtype == b'h') & (relval == abs_delta)\n\n        dat[~read_abs, i_smp] = dat[~read_abs, i_smp - 1] + relval[~read_abs]\n        dat[read_abs, i_smp] = fromfile(f, 'i', count=read_abs.sum())\n\n    return dat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the raw data and return a 2d matrix.", "response": "def _read_erd(erd_file, begsam, endsam):\n    \"\"\"Read the raw data and return a matrix, converted to microvolts.\n\n    Parameters\n    ----------\n    erd_file : str\n        one of the .erd files to read\n    begsam : int\n        index of the first sample to read\n    endsam : int\n        index of the last sample (excluded, per python convention)\n\n    Returns\n    -------\n    numpy.ndarray\n        2d matrix with the data, as read from the file\n\n    Error\n    -----\n    It checks whether the event byte (the first byte) is x00 as expected.\n    It can also be x01, meaning that an event was generated by an external\n    trigger. According to the manual, \"a photic stimulator is the only\n    supported device which generates an external trigger.\"\n    If the eventbyte is something else, it throws an error.\n\n    Notes\n    -----\n    Each sample point consists of these parts:\n      - Event Byte\n      - Frequency byte (only if file_schema >= 8 and one chan has != freq)\n      - Delta mask (only if file_schema >= 8)\n      - Delta Information\n      - Absolute Channel Values\n\n    Event Byte:\n      Bit 0 of the event byte indicates the presence of the external trigger\n      during the sample period. It's very rare.\n\n    Delta Mask:\n      Bit-mask of a size int( number_of_channels / 8 + 0.5). Each 1 in the mask\n      indicates that corresponding channel has 2*n bit delta, 0 means that\n      corresponding channel has n bit delta.\n      The rest of the byte of the delta mask is filled with \"1\".\n      If file_schema <= 7, it generates a \"fake\" delta, where everything is 0.\n\n    Some channels are shorted (i.e. not recorded), however they are stored in\n    a non-intuitive way: deltamask takes them into account, but for the rest\n    they are never used/recorded. So, we need to keep track both of all the\n    channels (including the non-shorted) and of the actual channels only.\n\n    When we save the data as memory-mapped, we only save the real channels.\n    However, the data in the output have both shorted and non-shorted channels.\n    Shorted channels have NaN's only.\n\n    About the actual implementation, we always follow the python convention\n    that the first sample is included and the last sample is not.\n    \"\"\"\n    hdr = _read_hdr_file(erd_file)\n    n_allchan = hdr['num_channels']\n    shorted = hdr['shorted']  # does this exist for Schema 7 at all?\n    n_shorted = sum(shorted)\n    if n_shorted > 0:\n        raise NotImplementedError('shorted channels not tested yet')\n\n    if hdr['file_schema'] in (7,):\n        abs_delta = b'\\x80'  # one byte: 10000000\n        raise NotImplementedError('schema 7 not tested yet')\n\n    if hdr['file_schema'] in (8, 9):\n        abs_delta = b'\\xff\\xff'\n\n    n_smp = endsam - begsam\n    data = empty((n_allchan, n_smp))\n    data.fill(NaN)\n\n    # it includes the sample in both cases\n    etc = _read_etc(erd_file.with_suffix('.etc'))\n    all_beg = etc['samplestamp']\n    all_end = etc['samplestamp'] + etc['sample_span'] - 1\n\n    try:\n        begrec = where((all_end >= begsam))[0][0]\n        endrec = where((all_beg < endsam))[0][-1]\n    except IndexError:\n        return data\n\n    with erd_file.open('rb') as f:\n        for rec in range(begrec, endrec + 1):\n\n            # [begpos_rec, endpos_rec]\n            begpos_rec = begsam - all_beg[rec]\n            endpos_rec = endsam - all_beg[rec]\n\n            begpos_rec = max(begpos_rec, 0)\n            endpos_rec = min(endpos_rec, all_end[rec] - all_beg[rec] + 1)\n\n            # [d1, d2)\n            d1 = begpos_rec + all_beg[rec] - begsam\n            d2 = endpos_rec + all_beg[rec] - begsam\n\n            dat = _read_packet(f, etc['offset'][rec], endpos_rec, n_allchan,\n                               abs_delta)\n            data[:, d1:d2] = dat[:, begpos_rec:endpos_rec]\n\n\n    # fill up the output data, put NaN for shorted channels\n    if n_shorted > 0:\n        full_channels = where(asarray([x == 0 for x in shorted]))[0]\n        output = empty((n_allchan, n_smp))\n        output.fill(NaN)\n        output[full_channels, :] = data\n    else:\n        output = data\n\n    factor = _calculate_conversion(hdr)\n    return expand_dims(factor, 1) * output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread information about table of content for each erd.", "response": "def _read_etc(etc_file):\n    \"\"\"Return information about table of content for each erd.\n    \"\"\"\n    etc_type = dtype([('offset', '<i'),\n                      ('samplestamp', '<i'),\n                      ('sample_num', '<i'),\n                      ('sample_span', '<h'),\n                      ('unknown', '<h')])\n\n    with etc_file.open('rb') as f:\n        f.seek(352)  # end of header\n        etc = fromfile(f, dtype=etc_type)\n\n    return etc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_snc(snc_file):\n    snc_raw_dtype = dtype([('sampleStamp', '<i'),\n                           ('sampleTime', '<q')])\n\n    with snc_file.open('rb') as f:\n        f.seek(352)  # end of header\n        snc_raw = fromfile(f, dtype=snc_raw_dtype)\n\n    sampleStamp = snc_raw['sampleStamp']\n    sampleTime = asarray([_filetime_to_dt(x) for x in snc_raw['sampleTime']])\n\n    return sampleStamp, sampleTime", "response": "Read Synchronization File and return sample stamp and time"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_stc(stc_file):\n    hdr = _read_hdr_file(stc_file)  # read header the normal way\n\n    stc_dtype = dtype([('segment_name', 'a256'),\n                       ('start_stamp', '<i'),\n                       ('end_stamp', '<i'),\n                       ('sample_num', '<i'),\n                       ('sample_span', '<i')])\n\n    with stc_file.open('rb') as f:\n        f.seek(352)  # end of header\n        hdr['next_segment'] = unpack('<i', f.read(4))[0]\n        hdr['final'] = unpack('<i', f.read(4))[0]\n        hdr['padding'] = unpack('<' + 'i' * 12, f.read(48))\n\n        stamps = fromfile(f, dtype=stc_dtype)\n\n    return hdr, stamps", "response": "Read the segment table of Contents file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_vtc(vtc_file):\n    with vtc_file.open('rb') as f:\n        filebytes = f.read()\n\n    hdr = {}\n    hdr['file_guid'] = hexlify(filebytes[:16])\n    # not sure about the 4 Bytes inbetween\n\n    i = 20\n    mpg_file = []\n    start_time = []\n    end_time = []\n    while i < len(filebytes):\n        mpg_file.append(_make_str(unpack('c' * 261, filebytes[i:i + 261])))\n        i += 261\n        Location = filebytes[i:i + 16]\n        correct = b'\\xff\\xfe\\xf8^\\xfc\\xdc\\xe5D\\x8f\\xae\\x19\\xf5\\xd6\"\\xb6\\xd4'\n        assert Location == correct\n        i += 16\n        start_time.append(_filetime_to_dt(unpack('<q',\n                                                 filebytes[i:(i + 8)])[0]))\n        i += 8\n        end_time.append(_filetime_to_dt(unpack('<q',\n                                               filebytes[i:(i + 8)])[0]))\n        i += 8\n\n    return mpg_file, start_time, end_time", "response": "Read the VTC file and return the MPG file start_time and end_time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the header of one KTLX file.", "response": "def _read_hdr_file(ktlx_file):\n    \"\"\"Reads header of one KTLX file.\n\n    Parameters\n    ----------\n    ktlx_file : Path\n        name of one of the ktlx files inside the directory (absolute path)\n\n    Returns\n    -------\n    dict\n        dict with information about the file\n\n    Notes\n    -----\n    p.3: says long, but python-long requires 8 bytes, so we use f.read(4)\n\n    GUID is correct, BUT little/big endian problems somewhere\n    \"\"\"\n    with ktlx_file.open('rb') as f:\n\n        hdr = {}\n        assert f.tell() == 0\n\n        hdr['file_guid'] = hexlify(f.read(16))\n        hdr['file_schema'], = unpack('<H', f.read(2))\n        if not hdr['file_schema'] in (1, 3, 7, 8, 9):\n            raise NotImplementedError('Reading header not implemented for ' +\n                                      'file_schema ' + str(hdr['file_schema']))\n\n        hdr['base_schema'], = unpack('<H', f.read(2))\n        if not hdr['base_schema'] == 1:  # p.3: base_schema 0 is rare, I think\n            raise NotImplementedError('Reading header not implemented for ' +\n                                      'base_schema ' + str(hdr['base_schema']))\n\n        hdr['creation_time'] = datetime.fromtimestamp(unpack('<i',\n                                                             f.read(4))[0])\n        hdr['patient_id'], = unpack('<i', f.read(4))\n        hdr['study_id'], = unpack('<i', f.read(4))\n        hdr['pat_last_name'] = _make_str(unpack('c' * 80, f.read(80)))\n        hdr['pat_first_name'] = _make_str(unpack('c' * 80, f.read(80)))\n        hdr['pat_middle_name'] = _make_str(unpack('c' * 80, f.read(80)))\n        hdr['patient_id'] = _make_str(unpack('c' * 80, f.read(80)))\n        assert f.tell() == 352\n\n        if hdr['file_schema'] >= 7:\n            hdr['sample_freq'], = unpack('<d', f.read(8))\n            n_chan, = unpack('<i', f.read(4))\n            hdr['num_channels'] = n_chan\n            hdr['deltabits'], = unpack('<i', f.read(4))\n            hdr['phys_chan'] = unpack('<' + 'i' * hdr['num_channels'],\n                                      f.read(hdr['num_channels'] * 4))\n\n            f.seek(4464)\n            hdr['headbox_type'] = unpack('<' + 'i' * 4, f.read(16))\n            hdr['headbox_sn'] = unpack('<' + 'i' * 4, f.read(16))\n            hdr['headbox_sw_version'] = _make_str(unpack('c' * 40, f.read(40)))\n            hdr['dsp_hw_version'] = _make_str(unpack('c' * 10, f.read(10)))\n            hdr['dsp_sw_version'] = _make_str(unpack('c' * 10, f.read(10)))\n            hdr['discardbits'], = unpack('<i', f.read(4))\n\n        if hdr['file_schema'] >= 8:\n            hdr['shorted'] = unpack('<' + 'h' * 1024, f.read(2048))[:n_chan]\n            hdr['frequency_factor'] = unpack('<' + 'h' * 1024,\n                                             f.read(2048))[:n_chan]\n    return hdr"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_hdr_dir(self):\n        foldername = Path(self.filename)\n        stc_file = foldername / (foldername.stem + '.stc')\n\n        if stc_file.exists():\n            self._filename = stc_file.with_suffix('')\n\n        else:  # if the folder was renamed\n            stc_file = list(foldername.glob('*.stc'))\n            if len(stc_file) == 1:\n                self._filename = foldername / stc_file[0].stem\n            elif len(stc_file) == 0:\n                raise FileNotFoundError('Could not find any .stc file.')\n            else:\n                raise OSError('Found too many .stc files: ' +\n                              '\\n'.join(str(x) for x in stc_file))\n\n        hdr = {}\n        # use .erd because it has extra info, such as sampling freq\n        # try to read any possible ERD (in case one or two ERD are missing)\n        # don't read very first erd because creation_time is slightly off\n        for erd_file in foldername.glob(self._filename.stem + '_*.erd'):\n            try:\n                hdr['erd'] = _read_hdr_file(erd_file)\n                # we need this to look up stc\n                hdr['erd'].update({'filename': erd_file.stem})\n                break\n\n            except (FileNotFoundError, PermissionError):\n                pass\n\n        stc = _read_stc(self._filename.with_suffix('.stc'))\n\n        hdr['stc'], hdr['stamps'] = stc\n\n        return hdr", "response": "Read the header for the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_dat(self, chan, begsam, endsam):\n        dat = empty((len(chan), endsam - begsam))\n        dat.fill(NaN)\n\n        stc, all_stamp = _read_stc(self._filename.with_suffix('.stc'))\n\n        all_erd = all_stamp['segment_name'].astype('U')  # convert to str\n        all_beg = all_stamp['start_stamp']\n        all_end = all_stamp['end_stamp']\n\n        try:\n            begrec = where((all_end >= begsam))[0][0]\n            endrec = where((all_beg < endsam))[0][-1]\n        except IndexError:\n            return dat\n\n        for rec in range(begrec, endrec + 1):\n\n            begpos_rec = max(begsam, all_beg[rec])\n            endpos_rec = min(endsam, all_end[rec] + 1)  # check + 1\n\n            # this looks weird, but it takes into account whether the values\n            # are outside of the limits of the file\n            d1 = begpos_rec - begsam\n            d2 = endpos_rec - begsam\n\n            erd_file = (Path(self.filename) / all_erd[rec]).with_suffix('.erd')\n\n            try:\n                dat_rec = _read_erd(erd_file, begpos_rec, endpos_rec)\n                dat[:, d1:d2] = dat_rec[chan, :]\n            except (FileNotFoundError, PermissionError):\n                lg.warning('{} does not exist'.format(erd_file))\n\n        return dat", "response": "Read the data based on the begsam and endsam."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef return_hdr(self):\n        # information contained in .erd\n        orig = self._hdr['erd']\n        if orig['patient_id']:\n            subj_id = orig['patient_id']\n        else:\n            subj_id = (orig['pat_first_name'] + orig['pat_middle_name'] +\n                       orig['pat_last_name'])\n\n        s_freq = orig['sample_freq']\n        start_time = _find_start_time(self._hdr, s_freq)\n\n        # information contained in .stc\n        n_samples = self._hdr['stamps'][-1]['end_stamp']\n\n        try:\n            ent_file = self._filename.with_suffix('.ent')\n            if not ent_file.exists():\n                ent_file = self._filename.with_suffix('.ent.old')\n            ent_notes = _read_ent(ent_file)\n        except (FileNotFoundError, PermissionError):\n            lg.warning('could not find .ent file, channels have arbitrary '\n                       'names')\n            chan_name = ['chan{0:03}'.format(x) for x in\n                         range(orig['num_channels'])]\n        else:\n            # use the last montage, hoping that it's the most accurate\n            for ent_note in reversed(ent_notes):\n                try:\n                    chan_name = _find_channels(ent_note['value'])\n                    chan_name = chan_name[:orig['num_channels']]\n                except:\n                    continue\n                else:\n                    break\n\n        try:\n            vtc_file = self._filename.with_suffix('.vtc')\n            orig['vtc'] = _read_vtc(vtc_file)\n        except (FileNotFoundError, PermissionError):\n            orig['vtc'] = None\n\n        try:\n            snc_file = self._filename.with_suffix('.snc')\n            orig['snc'] = _read_snc(snc_file)\n        except (FileNotFoundError, PermissionError):\n            orig['snc'] = None\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the notes of the Ktlx recordings and returns a list of the markers that are present in the Ktlx recordings.", "response": "def return_markers(self):\n        \"\"\"Reads the notes of the Ktlx recordings.\n        \"\"\"\n        ent_file = self._filename.with_suffix('.ent')\n        if not ent_file.exists():\n            ent_file = self._filename.with_suffix('.ent.old')\n\n        try:\n            ent_notes = _read_ent(ent_file)\n\n        except (FileNotFoundError, PermissionError):\n            markers = []\n\n        else:\n            allnote = []\n            for n in ent_notes:\n                try:\n                    n['value'].keys()\n                    allnote.append(n['value'])\n                except AttributeError:\n                    lg.debug('Note of length {} was not '\n                             'converted to dict'.format(n['length']))\n\n            s_freq = self._hdr['erd']['sample_freq']\n            pcname = '0CFEBE72-DA20-4b3a-A8AC-CDD41BFE2F0D'\n            note_time = []\n            note_name = []\n            note_note = []\n            for n in allnote:\n                if n['Text'] == 'Analyzed Data Note':\n                    continue\n                if not n['Text']:\n                    continue\n                if 'User' not in n['Data'].keys():\n                    continue\n                user1 = n['Data']['User'] == 'Persyst'\n                user2 = False  # n['Data']['User'] == 'eeg'\n                user3 = n['Data']['User'] == pcname\n                user4 = n['Data']['User'] == 'XLSpike - Intracranial'\n                user5 = n['Data']['User'] == 'XLEvent - Intracranial'\n                if user1 or user2 or user3 or user4 or user5:\n                    continue\n                if len(n['Data']['User']) == 0:\n                    note_name.append('-unknown-')\n                else:\n                    note_name.append(n['Data']['User'].split()[0])\n                note_time.append(n['Stamp'] / s_freq)\n                note_note.append(n['Text'])\n\n            markers = []\n            for time, name, note in zip(note_time, note_name, note_note):\n                m = {'name': note + ' (' + name + ')',\n                     'start': time,\n                     'end': time,\n                     'chan': None,\n                     }\n                markers.append(m)\n\n        return markers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads some information from NEV file.", "response": "def _read_neuralev(filename, read_markers=False, trigger_bits=16,\n                   trigger_zero=True):\n    \"\"\"Read some information from NEV\n\n    Parameters\n    ----------\n    filename : str\n        path to NEV file\n    read_markers : bool\n        whether to read markers or not (it can get really large)\n    trigger_bits : int, optional\n        8 or 16, read the triggers as one or two bytes\n    trigger_zero : bool, optional\n        read the trigger zero or not\n\n    Returns\n    -------\n    MetaTags : list of dict\n        which corresponds to MetaTags of openNEV\n    Markers : list of dict\n        markers in NEV file\n\n    Notes\n    -----\n    The conversion to DateTime in openNEV.m is not correct. They add a value of\n    2 to the day. Instead, they should add it to the index of the weekday\n\n    It returns triggers as strings (format of EDFBrowser), but it does not read\n    the othe types of events (waveforms, videos, etc).\n\n    The time stamps are stored in UTC in the NSx files. However, time stamps\n    in the NEV files are stored as local time up to Central 6.03 included and\n    stored as UTC after Central 6.05. It's impossible to know the version of\n    Central from the header.\n    \"\"\"\n    hdr = {}\n    with open(filename, 'rb') as f:\n\n        BasicHdr = f.read(336)\n\n        i1 = 8\n        hdr['FileTypeID'] = BasicHdr[:i1].decode('utf-8')\n        assert hdr['FileTypeID'] == 'NEURALEV'\n        i0, i1 = i1, i1 + 2\n        filespec = unpack('bb', BasicHdr[i0:i1])\n        hdr['FileSpec'] = str(filespec[0]) + '.' + str(filespec[1])\n        i0, i1 = i1, i1 + 2\n        hdr['Flags'] = unpack('<H', BasicHdr[i0:i1])[0]\n        i0, i1 = i1, i1 + 4\n        hdr['HeaderOffset'] = unpack('<I', BasicHdr[i0:i1])[0]\n        i0, i1 = i1, i1 + 4\n        hdr['PacketBytes'] = unpack('<I', BasicHdr[i0:i1])[0]\n        i0, i1 = i1, i1 + 4\n        hdr['TimeRes'] = unpack('<I', BasicHdr[i0:i1])[0]\n        i0, i1 = i1, i1 + 4\n        hdr['SampleRes'] = unpack('<I', BasicHdr[i0:i1])[0]\n        i0, i1 = i1, i1 + 16\n        time = unpack('<' + 'H' * 8, BasicHdr[i0:i1])\n        hdr['DateTimeRaw'] = time\n        lg.warning('DateTime is in local time with Central version <= 6.03'\n                   ' and in UTC with Central version > 6.05')\n        hdr['DateTime'] = datetime(time[0], time[1], time[3],\n                                   time[4], time[5], time[6], time[7] * 1000)\n        i0, i1 = i1, i1 + 32\n        # hdr['Application'] = _str(BasicHdr[i0:i1].decode('utf-8'))\n        i0, i1 = i1, i1 + 256\n        hdr['Comment'] = _str(BasicHdr[i0:i1].decode('utf-8',\n                                                     errors='replace'))\n        i0, i1 = i1, i1 + 4\n        countExtHeader = unpack('<I', BasicHdr[i0:i1])[0]\n\n        # you can read subject name from sif\n\n        # Check data duration\n        f.seek(-hdr['PacketBytes'], SEEK_END)\n        hdr['DataDuration'] = unpack('<I', f.read(4))[0]\n        hdr['DataDurationSec'] = hdr['DataDuration'] / hdr['SampleRes']\n\n        # Read the Extended Header\n        f.seek(336)\n        ElectrodesInfo = []\n        IOLabels = []\n\n        for i in range(countExtHeader):\n\n            ExtendedHeader = f.read(32)\n            i1 = 8\n            PacketID = ExtendedHeader[:i1].decode('utf-8')\n\n            if PacketID == 'NEUEVWAV':\n                elec = {}\n                i0, i1 = i1, i1 + 2\n                elec['ElectrodeID'] = unpack('<H', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 1\n                elec['ConnectorBank'] = chr(ExtendedHeader[i0] + 64)\n                i0, i1 = i1, i1 + 1\n                elec['ConnectorPin'] = ExtendedHeader[i0]\n                i0, i1 = i1, i1 + 2\n                df = unpack('<h', ExtendedHeader[i0:i1])[0]\n                # This is a workaround for the DigitalFactor overflow\n                if df == 21516:\n                    elec['DigitalFactor'] = 152592.547\n                else:\n                    elec['DigitalFactor'] = df\n\n                i0, i1 = i1, i1 + 2\n                elec['EnergyThreshold'] = unpack('<H', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 2\n                elec['HighThreshold'] = unpack('<h', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 2\n                elec['LowThreshold'] = unpack('<h', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 1\n                elec['Units'] = ExtendedHeader[i0]\n                i0, i1 = i1, i1 + 1\n                elec['WaveformBytes'] = ExtendedHeader[i0]\n                ElectrodesInfo.append(elec)\n\n            elif PacketID == 'NEUEVLBL':\n                i0, i1 = i1, i1 + 2\n                ElectrodeID = unpack('<H', ExtendedHeader[i0:i1])[0] - 1\n                s = _str(ExtendedHeader[i1:].decode('utf-8'))\n                ElectrodesInfo[ElectrodeID]['ElectrodeLabel'] = s\n\n            elif PacketID == 'NEUEVFLT':\n                elec = {}\n                i0, i1 = i1, i1 + 2\n                ElectrodeID = unpack('<H', ExtendedHeader[i0:i1])[0] - 1\n                i0, i1 = i1, i1 + 4\n                elec['HighFreqCorner'] = unpack('<I', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 4\n                elec['HighFreqOrder'] = unpack('<I', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 2\n                elec['HighFilterType'] = unpack('<H', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 4\n                elec['LowFreqCorner'] = unpack('<I', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 4\n                elec['LowFreqOrder'] = unpack('<I', ExtendedHeader[i0:i1])[0]\n                i0, i1 = i1, i1 + 2\n                elec['LowFilterType'] = unpack('<H', ExtendedHeader[i0:i1])[0]\n                ElectrodesInfo[ElectrodeID].update(elec)\n\n            elif PacketID == 'DIGLABEL':\n                # TODO: the order is not taken into account and probably wrong!\n                iolabel = {}\n\n                iolabel['mode'] = ExtendedHeader[24] + 1\n                s = _str(ExtendedHeader[8:25].decode('utf-8'))\n                iolabel['label'] = s\n                IOLabels.append(iolabel)\n\n            else:\n                raise NotImplementedError(PacketID + ' not implemented yet')\n\n        hdr['ChannelID'] = [x['ElectrodeID'] for x in ElectrodesInfo]\n\n        fExtendedHeader = f.tell()\n        fData = f.seek(0, SEEK_END)\n        countDataPacket = int((fData - fExtendedHeader) / hdr['PacketBytes'])\n\n        markers = []\n        if read_markers and countDataPacket:\n\n            f.seek(fExtendedHeader)\n            x = f.read(countDataPacket * hdr['PacketBytes'])\n\n            DigiValues = []\n            for j in range(countDataPacket):\n                i = j * hdr['PacketBytes']\n\n                if trigger_bits == 16:\n                    tempDigiVals = unpack('<H', x[8 + i:10 + i])[0]\n                else:\n                    tempDigiVals = unpack('<H', x[8 + i:9 + i] + b'\\x00')[0]\n\n                val = {'timestamp': unpack('<I', x[0 + i:4 + i])[0],\n                       'packetID': unpack('<H', x[4 + i:6 + i])[0],\n                       'tempClassOrReason': unpack('<B', x[6 + i:7 + i])[0],\n                       'tempDigiVals': tempDigiVals}\n\n                if tempDigiVals != 0 or False:\n                    DigiValues.append(val)\n\n            digserPacketID = 0\n            not_serialdigital = [x for x in DigiValues\n                                 if not x['packetID'] == digserPacketID]\n\n            if not_serialdigital:\n                lg.debug('Code not implemented to read PacketID ' +\n                         str(not_serialdigital[0]['packetID']))\n\n            # convert to markers\n            for val in DigiValues:\n                m = {'name': str(val['tempDigiVals']),\n                     'start': val['timestamp'] / hdr['SampleRes'],\n                     'end': val['timestamp'] / hdr['SampleRes'],\n                     'chan': [''],\n                     }\n                markers.append(m)\n\n    if read_markers:\n        return markers\n    else:\n        return hdr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the header for further use.", "response": "def return_hdr(self):\n        \"\"\"Return the header for further use.\n\n        Parameters\n        ----------\n        trigger_bits : int, optional\n            8 or 16, read the triggers as one or two bytes\n        trigger_zero : bool, optional\n            read the trigger zero or not\n\n        Returns\n        -------\n        subj_id : str\n            subject identification code\n        start_time : datetime\n            start time of the dataset\n        s_freq : float\n            sampling frequency\n        chan_name : list of str\n            list of all the channels\n        n_samples : int\n            number of samples in the dataset\n        orig : dict\n            additional information taken directly from the header\n\n        Notes\n        -----\n        The implementation needs to be updated for NEURALSG\n\n        \"\"\"\n        with open(self.filename, 'rb') as f:\n            file_header = f.read(8)\n\n        if file_header == b'NEURALEV':\n            orig = _read_neuralev(self.filename)\n\n            s_freq = orig['SampleRes']\n            n_samples = orig['DataDuration']\n            chan_name = []  # TODO: digital channels here instead of notes\n\n        elif file_header == b'NEURALCD':\n            orig = _read_neuralcd(self.filename)\n\n            s_freq = orig['SamplingFreq']\n            n_samples = sum(orig['DataPoints']) + sum(orig['Timestamps'])\n            chan_name = [x['Label'] for x in orig['ElectrodesInfo']]\n\n            # INFO to read the data\n            self.BOData = orig['BOData']\n            self.sess_begin, self.sess_end = _calc_sess_intervals(orig)\n            self.factor = _convert_factor(orig['ElectrodesInfo'])\n\n            nev_file = splitext(self.filename)[0] + '.nev'\n            try:\n                disable(WARNING)\n                nev_orig = _read_neuralev(nev_file)\n                disable(NOTSET)\n            except FileNotFoundError:\n                pass\n\n            else:\n                nev_orig.update(orig)  # precedence to orig\n                orig = nev_orig\n\n        elif file_header == b'NEURALSG':\n            orig = _read_neuralsg(self.filename)\n            # raise NotImplementedError('This implementation needs to be updated')\n\n            s_freq = orig['SamplingFreq']\n            n_samples = orig['DataPoints']\n\n            self.n_samples = n_samples\n            self.factor = 0.25 * ones(len(orig['ChannelID']))\n\n            # make up names\n            chan_name = ['chan{0:04d}'.format(x) for x in orig['ChannelID']]\n\n        subj_id = str()\n        start_time = orig['DateTime']\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the data as 2D numpy. ndarray.", "response": "def return_dat(self, chan, begsam, endsam):\n        \"\"\"Return the data as 2D numpy.ndarray.\n\n        Parameters\n        ----------\n        chan : int or list\n            index (indices) of the channels to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A 2d matrix, with dimension chan X samples\n\n        \"\"\"\n        ext = splitext(self.filename)[1]\n        if ext == '.nev':\n            raise TypeError('NEV contains only header info, not data')\n\n        data = _read_nsx(self.filename, self.BOData, self.sess_begin,\n                         self.sess_end, self.factor, begsam, endsam)\n\n        return data[chan, :]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef return_markers(self, trigger_bits=8, trigger_zero=True):\n        nev_file = splitext(self.filename)[0] + '.nev'\n        markers = _read_neuralev(nev_file, read_markers=True)\n\n        if trigger_bits == 8:\n            to8 = lambda x: str(int(x) - (256 ** 2 - 256))\n            for m in markers:\n                m['name'] = to8(m['name'])\n\n        if trigger_zero:\n            no_zero = (i for i, m in enumerate(markers) if m['name'] != '0')\n\n            markers_no_zero = []\n            for i in no_zero:\n                if (i + 1) < len(markers) and markers[i + 1]['name'] == '0':\n                    markers[i]['end'] = markers[i + 1]['start']\n                markers_no_zero.append(markers[i])\n\n        return markers_no_zero", "response": "Read the markers from the neural ev file and return them as a list of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dpss_windows(N, NW, Kmax, interp_from=None, interp_kind='linear'):\n    Kmax = int(Kmax)\n    W = float(NW) / N\n    nidx = np.arange(N, dtype='d')\n\n    # In this case, we create the dpss windows of the smaller size\n    # (interp_from) and then interpolate to the larger size (N)\n    if interp_from is not None:\n        if interp_from > N:\n            e_s = 'In dpss_windows, interp_from is: %s ' % interp_from\n            e_s += 'and N is: %s. ' % N\n            e_s += 'Please enter interp_from smaller than N.'\n            raise ValueError(e_s)\n        dpss = []\n        d, e = dpss_windows(interp_from, NW, Kmax)\n        for this_d in d:\n            x = np.arange(this_d.shape[-1])\n            I = interpolate.interp1d(x, this_d, kind=interp_kind)\n            d_temp = I(np.linspace(0, this_d.shape[-1] - 1, N, endpoint=False))\n\n            # Rescale:\n            d_temp = d_temp / np.sqrt(np.sum(d_temp ** 2))\n\n            dpss.append(d_temp)\n\n        dpss = np.array(dpss)\n\n    else:\n        # here we want to set up an optimization problem to find a sequence\n        # whose energy is maximally concentrated within band [-W,W].\n        # Thus, the measure lambda(T,W) is the ratio between the energy within\n        # that band, and the total energy. This leads to the eigen-system\n        # (A - (l1)I)v = 0, where the eigenvector corresponding to the largest\n        # eigenvalue is the sequence with maximally concentrated energy. The\n        # collection of eigenvectors of this system are called Slepian\n        # sequences, or discrete prolate spheroidal sequences (DPSS). Only the\n        # first K, K = 2NW/dt orders of DPSS will exhibit good spectral\n        # concentration\n        # [see http://en.wikipedia.org/wiki/Spectral_concentration_problem]\n\n        # Here I set up an alternative symmetric tri-diagonal eigenvalue\n        # problem such that\n        # (B - (l2)I)v = 0, and v are our DPSS (but eigenvalues l2 != l1)\n        # the main diagonal = ([N-1-2*t]/2)**2 cos(2PIW), t=[0,1,2,...,N-1]\n        # and the first off-diagonal = t(N-t)/2, t=[1,2,...,N-1]\n        # [see Percival and Walden, 1993]\n        diagonal = ((N - 1 - 2 * nidx) / 2.) ** 2 * np.cos(2 * np.pi * W)\n        off_diag = np.zeros_like(nidx)\n        off_diag[:-1] = nidx[1:] * (N - nidx[1:]) / 2.\n        # put the diagonals in LAPACK \"packed\" storage\n        ab = np.zeros((2, N), 'd')\n        ab[1] = diagonal\n        ab[0, 1:] = off_diag[:-1]\n        # only calculate the highest Kmax eigenvalues\n        w = linalg.eigvals_banded(ab, select='i',\n                                  select_range=(N - Kmax, N - 1))\n        w = w[::-1]\n\n        # find the corresponding eigenvectors via inverse iteration\n        t = np.linspace(0, np.pi, N)\n        dpss = np.zeros((Kmax, N), 'd')\n        for k in range(Kmax):\n            dpss[k] = tridi_inverse_iteration(\n                diagonal, off_diag, w[k], x0=np.sin((k + 1) * t)\n                )\n\n    # By convention (Percival and Walden, 1993 pg 379)\n    # * symmetric tapers (k=0,2,4,...) should have a positive average.\n    # * antisymmetric tapers should begin with a positive lobe\n    fix_symmetric = (dpss[0::2].sum(axis=1) < 0)\n    for i, f in enumerate(fix_symmetric):\n        if f:\n            dpss[2 * i] *= -1\n    # rather than test the sign of one point, test the sign of the\n    # linear slope up to the first (largest) peak\n    pk = np.argmax(np.abs(dpss[1::2, :N//2]), axis=1)\n    for i, p in enumerate(pk):\n        if np.sum(dpss[2 * i + 1, :p]) < 0:\n            dpss[2 * i + 1] *= -1\n\n    # Now find the eigenvalues of the original spectral concentration problem\n    # Use the autocorr sequence technique from Percival and Walden, 1993 pg 390\n    dpss_rxx = autocorr(dpss) * N\n    r = 4 * W * np.sinc(2 * W * nidx)\n    r[0] = 2 * W\n    eigvals = np.dot(dpss_rxx, r)\n\n    return dpss, eigvals", "response": "Returns the Discrete Prolate Spheroidal Sequences of orders 0 Kmax - 1 for a given frequency - spacing multiple NW and sequence length N."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform an inverse iteration to find the eigenvector corresponding to the given eigenvalue stored in e.", "response": "def tridi_inverse_iteration(d, e, w, x0=None, rtol=1e-8):\n    \"\"\"Perform an inverse iteration to find the eigenvector corresponding\n    to the given eigenvalue in a symmetric tridiagonal system.\n\n    Parameters\n    ----------\n\n    d : ndarray\n      main diagonal of the tridiagonal system\n    e : ndarray\n      offdiagonal stored in e[:-1]\n    w : float\n      eigenvalue of the eigenvector\n    x0 : ndarray\n      initial point to start the iteration\n    rtol : float\n      tolerance for the norm of the difference of iterates\n\n    Returns\n    -------\n\n    e : ndarray\n      The converged eigenvector\n\n    \"\"\"\n    eig_diag = d - w\n    if x0 is None:\n        x0 = np.random.randn(len(d))\n    x_prev = np.zeros_like(x0)\n    norm_x = np.linalg.norm(x0)\n    # the eigenvector is unique up to sign change, so iterate\n    # until || |x^(n)| - |x^(n-1)| ||^2 < rtol\n    x0 /= norm_x\n    while np.linalg.norm(np.abs(x0) - np.abs(x_prev)) > rtol:\n        x_prev = x0.copy()\n        tridisolve(eig_diag, e, x0)\n        norm_x = np.linalg.norm(x0)\n        x0 /= norm_x\n    return x0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the autocovariance of signal s at all nonzero lags.", "response": "def autocov(x, **kwargs):\n    \"\"\"Returns the autocovariance of signal s at all lags.\n\n    Parameters\n    ----------\n\n    x : ndarray\n    axis : time axis\n    all_lags : {True/False}\n       whether to return all nonzero lags, or to clip the length of r_xy\n       to be the length of x and y. If False, then the zero lag correlation\n       is at index 0. Otherwise, it is found at (len(x) + len(y) - 1)/2\n\n    Returns\n    -------\n\n    cxx : ndarray\n       The autocovariance function\n\n    Notes\n    -----\n\n    Adheres to the definition\n\n    .. math::\n\n    C_{xx}[k]=E\\{(X[n+k]-E\\{X\\})(X[n]-E\\{X\\})^{*}\\}\n\n    where X is a discrete, stationary (ergodic) random process\n    \"\"\"\n    # only remove the mean once, if needed\n    debias = kwargs.pop('debias', True)\n    axis = kwargs.get('axis', -1)\n    if debias:\n        x = remove_bias(x, axis)\n    kwargs['debias'] = False\n    return crosscov(x, x, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the crosscovariance sequence between two ndarrays.", "response": "def crosscov(x, y, axis=-1, all_lags=False, debias=True, normalize=True):\n    \"\"\"Returns the crosscovariance sequence between two ndarrays.\n    This is performed by calling fftconvolve on x, y[::-1]\n\n    Parameters\n    ----------\n\n    x : ndarray\n    y : ndarray\n    axis : time axis\n    all_lags : {True/False}\n       whether to return all nonzero lags, or to clip the length of s_xy\n       to be the length of x and y. If False, then the zero lag covariance\n       is at index 0. Otherwise, it is found at (len(x) + len(y) - 1)/2\n    debias : {True/False}\n       Always removes an estimate of the mean along the axis, unless\n       told not to (eg X and Y are known zero-mean)\n\n    Returns\n    -------\n\n    cxy : ndarray\n       The crosscovariance function\n\n    Notes\n    -----\n\n    cross covariance of processes x and y is defined as\n\n    .. math::\n\n    C_{xy}[k]=E\\{(X(n+k)-E\\{X\\})(Y(n)-E\\{Y\\})^{*}\\}\n\n    where X and Y are discrete, stationary (or ergodic) random processes\n\n    Also note that this routine is the workhorse for all auto/cross/cov/corr\n    functions.\n\n    \"\"\"\n    if x.shape[axis] != y.shape[axis]:\n        raise ValueError(\n            'crosscov() only works on same-length sequences for now'\n            )\n    if debias:\n        x = remove_bias(x, axis)\n        y = remove_bias(y, axis)\n    slicing = [slice(d) for d in x.shape]\n    slicing[axis] = slice(None, None, -1)\n    cxy = fftconvolve(x, y[tuple(slicing)].conj(), axis=axis, mode='full')\n    N = x.shape[axis]\n    if normalize:\n        cxy /= N\n    if all_lags:\n        return cxy\n    slicing[axis] = slice(N - 1, 2 * N - 1)\n    return cxy[tuple(slicing)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frequency(data, output='spectraldensity', scaling='power', sides='one',\n              taper=None, halfbandwidth=3, NW=None, duration=None,\n              overlap=0.5, step=None, detrend='linear', n_fft=None,\n              log_trans=False, centend='mean'):\n    \"\"\"Compute the\n    power spectral density (PSD, output='spectraldensity', scaling='power'), or\n    energy spectral density (ESD, output='spectraldensity', scaling='energy') or\n    the complex fourier transform (output='complex', sides='two')\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        one of the datatypes\n    detrend : str\n        None (no detrending), 'constant' (remove mean), 'linear' (remove linear\n        trend)\n    output : str\n        'spectraldensity' or 'csd' or 'complex'\n        'spectraldensity' meaning the autospectrum or auto-spectral density,\n        a special case of 'csd' (cross-spectral density), where the signal is\n        cross-correlated with itself\n        if 'csd', both channels in data are used as input\n    sides : str\n        'one' or 'two', where 'two' implies negative frequencies\n    scaling : str\n        'power' (units: V ** 2 / Hz), 'energy' (units: V ** 2), 'fieldtrip',\n        'chronux'\n    taper : str\n        Taper to use, commonly used tapers are 'boxcar', 'hann', 'dpss'\n    halfbandwidth : int\n        (only if taper='dpss') Half bandwidth (in Hz), frequency smoothing will\n        be from +halfbandwidth to -halfbandwidth\n    NW : int\n        (only if taper='dpss') Normalized half bandwidth\n        (NW = halfbandwidth * dur). Number of DPSS tapers is 2 * NW - 1.\n        If specified, NW takes precedence over halfbandwidth\n    duration : float, in s\n        If not None, it divides the signal in epochs of this length (in seconds)\n        and then average over the PSD / ESD (not the complex result)\n    overlap : float, between 0 and 1\n        The amount of overlap between epochs (0.5 = 50%, 0.95 = almost complete\n        overlap).\n    step : float, in s\n        step in seconds between epochs (alternative to overlap)\n    n_fft: int\n        Length of FFT, in samples. If less than input axis, input is cropped.\n        If longer than input axis, input is padded with zeros. If None, FFT\n        length set to axis length.\n    log_trans : bool\n        If True, spectral values will be natural log-transformed. The\n        transformation is applied before averaging (or taking the median).\n    centend : str\n        (only if duration is not None). Central tendency measure to use, either\n        mean (arithmetic) or median.\n\n    Returns\n    -------\n    instance of ChanFreq\n        If output='complex', there is an additional dimension ('taper') which\n        is useful for 'dpss' but it's also present for all the other tapers.\n\n    Raises\n    ------\n    TypeError\n        If the data does not have a 'time' axis. It might work in the\n        future on other axes, but I cannot imagine how.\n\n    ValueError\n        If you use duration (to create multiple epochs) and output='complex',\n        because it does not average the complex output of multiple epochs.\n\n    Notes\n    -----\n    See extensive notes at wonambi.trans.frequency._fft\n\n    It uses sampling frequency as specified in s_freq, it does not\n    recompute the sampling frequency based on the time axis.\n    \n    Use of log or median for Welch's method is included based on \n    recommendations from Izhikevich et al., bioRxiv, 2018.\n    \"\"\"\n    if output not in ('spectraldensity', 'complex', 'csd'):\n        raise TypeError(f'output can be \"spectraldensity\", \"complex\" or \"csd\",'\n                        ' not \"{output}\"')\n    if 'time' not in data.list_of_axes:\n        raise TypeError('\\'time\\' is not in the axis ' +\n                        str(data.list_of_axes))\n    if len(data.list_of_axes) != data.index_of('time') + 1:\n        raise TypeError('\\'time\\' should be the last axis')  # this might be improved\n\n    if duration is not None and output == 'complex':\n        raise ValueError('cannot average the complex spectrum over multiple epochs')\n\n    if output == 'csd' and data.number_of('chan') != 2:\n        raise ValueError('CSD can only be computed between two channels')\n\n    if duration is not None:\n        nperseg = int(duration * data.s_freq)\n        if step is not None:\n            nstep = int(step * data.s_freq)\n        else:\n            nstep = nperseg - int(overlap * nperseg)\n\n    freq = ChanFreq()\n    freq.attr = deepcopy(data.attr)\n    freq.s_freq = data.s_freq\n    freq.start_time = data.start_time\n    freq.axis['chan'] = copy(data.axis['chan'])\n    freq.axis['freq'] = empty(data.number_of('trial'), dtype='O')\n    if output == 'complex':\n        freq.axis['taper'] = empty(data.number_of('trial'), dtype='O')\n    freq.data = empty(data.number_of('trial'), dtype='O')\n\n    for i in range(data.number_of('trial')):\n        x = data(trial=i)\n        if duration is not None:\n            x = _create_subepochs(x, nperseg, nstep)\n\n        f, Sxx = _fft(x,\n                      s_freq=data.s_freq,\n                      detrend=detrend,\n                      taper=taper,\n                      output=output,\n                      sides=sides,\n                      scaling=scaling,\n                      halfbandwidth=halfbandwidth,\n                      NW=NW,\n                      n_fft=n_fft)\n\n        if log_trans:\n            Sxx = log(Sxx)\n        \n        if duration is not None:\n            if centend == 'mean':\n                Sxx = Sxx.mean(axis=-2)\n            elif centend == 'median':\n                Sxx = median(Sxx, axis=-2)\n            else:\n                raise ValueError('Invalid central tendency measure. '\n                                 'Use mean or median.')\n\n        freq.axis['freq'][i] = f\n        if output == 'complex':\n            freq.axis['taper'][i] = arange(Sxx.shape[-1])\n        if output == 'csd':\n            newchan = ' * '.join(freq.axis['chan'][i])\n            freq.axis['chan'][i] = asarray([newchan], dtype='U')\n        freq.data[i] = Sxx\n\n    return freq", "response": "Compute the frequency of the specified channel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the power spectrum over time.", "response": "def timefrequency(data, method='morlet', time_skip=1, **options):\n    \"\"\"Compute the power spectrum over time.\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data to analyze\n    method : str\n        the method to compute the time-frequency representation, such as\n        'morlet' (wavelet using complex morlet window),\n        'spectrogram' (corresponds to 'spectraldensity' in frequency()),\n        'stft' (short-time fourier transform, corresponds to 'complex' in\n        frequency())\n    options : dict\n        options depend on the method used, see below.\n\n    Returns\n    -------\n    instance of ChanTimeFreq\n        data in time-frequency representation. The exact output depends on\n        the method. Using 'morlet', you get a complex output at each frequency\n        where the wavelet was computed.\n\n    Examples\n    --------\n    The data in ChanTimeFreq are complex and they should stay that way. You\n    can also get the magnitude or power the easy way using Math.\n\n    >>> from wonambi.trans import math, timefreq\n    >>> tf = timefreq(data, foi=(8, 10))\n    >>> tf_abs = math(tf, operator_name='abs')\n    >>> tf_abs.data[0][0, 0, 0]\n    1737.4662329214384)\n\n    Notes\n    -----\n    It uses sampling frequency as specified in s_freq, it does not\n    recompute the sampling frequency based on the time axis.\n\n    For method 'morlet', the following options should be specified:\n        foi : ndarray or list or tuple\n            vector with frequency of interest\n        ratio : float\n            ratio for a wavelet family ( = freq / sigma_f)\n        sigma_f : float\n            standard deviation of the wavelet in frequency domain\n        dur_in_sd : float\n            duration of the wavelet, given as number of the standard deviation\n            in the time domain, in one side.\n        dur_in_s : float\n            total duration of the wavelet, two-sided (i.e. from start to\n            finish)\n        time_skip : int, in samples\n            number of time points to skip (it runs convolution on all the\n            data points, but you don't need to store them all)\n        normalization : str\n            'area' means that energy is normalized to 1, 'peak' means that the\n            peak of the wavelet is set at 1, 'max' is a normalization used by\n            nitime where the max value of the output of the convolution remains\n            the same even if you change the sigma_f.\n        zero_mean : bool\n            make sure that the wavelet has zero mean (only relevant if ratio\n            < 5)\n\n    For method 'spectrogram' or 'stft', the following options should be specified:\n        duraton : int\n            duration of the window to compute the power spectrum, in s\n        overlap : int\n            amount of overlap (0 -> no overlap, 1 -> full overlap)\n    \"\"\"\n    implemented_methods = ('morlet',\n                           'spectrogram',  # this is output spectraldensity\n                           'stft')  # this is output complex\n\n    if method not in implemented_methods:\n        raise ValueError('Method ' + method + ' is not implemented yet.\\n'\n                         'Currently implemented methods are ' +\n                         ', '.join(implemented_methods))\n\n    if method == 'morlet':\n        default_options = {'foi': None,\n                           'ratio': 5,\n                           'sigma_f': None,\n                           'dur_in_sd': 4,\n                           'dur_in_s': None,\n                           'normalization': 'area',\n                           'zero_mean': False,\n                           }\n    elif method in ('spectrogram', 'stft'):\n        default_options = {'duration': 1,\n                           'overlap': 0.5,\n                           'step': None,\n                           'detrend': 'linear',\n                           'taper': 'hann',\n                           'sides': 'one',\n                           'scaling': 'power',\n                           'halfbandwidth': 2,\n                           'NW': None,\n                           }\n\n    default_options.update(options)\n    options = default_options\n\n    timefreq = ChanTimeFreq()\n    timefreq.attr = deepcopy(data.attr)\n    timefreq.s_freq = data.s_freq\n    timefreq.start_time = data.start_time\n    timefreq.axis['chan'] = data.axis['chan']\n    timefreq.axis['time'] = empty(data.number_of('trial'), dtype='O')\n    timefreq.axis['freq'] = empty(data.number_of('trial'), dtype='O')\n    if method == 'stft':\n        timefreq.axis['taper'] = empty(data.number_of('trial'), dtype='O')\n    timefreq.data = empty(data.number_of('trial'), dtype='O')\n\n    if method == 'morlet':\n\n        wavelets = _create_morlet(deepcopy(options), data.s_freq)\n\n        for i in range(data.number_of('trial')):\n            lg.info('Processing trial # {0: 6}'.format(i))\n            timefreq.axis['freq'][i] = array(options['foi'])\n            timefreq.axis['time'][i] = data.axis['time'][i][::time_skip]\n\n            timefreq.data[i] = empty((data.number_of('chan')[i],\n                                      data.number_of('time')[i] // time_skip,\n                                      len(options['foi'])),\n                                     dtype='complex')\n            for i_c, chan in enumerate(data.axis['chan'][i]):\n                dat = data(trial=i, chan=chan)\n                for i_f, wavelet in enumerate(wavelets):\n                    tf = fftconvolve(dat, wavelet, 'same')\n                    timefreq.data[i][i_c, :, i_f] = tf[::time_skip]\n\n        if time_skip != 1:\n            warn('sampling frequency in s_freq refers to the input data, '\n                 'not to the timefrequency output')\n\n    elif method in ('spectrogram', 'stft'):  # TODO: add timeskip\n        nperseg = int(options['duration'] * data.s_freq)\n        if options['step'] is not None:\n            nstep = int(options['step'] * data.s_freq)\n        else:\n            nstep = nperseg - int(options['overlap'] * nperseg)\n\n        if method == 'spectrogram':\n            output = 'spectraldensity'\n        elif method == 'stft':\n            output = 'complex'\n\n        for i in range(data.number_of('trial')):\n            t = _create_subepochs(data.time[i], nperseg, nstep).mean(axis=1)\n            x = _create_subepochs(data(trial=i), nperseg, nstep)\n\n            f, Sxx = _fft(x,\n                          s_freq=data.s_freq,\n                          detrend=options['detrend'],\n                          taper=options['taper'],\n                          output=output,\n                          sides=options['sides'],\n                          scaling=options['scaling'],\n                          halfbandwidth=options['halfbandwidth'],\n                          NW=options['NW'])\n\n            timefreq.axis['time'][i] = t\n            timefreq.axis['freq'][i] = f\n            if method == 'stft':\n                timefreq.axis['taper'][i] = arange(Sxx.shape[-1])\n            timefreq.data[i] = Sxx\n\n    return timefreq"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef band_power(data, freq, scaling='power', n_fft=None, detrend=None,\n               array_out=False):\n    \"\"\"Compute power or energy acoss a frequency band, and its peak frequency.\n    Power is estimated using the mid-point rectangle rule. Input can be \n    ChanTime or ChanFreq.\n\n    Parameters\n    ----------\n    data : instance of ChanTime or ChanFreq\n        data to be analyzed, one trial only\n    freq : tuple of float\n        Frequencies for band of interest. Power will be integrated across this\n        band, inclusively, and peak frequency determined within it. If a value \n        is None, the band is unbounded in that direction.\n    input_type : str\n        'time' or 'spectrum'\n    scaling : str\n        'power' or 'energy', only used if data is ChanTime\n    n_fft : int\n        length of FFT. if shorter than input signal, signal is truncated; if \n        longer, signal is zero-padded to length\n    array_out : bool\n        if True, will return two arrays instead of two dict.\n\n    Returns\n    -------\n    dict of float, or ndarray\n        keys are channels, values are power or energy\n    dict of float, or ndarray\n        keys are channels, values are respective peak frequency\n    \"\"\"\n    if not array_out:\n        power = {}\n        peakf = {}\n    else:\n        power = zeros((data.number_of('chan')[0], 1))\n        peakf = zeros((data.number_of('chan')[0], 1))\n\n    if isinstance(data, ChanFreq):\n        Sxx = data\n    elif isinstance(data, ChanTime):\n        Sxx = frequency(data, scaling=scaling, n_fft=n_fft, detrend=detrend)\n    else:\n        raise ValueError('Invalid data type')\n    \n    if detrend is None:\n        if 'power' == scaling:\n            detrend = 'linear'\n        elif 'energy' == scaling:\n            detrend = None\n    \n    sf = Sxx.axis['freq'][0]\n    f_res = sf[1] - sf[0] # frequency resolution\n    \n    if freq[0] is not None:\n        idx_f1 = asarray([abs(x - freq[0]) for x in sf]).argmin()\n    else:\n        idx_f1 = 0\n    if freq[1] is not None:\n        idx_f2 = min(asarray([abs(x - freq[1]) for x in sf]).argmin() + 1,\n                     len(sf) - 1) # inclusive, to follow convention\n    else:\n        idx_f2 = len(sf) - 1\n\n    for i, chan in enumerate(Sxx.axis['chan'][0]):\n        s = Sxx(chan=chan)[0]\n        pw = sum(s[idx_f1:idx_f2]) * f_res\n\n        idx_peak = s[idx_f1:idx_f2].argmax()\n        pf = sf[idx_f1:idx_f2][idx_peak]\n        \n        if array_out:\n            power[i, 0] = pw\n            peakf[i, 0] = pf\n        else:\n            power[chan] = pw\n            peakf[chan] = pf\n\n    return power, peakf", "response": "Compute the power or energy acoss a frequency band and its peak frequency."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_morlet(options, s_freq):\n    wavelets = []\n    foi = options.pop('foi')\n    for f in foi:\n        wavelets.append(morlet(f, s_freq, **options))\n\n    return wavelets", "response": "Create a list of morlet wavelets with scipy. signal doing the actual computation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a complex Morlet wavelet.", "response": "def morlet(freq, s_freq, ratio=5, sigma_f=None, dur_in_sd=4, dur_in_s=None,\n           normalization='peak', zero_mean=False):\n    \"\"\"Create a Morlet wavelet.\n\n    Parameters\n    ----------\n    freq : float\n        central frequency of the wavelet\n    s_freq : int\n        sampling frequency\n    ratio : float\n        ratio for a wavelet family ( = freq / sigma_f)\n    sigma_f : float\n        standard deviation of the wavelet in frequency domain\n    dur_in_sd : float\n        duration of the wavelet, given as number of the standard deviation in\n        the time domain, in one side.\n    dur_in_s : float\n        total duration of the wavelet, two-sided (i.e. from start to finish)\n    normalization : str\n        'area' means that energy is normalized to 1, 'peak' means that the peak\n        is set at 1, 'max' is a normalization used by nitime which does not\n        change max value of output when you change sigma_f.\n    zero_mean : bool\n        make sure that the wavelet has zero mean (only relevant if ratio < 5)\n\n    Returns\n    -------\n    ndarray\n        vector containing the complex Morlet wavelets\n\n    Notes\n    -----\n    'ratio' and 'sigma_f' are mutually exclusive. If you use 'sigma_f', the\n    standard deviation stays the same for all the frequency. It's more common\n    to specify a constant ratio for the wavelet family, so that the frequency\n    resolution changes with the frequency of interest.\n\n    'dur_in_sd' and 'dur_in_s' are mutually exclusive. 'dur_in_s' specifies the\n    total duration (from start to finish) of the window. 'dur_in_sd' calculates\n    the total duration as the length in standard deviations in the time domain:\n    dur_in_s = dur_in_sd * 2 * sigma_t, with sigma_t = 1 / (2 * pi * sigma_f)\n    \"\"\"\n    if sigma_f is None:\n        sigma_f = freq / ratio\n    else:\n        ratio = freq / sigma_f\n    sigma_t = 1 / (2 * pi * sigma_f)\n\n    if ratio < 5 and not zero_mean:\n        lg.info('The wavelet won\\'t have zero mean, set zero_mean=True to '\n                'correct it')\n\n    if dur_in_s is None:\n        dur_in_s = sigma_t * dur_in_sd * 2\n\n    t = arange(-dur_in_s / 2, dur_in_s / 2, 1 / s_freq)\n\n    w = exp(1j * 2 * pi * freq * t)\n    if zero_mean:\n        w -= exp(-1 / 2 * ratio ** 2)\n\n    w *= exp(-t ** 2 / (2 * sigma_t ** 2))\n\n    if normalization == 'area':\n        w /= sqrt(sqrt(pi) * sigma_t * s_freq)\n    elif normalization == 'max':\n        w /= 2 * sigma_t * sqrt(2 * pi) / s_freq\n    elif normalization == 'peak':\n        pass\n\n    lg.info('At freq {0: 9.3f}Hz, sigma_f={1: 9.3f}Hz, sigma_t={2: 9.3f}s, '\n            'total duration={3: 9.3f}s'.format(freq, sigma_f, sigma_t,\n                                               dur_in_s))\n    lg.debug('    Real peak={0: 9.3f}, Mean={1: 12.6f}, '\n             'Energy={2: 9.3f}'.format(max(real(w)), mean(w), norm(w) ** 2))\n\n    return w"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fft(x, s_freq, detrend='linear', taper=None, output='spectraldensity',\n         sides='one', scaling='power', halfbandwidth=4, NW=None, n_fft=None):\n    \"\"\"\n    Core function taking care of computing the power spectrum / power spectral\n    density or the complex representation.\n\n    Parameters\n    ----------\n    x : 1d or 2d numpy array\n        input data (fft will be computed on the last dimension)\n    s_freq : int\n        sampling frequency\n    detrend : str\n        None (no detrending), 'constant' (remove mean), 'linear' (remove linear\n        trend)\n    output : str\n        'spectraldensity' (= 'psd' in scipy) or 'complex' (for complex output)\n    sides : str\n        'one' or 'two', where 'two' implies negative frequencies\n    scaling : str\n        'power' (= 'density' in scipy, units: uV ** 2 / Hz),\n        'energy' (= 'spectrum' in scipy, units: uV ** 2),\n        'fieldtrip', 'chronux'\n    taper : str\n        Taper to use, commonly used tapers are 'boxcar', 'hann', 'dpss' (see\n        below)\n    halfbandwidth : int\n        (only if taper='dpss') Half bandwidth (in Hz), frequency smoothing will\n        be from +halfbandwidth to -halfbandwidth\n    NW : int\n        (only if taper='dpss') Normalized half bandwidth\n        (NW = halfbandwidth * dur). Number of DPSS tapers is 2 * NW - 1.\n        If specified, NW takes precedence over halfbandwidth\n    n_fft: int\n        Length of FFT, in samples. If less than input axis, input is cropped.\n        If longer than input axis, input is padded with zeros. If None, FFT\n        length set to axis length.\n\n    Returns\n    -------\n    freqs : 1d ndarray\n        vector with frequencies at which the PSD / ESD / complex fourier was\n        computed\n    result: ndarray\n        PSD / ESD / complex fourier. It has the same number of dim as the input.\n        Frequency transform is computed on the last dimension. If\n        output='complex', there is one additional dimension with the taper(s).\n\n    Notes\n    -----\n    The nomenclature of the frequency-domain analysis is not very consistent\n    across packages / toolboxes. The convention used here is based on `wikipedia`_\n\n    So, you can have the spectral density (called sometimes power spectrum) or\n    a complex output. Conceptually quite different but they can both be computed\n    using the fft algorithm, so we do both here.\n\n    Regarding the spectral density, you can have the power spectral density\n    (PSD) or the energy spectral density (ESD). PSD should be used for\n    stationary signals (gamma activity), while ESD should be used for signals\n    that have a clear beginning and end (spindles). ESD gives the energy over\n    the whole duration of the input window, while PSD is normalized by the\n    window length.\n\n    Parseval's theorem says that the energy of the signal in the time-domain\n    must be equal to the energy in the frequency domain. All the tapers are\n    correct to comply with this theorem (see tests/test_trans_frequency.py for\n    all the examples). Note that packages such as 'FieldTrip' and 'Chronux' do\n    not usually respect this convention (and use some ad-hoc convention).\n    You can use the scaling of these packages to compare the results from those\n    matlab toolboxes, but note that the results probably don't satisty Parseval's\n    theorem.\n\n    Note that scipy.signal is not consistent with these names, but the\n    formulas are the same. Also, scipy (v1.1 at least) does not handle dpss.\n\n    Finally, the complex output has an additional dimension (taper), for each\n    taper (even for the boxcar or hann taper). This is useful for multitaper\n    analysis (DPSS), where it doesn't make sense to average complex results.\n\n    .. _wikipedia:\n        https://en.wikipedia.org/wiki/Spectral_density\n\n    TODO\n    ----\n    Scipy v1.1 can generate dpss tapers. Once scipy v1.1 is available, use\n    that instead of the extern folder.\n    \"\"\"\n    if output == 'complex' and sides == 'one':\n        print('complex always returns both sides')\n        sides = 'two'\n\n    axis = x.ndim - 1\n    n_smp = x.shape[axis]\n\n    if n_fft is None:\n        n_fft = n_smp\n\n    if sides == 'one':\n        freqs = np_fft.rfftfreq(n_fft, 1 / s_freq)\n    elif sides == 'two':\n        freqs = fftpack.fftfreq(n_fft, 1 / s_freq)\n\n    if taper is None:\n        taper = 'boxcar'\n\n    if taper == 'dpss':\n        if NW is None:\n            NW = halfbandwidth * n_smp / s_freq\n        tapers, eig = dpss_windows(n_smp, NW, 2 * NW - 1)\n        if scaling == 'chronux':\n            tapers *= sqrt(s_freq)\n\n    else:\n        if taper == 'hann':\n            tapers = windows.hann(n_smp, sym=False)[None, :]\n        else:\n            # TODO: it'd be nice to use sym=False if possible, but the difference is very small\n            tapers = get_window(taper, n_smp)[None, :]\n\n        if scaling == 'energy':\n            rms = sqrt(mean(tapers ** 2))\n            tapers /= rms * sqrt(n_smp)\n        elif scaling != 'chronux':\n            # idk how chronux treats other windows apart from dpss\n            tapers /= norm(tapers)\n\n    if detrend is not None:\n        x = detrend_func(x, axis=axis, type=detrend)\n    tapered = tapers * x[..., None, :]\n\n    if sides == 'one':\n        result = np_fft.rfft(tapered, n=n_fft)\n    elif sides == 'two':\n        result = fftpack.fft(tapered, n=n_fft)\n\n    if scaling == 'chronux':\n        result /= s_freq\n    elif scaling == 'fieldtrip':\n        result *= sqrt(2 / n_smp)\n\n    if output == 'spectraldensity':\n        result = (result.conj() * result)\n    elif output == 'csd':\n        result = (result[None, 0, ...].conj() * result[None, 1, ...])\n\n    if (sides == 'one' and output in ('spectraldensity', 'csd') and \\\n        scaling != 'chronux'):\n        if n_fft % 2:\n            result[..., 1:] *= 2\n        else:\n            # Last point is unpaired Nyquist freq point, don't double\n            result[..., 1:-1] *= 2\n\n    if scaling == 'power':\n        scale = 1.0 / s_freq\n    elif scaling == 'energy':\n        scale = 1.0 / n_smp\n    else:\n        scale = 1\n    if output == 'complex' and scaling in ('power', 'energy'):\n        scale = sqrt(scale)\n    result *= scale\n\n    if scaling == 'fieldtrip' and output in ('spectraldensity', 'csd'):\n        # fieldtrip uses only one side\n        result /= 2\n\n    if output in ('spectraldensity', 'csd'):\n        if output == 'spectraldensity':\n            result = result.real\n        result = mean(result, axis=axis)\n    elif output == 'complex':\n        # dpss should be last dimension in complex, no mean\n        result = swapaxes(result, axis, -1)\n\n    return freqs, result", "response": "Compute the FFT of a single object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_widgets(self):\n        self.bbox = QDialogButtonBox(\n                QDialogButtonBox.Ok | QDialogButtonBox.Cancel)\n        self.idx_ok = self.bbox.button(QDialogButtonBox.Ok)\n        self.idx_cancel = self.bbox.button(QDialogButtonBox.Cancel)\n\n        self.idx_group = FormMenu([gr['name'] for gr in self.groups])\n\n        chan_box = QListWidget()\n        self.idx_chan = chan_box\n\n        stage_box = QListWidget()\n        stage_box.addItems(STAGE_NAME)\n        stage_box.setSelectionMode(QAbstractItemView.ExtendedSelection)\n        self.idx_stage = stage_box\n\n        cycle_box = QListWidget()\n        cycle_box.setSelectionMode(QAbstractItemView.ExtendedSelection)\n        self.idx_cycle = cycle_box", "response": "Create widgets for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_groups(self):\n        self.groups = self.parent.channels.groups\n        self.idx_group.clear()\n        for gr in self.groups:\n            self.idx_group.addItem(gr['name'])\n\n        self.update_channels()", "response": "Update the channel groups list when dialog is opened."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_channels(self):\n        group_dict = {k['name']: i for i, k in enumerate(self.groups)}\n        group_index = group_dict[self.idx_group.currentText()]\n        self.one_grp = self.groups[group_index]\n\n        self.idx_chan.clear()\n\n        self.idx_chan.setSelectionMode(QAbstractItemView.ExtendedSelection)\n        for chan in self.one_grp['chan_to_plot']:\n            name = chan + '\u2014(' + '+'.join(self.one_grp['ref_chan']) + ')'\n            item = QListWidgetItem(name)\n            self.idx_chan.addItem(item)", "response": "Update the channels list when a new group is selected."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables cycles checkbox only if there are cycles marked with no errors.", "response": "def update_cycles(self):\n        \"\"\"Enable cycles checkbox only if there are cycles marked, with no\n        errors.\"\"\"\n        self.idx_cycle.clear()\n\n        try:\n            self.cycles = self.parent.notes.annot.get_cycles()\n\n        except ValueError as err:\n            self.idx_cycle.setEnabled(False)\n            msg = 'There is a problem with the cycle markers: ' + str(err)\n            self.parent.statusBar().showMessage(msg)\n\n        else:\n            if self.cycles is None:\n                self.idx_cycle.setEnabled(False)\n            else:\n                self.idx_cycle.setEnabled(True)\n                for i in range(len(self.cycles)):\n                    self.idx_cycle.addItem(str(i+1))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_channels(self):\n        selectedItems = self.idx_chan.selectedItems()\n        selected_chan = [x.text().split('\u2014')[0] for x in selectedItems]\n        chan_in_order = []\n        for chan in self.one_grp['chan_to_plot']:\n            if chan in selected_chan:\n                chan_in_order.append(chan)\n\n        return chan_in_order", "response": "Get the selected channel names in order."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the selected cycles in order.", "response": "def get_cycles(self):\n        \"\"\"Get the selected cycle(s in order).\n\n        Returns\n        -------\n        list of tuple\n            Each tuple is (start time (sec), end time (sec), index (starting\n            at 1).\"\"\"\n        idx_cyc_sel = [\n                int(x.text()) - 1 for x in self.idx_cycle.selectedItems()]\n        if not idx_cyc_sel:\n            cycle = None\n        else:\n            cycle = itemgetter(*idx_cyc_sel)(self.cycles)\n            if len(idx_cyc_sel) == 1:\n                cycle = [cycle]\n\n        return cycle"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_data_to_plot(data, chan_groups):\n    # chan_to_plot only gives the number of channels to plot, for prealloc\n    chan_to_plot = [one_chan for one_grp in chan_groups\n                    for one_chan in one_grp['chan_to_plot']]\n\n    output = ChanTime()\n    output.s_freq = data.s_freq\n    output.start_time = data.start_time\n    output.axis['time'] = data.axis['time']\n    output.axis['chan'] = empty(1, dtype='O')\n    output.data = empty(1, dtype='O')\n    output.data[0] = empty((len(chan_to_plot), data.number_of('time')[0]),\n                           dtype='f')\n\n    all_chan_grp_name = []\n    i_ch = 0\n    for one_grp in chan_groups:\n\n        sel_data = _select_channels(data,\n                                    one_grp['chan_to_plot'] +\n                                    one_grp['ref_chan'])\n        data1 = montage(sel_data, ref_chan=one_grp['ref_chan'])\n\n        data1.data[0] = nan_to_num(data1.data[0])\n\n        if one_grp['hp'] is not None:\n            data1 = filter_(data1, low_cut=one_grp['hp'])\n\n        if one_grp['lp'] is not None:\n            data1 = filter_(data1, high_cut=one_grp['lp'])\n\n        for chan in one_grp['chan_to_plot']:\n            chan_grp_name = chan + ' (' + one_grp['name'] + ')'\n            all_chan_grp_name.append(chan_grp_name)\n\n            dat = data1(chan=chan, trial=0)\n            dat = dat - nanmean(dat)\n            output.data[0][i_ch, :] = dat * one_grp['scale']\n            i_ch += 1\n\n    output.axis['chan'][0] = asarray(all_chan_grp_name, dtype='U')\n\n    return output", "response": "Create data after montage and filtering."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert_timestr_to_seconds(time_str, rec_start):\n    if not CHECK_TIME_STR.match(time_str):\n        raise ValueError('Input can only contain digits and colons')\n\n    if ':' in time_str:\n        time_split = [int(x) for x in time_str.split(':')]\n\n        # if it's in 'HH:MM' format, add ':SS'\n        if len(time_split) == 2:\n            time_split.append(0)\n        clock_time = time(*time_split)\n\n        chosen_start = datetime.combine(rec_start.date(), clock_time)\n        # if the clock time is after start of the recordings, assume it's the next day\n        if clock_time < rec_start.time():\n            chosen_start += timedelta(days=1)\n\n        window_start = int((chosen_start - rec_start).total_seconds())\n    else:\n        window_start = int(time_str)\n\n    return window_start", "response": "Convert input from user about time string to an absolute time for the recordings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating actions associated with this widget.", "response": "def create_action(self):\n        \"\"\"Create actions associated with this widget.\"\"\"\n        actions = {}\n\n        act = QAction(QIcon(ICON['step_prev']), 'Previous Step', self)\n        act.setShortcut('[')\n        act.triggered.connect(self.step_prev)\n        actions['step_prev'] = act\n\n        act = QAction(QIcon(ICON['step_next']), 'Next Step', self)\n        act.setShortcut(']')\n        act.triggered.connect(self.step_next)\n        actions['step_next'] = act\n\n        act = QAction(QIcon(ICON['page_prev']), 'Previous Page', self)\n        act.setShortcut(QKeySequence.MoveToPreviousChar)\n        act.triggered.connect(self.page_prev)\n        actions['page_prev'] = act\n\n        act = QAction(QIcon(ICON['page_next']), 'Next Page', self)\n        act.setShortcut(QKeySequence.MoveToNextChar)\n        act.triggered.connect(self.page_next)\n        actions['page_next'] = act\n\n        act = QAction('Go to Epoch', self)\n        act.setShortcut(QKeySequence.FindNext)\n        act.triggered.connect(self.go_to_epoch)\n        actions['go_to_epoch'] = act\n\n        act = QAction('Line Up with Epoch', self)\n        act.setShortcut('F4')\n        act.triggered.connect(self.line_up_with_epoch)\n        actions['line_up_with_epoch'] = act\n\n        act = QAction(QIcon(ICON['zoomprev']), 'Wider Time Window', self)\n        act.setShortcut(QKeySequence.ZoomIn)\n        act.triggered.connect(self.X_more)\n        actions['X_more'] = act\n\n        act = QAction(QIcon(ICON['zoomnext']), 'Narrower Time Window', self)\n        act.setShortcut(QKeySequence.ZoomOut)\n        act.triggered.connect(self.X_less)\n        actions['X_less'] = act\n\n        act = QAction(QIcon(ICON['zoomin']), 'Larger Scaling', self)\n        act.setShortcut(QKeySequence.MoveToPreviousLine)\n        act.triggered.connect(self.Y_more)\n        actions['Y_less'] = act\n\n        act = QAction(QIcon(ICON['zoomout']), 'Smaller Scaling', self)\n        act.setShortcut(QKeySequence.MoveToNextLine)\n        act.triggered.connect(self.Y_less)\n        actions['Y_more'] = act\n\n        act = QAction(QIcon(ICON['ydist_more']), 'Larger Y Distance', self)\n        act.triggered.connect(self.Y_wider)\n        actions['Y_wider'] = act\n\n        act = QAction(QIcon(ICON['ydist_less']), 'Smaller Y Distance', self)\n        act.triggered.connect(self.Y_tighter)\n        actions['Y_tighter'] = act\n\n        act = QAction(QIcon(ICON['chronometer']), '6 Hours Earlier', self)\n        act.triggered.connect(partial(self.add_time, -6 * 60 * 60))\n        actions['addtime_-6h'] = act\n\n        act = QAction(QIcon(ICON['chronometer']), '1 Hour Earlier', self)\n        act.triggered.connect(partial(self.add_time, -60 * 60))\n        actions['addtime_-1h'] = act\n\n        act = QAction(QIcon(ICON['chronometer']), '10 Minutes Earlier', self)\n        act.triggered.connect(partial(self.add_time, -10 * 60))\n        actions['addtime_-10min'] = act\n\n        act = QAction(QIcon(ICON['chronometer']), '10 Minutes Later', self)\n        act.triggered.connect(partial(self.add_time, 10 * 60))\n        actions['addtime_10min'] = act\n\n        act = QAction(QIcon(ICON['chronometer']), '1 Hour Later', self)\n        act.triggered.connect(partial(self.add_time, 60 * 60))\n        actions['addtime_1h'] = act\n\n        act = QAction(QIcon(ICON['chronometer']), '6 Hours Later', self)\n        act.triggered.connect(partial(self.add_time, 6 * 60 * 60))\n        actions['addtime_6h'] = act\n\n        act = QAction('Go to Next Event', self)\n        act.setShortcut('s')\n        act.triggered.connect(self.next_event)\n        actions['next_event'] = act\n        \n        act = QAction('Delete Event and Go to Next', self)\n        act.setShortcut('d')\n        act.triggered.connect(partial(self.next_event, True))\n        actions['del_and_next_event'] = act\n        \n        act = QAction('Next Event of Same Type', self)\n        act.setCheckable(True)\n        act.setChecked(True)\n        actions['next_of_same_type'] = act\n        \n        act = QAction('Change Event Type', self)\n        act.setShortcut('e')\n        act.triggered.connect(self.change_event_type)\n        actions['change_event_type'] = act\n        \n        act = QAction('Centre Window Around Event', self)\n        act.setCheckable(True)\n        act.setChecked(True)\n        actions['centre_event'] = act\n        \n        act = QAction('Full-length Markers', self)\n        act.setCheckable(True)\n        act.setChecked(True)\n        act.triggered.connect(self.display_annotations)\n        actions['cross_chan_mrk'] = act\n\n        # Misc\n        act = QAction('Export to svg...', self)\n        act.triggered.connect(partial(export_graphics, MAIN=self.parent))\n        actions['export_svg'] = act\n\n        self.action = actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the data to plot.", "response": "def read_data(self):\n        \"\"\"Read the data to plot.\"\"\"\n        window_start = self.parent.value('window_start')\n        window_end = window_start + self.parent.value('window_length')\n        dataset = self.parent.info.dataset\n        groups = self.parent.channels.groups\n\n        chan_to_read = []\n        for one_grp in groups:\n            chan_to_read.extend(one_grp['chan_to_plot'] + one_grp['ref_chan'])\n\n        if not chan_to_read:\n            return\n        data = dataset.read_data(chan=chan_to_read,\n                                 begtime=window_start,\n                                 endtime=window_end)\n\n        max_s_freq = self.parent.value('max_s_freq')\n        if data.s_freq > max_s_freq:\n            q = int(data.s_freq / max_s_freq)\n            lg.debug('Decimate (no low-pass filter) at ' + str(q))\n\n            data.data[0] = data.data[0][:, slice(None, None, q)]\n            data.axis['time'][0] = data.axis['time'][0][slice(None, None, q)]\n            data.s_freq = int(data.s_freq / q)\n\n        self.data = _create_data_to_plot(data, self.parent.channels.groups)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_chan_labels(self):\n        self.idx_label = []\n        for one_grp in self.parent.channels.groups:\n            for one_label in one_grp['chan_to_plot']:\n                item = QGraphicsSimpleTextItem(one_label)\n                item.setBrush(QBrush(QColor(one_grp['color'])))\n                item.setFlag(QGraphicsItem.ItemIgnoresTransformations)\n                self.idx_label.append(item)", "response": "Create the channel labels but don t plot them yet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the time labels but don t plot them yet.", "response": "def create_time_labels(self):\n        \"\"\"Create the time labels, but don't plot them yet.\n\n        Notes\n        -----\n        It's necessary to have the height of the time labels, so that we can\n        adjust the main scene.\n\n        Not very robust, because it uses seconds as integers.\n        \"\"\"\n        min_time = int(floor(min(self.data.axis['time'][0])))\n        max_time = int(ceil(max(self.data.axis['time'][0])))\n        n_time_labels = self.parent.value('n_time_labels')\n\n        self.idx_time = []\n        self.time_pos = []\n        for one_time in linspace(min_time, max_time, n_time_labels):\n            x_label = (self.data.start_time +\n                       timedelta(seconds=one_time)).strftime('%H:%M:%S')\n            item = QGraphicsSimpleTextItem(x_label)\n            item.setFlag(QGraphicsItem.ItemIgnoresTransformations)\n            self.idx_time.append(item)\n            self.time_pos.append(QPointF(one_time,\n                                         len(self.idx_label) *\n                                         self.parent.value('y_distance')))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_chan_labels(self):\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        label_width = window_length * self.parent.value('label_ratio')\n\n        for row, one_label_item in enumerate(self.idx_label):\n            self.scene.addItem(one_label_item)\n            one_label_item.setPos(window_start - label_width,\n                                  self.parent.value('y_distance') * row +\n                                  self.parent.value('y_distance') / 2)", "response": "Add channel labels on the left."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd time labels at the bottom.", "response": "def add_time_labels(self):\n        \"\"\"Add time labels at the bottom.\"\"\"\n        for text, pos in zip(self.idx_time, self.time_pos):\n            self.scene.addItem(text)\n            text.setPos(pos)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_traces(self):\n        y_distance = self.parent.value('y_distance')\n        self.chan = []\n        self.chan_pos = []\n        self.chan_scale = []\n\n        row = 0\n        for one_grp in self.parent.channels.groups:\n            for one_chan in one_grp['chan_to_plot']:\n\n                # channel name\n                chan_name = one_chan + ' (' + one_grp['name'] + ')'\n\n                # trace\n                dat = (self.data(trial=0, chan=chan_name) *\n                       self.parent.value('y_scale'))\n                dat *= -1  # flip data, upside down\n                path = self.scene.addPath(Path(self.data.axis['time'][0],\n                                               dat))\n                path.setPen(QPen(QColor(one_grp['color']), LINE_WIDTH))\n\n                # adjust position\n                chan_pos = y_distance * row + y_distance / 2\n                path.setPos(0, chan_pos)\n                row += 1\n\n                self.chan.append(chan_name)\n                self.chan_scale.append(one_grp['scale'])\n                self.chan_pos.append(chan_pos)", "response": "Add traces based on self. data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays grid on x - axis and y - axis.", "response": "def display_grid(self):\n        \"\"\"Display grid on x-axis and y-axis.\"\"\"\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        window_end = window_start + window_length\n\n        if self.parent.value('grid_x'):\n            x_tick = self.parent.value('grid_xtick')\n            x_ticks = arange(window_start, window_end + x_tick, x_tick)\n            for x in x_ticks:\n                x_pos = [x, x]\n                y_pos = [0,\n                         self.parent.value('y_distance') * len(self.idx_label)]\n                path = self.scene.addPath(Path(x_pos, y_pos))\n                path.setPen(QPen(QColor(LINE_COLOR), LINE_WIDTH,\n                                 Qt.DotLine))\n\n        if self.parent.value('grid_y'):\n            y_tick = (self.parent.value('grid_ytick') *\n                      self.parent.value('y_scale'))\n            for one_label_item in self.idx_label:\n                x_pos = [window_start, window_end]\n                y = one_label_item.y()\n\n                y_pos_0 = [y, y]\n                path_0 = self.scene.addPath(Path(x_pos, y_pos_0))\n                path_0.setPen(QPen(QColor(LINE_COLOR), LINE_WIDTH,\n                                   Qt.DotLine))\n\n                y_up = one_label_item.y() + y_tick\n                y_pos_up = [y_up, y_up]\n                path_up = self.scene.addPath(Path(x_pos, y_pos_up))\n                path_up.setPen(QPen(QColor(LINE_COLOR), LINE_WIDTH,\n                                    Qt.DotLine))\n\n                y_down = one_label_item.y() - y_tick\n                y_pos_down = [y_down, y_down]\n                path_down = self.scene.addPath(Path(x_pos, y_pos_down))\n                path_down.setPen(QPen(QColor(LINE_COLOR), LINE_WIDTH,\n                                      Qt.DotLine))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding markers on top of first plot.", "response": "def display_markers(self):\n        \"\"\"Add markers on top of first plot.\"\"\"\n        for item in self.idx_markers:\n            self.scene.removeItem(item)\n        self.idx_markers = []\n\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        window_end = window_start + window_length\n        y_distance = self.parent.value('y_distance')\n\n        markers = []\n        if self.parent.info.markers is not None:\n            if self.parent.value('marker_show'):\n                markers = self.parent.info.markers\n\n        for mrk in markers:\n            if window_start <= mrk['end'] and window_end >= mrk['start']:\n\n                mrk_start = max((mrk['start'], window_start))\n                mrk_end = min((mrk['end'], window_end))\n                color = QColor(self.parent.value('marker_color'))\n\n                item = QGraphicsRectItem(mrk_start, 0,\n                                         mrk_end - mrk_start,\n                                         len(self.idx_label) * y_distance)\n                item.setPen(color)\n                item.setBrush(color)\n                item.setZValue(-9)\n                self.scene.addItem(item)\n\n                item = TextItem_with_BG(color.darker(200))\n                item.setText(mrk['name'])\n                item.setPos(mrk['start'],\n                            len(self.idx_label) *\n                            self.parent.value('y_distance'))\n                item.setFlag(QGraphicsItem.ItemIgnoresTransformations)\n                item.setRotation(-90)\n                self.scene.addItem(item)\n                self.idx_markers.append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay all the annotations on top of first plot.", "response": "def display_annotations(self):\n        \"\"\"Mark all the bookmarks/events, on top of first plot.\"\"\"\n        for item in self.idx_annot:\n            self.scene.removeItem(item)\n        self.idx_annot = []\n        for item in self.idx_annot_labels:\n            self.scene.removeItem(item)\n        self.idx_annot_labels = []\n        self.highlight = None\n\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        window_end = window_start + window_length\n        y_distance = self.parent.value('y_distance')\n\n        bookmarks = []\n        events = []\n\n        if self.parent.notes.annot is not None:\n            if self.parent.value('annot_show'):\n                bookmarks = self.parent.notes.annot.get_bookmarks()\n                events = self.parent.notes.get_selected_events((window_start,\n                                                                window_end))\n        annotations = bookmarks + events\n\n        for annot in annotations:\n\n            if window_start <= annot['end'] and window_end >= annot['start']:\n\n                mrk_start = max((annot['start'], window_start))\n                mrk_end = min((annot['end'], window_end))\n                if annot in bookmarks:\n                    color = QColor(self.parent.value('annot_bookmark_color'))\n                if annot in events:\n                    color = convert_name_to_color(annot['name'])\n\n                if logical_or(annot['chan'] == [''],\n                              self.action['cross_chan_mrk'].isChecked()):\n                    h_annot = len(self.idx_label) * y_distance\n\n                    item = TextItem_with_BG(color.darker(200))\n                    item.setText(annot['name'])\n                    item.setPos(annot['start'],\n                                len(self.idx_label) * y_distance)\n                    item.setFlag(QGraphicsItem.ItemIgnoresTransformations)\n                    item.setRotation(-90)\n                    self.scene.addItem(item)\n                    self.idx_annot_labels.append(item)\n                    mrk_dur = amax((mrk_end - mrk_start, \n                                  self.parent.value('min_marker_display_dur')))\n\n                    item = RectMarker(mrk_start, 0, mrk_dur,\n                                      h_annot, zvalue=-8,\n                                      color=color.lighter(120))\n\n                    self.scene.addItem(item)\n                    self.idx_annot.append(item)\n\n                if annot['chan'] != ['']:\n                    # find indices of channels with annotations\n                    chan_idx_in_mrk = in1d(self.chan, annot['chan'])\n                    y_annot = asarray(self.chan_pos)[chan_idx_in_mrk]\n                    y_annot -= y_distance / 2\n                    mrk_dur = amax((mrk_end - mrk_start, \n                                  self.parent.value('min_marker_display_dur')))\n\n                    for y in y_annot:\n                        item = RectMarker(mrk_start, y, mrk_dur,\n                                          y_distance, zvalue=-7, color=color)\n                        self.scene.addItem(item)\n                        self.idx_annot.append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngoing to the previous step.", "response": "def step_prev(self):\n        \"\"\"Go to the previous step.\"\"\"\n        window_start = around(self.parent.value('window_start') -\n                              self.parent.value('window_length') /\n                              self.parent.value('window_step'), 2)\n        if window_start < 0:\n            return\n        self.parent.overview.update_position(window_start)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngoes to the next step.", "response": "def step_next(self):\n        \"\"\"Go to the next step.\"\"\"\n        window_start = around(self.parent.value('window_start') +\n                              self.parent.value('window_length') /\n                              self.parent.value('window_step'), 2)\n\n        self.parent.overview.update_position(window_start)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngoes to the previous page.", "response": "def page_prev(self):\n        \"\"\"Go to the previous page.\"\"\"\n        window_start = (self.parent.value('window_start') -\n                        self.parent.value('window_length'))\n        if window_start < 0:\n            return\n        self.parent.overview.update_position(window_start)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef page_next(self):\n        window_start = (self.parent.value('window_start') +\n                        self.parent.value('window_length'))\n        self.parent.overview.update_position(window_start)", "response": "Go to the next page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngo to any window", "response": "def go_to_epoch(self, checked=False, test_text_str=None):\n        \"\"\"Go to any window\"\"\"\n        if test_text_str is not None:\n            time_str = test_text_str\n            ok = True\n        else:\n            time_str, ok = QInputDialog.getText(self,\n                                                'Go To Epoch',\n                                                'Enter start time of the '\n                                                'epoch,\\nin seconds (\"1560\") '\n                                                'or\\nas absolute time '\n                                                '(\"22:30\")')\n\n        if not ok:\n            return\n\n        try:\n            rec_start_time = self.parent.info.dataset.header['start_time']\n            window_start = _convert_timestr_to_seconds(time_str, rec_start_time)\n        except ValueError as err:\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error moving to epoch')\n            error_dialog.showMessage(str(err))\n            if test_text_str is None:\n                error_dialog.exec()\n            self.parent.statusBar().showMessage(str(err))\n            return\n\n        self.parent.overview.update_position(window_start)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngoing to the start of the present epoch.", "response": "def line_up_with_epoch(self):\n        \"\"\"Go to the start of the present epoch.\"\"\"\n        if self.parent.notes.annot is None:  # TODO: remove if buttons are disabled\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error moving to epoch')\n            error_dialog.showMessage('No score file loaded')\n            error_dialog.exec()\n            return\n\n        new_window_start = self.parent.notes.annot.get_epoch_start(\n            self.parent.value('window_start'))\n\n        self.parent.overview.update_position(new_window_start)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_time(self, extra_time):\n        window_start = self.parent.value('window_start') + extra_time\n        self.parent.overview.update_position(window_start)", "response": "Go to the predefined time forward."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nzoom in on the x - axis.", "response": "def X_more(self):\n        \"\"\"Zoom in on the x-axis.\"\"\"\n        if self.parent.value('window_length') < 0.3:\n            return\n        self.parent.value('window_length',\n                          self.parent.value('window_length') * 2)\n        self.parent.overview.update_position()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nzooming out on the x - axis.", "response": "def X_less(self):\n        \"\"\"Zoom out on the x-axis.\"\"\"\n        self.parent.value('window_length',\n                          self.parent.value('window_length') / 2)\n        self.parent.overview.update_position()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef X_length(self, new_window_length):\n        self.parent.value('window_length', new_window_length)\n        self.parent.overview.update_position()", "response": "Use presets for length of the window."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Y_ampl(self, new_y_scale):\n        self.parent.value('y_scale', new_y_scale)\n        self.parent.traces.display()", "response": "Make scaling on Y axis using predefined values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Y_wider(self):\n        self.parent.value('y_distance', self.parent.value('y_distance') * 1.4)\n        self.parent.traces.display()", "response": "Increase the distance of the lines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecrease the distance of the lines.", "response": "def Y_tighter(self):\n        \"\"\"Decrease the distance of the lines.\"\"\"\n        self.parent.value('y_distance', self.parent.value('y_distance') / 1.4)\n        self.parent.traces.display()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the y_distance of the log file.", "response": "def Y_dist(self, new_y_distance):\n        \"\"\"Use preset values for the distance between lines.\"\"\"\n        self.parent.value('y_distance', new_y_distance)\n        self.parent.traces.display()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a marker or start selection based on the event.", "response": "def mousePressEvent(self, event):\n        \"\"\"Create a marker or start selection\n\n        Parameters\n        ----------\n        event : instance of QtCore.QEvent\n            it contains the position that was clicked.\n        \"\"\"\n        if not self.scene:\n            return            \n\n        if self.event_sel or self.current_event:\n            self.parent.notes.idx_eventtype.setCurrentText(self.current_etype)\n            self.current_etype = None\n            self.current_event = None\n            self.deselect = True\n            self.event_sel = None\n            self.current_event_row = None\n            self.scene.removeItem(self.highlight)\n            self.highlight = None\n            self.parent.statusBar().showMessage('')\n            return        \n\n        self.ready = False\n        self.event_sel = None\n\n        xy_scene = self.mapToScene(event.pos())\n        chan_idx = argmin(abs(asarray(self.chan_pos) - xy_scene.y()))\n        self.sel_chan = chan_idx\n        self.sel_xy = (xy_scene.x(), xy_scene.y())\n\n        chk_marker = self.parent.notes.action['new_bookmark'].isChecked()\n        chk_event = self.parent.notes.action['new_event'].isChecked()\n\n        if not (chk_marker or chk_event):\n            channame = self.chan[self.sel_chan] + ' in selected window'\n            self.parent.spectrum.show_channame(channame)\n\n        # Make annotations clickable\n        else:\n            for annot in self.idx_annot:\n                if annot.contains(xy_scene):\n                    self.highlight_event(annot)\n                    if chk_event:\n                        row = self.parent.notes.find_row(annot.marker.x(),\n                                    annot.marker.x() + annot.marker.width())\n                        self.parent.notes.idx_annot_list.setCurrentCell(row, 0)\n                    break\n\n        self.ready = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mouseReleaseEvent(self, event):\n        if not self.scene:\n            return\n\n        if self.event_sel:\n            return\n\n        if self.deselect:\n            self.deselect = False\n            return\n\n        if not self.ready:\n            return\n\n        chk_marker = self.parent.notes.action['new_bookmark'].isChecked()\n        chk_event = self.parent.notes.action['new_event'].isChecked()\n        y_distance = self.parent.value('y_distance')\n\n        if chk_marker or chk_event:\n\n            x_in_scene = self.mapToScene(event.pos()).x()\n            y_in_scene = self.mapToScene(event.pos()).y()\n\n            # it can happen that selection is empty (f.e. double-click)\n            if self.sel_xy[0] is not None:\n                # max resolution = sampling frequency\n                # in case there is no data\n                s_freq = self.parent.info.dataset.header['s_freq']\n                at_s_freq = lambda x: round(x * s_freq) / s_freq\n                start = at_s_freq(self.sel_xy[0])\n                end = at_s_freq(x_in_scene)\n\n                if abs(end - start) < self.parent.value('min_marker_dur'):\n                    end = start\n\n                if start <= end:\n                    time = (start, end)\n                else:\n                    time = (end, start)\n\n                if chk_marker:\n                    self.parent.notes.add_bookmark(time)\n\n                elif chk_event and start != end:\n                    eventtype = self.parent.notes.idx_eventtype.currentText()\n                    \n                    # if dragged across > 1.5 chan, event is marked on all chan\n                    if abs(y_in_scene - self.sel_xy[1]) > 1.5 * y_distance:\n                        chan = ''\n                    else:\n                        chan_idx = int(floor(self.sel_xy[1] / y_distance))\n                        chan = self.chan[chan_idx]\n                        \n                    self.parent.notes.add_event(eventtype, time, chan)\n\n        else:  # normal selection\n\n            if self.idx_info in self.scene.items():\n                self.scene.removeItem(self.idx_info)\n            self.idx_info = None\n\n            # restore spectrum\n            self.parent.spectrum.update()\n            self.parent.spectrum.display_window()\n\n        # general garbage collection\n        self.sel_chan = None\n        self.sel_xy = (None, None)\n\n        if self.idx_sel in self.scene.items():\n            self.scene.removeItem(self.idx_sel)\n            self.idx_sel = None", "response": "Create a new event or marker or show the previous power spectrum if the event is dragged across 1. 5 channels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef highlight_event(self, annot):\n        beg = annot.marker.x()\n        end = beg + annot.marker.width()\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        events = self.parent.notes.get_selected_events((window_start, \n                                                window_start + window_length))\n        ev = [x for x in events if (x['start'] == annot.marker.x() or \\\n                                    x['end'] == annot.marker.y())]\n        \n        if ev:\n            annot_name = ev[0]['name']\n            \n            msg = \"Event of type '{}' from {} to {}\".format(\n                    annot_name, beg, end)\n            self.current_etype = self.parent.notes.idx_eventtype.currentText()\n            self.parent.notes.idx_eventtype.setCurrentText(annot_name)\n            self.current_event = ev[0]\n        else:\n            msg = \"Marker from {} to {}\".format(beg, end)\n        self.parent.statusBar().showMessage(msg)\n            \n        highlight = self.highlight = RectMarker(annot.marker.x(),\n                                               annot.marker.y(),\n                                               annot.marker.width(),\n                                               annot.marker.height(),\n                                               zvalue=-5,\n                                               color=QColor(255, 255, 51))\n        self.scene.addItem(highlight)\n        self.event_sel = annot", "response": "Highlight an event on the trace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef next_event(self, delete=False):\n        if delete:\n            msg = \"Delete this event? This cannot be undone.\"\n            msgbox = QMessageBox(QMessageBox.Question, 'Delete event', msg)\n            msgbox.setStandardButtons(QMessageBox.Yes | QMessageBox.No)\n            msgbox.setDefaultButton(QMessageBox.Yes)\n            response = msgbox.exec_()\n            if response == QMessageBox.No:\n                return\n        \n        event_sel = self.event_sel\n        if event_sel is None:\n            return\n        \n        notes = self.parent.notes\n        \n        if not self.current_event_row:\n            row = notes.find_row(event_sel.marker.x(),\n                            event_sel.marker.x() + event_sel.marker.width())\n        else:\n            row = self.current_event_row\n            \n        same_type = self.action['next_of_same_type'].isChecked()\n        if same_type:\n            target = notes.idx_annot_list.item(row, 2).text()\n        \n        if delete:            \n            notes.delete_row()\n            msg = 'Deleted event from {} to {}.'.format(event_sel.marker.x(), \n                            event_sel.marker.x() + event_sel.marker.width())\n            self.parent.statusBar().showMessage(msg)\n            row -= 1\n        \n        if row + 1 == notes.idx_annot_list.rowCount():\n            return\n        \n        if not same_type:\n            next_row = row + 1\n        else:\n            next_row = None\n            types = notes.idx_annot_list.property('name')[row + 1:]\n            \n            for i, ty in enumerate(types):\n                if ty == target:\n                    next_row = row + 1 + i\n                    break\n                    \n            if next_row is None:\n                return                \n                    \n        self.current_event_row = next_row\n        notes.go_to_marker(next_row, 0, 'annot')\n        notes.idx_annot_list.setCurrentCell(next_row, 0)", "response": "Go to next event in notes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nactions : change highlighted event s type by cycling through event type list.", "response": "def change_event_type(self):\n        \"\"\"Action: change highlighted event's type by cycling through event \n        type list.\"\"\"\n        if self.current_event is None:\n            return\n        \n        hl_params = self.highlight.params\n        self.scene.removeItem(self.highlight)\n\n        ev = self.current_event\n        new_name = self.parent.notes.change_event_type(name=ev['name'], \n                                                       time=(ev['start'],\n                                                             ev['end']), \n                                                       chan=ev['chan'])\n        msg = \"Event from {} to {} changed type from '{}' to '{}'\".format(\n                ev['start'], ev['end'], ev['name'], new_name)\n        ev['name'] = new_name\n        \n        self.current_event = ev\n        self.current_etype = new_name\n        #self.event_sel = True\n        self.parent.notes.idx_eventtype.setCurrentText(new_name)\n        self.parent.statusBar().showMessage(msg)\n        self.display_annotations()\n        self.highlight = RectMarker(*hl_params)\n        self.scene.addItem(self.highlight)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport data to EDF file.", "response": "def write_edf(data, filename, subj_id='X X X X', physical_max=1000, \n              physical_min=None):\n    \"\"\"Export data to FieldTrip.\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data with only one trial\n    filename : path to file\n        file to export to (include '.mat')\n    subj_id : str\n        subject id\n    physical_max : int\n        values above this parameter will be considered saturated (and also\n        those that are too negative). This parameter defines the precision.\n\n    Notes\n    -----\n    Data is always recorded as 2 Byte int (which is 'int16'), so precision is\n    limited. You can control the precision with physical_max. To get the\n    precision:\n\n    >>> precision = physical_max / DIGITAL_MAX\n\n    where DIGITAL_MAX is 32767.\n    \"\"\"\n    if data.start_time is None:\n        raise ValueError('Data should contain a valid start_time (as datetime)')\n    start_time = data.start_time + timedelta(seconds=data.axis['time'][0][0])\n\n    if physical_max is None:\n        physical_max = max(abs(data.data[0]))\n\n    precision = physical_max / DIGITAL_MAX\n    lg.info('Data exported to EDF will have precision ' + str(precision))\n\n    if physical_min is None:\n        physical_min = -1 * physical_max\n    \n    dat = data.data[0] / physical_max * DIGITAL_MAX\n    dat = dat.astype(EDF_FORMAT)\n    dat[dat > DIGITAL_MAX] = DIGITAL_MAX\n    dat[dat < DIGITAL_MIN] = DIGITAL_MIN\n\n    with open(filename, 'wb') as f:\n        f.write('{:<8}'.format(0).encode('ascii'))\n        f.write('{:<80}'.format(subj_id).encode('ascii'))  # subject_id\n        f.write('{:<80}'.format('Startdate X X X X').encode('ascii'))\n        f.write(start_time.strftime('%d.%m.%y').encode('ascii'))\n        f.write(start_time.strftime('%H.%M.%S').encode('ascii'))\n\n        n_smp = data.data[0].shape[1]\n        s_freq = int(data.s_freq)\n        n_records = n_smp // s_freq  # floor\n        record_length = 1\n        n_channels = data.number_of('chan')[0]\n\n        header_n_bytes = 256 + 256 * n_channels\n        f.write('{:<8d}'.format(header_n_bytes).encode('ascii'))\n        f.write((' ' * 44).encode('ascii'))  # reserved for EDF+\n\n        f.write('{:<8}'.format(n_records).encode('ascii'))\n        f.write('{:<8d}'.format(record_length).encode('ascii'))\n        f.write('{:<4}'.format(n_channels).encode('ascii'))\n\n        for chan in data.axis['chan'][0]:\n            f.write('{:<16}'.format(chan).encode('ascii'))  # label\n        for _ in range(n_channels):\n            f.write(('{:<80}').format('').encode('ascii'))  # tranducer\n        for _ in range(n_channels):\n            f.write('{:<8}'.format('uV').encode('ascii'))  # physical_dim\n        for _ in range(n_channels):\n            f.write('{:<8}'.format(physical_min).encode('ascii'))\n        for _ in range(n_channels):\n            f.write('{:<8}'.format(physical_max).encode('ascii'))\n        for _ in range(n_channels):\n            f.write('{:<8}'.format(DIGITAL_MIN).encode('ascii'))\n        for _ in range(n_channels):\n            f.write('{:<8}'.format(DIGITAL_MAX).encode('ascii'))\n        for _ in range(n_channels):\n            f.write('{:<80}'.format('').encode('ascii'))  # prefiltering\n        for _ in range(n_channels):\n            f.write('{:<8d}'.format(s_freq).encode('ascii'))  # n_smp in record\n        for _ in range(n_channels):\n            f.write((' ' * 32).encode('ascii'))\n\n        length_record = s_freq * n_channels  # length of one record\n        for i in range(n_records):\n            i0 = i * s_freq\n            i1 = i0 + s_freq\n            x = dat[:, i0:i1].flatten(order='C')  # assumes it's ChanTimeData\n            f.write(pack('<' + 'h' * length_record, *x))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread TAL from rawbytes.", "response": "def _read_tal(rawbytes):\n    \"\"\"Read TAL (Time-stamped Annotations Lists) using regex\n\n    Parameters\n    ----------\n    rawbytes : bytes\n        raw information from file\n\n    Returns\n    -------\n    annotation : list of dict\n        where each dict contains onset, duration, and list with the annotations\n    \"\"\"\n    annotations = []\n\n    for m in finditer(PATTERN, rawbytes):\n        d = m.groupdict()\n        annot = {'onset': float(decode(d['onset'])),\n                 'dur': float(decode(d['duration'])) if d['duration'] else 0.,\n                 'annotation': [decode(a) for a in d['annotation'].split(b'\\x14') if a],\n                 }\n\n        if annot['annotation']:\n            annotations.append(annot)\n\n    return annotations"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_datetime(filename):\n    with Path(filename).open('r+b') as f:\n        f.seek(168)\n        f.write(16 * b' ')", "response": "Remove datetime from file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_hdr(self):\n        with self.filename.open('rb') as f:\n\n            hdr = {}\n            assert f.tell() == 0\n            assert f.read(8) == b'0       '\n\n            # recording info\n            hdr['subject_id'] = decode(f.read(80)).strip()\n            hdr['recording_id'] = decode(f.read(80)).strip()\n\n            # parse timestamp\n            date_str = decode(f.read(8)).strip()\n\n            if date_str == '':\n                edf_date = DEFAULT_DATETIME.date()\n            else:\n                (day, month, year) = [int(x) for x in findall('(\\d+)', date_str)]\n                # Y2K: cutoff is 1985\n                if year >= 85:\n                    year += 1900\n                else:\n                    year += 2000\n                edf_date = date(year, month, day)\n\n            time_str = decode(f.read(8)).strip()\n            if time_str == '':\n                edf_time = DEFAULT_DATETIME.time()\n            else:\n                (hour, minute, day) = [int(x) for x in findall('(\\d+)', time_str)]\n\n                edf_time = time(hour, minute, day)\n\n            hdr['start_time'] = datetime.combine(edf_date, edf_time)\n\n            # misc\n            hdr['header_n_bytes'] = int(f.read(8))\n            f.seek(44, 1)  # reserved for EDF+\n            hdr['n_records'] = int(f.read(8))\n            hdr['record_length'] = float(f.read(8))  # in seconds\n            nchannels = hdr['n_channels'] = int(f.read(4))\n\n            # read channel info\n            channels = range(hdr['n_channels'])\n            hdr['label'] = [decode(f.read(16)).strip() for n in\n                            channels]\n            hdr['transducer'] = [decode(f.read(80)).strip()\n                                 for n in channels]\n            hdr['physical_dim'] = [decode(f.read(8)).strip() for n in\n                                   channels]\n            hdr['physical_min'] = [float(f.read(8)) for n in channels]\n            hdr['physical_max'] = [float(f.read(8)) for n in channels]\n            hdr['digital_min'] = [float(f.read(8)) for n in channels]\n            hdr['digital_max'] = [float(f.read(8)) for n in channels]\n            hdr['prefiltering'] = [decode(f.read(80)).strip()\n                                   for n in channels]\n            hdr['n_samples_per_record'] = [int(f.read(8)) for n in channels]\n            f.seek(32 * nchannels, 1)  # reserved\n\n            assert f.tell() == hdr['header_n_bytes']\n\n            self.hdr = hdr", "response": "Reads the header from the EDF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_hdr(self):\n        try:\n            self.i_annot = self.hdr['label'].index(ANNOT_NAME)\n        except ValueError:\n            self.i_annot = None\n\n        self.smp_in_blk = sum(self.hdr['n_samples_per_record'])\n\n        self.max_smp = max(self.hdr['n_samples_per_record'])\n        n_blocks = self.hdr['n_records']\n        self.blocks = ones(n_blocks, dtype='int') * self.max_smp\n\n        self.dig_min = asarray(self.hdr['digital_min'])\n        self.phys_min = asarray(self.hdr['physical_min'])\n        phys_range = asarray(self.hdr['physical_max']) - self.phys_min\n        dig_range = asarray(self.hdr['digital_max']) - self.dig_min\n        if (phys_range < 0).any():\n            lg.warning('physical_min is higher than physical_max. Check whether the polarity of your recordings is correct')\n        if (dig_range < 0).any():\n            lg.warning('digital_min is higher than digital_max. Check whether the polarity of your recordings is correct')\n        if sum(self.dig_min) == 0:\n            lg.warning('digital_min is zero. Setting to -32767.')\n            self.dig_min = ones(self.dig_min.shape) * -32767.0\n            dig_range = 2 * 32767.0\n        self.gain = phys_range / dig_range\n\n        subj_id = self.hdr['subject_id']\n        start_time = self.hdr['start_time']\n        s_freq = self.max_smp / self.hdr['record_length']\n        chan_name = [label for label in self.hdr['label'] if label != ANNOT_NAME]\n        n_samples = self.max_smp * self.hdr['n_records']\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, self.hdr", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_dat(self, chan, begsam, endsam):\n        assert begsam < endsam\n\n        dat = empty((len(chan), endsam - begsam))\n        dat.fill(NaN)\n\n        with self.filename.open('rb') as f:\n\n            for i_dat, blk, i_blk in _select_blocks(self.blocks, begsam, endsam):\n                dat_in_rec = self._read_record(f, blk, chan)\n                dat[:, i_dat[0]:i_dat[1]] = dat_in_rec[:, i_blk[0]:i_blk[1]]\n\n        # calibration\n        dat = ((dat.astype('float64') - self.dig_min[chan, newaxis]) *\n               self.gain[chan, newaxis] + self.phys_min[chan, newaxis])\n\n        return dat", "response": "Read data from an EDF file and return it as a 2d matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread raw data from a single EDF channel.", "response": "def _read_record(self, f, blk, chans):\n        \"\"\"Read raw data from a single EDF channel.\n\n        Parameters\n        ----------\n        i_chan : int\n            index of the channel to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A vector with the data as written on file, in 16-bit precision\n        \"\"\"\n        dat_in_rec = empty((len(chans), self.max_smp))\n\n        i_ch_in_dat = 0\n        for i_ch in chans:\n            offset, n_smp_per_chan = self._offset(blk, i_ch)\n\n            f.seek(offset)\n            x = fromfile(f, count=n_smp_per_chan, dtype=EDF_FORMAT)\n\n            ratio = int(self.max_smp / n_smp_per_chan)\n            dat_in_rec[i_ch_in_dat, :] = repeat(x, ratio)\n            i_ch_in_dat += 1\n\n        return dat_in_rec"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport data in BrainVision format", "response": "def write_brainvision(data, filename, markers=None):\n    \"\"\"Export data in BrainVision format\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data with only one trial\n    filename : path to file\n        file to export to (use '.vhdr' as extension)\n    \"\"\"\n    filename = Path(filename).resolve().with_suffix('.vhdr')\n    if markers is None:\n        markers = []\n\n    with filename.open('w') as f:\n        f.write(_write_vhdr(data, filename))\n\n    with filename.with_suffix('.vmrk').open('w') as f:\n        f.write(_write_vmrk(data, filename, markers))\n\n    _write_eeg(data, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef return_hdr(self):\n        subj_id = ''  # no subject information in the header\n\n        hdr = _parse_ini(self.filename)\n\n        self.eeg_file = self.filename.parent / hdr['Common Infos']['DataFile']\n        self.vmrk_file = self.filename.parent / hdr['Common Infos']['MarkerFile']\n        self.mrk = _parse_ini(self.vmrk_file)\n\n        start_time = _read_datetime(self.mrk)\n        self.s_freq = 1e6 / float(hdr['Common Infos']['SamplingInterval'])\n        chan_name = [v[0] for v in hdr['Channel Infos'].values()]\n        self.gain = array([float(v[2]) for v in hdr['Channel Infos'].values()])\n\n        # number of samples\n        self.data_type = BV_DATATYPE[hdr['Binary Infos']['BinaryFormat']]\n        N_BYTES = dtype(self.data_type).itemsize\n        n_samples = int(self.eeg_file.stat().st_size / N_BYTES / len(chan_name))\n\n        self.dshape = len(chan_name), int(n_samples)\n        self.data_order = BV_ORIENTATION[hdr['Common Infos']['DataOrientation']]\n\n        orig = {\n            'vhdr': hdr,\n            'vmrk': self.mrk,\n            }\n\n        return subj_id, start_time, self.s_freq, chan_name, n_samples, orig", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the data as 2D numpy. ndarray.", "response": "def return_dat(self, chan, begsam, endsam):\n        \"\"\"Return the data as 2D numpy.ndarray.\n\n        Parameters\n        ----------\n        chan : int or list\n            index (indices) of the channels to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A 2d matrix, with dimension chan X samples\n        \"\"\"\n        dat = _read_memmap(self.eeg_file, self.dshape, begsam, endsam,\n                           self.data_type, self.data_order)\n\n        return dat[chan, :] * self.gain[chan, None]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef return_markers(self):\n        markers = []\n        for v in self.mrk['Marker Infos'].values():\n            if v[0] == 'New Segment':\n                continue\n\n            markers.append({\n                'name': v[1],\n                'start': float(v[2]) / self.s_freq,\n                'end': (float(v[2]) + float(v[3])) / self.s_freq,\n                })\n\n        return markers", "response": "Return all the markers in the recordings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_xyz2surf(surf, xyz, threshold=20, exponent=None, std=None):\n    if exponent is None and std is None:\n        exponent = 1\n\n    if exponent is not None:\n        lg.debug('Vertex values based on inverse-law, with exponent ' +\n                 str(exponent))\n        funct = partial(calc_one_vert_inverse, xyz=xyz, exponent=exponent)\n    elif std is not None:\n        lg.debug('Vertex values based on gaussian, with s.d. ' + str(std))\n        funct = partial(calc_one_vert_gauss, xyz=xyz, std=std)\n\n    with Pool() as p:\n        xyz2surf = p.map(funct, surf.vert)\n\n    xyz2surf = asarray(xyz2surf)\n\n    if exponent is not None:\n        threshold_value = (1 / (threshold ** exponent))\n        external_threshold_value = threshold_value\n    elif std is not None:\n        threshold_value = gauss(threshold, std)\n        external_threshold_value = gauss(std, std) # this is around 0.607\n    lg.debug('Values thresholded at ' + str(threshold_value))\n\n    xyz2surf[xyz2surf < threshold_value] = NaN\n\n    # here we deal with vertices that are within the threshold value but far\n    # from a single electrodes, so those remain empty\n    sumval = nansum(xyz2surf, axis=1)\n    sumval[sumval < external_threshold_value] = NaN\n\n    # normalize by the number of electrodes\n    xyz2surf /= atleast_2d(sumval).T\n    xyz2surf[isnan(xyz2surf)] = 0\n\n    return xyz2surf", "response": "Calculates the transformation matrix from xyz values to vertices."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates how many electrodes influence one vertex using the inverse function.", "response": "def calc_one_vert_inverse(one_vert, xyz=None, exponent=None):\n    \"\"\"Calculate how many electrodes influence one vertex, using the inverse\n    function.\n\n    Parameters\n    ----------\n    one_vert : ndarray\n        vector of xyz position of a vertex\n    xyz : ndarray\n        nChan X 3 with the position of all the channels\n    exponent : int\n        inverse law (1-> direct inverse, 2-> inverse square, 3-> inverse cube)\n\n    Returns\n    -------\n    ndarray\n        one vector with values for one vertex\n    \"\"\"\n    trans = empty(xyz.shape[0])\n    for i, one_xyz in enumerate(xyz):\n        trans[i] = 1 / (norm(one_vert - one_xyz) ** exponent)\n    return trans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_one_vert_gauss(one_vert, xyz=None, std=None):\n    trans = empty(xyz.shape[0])\n    for i, one_xyz in enumerate(xyz):\n        trans[i] = gauss(norm(one_vert - one_xyz), std)\n    return trans", "response": "Calculates how many electrodes influence one vertex using a Gaussian\n    function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_hdr(self):\n\n        subj_id = self._header['name'] + ' ' + self._header['surname']\n        chan_name = [ch['chan_name'] for ch in self._header['chans']]\n\n        return subj_id, self._header['start_time'], self._header['s_freq'], chan_name, self._n_smp, self._header", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef return_dat(self, chan, begsam, endsam):\n\n        if type(chan) == int:  # if single value is provided it needs to be transformed to list to generate a 2d matrix\n            chan = [chan, ]\n\n        if (begsam >= self._n_smp) or (endsam < 0):\n            dat = empty((len(chan), endsam - begsam))\n            dat.fill(NaN)\n            return dat\n\n        if begsam < 0:\n            begpad = -1 * begsam\n            begsam = 0\n        else:\n            begpad = 0\n\n        if endsam > self._n_smp:\n            endpad = endsam - self._n_smp\n            endsam = self._n_smp\n        else:\n            endpad = 0\n\n        dshape = (self._n_chan, endsam - begsam)\n        sig_dtype = 'u' + str(self._n_bytes)\n        offset = self._bodata + begsam * self._n_bytes * self._n_chan\n        dat = memmap(str(self.filename), dtype=sig_dtype, order='F', mode='r',\n                     shape=dshape, offset=offset).astype('float')\n\n        dat = pad(dat[chan, :], ((0, 0), (begpad, endpad)), mode='constant',\n                  constant_values=NaN)\n\n        return (dat - self._offset[chan, None]) * self._factors[chan, None]", "response": "Return the data as 2D numpy. ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_markers(self):\n        markers = []\n\n        triggers = self._triggers\n        DTYPE_MAX = iinfo(triggers.dtype['sample']).max\n        triggers = triggers[triggers['sample'] != DTYPE_MAX]\n\n        for trig in triggers:\n            markers.append(\n                {'name': str(trig['code']),\n                 'start': trig['sample'] / self._s_freq,\n                 'end': trig['sample'] / self._s_freq,\n                 })\n\n        return markers", "response": "Return all the markers in the recordings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping video if tick is more than the end of the file.", "response": "def stop_video(self, tick):\n        \"\"\"Stop video if tick is more than the end, only for last file.\n\n        Parameters\n        ----------\n        tick : int\n            time in ms from the beginning of the file\n\n        useless?\n        \"\"\"\n        if self.cnt_video == self.n_video:\n            if tick >= self.end_diff:\n                self.idx_button.setText('Start')\n                self.video.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the video index", "response": "def next_video(self, _):\n        \"\"\"Also runs when file is loaded, so index starts at 2.\"\"\"\n        self.cnt_video += 1\n        lg.info('Update video to ' + str(self.cnt_video))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_stop_video(self):\n        if self.parent.info.dataset is None:\n            self.parent.statusBar().showMessage('No Dataset Loaded')\n            return\n\n        # & is added automatically by PyQt, it seems\n        if 'Start' in self.idx_button.text().replace('&', ''):\n            try:\n                self.update_video()\n            except IndexError as er:\n                lg.debug(er)\n                self.idx_button.setText('Not Available / Start')\n                return\n            except OSError as er:\n                lg.debug(er)\n                self.idx_button.setText('NO VIDEO for this dataset')\n                return\n\n            self.idx_button.setText('Stop')\n\n        elif 'Stop' in self.idx_button.text():\n            self.idx_button.setText('Start')\n            self.medialistplayer.stop()\n            self.t.stop()", "response": "Start and stop the video and change the button."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_video(self):\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n        d = self.parent.info.dataset\n\n        videos, begsec, endsec = d.read_videos(window_start,\n                                               window_start + window_length)\n        \n        lg.debug(f'Video: {begsec} - {endsec}')\n        self.endsec = endsec\n        videos = [str(v) for v in videos]  # make sure it's a str (not path)\n        medialist = vlc.MediaList(videos)\n        self.medialistplayer.set_media_list(medialist)\n\n        self.cnt_video = 0\n        self.n_video = len(videos)\n\n        self.t = QTimer()\n        self.t.timeout.connect(self.check_if_finished)\n        self.t.start(100)\n\n        self.medialistplayer.play()\n        self.mediaplayer.set_time(int(begsec * 1000))", "response": "Read list of files convert to video time and add video to queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef montage(data, ref_chan=None, ref_to_avg=False, bipolar=None,\n            method='average'):\n    \"\"\"Apply linear transformation to the channels.\n\n    Parameters\n    ----------\n    data : instance of DataRaw\n        the data to filter\n    ref_chan : list of str\n        list of channels used as reference\n    ref_to_avg : bool\n        if re-reference to average or not\n    bipolar : float\n        distance in mm to consider two channels as neighbors and then compute\n        the bipolar montage between them.\n    method : str\n        'average' or 'regression'. 'average' takes the\n        average across the channels selected as reference (it can be all) and\n        subtract it from each channel. 'regression' keeps the residuals after\n        regressing out the mean across channels.\n\n    Returns\n    -------\n    filtered_data : instance of DataRaw\n        filtered data\n\n    Notes\n    -----\n    If you don't change anything, it returns the same instance of data.\n    \"\"\"\n    if ref_to_avg and ref_chan is not None:\n        raise TypeError('You cannot specify reference to the average and '\n                        'the channels to use as reference')\n\n    if ref_chan is not None:\n        if (not isinstance(ref_chan, (list, tuple)) or\n            not all(isinstance(x, str) for x in ref_chan)):\n                raise TypeError('chan should be a list of strings')\n\n    if ref_chan is None:\n        ref_chan = []  # TODO: check bool for ref_chan\n\n    if bipolar:\n        if not data.attr['chan']:\n            raise ValueError('Data should have Chan information in attr')\n\n        _assert_equal_channels(data.axis['chan'])\n        chan_in_data = data.axis['chan'][0]\n        chan = data.attr['chan']\n        chan = chan(lambda x: x.label in chan_in_data)\n        chan, trans = create_bipolar_chan(chan, bipolar)\n        data.attr['chan'] = chan\n\n    if ref_to_avg or ref_chan or bipolar:\n        mdata = data._copy()\n\n        idx_chan = mdata.index_of('chan')\n\n        for i in range(mdata.number_of('trial')):\n            if ref_to_avg or ref_chan:\n                if ref_to_avg:\n                    ref_chan = data.axis['chan'][i]\n\n                ref_data = data(trial=i, chan=ref_chan)\n                if method == 'average':\n                    mdata.data[i] = (data(trial=i) - mean(ref_data, axis=idx_chan))\n                elif method == 'regression':\n                    mdata.data[i] = compute_average_regress(data(trial=i), idx_chan)\n\n            elif bipolar:\n\n                if not data.index_of('chan') == 0:\n                    raise ValueError('For matrix multiplication to work, '\n                                     'the first dimension should be chan')\n                mdata.data[i] = dot(trans, data(trial=i))\n                mdata.axis['chan'][i] = asarray(chan.return_label(),\n                                                dtype='U')\n\n    else:\n        mdata = data\n\n    return mdata", "response": "Apply linear transformation to the channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that all the trials have the same channels in the same order.", "response": "def _assert_equal_channels(axis):\n    \"\"\"check that all the trials have the same channels, in the same order.\n\n    Parameters\n    ----------\n    axis : ndarray of ndarray\n        one of the data axis\n\n    Raises\n    ------\n\n    \"\"\"\n    for i0 in axis:\n        for i1 in axis:\n            if not all(i0 == i1):\n                raise ValueError('The channels for all the trials should have '\n                                 'the same labels, in the same order.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake the mean across channels and regress out the mean from each channel", "response": "def compute_average_regress(x, idx_chan):\n    \"\"\"Take the mean across channels and regress out the mean from each channel\n\n    Parameters\n    ----------\n    x : ndarray\n        2d array with channels on one dimension\n    idx_chan:\n        which axis contains channels\n\n    Returns\n    -------\n    ndarray\n        same as x, but with the mean being regressed out\n    \"\"\"\n    if x.ndim != 2:\n        raise ValueError(f'The number of dimensions must be 2, not {x.ndim}')\n\n    x = moveaxis(x, idx_chan, 0)  # move axis to the front\n    avg = mean(x, axis=0)\n\n    x_o = []\n    for i in range(x.shape[0]):\n        r = lstsq(avg[:, None], x[i, :][:, None], rcond=0)[0]\n        x_o.append(\n            x[i, :] - r[0, 0] * avg\n            )\n    return moveaxis(asarray(x_o), 0, idx_chan)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkeeping track of the most recent datasets.", "response": "def keep_recent_datasets(max_dataset_history, info=None):\n    \"\"\"Keep track of the most recent recordings.\n\n    Parameters\n    ----------\n    max_dataset_history : int\n        maximum number of datasets to remember\n    info : str, optional TODO\n        path to file\n\n    Returns\n    -------\n    list of str\n        paths to most recent datasets (only if you don't specify\n        new_dataset)\n    \"\"\"\n    history = settings.value('recent_recordings', [])\n    if isinstance(history, str):\n        history = [history]\n\n    if info is not None and info.filename is not None:\n        new_dataset = info.filename\n\n        if new_dataset in history:\n            lg.debug(new_dataset + ' already present, will be replaced')\n            history.remove(new_dataset)\n        if len(history) > max_dataset_history:\n            lg.debug('Removing last dataset ' + history[-1])\n            history.pop()\n\n        lg.debug('Adding ' + new_dataset + ' to list of recent datasets')\n        history.insert(0, new_dataset)\n        settings.setValue('recent_recordings', history)\n        return None\n\n    else:\n        return history"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a simple message box to see if the user wants to open a file or a directory Returns a string of type dir or file or abort", "response": "def choose_file_or_dir():\n    \"\"\"Create a simple message box to see if the user wants to open dir or file\n\n    Returns\n    -------\n    str\n        'dir' or 'file' or 'abort'\n\n    \"\"\"\n    question = QMessageBox(QMessageBox.Information, 'Open Dataset',\n                           'Do you want to open a file or a directory?')\n    dir_button = question.addButton('Directory', QMessageBox.YesRole)\n    file_button = question.addButton('File', QMessageBox.NoRole)\n    question.addButton(QMessageBox.Cancel)\n    question.exec_()\n    response = question.clickedButton()\n\n    if response == dir_button:\n        return 'dir'\n    elif response == file_button:\n        return 'file'\n    else:\n        return 'abort'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert any string to an RGB color.", "response": "def convert_name_to_color(s):\n    \"\"\"Convert any string to an RGB color.\n\n    Parameters\n    ----------\n    s : str\n        string to convert\n    selection : bool, optional\n        if an event is being selected, it's lighter\n\n    Returns\n    -------\n    instance of QColor\n        one of the possible color\n\n    Notes\n    -----\n    It takes any string and converts it to RGB color. The same string always\n    returns the same color. The numbers are a bit arbitrary but not completely.\n    h is the baseline color (keep it high to have brighter colors). Make sure\n    that the max module + h is less than 256 (RGB limit).\n\n    The number you multiply ord for is necessary to differentiate the letters\n    (otherwise 'r' and 's' are too close to each other).\n    \"\"\"\n    h = 100\n    v = [5 * ord(x) for x in s]\n    sum_mod = lambda x: sum(x) % 100\n    color = QColor(sum_mod(v[::3]) + h, sum_mod(v[1::3]) + h,\n                   sum_mod(v[2::3]) + h)\n    return color"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef freq_from_str(freq_str):\n    freq = []\n    as_list = freq_str[1:-1].replace(' ', '').split(',')\n\n    try:\n        if freq_str[0] == '[' and freq_str[-1] == ']':\n            for i in as_list:\n                one_band = i[1:-1].split('-')\n                one_band = float(one_band[0]), float(one_band[1])\n                freq.append(one_band)\n    \n        elif freq_str[0] == '(' and freq_str[-1] == ')':\n    \n            if len(as_list) == 4:\n                start = float(as_list[0])\n                stop = float(as_list[1])\n                halfwidth = float(as_list[2]) / 2\n                step = float(as_list[3])\n                centres = arange(start, stop, step)\n                for i in centres:\n                    freq.append((i - halfwidth, i + halfwidth))\n            else:\n                return None\n    \n        else:\n            return None           \n    except:\n        return None\n\n    return freq", "response": "Obtain frequency ranges from input string either as a list or dynamic\n    notation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_graphics_to_svg(widget, filename):\n    generator = QSvgGenerator()\n    generator.setFileName(filename)\n    generator.setSize(widget.size())\n    generator.setViewBox(widget.rect())\n\n    painter = QPainter()\n    painter.begin(generator)\n    widget.render(painter)\n    painter.end()", "response": "Export graphics to svg"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value of the checkbox.", "response": "def set_value(self, value):\n        \"\"\"Set value of the checkbox.\n\n        Parameters\n        ----------\n        value : bool\n            value for the checkbox\n\n        \"\"\"\n        if value:\n            self.setCheckState(Qt.Checked)\n        else:\n            self.setCheckState(Qt.Unchecked)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value of the checkbox.", "response": "def set_value(self, value):\n        \"\"\"Set value of the checkbox.\n\n        Parameters\n        ----------\n        value : bool\n            value for the checkbox\n\n        \"\"\"\n        if value:\n            self.setChecked(Qt.Checked)\n        else:\n            self.setChecked(Qt.Unchecked)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting int from widget.", "response": "def get_value(self, default=None):\n        \"\"\"Get int from widget.\n\n        Parameters\n        ----------\n        default : list\n            list with widgets\n\n        Returns\n        -------\n        list\n            list that might contain int or str or float etc\n\n        \"\"\"\n        if default is None:\n            default = []\n\n        try:\n            text = literal_eval(self.text())\n            if not isinstance(text, list):\n                pass\n                # raise ValueError\n\n        except ValueError:\n            lg.debug('Cannot convert \"' + str(text) + '\" to list. ' +\n                     'Using default ' + str(default))\n            text = default\n            self.set_value(text)\n\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to the selected file.", "response": "def connect(self, funct):\n        \"\"\"Call funct when the text was changed.\n\n        Parameters\n        ----------\n        funct : function\n            function that broadcasts a change.\n\n        Notes\n        -----\n        There is something wrong here. When you run this function, it calls\n        for opening a directory three or four times. This is obviously wrong\n        but I don't understand why this happens three times. Traceback did not\n        help.\n\n        \"\"\"\n        def get_directory():\n            rec = QFileDialog.getExistingDirectory(self,\n                                                   'Path to Recording'\n                                                   ' Directory')\n            if rec == '':\n                return\n\n            self.setText(rec)\n            funct()\n\n        self.clicked.connect(get_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_value(self, default=None):\n        if default is None:\n            default = ''\n\n        try:\n            text = self.currentText()\n\n        except ValueError:\n            lg.debug('Cannot convert \"' + str(text) + '\" to list. ' +\n                     'Using default ' + str(default))\n            text = default\n            self.set_value(text)\n\n        return text", "response": "Get selection from widget.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the iEEG dataset info to a JSON file.", "response": "def _write_ieeg_json(output_file):\n    \"\"\"Use only required fields\n    \"\"\"\n    dataset_info = {\n        \"TaskName\": \"unknown\",\n        \"Manufacturer\": \"n/a\",\n        \"PowerLineFrequency\": 50,\n        \"iEEGReference\": \"n/a\",\n        }\n\n    with output_file.open('w') as f:\n        dump(dataset_info, f, indent=' ')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the ieg channels to a file.", "response": "def _write_ieeg_channels(output_file, data):\n    \"\"\"\n    TODO\n    ----\n    Make sure that the channels in all the trials are the same.\n    \"\"\"\n    CHAN_TYPE = 'ECOG'\n    CHAN_UNIT = '\u00b5V'\n\n    with output_file.open('w') as f:\n        f.write('name\\ttype\\tunits\\tsampling_frequency\\tlow_cutoff\\thigh_cutoff\\tnotch\\treference\\n')\n        for one_chan in data.chan[0]:\n            f.write('\\t'.join([\n                one_chan,\n                CHAN_TYPE,\n                CHAN_UNIT,\n                f'{data.s_freq:f}',\n                'n/a',\n                'n/a',\n                'n/a',\n                'n/a',\n                ]) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting BIDS channels TSV from Dataset.", "response": "def write_bids_channels(output_file, dataset):\n    \"\"\"Export BIDS channels TSV from Dataset.\n    \n    Parameters\n    ----------\n    output_file : path to file\n        file to export to (use '.tsv' as extension)\n    dataset : instance of wonambi.Dataset\n        Dataset with record metadata\n    \"\"\"\n    if dataset.IOClass is Edf:\n        hdr = dataset.header['orig']\n        channels = hdr['label']\n        units = [x if x.encode('utf-8') != b'\\xef\\xbf\\xbd' else '?' \\\n                 for x in hdr['physical_dim']]\n        low_cut = [x[x.index('HP:') + 3:x.index('Hz')] \\\n                     if 'HP:' in x else '0' for x in hdr['prefiltering']]\n        high_cut = [x[x.index('LP:') + 3:x[x.index('LP:'):].index('Hz') \\\n                      + x.index('LP:')] \\\n                      if 'LP:' in x else 'Inf' for x in hdr['prefiltering']]\n        notch = [x[x.index('N:') + 2:-2] \\\n                      if 'N:' in x else 'n/a' for x in hdr['prefiltering']]        \n        s_freq = [x / hdr['record_length'] \\\n                  for x in hdr['n_samples_per_record']]\n        \n        chan_type = []\n        for one_chan in channels:\n            ch = one_chan.lower()\n            if 'eog' in ch or ch == 'e1' or ch == 'e2':\n                chan_type.append('EOG')\n            elif any(x in ch for x in ['ecg', 'ekg']):\n                chan_type.append('ECG')\n            elif any(x in ch for x in ['emg', 'chin', 'leg']):\n                chan_type.append('EMG')\n            elif (ch[-1].isdigit() and ch[:2] != 'sp') or ch[-1] == 'z': \n                # not a perfect test\n                #print(f'yessir, {ch} fits the bill alright!')\n                chan_type.append('EEG')\n            else:\n                chan_type.append('MISC')\n        \n        with output_file.open('w') as f:\n            \n            f.write('name\\ttype\\tunits\\tsampling_frequency\\tlow_cutoff'\n                    '\\thigh_cutoff\\tnotch\\treference\\n')\n            \n            for i, one_chan in enumerate(channels):\n                f.write('\\t'.join([\n                    one_chan,\n                    chan_type[i],\n                    units[i],\n                    f'{s_freq[i]:f}',\n                    low_cut[i],\n                    high_cut[i],\n                    notch[i],\n                    'n/a',\n                    ]) + '\\n')\n    \n    else:\n        print(str(dataset.IOClass) + ' not currently supported.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the header for further use.", "response": "def return_hdr(self):\n        \"\"\"Return the header for further use.\n\n        Returns\n        -------\n        subj_id : str\n            subject identification code\n        start_time : datetime\n            start time of the dataset\n        s_freq : float\n            sampling frequency\n        chan_name : list of str\n            list of all the channels\n        n_samples : int\n            number of samples in the dataset\n        orig : dict\n            additional information taken directly from the header\n        \"\"\"\n        subj_id = self.task.subject\n\n        sampling_freq = set(self.task.channels.get(map_lambda=lambda x: x['sampling_frequency']))\n        if len(sampling_freq) > 1:\n            raise ValueError('Multiple sampling frequencies not supported')\n\n        s_freq = float(next(iter(sampling_freq)))\n        chan_name = self.task.channels.get(map_lambda=lambda x: x['name'])\n        self.chan_name = array(chan_name)\n\n        # read these values directly from dataset\n        orig = self.baseformat.header\n        start_time = orig['start_time']\n        n_samples = orig['n_samples']\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_dat(self, chan, begsam, endsam):\n        return self.baseformat.dataset.return_dat(chan, begsam, endsam)", "response": "Return the data as 2D numpy. ndarray.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_markers(self):\n        markers = []\n        for mrk in self.task.events.tsv:\n            markers.append({\n                'start': float(mrk['onset']),\n                'end': float(mrk['onset']) + float(mrk['duration']),\n                'name': mrk['trial_type']\n            })\n\n        return markers", "response": "Return all the markers in the task."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef value(self, parameter, new_value=None):\n        for widget_name, values in DEFAULTS.items():\n            if parameter in values.keys():\n                widget = getattr(self, widget_name)\n                if new_value is None:\n                    return widget.config.value[parameter]\n                else:\n                    lg.debug('setting value {0} of {1} to {2}'\n                             ''.format(parameter, widget_name, new_value))\n                    widget.config.value[parameter] = new_value", "response": "This function is a shortcut for any parameter. It returns the current value of the parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting all the information from previous dataset before loading a new dataset.", "response": "def reset(self):\n        \"\"\"Remove all the information from previous dataset before loading a\n        new dataset.\n        \"\"\"\n\n        # store current dataset\n        max_dataset_history = self.value('max_dataset_history')\n        keep_recent_datasets(max_dataset_history, self.info)\n\n        # reset all the widgets\n        self.labels.reset()\n        self.channels.reset()\n        self.info.reset()\n        self.notes.reset()\n        self.overview.reset()\n        self.spectrum.reset()\n        self.traces.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen the Setting windows after updating the values in GUI.", "response": "def show_settings(self):\n        \"\"\"Open the Setting windows, after updating the values in GUI. \"\"\"\n        self.notes.config.put_values()\n        self.overview.config.put_values()\n        self.settings.config.put_values()\n        self.spectrum.config.put_values()\n        self.traces.config.put_values()\n        self.video.config.put_values()\n\n        self.settings.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the spindle detection dialog.", "response": "def show_spindle_dialog(self):\n        \"\"\"Create the spindle detection dialog.\"\"\"\n        self.spindle_dialog.update_groups()\n        self.spindle_dialog.update_cycles()\n        self.spindle_dialog.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the SW detection dialog.", "response": "def show_slow_wave_dialog(self):\n        \"\"\"Create the SW detection dialog.\"\"\"\n        self.slow_wave_dialog.update_groups()\n        self.slow_wave_dialog.update_cycles()\n        self.slow_wave_dialog.show()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the event analysis dialog.", "response": "def show_event_analysis_dialog(self):\n        \"\"\"Create the event analysis dialog.\"\"\"\n        self.event_analysis_dialog.update_types()\n        self.event_analysis_dialog.update_groups()\n        self.event_analysis_dialog.update_cycles()\n        self.event_analysis_dialog.show()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the analysis dialog.", "response": "def show_analysis_dialog(self):\n        \"\"\"Create the analysis dialog.\"\"\"\n        self.analysis_dialog.update_evt_types()\n        self.analysis_dialog.update_groups()\n        self.analysis_dialog.update_cycles()\n        self.analysis_dialog.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the name of the last open dataset.", "response": "def closeEvent(self, event):\n        \"\"\"save the name of the last open dataset.\"\"\"\n        max_dataset_history = self.value('max_dataset_history')\n        keep_recent_datasets(max_dataset_history, self.info)\n\n        settings.setValue('window/geometry', self.saveGeometry())\n        settings.setValue('window/state', self.saveState())\n\n        event.accept()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_header(filename):\n    header = _read_header_text(filename)\n    first_row = header[0]\n    EXTRA_ROWS = 3  # drop DefaultValue1 LowRange1 HighRange1\n\n    hdr = {}\n    for group in finditer('(\\w*)= ([\\w.]*)', first_row):\n        hdr[group.group(1)] = group.group(2)\n\n    if first_row.startswith('BCI2000V'):\n        VERSION = hdr['BCI2000V']\n\n    else:\n        VERSION = '1'\n        hdr['DataFormat'] = 'int16'\n\n    for row in header[1:]:\n        if row.startswith('['):  # remove '[ ... Definition ]'\n            section = row[2:-14].replace(' ', '')\n\n            if section == 'StateVector':\n                hdr[section] = []\n            else:\n                hdr[section] = {} # defaultdict(dict)\n            continue\n\n        if row == '':\n            continue\n\n        elif section == 'StateVector':\n            statevector = {key: value for key, value in list(zip(STATEVECTOR, row.split(' ')))}\n            hdr[section].append(statevector)\n\n        else:\n            group = match('(?P<subsection>[\\w:%]*) (?P<format>\\w*) (?P<key>\\w*)= (?P<value>.*) // ', row)\n            onerow = group.groupdict()\n\n            values = onerow['value'].split(' ')\n            if len(values) > EXTRA_ROWS:\n                value = ' '.join(onerow['value'].split(' ')[:-EXTRA_ROWS])\n            else:\n                value = ' '.join(values)\n\n            hdr[section][onerow['key']] = value  # similar to matlab's output\n\n    return hdr", "response": "Read the header text and return a dict of the header data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef return_hdr(self):\n        orig = {}\n        orig = _read_header(self.filename)\n\n        nchan = int(orig['SourceCh'])\n        chan_name = ['ch{:03d}'.format(i + 1) for i in range(nchan)]\n        chan_dtype = dtype(orig['DataFormat'])\n        self.statevector_len = int(orig['StatevectorLen'])\n\n        s_freq = orig['Parameter']['SamplingRate']\n        if s_freq.endswith('Hz'):\n            s_freq = s_freq.replace('Hz', '')\n        s_freq = int(s_freq.strip())\n        self.s_freq = s_freq\n\n        storagetime = orig['Parameter']['StorageTime'].replace('%20', ' ')\n        try:  # newer version\n            start_time = datetime.strptime(storagetime, '%a %b %d %H:%M:%S %Y')\n        except:\n            start_time = datetime.strptime(storagetime, '%Y-%m-%dT%H:%M:%S')\n\n        subj_id = orig['Parameter']['SubjectName']\n\n        self.dtype = dtype([(chan, chan_dtype) for chan in chan_name] +\n                           [('statevector', 'S', self.statevector_len)])\n\n        # compute n_samples based on file size - header\n        with open(self.filename, 'rb') as f:\n            f.seek(0, SEEK_END)\n            EOData = f.tell()\n        n_samples = int((EOData - int(orig['HeaderLen'])) / self.dtype.itemsize)\n\n        self.s_freq = s_freq\n        self.header_len = int(orig['HeaderLen'])\n        self.n_samples = n_samples\n        self.statevectors = _prepare_statevectors(orig['StateVector'])\n        # TODO: a better way to parse header\n        self.gain = array([float(x) for x in orig['Parameter']['SourceChGain'].split(' ')[1:]])\n\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig", "response": "Return the header for further use."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the data as 2D numpy. ndarray.", "response": "def return_dat(self, chan, begsam, endsam):\n        \"\"\"Return the data as 2D numpy.ndarray.\n\n        Parameters\n        ----------\n        chan : int or list\n            index (indices) of the channels to read\n        begsam : int\n            index of the first sample\n        endsam : int\n            index of the last sample\n\n        Returns\n        -------\n        numpy.ndarray\n            A 2d matrix, with dimension chan X samples\n        \"\"\"\n        dat_begsam = max(begsam, 0)\n        dat_endsam = min(endsam, self.n_samples)\n        dur = dat_endsam - dat_begsam\n\n        dtype_onlychan = dtype({k: v for k, v in self.dtype.fields.items() if v[0].kind != 'S'})\n\n        # make sure we read some data at least, otherwise segfault\n        if dat_begsam < self.n_samples and dat_endsam > 0:\n\n            with self.filename.open('rb') as f:\n                f.seek(self.header_len, SEEK_SET)  # skip header\n\n                f.seek(self.dtype.itemsize * dat_begsam, SEEK_CUR)\n                dat = fromfile(f, dtype=self.dtype, count=dur)\n\n            dat = ndarray(dat.shape, dtype_onlychan, dat, 0, dat.strides).view((dtype_onlychan[0], len(dtype_onlychan.names))).T\n\n        else:\n            n_chan = len(dtype_onlychan.names)\n            dat = empty((n_chan, 0))\n\n        if begsam < 0:\n\n            pad = empty((dat.shape[0], 0 - begsam))\n            pad.fill(NaN)\n            dat = c_[pad, dat]\n\n        if endsam >= self.n_samples:\n\n            pad = empty((dat.shape[0], endsam - self.n_samples))\n            pad.fill(NaN)\n            dat = c_[dat, pad]\n\n        return dat[chan, :] * self.gain[chan][:, None]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_markers(self, state='MicromedCode'):\n        markers = []\n        try:\n            all_states = self._read_states()\n        except ValueError:  # cryptic error when reading states\n            return markers\n\n        try:\n            x = all_states[state]\n        except KeyError:\n            return markers\n\n        markers = []\n        i_mrk = hstack((0, where(diff(x))[0] + 1, len(x)))\n        for i0, i1 in zip(i_mrk[:-1], i_mrk[1:]):\n            marker = {'name': str(x[i0]),\n                      'start': (i0) / self.s_freq,\n                      'end': i1 / self.s_freq,\n                     }\n            markers.append(marker)\n\n        return markers", "response": "Return all the markers in the state specified by state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_hdr(self):\n        self.fdtfile = None\n\n        try:\n            self.EEG = loadmat(str(self.filename), struct_as_record=False,\n                               squeeze_me=True)['EEG']\n            self.hdf5 = False\n\n        except NotImplementedError:\n            self.hdf5 = True\n\n        if not self.hdf5:\n            self.s_freq = self.EEG.srate\n            chan_name = [chan.labels for chan in self.EEG.chanlocs]\n            n_samples = self.EEG.pnts\n\n            if isinstance(self.EEG.subject, str):\n                subj_id = self.EEG.subject\n            else:\n                subj_id = ''\n            try:\n                start_time = datetime(*self.EEG.etc.T0)\n            except AttributeError:\n                start_time = DEFAULT_DATETIME\n\n            if isinstance(self.EEG.datfile, str):\n                self.fdtfile = self.EEG.datfile\n            else:\n                self.data = self.EEG.data\n\n        else:\n\n            with File(self.filename) as f:\n                EEG = f['EEG']\n                self.s_freq = EEG['srate'].value.item()\n                chan_name = read_hdf5_chan_name(f, EEG['chanlocs']['labels'])\n                n_samples = int(EEG['pnts'].value.item())\n\n                subj_id = read_hdf5_str(EEG['subject'])\n                try:\n                    start_time = datetime(*EEG['etc']['T0'])\n                except ValueError:\n                    start_time = DEFAULT_DATETIME\n\n                datfile = read_hdf5_str(EEG['datfile'])\n                if datfile == '':\n                    self.data = EEG['data'].value.T  # for some reason, you need to transpose this\n                else:\n                    self.fdtfile = datfile\n\n        if self.fdtfile is not None:\n            memshape = (len(chan_name), int(n_samples))\n            memmap_file = self.filename.parent / self.fdtfile\n            if not memmap_file.exists():\n                renamed_memmap_file = self.filename.with_suffix('.fdt')\n                if not renamed_memmap_file.exists():\n                    raise FileNotFoundError(f'No file {memmap_file} or {renamed_memmap_file}')\n                else:\n                    memmap_file = renamed_memmap_file\n\n            self.data = memmap(str(memmap_file), 'float32', mode='c', shape=memshape, order='F')\n\n        return subj_id, start_time, self.s_freq, chan_name, n_samples, {}", "response": "Returns the header of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncorrect times to remove events marked Artefact.", "response": "def remove_artf_evts(times, annot, chan=None, min_dur=0.1):\n    \"\"\"Correct times to remove events marked 'Artefact'.\n\n    Parameters\n    ----------\n    times : list of tuple of float\n        the start and end times of each segment\n    annot : instance of Annotations\n        the annotation file containing events and epochs\n    chan : str, optional\n        full name of channel on which artefacts were marked. Channel format is \n        'chan_name (group_name)'. If None, artefacts from any channel will be\n        removed.\n    min_dur : float\n        resulting segments, after concatenation, are rejected if shorter than\n        this duration\n\n    Returns\n    -------\n    list of tuple of float\n        the new start and end times of each segment, with artefact periods \n        taken out            \n    \"\"\"    \n    new_times = times\n    beg = times[0][0]\n    end = times[-1][-1]\n    chan = (chan, '') if chan else None # '' is for channel-global artefacts\n    \n    artefact = annot.get_events(name='Artefact', time=(beg, end), chan=chan,\n                                qual='Good')\n        \n    if artefact:\n        new_times = []\n        \n        for seg in times:\n            reject = False\n            new_seg = True\n            \n            while new_seg is not False:\n                if type(new_seg) is tuple:\n                    seg = new_seg\n                end = seg[1]\n            \n                for artf in artefact:\n                    \n                    if artf['start'] <= seg[0] and seg[1] <= artf['end']:\n                        reject = True\n                        new_seg = False\n                        break\n                    \n                    a_starts_in_s = seg[0] <= artf['start'] <= seg[1]\n                    a_ends_in_s = seg[0] <= artf['end'] <= seg[1]\n                    \n                    if a_ends_in_s and not a_starts_in_s:\n                        seg = artf['end'], seg[1]\n                        \n                    elif a_starts_in_s:\n                        seg = seg[0], artf['start']\n    \n                        if a_ends_in_s:\n                            new_seg = artf['end'], end\n                        else:\n                            new_seg = False\n                        break\n                    \n                    new_seg = False\n            \n                if reject is False and seg[1] - seg[0] >= min_dur:\n                    new_times.append(seg)\n        \n    return new_times"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns decorator for backoff and retry triggered by exception. Args: wait_gen: A generator yielding successive wait times in seconds. exception: An exception type (or tuple of types) which triggers backoff. max_tries: The maximum number of attempts to make before giving up. Once exhausted, the exception will be allowed to escape. The default value of None means their is no limit to the number of tries. If a callable is passed, it will be evaluated at runtime and its return value used. jitter: A function of the value yielded by wait_gen returning the actual time to wait. This distributes wait times stochastically in order to avoid timing collisions across concurrent clients. Wait times are jittered by default using the full_jitter function. Jittering may be disabled altogether by passing jitter=None. giveup: Function accepting an exception instance and returning whether or not to give up. Optional. The default is to always continue. on_success: Callable (or iterable of callables) with a unary signature to be called in the event of success. The parameter is a dict containing details about the invocation. on_backoff: Callable (or iterable of callables) with a unary signature to be called in the event of a backoff. The parameter is a dict containing details about the invocation. on_giveup: Callable (or iterable of callables) with a unary signature to be called in the event that max_tries is exceeded. The parameter is a dict containing details about the invocation. **wait_gen_kwargs: Any additional keyword args specified will be passed to wait_gen when it is initialized. Any callable args will first be evaluated and their return values passed. This is useful for runtime configuration.", "response": "def on_exception(wait_gen,\n                 exception,\n                 max_tries=None,\n                 jitter=full_jitter,\n                 giveup=lambda e: False,\n                 on_success=None,\n                 on_backoff=None,\n                 on_giveup=None,\n                 **wait_gen_kwargs):\n    \"\"\"Returns decorator for backoff and retry triggered by exception.\n\n    Args:\n        wait_gen: A generator yielding successive wait times in\n            seconds.\n        exception: An exception type (or tuple of types) which triggers\n            backoff.\n        max_tries: The maximum number of attempts to make before giving\n            up. Once exhausted, the exception will be allowed to escape.\n            The default value of None means their is no limit to the\n            number of tries. If a callable is passed, it will be\n            evaluated at runtime and its return value used.\n        jitter: A function of the value yielded by wait_gen returning\n            the actual time to wait. This distributes wait times\n            stochastically in order to avoid timing collisions across\n            concurrent clients. Wait times are jittered by default\n            using the full_jitter function. Jittering may be disabled\n            altogether by passing jitter=None.\n        giveup: Function accepting an exception instance and\n            returning whether or not to give up. Optional. The default\n            is to always continue.\n        on_success: Callable (or iterable of callables) with a unary\n            signature to be called in the event of success. The\n            parameter is a dict containing details about the invocation.\n        on_backoff: Callable (or iterable of callables) with a unary\n            signature to be called in the event of a backoff. The\n            parameter is a dict containing details about the invocation.\n        on_giveup: Callable (or iterable of callables) with a unary\n            signature to be called in the event that max_tries\n            is exceeded.  The parameter is a dict containing details\n            about the invocation.\n        **wait_gen_kwargs: Any additional keyword args specified will be\n            passed to wait_gen when it is initialized.  Any callable\n            args will first be evaluated and their return values passed.\n            This is useful for runtime configuration.\n    \"\"\"\n    success_hdlrs = _handlers(on_success)\n    backoff_hdlrs = _handlers(on_backoff, _log_backoff)\n    giveup_hdlrs = _handlers(on_giveup, _log_giveup)\n\n    def decorate(target):\n\n        @functools.wraps(target)\n        def retry(*args, **kwargs):\n            # change names because python 2.x doesn't have nonlocal\n            max_tries_ = _maybe_call(max_tries)\n\n            # there are no dictionary comprehensions in python 2.6\n            wait = wait_gen(**dict((k, _maybe_call(v))\n                                   for k, v in wait_gen_kwargs.items()))\n\n            tries = 0\n            while True:\n                try:\n                    tries += 1\n                    ret = target(*args, **kwargs)\n                except exception as e:\n                    if giveup(e) or tries == max_tries_:\n                        for hdlr in giveup_hdlrs:\n                            hdlr({'target': target,\n                                  'args': args,\n                                  'kwargs': kwargs,\n                                  'tries': tries})\n                        raise\n\n                    value = next(wait)\n                    try:\n                        if jitter is not None:\n                            seconds = jitter(value)\n                        else:\n                            seconds = value\n                    except TypeError:\n                        # support deprecated nullary jitter function signature\n                        # which returns a delta rather than a jittered value\n                        seconds = value + jitter()\n\n                    for hdlr in backoff_hdlrs:\n                        hdlr({'target': target,\n                              'args': args,\n                              'kwargs': kwargs,\n                              'tries': tries,\n                              'wait': seconds})\n\n                    time.sleep(seconds)\n                else:\n                    for hdlr in success_hdlrs:\n                        hdlr({'target': target,\n                              'args': args,\n                              'kwargs': kwargs,\n                              'tries': tries})\n\n                    return ret\n\n        return retry\n\n    # Return a function which decorates a target with a retry loop.\n    return decorate"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the retrieved_pid of the item if the new item is newer than the olditem.", "response": "def update_retrieved_if_new(olditem, newitem, days=180, retrieved_pid='P813'):\n    \"\"\"\n    # modifies olditem in place\n    \"\"\"\n    def ref_overwrite(oldref, newref, days):\n        \"\"\"\n        If the newref is the same as the oldref except the retrieved date is `days` newer, return True\n                                                       the retrieved date is NOT `days` newer, return False\n        the refs are different, return True\n        \"\"\"\n        if len(oldref) != len(newref):\n            return True\n        oldref_minus_retrieved = [x for x in oldref if x.get_prop_nr() != retrieved_pid]\n        newref_minus_retrieved = [x for x in newref if x.get_prop_nr() != retrieved_pid]\n        if not all(x in oldref_minus_retrieved for x in newref_minus_retrieved):\n            return True\n        oldref_retrieved = [x for x in oldref if x.get_prop_nr() == retrieved_pid]\n        newref_retrieved = [x for x in newref if x.get_prop_nr() == retrieved_pid]\n        if (len(newref_retrieved) != len(oldref_retrieved)) or not (\n                len(newref_retrieved) == len(oldref_retrieved) == 1):\n            return True\n        datefmt = '+%Y-%m-%dT%H:%M:%SZ'\n        retold = list([datetime.strptime(r.get_value()[0], datefmt) for r in oldref if r.get_prop_nr() == retrieved_pid])[0]\n        retnew = list([datetime.strptime(r.get_value()[0], datefmt) for r in newref if r.get_prop_nr() == retrieved_pid])[0]\n        return (retnew - retold).days >= days\n\n    newrefs = newitem.references\n    oldrefs = olditem.references\n    if not (len(newrefs) == len(oldrefs) == 1):\n        #print(\"overwriting refs, not 1\")\n        olditem.references = copy.deepcopy(newitem.references)\n        return None\n    overwrite = ref_overwrite(oldrefs[0], newrefs[0], days)\n    if overwrite:\n        #print(\"updating ref\")\n        olditem.references = newrefs\n    else:\n        #print(\"don't change\")\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the properties that are distinct from the properties of the current object.", "response": "def get_distinct_value_props(cls, sparql_endpoint_url='https://query.wikidata.org/sparql'):\n        \"\"\"\n        On wikidata, the default core IDs will be the properties with a distinct values constraint\n        select ?p where {?p wdt:P2302 wd:Q21502410}\n        See: https://www.wikidata.org/wiki/Help:Property_constraints_portal\n        https://www.wikidata.org/wiki/Help:Property_constraints_portal/Unique_value\n        \"\"\"\n        pcpid = config['PROPERTY_CONSTRAINT_PID']\n        dvcqid = config['DISTINCT_VALUES_CONSTRAINT_QID']\n        try:\n            h = WikibaseHelper(sparql_endpoint_url)\n            pcpid = h.get_pid(pcpid)\n            dvcqid = h.get_qid(dvcqid)\n        except Exception:\n            warnings.warn(\"Unable to determine PIDs or QIDs for retrieving distinct value properties.\\n\" +\n                  \"Please set P2302 and Q21502410 in your wikibase or set `core_props` manually.\\n\" +\n                  \"Continuing with no core_props\")\n            cls.DISTINCT_VALUE_PROPS[sparql_endpoint_url] = set()\n            return None\n\n        query = \"select ?p where {{?p wdt:{} wd:{}}}\".format(pcpid, dvcqid)\n        df = cls.execute_sparql_query(query, endpoint=sparql_endpoint_url, as_dataframe=True)\n        if df.empty:\n            warnings.warn(\"Warning: No distinct value properties found\")\n            cls.DISTINCT_VALUE_PROPS[sparql_endpoint_url] = set()\n            return None\n        df.p = df.p.str.rsplit(\"/\", 1).str[-1]\n        cls.DISTINCT_VALUE_PROPS[sparql_endpoint_url] = set(df.p)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a WD item in json representation from Wikidata AttributeNames", "response": "def get_wd_entity(self):\n        \"\"\"\n        retrieve a WD item in json representation from Wikidata\n        :rtype: dict\n        :return: python complex dictionary represenation of a json\n        \"\"\"\n        params = {\n            'action': 'wbgetentities',\n            'sites': 'enwiki',\n            'ids': self.wd_item_id,\n            'format': 'json'\n        }\n        headers = {\n            'User-Agent': self.user_agent\n        }\n        json_data = self.mediawiki_api_call(\"GET\", self.mediawiki_api_url, params=params, headers=headers)\n        return self.parse_wd_json(wd_json=json_data['entities'][self.wd_item_id])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a WD entity json and generates the datatype objects.", "response": "def parse_wd_json(self, wd_json):\n        \"\"\"\n        Parses a WD entity json and generates the datatype objects, sets self.wd_json_representation\n        :param wd_json: the json of a WD entity\n        :type wd_json: A Python Json representation of a WD item\n        :return: returns the json representation containing 'labels', 'descriptions', 'claims', 'aliases', 'sitelinks'.\n        \"\"\"\n        wd_data = {x: wd_json[x] for x in ('labels', 'descriptions', 'claims', 'aliases') if x in wd_json}\n        wd_data['sitelinks'] = dict()\n        self.entity_metadata = {x: wd_json[x] for x in wd_json if x not in\n                                ('labels', 'descriptions', 'claims', 'aliases', 'sitelinks')}\n        self.sitelinks = wd_json.get('sitelinks', dict())\n\n        self.statements = []\n        for prop in wd_data['claims']:\n            for z in wd_data['claims'][prop]:\n                data_type = [x for x in WDBaseDataType.__subclasses__() if x.DTYPE == z['mainsnak']['datatype']][0]\n                statement = data_type.from_json(z)\n                self.statements.append(statement)\n\n        self.wd_json_representation = wd_data\n        self.original_statements = copy.deepcopy(self.statements)\n\n        return wd_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a search in WD for a certain WD search string :param search_string: a string which should be searched for in WD :type search_string: str :param mediawiki_api_url: Specify the mediawiki_api_url. :type mediawiki_api_url: str :param user_agent: The user agent string transmitted in the http header :type user_agent: str :param max_results: The maximum number of search results returned. Default 500 :type max_results: int :param language: The language in which to perform the search. Default 'en' :type language: str :return: returns a list of QIDs found in the search and a list of labels complementary to the QIDs", "response": "def get_wd_search_results(search_string='', mediawiki_api_url='https://www.wikidata.org/w/api.php',\n                              user_agent=config['USER_AGENT_DEFAULT'],\n                              max_results=500, language='en'):\n        \"\"\"\n        Performs a search in WD for a certain WD search string\n        :param search_string: a string which should be searched for in WD\n        :type search_string: str\n        :param mediawiki_api_url: Specify the mediawiki_api_url.\n        :type mediawiki_api_url: str\n        :param user_agent: The user agent string transmitted in the http header\n        :type user_agent: str\n        :param max_results: The maximum number of search results returned. Default 500\n        :type max_results: int\n        :param language: The language in which to perform the search. Default 'en'\n        :type language: str\n        :return: returns a list of QIDs found in the search and a list of labels complementary to the QIDs\n        \"\"\"\n        params = {\n            'action': 'wbsearchentities',\n            'language': language,\n            'search': search_string,\n            'format': 'json',\n            'limit': 50\n        }\n\n        headers = {\n            'User-Agent': user_agent\n        }\n\n        cont_count = 1\n        id_list = []\n        id_labels = []\n\n        while cont_count > 0:\n            params.update({'continue': 0 if cont_count == 1 else cont_count})\n\n            reply = requests.get(mediawiki_api_url, params=params, headers=headers)\n            reply.raise_for_status()\n            search_results = reply.json()\n\n            if search_results['success'] != 1:\n                raise WDSearchError('WD search failed')\n            else:\n                for i in search_results['search']:\n                    id_list.append(i['id'])\n                    id_labels.append(i['label'])\n\n            if 'search-continue' not in search_results:\n                cont_count = 0\n            else:\n                cont_count = search_results['search-continue']\n\n            if cont_count > max_results:\n                break\n\n        return id_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of properties on the current item .", "response": "def get_property_list(self):\n        \"\"\"\n        List of properties on the current item\n        :return: a list of WD property ID strings (Pxxxx).\n        \"\"\"\n        property_list = set()\n        for x in self.statements:\n            property_list.add(x.get_prop_nr())\n\n        return list(property_list)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __select_wd_item(self):\n        qid_list = set()\n        conflict_source = {}\n        if self.mrh:\n            exact_qid = self.mrh.mrt_qids['http://www.w3.org/2004/02/skos/core#exactMatch']\n            mrt_pid = self.mrh.mrt_pid\n        else:\n            # This is a `hack` for if initializing the mapping relation helper fails. We can't determine the\n            # mapping relation type PID or the exact match QID. If we set mrt_pid to \"Pxxx\", then no qualifier will\n            # ever match it (and exact_qid will never get checked), and so what happens is exactly what would\n            # happen if the statement had no mapping relation qualifiers\n            exact_qid = \"Q0\"\n            mrt_pid = \"PXXX\"\n\n        for statement in self.data:\n            wd_property = statement.get_prop_nr()\n\n            # only use this statement if mapping relation type is exact, or mrt is not specified\n            mrt_qualifiers = [q for q in statement.get_qualifiers() if q.get_prop_nr() == mrt_pid]\n            if (len(mrt_qualifiers) == 1) and (mrt_qualifiers[0].get_value() != int(exact_qid[1:])):\n                continue\n\n            # TODO: implement special treatment when searching for date/coordinate values\n            data_point = statement.get_value()\n            if isinstance(data_point, tuple):\n                data_point = data_point[0]\n\n            core_props = self.core_props\n            if wd_property in core_props:\n                tmp_qids = set()\n                # if mrt_pid is \"PXXX\", this is fine, because the part of the SPARQL query using it is optional\n                query = statement.sparql_query.format(mrt_pid=mrt_pid, pid=wd_property, value=data_point)\n                results = WDItemEngine.execute_sparql_query(query=query, endpoint=self.sparql_endpoint_url)\n\n                for i in results['results']['bindings']:\n                    qid = i['item_id']['value'].split('/')[-1]\n                    if ('mrt' not in i) or ('mrt' in i and i['mrt']['value'].split('/')[-1] == exact_qid):\n                        tmp_qids.add(qid)\n\n                qid_list.update(tmp_qids)\n\n                # Protocol in what property the conflict arises\n                if wd_property in conflict_source:\n                    conflict_source[wd_property].append(tmp_qids)\n                else:\n                    conflict_source[wd_property] = [tmp_qids]\n\n                if len(tmp_qids) > 1:\n                    raise ManualInterventionReqException(\n                        'More than one WD item has the same property value', wd_property, tmp_qids)\n\n        if len(qid_list) == 0:\n            self.create_new_item = True\n            return ''\n\n        if not __debug__:\n            print(qid_list)\n\n        unique_qids = set(qid_list)\n        if len(unique_qids) > 1:\n            raise ManualInterventionReqException('More than one WD item has the same property value',\n                                                 conflict_source, unique_qids)\n        elif len(unique_qids) == 1:\n            return list(unique_qids)[0]", "response": "Select the most likely WD item QID for the most likely one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __construct_claim_json(self):\n\n        def handle_qualifiers(old_item, new_item):\n            if not new_item.check_qualifier_equality:\n                old_item.set_qualifiers(new_item.get_qualifiers())\n\n        def is_good_ref(ref_block):\n\n            if len(WDItemEngine.databases) == 0:\n                WDItemEngine._init_ref_system()\n\n            prop_nrs = [x.get_prop_nr() for x in ref_block]\n            values = [x.get_value() for x in ref_block]\n            good_ref = True\n            prop_value_map = dict(zip(prop_nrs, values))\n\n            # if self.good_refs has content, use these to determine good references\n            if self.good_refs and len(self.good_refs) > 0:\n                found_good = True\n                for rblock in self.good_refs:\n\n                    if not all([k in prop_value_map for k, v in rblock.items()]):\n                        found_good = False\n\n                    if not all([v in prop_value_map[k] for k, v in rblock.items() if v]):\n                        found_good = False\n\n                    if found_good:\n                        return True\n\n                return False\n\n            # stated in, title, retrieved\n            ref_properties = ['P248', 'P1476', 'P813']  # 'P407' language of work,\n\n            for v in values:\n                if prop_nrs[values.index(v)] == 'P248':\n                    return True\n                elif v == 'P698':\n                    return True\n\n            for p in ref_properties:\n                if p not in prop_nrs:\n                    return False\n\n            for ref in ref_block:\n                pn = ref.get_prop_nr()\n                value = ref.get_value()\n\n                if pn == 'P248' and value not in WDItemEngine.databases and 'P854' not in prop_nrs:\n                    return False\n                elif pn == 'P248' and value in WDItemEngine.databases:\n                    db_props = WDItemEngine.databases[value]\n                    if not any([False if x not in prop_nrs else True for x in db_props]) and 'P854' not in prop_nrs:\n                        return False\n\n            return good_ref\n\n        def handle_references(old_item, new_item):\n            \"\"\"\n            Local function to handle references\n            :param old_item: An item containing the data as currently in WD\n            :type old_item: A child of WDBaseDataType\n            :param new_item: An item containing the new data which should be written to WD\n            :type new_item: A child of WDBaseDataType\n            \"\"\"\n            # stated in, title, language of work, retrieved, imported from\n            ref_properties = ['P248', 'P1476', 'P407', 'P813', 'P143']\n            new_references = new_item.get_references()\n            old_references = old_item.get_references()\n\n            if any([z.overwrite_references for y in new_references for z in y]) \\\n                    or sum(map(lambda z: len(z), old_references)) == 0 \\\n                    or self.global_ref_mode == 'STRICT_OVERWRITE':\n                old_item.set_references(new_references)\n\n            elif self.global_ref_mode == 'STRICT_KEEP' or new_item.statement_ref_mode == 'STRICT_KEEP':\n                pass\n\n            elif self.global_ref_mode == 'STRICT_KEEP_APPEND' or new_item.statement_ref_mode == 'STRICT_KEEP_APPEND':\n                old_references.extend(new_references)\n                old_item.set_references(old_references)\n\n            elif self.global_ref_mode == 'CUSTOM' or new_item.statement_ref_mode == 'CUSTOM':\n                self.ref_handler(old_item, new_item)\n\n            elif self.global_ref_mode == 'KEEP_GOOD' or new_item.statement_ref_mode == 'KEEP_GOOD':\n                keep_block = [False for x in old_references]\n                for count, ref_block in enumerate(old_references):\n                    stated_in_value = [x.get_value() for x in ref_block if x.get_prop_nr() == 'P248']\n                    if is_good_ref(ref_block):\n                        keep_block[count] = True\n\n                    new_ref_si_values = [x.get_value() if x.get_prop_nr() == 'P248' else None\n                                         for z in new_references for x in z]\n\n                    for si in stated_in_value:\n                        if si in new_ref_si_values:\n                            keep_block[count] = False\n\n                refs = [x for c, x in enumerate(old_references) if keep_block[c]]\n                refs.extend(new_references)\n                old_item.set_references(refs)\n\n        # sort the incoming data according to the WD property number\n        self.data.sort(key=lambda z: z.get_prop_nr().lower())\n\n        # collect all statements which should be deleted\n        statements_for_deletion = []\n        for item in self.data:\n            if item.get_value() == '' and isinstance(item, WDBaseDataType):\n                statements_for_deletion.append(item.get_prop_nr())\n\n        if self.create_new_item:\n            self.statements = copy.copy(self.data)\n        else:\n            for stat in self.data:\n                prop_nr = stat.get_prop_nr()\n\n                prop_data = [x for x in self.statements if x.get_prop_nr() == prop_nr]\n                prop_pos = [x.get_prop_nr() == prop_nr for x in self.statements]\n                prop_pos.reverse()\n                insert_pos = len(prop_pos) - (prop_pos.index(True) if any(prop_pos) else 0)\n\n                # If value should be appended, check if values exists, if not, append\n                if prop_nr in self.append_value:\n                    equal_items = [stat == x for x in prop_data]\n                    if True not in equal_items:\n                        self.statements.insert(insert_pos + 1, stat)\n                    else:\n                        # if item exists, modify rank\n                        current_item = prop_data[equal_items.index(True)]\n                        current_item.set_rank(stat.get_rank())\n                        handle_references(old_item=current_item, new_item=stat)\n                        handle_qualifiers(old_item=current_item, new_item=stat)\n                    continue\n\n                # set all existing values of a property for removal\n                for x in prop_data:\n                    # for deletion of single statements, do not set all others to delete\n                    if hasattr(stat, 'remove'):\n                        break\n                    elif x.get_id() and not hasattr(x, 'retain'):\n                        # keep statements with good references if keep_good_ref_statements is True\n                        if self.keep_good_ref_statements:\n                            if any([is_good_ref(r) for r in x.get_references()]):\n                                setattr(x, 'retain', '')\n                        else:\n                            setattr(x, 'remove', '')\n\n                match = []\n                for i in prop_data:\n                    if stat == i and hasattr(stat, 'remove'):\n                        match.append(True)\n                        setattr(i, 'remove', '')\n                    elif stat == i:\n                        match.append(True)\n                        setattr(i, 'retain', '')\n                        if hasattr(i, 'remove'):\n                            delattr(i, 'remove')\n                        handle_references(old_item=i, new_item=stat)\n                        handle_qualifiers(old_item=i, new_item=stat)\n\n                        i.set_rank(rank=stat.get_rank())\n                    # if there is no value, do not add an element, this is also used to delete whole properties.\n                    elif i.get_value():\n                        match.append(False)\n\n                if True not in match and not hasattr(stat, 'remove'):\n                    self.statements.insert(insert_pos + 1, stat)\n\n        # For whole property deletions, add remove flag to all statements which should be deleted\n        for item in copy.deepcopy(self.statements):\n            if item.get_prop_nr() in statements_for_deletion and item.get_id() != '':\n                setattr(item, 'remove', '')\n            elif item.get_prop_nr() in statements_for_deletion:\n                self.statements.remove(item)\n\n        # regenerate claim json\n        self.wd_json_representation['claims'] = {}\n        for stat in self.statements:\n            prop_nr = stat.get_prop_nr()\n            if prop_nr not in self.wd_json_representation['claims']:\n                self.wd_json_representation['claims'][prop_nr] = []\n            self.wd_json_representation['claims'][prop_nr].append(stat.get_json_representation())", "response": "Constructs the claim json from the properties of the item."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_label(self, lang='en'):\n        if self.fast_run:\n            return list(self.fast_run_container.get_language_data(self.wd_item_id, lang, 'label'))[0]\n        try:\n            return self.wd_json_representation['labels'][lang]['value']\n        except KeyError:\n            return ''", "response": "Returns the label for a certain language"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the label for a WD item in a certain language.", "response": "def set_label(self, label, lang='en'):\n        \"\"\"\n        Set the label for a WD item in a certain language\n        :param label: The description of the item in a certain language\n        :type label: str\n        :param lang: The language a label should be set for.\n        :type lang: str\n        :return: None\n        \"\"\"\n        if self.fast_run and not self.require_write:\n            self.require_write = self.fast_run_container.check_language_data(qid=self.wd_item_id,\n                                                                             lang_data=[label], lang=lang,\n                                                                             lang_data_type='label')\n            if self.require_write:\n                self.init_data_load()\n            else:\n                return\n\n        if 'labels' not in self.wd_json_representation:\n            self.wd_json_representation['labels'] = {}\n        self.wd_json_representation['labels'][lang] = {\n            'language': lang,\n            'value': label\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the aliases in a certain language", "response": "def get_aliases(self, lang='en'):\n        \"\"\"\n        Retrieve the aliases in a certain language\n        :param lang: The Wikidata language the description should be retrieved for\n        :return: Returns a list of aliases, an empty list if none exist for the specified language\n        \"\"\"\n        if self.fast_run:\n            return list(self.fast_run_container.get_language_data(self.wd_item_id, lang, 'aliases'))\n\n        alias_list = []\n        if 'aliases' in self.wd_json_representation and lang in self.wd_json_representation['aliases']:\n            for alias in self.wd_json_representation['aliases'][lang]:\n                alias_list.append(alias['value'])\n\n        return alias_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the aliases for a WD item.", "response": "def set_aliases(self, aliases, lang='en', append=True):\n        \"\"\"\n        set the aliases for a WD item\n        :param aliases: a list of strings representing the aliases of a WD item\n        :param lang: The language a description should be set for\n        :param append: If true, append a new alias to the list of existing aliases, else, overwrite. Default: True\n        :return: None\n        \"\"\"\n        if self.fast_run and not self.require_write:\n            self.require_write = self.fast_run_container.check_language_data(qid=self.wd_item_id,\n                                                                             lang_data=aliases, lang=lang,\n                                                                             lang_data_type='aliases')\n            if self.require_write:\n                self.init_data_load()\n            else:\n                return\n\n        if 'aliases' not in self.wd_json_representation:\n            self.wd_json_representation['aliases'] = {}\n\n        if not append or lang not in self.wd_json_representation['aliases']:\n            self.wd_json_representation['aliases'][lang] = []\n\n        for alias in aliases:\n            found = False\n            for current_aliases in self.wd_json_representation['aliases'][lang]:\n                if alias.strip().lower() != current_aliases['value'].strip().lower():\n                    continue\n                else:\n                    found = True\n                    break\n\n            if not found:\n                self.wd_json_representation['aliases'][lang].append({\n                    'language': lang,\n                    'value': alias\n                })"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_description(self, lang='en'):\n        if self.fast_run:\n            return list(self.fast_run_container.get_language_data(self.wd_item_id, lang, 'description'))[0]\n        if 'descriptions' not in self.wd_json_representation or lang not in self.wd_json_representation['descriptions']:\n            return ''\n        else:\n            return self.wd_json_representation['descriptions'][lang]['value']", "response": "Retrieve the description in a certain language"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the description for a WD item in a certain language.", "response": "def set_description(self, description, lang='en'):\n        \"\"\"\n        Set the description for a WD item in a certain language\n        :param description: The description of the item in a certain language\n        :type description: str\n        :param lang: The language a description should be set for.\n        :type lang: str\n        :return: None\n        \"\"\"\n        if self.fast_run and not self.require_write:\n            self.require_write = self.fast_run_container.check_language_data(qid=self.wd_item_id,\n                                                                             lang_data=[description], lang=lang,\n                                                                             lang_data_type='description')\n            if self.require_write:\n                self.init_data_load()\n            else:\n                return\n\n        if 'descriptions' not in self.wd_json_representation:\n            self.wd_json_representation['descriptions'] = {}\n        self.wd_json_representation['descriptions'][lang] = {\n            'language': lang,\n            'value': description\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset sitelinks to corresponding Wikipedia pages", "response": "def set_sitelink(self, site, title, badges=()):\n        \"\"\"\n        Set sitelinks to corresponding Wikipedia pages\n        :param site: The Wikipedia page a sitelink is directed to (e.g. 'enwiki')\n        :param title: The title of the Wikipedia page the sitelink is directed to\n        :param badges: An iterable containing Wikipedia badge strings.\n        :return:\n        \"\"\"\n        sitelink = {\n            'site': site,\n            'title': title,\n            'badges': badges\n        }\n        self.wd_json_representation['sitelinks'][site] = sitelink\n        self.sitelinks[site] = sitelink"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the WD item Json to WD and returns the WD QID on sucessful write.", "response": "def write(self, login, bot_account=True, edit_summary='', entity_type='item', property_datatype='string',\n              max_retries=10, retry_after=30):\n        \"\"\"\n        Writes the WD item Json to WD and after successful write, updates the object with new ids and hashes generated\n        by WD. For new items, also returns the new QIDs.\n        :param login: a instance of the class PBB_login which provides edit-cookies and edit-tokens\n        :param bot_account: Tell the Wikidata API whether the script should be run as part of a bot account or not.\n        :type bot_account: bool\n        :param edit_summary: A short (max 250 characters) summary of the purpose of the edit. This will be displayed as\n            the revision summary of the Wikidata item.\n        :type edit_summary: str\n        :param entity_type: Decides wether the object will become an item (default) or a property (with 'property')\n        :type entity_type: str\n        :param property_datatype: When payload_type is 'property' then this parameter set the datatype for the property\n        :type property_datatype: str\n        :param max_retries: If api request fails due to rate limiting, maxlag, or readonly mode, retry up to\n        `max_retries` times\n        :type max_retries: int\n        :param retry_after: Number of seconds to wait before retrying request (see max_retries)\n        :type retry_after: int\n        :return: the WD QID on sucessful write\n        \"\"\"\n        if not self.require_write:\n            return self.wd_item_id\n\n        if entity_type == 'property':\n            self.wd_json_representation['datatype'] = property_datatype\n            if 'sitelinks' in self.wd_json_representation:\n                del self.wd_json_representation['sitelinks']\n\n        payload = {\n            'action': 'wbeditentity',\n            'data': json.JSONEncoder().encode(self.wd_json_representation),\n            'format': 'json',\n            'token': login.get_edit_token(),\n            'summary': edit_summary,\n            'maxlag': config['MAXLAG']\n        }\n        headers = {\n            'content-type': 'application/x-www-form-urlencoded',\n            'charset': 'utf-8'\n        }\n\n        if bot_account:\n            payload.update({'bot': ''})\n\n        if self.create_new_item:\n            payload.update({u'new': entity_type})\n        else:\n            payload.update({u'id': self.wd_item_id})\n\n        try:\n            json_data = self.mediawiki_api_call('POST', self.mediawiki_api_url, session=login.get_session(),\n                                                max_retries=max_retries, retry_after=retry_after,\n                                                headers=headers, data=payload)\n\n            if 'error' in json_data and 'messages' in json_data['error']:\n                error_msg_names = set(x.get('name') for x in json_data[\"error\"]['messages'])\n                if 'wikibase-validator-label-with-description-conflict' in error_msg_names:\n                    raise NonUniqueLabelDescriptionPairError(json_data)\n                else:\n                    raise WDApiError(json_data)\n            elif 'error' in json_data.keys():\n                raise WDApiError(json_data)\n        except Exception:\n            print('Error while writing to Wikidata')\n            raise\n\n        # after successful write, update this object with latest json, QID and parsed data types.\n        self.create_new_item = False\n        self.wd_item_id = json_data['entity']['id']\n        self.parse_wd_json(wd_json=json_data['entity'])\n        self.data = []\n        if \"success\" in json_data and \"entity\" in json_data and \"lastrevid\" in json_data[\"entity\"]:\n            self.lastrevid = json_data[\"entity\"][\"lastrevid\"]\n\n        return self.wd_item_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls the wikidata API.", "response": "def mediawiki_api_call(method, mediawiki_api_url='https://www.wikidata.org/w/api.php',\n                           session=None, max_retries=10, retry_after=30, **kwargs):\n        \"\"\"\n        :param method: 'GET' or 'POST'\n        :param mediawiki_api_url:\n        :param session: If a session is passed, it will be used. Otherwise a new requests session is created\n        :param max_retries: If api request fails due to rate limiting, maxlag, or readonly mode, retry up to\n        `max_retries` times\n        :type max_retries: int\n        :param retry_after: Number of seconds to wait before retrying request (see max_retries)\n        :type retry_after: int\n        :param kwargs: Passed to requests.request\n        :return:\n        \"\"\"\n        response = None\n        session = session if session else requests.session()\n        for n in range(max_retries):\n            try:\n                response = session.request(method, mediawiki_api_url, **kwargs)\n            except requests.exceptions.ConnectionError as e:\n                print(\"Connection error: {}. Sleeping for {} seconds.\".format(e, retry_after))\n                time.sleep(retry_after)\n                continue\n            if response.status_code == 503:\n                print(\"service unavailable. sleeping for {} seconds\".format(retry_after))\n                time.sleep(retry_after)\n                continue\n\n            response.raise_for_status()\n            json_data = response.json()\n            \"\"\"\n            wikidata api response has code = 200 even if there are errors.\n            rate limit doesn't return HTTP 429 either. may in the future\n            https://phabricator.wikimedia.org/T172293\n            \"\"\"\n            if 'error' in json_data:\n                # rate limiting\n                error_msg_names = set()\n                if 'messages' in json_data['error']:\n                    error_msg_names = set(x.get('name') for x in json_data[\"error\"]['messages'])\n                if 'actionthrottledtext' in error_msg_names:\n                    sleep_sec = int(response.headers.get('retry-after', retry_after))\n                    print(\"{}: rate limited. sleeping for {} seconds\".format(datetime.datetime.utcnow(), sleep_sec))\n                    time.sleep(sleep_sec)\n                    continue\n\n                # maxlag\n                if 'code' in json_data['error'] and json_data['error']['code'] == 'maxlag':\n                    sleep_sec = json_data['error'].get('lag', retry_after)\n                    print(\"{}: maxlag. sleeping for {} seconds\".format(datetime.datetime.utcnow(), sleep_sec))\n                    time.sleep(sleep_sec)\n                    continue\n\n                # readonly\n                if 'code' in json_data['error'] and json_data['error']['code'] == 'readonly':\n                    print('Wikidata currently is in readonly mode, waiting for {} seconds'.format(retry_after))\n                    time.sleep(retry_after)\n                    continue\n\n            # there is no error or waiting. break out of this loop and parse response\n            break\n        else:\n            # the first time I've ever used for - else!!\n            # else executes if the for loop completes normally. i.e. does not encouter a `break`\n            # in this case, that means it tried this api call 10 times\n            raise WDApiError(response.json() if response else dict())\n\n        return json_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging a message to the log file.", "response": "def log(cls, level, message):\n        \"\"\"\n        :param level: The log level as in the Python logging documentation, 5 different possible values with increasing\n         severity\n        :type level: String of value 'DEBUG', 'INFO', 'WARNING', 'ERROR' or 'CRITICAL'.\n        :param message: The logging data which should be written to the log file. In order to achieve a csv-file\n         compatible format, all fields must be separated by a colon. Furthermore, all strings which could contain\n         colons, spaces or other special characters must be enclosed in double-quotes.\n         e.g. '{main_data_id}, \"{exception_type}\", \"{message}\", {wd_id}, {duration}'.format(\n                        main_data_id=<main_id>,\n                        exception_type=<excpetion type>,\n                        message=<exception message>,\n                        wd_id=<wikidata id>,\n                        duration=<duration of action>\n        :type message: str\n        \"\"\"\n        if cls.logger is None:\n            cls.setup_logging()\n\n        log_levels = {'DEBUG': logging.DEBUG, 'ERROR': logging.ERROR, 'INFO': logging.INFO, 'WARNING': logging.WARNING,\n                      'CRITICAL': logging.CRITICAL}\n\n        cls.logger.log(level=log_levels[level], msg=message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_sparql_query(query, prefix=None, endpoint='https://query.wikidata.org/sparql',\n                             user_agent=config['USER_AGENT_DEFAULT'], as_dataframe=False):\n        \"\"\"\n        Static method which can be used to execute any SPARQL query\n        :param prefix: The URI prefixes required for an endpoint, default is the Wikidata specific prefixes\n        :param query: The actual SPARQL query string\n        :param endpoint: The URL string for the SPARQL endpoint. Default is the URL for the Wikidata SPARQL endpoint\n        :param user_agent: Set a user agent string for the HTTP header to let the WDQS know who you are.\n        :param as_dataframe: Return result as pandas dataframe\n        :type user_agent: str\n        :return: The results of the query are returned in JSON format\n        \"\"\"\n\n        if not endpoint:\n            endpoint = 'https://query.wikidata.org/sparql'\n\n        if prefix:\n            query = prefix + '\\n' + query\n\n        params = {\n            'query': '#Tool: PBB_core fastrun\\n' + query,\n            'format': 'json'\n        }\n\n        headers = {\n            'Accept': 'application/sparql-results+json',\n            'User-Agent': user_agent\n        }\n        response = requests.get(endpoint, params=params, headers=headers)\n        response.raise_for_status()\n        results = response.json()\n\n        if as_dataframe:\n            return WDItemEngine._sparql_query_result_to_df(results)\n        else:\n            return results", "response": "Execute a SPARQL query and return the results as a pandas dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_shex_manifest(manifest_url, index=0, debug=False):\n        manifest = json.loads(manifest_url, debug=False)\n        manifest_results = dict()\n        for case in manifest[index]:\n            if case.data.startswith(\"Endpoint:\"):\n                sparql_endpoint = case.data.replace(\"Endpoint: \", \"\")\n                schema = requests.get(case.schemaURL).text\n                shex = ShExC(schema).schema\n                evaluator = ShExEvaluator(schema=shex, debug=debug)\n                sparql_query = case.queryMap.replace(\"SPARQL '''\", \"\").replace(\"'''@START\", \"\")\n\n                df = WDItemEngine.execute_sparql_query(sparql_query)\n                for row in df[\"results\"][\"bindings\"]:\n                    wdid = row[\"item\"][\"value\"]\n                    if wdid not in  manifest_results.keys():\n                        manifest_results[wdid] = dict()\n                    slurpeddata = SlurpyGraph(sparql_endpoint)\n                    results = evaluator.evaluate(rdf=slurpeddata, focus=wdid, debug=debug)\n                    for result in results:\n                        if result.result:\n                            manifest_results[wdid][\"status\"] = \"CONFORMS\"\n                        else:\n                            manifest_results[wdid][\"status\"] = \"DOES NOT CONFORM\"\n                            manifest_results[wdid][\"debug\"] = result.reason\n        return manifest_results", "response": "This method runs a shex conformance test for all the ingredients in a manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_items(from_id, to_id, login_obj, mediawiki_api_url='https://www.wikidata.org/w/api.php',\n                    ignore_conflicts='', user_agent=config['USER_AGENT_DEFAULT']):\n        \"\"\"\n        A static method to merge two Wikidata items\n        :param from_id: The QID which should be merged into another item\n        :type from_id: string with 'Q' prefix\n        :param to_id: The QID into which another item should be merged\n        :type to_id: string with 'Q' prefix\n        :param login_obj: The object containing the login credentials and cookies\n        :type login_obj: instance of PBB_login.WDLogin\n        :param mediawiki_api_url: The MediaWiki url which should be used\n        :type mediawiki_api_url: str\n        :param ignore_conflicts: A string with the values 'description', 'statement' or 'sitelink', separated\n                by a pipe ('|') if using more than one of those.\n        :type ignore_conflicts: str\n        \"\"\"\n        url = mediawiki_api_url\n\n        headers = {\n            'content-type': 'application/x-www-form-urlencoded',\n            'charset': 'utf-8',\n            'User-Agent': user_agent\n        }\n\n        params = {\n            'action': 'wbmergeitems',\n            'fromid': from_id,\n            'toid': to_id,\n            'token': login_obj.get_edit_token(),\n            'format': 'json',\n            'bot': '',\n            'ignoreconflicts': ignore_conflicts\n        }\n\n        try:\n            # TODO: should we retry this?\n            merge_reply = requests.post(url=url, data=params, headers=headers, cookies=login_obj.get_edit_cookie())\n            merge_reply.raise_for_status()\n\n            if 'error' in merge_reply.json():\n                raise MergeError(merge_reply.json())\n\n        except requests.HTTPError as e:\n            print(e)\n            # TODO: should we return this?\n            return {'error': 'HTTPError'}\n\n        return merge_reply.json()", "response": "This method is used to merge two items into one item."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes items from the Wikidata moderators.", "response": "def delete_items(item_list, reason, login, mediawiki_api_url='https://www.wikidata.org/w/api.php',\n                     user_agent=config['USER_AGENT_DEFAULT']):\n        \"\"\"\n        Takes a list of items and posts them for deletion by Wikidata moderators, appends at the end of the deletion\n        request page.\n        :param item_list: a list of QIDs which should be deleted\n        :type item_list: list\n        :param reason: short text about the reason for the deletion request\n        :type reason: str\n        :param login: A WDI login object which contains username and password the edit should be performed with.\n        :type login: wdi_login.WDLogin\n        \"\"\"\n\n        url = mediawiki_api_url\n        bulk_deletion_string = '\\n==Bulk deletion request==\\n'\n        bulk_deletion_string += '{{{{subst:Rfd group | {0} | reason = {1} }}}}'.format(' | '.join(item_list), reason)\n\n        # get page text\n        params = {\n            'action': 'query',\n            'titles': 'Wikidata:Requests_for_deletions',\n            'prop': 'revisions',\n            'rvprop': 'content',\n            'format': 'json'\n        }\n\n        headers = {\n            'User-Agent': user_agent\n        }\n\n        page_text = [x['revisions'][0]['*']\n                     for x in requests.get(url=url, params=params, headers=headers).json()['query']['pages'].values()][\n            0]\n\n        if not login:\n            print(page_text)\n            print(bulk_deletion_string)\n        else:\n            # Append new deletion request to existing list of deletions being processed\n            params = {\n                'action': 'edit',\n                'title': 'Portal:Gene_Wiki/Quick_Links',\n                'section': '0',\n                'text': page_text + bulk_deletion_string,\n                'token': login.get_edit_token(),\n                'format': 'json'\n            }\n\n            r = requests.post(url=url, data=params, cookies=login.get_edit_cookie(), headers=headers)\n\n            print(r.json())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wikibase_item_engine_factory(cls, mediawiki_api_url, sparql_endpoint_url, name='LocalItemEngine'):\n\n        class SubCls(cls):\n            def __init__(self, *args, **kwargs):\n                kwargs['mediawiki_api_url'] = mediawiki_api_url\n                kwargs['sparql_endpoint_url'] = sparql_endpoint_url\n                super(SubCls, self).__init__(*args, **kwargs)\n\n        SubCls.__name__ = name\n        return SubCls", "response": "Returns a WDItemEngine class with the specified arguments set for a different Wikidata instance than a Wikidata instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the reference mode for a statement always overrides the global reference state.", "response": "def statement_ref_mode(self, value):\n        \"\"\"Set the reference mode for a statement, always overrides the global reference state.\"\"\"\n        valid_values = ['STRICT_KEEP', 'STRICT_KEEP_APPEND', 'STRICT_OVERWRITE', 'KEEP_GOOD', 'CUSTOM']\n        if value not in valid_values:\n            raise ValueError('Not an allowed reference mode, allowed values {}'.format(' '.join(valid_values)))\n\n        self._statement_ref_mode = value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_statement(cls, prop_nr):\n        return cls(value='', snak_type='value', data_type='', is_reference=False, is_qualifier=False, references=[],\n                   qualifiers=[], rank='', prop_nr=prop_nr, check_qualifier_equality=True)", "response": "This is an alternative constructor for creating a WDBaseDataType object that is used to delete a WD property."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef equals(self, that, include_ref=False, fref=None):\n        if not include_ref:\n            # return the result of WDBaseDataType.__eq__, which is testing for equality of value and qualifiers\n            return self == that\n        if include_ref and self != that:\n            return False\n        if include_ref and fref is None:\n            fref = WDBaseDataType.refs_equal\n        return fref(self, that)", "response": "Tests for equality of two statements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntesting for exactly identical references", "response": "def refs_equal(olditem, newitem):\n        \"\"\"\n        tests for exactly identical references\n        \"\"\"\n        oldrefs = olditem.references\n        newrefs = newitem.references\n\n        ref_equal = lambda oldref, newref: True if (len(oldref) == len(newref)) and all(\n            x in oldref for x in newref) else False\n        if len(oldrefs) == len(newrefs) and all(\n                any(ref_equal(oldref, newref) for oldref in oldrefs) for newref in newrefs):\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef try_write(wd_item, record_id, record_prop, login, edit_summary='', write=True):\n    if wd_item.require_write:\n        if wd_item.create_new_item:\n            msg = \"CREATE\"\n        else:\n            msg = \"UPDATE\"\n    else:\n        msg = \"SKIP\"\n\n    try:\n        if write:\n            wd_item.write(login=login, edit_summary=edit_summary)\n        wdi_core.WDItemEngine.log(\"INFO\", format_msg(record_id, record_prop, wd_item.wd_item_id, msg) + \";\" + str(\n            wd_item.lastrevid))\n    except wdi_core.WDApiError as e:\n        print(e)\n        wdi_core.WDItemEngine.log(\"ERROR\",\n                                  format_msg(record_id, record_prop, wd_item.wd_item_id, json.dumps(e.wd_error_msg),\n                                             type(e)))\n        return e\n    except Exception as e:\n        print(e)\n        wdi_core.WDItemEngine.log(\"ERROR\", format_msg(record_id, record_prop, wd_item.wd_item_id, str(e), type(e)))\n        return e\n\n    return True", "response": "Try to write a PBB_core item. Log if item was created updated or skipped."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat message for logging", "response": "def format_msg(external_id, external_id_prop, wdid, msg, msg_type=None, delimiter=\";\"):\n    \"\"\"\n    Format message for logging\n    :return: str\n    \"\"\"\n    fmt = ('{}' + delimiter) * 4 + '{}'  # '{};{};{};{};{}'\n    d = {'external_id': external_id,\n         'external_id_prop': external_id_prop,\n         'wdid': wdid,\n         'msg': msg,\n         'msg_type': msg_type}\n    for k, v in d.items():\n        if isinstance(v, str) and delimiter in v and '\"' in v:\n            v = v.replace('\"', \"'\")\n        if isinstance(v, str) and delimiter in v:\n            d[k] = '\"' + v + '\"'\n\n    s = fmt.format(d['external_id'], d['external_id_prop'], d['wdid'], d['msg'], d['msg_type'])\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prop2qid(prop, value, endpoint='https://query.wikidata.org/sparql'):\n    arguments = '?item wdt:{} \"{}\"'.format(prop, value)\n    query = 'SELECT * WHERE {{{}}}'.format(arguments)\n    results = wdi_core.WDItemEngine.execute_sparql_query(query, endpoint=endpoint)\n    result = results['results']['bindings']\n    if len(result) == 0:\n        # not found\n        return None\n    elif len(result) > 1:\n        raise ValueError(\"More than one wikidata ID found for {} {}: {}\".format(prop, value, result))\n    else:\n        return result[0]['item']['value'].split(\"/\")[-1]", "response": "Lookup a wikidata item ID from a property and string value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef id_mapper(prop, filters=None, raise_on_duplicate=False, return_as_set=False, prefer_exact_match=False,\n              endpoint='https://query.wikidata.org/sparql'):\n    \"\"\"\n    Get all wikidata ID <-> prop <-> value mappings\n    Example: id_mapper(\"P352\") -> { 'A0KH68': 'Q23429083',\n                                     'Q5ZWJ4': 'Q22334494',\n                                     'Q53WF2': 'Q21766762', .... }\n    Optional filters can filter query results.\n    Example (get all uniprot to wdid, where taxon is human): id_mapper(\"P352\",((\"P703\", \"Q15978631\"),))\n    :param prop: wikidata property\n    :type prop: str\n    :param filters: list of tuples, where the first item is a property, second is a value\n    :param raise_on_duplicate: If an ID is found on more than one wikidata item, what action to take?\n        This is equivalent to the Distinct values constraint. e.g.: http://tinyurl.com/ztpncyb\n        Note that a wikidata item can have more than one ID. This is not checked for\n        True: raise ValueError\n        False: only one of the values is kept if there are duplicates (unless return_as_set if True)\n    :type raise_on_duplicate: bool\n    :param return_as_set: If True, all values in the returned dict will be a set of strings\n    :type return_as_set: bool\n    :param prefer_exact_match: If True, the mapping relation type qualifier will be queried. If an ID mapping has\n    multiple values, the ones marked as an 'exactMatch' will be returned while the others discarded. If none have\n    an exactMatch qualifier, all will be returned. If multiple has 'exactMatch', they will not be discarded.\n    https://www.wikidata.org/wiki/Property:P4390\n    :type prefer_exact_match: bool\n\n\n    If `raise_on_duplicate` is False and `return_as_set` is True, the following can be returned:\n    { 'A0KH68': {'Q23429083'}, 'B023F44': {'Q237623', 'Q839742'} }\n\n    :return: dict\n\n    \"\"\"\n    query = \"SELECT ?id ?item ?mrt WHERE {\"\n    query += \"?item p:{} ?s .\\n?s ps:{} ?id .\\n\".format(prop, prop)\n    query += \"OPTIONAL {?s pq:P4390 ?mrt}\\n\"\n    if filters:\n        for f in filters:\n            query += \"?item wdt:{} wd:{} .\\n\".format(f[0], f[1])\n    query = query + \"}\"\n    results = wdi_core.WDItemEngine.execute_sparql_query(query, endpoint=endpoint)['results']['bindings']\n    results = [{k: v['value'] for k, v in x.items()} for x in results]\n    for r in results:\n        r['item'] = r['item'].split('/')[-1]\n        if 'mrt' in r:\n            r['mrt'] = r['mrt'].split('/')[-1]\n    if not results:\n        return None\n\n    if prefer_exact_match:\n        df = pd.DataFrame(results)\n        if 'mrt' not in df:\n            df['mrt'] = ''\n        df.mrt = df.mrt.fillna('')\n        df['keep'] = True\n\n        # check if a QID has more than one extID\n        # i.e single value constraint\n        # example: https://www.wikidata.org/w/index.php?title=Q3840916&oldid=645203228#P486 (D018311)\n        # example 2: https://www.wikidata.org/w/index.php?title=Q388113&oldid=648588555#P486 (D000037, D000033)\n        df.sort_values(\"item\", inplace=True)\n        dupe_df = df[df.duplicated(subset=[\"item\"], keep=False)]\n        for item, subdf in dupe_df.groupby(\"item\"):\n            # if there is one with exact match, take it, otherwise, skip\n            if sum(subdf.mrt == RELATIONS['exact']) == 1:\n                df.loc[(df.item == item) & (df.mrt != RELATIONS['exact']), 'keep'] = False\n\n                # check if a extID has more than one QID\n                # example: https://www.wikidata.org/w/index.php?title=Q846227&oldid=648565663#P486\n                # https://www.wikidata.org/w/index.php?title=Q40207875&oldid=648565770#P486\n        df.sort_values(\"id\", inplace=True)\n        dupe_df = df[df.duplicated(subset=[\"id\"], keep=False)]\n        for ext_id, subdf in dupe_df.groupby(\"id\"):\n            # if there is one with exact match, take it, otherwise, skip\n            if sum(subdf.mrt == RELATIONS['exact']) == 1:\n                df.loc[(df.id == ext_id) & (df.mrt != RELATIONS['exact']), 'keep'] = False\n\n        df = df[df.keep]\n        results = df.to_dict(\"records\")\n\n    id_qid = defaultdict(set)\n    for r in results:\n        id_qid[r['id']].add(r['item'])\n    dupe = {k: v for k, v in id_qid.items() if len(v) > 1}\n    if raise_on_duplicate and dupe:\n        raise ValueError(\"duplicate ids: {}\".format(dupe))\n\n    if return_as_set:\n        return dict(id_qid)\n    else:\n        return {x['id']: x['item'] for x in results}", "response": "Get all wikidata ID mappings from a property to a value mapping."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_retrieved_if_new_multiple_refs(olditem, newitem, days=180, retrieved_pid='P813'):\n\n    def is_equal_not_retrieved(oldref, newref):\n        \"\"\"\n        Return True if the oldref == newref, NOT including any \"retrieved\" statements\n        :param oldref:\n        :param newref:\n        :return:\n        \"\"\"\n        if len(oldref) != len(newref):\n            return False\n        oldref_minus_retrieved = [x for x in oldref if x.get_prop_nr() != retrieved_pid]\n        newref_minus_retrieved = [x for x in newref if x.get_prop_nr() != retrieved_pid]\n        if not all(x in oldref_minus_retrieved for x in newref_minus_retrieved):\n            return False\n        oldref_retrieved = [x for x in oldref if x.get_prop_nr() == retrieved_pid]\n        newref_retrieved = [x for x in newref if x.get_prop_nr() == retrieved_pid]\n        if (len(newref_retrieved) != len(oldref_retrieved)):\n            return False\n        return True\n\n    def ref_overwrite(oldref, newref, days):\n        \"\"\"\n        If the newref is the same as the oldref except the retrieved date is `days` newer, return True\n                                                       the retrieved date is NOT `days` newer, return False\n        the refs are different, return True\n        \"\"\"\n        if len(oldref) != len(newref):\n            return True\n        oldref_minus_retrieved = [x for x in oldref if x.get_prop_nr() != retrieved_pid]\n        newref_minus_retrieved = [x for x in newref if x.get_prop_nr() != retrieved_pid]\n        if not all(x in oldref_minus_retrieved for x in newref_minus_retrieved):\n            return True\n        oldref_retrieved = [x for x in oldref if x.get_prop_nr() == retrieved_pid]\n        newref_retrieved = [x for x in newref if x.get_prop_nr() == retrieved_pid]\n        if (len(newref_retrieved) != len(oldref_retrieved)) or not (\n                        len(newref_retrieved) == len(oldref_retrieved) == 1):\n            return True\n        datefmt = '+%Y-%m-%dT%H:%M:%SZ'\n        retold = list([datetime.strptime(r.get_value()[0], datefmt) for r in oldref if r.get_prop_nr() == retrieved_pid])[0]\n        retnew = list([datetime.strptime(r.get_value()[0], datefmt) for r in newref if r.get_prop_nr() == retrieved_pid])[0]\n        return (retnew - retold).days >= days\n\n    newrefs = newitem.references\n    oldrefs = olditem.references\n\n    found_mate = [False] * len(newrefs)\n    for new_n, newref in enumerate(newrefs):\n        for old_n, oldref in enumerate(oldrefs):\n            if is_equal_not_retrieved(oldref, newref):\n                found_mate[new_n] = True\n                if ref_overwrite(oldref, newref, days):\n                    oldrefs[old_n] = newref\n    for f_idx, f in enumerate(found_mate):\n        if not f:\n            oldrefs.append(newrefs[f_idx])", "response": "Update the retrieved state if the new item has multiple references."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_mrt(self, s, mrt: str):\n        valid_mrts_abv = self.ABV_MRT.keys()\n        valid_mrts_uri = self.ABV_MRT.values()\n        if mrt in valid_mrts_abv:\n            mrt_uri = self.ABV_MRT[mrt]\n        elif mrt in valid_mrts_uri:\n            mrt_uri = mrt\n        else:\n            raise ValueError(\"mrt must be one of {}, found {}\".format(valid_mrts_abv, mrt))\n        mrt_qid = self.mrt_qids[mrt_uri]\n\n        q = wdi_core.WDItemID(mrt_qid, self.mrt_pid, is_qualifier=True)\n        s.qualifiers.append(q)\n        return s", "response": "accepts a statement and adds a qualifer setting the mrt\n        modifies s in place\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_edit_credentials(self):\n        params = {\n            'action': 'query',\n            'meta': 'tokens',\n            'format': 'json'\n        }\n        response = self.s.get(self.base_url, params=params)\n        self.edit_token = response.json()['query']['tokens']['csrftoken']\n\n        return self.s.cookies", "response": "Request an edit token and update the cookie jar in order to add the session cookie jar"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_edit_token(self):\n        if not self.edit_token or (time.time() - self.instantiation_time) > self.token_renew_period:\n            self.generate_edit_credentials()\n            self.instantiation_time = time.time()\n\n        return self.edit_token", "response": "Returns the edit token from an instance of WDLogin\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef continue_oauth(self, oauth_callback_data=None):\n        self.response_qs = oauth_callback_data\n\n        if not self.response_qs:\n            webbrowser.open(self.redirect)\n            self.response_qs = input(\"Callback URL: \")\n\n        # input the url from redirect after authorization\n        response_qs = self.response_qs.split(b'?')[-1]\n\n        # Step 3: Complete -- obtain authorized key/secret for \"resource owner\"\n        access_token = self.handshaker.complete(self.request_token, response_qs)\n\n        # input the access token to return a csrf (edit) token\n        auth1 = OAuth1(self.consumer_token.key,\n                       client_secret=self.consumer_token.secret,\n                       resource_owner_key=access_token.key,\n                       resource_owner_secret=access_token.secret)\n\n        self.s.auth = auth1\n        self.generate_edit_credentials()", "response": "This method is called by the OAuth server to continue the OAuth process."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_app_dict(site, request, label=None):\n    app_dict = {}\n\n    if label:\n        models = {\n            m: m_a for m, m_a in site._registry.items()\n            if m._meta.app_label == label\n        }\n    else:\n        models = site._registry\n\n    for model, model_admin in models.items():\n        app_label = model._meta.app_label\n\n        has_module_perms = model_admin.has_module_permission(request)\n        if not has_module_perms:\n            continue\n\n        perms = model_admin.get_model_perms(request)\n\n        # Check whether user has any perm for this module.\n        # If so, add the module to the model_list.\n        if True not in perms.values():\n            continue\n\n        info = (app_label, model._meta.model_name)\n        model_dict = {\n            'name': capfirst(model._meta.verbose_name_plural),\n            'object_name': model._meta.object_name,\n            'perms': perms,\n        }\n        if perms.get('change'):\n            try:\n                model_dict['admin_url'] = reverse('admin:%s_%s_changelist' % info, current_app=site.name)\n            except NoReverseMatch:\n                pass\n        if perms.get('add'):\n            try:\n                model_dict['add_url'] = reverse('admin:%s_%s_add' % info, current_app=site.name)\n            except NoReverseMatch:\n                pass\n\n        if app_label in app_dict:\n            app_dict[app_label]['models'].append(model_dict)\n        else:\n            app_dict[app_label] = {\n                'name': apps.get_app_config(app_label).verbose_name,\n                'app_label': app_label,\n                'app_url': reverse(\n                    'admin:app_list',\n                    kwargs={'app_label': app_label},\n                    current_app=site.name,\n                ),\n                'has_module_perms': has_module_perms,\n                'models': [model_dict],\n            }\n\n    if label:\n        return app_dict.get(label)\n    return app_dict", "response": "Builds the app dictionary. Takes an optional label parameter to filter the app dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_app_list(site, request):\n    app_dict = _build_app_dict(site, request)\n\n    # Sort the apps alphabetically.\n    app_list = sorted(app_dict.values(), key=lambda x: x['name'].lower())\n\n    # Sort the models alphabetically within each app.\n    for app in app_list:\n        app['models'].sort(key=lambda x: x['name'])\n\n    return app_list", "response": "Returns a sorted list of all the installed apps that have been\n    registered in this site."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing language data store.", "response": "def init_language_data(self, lang, lang_data_type):\n        \"\"\"\n        Initialize language data store\n        :param lang: language code\n        :param lang_data_type: 'label', 'description' or 'aliases'\n        :return: None\n        \"\"\"\n        if lang not in self.loaded_langs:\n            self.loaded_langs[lang] = {}\n\n        if lang_data_type not in self.loaded_langs[lang]:\n            result = self._query_lang(lang=lang, lang_data_type=lang_data_type)\n            data = self._process_lang(result)\n            self.loaded_langs[lang].update({lang_data_type: data})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets language data for specified qid", "response": "def get_language_data(self, qid, lang, lang_data_type):\n        \"\"\"\n        get language data for specified qid\n        :param qid:\n        :param lang: language code\n        :param lang_data_type: 'label', 'description' or 'aliases'\n        :return: list of strings\n        If nothing is found:\n            If lang_data_type == label: returns ['']\n            If lang_data_type == description: returns ['']\n            If lang_data_type == aliases: returns []\n        \"\"\"\n        self.init_language_data(lang, lang_data_type)\n\n        current_lang_data = self.loaded_langs[lang][lang_data_type]\n        all_lang_strings = current_lang_data.get(qid, [])\n        if not all_lang_strings and lang_data_type in {'label', 'description'}:\n            all_lang_strings = ['']\n        return all_lang_strings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_language_data(self, qid, lang_data, lang, lang_data_type):\n        all_lang_strings = set(x.strip().lower() for x in self.get_language_data(qid, lang, lang_data_type))\n\n        for s in lang_data:\n            if s.strip().lower() not in all_lang_strings:\n                print('fastrun failed at label: {}, string: {}'.format(lang_data_type, s))\n                return True\n\n        return False", "response": "Method to check if certain language data exists as a label description or aliases"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_query_results(self, r, prop_nr):\n        prop_dt = self.get_prop_datatype(prop_nr)\n        for i in r:\n            for value in {'item', 'sid', 'pq', 'pr', 'ref'}:\n                if value in i:\n                    # these are always URIs for the local wikibase\n                    i[value] = i[value]['value'].split('/')[-1]\n\n            # make sure datetimes are formatted correctly.\n            # the correct format is '+%Y-%m-%dT%H:%M:%SZ', but is sometimes missing the plus??\n            # some difference between RDF and xsd:dateTime that I don't understand\n            for value in {'v', 'qval', 'rval'}:\n                if value in i:\n                    if i[value].get(\"datatype\") == 'http://www.w3.org/2001/XMLSchema#dateTime' and not \\\n                            i[value]['value'][0] in '+-':\n                        # if it is a dateTime and doesn't start with plus or minus, add a plus\n                        i[value]['value'] = '+' + i[value]['value']\n\n            # these three ({'v', 'qval', 'rval'}) are values that can be any data type\n            # strip off the URI if they are wikibase-items\n            if 'v' in i:\n                if i['v']['type'] == 'uri' and prop_dt == 'wikibase-item':\n                    i['v'] = i['v']['value'].split('/')[-1]\n                else:\n                    i['v'] = i['v']['value']\n\n                # Note: no-value and some-value don't actually show up in the results here\n                # see for example: select * where { wd:Q7207 p:P40 ?c . ?c ?d ?e }\n                if type(i['v']) is not dict:\n                    self.rev_lookup[i['v']].add(i['item'])\n\n            # handle qualifier value\n            if 'qval' in i:\n                qual_prop_dt = self.get_prop_datatype(prop_nr=i['pq'])\n                if i['qval']['type'] == 'uri' and qual_prop_dt == 'wikibase-item':\n                    i['qval'] = i['qval']['value'].split('/')[-1]\n                else:\n                    i['qval'] = i['qval']['value']\n\n            # handle reference value\n            if 'rval' in i:\n                ref_prop_dt = self.get_prop_datatype(prop_nr=i['pr'])\n                if i['rval']['type'] == 'uri' and ref_prop_dt == 'wikibase-item':\n                    i['rval'] = i['rval']['value'].split('/')[-1]\n                else:\n                    i['rval'] = i['rval']['value']", "response": "This function formats the query results of a Wikimedia item into a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the pid for the property in this wikidata instance", "response": "def get_pid(self, uri):\n        \"\"\"\n        Get the pid for the property in this wikibase instance ( the one at `sparql_endpoint_url` ),\n         that corresponds to (i.e. has the equivalent property) `uri`\n        \"\"\"\n        # if the wikibase is wikidata, and we give a wikidata uri or a PID with no URI specified:\n        if (self.sparql_endpoint_url == 'https://query.wikidata.org/sparql' and\n                (uri.startswith(\"P\") or uri.startswith(\"http://www.wikidata.org/entity/\"))):\n            # don't look up anything, just return the same value\n            return uri\n        if uri.startswith(\"P\"):\n            uri = \"http://www.wikidata.org/entity/\" + uri\n        return self.URI_PID[uri]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the local item QID for a Wikidata item that has a certain property -> value.", "response": "def prop2qid(self, prop, value):\n        \"\"\"\n        Lookup the local item QID for a Wikidata item that has a certain `prop` -> `value`\n        in the case where the local item has a `equivalent item` statement to that wikidata item\n        Example: In my wikibase, I have CDK2 (Q79363) with the only statement:\n            equivalent class -> http://www.wikidata.org/entity/Q14911732\n            Calling prop2qid(\"P351\", \"1017\") will return the local QID (Q79363)\n        :param prop:\n        :param value:\n        :return:\n        \"\"\"\n        equiv_class_pid = self.URI_PID['http://www.w3.org/2002/07/owl#equivalentClass']\n        query = \"\"\"\n        PREFIX wwdt: <http://www.wikidata.org/prop/direct/>\n        SELECT ?wditem ?localitem ?id WHERE {{\n          SERVICE <https://query.wikidata.org/sparql> {{\n            ?wditem wwdt:{prop} \"{value}\"\n          }}\n          ?localitem wdt:{equiv_class_pid} ?wditem\n        }}\"\"\"\n        query = query.format(prop=prop, value=value, equiv_class_pid=equiv_class_pid)\n\n        results = wdi_core.WDItemEngine.execute_sparql_query(query, endpoint=self.sparql_endpoint_url)\n        result = results['results']['bindings']\n        if len(result) == 0:\n            return None\n        elif len(result) > 1:\n            raise ValueError(\"More than one wikidata ID found for {} {}: {}\".format(prop, value, result))\n        else:\n            return result[0]['localitem']['value'].split(\"/\")[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef id_mapper(self, prop, filters=None, return_as_set=False):\n        if filters:\n            filter_str = \"\\n\".join(\"?wditem wwdt:{} wwd:{} .\".format(x[0], x[1]) for x in filters)\n        else:\n            filter_str = \"\"\n        query = \"\"\"\n        PREFIX wwdt: <http://www.wikidata.org/prop/direct/>\n        PREFIX wwd: <http://www.wikidata.org/entity/>\n\n        SELECT ?localitem ?ext_id WHERE {{\n          SERVICE <https://query.wikidata.org/sparql> {{\n            ?wditem wwdt:{prop} ?ext_id .\n            {filter_str}\n          }}\n          ?localitem wdt:P3 ?wditem\n        }}\n        \"\"\".format(prop=prop, filter_str=filter_str)\n\n        results = wdi_core.WDItemEngine.execute_sparql_query(query, endpoint=self.sparql_endpoint_url)['results']['bindings']\n        results = [{k: v['value'] for k, v in x.items()} for x in results]\n        for r in results:\n            r['localitem'] = r['localitem'].split('/')[-1]\n        if not results:\n            return None\n\n        id_qid = defaultdict(set)\n        for r in results:\n            id_qid[r['ext_id']].add(r['localitem'])\n\n        if return_as_set:\n            return dict(id_qid)\n        else:\n            return {x['ext_id']: x['localitem'] for x in results}", "response": "This function returns the QIDs of the objects in the wikidata database that match the given property."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a publication object from an europepmc api.", "response": "def europepmc_api_to_publication(ext_id, id_type):\n    # https://europepmc.org/docs/EBI_Europe_PMC_Web_Service_Reference.pdf\n    assert id_type in Publication.ID_TYPES, \"id_type must be in {}\".format(Publication.ID_TYPES.keys())\n    # Request the data\n    if id_type == \"pmcid\":\n        url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=PMCID:PMC{}&resulttype=core&format=json'\n        url = url.format(ext_id)\n    elif id_type == \"doi\":\n        url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=DOI:%22{}%22&resulttype=core&format=json\"\n        url = url.format(ext_id)\n    elif id_type == \"pmid\":\n        url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID:{}%20AND%20SRC:MED&resulttype=core&format=json'\n        url = url.format(ext_id, id_type)\n    else:\n        raise ValueError(\"id_type must be in {}\".format(Publication.ID_TYPES.keys()))\n    headers = {\n        'User-Agent': config['USER_AGENT_DEFAULT']\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    d = response.json()\n    if d['hitCount'] != 1:\n        raise ValueError(\"No results\")\n    article = d['resultList']['result'][0]\n\n    p = Publication()\n    p.ref_url = url\n    p.source = \"europepmc\"\n    original_title = article['title']\n    # to remove trailing dot\n    # to exclude '...' and abbreviations such as U.S.A.\n    p.title = original_title\n    if original_title[-1] == '.' and original_title[-3] != '.':\n        p.title = original_title[:-1]  # drop the trailing dot\n\n    authors = []\n    authorlist = article[\"authorList\"][\"author\"]\n    for author in authorlist:\n        full_name = None\n        if 'firstName' in author and 'lastName' in author:\n            full_name = author['firstName'] + ' ' + author['lastName']\n        elif 'fullName' in author:\n            full_name = author['fullName']\n        elif 'collectiveName' in author:\n            full_name = author['collectiveName']\n        else:\n            msg = \"unknown author: {}\".format(author)\n            p.warnings.append(msg)\n        # orcids\n        orcid = None\n        if 'authorId' in author:\n            if author['authorId']['type'] == \"ORCID\":\n                orcid = author['authorId']['value']\n        authors.append({'full_name': full_name, 'orcid': orcid})\n    p.authors = authors\n    p.publication_date = du.parse(article['firstPublicationDate'])\n    p.volume = article['journalInfo'].get('volume')\n    p.issue = article['journalInfo'].get('issue')\n    p.pages = article.get('pageInfo')\n\n    p.ids = dict()\n    p.ids['doi'] = article.get('doi')\n    p.ids['pmid'] = article.get('pmid')\n    p.ids['pmcid'] = article.get('pmcid', '').replace(\"PMC\", \"\")\n\n    if 'pageInfo' in article:\n        # according to documentation, they should be in the form : 145-178, but also \"D36-42\" has been in the responses\n        try:\n            pageString = article['pageInfo']\n            pageStringParts = pageString.split('-')\n            pageStartString = pageStringParts[0]\n            if pageStartString.startswith('D'):\n                pageStartString = pageStartString[1:]\n            pageStart = int(pageStartString)\n            pageEndString = pageStringParts[1]\n            pageEnd = int(pageEndString)\n            pages = pageEnd - pageStart + 1\n            p.number_of_pages = pages\n        except Exception:\n            pass\n    \"\"\"\n    # this is probably a list, and we need to be able to look up a qid from \"NCI NIH HHS\" for example...\n    if \"grantsList\" in article and \"grant\" in article['grantsList'] and \"agency\" in article['grantsList'][\"grant\"]:\n        p.sponsor = article['grantsList'][\"grant\"][\"agency\"]\n    \"\"\"\n\n    # get the type of publication\n    # if not told that it is a research-article, we will make it a general publication\n    isScientificArticle = False\n    if 'pubTypeList' in article:\n        pubtypes = article['pubTypeList']['pubType']\n        if isinstance(pubtypes, str):\n            if pubtypes == 'research-article':\n                isScientificArticle = True\n        elif isinstance(pubtypes, list):\n            if 'research-article' in pubtypes:\n                isScientificArticle = True\n    if isScientificArticle:\n        p.instance_of = \"scientific_article\"\n    else:\n        p.instance_of = \"publication\"\n        p.warnings.append(\"unknown publication type, assuming publication\")\n\n    issns = []\n    if 'issn' in article['journalInfo']['journal']:\n        issns.append(article['journalInfo']['journal']['issn'])\n    elif 'essn' in article['journalInfo']['journal']:\n        issns.append(article['journalInfo']['journal']['essn'])\n    else:\n        msg = \"unknown journal: {}\".format(article['journalInfo']['journal'])\n        p.warnings.append(msg)\n    p.published_in_issn = issns\n\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the qid of the item by its external id or create if it doesn t exist.", "response": "def get_or_create(self, login):\n        \"\"\"\n        Get the qid of the item by its external id or create if doesn't exist\n        :param login: WDLogin item\n        :return: tuple of (qid, list of warnings (strings), success (True if success, returns the Exception otherwise))\n        \"\"\"\n        if self.p:\n            try:\n                return self.p.get_or_create(login)\n            except Exception as e:\n                return None, self.p.warnings, e\n        else:\n            return None, [], self.e"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\njoint two or more parsers implements the operator of ( +.", "response": "def joint(*parsers):\n    '''Joint two or more parsers, implements the operator of `(+)`.'''\n    @Parser\n    def joint_parser(text, index):\n        values = []\n        prev_v = None\n        for p in parsers:\n            if prev_v:\n                index = prev_v.index\n            prev_v = v = p(text, index)\n            if not v.status:\n                return v\n            values.append(v)\n        return Value.combinate(values)\n    return joint_parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef times(p, mint, maxt=None):\n    '''Repeat a parser between `mint` and `maxt` times. DO AS MUCH MATCH AS IT CAN.\n    Return a list of values.'''\n    maxt = maxt if maxt else mint\n\n    @Parser\n    def times_parser(text, index):\n        cnt, values, res = 0, Value.success(index, []), None\n        while cnt < maxt:\n            res = p(text, index)\n            if res.status:\n                values = values.aggregate(\n                    Value.success(res.index, [res.value]))\n                index, cnt = res.index, cnt + 1\n            else:\n                if cnt >= mint:\n                    break\n                else:\n                    return res  # failed, throw exception.\n            if cnt >= maxt:  # finish.\n                break\n            # If we don't have any remaining text to start next loop, we need break.\n            #\n            # We cannot put the `index < len(text)` in where because some parser can\n            # success even when we have no any text. We also need to detect if the\n            # parser consume no text.\n            #\n            # See: #28\n            if index >= len(text):\n                if cnt >= mint:\n                    break  # we already have decent result to return\n                else:\n                    r = p(text, index)\n                    if index != r.index:  # report error when the parser cannot success with no text\n                        return Value.failure(index, \"already meets the end, no enough text\")\n        return values\n    return times_parser", "response": "Repeat a parser between mint and maxt times. DO AS MUCH MATCH AS IT CAN."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a parser as optional.", "response": "def optional(p, default_value=None):\n    '''`Make a parser as optional. If success, return the result, otherwise return\n    default_value silently, without raising any exception. If default_value is not\n    provided None is returned instead.\n    '''\n    @Parser\n    def optional_parser(text, index):\n        res = p(text, index)\n        if res.status:\n            return Value.success(res.index, res.value)\n        else:\n            # Return the maybe existing default value without doing anything.\n            return Value.success(res.index, default_value)\n    return optional_parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrepeats a parser p separated by s between mint and maxt times.", "response": "def separated(p, sep, mint, maxt=None, end=None):\n    '''Repeat a parser `p` separated by `s` between `mint` and `maxt` times.\n    When `end` is None, a trailing separator is optional.\n    When `end` is True, a trailing separator is required.\n    When `end` is False, a trailing separator is not allowed.\n    MATCHES AS MUCH AS POSSIBLE.\n    Return list of values returned by `p`.'''\n    maxt = maxt if maxt else mint\n\n    @Parser\n    def sep_parser(text, index):\n        cnt, values, res = 0, Value.success(index, []), None\n        while cnt < maxt:\n            if end in [False, None] and cnt > 0:\n                res = sep(text, index)\n                if res.status:  # `sep` found, consume it (advance index)\n                    index, values = res.index, Value.success(\n                        res.index, values.value)\n                elif cnt < mint:\n                    return res  # error: need more elemnts, but no `sep` found.\n                else:\n                    break\n\n            res = p(text, index)\n            if res.status:\n                values = values.aggregate(\n                    Value.success(res.index, [res.value]))\n                index, cnt = res.index, cnt + 1\n            elif cnt >= mint:\n                break\n            else:\n                return res  # error: need more elements, but no `p` found.\n\n            if end is True:\n                res = sep(text, index)\n                if res.status:\n                    index, values = res.index, Value.success(\n                        res.index, values.value)\n                else:\n                    return res  # error: trailing `sep` not found\n\n            if cnt >= maxt:\n                break\n        return values\n    return sep_parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sepBy(p, sep):\n    '''`sepBy(p, sep)` parses zero or more occurrences of p, separated by `sep`.\n    Returns a list of values returned by `p`.'''\n    return separated(p, sep, 0, maxt=float('inf'), end=False)", "response": "Parses zero or more occurrences of p separated by sep. Returns a list of values returned by p."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses one or more occurrences of p separated by sep. Returns a list of values returned by p.", "response": "def sepBy1(p, sep):\n    '''`sepBy1(p, sep)` parses one or more occurrences of `p`, separated by\n    `sep`. Returns a list of values returned by `p`.'''\n    return separated(p, sep, 1, maxt=float('inf'), end=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef endBy(p, sep):\n    '''`endBy(p, sep)` parses zero or more occurrences of `p`, separated and\n    ended by `sep`. Returns a list of values returned by `p`.'''\n    return separated(p, sep, 0, maxt=float('inf'), end=True)", "response": "Parses zero or more occurrences of p separated and\n    ended by sep. Returns a list of values returned by p."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing one or more occurrences of p separated and ended by sep. Returns a list of values returned by p.", "response": "def endBy1(p, sep):\n    '''`endBy1(p, sep) parses one or more occurrences of `p`, separated and\n    ended by `sep`. Returns a list of values returned by `p`.'''\n    return separated(p, sep, 1, maxt=float('inf'), end=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef none_of(s):\n    '''Parser a char NOT from specified string.'''\n    @Parser\n    def none_of_parser(text, index=0):\n        if index < len(text) and text[index] not in s:\n            return Value.success(index + 1, text[index])\n        else:\n            return Value.failure(index, 'none of {}'.format(s))\n    return none_of_parser", "response": "Parser a char NOT from specified string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef letter():\n    '''Parse a letter in alphabet.'''\n    @Parser\n    def letter_parser(text, index=0):\n        if index < len(text) and text[index].isalpha():\n            return Value.success(index + 1, text[index])\n        else:\n            return Value.failure(index, 'a letter')\n    return letter_parser", "response": "Parse a letter in alphabet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef digit():\n    '''Parse a digit character.'''\n    @Parser\n    def digit_parser(text, index=0):\n        if index < len(text) and text[index].isdigit():\n            return Value.success(index + 1, text[index])\n        else:\n            return Value.failure(index, 'a digit')\n    return digit_parser", "response": "Parses a digit character."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning line and column number of index in source code text.", "response": "def loc_info(text, index):\n        '''Location of `index` in source code `text`.'''\n        if index > len(text):\n            raise ValueError('Invalid index.')\n        line, last_ln = text.count('\\n', 0, index), text.rfind('\\n', 0, index)\n        col = index - (last_ln + 1)\n        return (line, col)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlocates the error position in the source code text.", "response": "def loc(self):\n        '''Locate the error position in the source code text.'''\n        try:\n            return '{}:{}'.format(*ParseError.loc_info(self.text, self.index))\n        except ValueError:\n            return '<out of bounds index {!r}>'.format(self.index)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect the furthest failure from self and other.", "response": "def aggregate(self, other=None):\n        '''collect the furthest failure from self and other.'''\n        if not self.status:\n            return self\n        if not other:\n            return self\n        if not other.status:\n            return other\n        return Value(True, other.index, self.value + other.value, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\naggregates multiple values into tuple", "response": "def combinate(values):\n        '''aggregate multiple values into tuple'''\n        prev_v = None\n        for v in values:\n            if prev_v:\n                if not v:\n                    return prev_v\n            if not v.status:\n                return v\n        out_values = tuple([v.value for v in values])\n        return Value(True, values[-1].index, out_values, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the longest possible prefix of a given string.", "response": "def parse_partial(self, text):\n        '''Parse the longest possible prefix of a given string.\n        Return a tuple of the result value and the rest of the string.\n        If failed, raise a ParseError. '''\n        if not isinstance(text, str):\n            raise TypeError(\n                'Can only parsing string but got {!r}'.format(text))\n        res = self(text, 0)\n        if res.status:\n            return (res.value, text[res.index:])\n        else:\n            raise ParseError(res.expected, text, res.index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compose(self, other):\n        '''(>>) Sequentially compose two actions, discarding any value produced\n        by the first.'''\n        @Parser\n        def compose_parser(text, index):\n            res = self(text, index)\n            return res if not res.status else other(text, res.index)\n        return compose_parser", "response": "A factory function that returns a parser that will compose two actions discarding any value produced\n        by the first."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef skip(self, other):\n        '''(<<) Ends with a specified parser, and at the end parser consumed the\n        end flag.'''\n        @Parser\n        def ends_with_parser(text, index):\n            res = self(text, index)\n            if not res.status:\n                return res\n            end = other(text, res.index)\n            if end.status:\n                return Value.success(end.index, res.value)\n            else:\n                return Value.failure(end.index, 'ends with {}'.format(end.expected))\n        return ends_with_parser", "response": "Return a function that returns True if the parser ends with the specified parser and at the end parser consumed the\n        end flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ends_with(self, other):\n        '''(<) Ends with a specified parser, and at the end parser hasn't consumed\n        any input.'''\n        @Parser\n        def ends_with_parser(text, index):\n            res = self(text, index)\n            if not res.status:\n                return res\n            end = other(text, res.index)\n            if end.status:\n                return res\n            else:\n                return Value.failure(end.index, 'ends with {}'.format(end.expected))\n        return ends_with_parser", "response": "A function to create a parser that checks if the parser ends with the specified parser."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a parser that transforms the produced value of parser with fn.", "response": "def parsecmap(self, fn):\n        '''Returns a parser that transforms the produced value of parser with `fn`.'''\n        return self.bind(lambda res: Parser(lambda _, index: Value.success(index, fn(res))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parsecapp(self, other):\n        '''Returns a parser that applies the produced value of this parser to the produced value of `other`.'''\n        # pylint: disable=unnecessary-lambda\n        return self.bind(lambda res: other.parsecmap(lambda x: res(x)))", "response": "Returns a parser that applies the produced value of this parser to the produced value of other."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef result(self, res):\n        '''Return a value according to the parameter `res` when parse successfully.'''\n        return self >> Parser(lambda _, index: Value.success(index, res))", "response": "Return a value according to the parameter res when parse successfully."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmarking the line and column information of the result of this parser.", "response": "def mark(self):\n        '''Mark the line and column information of the result of this parser.'''\n        def pos(text, index):\n            return ParseError.loc_info(text, index)\n\n        @Parser\n        def mark_parser(text, index):\n            res = self(text, index)\n            if res.status:\n                return Value.success(res.index, (pos(text, index), res.value, pos(text, res.index)))\n            else:\n                return res  # failed.\n        return mark_parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef desc(self, description):\n        '''Describe a parser, when it failed, print out the description text.'''\n        return self | Parser(lambda _, index: Value.failure(index, description))", "response": "Describe a parser when it failed print out the description text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing string. (normal string and escaped string)", "response": "def charseq():\n    '''Parse string. (normal string and escaped string)'''\n    def string_part():\n        '''Parse normal string.'''\n        return regex(r'[^\"\\\\]+')\n\n    def string_esc():\n        '''Parse escaped string.'''\n        return string('\\\\') >> (\n            string('\\\\')\n            | string('/')\n            | string('\"')\n            | string('b').result('\\b')\n            | string('f').result('\\f')\n            | string('n').result('\\n')\n            | string('r').result('\\r')\n            | string('t').result('\\t')\n            | regex(r'u[0-9a-fA-F]{4}').parsecmap(lambda s: chr(int(s[1:], 16)))\n        )\n    return string_part() | string_esc()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef route(*args):\n\n    def _validate_route(route):\n        if not isinstance(route, six.string_types):\n            raise TypeError('%s must be a string' % route)\n\n        if route in ('.', '..') or not re.match(\n            '^[0-9a-zA-Z-_$\\(\\)\\.~!,;:*+@=]+$', route\n        ):\n            raise ValueError(\n                '%s must be a valid path segment.  Keep in mind '\n                'that path segments should not contain path separators '\n                '(e.g., /) ' % route\n            )\n\n    if len(args) == 2:\n        # The handler in this situation is a @pecan.expose'd callable,\n        # and is generally only used by the @expose() decorator itself.\n        #\n        # This sets a special attribute, `custom_route` on the callable, which\n        # pecan's routing logic knows how to make use of (as a special case)\n        route, handler = args\n        if ismethod(handler):\n            handler = handler.__func__\n        if not iscontroller(handler):\n            raise TypeError(\n                '%s must be a callable decorated with @pecan.expose' % handler\n            )\n        obj, attr, value = handler, 'custom_route', route\n\n        if handler.__name__ in ('_lookup', '_default', '_route'):\n            raise ValueError(\n                '%s is a special method in pecan and cannot be used in '\n                'combination with custom path segments.' % handler.__name__\n            )\n    elif len(args) == 3:\n        # This is really just a setattr on the parent controller (with some\n        # additional validation for the path segment itself)\n        _, route, handler = args\n        obj, attr, value = args\n\n        if hasattr(obj, attr):\n            raise RuntimeError(\n                (\n                    \"%(module)s.%(class)s already has an \"\n                    \"existing attribute named \\\"%(route)s\\\".\" % {\n                        'module': obj.__module__,\n                        'class': obj.__name__,\n                        'route': attr\n                    }\n                ),\n            )\n    else:\n        raise TypeError(\n            'pecan.route should be called in the format '\n            'route(ParentController, \"path-segment\", SubController())'\n        )\n\n    _validate_route(route)\n    setattr(obj, attr, value)", "response": "This function is used to define an explicit route for a path segment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntraverse the requested url path and returns the appropriate controller object, including default routes. Handles common errors gracefully.", "response": "def lookup_controller(obj, remainder, request=None):\n    '''\n    Traverses the requested url path and returns the appropriate controller\n    object, including default routes.\n\n    Handles common errors gracefully.\n    '''\n    if request is None:\n        warnings.warn(\n            (\n                \"The function signature for %s.lookup_controller is changing \"\n                \"in the next version of pecan.\\nPlease update to: \"\n                \"`lookup_controller(self, obj, remainder, request)`.\" % (\n                    __name__,\n                )\n            ),\n            DeprecationWarning\n        )\n\n    notfound_handlers = []\n    while True:\n        try:\n            obj, remainder = find_object(obj, remainder, notfound_handlers,\n                                         request)\n            handle_security(obj)\n            return obj, remainder\n        except (exc.HTTPNotFound, exc.HTTPMethodNotAllowed,\n                PecanNotFound) as e:\n            if isinstance(e, PecanNotFound):\n                e = exc.HTTPNotFound()\n            while notfound_handlers:\n                name, obj, remainder = notfound_handlers.pop()\n                if name == '_default':\n                    # Notfound handler is, in fact, a controller, so stop\n                    #   traversal\n                    return obj, remainder\n                else:\n                    # Notfound handler is an internal redirect, so continue\n                    #   traversal\n                    result = handle_lookup_traversal(obj, remainder)\n                    if result:\n                        # If no arguments are passed to the _lookup, yet the\n                        # argspec requires at least one, raise a 404\n                        if (\n                            remainder == [''] and\n                            len(obj._pecan['argspec'].args) > 1\n                        ):\n                            raise e\n                        obj_, remainder_ = result\n                        return lookup_controller(obj_, remainder_, request)\n            else:\n                raise e"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwalks the url path in search of an action for which a controller isisite and returns that controller object along with what s left of the remainder.", "response": "def find_object(obj, remainder, notfound_handlers, request):\n    '''\n    'Walks' the url path in search of an action for which a controller is\n    implemented and returns that controller object along with what's left\n    of the remainder.\n    '''\n    prev_obj = None\n    while True:\n        if obj is None:\n            raise PecanNotFound\n        if iscontroller(obj):\n            if getattr(obj, 'custom_route', None) is None:\n                return obj, remainder\n\n        _detect_custom_path_segments(obj)\n\n        if remainder:\n            custom_route = __custom_routes__.get((obj.__class__, remainder[0]))\n            if custom_route:\n                return getattr(obj, custom_route), remainder[1:]\n\n        # are we traversing to another controller\n        cross_boundary(prev_obj, obj)\n        try:\n            next_obj, rest = remainder[0], remainder[1:]\n            if next_obj == '':\n                index = getattr(obj, 'index', None)\n                if iscontroller(index):\n                    return index, rest\n        except IndexError:\n            # the URL has hit an index method without a trailing slash\n            index = getattr(obj, 'index', None)\n            if iscontroller(index):\n                raise NonCanonicalPath(index, [])\n\n        default = getattr(obj, '_default', None)\n        if iscontroller(default):\n            notfound_handlers.append(('_default', default, remainder))\n\n        lookup = getattr(obj, '_lookup', None)\n        if iscontroller(lookup):\n            notfound_handlers.append(('_lookup', lookup, remainder))\n\n        route = getattr(obj, '_route', None)\n        if iscontroller(route):\n            if len(getargspec(route).args) == 2:\n                warnings.warn(\n                    (\n                        \"The function signature for %s.%s._route is changing \"\n                        \"in the next version of pecan.\\nPlease update to: \"\n                        \"`def _route(self, args, request)`.\" % (\n                            obj.__class__.__module__,\n                            obj.__class__.__name__\n                        )\n                    ),\n                    DeprecationWarning\n                )\n                next_obj, next_remainder = route(remainder)\n            else:\n                next_obj, next_remainder = route(remainder, request)\n            cross_boundary(route, next_obj)\n            return next_obj, next_remainder\n\n        if not remainder:\n            raise PecanNotFound\n\n        prev_remainder = remainder\n        prev_obj = obj\n        remainder = rest\n        try:\n            obj = getattr(obj, next_obj, None)\n        except UnicodeEncodeError:\n            obj = None\n\n        # Last-ditch effort: if there's not a matching subcontroller, no\n        # `_default`, no `_lookup`, and no `_route`, look to see if there's\n        # an `index` that has a generic method defined for the current request\n        # method.\n        if not obj and not notfound_handlers and hasattr(prev_obj, 'index'):\n            if request.method in _cfg(prev_obj.index).get('generic_handlers',\n                                                          {}):\n                return prev_obj.index, prev_remainder"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_wrapper(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        return f(*args, **kwargs)\n    wrapper._pecan = f._pecan.copy()\n    return wrapper", "response": "return a wrapped function with a copy of the _pecan context"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_security(controller, im_self=None):\n    if controller._pecan.get('secured', False):\n        check_permissions = controller._pecan['check_permissions']\n\n        if isinstance(check_permissions, six.string_types):\n            check_permissions = getattr(\n                im_self or six.get_method_self(controller),\n                check_permissions\n            )\n\n        if not check_permissions():\n            raise exc.HTTPUnauthorized", "response": "Checks the security of a controller."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks permissions as we move between object instances.", "response": "def cross_boundary(prev_obj, obj):\n    \"\"\" Check permissions as we move between object instances. \"\"\"\n    if prev_obj is None:\n        return\n\n    if isinstance(obj, _SecuredAttribute):\n        # a secure attribute can live in unsecure class so we have to set\n        # while we walk the route\n        obj.parent = prev_obj\n\n    if hasattr(prev_obj, '_pecan'):\n        if obj not in prev_obj._pecan.get('unlocked', []):\n            handle_security(prev_obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies a directory from source to dest.", "response": "def copy_dir(source, dest, variables, out_=sys.stdout, i=0):\n    \"\"\"\n    Copies the ``source`` directory to the ``dest`` directory, where\n    ``source`` is some tuple representing an installed package and a\n    subdirectory in the package, e.g.,\n\n    ('pecan', os.path.join('scaffolds', 'base'))\n    ('pecan_extension', os.path.join('scaffolds', 'scaffold_name'))\n\n    ``variables``: A dictionary of variables to use in any substitutions.\n    Substitution is performed via ``string.Template``.\n\n    ``out_``: File object to write to (default is sys.stdout).\n    \"\"\"\n    def out(msg):\n        out_.write('%s%s' % (' ' * (i * 2), msg))\n        out_.write('\\n')\n        out_.flush()\n\n    names = sorted(pkg_resources.resource_listdir(source[0], source[1]))\n    if not os.path.exists(dest):\n        out('Creating %s' % dest)\n        makedirs(dest)\n    else:\n        out('%s already exists' % dest)\n        return\n\n    for name in names:\n\n        full = '/'.join([source[1], name])\n        dest_full = os.path.join(dest, substitute_filename(name, variables))\n\n        sub_file = False\n        if dest_full.endswith('_tmpl'):\n            dest_full = dest_full[:-5]\n            sub_file = True\n\n        if pkg_resources.resource_isdir(source[0], full):\n            out('Recursing into %s' % os.path.basename(full))\n            copy_dir((source[0], full), dest_full, variables, out_, i + 1)\n            continue\n        else:\n            content = pkg_resources.resource_string(source[0], full)\n\n        if sub_file:\n            content = render_template(content, variables)\n            if content is None:\n                continue  # pragma: no cover\n\n        out('Copying %s to %s' % (full, dest_full))\n\n        f = open(dest_full, 'wb')\n        f.write(content)\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makedirs(directory):\n    parent = os.path.dirname(os.path.abspath(directory))\n    if not os.path.exists(parent):\n        makedirs(parent)\n    os.mkdir(directory)", "response": "Resursively create a named directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef substitute_filename(fn, variables):\n    for var, value in variables.items():\n        fn = fn.replace('+%s+' % var, str(value))\n    return fn", "response": "Substitute + variables + in file directory names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender a templated file based on the content and the variable names defined in variables.", "response": "def render_template(content, variables):\n    \"\"\"\n    Return a bytestring representing a templated file based on the\n    input (content) and the variable names defined (vars).\n    \"\"\"\n    fsenc = sys.getfilesystemencoding()\n\n    def to_native(s, encoding='latin-1', errors='strict'):\n        if six.PY3:\n            if isinstance(s, six.text_type):\n                return s\n            return str(s, encoding, errors)\n        else:\n            if isinstance(s, six.text_type):\n                return s.encode(encoding, errors)\n            return str(s)\n\n    output = Template(\n        to_native(content, fsenc)\n    ).substitute(variables)\n    if isinstance(output, six.text_type):\n        output = output.encode(fsenc, 'strict')\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_app(root, **kw):\n    '''\n    Utility for creating the Pecan application object.  This function should\n    generally be called from the ``setup_app`` function in your project's\n    ``app.py`` file.\n\n    :param root: A string representing a root controller object (e.g.,\n                 \"myapp.controller.root.RootController\")\n    :param static_root: The relative path to a directory containing static\n                        files.  Serving static files is only enabled when\n                        debug mode is set.\n    :param debug: A flag to enable debug mode.  This enables the debug\n                  middleware and serving static files.\n    :param wrap_app: A function or middleware class to wrap the Pecan app.\n                     This must either be a wsgi middleware class or a\n                     function that returns a wsgi application. This wrapper\n                     is applied first before wrapping the application in\n                     other middlewares such as Pecan's debug middleware.\n                     This should be used if you want to use middleware to\n                     perform authentication or intercept all requests before\n                     they are routed to the root controller.\n    :param logging: A dictionary used to configure logging.  This uses\n                    ``logging.config.dictConfig``.\n\n    All other keyword arguments are passed in to the Pecan app constructor.\n\n    :returns: a ``Pecan`` object.\n    '''\n    # Pass logging configuration (if it exists) on to the Python logging module\n    logging = kw.get('logging', {})\n    debug = kw.get('debug', False)\n    if logging:\n        if debug:\n            try:\n                #\n                # By default, Python 2.7+ silences DeprecationWarnings.\n                # However, if conf.app.debug is True, we should probably ensure\n                # that users see these types of warnings.\n                #\n                from logging import captureWarnings\n                captureWarnings(True)\n                warnings.simplefilter(\"default\", DeprecationWarning)\n            except ImportError:\n                # No captureWarnings on Python 2.6, DeprecationWarnings are on\n                pass\n\n        if isinstance(logging, Config):\n            logging = logging.to_dict()\n        if 'version' not in logging:\n            logging['version'] = 1\n        load_logging_config(logging)\n\n    # Instantiate the WSGI app by passing **kw onward\n    app = Pecan(root, **kw)\n\n    # Optionally wrap the app in another WSGI app\n    wrap_app = kw.get('wrap_app', None)\n    if wrap_app:\n        app = wrap_app(app)\n\n    # Configuration for serving custom error messages\n    errors = kw.get('errors', getattr(conf.app, 'errors', {}))\n    if errors:\n        app = middleware.errordocument.ErrorDocumentMiddleware(app, errors)\n\n    # Included for internal redirect support\n    app = middleware.recursive.RecursiveMiddleware(app)\n\n    # When in debug mode, load exception debugging middleware\n    static_root = kw.get('static_root', None)\n    if debug:\n        debug_kwargs = getattr(conf, 'debug', {})\n        debug_kwargs.setdefault('context_injectors', []).append(\n            lambda environ: {\n                'request': environ.get('pecan.locals', {}).get('request')\n            }\n        )\n        app = DebugMiddleware(\n            app,\n            **debug_kwargs\n        )\n\n        # Support for serving static files (for development convenience)\n        if static_root:\n            app = middleware.static.StaticFileMiddleware(app, static_root)\n\n    elif static_root:\n        warnings.warn(\n            \"`static_root` is only used when `debug` is True, ignoring\",\n            RuntimeWarning\n        )\n\n    if hasattr(conf, 'requestviewer'):\n        warnings.warn(''.join([\n            \"`pecan.conf.requestviewer` is deprecated.  To apply the \",\n            \"`RequestViewerHook` to your application, add it to \",\n            \"`pecan.conf.app.hooks` or manually in your project's `app.py` \",\n            \"file.\"]),\n            DeprecationWarning\n        )\n\n    return app", "response": "Utility for creating a Pecan application object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a configuration dictionary from a file.", "response": "def conf_from_file(filepath):\n    '''\n    Creates a configuration dictionary from a file.\n\n    :param filepath: The path to the file.\n    '''\n\n    abspath = os.path.abspath(os.path.expanduser(filepath))\n    conf_dict = {}\n    if not os.path.isfile(abspath):\n        raise RuntimeError('`%s` is not a file.' % abspath)\n\n    # First, make sure the code will actually compile (and has no SyntaxErrors)\n    with open(abspath, 'rb') as f:\n        compiled = compile(f.read(), abspath, 'exec')\n\n    # Next, attempt to actually import the file as a module.\n    # This provides more verbose import-related error reporting than exec()\n    absname, _ = os.path.splitext(abspath)\n    basepath, module_name = absname.rsplit(os.sep, 1)\n    if six.PY3:\n        SourceFileLoader(module_name, abspath).load_module(module_name)\n    else:\n        imp.load_module(\n            module_name,\n            *imp.find_module(module_name, [basepath])\n        )\n\n    # If we were able to import as a module, actually exec the compiled code\n    exec(compiled, globals(), conf_dict)\n    conf_dict['__file__'] = abspath\n\n    return conf_from_dict(conf_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_conf_path_from_env():\n    '''\n    If the ``PECAN_CONFIG`` environment variable exists and it points to\n    a valid path it will return that, otherwise it will raise\n    a ``RuntimeError``.\n    '''\n    config_path = os.environ.get('PECAN_CONFIG')\n    if not config_path:\n        error = \"PECAN_CONFIG is not set and \" \\\n                \"no config file was passed as an argument.\"\n    elif not os.path.isfile(config_path):\n        error = \"PECAN_CONFIG was set to an invalid path: %s\" % config_path\n    else:\n        return config_path\n\n    raise RuntimeError(error)", "response": "Returns the path to the config file from the environment variable PPECAN_CONFIG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a configuration dictionary from a dictionary.", "response": "def conf_from_dict(conf_dict):\n    '''\n    Creates a configuration dictionary from a dictionary.\n\n    :param conf_dict: The configuration dictionary.\n    '''\n    conf = Config(filename=conf_dict.get('__file__', ''))\n\n    for k, v in six.iteritems(conf_dict):\n        if k.startswith('__'):\n            continue\n        elif inspect.ismodule(v):\n            continue\n\n        conf[k] = v\n    return conf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the global configuration.", "response": "def set_config(config, overwrite=False):\n    '''\n    Updates the global configuration.\n\n    :param config: Can be a dictionary containing configuration, or a string\n                   which represents a (relative) configuration filename.\n    '''\n\n    if config is None:\n        config = get_conf_path_from_env()\n\n    # must be after the fallback other a bad fallback will incorrectly clear\n    if overwrite is True:\n        _runtime_conf.empty()\n\n    if isinstance(config, six.string_types):\n        config = conf_from_file(config)\n        _runtime_conf.update(config)\n        if config.__file__:\n            _runtime_conf.__file__ = config.__file__\n    elif isinstance(config, dict):\n        _runtime_conf.update(conf_from_dict(config))\n    else:\n        raise TypeError('%s is neither a dictionary or a string.' % config)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, conf_dict):\n        '''\n        Updates this configuration with a dictionary.\n\n        :param conf_dict: A python dictionary to update this configuration\n                          with.\n        '''\n\n        if isinstance(conf_dict, dict):\n            iterator = six.iteritems(conf_dict)\n        else:\n            iterator = iter(conf_dict)\n\n        for k, v in iterator:\n            if not IDENTIFIER.match(k):\n                raise ValueError('\\'%s\\' is not a valid indentifier' % k)\n\n            cur_val = self.__values__.get(k)\n\n            if isinstance(cur_val, Config):\n                cur_val.update(conf_dict[k])\n            else:\n                self[k] = conf_dict[k]", "response": "Updates this configuration with a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_dict(self, prefix=None):\n        '''\n        Converts recursively the Config object into a valid dictionary.\n\n        :param prefix: A string to optionally prefix all key elements in the\n                       returned dictonary.\n        '''\n\n        conf_obj = dict(self)\n        return self.__dictify__(conf_obj, prefix)", "response": "Converts the Config object into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides the template that is used in your response.", "response": "def override_template(template, content_type=None):\n    '''\n    Call within a controller to override the template that is used in\n    your response.\n\n    :param template: a valid path to a template file, just as you would specify\n                     in an ``@expose``.\n    :param content_type: a valid MIME type to use for the response.func_closure\n    '''\n\n    request.pecan['override_template'] = template\n    if content_type:\n        request.pecan['override_content_type'] = content_type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nraise an HTTP status code as specified. Useful for returning 401 Unauthorized or 403 Forbidden.", "response": "def abort(status_code, detail='', headers=None, comment=None, **kw):\n    '''\n    Raise an HTTP status code, as specified. Useful for returning status\n    codes like 401 Unauthorized or 403 Forbidden.\n\n    :param status_code: The HTTP status code as an integer.\n    :param detail: The message to send along, as a string.\n    :param headers: A dictionary of headers to send along with the response.\n    :param comment: A comment to include in the response.\n    '''\n\n    # If there is a traceback, we need to catch it for a re-raise\n    try:\n        _, _, traceback = sys.exc_info()\n        webob_exception = exc.status_map[status_code](\n            detail=detail,\n            headers=headers,\n            comment=comment,\n            **kw\n        )\n\n        if six.PY3:\n            raise webob_exception.with_traceback(traceback)\n        else:\n            # Using exec to avoid python 3 parsers from crashing\n            exec('raise webob_exception, None, traceback')\n    finally:\n        # Per the suggestion of the Python docs, delete the traceback object\n        del traceback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a redirect to the specified location.", "response": "def redirect(location=None, internal=False, code=None, headers={},\n             add_slash=False, request=None):\n    '''\n    Perform a redirect, either internal or external. An internal redirect\n    performs the redirect server-side, while the external redirect utilizes\n    an HTTP 302 status code.\n\n    :param location: The HTTP location to redirect to.\n    :param internal: A boolean indicating whether the redirect should be\n                     internal.\n    :param code: The HTTP status code to use for the redirect. Defaults to 302.\n    :param headers: Any HTTP headers to send with the response, as a\n                    dictionary.\n    :param request: The :class:`pecan.Request` instance to use.\n    '''\n    request = request or state.request\n\n    if add_slash:\n        if location is None:\n            split_url = list(urlparse.urlsplit(request.url))\n            new_proto = request.environ.get(\n                'HTTP_X_FORWARDED_PROTO', split_url[0]\n            )\n            split_url[0] = new_proto\n        else:\n            split_url = urlparse.urlsplit(location)\n\n        split_url[2] = split_url[2].rstrip('/') + '/'\n        location = urlparse.urlunsplit(split_url)\n\n    if not headers:\n        headers = {}\n    if internal:\n        if code is not None:\n            raise ValueError('Cannot specify a code for internal redirects')\n        request.environ['pecan.recursive.context'] = request.context\n        raise ForwardRequestException(location)\n    if code is None:\n        code = 302\n    raise exc.status_map[code](location=location, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders the specified template using the Pecan rendering framework with the specified namespace as a dictionary.", "response": "def render(template, namespace, app=None):\n    '''\n    Render the specified template using the Pecan rendering framework\n    with the specified template namespace as a dictionary. Useful in a\n    controller where you have no template specified in the ``@expose``.\n\n    :param template: The path to your template, as you would specify in\n                     ``@expose``.\n    :param namespace: The namespace to use for rendering the template, as a\n                      dictionary.\n    :param app: The instance of :class:`pecan.Pecan` to use\n    '''\n    app = app or state.app\n    return app.render(template, namespace)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a pecan. Pecan application and its environment based on passed configuration.", "response": "def load_app(config, **kwargs):\n    '''\n    Used to load a ``Pecan`` application and its environment based on passed\n    configuration.\n\n    :param config: Can be a dictionary containing configuration, a string which\n                    represents a (relative) configuration filename\n\n    returns a pecan.Pecan object\n    '''\n    from .configuration import _runtime_conf, set_config\n    set_config(config, overwrite=True)\n\n    for package_name in getattr(_runtime_conf.app, 'modules', []):\n        module = __import__(package_name, fromlist=['app'])\n        if hasattr(module, 'app') and hasattr(module.app, 'setup_app'):\n            app = module.app.setup_app(_runtime_conf, **kwargs)\n            app.config = _runtime_conf\n            return app\n    raise RuntimeError(\n        'No app.setup_app found in any of the configured app.modules'\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef route(self, req, node, path):\n        '''\n        Looks up a controller from a node based upon the specified path.\n\n        :param node: The node, such as a root controller object.\n        :param path: The path to look up on this node.\n        '''\n        path = path.split('/')[1:]\n        try:\n            node, remainder = lookup_controller(node, path, req)\n            return node, remainder\n        except NonCanonicalPath as e:\n            if self.force_canonical and \\\n                    not _cfg(e.controller).get('accept_noncanonical', False):\n                if req.method == 'POST':\n                    raise RuntimeError(\n                        \"You have POSTed to a URL '%s' which \"\n                        \"requires a slash. Most browsers will not maintain \"\n                        \"POST data when redirected. Please update your code \"\n                        \"to POST to '%s/' or set force_canonical to False\" %\n                        (req.pecan['routing_path'],\n                            req.pecan['routing_path'])\n                    )\n                redirect(code=302, add_slash=True, request=req)\n            return e.controller, e.remainder", "response": "Look up a controller from a node based upon the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef determine_hooks(self, controller=None):\n        '''\n        Determines the hooks to be run, in which order.\n\n        :param controller: If specified, includes hooks for a specific\n                           controller.\n        '''\n\n        controller_hooks = []\n        if controller:\n            controller_hooks = _cfg(controller).get('hooks', [])\n            if controller_hooks:\n                return list(\n                    sorted(\n                        chain(controller_hooks, self.hooks),\n                        key=operator.attrgetter('priority')\n                    )\n                )\n        return self.hooks", "response": "Determines the hooks to be run in which order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the specified hooks of the specified type.", "response": "def handle_hooks(self, hooks, hook_type, *args):\n        '''\n        Processes hooks of the specified type.\n\n        :param hook_type: The type of hook, including ``before``, ``after``,\n                          ``on_error``, and ``on_route``.\n        :param \\*args: Arguments to pass to the hooks.\n        '''\n        if hook_type not in ['before', 'on_route']:\n            hooks = reversed(hooks)\n\n        for hook in hooks:\n            result = getattr(hook, hook_type)(*args)\n            # on_error hooks can choose to return a Response, which will\n            # be used instead of the standard error pages.\n            if hook_type == 'on_error' and isinstance(result, WebObResponse):\n                return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the arguments for a controller based upon parameters and the argument specification for the controller.", "response": "def get_args(self, state, all_params, remainder, argspec, im_self):\n        '''\n        Determines the arguments for a controller based upon parameters\n        passed the argument specification for the controller.\n        '''\n        args = []\n        varargs = []\n        kwargs = dict()\n        valid_args = argspec.args[:]\n        if ismethod(state.controller) or im_self:\n            valid_args.pop(0)  # pop off `self`\n        pecan_state = state.request.pecan\n\n        remainder = [x for x in remainder if x]\n\n        if im_self is not None:\n            args.append(im_self)\n\n        # grab the routing args from nested REST controllers\n        if 'routing_args' in pecan_state:\n            remainder = pecan_state['routing_args'] + list(remainder)\n            del pecan_state['routing_args']\n\n        # handle positional arguments\n        if valid_args and remainder:\n            args.extend(remainder[:len(valid_args)])\n            remainder = remainder[len(valid_args):]\n            valid_args = valid_args[len(args):]\n\n        # handle wildcard arguments\n        if [i for i in remainder if i]:\n            if not argspec[1]:\n                abort(404)\n            varargs.extend(remainder)\n\n        # get the default positional arguments\n        if argspec[3]:\n            defaults = dict(izip(argspec[0][-len(argspec[3]):], argspec[3]))\n        else:\n            defaults = dict()\n\n        # handle positional GET/POST params\n        for name in valid_args:\n            if name in all_params:\n                args.append(all_params.pop(name))\n            elif name in defaults:\n                args.append(defaults[name])\n            else:\n                break\n\n        # handle wildcard GET/POST params\n        if argspec[2]:\n            for name, value in six.iteritems(all_params):\n                if name not in argspec[0]:\n                    kwargs[name] = value\n\n        return args, varargs, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the next controller for the current application.", "response": "def find_controller(self, state):\n        '''\n        The main request handler for Pecan applications.\n        '''\n        # get a sorted list of hooks, by priority (no controller hooks yet)\n        req = state.request\n        pecan_state = req.pecan\n\n        # store the routing path for the current application to allow hooks to\n        # modify it\n        pecan_state['routing_path'] = path = req.path_info\n\n        # handle \"on_route\" hooks\n        self.handle_hooks(self.hooks, 'on_route', state)\n\n        # lookup the controller, respecting content-type as requested\n        # by the file extension on the URI\n        pecan_state['extension'] = None\n\n        # attempt to guess the content type based on the file extension\n        if self.guess_content_type_from_ext \\\n                and not pecan_state['content_type'] \\\n                and '.' in path:\n            _, extension = splitext(path.rstrip('/'))\n\n            # preface with a letter to ensure compat for 2.5\n            potential_type = guess_type('x' + extension)[0]\n\n            if extension and potential_type is not None:\n                path = ''.join(path.rsplit(extension, 1))\n                pecan_state['extension'] = extension\n                pecan_state['content_type'] = potential_type\n\n        controller, remainder = self.route(req, self.root, path)\n        cfg = _cfg(controller)\n\n        if cfg.get('generic_handler'):\n            raise exc.HTTPNotFound\n\n        # handle generic controllers\n        im_self = None\n        if cfg.get('generic'):\n            im_self = six.get_method_self(controller)\n            handlers = cfg['generic_handlers']\n            controller = handlers.get(req.method, handlers['DEFAULT'])\n            handle_security(controller, im_self)\n            cfg = _cfg(controller)\n\n        # add the controller to the state so that hooks can use it\n        state.controller = controller\n\n        # if unsure ask the controller for the default content type\n        content_types = cfg.get('content_types', {})\n        if not pecan_state['content_type']:\n            # attempt to find a best match based on accept headers (if they\n            # exist)\n            accept = getattr(req.accept, 'header_value', '*/*') or '*/*'\n            if accept == '*/*' or (\n                    accept.startswith('text/html,') and\n                    list(content_types.keys()) in self.SIMPLEST_CONTENT_TYPES):\n                pecan_state['content_type'] = cfg.get(\n                    'content_type',\n                    'text/html'\n                )\n            else:\n                best_default = acceptparse.MIMEAccept(\n                    accept\n                ).best_match(\n                    content_types.keys()\n                )\n\n                if best_default is None:\n                    msg = \"Controller '%s' defined does not support \" + \\\n                          \"content_type '%s'. Supported type(s): %s\"\n                    logger.error(\n                        msg % (\n                            controller.__name__,\n                            pecan_state['content_type'],\n                            content_types.keys()\n                        )\n                    )\n                    raise exc.HTTPNotAcceptable()\n\n                pecan_state['content_type'] = best_default\n        elif cfg.get('content_type') is not None and \\\n                pecan_state['content_type'] not in content_types:\n\n            msg = \"Controller '%s' defined does not support content_type \" + \\\n                  \"'%s'. Supported type(s): %s\"\n            logger.error(\n                msg % (\n                    controller.__name__,\n                    pecan_state['content_type'],\n                    content_types.keys()\n                )\n            )\n            raise exc.HTTPNotFound\n\n        # fetch any parameters\n        if req.method == 'GET':\n            params = req.GET\n        elif req.content_type in ('application/json',\n                                  'application/javascript'):\n            try:\n                if not isinstance(req.json, dict):\n                    raise TypeError('%s is not a dict' % req.json)\n                params = NestedMultiDict(req.GET, req.json)\n            except (TypeError, ValueError):\n                params = req.params\n        else:\n            params = req.params\n\n        # fetch the arguments for the controller\n        args, varargs, kwargs = self.get_args(\n            state,\n            params.mixed(),\n            remainder,\n            cfg['argspec'],\n            im_self\n        )\n        state.arguments = Arguments(args, varargs, kwargs)\n\n        # handle \"before\" hooks\n        self.handle_hooks(self.determine_hooks(controller), 'before', state)\n\n        return controller, args + varargs, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef invoke_controller(self, controller, args, kwargs, state):\n        '''\n        The main request handler for Pecan applications.\n        '''\n        cfg = _cfg(controller)\n        content_types = cfg.get('content_types', {})\n        req = state.request\n        resp = state.response\n        pecan_state = req.pecan\n\n        # If a keyword is supplied via HTTP GET or POST arguments, but the\n        # function signature does not allow it, just drop it (rather than\n        # generating a TypeError).\n        argspec = getargspec(controller)\n        keys = kwargs.keys()\n        for key in keys:\n            if key not in argspec.args and not argspec.keywords:\n                kwargs.pop(key)\n\n        # get the result from the controller\n        result = controller(*args, **kwargs)\n\n        # a controller can return the response object which means they've taken\n        # care of filling it out\n        if result is response:\n            return\n        elif isinstance(result, WebObResponse):\n            state.response = result\n            return\n\n        raw_namespace = result\n\n        # pull the template out based upon content type and handle overrides\n        template = content_types.get(pecan_state['content_type'])\n\n        # check if for controller override of template\n        template = pecan_state.get('override_template', template)\n        if template is None and cfg['explicit_content_type'] is False:\n            if self.default_renderer == 'json':\n                template = 'json'\n\n        pecan_state['content_type'] = pecan_state.get(\n            'override_content_type',\n            pecan_state['content_type']\n        )\n\n        # if there is a template, render it\n        if template:\n            if template == 'json':\n                pecan_state['content_type'] = 'application/json'\n            result = self.render(template, result)\n\n        # If we are in a test request put the namespace where it can be\n        # accessed directly\n        if req.environ.get('paste.testing'):\n            testing_variables = req.environ['paste.testing_variables']\n            testing_variables['namespace'] = raw_namespace\n            testing_variables['template_name'] = template\n            testing_variables['controller_output'] = result\n\n        # set the body content\n        if result and isinstance(result, six.text_type):\n            resp.text = result\n        elif result:\n            resp.body = result\n\n        if pecan_state['content_type']:\n            # set the content type\n            resp.content_type = pecan_state['content_type']", "response": "Invoke the controller and return the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvoke an interactive python shell.", "response": "def invoke(cls, ns, banner):  # pragma: nocover\n        \"\"\"\n        :param ns: local namespace\n        :param banner: interactive shell startup banner\n\n        Embed an interactive native python shell.\n        \"\"\"\n        import code\n        py_prefix = sys.platform.startswith('java') and 'J' or 'P'\n        shell_banner = 'Pecan Interactive Shell\\n%sython %s\\n\\n' % \\\n            (py_prefix, sys.version)\n        shell = code.InteractiveConsole(locals=ns)\n        try:\n            import readline  # noqa\n        except ImportError:\n            pass\n        shell.interact(shell_banner + banner)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef invoke(cls, ns, banner):  # pragma: nocover\n        try:\n            from IPython.frontend.terminal.embed import (\n                InteractiveShellEmbed\n            )\n            # try and load their default profile\n            from IPython.frontend.terminal.ipapp import (\n                load_default_config\n            )\n            config = load_default_config()\n            shell = InteractiveShellEmbed(config=config, banner2=banner)\n            shell(local_ns=ns)\n        except ImportError:\n            # Support for the IPython <= 0.10 shell API\n            from IPython.Shell import IPShellEmbed\n            shell = IPShellEmbed(argv=[])\n            shell.set_banner(shell.IP.BANNER + '\\n\\n' + banner)\n            shell(local_ns=ns, global_ns={})", "response": "Invoke the ipython interpreter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, args):\n        super(ShellCommand, self).run(args)\n\n        # load the application\n        app = self.load_app()\n\n        # prepare the locals\n        locs = dict(__name__='pecan-admin')\n        locs['wsgiapp'] = app\n        locs['app'] = TestApp(app)\n\n        model = self.load_model(app.config)\n        if model:\n            locs['model'] = model\n\n        # insert the pecan locals\n        from pecan import abort, conf, redirect, request, response\n        locs['abort'] = abort\n        locs['conf'] = conf\n        locs['redirect'] = redirect\n        locs['request'] = request\n        locs['response'] = response\n\n        # prepare the banner\n        banner = '  The following objects are available:\\n'\n        banner += '  %-10s - This project\\'s WSGI App instance\\n' % 'wsgiapp'\n        banner += '  %-10s - The current configuration\\n' % 'conf'\n        banner += '  %-10s - webtest.TestApp wrapped around wsgiapp\\n' % 'app'\n        if model:\n            model_name = getattr(\n                model,\n                '__module__',\n                getattr(model, '__name__', 'model')\n            )\n            banner += '  %-10s - Models from %s\\n' % ('model', model_name)\n\n        self.invoke_shell(locs, banner)", "response": "Load the pecan app prepare the locals sets the\n        banner and invokes the python shell."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninvokes the appropriate flavor of the python shell.", "response": "def invoke_shell(self, locs, banner):\n        \"\"\"\n        Invokes the appropriate flavor of the python shell.\n        Falls back on the native python shell if the requested\n        flavor (ipython, bpython,etc) is not installed.\n        \"\"\"\n        shell = self.SHELLS[self.args.shell]\n        try:\n            shell().invoke(locs, banner)\n        except ImportError as e:\n            warn((\n                \"%s is not installed, `%s`, \"\n                \"falling back to native shell\") % (self.args.shell, e),\n                RuntimeWarning\n            )\n            if shell == NativePythonShell:\n                raise\n            NativePythonShell().invoke(locs, banner)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_model(self, config):\n        for package_name in getattr(config.app, 'modules', []):\n            module = __import__(package_name, fromlist=['model'])\n            if hasattr(module, 'model'):\n                return module.model\n        return None", "response": "Load the model extension module"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_args_for_controller(self, controller):\n        argspec = getargspec(controller)\n        from pecan import request\n        try:\n            request.path\n        except AttributeError:\n            return argspec.args[3:]\n        return argspec.args[1:]", "response": "Retrieve the arguments we actually care about for a controller."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_bad_rest_arguments(self, controller, remainder, request):\n        argspec = self._get_args_for_controller(controller)\n        fixed_args = len(argspec) - len(\n            request.pecan.get('routing_args', [])\n        )\n        if len(remainder) < fixed_args:\n            # For controllers that are missing intermediate IDs\n            # (e.g., /authors/books vs /authors/1/books), return a 404 for an\n            # invalid path.\n            abort(404)", "response": "Ensure that the argspec for a discovered controller actually matched the positional arguments in the request path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nroutes a request to the appropriate controller and returns its result.", "response": "def _route(self, args, request=None):\n        '''\n        Routes a request to the appropriate controller and returns its result.\n\n        Performs a bit of validation - refuses to route delete and put actions\n        via a GET request).\n        '''\n        if request is None:\n            from pecan import request\n        # convention uses \"_method\" to handle browser-unsupported methods\n        method = request.params.get('_method', request.method).lower()\n\n        # make sure DELETE/PUT requests don't use GET\n        if request.method == 'GET' and method in ('delete', 'put'):\n            abort(405)\n\n        # check for nested controllers\n        result = self._find_sub_controllers(args, request)\n        if result:\n            return result\n\n        # handle the request\n        handler = getattr(\n            self,\n            '_handle_%s' % method,\n            self._handle_unknown_method\n        )\n\n        try:\n            if len(getargspec(handler).args) == 3:\n                result = handler(method, args)\n            else:\n                result = handler(method, args, request)\n\n            #\n            # If the signature of the handler does not match the number\n            # of remaining positional arguments, attempt to handle\n            # a _lookup method (if it exists)\n            #\n            argspec = self._get_args_for_controller(result[0])\n            num_args = len(argspec)\n            if num_args < len(args):\n                _lookup_result = self._handle_lookup(args, request)\n                if _lookup_result:\n                    return _lookup_result\n        except (exc.HTTPClientError, exc.HTTPNotFound,\n                exc.HTTPMethodNotAllowed) as e:\n            #\n            # If the matching handler results in a 400, 404, or 405, attempt to\n            # handle a _lookup method (if it exists)\n            #\n            _lookup_result = self._handle_lookup(args, request)\n            if _lookup_result:\n                return _lookup_result\n\n            # Build a correct Allow: header\n            if isinstance(e, exc.HTTPMethodNotAllowed):\n\n                def method_iter():\n                    for func in ('get', 'get_one', 'get_all', 'new', 'edit',\n                                 'get_delete'):\n                        if self._find_controller(func):\n                            yield 'GET'\n                            break\n                    for method in ('HEAD', 'POST', 'PUT', 'DELETE', 'TRACE',\n                                   'PATCH'):\n                        func = method.lower()\n                        if self._find_controller(func):\n                            yield method\n\n                e.allow = sorted(method_iter())\n\n            raise\n\n        # return the result\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_controller(self, *args):\n        '''\n        Returns the appropriate controller for routing a custom action.\n        '''\n        for name in args:\n            obj = self._lookup_child(name)\n            if obj and iscontroller(obj):\n                return obj\n        return None", "response": "Returns the appropriate controller for routing a custom action."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the correct controller to route to by analyzing the request URI.", "response": "def _find_sub_controllers(self, remainder, request):\n        '''\n        Identifies the correct controller to route to by analyzing the\n        request URI.\n        '''\n        # need either a get_one or get to parse args\n        method = None\n        for name in ('get_one', 'get'):\n            if hasattr(self, name):\n                method = name\n                break\n        if not method:\n            return\n\n        # get the args to figure out how much to chop off\n        args = self._get_args_for_controller(getattr(self, method))\n        fixed_args = len(args) - len(\n            request.pecan.get('routing_args', [])\n        )\n        var_args = getargspec(getattr(self, method)).varargs\n\n        # attempt to locate a sub-controller\n        if var_args:\n            for i, item in enumerate(remainder):\n                controller = self._lookup_child(item)\n                if controller and not ismethod(controller):\n                    self._set_routing_args(request, remainder[:i])\n                    return lookup_controller(controller, remainder[i + 1:],\n                                             request)\n        elif fixed_args < len(remainder) and hasattr(\n            self, remainder[fixed_args]\n        ):\n            controller = self._lookup_child(remainder[fixed_args])\n            if not ismethod(controller):\n                self._set_routing_args(request, remainder[:fixed_args])\n                return lookup_controller(\n                    controller,\n                    remainder[fixed_args + 1:],\n                    request\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle unknown HTTP methods.", "response": "def _handle_unknown_method(self, method, remainder, request=None):\n        '''\n        Routes undefined actions (like RESET) to the appropriate controller.\n        '''\n        if request is None:\n            self._raise_method_deprecation_warning(self._handle_unknown_method)\n\n        # try finding a post_{custom} or {custom} method first\n        controller = self._find_controller('post_%s' % method, method)\n        if controller:\n            return controller, remainder\n\n        # if no controller exists, try routing to a sub-controller; note that\n        # since this isn't a safe GET verb, any local exposes are 405'd\n        if remainder:\n            if self._find_controller(remainder[0]):\n                abort(405)\n            sub_controller = self._lookup_child(remainder[0])\n            if sub_controller:\n                return lookup_controller(sub_controller, remainder[1:],\n                                         request)\n\n        abort(405)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_get(self, method, remainder, request=None):\n        '''\n        Routes ``GET`` actions to the appropriate controller.\n        '''\n        if request is None:\n            self._raise_method_deprecation_warning(self._handle_get)\n\n        # route to a get_all or get if no additional parts are available\n        if not remainder or remainder == ['']:\n            remainder = list(six.moves.filter(bool, remainder))\n            controller = self._find_controller('get_all', 'get')\n            if controller:\n                self._handle_bad_rest_arguments(controller, remainder, request)\n                return controller, []\n            abort(405)\n\n        method_name = remainder[-1]\n        # check for new/edit/delete GET requests\n        if method_name in ('new', 'edit', 'delete'):\n            if method_name == 'delete':\n                method_name = 'get_delete'\n            controller = self._find_controller(method_name)\n            if controller:\n                return controller, remainder[:-1]\n\n        match = self._handle_custom_action(method, remainder, request)\n        if match:\n            return match\n\n        controller = self._lookup_child(remainder[0])\n        if controller and not ismethod(controller):\n            return lookup_controller(controller, remainder[1:], request)\n\n        # finally, check for the regular get_one/get requests\n        controller = self._find_controller('get_one', 'get')\n        if controller:\n            self._handle_bad_rest_arguments(controller, remainder, request)\n            return controller, remainder\n\n        abort(405)", "response": "Routes ``GET`` actions to the appropriate controller."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_delete(self, method, remainder, request=None):\n        '''\n        Routes ``DELETE`` actions to the appropriate controller.\n        '''\n        if request is None:\n            self._raise_method_deprecation_warning(self._handle_delete)\n\n        if remainder:\n            match = self._handle_custom_action(method, remainder, request)\n            if match:\n                return match\n\n            controller = self._lookup_child(remainder[0])\n            if controller and not ismethod(controller):\n                return lookup_controller(controller, remainder[1:], request)\n\n        # check for post_delete/delete requests first\n        controller = self._find_controller('post_delete', 'delete')\n        if controller:\n            return controller, remainder\n\n        # if no controller exists, try routing to a sub-controller; note that\n        # since this is a DELETE verb, any local exposes are 405'd\n        if remainder:\n            if self._find_controller(remainder[0]):\n                abort(405)\n            sub_controller = self._lookup_child(remainder[0])\n            if sub_controller:\n                return lookup_controller(sub_controller, remainder[1:],\n                                         request)\n\n        abort(405)", "response": "Routes ``DELETE`` actions to the appropriate controller."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_post(self, method, remainder, request=None):\n        '''\n        Routes ``POST`` requests.\n        '''\n        if request is None:\n            self._raise_method_deprecation_warning(self._handle_post)\n\n        # check for custom POST/PUT requests\n        if remainder:\n            match = self._handle_custom_action(method, remainder, request)\n            if match:\n                return match\n\n            controller = self._lookup_child(remainder[0])\n            if controller and not ismethod(controller):\n                return lookup_controller(controller, remainder[1:], request)\n\n        # check for regular POST/PUT requests\n        controller = self._find_controller(method)\n        if controller:\n            return controller, remainder\n\n        abort(405)", "response": "Handles a POST request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_line_context(filename, lineno, context=10):\n    '''\n    Formats the the line context for error rendering.\n\n    :param filename: the location of the file, within which the error occurred\n    :param lineno: the offending line number\n    :param context: number of lines of code to display before and after the\n                    offending line.\n    '''\n    with open(filename) as f:\n        lines = f.readlines()\n\n    lineno = lineno - 1  # files are indexed by 1 not 0\n    if lineno > 0:\n        start_lineno = max(lineno - context, 0)\n        end_lineno = lineno + context\n\n        lines = [escape(l, True) for l in lines[start_lineno:end_lineno]]\n        i = lineno - start_lineno\n        lines[i] = '<strong>%s</strong>' % lines[i]\n\n    else:\n        lines = [escape(l, True) for l in lines[:context]]\n    msg = '<pre style=\"background-color:#ccc;padding:2em;\">%s</pre>'\n    return msg % ''.join(lines)", "response": "Formats the line context for error rendering."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_ns(self, ns):\n        '''\n        Returns the `lazily` created template namespace.\n        '''\n        if self.namespace:\n            val = {}\n            val.update(self.namespace)\n            val.update(ns)\n            return val\n        else:\n            return ns", "response": "Returns the lazily created template namespace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, name, template_path):\n        '''\n        Returns the renderer object.\n\n        :param name: name of the requested renderer\n        :param template_path: path to the template\n        '''\n        if name not in self._renderers:\n            cls = self._renderer_classes.get(name)\n            if cls is None:\n                return None\n            else:\n                self._renderers[name] = cls(template_path, self.extra_vars)\n        return self._renderers[name]", "response": "Returns the renderer object for the requested renderer name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default(self, obj):\n        '''\n        Converts an object and returns a ``JSON``-friendly structure.\n\n        :param obj: object or structure to be converted into a\n                    ``JSON``-ifiable structure\n\n        Considers the following special cases in order:\n\n        * object has a callable __json__() attribute defined\n            returns the result of the call to __json__()\n        * date and datetime objects\n            returns the object cast to str\n        * Decimal objects\n            returns the object cast to float\n        * SQLAlchemy objects\n            returns a copy of the object.__dict__ with internal SQLAlchemy\n            parameters removed\n        * SQLAlchemy ResultProxy objects\n            Casts the iterable ResultProxy into a list of tuples containing\n            the entire resultset data, returns the list in a dictionary\n            along with the resultset \"row\" count.\n\n            .. note:: {'count': 5, 'rows': [('Ed Jones',), ('Pete Jones',),\n                ('Wendy Williams',), ('Mary Contrary',), ('Fred Smith',)]}\n\n        * SQLAlchemy RowProxy objects\n            Casts the RowProxy cursor object into a dictionary, probably\n            losing its ordered dictionary behavior in the process but\n            making it JSON-friendly.\n        * webob_dicts objects\n            returns webob_dicts.mixed() dictionary, which is guaranteed\n            to be JSON-friendly.\n        '''\n        if hasattr(obj, '__json__') and six.callable(obj.__json__):\n            return obj.__json__()\n        elif isinstance(obj, (date, datetime)):\n            return str(obj)\n        elif isinstance(obj, Decimal):\n            # XXX What to do about JSONEncoder crappy handling of Decimals?\n            # SimpleJSON has better Decimal encoding than the std lib\n            # but only in recent versions\n            return float(obj)\n        elif is_saobject(obj):\n            props = {}\n            for key in obj.__dict__:\n                if not key.startswith('_sa_'):\n                    props[key] = getattr(obj, key)\n            return props\n        elif isinstance(obj, ResultProxy):\n            props = dict(rows=list(obj), count=obj.rowcount)\n            if props['count'] < 0:\n                props['count'] = len(props['rows'])\n            return props\n        elif isinstance(obj, RowProxy):\n            return dict(obj)\n        elif isinstance(obj, webob_dicts):\n            return obj.mixed()\n        else:\n            return JSONEncoder.default(self, obj)", "response": "Converts an object or structure into a JSON - friendly structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getargspec(method):\n\n    argspec = _getargspec(method)\n    args = argspec[0]\n    if args and args[0] == 'self':\n        return argspec\n    if hasattr(method, '__func__'):\n        method = method.__func__\n\n    func_closure = six.get_function_closure(method)\n\n    # NOTE(sileht): if the closure is None we cannot look deeper,\n    # so return actual argspec, this occurs when the method\n    # is static for example.\n    if not func_closure:\n        return argspec\n\n    closure = None\n    # In the case of deeply nested decorators (with arguments), it's possible\n    # that there are several callables in scope;  Take a best guess and go\n    # with the one that looks most like a pecan controller function\n    # (has a __code__ object, and 'self' is the first argument)\n    func_closure = filter(\n        lambda c: (\n            six.callable(c.cell_contents) and\n            hasattr(c.cell_contents, '__code__')\n        ),\n        func_closure\n    )\n    func_closure = sorted(\n        func_closure,\n        key=lambda c: 'self' in c.cell_contents.__code__.co_varnames,\n        reverse=True\n    )\n\n    closure = func_closure[0]\n\n    method = closure.cell_contents\n    return getargspec(method)", "response": "Get the argspec of a method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gunicorn_run():\n    try:\n        from gunicorn.app.wsgiapp import WSGIApplication\n    except ImportError as exc:\n        args = exc.args\n        arg0 = args[0] if args else ''\n        arg0 += ' (are you sure `gunicorn` is installed?)'\n        exc.args = (arg0,) + args[1:]\n        raise\n\n    class PecanApplication(WSGIApplication):\n\n        def init(self, parser, opts, args):\n            if len(args) != 1:\n                parser.error(\"No configuration file was specified.\")\n\n            self.cfgfname = os.path.normpath(\n                os.path.join(os.getcwd(), args[0])\n            )\n            self.cfgfname = os.path.abspath(self.cfgfname)\n            if not os.path.exists(self.cfgfname):\n                parser.error(\"Config file not found: %s\" % self.cfgfname)\n\n            from pecan.configuration import _runtime_conf, set_config\n            set_config(self.cfgfname, overwrite=True)\n\n            # If available, use the host and port from the pecan config file\n            cfg = {}\n            if _runtime_conf.get('server'):\n                server = _runtime_conf['server']\n                if hasattr(server, 'host') and hasattr(server, 'port'):\n                    cfg['bind'] = '%s:%s' % (\n                        server.host, server.port\n                    )\n            return cfg\n\n        def load(self):\n            from pecan.deploy import deploy\n            return deploy(self.cfgfname)\n\n    PecanApplication(\"%(prog)s [OPTIONS] config.py\").run()", "response": "This command runs gunicorn_pecan."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides the log_message method from the wsgiref server so that it can be used to log a message.", "response": "def log_message(self, format, *args):\n        \"\"\"\n        overrides the ``log_message`` method from the wsgiref server so that\n        normal logging works with whatever configuration the application has\n        been set to.\n\n        Levels are inferred from the HTTP status code, 4XX codes are treated as\n        warnings, 5XX as errors and everything else as INFO level.\n        \"\"\"\n        code = args[1][0]\n        levels = {\n            '4': 'warning',\n            '5': 'error'\n        }\n\n        log_handler = getattr(logger, levels.get(code, 'info'))\n        log_handler(format % args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeciding if a request should be wrapped in a transaction based on the state of the request.", "response": "def is_transactional(self, state):\n        '''\n        Decide if a request should be wrapped in a transaction, based\n        upon the state of the request. By default, wraps all but ``GET``\n        and ``HEAD`` requests in a transaction, along with respecting\n        the ``transactional`` decorator from :mod:pecan.decorators.\n\n        :param state: The Pecan state object for the current request.\n        '''\n\n        controller = getattr(state, 'controller', None)\n        if controller:\n            force_transactional = _cfg(controller).get('transactional', False)\n        else:\n            force_transactional = False\n\n        if state.request.method not in ('GET', 'HEAD') or force_transactional:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_controller(self, state):\n        '''\n        Retrieves the actual controller name from the application\n        Specific to Pecan (not available in the request object)\n        '''\n        path = state.request.pecan['routing_path'].split('/')[1:]\n        return state.controller.__str__().split()[2]", "response": "Retrieves the actual controller name from the application\n        Specific to Pecan"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_hooks(self, hooks):\n        '''\n        Tries to format the hook objects to be more readable\n        Specific to Pecan (not available in the request object)\n        '''\n        str_hooks = [str(i).split()[0].strip('<') for i in hooks]\n        return [i.split('.')[-1] for i in str_hooks if '.' in i]", "response": "Tries to format the hook objects to be more readable\n        Specific to Pecan"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef after_action(action_type, action):\n    '''\n    If utilizing the :mod:`pecan.hooks` ``TransactionHook``, allows you\n    to flag a controller method to perform a callable action after the\n    action_type is successfully issued.\n\n    :param action: The callable to call after the commit is successfully\n    issued.  '''\n\n    if action_type not in ('commit', 'rollback'):\n        raise Exception('action_type (%s) is not valid' % action_type)\n\n    def deco(func):\n        _cfg(func).setdefault('after_%s' % action_type, []).append(action)\n        return func\n    return deco", "response": "Decorator to mark a controller method as after_action."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self):\n    return {\n      \"timestamp\": int(self.timestamp),\n      \"timezone\": self.timezone,\n      \"time_of_day\": self.time_of_day,\n      \"day_of_week\": self.day_of_week,\n      \"day_of_month\": self.day_of_month,\n      \"month_of_year\": self.month_of_year,\n      \"utc_iso\": self.utc_iso\n    }", "response": "Returns the Time instance as a usable dictionary for craftai"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timestamp_from_datetime(date_time):\n    if date_time.tzinfo is None:\n      return time.mktime((date_time.year, date_time.month, date_time.day, date_time.hour,\n                          date_time.minute, date_time.second,\n                          -1, -1, -1)) + date_time.microsecond / 1e6\n    return (date_time - _EPOCH).total_seconds()", "response": "Returns POSIX timestamp as float"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an agent in the craftai server.", "response": "def create_agent(self, configuration, agent_id=\"\"):\n    \"\"\"Create an agent.\n\n    :param dict configuration: Form given by the craftai documentation.\n    :param str agent_id: Optional. The id of the agent to create. It\n    must be an str containing only characters in \"a-zA-Z0-9_-\" and\n    must be between 1 and 36 characters.\n    :default agent_id: \"\", the agent_id is generated.\n\n    :return: agent created.\n    :rtype: dict.\n\n    :raise CraftAiBadRequestError: if the input is not of\n    the right form.\n    \"\"\"\n    # Extra header in addition to the main session's\n    ct_header = {\"Content-Type\": \"application/json; charset=utf-8\"}\n\n    # Building payload and checking that it is valid for a JSON\n    # serialization\n    payload = {\"configuration\": configuration}\n\n    if agent_id != \"\":\n      # Raises an error when agent_id is invalid\n      self._check_agent_id(agent_id)\n\n      payload[\"id\"] = agent_id\n\n    try:\n      json_pl = json.dumps(payload)\n    except TypeError as err:\n      raise CraftAiBadRequestError(\"Invalid configuration or agent id given. {}\"\n                                   .format(err.__str__()))\n\n    req_url = \"{}/agents\".format(self._base_url)\n    resp = self._requests_session.post(req_url, headers=ct_header, data=json_pl)\n\n    agent = self._decode_response(resp)\n\n    return agent"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete an agent. :param str agent_id: The id of the agent to delete. It must be an str containing only characters in \"a-zA-Z0-9_-\" and must be between 1 and 36 characters. :return: agent deleted. :rtype: dict.", "response": "def delete_agent(self, agent_id):\n    \"\"\"Delete an agent.\n\n    :param str agent_id: The id of the agent to delete. It must\n    be an str containing only characters in \"a-zA-Z0-9_-\" and\n    must be between 1 and 36 characters.\n\n    :return: agent deleted.\n    :rtype: dict.\n    \"\"\"\n    # Raises an error when agent_id is invalid\n    self._check_agent_id(agent_id)\n\n    req_url = \"{}/agents/{}\".format(self._base_url, agent_id)\n    resp = self._requests_session.delete(req_url)\n\n    decoded_resp = self._decode_response(resp)\n\n    return decoded_resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_agents_bulk(self, payload):\n    # Check all ids, raise an error if all ids are invalid\n    valid_indices, invalid_indices, invalid_agents = self._check_agent_id_bulk(payload)\n\n    # Create the json file with the agents with valid id and send it\n    valid_agents = self._create_and_send_json_bulk([payload[i] for i in valid_indices],\n                                                   \"{}/bulk/agents\".format(self._base_url),\n                                                   \"DELETE\")\n\n    if invalid_indices == []:\n      return valid_agents\n\n    # Put the valid and invalid agents in their original index\n    return self._recreate_list_with_indices(valid_indices,\n                                            valid_agents,\n                                            invalid_indices,\n                                            invalid_agents)", "response": "Delete a group of agents in bulk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding operations to an agent.", "response": "def add_operations(self, agent_id, operations):\n    \"\"\"Add operations to an agent.\n\n    :param str agent_id: The id of the agent to delete. It must be\n    an str containing only characters in \"a-zA-Z0-9_-\" and must be\n    between 1 and 36 characters. It must referenced an existing agent.\n    :param list operations: Contains dictionnaries that has the\n    form given in the craftai documentation and the configuration\n    of the agent.\n\n    :return: message about the added operations.\n    :rtype: str\n\n    :raise CraftAiBadRequestError: if the input is not of\n    the right form.\n    \"\"\"\n    # Raises an error when agent_id is invalid\n    self._check_agent_id(agent_id)\n\n    # Extra header in addition to the main session's\n    ct_header = {\"Content-Type\": \"application/json; charset=utf-8\"}\n    offset = 0\n\n    is_looping = True\n\n    while is_looping:\n      next_offset = offset + self.config[\"operationsChunksSize\"]\n\n      try:\n        json_pl = json.dumps(operations[offset:next_offset])\n      except TypeError as err:\n        raise CraftAiBadRequestError(\"Invalid configuration or agent id given. {}\"\n                                     .format(err.__str__()))\n\n      req_url = \"{}/agents/{}/context\".format(self._base_url, agent_id)\n      resp = self._requests_session.post(req_url, headers=ct_header, data=json_pl)\n      self._decode_response(resp)\n\n      if next_offset >= len(operations):\n        is_looping = False\n\n      offset = next_offset\n\n    return {\n      \"message\": \"Successfully added %i operation(s) to the agent \\\"%s/%s/%s\\\" context.\"\n                 % (len(operations), self.config[\"owner\"], self.config[\"project\"], agent_id)\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_operations_bulk(self, chunked_data):\n    url = \"{}/bulk/context\".format(self._base_url)\n    ct_header = {\"Content-Type\": \"application/json; charset=utf-8\"}\n\n    responses = []\n    for chunk in chunked_data:\n      if len(chunk) > 1:\n        try:\n          json_pl = json.dumps(chunk)\n        except TypeError as err:\n          raise CraftAiBadRequestError(\"Error while dumping the payload into json\"\n                                       \"format when converting it for the bulk request. {}\"\n                                       .format(err.__str__()))\n        resp = self._requests_session.post(url, headers=ct_header, data=json_pl)\n        resp = self._decode_response(resp)\n        responses += resp\n      elif chunk:\n        message = self.add_operations(chunk[0][\"id\"], chunk[0][\"operations\"])\n        responses.append({\"id\": chunk[0][\"id\"], \"status\": 201, \"message\": message})\n\n    if responses == []:\n      raise CraftAiBadRequestError(\"Invalid or empty set of operations given\")\n\n    return responses", "response": "This function sends the requests to the agents and adds the operations to the agents."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_operations_bulk(self, payload):\n    # Check all ids, raise an error if all ids are invalid\n    valid_indices, _, _ = self._check_agent_id_bulk(payload)\n    valid_payload = [payload[i] for i in valid_indices]\n\n    chunked_data = []\n    current_chunk = []\n    current_chunk_size = 0\n\n    for agent in valid_payload:\n      if (agent[\"operations\"] and isinstance(agent[\"operations\"], list)):\n        if current_chunk_size + len(agent[\"operations\"]) > self.config[\"operationsChunksSize\"]:\n          chunked_data.append(current_chunk)\n          current_chunk_size = 0\n          current_chunk = []\n        if len(agent[\"operations\"]) > self.config[\"operationsChunksSize\"]:\n          chunked_data.append([agent])\n          current_chunk_size = 0\n        else:\n          current_chunk_size += len(agent[\"operations\"])\n          current_chunk.append(agent)\n\n    if current_chunk:\n      chunked_data.append(current_chunk)\n\n    return self._add_operations_bulk(chunked_data)", "response": "Add operations to a group of agents."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntools for the function get_decision_tree. :param str agent_id: the id of the agent to get the tree. It must be an str containing only characters in \"a-zA-Z0-9_-\" and must be between 1 and 36 characters. :param int timestamp: Optional. The decision tree is comptuted at this timestamp. :default timestamp: None, means that we get the tree computed with all its context history. :param version: version of the tree to get. :type version: str or int :default version: default version of the tree. :return: decision tree. :rtype: dict.", "response": "def _get_decision_tree(self, agent_id, timestamp, version):\n    \"\"\"Tool for the function get_decision_tree.\n\n    :param str agent_id: the id of the agent to get the tree. It\n    must be an str containing only characters in \"a-zA-Z0-9_-\" and\n    must be between 1 and 36 characters.\n    :param int timestamp: Optional. The decision tree is comptuted\n    at this timestamp.\n    :default timestamp: None, means that we get the tree computed\n    with all its context history.\n    :param version: version of the tree to get.\n    :type version: str or int\n    :default version: default version of the tree.\n\n    :return: decision tree.\n    :rtype: dict.\n    \"\"\"\n    headers = self._headers.copy()\n    headers[\"x-craft-ai-tree-version\"] = version\n    # If we give no timestamp the default behaviour is to give the tree from the latest timestamp\n    if timestamp is None:\n      req_url = \"{}/agents/{}/decision/tree?\".format(self._base_url, agent_id)\n    else:\n      req_url = \"{}/agents/{}/decision/tree?t={}\".format(self._base_url, agent_id, timestamp)\n\n    resp = self._requests_session.get(req_url)\n\n    decision_tree = self._decode_response(resp)\n\n    return decision_tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the decision tree for the given agent.", "response": "def get_decision_tree(self, agent_id, timestamp=None, version=DEFAULT_DECISION_TREE_VERSION):\n    \"\"\"Get decision tree.\n\n    :param str agent_id: the id of the agent to get the tree. It\n    must be an str containing only characters in \"a-zA-Z0-9_-\" and\n    must be between 1 and 36 characters.\n    :param int timestamp: Optional. The decision tree is comptuted\n    at this timestamp.\n    :default timestamp: None, means that we get the tree computed\n    with all its context history.\n    :param version: version of the tree to get.\n    :type version: str or int.\n    :default version: default version of the tree.\n\n    :return: decision tree.\n    :rtype: dict.\n\n    :raises CraftAiLongRequestTimeOutError: if the API doesn't get\n    the tree in the time given by the configuration.\n    \"\"\"\n    # Raises an error when agent_id is invalid\n    self._check_agent_id(agent_id)\n    if self._config[\"decisionTreeRetrievalTimeout\"] is False:\n      # Don't retry\n      return self._get_decision_tree(agent_id, timestamp, version)\n\n    start = current_time_ms()\n    while True:\n      now = current_time_ms()\n      if now - start > self._config[\"decisionTreeRetrievalTimeout\"]:\n        # Client side timeout\n        raise CraftAiLongRequestTimeOutError()\n      try:\n        return self._get_decision_tree(agent_id, timestamp, version)\n      except CraftAiLongRequestTimeOutError:\n        # Do nothing and continue.\n        continue"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntool for the function get_decision_trees_bulk. :param list payload: contains the informations necessary for getting the trees. Its form is the same than for the function. get_decision_trees_bulk. :param list valid_indices: list of the indices of the valid agent id. :param list invalid_indices: list of the indices of the valid agent id. :param list invalid_dts: list of the invalid agent id. :return: decision trees. :rtype: list of dict.", "response": "def _get_decision_trees_bulk(self, payload, valid_indices, invalid_indices, invalid_dts):\n    \"\"\"Tool for the function get_decision_trees_bulk.\n\n    :param list payload: contains the informations necessary for getting\n    the trees. Its form is the same than for the function.\n    get_decision_trees_bulk.\n    :param list valid_indices: list of the indices of the valid agent id.\n    :param list invalid_indices: list of the indices of the valid agent id.\n    :param list invalid_dts: list of the invalid agent id.\n\n    :return: decision trees.\n    :rtype: list of dict.\n    \"\"\"\n    valid_dts = self._create_and_send_json_bulk([payload[i] for i in valid_indices],\n                                                \"{}/bulk/decision_tree\".format(self._base_url),\n                                                \"POST\")\n\n    if invalid_indices == []:\n      return valid_dts\n\n    # Put the valid and invalid decision trees in their original index\n    return self._recreate_list_with_indices(valid_indices, valid_dts, invalid_indices, invalid_dts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_decision_trees_bulk(self, payload, version=DEFAULT_DECISION_TREE_VERSION):\n    # payload = [{\"id\": agent_id, \"timestamp\": timestamp}]\n    headers = self._headers.copy()\n    headers[\"x-craft-ai-tree-version\"] = version\n\n    # Check all ids, raise an error if all ids are invalid\n    valid_indices, invalid_indices, invalid_dts = self._check_agent_id_bulk(payload)\n\n    if self._config[\"decisionTreeRetrievalTimeout\"] is False:\n      # Don't retry\n      return self._get_decision_trees_bulk(payload,\n                                           valid_indices,\n                                           invalid_indices,\n                                           invalid_dts)\n    start = current_time_ms()\n    while True:\n      now = current_time_ms()\n      if now - start > self._config[\"decisionTreeRetrievalTimeout\"]:\n        # Client side timeout\n        raise CraftAiLongRequestTimeOutError()\n      try:\n        return self._get_decision_trees_bulk(payload,\n                                             valid_indices,\n                                             invalid_indices,\n                                             invalid_dts)\n      except CraftAiLongRequestTimeOutError:\n        # Do nothing and continue.\n        continue", "response": "Get a group of decision trees in bulk."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decode_response(response):\n    status_code = response.status_code\n\n    message = \"Status code \" + str(status_code)\n    try:\n      message = CraftAIClient._parse_body(response)[\"message\"]\n    except (CraftAiInternalError, KeyError, TypeError):\n      pass\n\n    if status_code in [200, 201, 204, 207]:\n      return CraftAIClient._parse_body(response)\n    else:\n      raise CraftAIClient._get_error_from_status(status_code, message)\n    return None", "response": "Decode the response of a request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _decode_response_bulk(response_bulk):\n    resp = []\n    for response in response_bulk:\n      if (\"status\" in response) and (response.get(\"status\") == 201):\n        agent = {\"id\": response[\"id\"],\n                 \"message\": response[\"message\"]}\n        resp.append(agent)\n      elif \"status\" in response:\n        status_code = response[\"status\"]\n        message = response[\"message\"]\n        agent = {\"error\": CraftAIClient._get_error_from_status(status_code, message)}\n        try:\n          agent[\"id\"] = response[\"id\"]\n        except KeyError:\n          pass\n        resp.append(agent)\n\n      else:\n        resp.append(response)\n\n    return resp", "response": "Decode the response of each agent given by a bulk function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_error_from_status(status_code, message):\n    if status_code == 202:\n      err = CraftAiLongRequestTimeOutError(message)\n    elif status_code == 401 or status_code == 403:\n      err = CraftAiCredentialsError(message)\n    elif status_code == 400:\n      err = CraftAiBadRequestError(message)\n    elif status_code == 404:\n      err = CraftAiNotFoundError(message)\n    elif status_code == 413:\n      err = CraftAiBadRequestError(\"Given payload is too large\")\n    elif status_code == 500:\n      err = CraftAiInternalError(message)\n    elif status_code == 503:\n      err = CraftAiNetworkError(\"\"\"Service momentarily unavailable, please try\"\"\"\n                                \"\"\"again in a few minutes. If the problem \"\"\"\n                                \"\"\"persists please contact us at support@craft.ai\"\"\")\n    elif status_code == 504:\n      err = CraftAiBadRequestError(\"Request has timed out\")\n    else:\n      err = CraftAiUnknownError(message)\n\n    return err", "response": "Give the error corresponding to the status code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_agent_id(agent_id):\n    if (not isinstance(agent_id, six.string_types) or\n        AGENT_ID_PATTERN.match(agent_id) is None):\n      raise CraftAiBadRequestError(ERROR_ID_MESSAGE)", "response": "Checks that the given agent_id is a valid non - empty string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_agent_id_bulk(self, payload):\n    invalid_agent_indices = []\n    valid_agent_indices = []\n    invalid_payload = []\n    for index, agent in enumerate(payload):\n      # Check if the agent ID is valid\n      try:\n        if \"id\" in agent:\n          self._check_agent_id(agent[\"id\"])\n      except CraftAiBadRequestError:\n        invalid_agent_indices.append(index)\n        invalid_payload.append({\"id\": agent[\"id\"],\n                                \"error\": CraftAiBadRequestError(ERROR_ID_MESSAGE)})\n      else:\n        # Check if the agent is serializable\n        try:\n          json.dumps([agent])\n        except TypeError as err:\n          invalid_agent_indices.append(index)\n          invalid_payload.append({\"id\": agent[\"id\"],\n                                  \"error\": err})\n        else:\n          valid_agent_indices.append(index)\n\n    if len(invalid_agent_indices) == len(payload):\n      raise CraftAiBadRequestError(ERROR_ID_MESSAGE)\n\n    return valid_agent_indices, invalid_agent_indices, invalid_payload", "response": "Checks that all the given agent ids are valid non - empty strings\n    and if the agents are serializable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a list of values with the given indices.", "response": "def _recreate_list_with_indices(indices1, values1, indices2, values2):\n    \"\"\"Create a list in the right order.\n\n    :param list indices1: contains the list of indices corresponding to\n    the values in values1.\n    :param list values1: contains the first list of values.\n    :param list indices2: contains the list of indices corresponding to\n    the values in values2.\n    :param list values2: contains the second list of values.\n\n    :return: list of the values in the correct order.\n    :rtype: list.\n    \"\"\"\n    # Test if indices are continuous\n    list_indices = sorted(indices1 + indices2)\n    for i, index in enumerate(list_indices):\n      if i != index:\n        raise CraftAiInternalError(\"The agents's indices are not continuous\")\n\n    full_list = [None] * (len(indices1) + len(indices2))\n    for i, index in enumerate(indices1):\n      full_list[index] = values1[i]\n    for i, index in enumerate(indices2):\n      full_list[index] = values2[i]\n    return full_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_and_send_json_bulk(self, payload, req_url, request_type=\"POST\"):\n    # Extra header in addition to the main session's\n    ct_header = {\"Content-Type\": \"application/json; charset=utf-8\"}\n\n    try:\n      json_pl = json.dumps(payload)\n    except TypeError as err:\n      raise CraftAiBadRequestError(\"Error while dumping the payload into json\"\n                                   \"format when converting it for the bulk request. {}\"\n                                   .format(err.__str__()))\n\n    if request_type == \"POST\":\n      resp = self._requests_session.post(req_url, headers=ct_header, data=json_pl)\n    elif request_type == \"DELETE\":\n      resp = self._requests_session.delete(req_url, headers=ct_header, data=json_pl)\n    else:\n      raise CraftAiBadRequestError(\"Request for the bulk API should be either a POST or DELETE\"\n                                   \"request\")\n\n    agents = self._decode_response(resp)\n    agents = self._decode_response_bulk(agents)\n\n    return agents", "response": "Create a json request to the URL and process the response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_ctcp(query):\n    query = query.strip(CTCP_DELIMITER)\n    query = query.replace(CTCP_ESCAPE_CHAR + '0', '\\0')\n    query = query.replace(CTCP_ESCAPE_CHAR + 'n', '\\n')\n    query = query.replace(CTCP_ESCAPE_CHAR + 'r', '\\r')\n    query = query.replace(CTCP_ESCAPE_CHAR + CTCP_ESCAPE_CHAR, CTCP_ESCAPE_CHAR)\n    if ' ' in query:\n        return query.split(' ', 1)\n    return query, None", "response": "Strip and de - quote CTCP messages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreply to the CTCP version.", "response": "async def on_ctcp_version(self, by, target, contents):\n        \"\"\" Built-in CTCP version as some networks seem to require it. \"\"\"\n        import pydle\n\n        version = '{name} v{ver}'.format(name=pydle.__name__, ver=pydle.__version__)\n        self.ctcp_reply(by, 'VERSION', version)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a CTCP request to a target.", "response": "async def ctcp(self, target, query, contents=None):\n        \"\"\" Send a CTCP request to a target. \"\"\"\n        if self.is_channel(target) and not self.in_channel(target):\n            raise client.NotInChannel(target)\n\n        await self.message(target, construct_ctcp(query, contents))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a CTCP reply to a target.", "response": "async def ctcp_reply(self, target, query, response):\n        \"\"\" Send a CTCP reply to a target. \"\"\"\n        if self.is_channel(target) and not self.in_channel(target):\n            raise client.NotInChannel(target)\n\n        await self.notice(target, construct_ctcp(query, response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmodifies PRIVMSG to redirect CTCP messages.", "response": "async def on_raw_privmsg(self, message):\n        \"\"\" Modify PRIVMSG to redirect CTCP messages. \"\"\"\n        nick, metadata = self._parse_user(message.source)\n        target, msg = message.params\n\n        if is_ctcp(msg):\n            self._sync_user(nick, metadata)\n            type, contents = parse_ctcp(msg)\n\n            # Find dedicated handler if it exists.\n            attr = 'on_ctcp_' + pydle.protocol.identifierify(type)\n            if hasattr(self, attr):\n                await getattr(self, attr)(nick, target, contents)\n            # Invoke global handler.\n            await self.on_ctcp(nick, target, type, contents)\n        else:\n            await super().on_raw_privmsg(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def on_raw_notice(self, message):\n        nick, metadata = self._parse_user(message.source)\n        target, msg = message.params\n\n        if is_ctcp(msg):\n            self._sync_user(nick, metadata)\n            type, response = parse_ctcp(msg)\n\n            # Find dedicated handler if it exists.\n            attr = 'on_ctcp_' + pydle.protocol.identifierify(type) + '_reply'\n            if hasattr(self, attr):\n                await getattr(self, attr)(user, target, response)\n            # Invoke global handler.\n            await self.on_ctcp_reply(user, target, type, response)\n        else:\n            await super().on_raw_notice(message)", "response": "Modify NOTICE to redirect CTCP messages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhijacking registration to send a CAP LS first.", "response": "async def _register(self):\n        \"\"\" Hijack registration to send a CAP LS first. \"\"\"\n        if self.registered:\n            self.logger.debug(\"skipping cap registration, already registered!\")\n            return\n\n        # Ask server to list capabilities.\n        await self.rawmsg('CAP', 'LS', '302')\n\n        # Register as usual.\n        await super()._register()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _capability_negotiated(self, capab):\n        self._capabilities_negotiating.discard(capab)\n\n        if not self._capabilities_requested and not self._capabilities_negotiating:\n            await self.rawmsg('CAP', 'END')", "response": "Mark the capability as negotiated and end negotiation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def on_raw_cap_ls(self, params):\n        to_request = set()\n\n        for capab in params[0].split():\n            capab, value = self._capability_normalize(capab)\n\n            # Only process new capabilities.\n            if capab in self._capabilities:\n                continue\n\n            # Check if we support the capability.\n            attr = 'on_capability_' + pydle.protocol.identifierify(capab) + '_available'\n            supported = (await getattr(self, attr)(value)) if hasattr(self, attr) else False\n\n            if supported:\n                if isinstance(supported, str):\n                    to_request.add(capab + CAPABILITY_VALUE_DIVIDER + supported)\n                else:\n                    to_request.add(capab)\n            else:\n                self._capabilities[capab] = False\n\n        if to_request:\n            # Request some capabilities.\n            self._capabilities_requested.update(x.split(CAPABILITY_VALUE_DIVIDER, 1)[0] for x in to_request)\n            await self.rawmsg('CAP', 'REQ', ' '.join(to_request))\n        else:\n            # No capabilities requested, end negotiation.\n            await self.rawmsg('CAP', 'END')", "response": "Update the capabilities mapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def on_raw_cap_ack(self, params):\n        for capab in params[0].split():\n            cp, value = self._capability_normalize(capab)\n            self._capabilities_requested.discard(cp)\n\n            # Determine capability type and callback.\n            if capab.startswith(DISABLED_PREFIX):\n                self._capabilities[cp] = False\n                attr = 'on_capability_' + pydle.protocol.identifierify(cp) + '_disabled'\n            elif capab.startswith(STICKY_PREFIX):\n                # Can't disable it. Do nothing.\n                self.logger.error('Could not disable capability %s.', cp)\n                continue\n            else:\n                self._capabilities[cp] = value if value else True\n                attr = 'on_capability_' + pydle.protocol.identifierify(cp) + '_enabled'\n\n            # Indicate we're gonna use this capability if needed.\n            if capab.startswith(ACKNOWLEDGEMENT_REQUIRED_PREFIX):\n                await self.rawmsg('CAP', 'ACK', cp)\n\n            # Run callback.\n            if hasattr(self, attr):\n                status = await getattr(self, attr)()\n            else:\n                status = NEGOTIATED\n\n            # If the process needs more time, add it to the database and end later.\n            if status == NEGOTIATING:\n                self._capabilities_negotiating.add(cp)\n            elif status == FAILED:\n                # Ruh-roh, negotiation failed. Disable the capability.\n                self.logger.warning('Capability negotiation for %s failed. Attempting to disable capability again.', cp)\n\n                await self.rawmsg('CAP', 'REQ', '-' + cp)\n                self._capabilities_requested.add(cp)\n\n        # If we have no capabilities left to process, end it.\n        if not self._capabilities_requested and not self._capabilities_negotiating:\n            await self.rawmsg('CAP', 'END')", "response": "Update active capabilities: requested capability accepted."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate active capabilities: requested capability rejected.", "response": "async def on_raw_cap_nak(self, params):\n        \"\"\" Update active capabilities: requested capability rejected. \"\"\"\n        for capab in params[0].split():\n            capab, _ = self._capability_normalize(capab)\n            self._capabilities[capab] = False\n            self._capabilities_requested.discard(capab)\n\n        # If we have no capabilities left to process, end it.\n        if not self._capabilities_requested and not self._capabilities_negotiating:\n            await self.rawmsg('CAP', 'END')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def on_raw_410(self, message):\n        self.logger.error('Server sent \"Unknown CAP subcommand: %s\". Aborting capability negotiation.', message.params[0])\n\n        self._capabilities_requested = set()\n        self._capabilities_negotiating = set()\n        await self.rawmsg('CAP', 'END')", "response": "Unknown CAP subcommand or CAP error. Force - end negotiations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrespond to SASL challenge with response.", "response": "async def _sasl_respond(self):\n        \"\"\" Respond to SASL challenge with response. \"\"\"\n        # Formulate a response.\n        if self._sasl_client:\n            try:\n                response = self._sasl_client.process(self._sasl_challenge)\n            except puresasl.SASLError:\n                response = None\n\n            if response is None:\n                self.logger.warning('SASL challenge processing failed: aborting SASL authentication.')\n                await self._sasl_abort()\n        else:\n            response = b''\n\n        response = base64.b64encode(response).decode(self.encoding)\n        to_send = len(response)\n        self._sasl_challenge = b''\n\n        # Send response in chunks.\n        while to_send > 0:\n            await self.rawmsg('AUTHENTICATE', response[:RESPONSE_LIMIT])\n            response = response[RESPONSE_LIMIT:]\n            to_send -= RESPONSE_LIMIT\n\n        # If our message fit exactly in SASL_RESPOSE_LIMIT-byte chunks, send an empty message to indicate we're done.\n        if to_send == 0:\n            await self.rawmsg('AUTHENTICATE', EMPTY_MESSAGE)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def on_capability_sasl_available(self, value):\n        if value:\n            self._sasl_mechanisms = value.upper().split(',')\n        else:\n            self._sasl_mechanisms = None\n\n        if self.sasl_mechanism == 'EXTERNAL' or (self.sasl_username and self.sasl_password):\n            if self.sasl_mechanism == 'EXTERNAL' or puresasl:\n                return True\n            self.logger.warning('SASL credentials set but puresasl module not found: not initiating SASL authentication.')\n        return False", "response": "Check whether or not SASL is available."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle the authentication challenge.", "response": "async def on_raw_authenticate(self, message):\n        \"\"\" Received part of the authentication challenge. \"\"\"\n        # Cancel timeout timer.\n        if self._sasl_timer:\n            self._sasl_timer.cancel()\n            self._sasl_timer = None\n\n        # Add response data.\n        response = ' '.join(message.params)\n        if response != EMPTY_MESSAGE:\n            self._sasl_challenge += base64.b64decode(response)\n\n        # If the response ain't exactly SASL_RESPONSE_LIMIT bytes long, it's the end. Process.\n        if len(response) % RESPONSE_LIMIT > 0:\n            await self._sasl_respond()\n        else:\n            # Response not done yet. Restart timer.\n            self._sasl_timer = self.eventloop.call_later(self.SASL_TIMEOUT, self._sasl_abort(timeout=True))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts monitoring the online status of a user. Returns whether or not the server supports monitoring.", "response": "def monitor(self, target):\n        \"\"\" Start monitoring the online status of a user. Returns whether or not the server supports monitoring. \"\"\"\n        if 'monitor-notify' in self._capabilities and not self.is_monitoring(target):\n            yield from self.rawmsg('MONITOR', '+', target)\n            self._monitoring.add(target)\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping monitoring the online status of a user. Returns whether or not the server supports monitoring.", "response": "def unmonitor(self, target):\n        \"\"\" Stop monitoring the online status of a user. Returns whether or not the server supports monitoring. \"\"\"\n        if 'monitor-notify' in self._capabilities and self.is_monitoring(target):\n            yield from self.rawmsg('MONITOR', '-', target)\n            self._monitoring.remove(target)\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsays we are monitoring got offline.", "response": "async def on_raw_731(self, message):\n        \"\"\" Someone we are monitoring got offline. \"\"\"\n        for nick in message.params[1].split(','):\n            self._destroy_user(nick, monitor_override=True)\n            await self.on_user_offline(nickname)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def on_raw_chghost(self, message):\n        if 'chghost' not in self._capabilities or not self._capabilities['chghost']:\n            return\n\n        nick, _ = self._parse_user(message.source)\n        if nick not in self.users:\n            return\n\n        # Update user and host.\n        metadata = {\n            'username': message.params[0],\n            'hostname': message.params[1]\n        }\n        self._sync_user(nick, metadata)", "response": "Change user and host of user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges in the associated account for a nickname.", "response": "async def on_raw_account(self, message):\n        \"\"\" Changes in the associated account for a nickname. \"\"\"\n        if not self._capabilities.get('account-notify', False):\n            return\n\n        nick, metadata = self._parse_user(message.source)\n        account = message.params[0]\n\n        if nick not in self.users:\n            return\n\n        self._sync_user(nick, metadata)\n        if account == NO_ACCOUNT:\n            self._sync_user(nick, { 'account': None, 'identified': False })\n        else:\n            self._sync_user(nick, { 'account': account, 'identified': True })"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing extended JOIN messages.", "response": "async def on_raw_join(self, message):\n        \"\"\" Process extended JOIN messages. \"\"\"\n        if 'extended-join' in self._capabilities and self._capabilities['extended-join']:\n            nick, metadata = self._parse_user(message.source)\n            channels, account, realname = message.params\n\n            self._sync_user(nick, metadata)\n\n            # Emit a fake join message.\n            fakemsg = self._create_message('JOIN', channels, source=message.source)\n            await super().on_raw_join(fakemsg)\n\n            if account == NO_ACCOUNT:\n                account = None\n            self.users[nick]['account'] = account\n            self.users[nick]['realname'] = realname\n        else:\n            await super().on_raw_join(message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget user metadata information.", "response": "async def get_metadata(self, target):\n        \"\"\"\n        Return user metadata information.\n        This is a blocking asynchronous method: it has to be called from a coroutine, as follows:\n\n            metadata = await self.get_metadata('#foo')\n        \"\"\"\n        if target not in self._pending['metadata']:\n            await self.rawmsg('METADATA', target, 'LIST')\n\n            self._metadata_queue.append(target)\n            self._metadata_info[target] = {}\n            self._pending['metadata'][target] = self.eventloop.create_future()\n\n        return self._pending['metadata'][target]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def on_raw_760(self, message):\n        target, targetmeta = self._parse_user(message.params[0])\n        key, _, value = message.params[1:4]\n\n        if target not in self._pending['whois']:\n            return\n        if target in self.users:\n            self._sync_user(target, targetmeta)\n\n        self._whois_info[target].setdefault('metadata', {})\n        self._whois_info[target]['metadata'][key] = value", "response": "Metadata key value for whois."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_tls_context(self):\n        # Create context manually, as we're going to set our own options.\n        tls_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n\n        # Load client/server certificate.\n        if self.tls_certificate_file:\n            tls_context.load_cert_chain(self.tls_certificate_file, self.tls_certificate_keyfile,\n                                        password=self.tls_certificate_password)\n\n        # Set some relevant options:\n        # - No server should use SSLv2 or SSLv3 any more, they are outdated and full of security holes. (RFC6176, RFC7568)\n        # - Disable compression in order to counter the CRIME attack. (https://en.wikipedia.org/wiki/CRIME_%28security_exploit%29)\n        # - Disable session resumption to maintain perfect forward secrecy. (https://timtaubert.de/blog/2014/11/the-sad-state-of-server-side-tls-session-resumption-implementations/)\n        for opt in ['NO_SSLv2', 'NO_SSLv3', 'NO_COMPRESSION', 'NO_TICKET']:\n            if hasattr(ssl, 'OP_' + opt):\n                tls_context.options |= getattr(ssl, 'OP_' + opt)\n\n        # Set TLS verification options.\n        if self.tls_verify:\n            # Set our custom verification callback, if the library supports it.\n            tls_context.set_servername_callback(self.verify_tls)\n\n            # Load certificate verification paths.\n            tls_context.set_default_verify_paths()\n            if sys.platform in DEFAULT_CA_PATHS and path.isdir(DEFAULT_CA_PATHS[sys.platform]):\n                tls_context.load_verify_locations(capath=DEFAULT_CA_PATHS[sys.platform])\n\n            # If we want to verify the TLS connection, we first need a certicate.\n            # Check this certificate and its entire chain, if possible, against revocation lists.\n            tls_context.verify_mode = ssl.CERT_REQUIRED\n            tls_context.verify_flags = ssl.VERIFY_CRL_CHECK_CHAIN\n\n        return tls_context", "response": "Create a TLS socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify a TLS connection.", "response": "def verify_tls(self, socket, hostname, context):\n        \"\"\"\n        Verify a TLS connection. Return behaviour is dependent on the as_callback parameter:\n            - If True, a return value of None means verification succeeded, else it failed.\n            - If False, a return value of True means verification succeeded, an exception or False means it failed.\n        \"\"\"\n        cert = socket.getpeercert()\n\n        try:\n            # Make sure the hostnames for which this certificate is valid include the one we're connecting to.\n            ssl.match_hostname(cert, hostname)\n        except ssl.CertificateError:\n            return ssl.ALERT_DESCRIPTION_BAD_CERTIFICATE\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def send(self, data):\n        self.writer.write(data)\n        await self.writer.drain()", "response": "Add data to send queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncleaning up a name so it works for a Python identifier.", "response": "def identifierify(name):\n    \"\"\" Clean up name so it works for a Python identifier. \"\"\"\n    name = name.lower()\n    name = re.sub('[^a-z0-9]', '_', name)\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms IRC connection registration.", "response": "async def _register(self):\n        \"\"\" Perform IRC connection registration. \"\"\"\n        if self.registered:\n            return\n        self._registration_attempts += 1\n\n        # Don't throttle during registration, most ircds don't care for flooding during registration,\n        # and it might speed it up significantly.\n        self.connection.throttle = False\n\n        # Password first.\n        if self.password:\n            await self.rawmsg('PASS', self.password)\n\n        # Then nickname...\n        await self.set_nickname(self._attempt_nicknames.pop(0))\n        # And now for the rest of the user information.\n        await self.rawmsg('USER', self.username, '0', '*', self.realname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _registration_completed(self, message):\n        if not self.registered:\n            # Re-enable throttling.\n            self.registered = True\n            self.connection.throttle = True\n\n            target = message.params[0]\n            fakemsg = self._create_message('NICK', target, source=self.nickname)\n            await self.on_raw_nick(fakemsg)", "response": "Called when a registration is completed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def join(self, channel, password=None):\n        if self.in_channel(channel):\n            raise AlreadyInChannel(channel)\n\n        if password:\n            await self.rawmsg('JOIN', channel, password)\n        else:\n            await self.rawmsg('JOIN', channel)", "response": "Join channel, optionally with password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def part(self, channel, message=None):\n        if not self.in_channel(channel):\n            raise NotInChannel(channel)\n\n        # Message seems to be an extension to the spec.\n        if message:\n            await self.rawmsg('PART', channel, message)\n        else:\n            await self.rawmsg('PART', channel)", "response": "Leave channel optionally with message."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nkicks user from channel.", "response": "async def kick(self, channel, target, reason=None):\n        \"\"\" Kick user from channel. \"\"\"\n        if not self.in_channel(channel):\n            raise NotInChannel(channel)\n\n        if reason:\n            await self.rawmsg('KICK', channel, target, reason)\n        else:\n            await self.rawmsg('KICK', channel, target)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def kickban(self, channel, target, reason=None, range=0):\n        await self.ban(channel, target, range)\n        await self.kick(channel, target, reason)", "response": "Kick and ban user from channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nquits the current user.", "response": "async def quit(self, message=None):\n        \"\"\" Quit network. \"\"\"\n        if message is None:\n            message = self.DEFAULT_QUIT_MESSAGE\n\n        await self.rawmsg('QUIT', message)\n        await self.disconnect(expected=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def message(self, target, message):\n        hostmask = self._format_user_mask(self.nickname)\n        # Leeway.\n        chunklen = protocol.MESSAGE_LENGTH_LIMIT - len(\n            '{hostmask} PRIVMSG {target} :'.format(hostmask=hostmask, target=target)) - 25\n\n        for line in message.replace('\\r', '').split('\\n'):\n            for chunk in chunkify(line, chunklen):\n                # Some IRC servers respond with \"412 Bot :No text to send\" on empty messages.\n                await self.rawmsg('PRIVMSG', target, chunk or ' ')", "response": "Send a message to a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the mode of the user on the specified target.", "response": "async def set_mode(self, target, *modes):\n        \"\"\"\n        Set mode on target.\n        Users should only rely on the mode actually being changed when receiving an on_{channel,user}_mode_change callback.\n        \"\"\"\n        if self.is_channel(target) and not self.in_channel(target):\n            raise NotInChannel(target)\n\n        await self.rawmsg('MODE', target, *modes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def set_topic(self, channel, topic):\n        if not self.is_channel(channel):\n            raise ValueError('Not a channel: {}'.format(channel))\n        elif not self.in_channel(channel):\n            raise NotInChannel(channel)\n\n        await self.rawmsg('TOPIC', channel, topic)", "response": "Set topic on channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns information about a user.", "response": "async def whois(self, nickname):\n        \"\"\"\n        Return information about user.\n        This is an blocking asynchronous method: it has to be called from a coroutine, as follows:\n\n            info = await self.whois('Nick')\n        \"\"\"\n        # Some IRCDs are wonky and send strange responses for spaces in nicknames.\n        # We just check if there's a space in the nickname -- if there is,\n        # then we immediately set the future's result to None and don't bother checking.\n        if protocol.ARGUMENT_SEPARATOR.search(nickname) is not None:\n            result = self.eventloop.create_future()\n            result.set_result(None)\n            return result\n\n        if nickname not in self._pending['whois']:\n            await self.rawmsg('WHOIS', nickname)\n            self._whois_info[nickname] = {\n                'oper': False,\n                'idle': 0,\n                'away': False,\n                'away_message': None\n            }\n\n            # Create a future for when the WHOIS requests succeeds.\n            self._pending['whois'][nickname] = self.eventloop.create_future()\n\n        return await self._pending['whois'][nickname]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn information about offline user.", "response": "async def whowas(self, nickname):\n        \"\"\"\n        Return information about offline user.\n        This is an blocking asynchronous method: it has to be called from a coroutine, as follows:\n\n            info = await self.whowas('Nick')\n        \"\"\"\n        # Same treatment as nicknames in whois.\n        if protocol.ARGUMENT_SEPARATOR.search(nickname) is not None:\n            result = self.eventloop.create_future()\n            result.set_result(None)\n            return result\n\n        if nickname not in self._pending['whowas']:\n            await self.rawmsg('WHOWAS', nickname)\n            self._whowas_info[nickname] = {}\n\n            # Create a future for when the WHOWAS requests succeeds.\n            self._pending['whowas'][nickname] = self.eventloop.create_future()\n\n        return await self._pending['whowas'][nickname]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_same_nick(self, left, right):\n        return self.normalize(left) == self.normalize(right)", "response": "Check if given nicknames are equal in the server s case mapping."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if given nicknames are equal in the server s case mapping.", "response": "def is_same_channel(self, left, right):\n        \"\"\" Check if given nicknames are equal in the server's case mapping. \"\"\"\n        return self.normalize(left) == self.normalize(right)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def on_raw_error(self, message):\n        error = protocol.ServerError(' '.join(message.params))\n        await self.on_data_error(error)", "response": "Server encountered an error."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nending of /WHOIS list.", "response": "async def on_raw_318(self, message):\n        \"\"\" End of /WHOIS list. \"\"\"\n        target, nickname = message.params[:2]\n\n        # Mark future as done.\n        if nickname in self._pending['whois']:\n            future = self._pending['whois'].pop(nickname)\n            future.set_result(self._whois_info[nickname])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def on_raw_333(self, message):\n        target, channel, setter, timestamp = message.params\n        if not self.in_channel(channel):\n            return\n\n        # No need to sync user since this is most likely outdated info.\n        self.channels[channel]['topic_by'] = self._parse_user(setter)[0]\n        self.channels[channel]['topic_set'] = datetime.datetime.fromtimestamp(int(timestamp))", "response": "Topic setter and time on channel join."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def on_raw_353(self, message):\n        target, visibility, channel, names = message.params\n        if not self.in_channel(channel):\n            return\n\n        # Set channel visibility.\n        if visibility == protocol.PUBLIC_CHANNEL_SIGIL:\n            self.channels[channel]['public'] = True\n        elif visibility in (protocol.PRIVATE_CHANNEL_SIGIL, protocol.SECRET_CHANNEL_SIGIL):\n            self.channels[channel]['public'] = False\n\n        # Update channel user list.\n        for entry in names.split():\n            statuses = []\n            # Make entry safe for _parse_user().\n            safe_entry = entry.lstrip(''.join(self._nickname_prefixes.keys()))\n            # Parse entry and update database.\n            nick, metadata = self._parse_user(safe_entry)\n            self._sync_user(nick, metadata)\n\n            # Get prefixes.\n            prefixes = set(entry.replace(safe_entry, ''))\n\n            # Check, record and strip status prefixes.\n            for prefix, status in self._nickname_prefixes.items():\n                # Add to list of statuses by user.\n                if prefix in prefixes:\n                    statuses.append(status)\n\n            # Add user to user list.\n            self.channels[channel]['users'].add(nick)\n            # And to channel modes..\n            for status in statuses:\n                if status not in self.channels[channel]['modes']:\n                    self.channels[channel]['modes'][status] = []\n                self.channels[channel]['modes'][status].append(nick)", "response": "Response to / NAMES."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts message of the day.", "response": "async def on_raw_375(self, message):\n        \"\"\" Start message of the day. \"\"\"\n        await self._registration_completed(message)\n        self.motd = message.params[1] + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to a server optionally over TLS.", "response": "async def connect(self, hostname=None, port=None, tls=False, **kwargs):\n        \"\"\" Connect to a server, optionally over TLS. See pydle.features.RFC1459Support.connect for misc parameters. \"\"\"\n        if not port:\n            if tls:\n                port = DEFAULT_TLS_PORT\n            else:\n                port = rfc1459.protocol.DEFAULT_PORT\n        return await super().connect(hostname, port, tls=tls, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect to IRC server optionally over TLS.", "response": "async def _connect(self, hostname, port, reconnect=False, password=None, encoding=pydle.protocol.DEFAULT_ENCODING, channels=[], tls=False, tls_verify=False, source_address=None):\n        \"\"\" Connect to IRC server, optionally over TLS. \"\"\"\n        self.password = password\n\n        # Create connection if we can't reuse it.\n        if not reconnect:\n            self._autojoin_channels = channels\n            self.connection = connection.Connection(hostname, port,\n                source_address=source_address,\n                tls=tls, tls_verify=tls_verify,\n                tls_certificate_file=self.tls_client_cert,\n                tls_certificate_keyfile=self.tls_client_cert_key,\n                tls_certificate_password=self.tls_client_cert_password,\n                eventloop=self.eventloop)\n            self.encoding = encoding\n\n        # Connect.\n        await self.connection.connect()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reset_attributes(self):\n        # Record-keeping.\n        self.channels = {}\n        self.users = {}\n\n        # Low-level data stuff.\n        self._receive_buffer = b''\n        self._pending = {}\n        self._handler_top_level = False\n        self._ping_checker_handle = None\n\n        # Misc.\n        self.logger = logging.getLogger(__name__)\n\n        # Public connection attributes.\n        self.nickname = DEFAULT_NICKNAME\n        self.network = None", "response": "Reset the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, *args, **kwargs):\n        self.eventloop.run_until_complete(self.connect(*args, **kwargs))\n        try:\n            self.eventloop.run_forever()\n        finally:\n            self.eventloop.stop()", "response": "Connect and run bot in event loop."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconnect to IRC server.", "response": "async def connect(self, hostname=None, port=None, reconnect=False, **kwargs):\n        \"\"\" Connect to IRC server. \"\"\"\n        if (not hostname or not port) and not reconnect:\n            raise ValueError('Have to specify hostname and port if not reconnecting.')\n\n        # Disconnect from current connection.\n        if self.connected:\n            await self.disconnect(expected=True)\n\n        # Reset attributes and connect.\n        if not reconnect:\n            self._reset_connection_attributes()\n        await self._connect(hostname=hostname, port=port, reconnect=reconnect, **kwargs)\n\n        # Set logger name.\n        if self.server_tag:\n            self.logger = logging.getLogger(self.__class__.__name__ + ':' + self.server_tag)\n\n        self.eventloop.create_task(self.handle_forever())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _connect(self, hostname, port, reconnect=False, channels=[],\n                       encoding=protocol.DEFAULT_ENCODING, source_address=None):\n        \"\"\" Connect to IRC host. \"\"\"\n        # Create connection if we can't reuse it.\n        if not reconnect or not self.connection:\n            self._autojoin_channels = channels\n            self.connection = connection.Connection(hostname, port, source_address=source_address,\n                                                    eventloop=self.eventloop)\n            self.encoding = encoding\n\n        # Connect.\n        await self.connection.connect()", "response": "Connect to IRC host."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the reconnection delay.", "response": "def _reconnect_delay(self):\n        \"\"\" Calculate reconnection delay. \"\"\"\n        if self.RECONNECT_ON_ERROR and self.RECONNECT_DELAYED:\n            if self._reconnect_attempts >= len(self.RECONNECT_DELAYS):\n                return self.RECONNECT_DELAYS[-1]\n            else:\n                return self.RECONNECT_DELAYS[self._reconnect_attempts]\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _perform_ping_timeout(self, delay: int):\n\n        # pause for delay seconds\n        await sleep(delay)\n        # then continue\n        error = TimeoutError(\n            'Ping timeout: no data received from server in {timeout} seconds.'.format(\n                timeout=self.PING_TIMEOUT))\n        await self.on_data_error(error)", "response": "Handle timeout gracefully.\n\n        Args:\n            delay (int): delay before raising the timeout (in seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a raw message to the master.", "response": "async def rawmsg(self, command, *args, **kwargs):\n        \"\"\" Send raw message. \"\"\"\n        message = str(self._create_message(command, *args, **kwargs))\n        await self._send(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def on_raw(self, message):\n        self.logger.debug('<< %s', message._raw)\n        if not message._valid:\n            self.logger.warning('Encountered strictly invalid IRC message from server: %s',\n                                message._raw)\n\n        if isinstance(message.command, int):\n            cmd = str(message.command).zfill(3)\n        else:\n            cmd = message.command\n\n        # Invoke dispatcher, if we have one.\n        method = 'on_raw_' + cmd.lower()\n        try:\n            # Set _top_level so __getattr__() can decide whether to return on_unknown or _ignored for unknown handlers.\n            # The reason for this is that features can always call super().on_raw_* safely and thus don't need to care for other features,\n            # while unknown messages for which no handlers exist at all are still logged.\n            self._handler_top_level = True\n            handler = getattr(self, method)\n            self._handler_top_level = False\n\n            await handler(message)\n        except:\n            self.logger.exception('Failed to execute %s handler.', method)", "response": "Handle a single IRC message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(self, client: BasicClient, *args, **kwargs):\n        self.clients.add(client)\n        self.connect_args[client] = (args, kwargs)\n        # hack the clients event loop to use the pools own event loop\n        client.eventloop = self.eventloop", "response": "Add client to pool."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef disconnect(self, client):\n        self.clients.remove(client)\n        del self.connect_args[client]\n        client.disconnect()", "response": "Disconnect a client from the pool."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_forever(self):\n        # container for all the client connection coros\n        connection_list = []\n        for client in self.clients:\n            args, kwargs = self.connect_args[client]\n            connection_list.append(client.connect(*args, **kwargs))\n        # single future for executing the connections\n        connections = gather(*connection_list, loop=self.eventloop)\n\n        # run the connections\n        self.eventloop.run_until_complete(connections)\n\n        # run the clients\n        self.eventloop.run_forever()\n\n        for client in self.clients:\n            client.disconnect()", "response": "Handle all the clients forever."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing input according to case mapping.", "response": "def normalize(input, case_mapping=protocol.DEFAULT_CASE_MAPPING):\n    \"\"\" Normalize input according to case mapping. \"\"\"\n    if case_mapping not in protocol.CASE_MAPPINGS:\n        raise pydle.protocol.ProtocolViolation('Unknown case mapping ({})'.format(case_mapping))\n\n    input = input.lower()\n\n    if case_mapping in ('rfc1459', 'rfc1459-strict'):\n        input = input.replace('{', '[').replace('}', ']').replace('|', '\\\\')\n    if case_mapping == 'rfc1459':\n        input = input.replace('~', '^')\n\n    return input"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_user(raw):\n    nick = raw\n    user = None\n    host = None\n\n    # Attempt to extract host.\n    if protocol.HOST_SEPARATOR in raw:\n        raw, host = raw.split(protocol.HOST_SEPARATOR)\n    # Attempt to extract user.\n    if protocol.USER_SEPARATOR in raw:\n        nick, user = raw.split(protocol.USER_SEPARATOR)\n\n    return nick, user, host", "response": "Parse nick user host."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the list of modes and return a new dictionary.", "response": "def parse_modes(modes, current, behaviour):\n    \"\"\" Parse mode change string(s) and return updated dictionary. \"\"\"\n    current = current.copy()\n    modes = modes[:]\n\n    # Iterate in a somewhat odd way over the list because we want to modify it during iteration.\n    i = 0\n    while i < len(modes):\n        piece = modes[i]\n        add = True\n        sigiled = False\n\n        for mode in piece:\n            # Set mode to addition or deletion of modes.\n            if mode == '+':\n                add = True\n                sigiled = True\n                continue\n            if mode == '-':\n                add = False\n                sigiled = True\n                continue\n\n            # Find mode behaviour.\n            for type, affected in behaviour.items():\n                if mode in affected:\n                    break\n            else:\n                # If we don't have a behaviour for this mode, assume it has no parameters...\n                type = protocol.BEHAVIOUR_NO_PARAMETER\n\n            # Don't parse modes that are meant for list retrieval.\n            if type == protocol.BEHAVIOUR_LIST and not sigiled:\n                continue\n\n            # Do we require a parameter?\n            if type in (protocol.BEHAVIOUR_PARAMETER, protocol.BEHAVIOUR_LIST) or (type == protocol.BEHAVIOUR_PARAMETER_ON_SET and add):\n                # Do we _have_ a parameter?\n                if i + 1 == len(modes):\n                    raise pydle.protocol.ProtocolViolation('Attempted to parse mode with parameter ({s}{mode}) but no parameters left in mode list.'.format(\n                        mode=mode, s='+' if add else '-'), ' '.join(modes))\n                param = modes.pop(i + 1)\n\n            # Now update the actual mode dict with our new values.\n            if type in (protocol.BEHAVIOUR_PARAMETER, protocol.BEHAVIOUR_LIST):\n                # Add/remove parameter from list.\n                if add:\n                    if mode not in current:\n                        current[mode] = []\n                    current[mode].append(param)\n                else:\n                    if mode in current and param in current[mode]:\n                        current[mode].remove(param)\n            elif type == protocol.BEHAVIOUR_PARAMETER_ON_SET and add:\n                # Simply set parameter.\n                current[mode] = param\n            else:\n                # Simply add/remove option.\n                if add:\n                    current[mode] = True\n                else:\n                    if mode in current:\n                        del current[mode]\n        i += 1\n\n    return current"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse given line into a Message structure.", "response": "def parse(cls, line, encoding=pydle.protocol.DEFAULT_ENCODING):\n        \"\"\"\n        Parse given line into IRC message structure.\n        Returns a Message.\n        \"\"\"\n        valid = True\n\n        # Decode message.\n        try:\n            message = line.decode(encoding)\n        except UnicodeDecodeError:\n            # Try our fallback encoding.\n            message = line.decode(pydle.protocol.FALLBACK_ENCODING)\n\n        # Sanity check for message length.\n        if len(message) > protocol.MESSAGE_LENGTH_LIMIT:\n            valid = False\n\n        # Strip message separator.\n        if message.endswith(protocol.LINE_SEPARATOR):\n            message = message[:-len(protocol.LINE_SEPARATOR)]\n        elif message.endswith(protocol.MINIMAL_LINE_SEPARATOR):\n            message = message[:-len(protocol.MINIMAL_LINE_SEPARATOR)]\n\n        # Sanity check for forbidden characters.\n        if any(ch in message for ch in protocol.FORBIDDEN_CHARACTERS):\n            valid = False\n\n        # Extract message sections.\n        # Format: (:source)? command parameter*\n        if message.startswith(':'):\n            parts = protocol.ARGUMENT_SEPARATOR.split(message[1:], 2)\n        else:\n            parts = [ None ] + protocol.ARGUMENT_SEPARATOR.split(message, 1)\n\n        if len(parts) == 3:\n            source, command, raw_params = parts\n        elif len(parts) == 2:\n            source, command = parts\n            raw_params = ''\n        else:\n            raise pydle.protocol.ProtocolViolation('Improper IRC message format: not enough elements.', message=message)\n\n        # Sanity check for command.\n        if not protocol.COMMAND_PATTERN.match(command):\n            valid = False\n\n        # Extract parameters properly.\n        # Format: (word|:sentence)*\n\n        # Only parameter is a 'trailing' sentence.\n        if raw_params.startswith(protocol.TRAILING_PREFIX):\n            params = [ raw_params[len(protocol.TRAILING_PREFIX):] ]\n        # We have a sentence in our parameters.\n        elif ' ' + protocol.TRAILING_PREFIX in raw_params:\n            index = raw_params.find(' ' + protocol.TRAILING_PREFIX)\n\n             # Get all single-word parameters.\n            params = protocol.ARGUMENT_SEPARATOR.split(raw_params[:index].rstrip(' '))\n            # Extract last parameter as sentence\n            params.append(raw_params[index + len(protocol.TRAILING_PREFIX) + 1:])\n        # We have some parameters, but no sentences.\n        elif raw_params:\n            params = protocol.ARGUMENT_SEPARATOR.split(raw_params)\n        # No parameters.\n        else:\n            params = []\n\n        # Commands can be either [a-zA-Z]+ or [0-9]+.\n        # In the former case, force it to uppercase.\n        # In the latter case (a numeric command), try to represent it as such.\n        try:\n            command = int(command)\n        except ValueError:\n            command = command.upper()\n\n        # Return parsed message.\n        return RFC1459Message(command, params, source=source, _valid=valid, _raw=message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a raw IRC message.", "response": "def construct(self, force=False):\n        \"\"\" Construct a raw IRC message. \"\"\"\n        # Sanity check for command.\n        command = str(self.command)\n        if not protocol.COMMAND_PATTERN.match(command) and not force:\n            raise pydle.protocol.ProtocolViolation('The constructed command does not follow the command pattern ({pat})'.format(pat=protocol.COMMAND_PATTERN.pattern), message=command)\n        message = command.upper()\n\n        # Add parameters.\n        if not self.params:\n            message += ' '\n        for idx, param in enumerate(self.params):\n            # Trailing parameter?\n            if not param or ' ' in param or param[0] == ':':\n                if idx + 1 < len(self.params) and not force:\n                    raise pydle.protocol.ProtocolViolation('Only the final parameter of an IRC message can be trailing and thus contain spaces, or start with a colon.', message=param)\n                message += ' ' + protocol.TRAILING_PREFIX + param\n            # Regular parameter.\n            else:\n                message += ' ' + param\n\n        # Prepend source.\n        if self.source:\n            message = ':' + self.source + ' ' + message\n\n        # Sanity check for characters.\n        if any(ch in message for ch in protocol.FORBIDDEN_CHARACTERS) and not force:\n            raise pydle.protocol.ProtocolViolation('The constructed message contains forbidden characters ({chs}).'.format(chs=', '.join(protocol.FORBIDDEN_CHARACTERS)), message=message)\n\n        # Sanity check for length.\n        message += protocol.LINE_SEPARATOR\n        if len(message) > protocol.MESSAGE_LENGTH_LIMIT and not force:\n            raise pydle.protocol.ProtocolViolation('The constructed message is too long. ({len} > {maxlen})'.format(len=len(message), maxlen=protocol.MESSAGE_LENGTH_LIMIT), message=message)\n\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nput features into proper MRO order.", "response": "def featurize(*features):\n    \"\"\" Put features into proper MRO order. \"\"\"\n    from functools import cmp_to_key\n\n    def compare_subclass(left, right):\n        if issubclass(left, right):\n            return -1\n        elif issubclass(right, left):\n            return 1\n        return 0\n\n    sorted_features = sorted(features, key=cmp_to_key(compare_subclass))\n    name = 'FeaturizedClient[{features}]'.format(\n        features=', '.join(feature.__name__ for feature in sorted_features))\n    return type(name, tuple(sorted_features), {})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def on_raw_join(self, message):\n        await super().on_raw_join(message)\n        nick, metadata = self._parse_user(message.source)\n        channels = message.params[0].split(',')\n\n        if self.is_same_nick(self.nickname, nick):\n            # We joined.\n            if 'WHOX' in self._isupport and self._isupport['WHOX']:\n                # Get more relevant channel info thanks to WHOX.\n                await self.rawmsg('WHO', ','.join(channels), '%tnurha,{id}'.format(id=WHOX_IDENTIFIER))\n        else:\n            # Find account name of person.\n            pass", "response": "Override JOIN to send WHOX."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_channel(self, channel):\n        super()._create_channel(channel)\n        if 'EXCEPTS' in self._isupport:\n            self.channels[channel]['exceptlist'] = None\n        if 'INVEX' in self._isupport:\n            self.channels[channel]['inviteexceptlist'] = None", "response": "Create channel with optional ban and invite exception lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets channel limits for user.", "response": "async def on_isupport_chanlimit(self, value):\n        \"\"\" Simultaneous channel limits for user. \"\"\"\n        self._channel_limits = {}\n\n        for entry in value.split(','):\n            types, limit = entry.split(':')\n\n            # Assign limit to channel type group and add lookup entry for type.\n            self._channel_limits[frozenset(types)] = int(limit)\n            for prefix in types:\n                self._channel_limit_groups[prefix] = frozenset(types)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def on_isupport_chanmodes(self, value):\n        list, param, param_set, noparams = [ set(modes) for modes in value.split(',')[:4] ]\n        self._channel_modes.update(set(value.replace(',', '')))\n\n        # The reason we have to do it like this is because other ISUPPORTs (e.g. PREFIX) may update these values as well.\n        if not rfc1459.protocol.BEHAVIOUR_LIST in self._channel_modes_behaviour:\n            self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_LIST] = set()\n        self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_LIST].update(list)\n\n        if not rfc1459.protocol.BEHAVIOUR_PARAMETER in self._channel_modes_behaviour:\n            self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_PARAMETER] = set()\n        self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_PARAMETER].update(param)\n\n        if not rfc1459.protocol.BEHAVIOUR_PARAMETER_ON_SET in self._channel_modes_behaviour:\n            self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_PARAMETER_ON_SET] = set()\n        self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_PARAMETER_ON_SET].update(param_set)\n\n        if not rfc1459.protocol.BEHAVIOUR_NO_PARAMETER in self._channel_modes_behaviour:\n            self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_NO_PARAMETER] = set()\n        self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_NO_PARAMETER].update(noparams)", "response": "Update the internal state of the channel modes and their behaviour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def on_isupport_invex(self, value):\n        if not value:\n            value = INVITE_EXCEPT_MODE\n        self._channel_modes.add(value)\n        self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_LIST].add(value)", "response": "Server allows invite exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def on_isupport_maxbans(self, value):\n        if 'MAXLIST' not in self._isupport:\n            if not self._list_limits:\n                self._list_limits = {}\n            self._list_limits['b'] = int(value)", "response": "Maximum entries in ban list. Replaced by MAXLIST."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the CHANLIMIT setting.", "response": "async def on_isupport_maxchannels(self, value):\n        \"\"\" Old version of CHANLIMIT. \"\"\"\n        if 'CHANTYPES' in self._isupport and 'CHANLIMIT' not in self._isupport:\n            self._channel_limits = {}\n\n            prefixes = self._isupport['CHANTYPES']\n            # Assume the limit is for all types of channels. Make a single group for all types.\n            self._channel_limits[frozenset(prefixes)] = int(value)\n            for prefix in prefixes:\n                self._channel_limit_groups[prefix] = frozenset(prefixes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlimits on channel modes involving lists.", "response": "async def on_isupport_maxlist(self, value):\n        \"\"\" Limits on channel modes involving lists. \"\"\"\n        self._list_limits = {}\n\n        for entry in value.split(','):\n            modes, limit = entry.split(':')\n\n            # Assign limit to mode group and add lookup entry for mode.\n            self._list_limits[frozenset(modes)] = int(limit)\n            for mode in modes:\n                self._list_limit_groups[mode] = frozenset(modes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnickname prefixes on channels and their associated modes.", "response": "async def on_isupport_prefix(self, value):\n        \"\"\" Nickname prefixes on channels and their associated modes. \"\"\"\n        if not value:\n            # No prefixes support.\n            self._nickname_prefixes = collections.OrderedDict()\n            return\n\n        modes, prefixes = value.lstrip('(').split(')', 1)\n\n        # Update valid channel modes and their behaviour as CHANMODES doesn't include PREFIX modes.\n        self._channel_modes.update(set(modes))\n        if not rfc1459.protocol.BEHAVIOUR_PARAMETER in self._channel_modes_behaviour:\n            self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_PARAMETER] = set()\n        self._channel_modes_behaviour[rfc1459.protocol.BEHAVIOUR_PARAMETER].update(set(modes))\n\n        self._nickname_prefixes = collections.OrderedDict()\n        for mode, prefix in zip(modes, prefixes):\n            self._nickname_prefixes[prefix] = mode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the maximum number of targets certain types of commands can affect.", "response": "async def on_isupport_targmax(self, value):\n        \"\"\" The maximum number of targets certain types of commands can affect. \"\"\"\n        if not value:\n            return\n\n        for entry in value.split(','):\n            command, limit = entry.split(':', 1)\n            if not limit:\n                continue\n            self._target_limits[command] = int(limit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def on_isupport_wallchops(self, value):\n        for prefix, mode in self._nickname_prefixes.items():\n            if mode == 'o':\n                break\n        else:\n            prefix = '@'\n        self._status_message_prefixes.add(prefix)", "response": "Support for messaging every opped member or higher on a channel. Replaced by STATUSMSG."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(cls, line, encoding=pydle.protocol.DEFAULT_ENCODING):\n        valid = True\n        # Decode message.\n        try:\n            message = line.decode(encoding)\n        except UnicodeDecodeError:\n            # Try our fallback encoding.\n            message = line.decode(pydle.protocol.FALLBACK_ENCODING)\n\n        # Sanity check for message length.\n        if len(message) > TAGGED_MESSAGE_LENGTH_LIMIT:\n            valid = False\n\n        # Strip message separator.\n        if message.endswith(rfc1459.protocol.LINE_SEPARATOR):\n            message = message[:-len(rfc1459.protocol.LINE_SEPARATOR)]\n        elif message.endswith(rfc1459.protocol.MINIMAL_LINE_SEPARATOR):\n            message = message[:-len(rfc1459.protocol.MINIMAL_LINE_SEPARATOR)]\n        raw = message\n\n        # Parse tags.\n        tags = {}\n        if message.startswith(TAG_INDICATOR):\n            message = message[len(TAG_INDICATOR):]\n            raw_tags, message = message.split(' ', 1)\n\n            for raw_tag in raw_tags.split(TAG_SEPARATOR):\n                if TAG_VALUE_SEPARATOR in raw_tag:\n                    tag, value = raw_tag.split(TAG_VALUE_SEPARATOR, 1)\n                else:\n                    tag = raw_tag\n                    value = True\n                tags[tag] = value\n\n        # Parse rest of message.\n        message = super().parse(message.lstrip().encode(encoding), encoding=encoding)\n        return TaggedMessage(_raw=raw, _valid=message._valid and valid, tags=tags, **message._kw)", "response": "Parse given line into TaggedMessage."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef construct(self, force=False):\n        message = super().construct(force=force)\n\n        # Add tags.\n        if self.tags:\n            raw_tags = []\n            for tag, value in self.tags.items():\n                if value == True:\n                    raw_tags.append(tag)\n                else:\n                    raw_tags.append(tag + TAG_VALUE_SEPARATOR + value)\n\n            message = TAG_INDICATOR + TAG_SEPARATOR.join(raw_tags) + ' ' + message\n\n        if len(message) > TAGGED_MESSAGE_LENGTH_LIMIT and not force:\n            raise protocol.ProtocolViolation('The constructed message is too long. ({len} > {maxlen})'.format(len=len(message), maxlen=TAGGED_MESSAGE_LENGTH_LIMIT), message=message)\n        return message", "response": "Construct raw IRC message and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_public_suffix_list():\n    local_psl = os.environ.get('PUBLIC_SUFFIX_LIST')\n    if local_psl:\n        with codecs.open(local_psl, 'r', 'utf-8') as f:\n            psl_raw = f.readlines()\n    else:\n        psl_raw = unicode(urlopen(PSL_URL).read(), 'utf-8').split('\\n')\n    psl = set()\n    for line in psl_raw:\n        item = line.strip()\n        if item != '' and not item.startswith('//'):\n            psl.add(item)\n    return psl", "response": "Return a set containing all Public Suffixes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize a URL. >>> normalize('hTtp://ExAMPLe.COM:80') 'http://example.com/'", "response": "def normalize(url):\n    \"\"\"Normalize a URL.\n\n    >>> normalize('hTtp://ExAMPLe.COM:80')\n    'http://example.com/'\n    \"\"\"\n    url = url.strip()\n    if url == '':\n        return ''\n    parts = split(url)\n    if parts.scheme:\n        netloc = parts.netloc\n        if parts.scheme in SCHEMES:\n            path = normalize_path(parts.path)\n        else:\n            path = parts.path\n    # url is relative, netloc (if present) is part of path\n    else:\n        netloc = parts.path\n        path = ''\n        if '/' in netloc:\n            netloc, path_raw = netloc.split('/', 1)\n            path = normalize_path('/' + path_raw)\n    username, password, host, port = split_netloc(netloc)\n    host = normalize_host(host)\n    port = _normalize_port(parts.scheme, port)\n    query = normalize_query(parts.query)\n    fragment = normalize_fragment(parts.fragment)\n    return construct(URL(parts.scheme, username, password, None, host, None,\n                         port, path, query, fragment, None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _encode_query(query):\n    if query == '':\n        return query\n    query_args = []\n    for query_kv in query.split('&'):\n        k, v = query_kv.split('=')\n        query_args.append(k + \"=\" + quote(v.encode('utf-8')))\n    return '&'.join(query_args)", "response": "Quote all values of a query string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef construct(parts):\n    url = ''\n    if parts.scheme:\n        if parts.scheme in SCHEMES:\n            url += parts.scheme + '://'\n        else:\n            url += parts.scheme + ':'\n    if parts.username and parts.password:\n        url += parts.username + ':' + parts.password + '@'\n    elif parts.username:\n        url += parts.username + '@'\n    if parts.subdomain:\n        url += parts.subdomain + '.'\n    url += parts.domain\n    if parts.tld:\n        url += '.' + parts.tld\n    if parts.port:\n        url += ':' + parts.port\n    if parts.path:\n        url += parts.path\n    if parts.query:\n        url += '?' + parts.query\n    if parts.fragment:\n        url += '#' + parts.fragment\n    return url", "response": "Construct a new URL from parts."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize host (decode IDNA).", "response": "def normalize_host(host):\n    \"\"\"Normalize host (decode IDNA).\"\"\"\n    if 'xn--' not in host:\n        return host\n    return '.'.join([_idna_decode(p) for p in host.split('.')])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _normalize_port(scheme, port):\n    if not scheme:\n        return port\n    if port and port != DEFAULT_PORT[scheme]:\n        return port", "response": "Return port if it is not default port else None."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnormalizes path for a resource.", "response": "def normalize_path(path):\n    \"\"\"Normalize path: collapse etc.\n\n    >>> normalize_path('/a/b///c')\n    '/a/b/c'\n    \"\"\"\n    if path in ['//', '/', '']:\n        return '/'\n    npath = normpath(unquote(path, exceptions=QUOTE_EXCEPTIONS['path']))\n    if path[-1] == '/' and npath != '/':\n        npath += '/'\n    return npath"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalize query to sort params by name remove params without value.", "response": "def normalize_query(query):\n    \"\"\"Normalize query: sort params by name, remove params without value.\n\n    >>> normalize_query('z=3&y=&x=1')\n    'x=1&z=3'\n    \"\"\"\n    if query == '' or len(query) <= 2:\n        return ''\n    nquery = unquote(query, exceptions=QUOTE_EXCEPTIONS['query'])\n    params = nquery.split('&')\n    nparams = []\n    for param in params:\n        if '=' in param:\n            k, v = param.split('=', 1)\n            if k and v:\n                nparams.append(\"%s=%s\" % (k, v))\n    nparams.sort()\n    return '&'.join(nparams)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(url):\n    parts = split(url)\n    if parts.scheme:\n        username, password, host, port = split_netloc(parts.netloc)\n        subdomain, domain, tld = split_host(host)\n    else:\n        username = password = subdomain = domain = tld = port = ''\n    return URL(parts.scheme, username, password, subdomain, domain, tld,\n               port, parts.path, parts.query, parts.fragment, url)", "response": "Parse a URL.\n\n    >>> parse('http://example.com/foo/')\n    URL(scheme='http', ..., domain='example', tld='com', ..., path='/foo/', ...)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract(url):\n    parts = split(url)\n    if parts.scheme:\n        netloc = parts.netloc\n        path = parts.path\n    else:\n        netloc = parts.path\n        path = ''\n        if '/' in netloc:\n            netloc, path_raw = netloc.split('/', 1)\n            path = '/' + path_raw\n    username, password, host, port = split_netloc(netloc)\n    subdomain, domain, tld = split_host(host)\n    return URL(parts.scheme, username, password, subdomain, domain, tld,\n               port, path, parts.query, parts.fragment, url)", "response": "Extract as much information from a relative URL as possible."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split(url):\n    scheme = netloc = path = query = fragment = ''\n    ip6_start = url.find('[')\n    scheme_end = url.find(':')\n    if ip6_start > 0 and ip6_start < scheme_end:\n        scheme_end = -1\n    if scheme_end > 0:\n        for c in url[:scheme_end]:\n            if c not in SCHEME_CHARS:\n                break\n        else:\n            scheme = url[:scheme_end].lower()\n            rest = url[scheme_end:].lstrip(':/')\n    if not scheme:\n        rest = url\n    l_path = rest.find('/')\n    l_query = rest.find('?')\n    l_frag = rest.find('#')\n    if l_path > 0:\n        if l_query > 0 and l_frag > 0:\n            netloc = rest[:l_path]\n            path = rest[l_path:min(l_query, l_frag)]\n        elif l_query > 0:\n            if l_query > l_path:\n                netloc = rest[:l_path]\n                path = rest[l_path:l_query]\n            else:\n                netloc = rest[:l_query]\n                path = ''\n        elif l_frag > 0:\n            netloc = rest[:l_path]\n            path = rest[l_path:l_frag]\n        else:\n            netloc = rest[:l_path]\n            path = rest[l_path:]\n    else:\n        if l_query > 0:\n            netloc = rest[:l_query]\n        elif l_frag > 0:\n            netloc = rest[:l_frag]\n        else:\n            netloc = rest\n    if l_query > 0:\n        if l_frag > 0:\n            query = rest[l_query+1:l_frag]\n        else:\n            query = rest[l_query+1:]\n    if l_frag > 0:\n        fragment = rest[l_frag+1:]\n    if not scheme:\n        path = netloc + path\n        netloc = ''\n    return SplitResult(scheme, netloc, path, query, fragment)", "response": "Split URL into scheme netloc path query and fragment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove trailing. and colon and tolower.", "response": "def _clean_netloc(netloc):\n    \"\"\"Remove trailing '.' and ':' and tolower.\n\n    >>> _clean_netloc('eXample.coM:')\n    'example.com'\n    \"\"\"\n    try:\n        return netloc.rstrip('.:').lower()\n    except:\n        return netloc.rstrip('.:').decode('utf-8').lower().encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_netloc(netloc):\n    username = password = host = port = ''\n    if '@' in netloc:\n        user_pw, netloc = netloc.split('@', 1)\n        if ':' in user_pw:\n            username, password = user_pw.split(':', 1)\n        else:\n            username = user_pw\n    netloc = _clean_netloc(netloc)\n    if ':' in netloc and netloc[-1] != ']':\n        host, port = netloc.rsplit(':', 1)\n    else:\n        host = netloc\n    return username, password, host, port", "response": "Split netloc into username password host and port."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_host(host):\n    # host is IPv6?\n    if '[' in host:\n        return '', host, ''\n    # host is IPv4?\n    for c in host:\n        if c not in IP_CHARS:\n            break\n    else:\n        return '', host, ''\n    # host is a domain name\n    domain = subdomain = tld = ''\n    parts = host.split('.')\n    for i in range(len(parts)):\n        tld = '.'.join(parts[i:])\n        wildcard_tld = '*.' + tld\n        exception_tld = '!' + tld\n        if exception_tld in PSL:\n            domain = '.'.join(parts[:i+1])\n            tld = '.'.join(parts[i+1:])\n            break\n        if tld in PSL:\n            domain = '.'.join(parts[:i])\n            break\n        if wildcard_tld in PSL:\n            domain = '.'.join(parts[:i-1])\n            tld = '.'.join(parts[i-1:])\n            break\n    if '.' in domain:\n        subdomain, domain = domain.rsplit('.', 1)\n    return subdomain, domain, tld", "response": "Use the Public Suffix List to split host into subdomain domain and tld."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the protocol object.", "response": "def connection_made(self, transport):\n        \"\"\"\n        Gets called when a connection to the gateway is established.\n        Initialise the protocol object.\n        \"\"\"\n        self.transport = transport\n        self.loop = transport.loop\n        self._cmd_lock = asyncio.Lock(loop=self.loop)\n        self._wd_lock = asyncio.Lock(loop=self.loop)\n        self._cmdq = asyncio.Queue(loop=self.loop)\n        self._msgq = asyncio.Queue(loop=self.loop)\n        self._updateq = asyncio.Queue(loop=self.loop)\n        self._readbuf = b''\n        self._update_cb = None\n        self._received_lines = 0\n        self._msg_task = self.loop.create_task(self._process_msgs())\n        self._report_task = None\n        self._watchdog_task = None\n        self.status = {}\n        self.connected = True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connection_lost(self, exc):\n        _LOGGER.error(\"Disconnected: %s\", exc)\n        self.connected = False\n        self.transport.close()\n        if self._report_task is not None:\n            self._report_task.cancel()\n        self._msg_task.cancel()\n        for q in [self._cmdq, self._updateq, self._msgq]:\n            while not q.empty():\n                q.get_nowait()\n        self.status = {}", "response": "Called when the connection to the gateway is lost."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef data_received(self, data):\n        # DIY line buffering...\n        newline = b'\\r\\n'\n        eot = b'\\x04'\n        self._readbuf += data\n        while newline in self._readbuf:\n            line, _, self._readbuf = self._readbuf.partition(newline)\n            if line:\n                if eot in line:\n                    # Discard everything before EOT\n                    _, _, line = line.partition(eot)\n                try:\n                    decoded = line.decode('ascii')\n                except UnicodeDecodeError:\n                    _LOGGER.debug(\"Invalid data received, ignoring...\")\n                    return\n                self.line_received(decoded)", "response": "Called when new data is received on the serial interface."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrigger a reconnect after timeout seconds of inactivity.", "response": "async def setup_watchdog(self, cb, timeout):\n        \"\"\"Trigger a reconnect after @timeout seconds of inactivity.\"\"\"\n        self._watchdog_timeout = timeout\n        self._watchdog_cb = cb\n        self._watchdog_task = self.loop.create_task(self._watchdog(timeout))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncanceling the watchdog task and related variables.", "response": "async def cancel_watchdog(self):\n        \"\"\"Cancel the watchdog task and related variables.\"\"\"\n        if self._watchdog_task is not None:\n            _LOGGER.debug(\"Canceling Watchdog task.\")\n            self._watchdog_task.cancel()\n            try:\n                await self._watchdog_task\n            except asyncio.CancelledError:\n                self._watchdog_task = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninforms the watchdog of activity.", "response": "async def _inform_watchdog(self):\n        \"\"\"Inform the watchdog of activity.\"\"\"\n        async with self._wd_lock:\n            if self._watchdog_task is None:\n                # Check within the Lock to deal with external cancel_watchdog\n                # calls with queued _inform_watchdog tasks.\n                return\n            self._watchdog_task.cancel()\n            try:\n                await self._watchdog_task\n            except asyncio.CancelledError:\n                self._watchdog_task = self.loop.create_task(self._watchdog(\n                    self._watchdog_timeout))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _watchdog(self, timeout):\n        await asyncio.sleep(timeout, loop=self.loop)\n        _LOGGER.debug(\"Watchdog triggered!\")\n        await self.cancel_watchdog()\n        await self._watchdog_cb()", "response": "Trigger and cancel the watchdog after timeout. Call callback."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a received line and process it if necessary.", "response": "def line_received(self, line):\n        \"\"\"\n        Gets called by data_received() when a complete line is\n        received.\n        Inspect the received line and process or queue accordingly.\n        \"\"\"\n        self._received_lines += 1\n        self.loop.create_task(self._inform_watchdog())\n        pattern = r'^(T|B|R|A|E)([0-9A-F]{8})$'\n        msg = re.match(pattern, line)\n        if msg:\n            src, mtype, mid, msb, lsb = self._dissect_msg(msg)\n            if lsb is not None:\n                self._msgq.put_nowait((src, mtype, mid, msb, lsb))\n        elif re.match(r'^[0-9A-F]{1,8}$', line) and self._received_lines == 1:\n            # Partial message on fresh connection. Ignore.\n            self._received_lines = 0\n            pass\n        else:\n            try:\n                self._cmdq.put_nowait(line)\n            except QueueFull:\n                _LOGGER.error('Queue full, discarded message: %s', line)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dissect_msg(self, match):\n        recvfrom = match.group(1)\n        frame = bytes.fromhex(match.group(2))\n        if recvfrom == 'E':\n            _LOGGER.warning(\"Received erroneous message, ignoring: %s\", frame)\n            return (None, None, None, None, None)\n        msgtype = self._get_msgtype(frame[0])\n        if msgtype in (READ_ACK, WRITE_ACK, READ_DATA, WRITE_DATA):\n            # Some info is best read from the READ/WRITE_DATA messages\n            # as the boiler may not support the data ID.\n            # Slice syntax is used to prevent implicit cast to int.\n            data_id = frame[1:2]\n            data_msb = frame[2:3]\n            data_lsb = frame[3:4]\n            return (recvfrom, msgtype, data_id, data_msb, data_lsb)\n        return (None, None, None, None, None)", "response": "Dissect a message into a tuple of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _process_msgs(self):\n        while True:\n            args = await self._msgq.get()\n            _LOGGER.debug('Processing: %s %02x %s %s %s', args[0], args[1],\n                          *[args[i].hex() for i in range(2, 5)])\n            await self._process_msg(*args)", "response": "Process all messages from the queue and pass them to _process_msg."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a message and update status variables where necessary.", "response": "async def _process_msg(self, src, msgtype, msgid, msb, lsb):\n        \"\"\"\n        Process message and update status variables where necessary.\n        Add status to queue if it was changed in the process.\n        \"\"\"\n        # Ignore output to the thermostat ('A') except MSG_TROVRD,\n        # MSG_TOUTSIDE and MSG_ROVRD as they may contain useful values.\n        # Other messages cause issues when overriding values sent to the\n        # boiler.\n        if src == 'A' and msgid not in [MSG_TROVRD, MSG_TOUTSIDE,\n                                        MSG_ROVRD]:\n            return\n        # Ignore upstream MSG_TROVRD if override is active on the gateway.\n        if (src == 'B' and msgid == MSG_TROVRD\n                and self.status.get(DATA_ROOM_SETPOINT_OVRD)):\n            return\n        if msgtype in (READ_DATA, WRITE_DATA):\n            # Data sent from thermostat\n            if msgid == MSG_STATUS:\n                # Master sends status\n                thermo_status = self._get_flag8(msb)\n                self.status[DATA_MASTER_CH_ENABLED] = thermo_status[0]\n                self.status[DATA_MASTER_DHW_ENABLED] = thermo_status[1]\n                self.status[DATA_MASTER_COOLING_ENABLED] = thermo_status[2]\n                self.status[DATA_MASTER_OTC_ENABLED] = thermo_status[3]\n                self.status[DATA_MASTER_CH2_ENABLED] = thermo_status[4]\n            elif msgid == MSG_MCONFIG:\n                # Master sends ID\n                self.status[DATA_MASTER_MEMBERID] = self._get_u8(lsb)\n            elif msgid == MSG_TRSET:\n                # Master changes room setpoint, support by the boiler\n                # is not mandatory, but we want the data regardless\n                self.status[DATA_ROOM_SETPOINT] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TRSET2:\n                # Master changes room setpoint 2, support by the boiler\n                # is not mandatory, but we want the data regardless\n                self.status[DATA_ROOM_SETPOINT_2] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TROOM:\n                # Master reports sensed room temperature\n                self.status[DATA_ROOM_TEMP] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_OTVERM:\n                # Master reports OpenTherm version\n                self.status[DATA_MASTER_OT_VERSION] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_MVER:\n                # Master reports product type and version\n                self.status[DATA_MASTER_PRODUCT_TYPE] = self._get_u8(msb)\n                self.status[DATA_MASTER_PRODUCT_VERSION] = self._get_u8(lsb)\n        elif msgtype in (READ_ACK, WRITE_ACK):\n            # Data sent from boiler\n            if msgid == MSG_STATUS:\n                # Slave reports status\n                boiler_status = self._get_flag8(lsb)\n                self.status[DATA_SLAVE_FAULT_IND] = boiler_status[0]\n                self.status[DATA_SLAVE_CH_ACTIVE] = boiler_status[1]\n                self.status[DATA_SLAVE_DHW_ACTIVE] = boiler_status[2]\n                self.status[DATA_SLAVE_FLAME_ON] = boiler_status[3]\n                self.status[DATA_SLAVE_COOLING_ACTIVE] = boiler_status[4]\n                self.status[DATA_SLAVE_CH2_ACTIVE] = boiler_status[5]\n                self.status[DATA_SLAVE_DIAG_IND] = boiler_status[6]\n            elif msgid == MSG_TSET:\n                # Slave confirms CH water setpoint\n                self.status[DATA_CONTROL_SETPOINT] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_SCONFIG:\n                # Slave reports config and ID\n                slave_status = self._get_flag8(msb)\n                self.status[DATA_SLAVE_DHW_PRESENT] = slave_status[0]\n                self.status[DATA_SLAVE_CONTROL_TYPE] = slave_status[1]\n                self.status[DATA_SLAVE_COOLING_SUPPORTED] = slave_status[2]\n                self.status[DATA_SLAVE_DHW_CONFIG] = slave_status[3]\n                self.status[DATA_SLAVE_MASTER_LOW_OFF_PUMP] = slave_status[4]\n                self.status[DATA_SLAVE_CH2_PRESENT] = slave_status[5]\n                self.status[DATA_SLAVE_MEMBERID] = self._get_u8(lsb)\n            elif msgid == MSG_COMMAND:\n                # TODO: implement command notification system\n                pass\n            elif msgid == MSG_ASFFLAGS:\n                # Slave reports fault flags\n                fault_flags = self._get_flag8(msb)\n                self.status[DATA_SLAVE_SERVICE_REQ] = fault_flags[0]\n                self.status[DATA_SLAVE_REMOTE_RESET] = fault_flags[1]\n                self.status[DATA_SLAVE_LOW_WATER_PRESS] = fault_flags[2]\n                self.status[DATA_SLAVE_GAS_FAULT] = fault_flags[3]\n                self.status[DATA_SLAVE_AIR_PRESS_FAULT] = fault_flags[4]\n                self.status[DATA_SLAVE_WATER_OVERTEMP] = fault_flags[5]\n                self.status[DATA_SLAVE_OEM_FAULT] = self._get_u8(lsb)\n            elif msgid == MSG_RBPFLAGS:\n                # Slave reports remote parameters\n                transfer_flags = self._get_flag8(msb)\n                rw_flags = self._get_flag8(lsb)\n                self.status[DATA_REMOTE_TRANSFER_DHW] = transfer_flags[0]\n                self.status[DATA_REMOTE_TRANSFER_MAX_CH] = transfer_flags[1]\n                self.status[DATA_REMOTE_RW_DHW] = rw_flags[0]\n                self.status[DATA_REMOTE_RW_MAX_CH] = rw_flags[1]\n            elif msgid == MSG_COOLING:\n                # Only report cooling control signal if slave acks it\n                self.status[DATA_COOLING_CONTROL] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TSETC2:\n                # Slave confirms CH2 water setpoint\n                self.status[DATA_CONTROL_SETPOINT_2] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TROVRD:\n                # OTGW (or downstream device) reports remote override\n                ovrd_value = self._get_f8_8(msb, lsb)\n                if ovrd_value > 0:\n                    # iSense quirk: the gateway keeps sending override value\n                    # even if the thermostat has cancelled the override.\n                    if self.status.get(OTGW_THRM_DETECT) == 'I':\n                        ovrd = await self.issue_cmd(\n                            OTGW_CMD_REPORT, OTGW_REPORT_SETPOINT_OVRD)\n                        match = re.match(r'^O=(N|[CT]([0-9]+.[0-9]+))$',\n                                         ovrd, re.IGNORECASE)\n                        if not match:\n                            return\n                        if match.group(1) in 'Nn':\n                            del self.status[DATA_ROOM_SETPOINT_OVRD]\n                        elif match.group(2):\n                            self.status[DATA_ROOM_SETPOINT_OVRD] = float(\n                                match.group(2))\n                    else:\n                        self.status[DATA_ROOM_SETPOINT_OVRD] = ovrd_value\n                elif self.status.get(DATA_ROOM_SETPOINT_OVRD):\n                    del self.status[DATA_ROOM_SETPOINT_OVRD]\n            elif msgid == MSG_MAXRMOD:\n                # Slave reports maximum modulation level\n                self.status[DATA_SLAVE_MAX_RELATIVE_MOD] = self._get_f8_8(\n                    msb, lsb)\n            elif msgid == MSG_MAXCAPMINMOD:\n                # Slave reports max capaxity and min modulation level\n                self.status[DATA_SLAVE_MAX_CAPACITY] = self._get_u8(msb)\n                self.status[DATA_SLAVE_MIN_MOD_LEVEL] = self._get_u8(lsb)\n            elif msgid == MSG_RELMOD:\n                # Slave reports relative modulation level\n                self.status[DATA_REL_MOD_LEVEL] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_CHPRESS:\n                # Slave reports CH circuit pressure\n                self.status[DATA_CH_WATER_PRESS] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_DHWFLOW:\n                # Slave reports DHW flow rate\n                self.status[DATA_DHW_FLOW_RATE] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TBOILER:\n                # Slave reports CH water temperature\n                self.status[DATA_CH_WATER_TEMP] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TDHW:\n                # Slave reports DHW temperature\n                self.status[DATA_DHW_TEMP] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TOUTSIDE:\n                # OTGW (or downstream device) reports outside temperature\n                self.status[DATA_OUTSIDE_TEMP] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TRET:\n                # Slave reports return water temperature\n                self.status[DATA_RETURN_WATER_TEMP] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TSTOR:\n                # Slave reports solar storage temperature\n                self.status[DATA_SOLAR_STORAGE_TEMP] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TCOLL:\n                # Slave reports solar collector temperature\n                self.status[DATA_SOLAR_COLL_TEMP] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TFLOWCH2:\n                # Slave reports CH2 water temperature\n                self.status[DATA_CH_WATER_TEMP_2] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TDHW2:\n                # Slave reports DHW2 temperature\n                self.status[DATA_DHW_TEMP_2] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_TEXHAUST:\n                # Slave reports exhaust temperature\n                self.status[DATA_EXHAUST_TEMP] = self._get_s16(msb, lsb)\n            elif msgid == MSG_TDHWSETUL:\n                # Slave reports min/max DHW setpoint\n                self.status[DATA_SLAVE_DHW_MAX_SETP] = self._get_s8(msb)\n                self.status[DATA_SLAVE_DHW_MIN_SETP] = self._get_s8(lsb)\n            elif msgid == MSG_TCHSETUL:\n                # Slave reports min/max CH setpoint\n                self.status[DATA_SLAVE_CH_MAX_SETP] = self._get_s8(msb)\n                self.status[DATA_SLAVE_CH_MIN_SETP] = self._get_s8(lsb)\n            elif msgid == MSG_TDHWSET:\n                # Slave reports or acks DHW setpoint\n                self.status[DATA_DHW_SETPOINT] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_MAXTSET:\n                # Slave reports or acks max CH setpoint\n                self.status[DATA_MAX_CH_SETPOINT] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_ROVRD:\n                # OTGW (or downstream device) reports remote override\n                # behaviour\n                rovrd_flags = self._get_flag8(lsb)\n                self.status[DATA_ROVRD_MAN_PRIO] = rovrd_flags[0]\n                self.status[DATA_ROVRD_AUTO_PRIO] = rovrd_flags[1]\n            elif msgid == MSG_OEMDIAG:\n                # Slave reports diagnostic info\n                self.status[DATA_OEM_DIAG] = self._get_u16(msb, lsb)\n            elif msgid == MSG_BURNSTARTS:\n                # Slave reports burner starts\n                self.status[DATA_TOTAL_BURNER_STARTS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_CHPUMPSTARTS:\n                # Slave reports CH pump starts\n                self.status[DATA_CH_PUMP_STARTS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_DHWPUMPSTARTS:\n                # Slave reports DHW pump starts\n                self.status[DATA_DHW_PUMP_STARTS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_DHWBURNSTARTS:\n                # Slave reports DHW burner starts\n                self.status[DATA_DHW_BURNER_STARTS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_BURNHRS:\n                # Slave reports CH burner hours\n                self.status[DATA_TOTAL_BURNER_HOURS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_CHPUMPHRS:\n                # Slave reports CH pump hours\n                self.status[DATA_CH_PUMP_HOURS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_DHWPUMPHRS:\n                # Slave reports DHW pump hours\n                self.status[DATA_DHW_PUMP_HOURS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_DHWBURNHRS:\n                # Slave reports DHW burner hours\n                self.status[DATA_DHW_BURNER_HOURS] = self._get_u16(msb, lsb)\n            elif msgid == MSG_OTVERS:\n                # Slave reports OpenTherm version\n                self.status[DATA_SLAVE_OT_VERSION] = self._get_f8_8(msb, lsb)\n            elif msgid == MSG_SVER:\n                # Slave reports product type and version\n                self.status[DATA_SLAVE_PRODUCT_TYPE] = self._get_u8(msb)\n                self.status[DATA_SLAVE_PRODUCT_VERSION] = self._get_u8(lsb)\n        self._updateq.put_nowait(dict(self.status))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_flag8(self, byte):\n        ret = [0, 0, 0, 0, 0, 0, 0, 0]\n        byte = byte[0]\n        for i in range(0, 8):\n            ret[i] = (byte & 1)\n            byte = byte >> 1\n        return ret", "response": "Split a byte into a list of 8 bits."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert 2 bytes into an unsigned int.", "response": "def _get_u16(self, msb, lsb):\n        \"\"\"\n        Convert 2 bytes into an unsigned int.\n        \"\"\"\n        buf = struct.pack('>BB', self._get_u8(msb), self._get_u8(lsb))\n        return int(struct.unpack('>H', buf)[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert 2 bytes into a signed int.", "response": "def _get_s16(self, msb, lsb):\n        \"\"\"\n        Convert 2 bytes into a signed int.\n        \"\"\"\n        buf = struct.pack('>bB', self._get_s8(msb), self._get_u8(lsb))\n        return int(struct.unpack('>h', buf)[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreports the current status of the items in the update queue.", "response": "async def _report(self):\n        \"\"\"\n        Call _update_cb with the status dict as an argument whenever a status\n        update occurs.\n\n        This method is a coroutine\n        \"\"\"\n        while True:\n            oldstatus = dict(self.status)\n            stat = await self._updateq.get()\n            if self._update_cb is not None and oldstatus != stat:\n                # Each client gets its own copy of the dict.\n                self.loop.create_task(self._update_cb(dict(stat)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister the update callback.", "response": "async def set_update_cb(self, cb):\n        \"\"\"Register the update callback.\"\"\"\n        if self._report_task is not None and not self._report_task.cancelled():\n            self.loop.create_task(self._report_task.cancel())\n        self._update_cb = cb\n        if cb is not None:\n            self._report_task = self.loop.create_task(self._report())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def issue_cmd(self, cmd, value, retry=3):\n        async with self._cmd_lock:\n            if not self.connected:\n                _LOGGER.debug(\n                    \"Serial transport closed, not sending command %s\", cmd)\n                return\n            while not self._cmdq.empty():\n                _LOGGER.debug(\"Clearing leftover message from command queue:\"\n                              \" %s\", await self._cmdq.get())\n            _LOGGER.debug(\"Sending command: %s with value %s\", cmd, value)\n            self.transport.write(\n                '{}={}\\r\\n'.format(cmd, value).encode('ascii'))\n            if cmd == OTGW_CMD_REPORT:\n                expect = r'^{}:\\s*([A-Z]{{2}}|{}=[^$]+)$'.format(cmd, value)\n            else:\n                expect = r'^{}:\\s*([^$]+)$'.format(cmd)\n\n            async def send_again(err):\n                \"\"\"Resend the command.\"\"\"\n                nonlocal retry\n                _LOGGER.warning(\"Command %s failed with %s, retrying...\", cmd,\n                                err)\n                retry -= 1\n                self.transport.write(\n                    '{}={}\\r\\n'.format(cmd, value).encode('ascii'))\n\n            async def process(msg):\n                \"\"\"Process a possible response.\"\"\"\n                _LOGGER.debug(\"Got possible response for command %s: %s\", cmd,\n                              msg)\n                if msg in OTGW_ERRS:\n                    # Some errors appear by themselves on one line.\n                    if retry == 0:\n                        raise OTGW_ERRS[msg]\n                    await send_again(msg)\n                    return\n                if cmd == OTGW_CMD_MODE and value == 'R':\n                    # Device was reset, msg contains build info\n                    while not re.match(\n                            r'OpenTherm Gateway \\d+\\.\\d+\\.\\d+', msg):\n                        msg = await self._cmdq.get()\n                    return True\n                match = re.match(expect, msg)\n                if match:\n                    if match.group(1) in OTGW_ERRS:\n                        # Some errors are considered a response.\n                        if retry == 0:\n                            raise OTGW_ERRS[match.group(1)]\n                        await send_again(msg)\n                        return\n                    ret = match.group(1)\n                    if cmd == OTGW_CMD_SUMMARY and ret == '1':\n                        # Expects a second line\n                        part2 = await self._cmdq.get()\n                        ret = [ret, part2]\n                    return ret\n                if re.match(r'Error 0[1-4]', msg):\n                    _LOGGER.warning(\"Received %s. If this happens during a \"\n                                    \"reset of the gateway it can be safely \"\n                                    \"ignored.\", msg)\n                    return\n                _LOGGER.warning(\"Unknown message in command queue: %s\", msg)\n                await send_again(msg)\n\n            while True:\n                msg = await self._cmdq.get()\n                ret = await process(msg)\n                if ret is not None:\n                    return ret", "response": "Issue a command to the serial device."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def connect(self, loop, port, baudrate=9600,\n                      bytesize=serial.EIGHTBITS, parity=serial.PARITY_NONE,\n                      stopbits=serial.STOPBITS_ONE, connection_timeout=10,\n                      inactivity_timeout=5):\n        \"\"\"\n        Connect to Opentherm Gateway at @port.\n        Initialize the parameters obtained from the PS= and PR=\n        commands and returns the status dict with the obtained values.\n        If called while connected, reconnect to the gateway.\n\n        This method is a coroutine\n        \"\"\"\n        if self._connected:\n            # We are actually reconnecting, cleanup first.\n            _LOGGER.debug(\"Reconnecting to serial device on %s\", port)\n            if self._gpio_task:\n                self._gpio_task.cancel()\n            self._connected = False\n            self._transport.close()\n            await asyncio.sleep(3)\n        self.loop = loop\n        transport = None\n        while transport is None:\n            try:\n                transport, protocol = (\n                    await serial_asyncio.create_serial_connection(\n                        loop, otgw.protocol, port, baudrate, bytesize, parity,\n                        stopbits, connection_timeout))\n            except serial.serialutil.SerialException as e:\n                if not self._conn_error:\n                    _LOGGER.error(\n                        \"Could not connect to serial device on %s. \"\n                        \"Will keep trying. Reported error was: %s\", port, e)\n                    self._conn_error = True\n                transport = None\n                await asyncio.sleep(5)\n        self._conn_error = False\n        _LOGGER.debug(\"Connected to serial device on %s\", port)\n        self._transport = transport\n        self._protocol = protocol\n        self.loop.create_task(self._protocol.set_update_cb(self._send_report))\n        if 0 < inactivity_timeout < 3:\n            _LOGGER.error(\"Inactivity timeout too low. Should be at least 3 \"\n                          \"seconds, got %d\", inactivity_timeout)\n        if inactivity_timeout >= 3:\n            async def reconnect():\n                \"\"\"Reconnect to the OpenTherm Gateway.\"\"\"\n                _LOGGER.debug(\"Scheduling reconnect...\")\n                await self.connect(\n                    loop, port, baudrate, bytesize, parity, stopbits,\n                    connection_timeout, inactivity_timeout)\n            self.loop.create_task(\n                self._protocol.setup_watchdog(reconnect, inactivity_timeout))\n        self._gpio_task = None\n        self._connected = True\n        await self.get_reports()\n        await self.get_status()\n        if (self._protocol.status.get(OTGW_GPIO_A)\n                or self._protocol.status.get(OTGW_GPIO_B)):\n            await self._poll_gpio(True)\n        return dict(self._protocol.status)", "response": "Connect to the OpenTherm Gateway at the given port."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_target_temp(self):\n        if not self._connected:\n            return\n        temp_ovrd = self._protocol.status.get(DATA_ROOM_SETPOINT_OVRD)\n        if temp_ovrd:\n            return temp_ovrd\n        return self._protocol.status.get(DATA_ROOM_SETPOINT)", "response": "Get the target temperature."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the thermostat setpoint and return the newly accepted version of the thermostat.", "response": "async def set_target_temp(self, temp, temporary=True,\n                              timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Set the thermostat setpoint and return the newly accepted\n        value.\n        kwarg @temporary specifies whether or not the thermostat\n            program may override this temperature.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = (OTGW_CMD_TARGET_TEMP if temporary\n               else OTGW_CMD_TARGET_TEMP_CONST)\n        value = '{:2.1f}'.format(temp)\n        status = {}\n        ret = await self._wait_for_cmd(cmd, value, timeout)\n        if ret is None:\n            return\n        ret = float(ret)\n        if 0 <= ret <= 30:\n            if ret == 0:\n                status[OTGW_SETP_OVRD_MODE] = OTGW_SETP_OVRD_DISABLED\n                status[DATA_ROOM_SETPOINT_OVRD] = None\n            else:\n                if temporary:\n                    ovrd_mode = OTGW_SETP_OVRD_TEMPORARY\n                else:\n                    ovrd_mode = OTGW_SETP_OVRD_PERMANENT\n                status[OTGW_SETP_OVRD_MODE] = ovrd_mode\n                status[DATA_ROOM_SETPOINT_OVRD] = ret\n            self._update_status(status)\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def set_outside_temp(self, temp, timeout=OTGW_DEFAULT_TIMEOUT):\n        cmd = OTGW_CMD_OUTSIDE_TEMP\n        status = {}\n        if temp < -40:\n            return None\n        value = '{:2.1f}'.format(temp)\n        ret = await self._wait_for_cmd(cmd, value, timeout)\n        if ret is None:\n            return\n        if ret not in ['-', None]:\n            ret = float(ret)\n        if ret == '-':\n            status[DATA_OUTSIDE_TEMP] = 0.0\n        else:\n            status[DATA_OUTSIDE_TEMP] = ret\n        self._update_status(status)\n        return ret", "response": "This method sets the outside temperature of the thermostat."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def set_clock(self, date=datetime.now(),\n                        timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Change the time and day of the week of the thermostat. The\n        gateway will send the specified time and day of the week in\n        response to the next time and date message from the thermostat.\n        @date is a :datetime: object which defaults to now()\n        Return the response from the gateway with format HH:MM/DOW,\n        where DOW is a single digit: 1=Monday, 7=Sunday.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = OTGW_CMD_SET_CLOCK\n        value = \"{}/{}\".format(date.strftime('%H:%M'), date.isoweekday())\n        return await self._wait_for_cmd(cmd, value, timeout)", "response": "This method is used to set the time and day of the week of the thermostat. It will set the time and day of the week of the thermostat to the specified date."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def get_reports(self):\n        cmd = OTGW_CMD_REPORT\n        reports = {}\n        for value in OTGW_REPORTS.keys():\n            ret = await self._wait_for_cmd(cmd, value)\n            if ret is None:\n                reports[value] = None\n                continue\n            reports[value] = ret[2:]\n        status = {\n            OTGW_ABOUT: reports.get(OTGW_REPORT_ABOUT),\n            OTGW_BUILD: reports.get(OTGW_REPORT_BUILDDATE),\n            OTGW_CLOCKMHZ: reports.get(OTGW_REPORT_CLOCKMHZ),\n            OTGW_MODE: reports.get(OTGW_REPORT_GW_MODE),\n            OTGW_SMART_PWR: reports.get(OTGW_REPORT_SMART_PWR),\n            OTGW_THRM_DETECT: reports.get(OTGW_REPORT_THERMOSTAT_DETECT),\n            OTGW_DHW_OVRD: reports.get(OTGW_REPORT_DHW_SETTING),\n        }\n        ovrd_mode = reports.get(OTGW_REPORT_SETPOINT_OVRD)\n        if ovrd_mode is not None:\n            ovrd_mode = str.upper(ovrd_mode[0])\n            status.update({OTGW_SETP_OVRD_MODE: ovrd_mode})\n        gpio_funcs = reports.get(OTGW_REPORT_GPIO_FUNCS)\n        if gpio_funcs is not None:\n            status.update({\n                OTGW_GPIO_A: int(gpio_funcs[0]),\n                OTGW_GPIO_B: int(gpio_funcs[1]),\n            })\n        led_funcs = reports.get(OTGW_REPORT_LED_FUNCS)\n        if led_funcs is not None:\n            status.update({\n                OTGW_LED_A: led_funcs[0],\n                OTGW_LED_B: led_funcs[1],\n                OTGW_LED_C: led_funcs[2],\n                OTGW_LED_D: led_funcs[3],\n                OTGW_LED_E: led_funcs[4],\n                OTGW_LED_F: led_funcs[5],\n            })\n        tweaks = reports.get(OTGW_REPORT_TWEAKS)\n        if tweaks is not None:\n            status.update({\n                OTGW_IGNORE_TRANSITIONS: int(tweaks[0]),\n                OTGW_OVRD_HB: int(tweaks[1]),\n            })\n        sb_temp = reports.get(OTGW_REPORT_SETBACK_TEMP)\n        if sb_temp is not None:\n            status.update({OTGW_SB_TEMP: float(sb_temp)})\n        vref = reports.get(OTGW_REPORT_VREF)\n        if vref is not None:\n            status.update({OTGW_VREF: int(vref)})\n        if (ovrd_mode is not None and ovrd_mode != OTGW_SETP_OVRD_DISABLED):\n            status[DATA_ROOM_SETPOINT_OVRD] = float(\n                reports[OTGW_REPORT_SETPOINT_OVRD][1:])\n        self._update_status(status)\n        return dict(self._protocol.status)", "response": "Get all of the GOEA reports and return the updated status dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the status of the current object from the PS command and return the updated status dict.", "response": "async def get_status(self):\n        \"\"\"\n        Update the pyotgw object with the information from the PS\n        command and return the updated status dict.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = OTGW_CMD_SUMMARY\n        ret = await self._wait_for_cmd(cmd, 1)\n        # Return to 'reporting' mode\n        asyncio.ensure_future(self._wait_for_cmd(cmd, 0), loop=self.loop)\n        if ret is None:\n            return\n        fields = ret[1].split(',')\n        device_status = fields[0].split('/')\n        master_status = device_status[0]\n        slave_status = device_status[1]\n        remote_params = fields[2].split('/')\n        capmodlimits = fields[4].split('/')\n        dhw_setp_bounds = fields[13].split('/')\n        ch_setp_bounds = fields[14].split('/')\n        status = {\n            DATA_MASTER_CH_ENABLED: int(master_status[7]),\n            DATA_MASTER_DHW_ENABLED: int(master_status[6]),\n            DATA_MASTER_COOLING_ENABLED: int(master_status[5]),\n            DATA_MASTER_OTC_ENABLED: int(master_status[4]),\n            DATA_MASTER_CH2_ENABLED: int(master_status[3]),\n            DATA_SLAVE_FAULT_IND: int(slave_status[7]),\n            DATA_SLAVE_CH_ACTIVE: int(slave_status[6]),\n            DATA_SLAVE_DHW_ACTIVE: int(slave_status[5]),\n            DATA_SLAVE_FLAME_ON: int(slave_status[4]),\n            DATA_SLAVE_COOLING_ACTIVE: int(slave_status[3]),\n            DATA_SLAVE_CH2_ACTIVE: int(slave_status[2]),\n            DATA_SLAVE_DIAG_IND: int(slave_status[1]),\n            DATA_CONTROL_SETPOINT: float(fields[1]),\n            DATA_REMOTE_TRANSFER_DHW: int(remote_params[0][7]),\n            DATA_REMOTE_TRANSFER_MAX_CH: int(remote_params[0][6]),\n            DATA_REMOTE_RW_DHW: int(remote_params[1][7]),\n            DATA_REMOTE_RW_MAX_CH: int(remote_params[1][6]),\n            DATA_SLAVE_MAX_RELATIVE_MOD: float(fields[3]),\n            DATA_SLAVE_MAX_CAPACITY: int(capmodlimits[0]),\n            DATA_SLAVE_MIN_MOD_LEVEL: int(capmodlimits[1]),\n            DATA_ROOM_SETPOINT: float(fields[5]),\n            DATA_REL_MOD_LEVEL: float(fields[6]),\n            DATA_CH_WATER_PRESS: float(fields[7]),\n            DATA_ROOM_TEMP: float(fields[8]),\n            DATA_CH_WATER_TEMP: float(fields[9]),\n            DATA_DHW_TEMP: float(fields[10]),\n            DATA_OUTSIDE_TEMP: float(fields[11]),\n            DATA_RETURN_WATER_TEMP: float(fields[12]),\n            DATA_SLAVE_DHW_MAX_SETP: int(dhw_setp_bounds[0]),\n            DATA_SLAVE_DHW_MIN_SETP: int(dhw_setp_bounds[1]),\n            DATA_SLAVE_CH_MAX_SETP: int(ch_setp_bounds[0]),\n            DATA_SLAVE_CH_MIN_SETP: int(ch_setp_bounds[1]),\n            DATA_DHW_SETPOINT: float(fields[15]),\n            DATA_MAX_CH_SETPOINT: float(fields[16]),\n            DATA_TOTAL_BURNER_STARTS: int(fields[17]),\n            DATA_CH_PUMP_STARTS: int(fields[18]),\n            DATA_DHW_PUMP_STARTS: int(fields[19]),\n            DATA_DHW_BURNER_STARTS: int(fields[20]),\n            DATA_TOTAL_BURNER_HOURS: int(fields[21]),\n            DATA_CH_PUMP_HOURS: int(fields[22]),\n            DATA_DHW_PUMP_HOURS: int(fields[23]),\n            DATA_DHW_BURNER_HOURS: int(fields[24])\n        }\n        self._update_status(status)\n        return dict(self._protocol.status)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the domestic hot water enable option.", "response": "async def set_hot_water_ovrd(self, state, timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Control the domestic hot water enable option. If the boiler has\n        been configured to let the room unit control when to keep a\n        small amount of water preheated, this command can influence\n        that.\n        @state should be 0 or 1 to enable the override in off or on\n        state, or any other single character to disable the override.\n        Return the accepted value, 'A' if the override is disabled\n        or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = OTGW_CMD_HOT_WATER\n        status = {}\n        ret = await self._wait_for_cmd(cmd, state, timeout)\n        if ret == 'A':\n            status[OTGW_DHW_OVRD] = None\n        elif ret in ['0', '1']:\n            status[OTGW_DHW_OVRD] = int(ret)\n        self._update_status(status)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def set_mode(self, mode, timeout=OTGW_DEFAULT_TIMEOUT):\n        cmd = OTGW_CMD_MODE\n        status = {}\n        ret = await self._wait_for_cmd(cmd, mode, timeout)\n        if ret is None:\n            return\n        if mode is OTGW_MODE_RESET:\n            self._protocol.status = {}\n            await self.get_reports()\n            await self.get_status()\n            return dict(self._protocol.status)\n        status[OTGW_MODE] = ret\n        self._update_status(status)\n        return ret", "response": "Set the operating mode of the device."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_led_mode(self, led_id):\n        if not self._connected:\n            return\n        return self._protocol.status.get(\"OTGW_LED_{}\".format(led_id))", "response": "Get the current LED mode for a given LED ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def set_led_mode(self, led_id, mode, timeout=OTGW_DEFAULT_TIMEOUT):\n        if led_id in \"ABCDEF\" and mode in \"RXTBOFHWCEMP\":\n            cmd = globals().get(\"OTGW_CMD_LED_{}\".format(led_id))\n            status = {}\n            ret = await self._wait_for_cmd(cmd, mode, timeout)\n            if ret is None:\n                return\n            var = globals().get(\"OTGW_LED_{}\".format(led_id))\n            status[var] = ret\n            self._update_status(status)\n            return ret", "response": "Set the mode of a specified LED."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the gpio mode for a given gpio ID.", "response": "def get_gpio_mode(self, gpio_id):\n        \"\"\"\n        Return the gpio mode for gpio :gpio_id:.\n        @gpio_id Character A or B.\n        \"\"\"\n        if not self._connected:\n            return\n        return self._protocol.status.get(\"OTGW_GPIO_{}\".format(gpio_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def set_gpio_mode(self, gpio_id, mode, timeout=OTGW_DEFAULT_TIMEOUT):\n        if gpio_id in \"AB\" and mode in range(8):\n            if mode == 7 and gpio_id != \"B\":\n                return None\n            cmd = globals().get(\"OTGW_CMD_GPIO_{}\".format(gpio_id))\n            status = {}\n            ret = await self._wait_for_cmd(cmd, mode, timeout)\n            if ret is None:\n                return\n            ret = int(ret)\n            var = globals().get(\"OTGW_GPIO_{}\".format(gpio_id))\n            status[var] = ret\n            self._update_status(status)\n            asyncio.ensure_future(\n                self._poll_gpio(self._protocol.status.get(OTGW_GPIO_A)\n                                or self._protocol.status.get(OTGW_GPIO_B)))\n            return ret", "response": "Set the mode of the specified GPIO port."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def set_setback_temp(self, sb_temp, timeout=OTGW_DEFAULT_TIMEOUT):\n        cmd = OTGW_CMD_SETBACK\n        status = {}\n        ret = await self._wait_for_cmd(cmd, sb_temp, timeout)\n        if ret is None:\n            return\n        ret = float(ret)\n        status[OTGW_SB_TEMP] = ret\n        self._update_status(status)\n        return ret", "response": "Set the setback temperature."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an alternative Data - ID to the list of commands that are supported by the boiler.", "response": "async def add_alternative(self, alt, timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Add the specified Data-ID to the list of alternative commands\n        to send to the boiler instead of a Data-ID that is known to be\n        unsupported by the boiler. Alternative Data-IDs will always be\n        sent to the boiler in a Read-Data request message with the\n        data-value set to zero. The table of alternative Data-IDs is\n        stored in non-volatile memory so it will persist even if the\n        gateway has been powered off. Data-ID values from 1 to 255 are\n        allowed.\n        Return the ID that was added to the list, or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = OTGW_CMD_ADD_ALT\n        alt = int(alt)\n        if alt < 1 or alt > 255:\n            return None\n        ret = await self._wait_for_cmd(cmd, alt, timeout)\n        if ret is not None:\n            return int(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def del_alternative(self, alt, timeout=OTGW_DEFAULT_TIMEOUT):\n        cmd = OTGW_CMD_DEL_ALT\n        alt = int(alt)\n        if alt < 1 or alt > 255:\n            return None\n        ret = await self._wait_for_cmd(cmd, alt, timeout)\n        if ret is not None:\n            return int(ret)", "response": "This method deletes the specified alternative Data - ID from the list of alternative data - IDs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an unknown ID to the current state of the object.", "response": "async def add_unknown_id(self, unknown_id, timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Inform the gateway that the boiler doesn't support the\n        specified Data-ID, even if the boiler doesn't indicate that\n        by returning an Unknown-DataId response. Using this command\n        allows the gateway to send an alternative Data-ID to the boiler\n        instead.\n        Return the added ID, or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = OTGW_CMD_UNKNOWN_ID\n        unknown_id = int(unknown_id)\n        if unknown_id < 1 or unknown_id > 255:\n            return None\n        ret = await self._wait_for_cmd(cmd, unknown_id, timeout)\n        if ret is not None:\n            return int(ret)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart forwarding the specified Data-ID to the boiler again. This command resets the counter used to determine if the specified Data-ID is supported by the boiler. Return the ID that was marked as supported, or None on failure. This method is a coroutine", "response": "async def del_unknown_id(self, unknown_id, timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Start forwarding the specified Data-ID to the boiler again.\n        This command resets the counter used to determine if the\n        specified Data-ID is supported by the boiler.\n        Return the ID that was marked as supported, or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = OTGW_CMD_KNOWN_ID\n        unknown_id = int(unknown_id)\n        if unknown_id < 1 or unknown_id > 255:\n            return None\n        ret = await self._wait_for_cmd(cmd, unknown_id, timeout)\n        if ret is not None:\n            return int(ret)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the maximum relative modulation from the thermostat.", "response": "async def set_max_relative_mod(self, max_mod,\n                                   timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Override the maximum relative modulation from the thermostat.\n        Valid values are 0 through 100. Clear the setting by specifying\n        a non-numeric value.\n        Return the newly accepted value, '-' if a previous value was\n        cleared, or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n        if isinstance(max_mod, int) and not 0 <= max_mod <= 100:\n            return None\n        cmd = OTGW_CMD_MAX_MOD\n        status = {}\n        ret = await self._wait_for_cmd(cmd, max_mod, timeout)\n        if ret not in ['-', None]:\n            ret = int(ret)\n        if ret == '-':\n            status[DATA_SLAVE_MAX_RELATIVE_MOD] = None\n        else:\n            status[DATA_SLAVE_MAX_RELATIVE_MOD] = ret\n        self._update_status(status)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def set_control_setpoint(self, setpoint,\n                                   timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Manipulate the control setpoint being sent to the boiler. Set\n        to 0 to pass along the value specified by the thermostat.\n        Return the newly accepted value, or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n        cmd = OTGW_CMD_CONTROL_SETPOINT\n        status = {}\n        ret = await self._wait_for_cmd(cmd, setpoint, timeout)\n        if ret is None:\n            return\n        ret = float(ret)\n        status[DATA_CONTROL_SETPOINT] = ret\n        self._update_status(status)\n        return ret", "response": "This method is used to set the control setpoint for the boiler."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the CH enable bit.", "response": "async def set_ch_enable_bit(self, ch_bit, timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Control the CH enable status bit when overriding the control\n        setpoint. By default the CH enable bit is set after a call to\n        set_control_setpoint with a value other than 0. With this\n        method, the bit can be manipulated.\n        @ch_bit can be either 0 or 1.\n        Return the newly accepted value (0 or 1), or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n        if ch_bit not in [0, 1]:\n            return None\n        cmd = OTGW_CMD_CONTROL_HEATING\n        status = {}\n        ret = await self._wait_for_cmd(cmd, ch_bit, timeout)\n        if ret is None:\n            return\n        ret = int(ret)\n        status[DATA_MASTER_CH_ENABLED] = ret\n        self._update_status(status)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def set_ventilation(self, pct, timeout=OTGW_DEFAULT_TIMEOUT):\n        if not 0 <= pct <= 100:\n            return None\n        cmd = OTGW_CMD_VENT\n        status = {}\n        ret = await self._wait_for_cmd(cmd, pct, timeout)\n        if ret is None:\n            return\n        ret = int(ret)\n        status[DATA_COOLING_CONTROL] = ret\n        self._update_status(status)\n        return ret", "response": "Set the ventilation setpoint override value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubscribing to status updates from the Opentherm Gateway.", "response": "def subscribe(self, coro):\n        \"\"\"\n        Subscribe to status updates from the Opentherm Gateway.\n        Can only be used after connect()\n        @coro is a coroutine which will be called with a single\n        argument (status) when a status change occurs.\n        Return True on success, False if not connected or already\n        subscribed.\n        \"\"\"\n        if coro not in self._notify:\n            self._notify.append(coro)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unsubscribe(self, coro):\n        if coro in self._notify:\n            self._notify.remove(coro)\n            return True\n        return False", "response": "Unsubscribe from status updates from the Opentherm Gateway."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a report to all subscribed coroutines.", "response": "async def _send_report(self, status):\n        \"\"\"\n        Call all subscribed coroutines in _notify whenever a status\n        update occurs.\n\n        This method is a coroutine\n        \"\"\"\n        if len(self._notify) > 0:\n            # Each client gets its own copy of the dict.\n            asyncio.gather(*[coro(dict(status)) for coro in self._notify],\n                           loop=self.loop)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _wait_for_cmd(self, cmd, value, timeout=OTGW_DEFAULT_TIMEOUT):\n        if not self._connected:\n            return\n        try:\n            return await asyncio.wait_for(self._protocol.issue_cmd(cmd, value),\n                                          timeout,\n                                          loop=self.loop)\n        except TimeoutError:\n            _LOGGER.error(\"Timed out waiting for command: %s, value: %s.\", cmd,\n                          value)\n            return\n        except (RuntimeError, SyntaxError, ValueError) as exc:\n            _LOGGER.error(\"Command %s with value %s raised exception: %s\", cmd,\n                          value, exc)", "response": "Wrap a command in applicable asyncio call."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npolling the GPIO states.", "response": "async def _poll_gpio(self, poll, interval=10):\n        \"\"\"\n        Start or stop polling GPIO states.\n\n        GPIO states aren't being pushed by the gateway, we need to poll\n        if we want updates.\n        \"\"\"\n        if poll and self._gpio_task is None:\n            async def polling_routine(interval):\n                \"\"\"Poll GPIO state every @interval seconds.\"\"\"\n                while True:\n                    try:\n                        pios = None\n                        ret = await self._wait_for_cmd(\n                            OTGW_CMD_REPORT, OTGW_REPORT_GPIO_STATES)\n                        if ret:\n                            pios = ret[2:]\n                            status = {\n                                OTGW_GPIO_A_STATE: int(pios[0]),\n                                OTGW_GPIO_B_STATE: int(pios[1]),\n                            }\n                            self._update_status(status)\n                        await asyncio.sleep(interval)\n                    except asyncio.CancelledError:\n                        status = {\n                            OTGW_GPIO_A_STATE: 0,\n                            OTGW_GPIO_B_STATE: 0,\n                        }\n                        self._update_status(status)\n                        self._gpio_task = None\n                        break\n            self._gpio_task = self.loop.create_task(polling_routine(interval))\n        elif not poll and self._gpio_task is not None:\n            self._gpio_task.cancel()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_status(self, update):\n        if isinstance(update, dict):\n            self._protocol.status.update(update)\n            self._protocol._updateq.put_nowait(self._protocol.status)", "response": "Update the status dict and push it to subscribers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new request object.", "response": "def create_request(self, method='', url='', query_params=None, body=None, headers=None):\n        \"\"\"\n        :param method:\n        :param url:\n        :param query_params:\n        :param body:\n        :param headers:\n        :return:requests.Request\n        \"\"\"\n\n        if query_params:\n            query = urlencode(query_params)\n            if query:\n                url = url + ('&' if url.find('?') > 0 else '?') + query\n\n        content_type = None\n        accept = None\n\n        if headers is None:\n            headers = {}\n\n        it = iterator(headers)\n\n        for key, value in it:\n            if key.lower().find('content-type') >= 0:\n                content_type = value\n            if key.lower().find('accept') >= 0:\n                accept = value\n\n        if content_type is None:\n            content_type = 'application/json'\n            headers['Content-Type'] = content_type\n\n        if accept is None:\n            accept = 'application/json'\n            headers['Accept'] = accept\n\n        if content_type.lower().find('application/json') >= 0:\n            body = json.dumps(body) if body else None\n        elif content_type.lower().find('application/x-www-form-urlencoded') >= 0:\n            body = urlencode(body) if body else None\n\n        return requests.Request(method, url, headers=headers, data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_dockerfile(self, output_dir):\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        lines = []\n        for istep, step in enumerate(self.steps):\n            if istep == 0:\n                lines.extend(step.dockerfile_lines)\n            else:\n                lines.extend(step.dockerfile_lines[1:])\n        path = os.path.join(output_dir, 'Dockerfile.%s' % self.imagename)\n        with open(path, 'w') as outfile:\n            outfile.write('\\n'.join(lines))\n        print('Wrote %s' % path)", "response": "Write a Dockerfile that will not be built by docker - make\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(self, client,\n              nobuild=False,\n              usecache=True,\n              pull=False):\n        \"\"\"\n        Drives the build of the final image - get the list of steps and execute them.\n\n        Args:\n            client (docker.Client): docker client object that will build the image\n            nobuild (bool): just create dockerfiles, don't actually build the image\n            usecache (bool): use docker cache, or rebuild everything from scratch?\n            pull (bool): try to pull new versions of repository images?\n        \"\"\"\n        if not nobuild:\n            self.update_source_images(client,\n                                      usecache=usecache,\n                                      pull=pull)\n\n        width = utils.get_console_width()\n        cprint('\\n' + '='*width,\n               color='white', attrs=['bold'])\n\n        line = 'STARTING BUILD for \"%s\" (image definition \"%s\" from %s)\\n' % (\n            self.targetname, self.imagename, self.steps[-1].sourcefile)\n\n        cprint(_centered(line, width), color='blue', attrs=['bold'])\n\n        for istep, step in enumerate(self.steps):\n            print(colored('* Step','blue'),\n                  colored('%d/%d' % (istep+1, len(self.steps)), 'blue', attrs=['bold']),\n                  colored('for image', color='blue'),\n                  colored(self.imagename, color='blue', attrs=['bold']))\n\n            if not nobuild:\n                if step.bust_cache:\n                    stackkey = self._get_stack_key(istep)\n                    if stackkey in _rebuilt:\n                        step.bust_cache = False\n\n                step.build(client, usecache=usecache)\n                print(colored(\"* Created intermediate image\", 'green'),\n                      colored(step.buildname, 'green', attrs=['bold']),\n                      end='\\n\\n')\n\n                if step.bust_cache:\n                    _rebuilt.add(stackkey)\n\n        finalimage = step.buildname\n\n        if not nobuild:\n            self.finalizenames(client, finalimage)\n            line = 'FINISHED BUILDING \"%s\" (image definition \"%s\" from %s)'%(\n                self.targetname, self.imagename, self.steps[-1].sourcefile)\n            cprint(_centered(line, width),\n                   color='green', attrs=['bold'])\n            cprint('=' * width, color='white', attrs=['bold'], end='\\n\\n')", "response": "Builds the image and returns the image ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef finalizenames(self, client, finalimage):\n        client.api.tag(finalimage, *self.targetname.split(':'))\n        cprint('Tagged final image as \"%s\"' % self.targetname,\n               'green')\n        if not self.keepbuildtags:\n            print('Untagging intermediate containers:', end='')\n            for step in self.steps:\n                client.api.remove_image(step.buildname, force=True)\n                print(step.buildname, end=',')\n            print()", "response": "Tag the built image with its final name and untag intermediate containers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the image and return the image ID.", "response": "def build(self, client, pull=False, usecache=True):\n        \"\"\"\n        Drives an individual build step. Build steps are separated by build_directory.\n        If a build has zero one or less build_directories, it will be built in a single\n        step.\n\n        Args:\n            client (docker.Client): docker client object that will build the image\n            pull (bool): whether to pull dependent layers from remote repositories\n            usecache (bool): whether to use cached layers or rebuild from scratch\n        \"\"\"\n        print(colored('  Building step', 'blue'),\n              colored(self.imagename, 'blue', attrs=['bold']),\n              colored('defined in', 'blue'),\n              colored(self.sourcefile, 'blue', attrs=['bold']))\n\n        if self.build_first and not self.build_first.built:\n            self.build_external_dockerfile(client, self.build_first)\n\n        if self.bust_cache:\n            usecache = False\n\n        if not usecache:\n            cprint('  Build cache disabled - this image will be rebuilt from scratch',\n                   'yellow')\n\n        dockerfile = u'\\n'.join(self.dockerfile_lines)\n\n        kwargs = dict(tag=self.buildname,\n                      pull=pull,\n                      nocache=not usecache,\n                      decode=True, rm=True,\n                      buildargs=self.buildargs,\n                      squash=self.squash)\n\n        if usecache:\n            utils.set_build_cachefrom(self.cache_from, kwargs, client)\n\n        if self.build_dir is not None:\n            tempdir = self.write_dockerfile(dockerfile)\n            context_path = os.path.abspath(os.path.expanduser(self.build_dir))\n            kwargs.update(fileobj=None,\n                          dockerfile=os.path.join(DOCKER_TMPDIR, 'Dockerfile'))\n            print(colored('  Build context:', 'blue'),\n                  colored(os.path.relpath(context_path), 'blue', attrs=['bold']))\n\n            if not self.custom_exclude:\n                kwargs.update(path=context_path)\n            else:\n                print(colored('  Custom .dockerignore from:', 'blue'),\n                      colored(os.path.relpath(self.ignoredefs_file),  'blue', attrs=['bold']))\n\n                # AMV - this is a brittle call to an apparently \"private' docker sdk method\n                context = docker.utils.tar(self.build_dir,\n                                           exclude=self.custom_exclude,\n                                           dockerfile=(os.path.join(DOCKER_TMPDIR, 'Dockerfile'),\n                                                       dockerfile),\n                                           gzip=False)\n                kwargs.update(fileobj=context,\n                                  custom_context=True)\n\n        else:\n            if sys.version_info.major == 2:\n                fileobj = StringIO(dockerfile)\n            else:\n                fileobj = BytesIO(dockerfile.encode('utf-8'))\n\n            kwargs.update(fileobj=fileobj,\n                          path=None,\n                          dockerfile=None)\n\n            tempdir = None\n\n        # start the build\n        stream = client.api.build(**kwargs)\n        try:\n            utils.stream_docker_logs(stream, self.buildname)\n        except docker.errors.APIError as e:\n            if self.squash and not client.version().get('Experimental', False):\n                raise errors.ExperimentalDaemonRequiredError(\n                        'Docker error message:\\n   ' + str(e) +\n                        '\\n\\nUsing `squash` and/or `secret_files` requires a docker'\n                        \" daemon with experimental features enabled. See\\n\"\n                        \"    https://github.com/docker/docker-ce/blob/master/components/cli/\"\n                        \"experimental/README.md\")\n            else:\n                raise errors.BuildError(dockerfile, str(e), kwargs)\n        except ValueError as e:\n            raise errors.BuildError(dockerfile, str(e), kwargs)\n\n        if self.squash and not self.bust_cache:\n            self._resolve_squash_cache(client)\n\n        # remove the temporary dockerfile\n        if tempdir is not None:\n            os.unlink(os.path.join(tempdir, 'Dockerfile'))\n            os.rmdir(tempdir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve squashed cache for this image.", "response": "def _resolve_squash_cache(self, client):\n        \"\"\"\n        Currently doing a \"squash\" basically negates the cache for any subsequent layers.\n        But we can work around this by A) checking if the cache was successful for the _unsquashed_\n        version of the image, and B) if so, re-using an older squashed version of the image.\n\n        Three ways to do this:\n            1. get the shas of the before/after images from `image.history` comments\n                OR the output stream (or both). Both are extremely brittle, but also easy to access\n           2. Build the image without squash first. If the unsquashed image sha matches\n                  a cached one, substitute the unsuqashed image for the squashed one.\n                  If no match, re-run the steps with squash=True and store the resulting pair\n                  Less brittle than 1., but harder and defs not elegant\n           3. Use docker-squash as a dependency - this is by far the most preferable solution,\n              except that they don't yet support the newest docker sdk version.\n\n        Currently option 1 is implemented - we parse the comment string in the image history\n        to figure out which layers the image was squashed from\n        \"\"\"\n        from .staging import BUILD_CACHEDIR\n\n        history = client.api.history(self.buildname)\n        comment = history[0].get('Comment', '').split()\n        if len(comment) != 4 or comment[0] != 'merge' or comment[2] != 'to':\n            print('WARNING: failed to parse this image\\'s pre-squash history. '\n                  'The build will continue, but all subsequent layers will be rebuilt.')\n            return\n\n        squashed_sha = history[0]['Id']\n        start_squash_sha = comment[1]\n        end_squash_sha = comment[3]\n        cprint('  Layers %s to %s were squashed.' % (start_squash_sha, end_squash_sha), 'yellow')\n\n        # check cache\n        squashcache = os.path.join(BUILD_CACHEDIR, 'squashes')\n        if not os.path.exists(squashcache):\n            os.makedirs(squashcache)\n        cachepath = os.path.join(BUILD_CACHEDIR,\n                                 'squashes', '%s-%s' % (start_squash_sha, end_squash_sha))\n\n        # on hit, tag the squashedsha as the result of this build step\n        if os.path.exists(cachepath):\n            self._get_squashed_layer_cache(client, squashed_sha, cachepath)\n        else:\n            self._cache_squashed_layer(squashed_sha, cachepath)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(self, client, pull=False, usecache=True):\n        stage = staging.StagedFile(self.sourceimage, self.sourcepath, self.destpath,\n                                   cache_from=self.cache_from)\n        stage.stage(self.baseimage, self.buildname)", "response": "Note:\n            `pull` and `usecache` are for compatibility only. They're irrelevant because\n            hey were applied when BUILDING self.sourceimage"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dockerfile_lines(self):\n        w1 = colored(\n                'WARNING: this build includes files that are built in other images!!! The generated'\n                '\\n         Dockerfile must be built in a directory that contains'\n                ' the file/directory:',\n                'red', attrs=['bold'])\n        w2 = colored('         ' + self.sourcepath, 'red')\n        w3 = (colored('         from image ', 'red')\n              + colored(self.sourcepath, 'blue', attrs=['bold']))\n        print('\\n'.join((w1, w2, w3)))\n        return [\"\",\n                \"# Warning: the file \\\"%s\\\" from the image \\\"%s\\\"\"\n                \" must be present in this build context!!\" %\n                (self.sourcepath, self.sourceimage),\n                \"ADD %s %s\" % (os.path.basename(self.sourcepath), self.destpath),\n                '']", "response": "Returns a list of lines that are used to build the image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_yaml_and_paths(ymlfilepath, yamldefs):\n        relpath = os.path.relpath(ymlfilepath)\n        if '/' not in relpath:\n            relpath = './%s' % relpath\n        pathroot = os.path.abspath(os.path.dirname(ymlfilepath))\n\n        for imagename, defn in iteritems(yamldefs):\n            if imagename == '_SOURCES_':\n                yamldefs['_SOURCES_'] = [os.path.relpath(_get_abspath(pathroot, p))\n                                         for p in yamldefs['_SOURCES_']]\n                continue\n            elif imagename in SPECIAL_FIELDS:\n                continue\n\n            for key in ('build_directory', 'FROM_DOCKERFILE', 'ignorefile'):\n                if key in defn:\n                    defn[key] = _get_abspath(pathroot, defn[key])\n\n            if 'copy_from' in defn:\n                if not isinstance(defn['copy_from'], dict):\n                    raise errors.ParsingFailure((\n                            'Syntax error in file \"%s\": \\n' +\n                            'The \"copy_from\" field in image definition \"%s\" is not \\n' \n                            'a key:value list.') % (ymlfilepath, imagename))\n                for otherimg, value in defn.get('copy_from', {}).items():\n                    if not isinstance(value, dict):\n                        raise errors.ParsingFailure((\n                            'Syntax error in field:\\n'\n                            '     %s . copy_from . %s\\nin file \"%s\". \\n'\n                            'All entries must be of the form \"sourcepath: destpath\"')%\n                                 (imagename, otherimg, ymlfilepath))\n\n            # save the file path for logging\n            defn['_sourcefile'] = relpath\n\n            if 'ignore' in defn and 'ignorefile' in defn:\n                raise errors.MultipleIgnoreError(\n                        'Image \"%s\" has both \"ignore\" AND \"ignorefile\" fields.' % imagename +\n                        ' At most ONE of these should be defined')\n\n            if 'secret_files' in defn and not defn.get('squash', True):\n                raise errors.ParsingFailure(\n                        \"Step '%s' defines secret_files, so 'squash' cannot be set to 'false'\"\n                        % imagename)\n\n            if defn.get('secret_files', None) and defn.get('copy_from', False):\n                raise errors.ParsingFailure(\n                        '`secret_files` currently is not implmemented to handle `copy_from`'\n                        ' (step %s)' % imagename)\n\n            for key in defn:\n                if key not in RECOGNIZED_KEYS:\n                    raise errors.UnrecognizedKeyError(\n                            'Field \"%s\" in image \"%s\" in file \"%s\" not recognized' %\n                            (key, imagename, relpath))", "response": "Checks YAML for errors and resolves all paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a build for the given image and targetname.", "response": "def generate_build(self, image, targetname, rebuilds=None, cache_repo='', cache_tag='',\n                       buildargs=None, **kwargs):\n        \"\"\"\n        Separate the build into a series of one or more intermediate steps.\n        Each specified build directory gets its own step\n\n        Args:\n            image (str): name of the image as defined in the dockermake.py file\n            targetname (str): name to tag the final built image with\n            rebuilds (List[str]): list of image layers to rebuild (i.e., without docker's cache)\n            cache_repo (str): repository to get images for caches in builds\n            cache_tag (str): tags to use from repository for caches in builds\n            buildargs (dict): build-time dockerfile arugments\n            **kwargs (dict): extra keyword arguments for the BuildTarget object\n        \"\"\"\n        from_image = self.get_external_base_image(image)\n        if cache_repo or cache_tag:\n            cache_from = utils.generate_name(image, cache_repo, cache_tag)\n        else:\n            cache_from = None\n        if from_image is None:\n            raise errors.NoBaseError(\"No base image found in %s's dependencies\" % image)\n        if isinstance(from_image, ExternalDockerfile):\n            build_first = from_image\n            base_image = from_image.tag\n        else:\n            base_image = from_image\n            build_first = None\n        build_steps = []\n        istep = 0\n        sourceimages = set()\n        if rebuilds is None:\n            rebuilds = []\n        else:\n            rebuilds = set(rebuilds)\n\n        for base_name in self.sort_dependencies(image):\n            istep += 1\n            buildname = 'dmkbuild_%s_%d' % (image, istep)\n            secret_files = self.ymldefs[base_name].get('secret_files', None)\n            squash = self.ymldefs[base_name].get('squash', bool(secret_files))\n            build_steps.append(\n                    dockermake.step.BuildStep(\n                            base_name,\n                            base_image,\n                            self.ymldefs[base_name],\n                            buildname,\n                            bust_cache=base_name in rebuilds,\n                            build_first=build_first, cache_from=cache_from,\n                            buildargs=buildargs,\n                            squash=squash,\n                            secret_files=secret_files))\n\n            base_image = buildname\n            build_first = None\n\n            for sourceimage, files in iteritems(self.ymldefs[base_name].get('copy_from', {})):\n                sourceimages.add(sourceimage)\n                for sourcepath, destpath in iteritems(files):\n                    istep += 1\n                    buildname = 'dmkbuild_%s_%d' % (image, istep)\n                    build_steps.append(\n                            dockermake.step.FileCopyStep(\n                                    sourceimage, sourcepath, destpath,\n                                    base_name, base_image, self.ymldefs[base_name],\n                                    buildname, bust_cache=base_name in rebuilds,\n                                    build_first=build_first, cache_from=cache_from))\n                    base_image = buildname\n\n        sourcebuilds = [self.generate_build(img,\n                                            img,\n                                            cache_repo=cache_repo,\n                                            cache_tag=cache_tag,\n                                            **kwargs)\n                        for img in sourceimages]\n\n        return builds.BuildTarget(imagename=image,\n                                  targetname=targetname,\n                                  steps=build_steps,\n                                  sourcebuilds=sourcebuilds,\n                                  from_image=from_image,\n                                  **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_external_base_image(self, image, stack=None):\n        if stack is None:\n            stack = list()\n\n        mydef = self.ymldefs[image]\n\n        if image in stack:\n            stack.append(image)\n            raise errors.CircularDependencyError('Circular dependency found:\\n' + '->'.join(stack))\n        stack.append(image)\n\n        # Deal with FROM and FROM_DOCKERFILE fields\n        if 'FROM' in mydef and 'FROM_DOCKERFILE' in mydef:\n            raise errors.MultipleBaseError(\n                    'ERROR: Image \"%s\" has both a \"FROM\" and a \"FROM_DOCKERFILE\" field.' % image +\n                    '       It should have at most ONE of these fields.')\n        if 'FROM' in mydef:\n            externalbase = mydef['FROM']\n        elif 'FROM_DOCKERFILE' in mydef:\n            path = mydef['FROM_DOCKERFILE']\n            if path not in self._external_dockerfiles:\n                self._external_dockerfiles[path] = ExternalDockerfile(path)\n            externalbase = self._external_dockerfiles[path]\n        else:\n            externalbase = None\n\n        requires = mydef.get('requires', [])\n        if not isinstance(requires, list):\n            raise errors.InvalidRequiresList('Requirements for image \"%s\" are not a list' % image)\n\n        for base in requires:\n            try:\n                otherexternal = self.get_external_base_image(base, stack)\n            except ValueError:\n                continue\n\n            if externalbase is None:\n                externalbase = otherexternal\n            elif otherexternal is None:\n                continue\n            elif externalbase != otherexternal:\n                raise errors.ConflictingBaseError(\n                        'Multiple external dependencies: definition \"%s\" depends on:\\n' % image +\n                        '  %s (FROM: %s), and\\n' % (image, externalbase) +\n                        '  %s (FROM: %s).' % (base, otherexternal))\n\n        assert stack.pop() == image\n        return externalbase", "response": "Returns the unique external base image for the given image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stage(self, startimage, newimage):\n        client = utils.get_client()\n        cprint('  Copying file from \"%s:/%s\" \\n                 to \"%s://%s/\"'\n               % (self.sourceimage, self.sourcepath, startimage, self.destpath),\n               'blue')\n\n        # copy build artifacts from the container if necessary\n        cachedir = self._setcache(client)\n        cacherelpath = os.path.relpath(cachedir, TMPDIR)\n\n        # if cached file doesn't exist (presumably purged by OS), trigger it to be recreated\n        if os.path.exists(cachedir) and not os.path.exists(os.path.join(cachedir, 'content.tar')):\n            shutil.rmtree(cachedir)\n\n        if not os.path.exists(cachedir):\n            print(' * Creating cache at %s' % cacherelpath)\n            container = client.containers.create(self.sourceimage)\n            try:\n                tarfile_stream, tarfile_stats = container.get_archive(self.sourcepath)\n            except docker.errors.NotFound:\n                raise errors.MissingFileError(\n                        'Cannot copy file \"%s\" from image \"%s\" - it does not exist!' %\n                        (self.sourcepath, self.sourceimage))\n\n            # write files to disk (would be nice to stream them, haven't gotten it to work)\n            tempdir = tempfile.mkdtemp(dir=BUILD_TEMPDIR)\n            with open(os.path.join(tempdir, 'content.tar'), 'wb') as localfile:\n                for chunk in tarfile_stream:\n                    localfile.write(chunk)\n            os.mkdir(cachedir)\n            os.rename(tempdir, cachedir)\n        else:\n            print('  Using cached files from %s' % cacherelpath)\n\n        # write Dockerfile for the new image and then build it\n        dockerfile = 'FROM %s\\nADD content.tar %s' % (startimage, self.destpath)\n        with open(os.path.join(cachedir, 'Dockerfile'), 'w') as df:\n            df.write(dockerfile)\n\n        buildargs = dict(path=cachedir,\n                         tag=newimage,\n                         decode=True)\n        utils.set_build_cachefrom(self.cache_from, buildargs, client)\n\n        # Build and show logs\n        stream = client.api.build(**buildargs)\n        try:\n            utils.stream_docker_logs(stream, newimage)\n        except ValueError as e:\n            raise errors.BuildError(dockerfile, e.args[0], build_args=buildargs)", "response": "Copy the file from source to target"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef human_readable_size(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)", "response": "Return a human readable size."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntraverse the source looking up each key. Returns None if can t find anything.", "response": "def lookup(source, keys, fallback = None):\n  \"\"\"Traverses the source, looking up each key.  Returns None if can't find anything instead of raising an exception.\"\"\"\n  try:\n    for key in keys:\n      source = source[key]\n    return source\n  except (KeyError, AttributeError, TypeError):\n    return fallback"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to the Graphite server if not already connected.", "response": "def connect(self):\n    \"\"\"Connects to the Graphite server if not already connected.\"\"\"\n    if self.sock is not None:\n      return\n    backoff = 0.01\n    while True:\n      try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(5)\n        sock.connect((self.host, self.port))\n        self.sock = sock\n        return\n      except socket.error:\n        time.sleep(random.uniform(0, 2.0*backoff))\n        backoff = min(backoff*2.0, 5.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisconnects from the Graphite server if connected.", "response": "def disconnect(self):\n    \"\"\"Disconnect from the Graphite server if connected.\"\"\"\n    if self.sock is not None:\n      try:\n        self.sock.close()\n      except socket.error:\n        pass\n      finally:\n        self.sock = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a line to graphite. Retry with exponential backoff.", "response": "def _sendMsg(self, msg):\n    \"\"\"Send a line to graphite. Retry with exponential backoff.\"\"\"\n    if not self.sock:\n      self.connect()\n    if not isinstance(msg, binary_type):\n      msg = msg.encode(\"UTF-8\")\n\n    backoff = 0.001\n    while True:\n      try:\n        self.sock.sendall(msg)\n        break\n      except socket.error:\n        log.warning('Graphite connection error', exc_info = True)\n        self.disconnect()\n        time.sleep(random.uniform(0, 2.0*backoff))\n        backoff = min(backoff*2.0, 5.0)\n        self.connect()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog a named numeric value. The value type may be value or count.", "response": "def log(self, name, value, valueType=None, stamp=None):\n    \"\"\"Log a named numeric value. The value type may be 'value',\n    'count', or None.\"\"\"\n    if type(value) == float:\n      form = \"%s%s %2.2f %d\\n\"\n    else:\n      form = \"%s%s %s %d\\n\"\n\n    if valueType is not None and len(valueType) > 0 and valueType[0] != '.':\n      valueType = '.' + valueType\n\n    if not stamp:\n      stamp = time.time()\n\n    self._sendMsg(form % (self._sanitizeName(name), valueType or '', value, stamp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, function):\n    with self.lock:\n      oldValue = self.value\n      self.value = function(oldValue)\n      return oldValue, self.value", "response": "Atomically apply function to the value and return the old and new values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tick(self):\n    count = self._uncounted.getAndSet(0)\n    instantRate = float(count) / self.interval\n\n    if self._initialized:\n      self.rate += (self.alpha * (instantRate - self.rate))\n    else:\n      self.rate = instantRate\n      self._initialized = True", "response": "Updates rates and decays based on the current count of uncounted items"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender a GET request by showing this nodes stats and children.", "response": "def bottlestats(server_name, path=''):\n    \"\"\"Renders a GET request, by showing this nodes stats and children.\"\"\"\n    path = path.lstrip('/')\n    parts = path.split('/')\n    if not parts[0]:\n        parts = parts[1:]\n    stat_dict = util.lookup(scales.getStats(), parts)\n\n    if stat_dict is None:\n        abort(404, \"Not Found\")\n        return\n\n    output = StringIO()\n    output_format = request.query.get('format', 'html')\n    query = request.query.get('query', None)\n    if output_format == 'json':\n        response.content_type = \"application/json\"\n        formats.jsonFormat(output, stat_dict, query)\n    elif output_format == 'prettyjson':\n        formats.jsonFormat(output, stat_dict, query, pretty=True)\n        response.content_type = \"application/json\"\n    else:\n        formats.htmlHeader(output, '/' + path, server_name, query)\n        formats.htmlFormat(output, tuple(parts), stat_dict, query)\n        response.content_type = \"text/html\"\n\n    return output.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_stats_handler(app, server_name, prefix='/status/'):\n    if not prefix.endswith('/'):\n        prefix += '/'\n    handler = functools.partial(bottlestats, server_name)\n\n    app.get(prefix, callback=handler)\n    app.get(prefix + '<path:path>', callback=handler)", "response": "Register the stats handler with a Flask app serving routes\n    with a given prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a unique ID for each object.", "response": "def statsId(obj):\n  \"\"\"Gets a unique ID for each object.\"\"\"\n  if hasattr(obj, ID_KEY):\n    return getattr(obj, ID_KEY)\n  newId = next(NEXT_ID)\n  setattr(obj, ID_KEY, newId)\n  return newId"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a filtered iteration over a list of items.", "response": "def filterCollapsedItems(data):\n  \"\"\"Return a filtered iteration over a list of items.\"\"\"\n  return ((key, value)\\\n          for key, value in six.iteritems(data) \\\n          if not (isinstance(value, StatContainer) and value.isCollapsed()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the stats dict to a file", "response": "def dumpStatsTo(filename):\n  \"\"\"Writes the stats dict to filanem\"\"\"\n  with open(filename, 'w') as f:\n    latest = getStats()\n    latest['last-updated'] = time.time()\n    json.dump(getStats(), f, cls=StatContainerEncoder)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a named stats collection object.", "response": "def collection(path, *stats):\n  \"\"\"Creates a named stats collection object.\"\"\"\n\n  def initMethod(self):\n    \"\"\"Init method for the underlying stat object's class.\"\"\"\n    init(self, path)\n\n  attributes = {'__init__': initMethod}\n  for stat in stats:\n    attributes[stat.getName()] = stat\n  newClass = type('Stats:%s' % path, (object,), attributes)\n  instance = newClass()\n  for stat in stats:\n    default = stat._getInit() # Consider this method package-protected. # pylint: disable=W0212\n    if default:\n      setattr(instance, stat.getName(), default)\n  return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the static state of the object.", "response": "def reset(cls):\n    \"\"\"Resets the static state.  Should only be called by tests.\"\"\"\n    cls.stats = StatContainer()\n    cls.parentMap = {}\n    cls.containerMap = {}\n    cls.subId = 0\n    for stat in gc.get_objects():\n      if isinstance(stat, Stat):\n        stat._aggregators = {}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __getStatContainer(cls, context, parent=None):\n    container = parent\n    if container is None:\n      container = cls.stats\n    if context is not None:\n      context = str(context).lstrip('/')\n      for key in context.split('/'):\n        container.setdefault(key, StatContainer())\n        container = container[key]\n    return container", "response": "Get the stat container for the given context under the given parent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getStat(cls, obj, name):\n    objClass = type(obj)\n    for theClass in objClass.__mro__:\n      if theClass == object:\n        break\n      for value in theClass.__dict__.values():\n        if isinstance(value, Stat) and value.getName() == name:\n          return value", "response": "Gets the stat for the given object with the given name or None if no such stat exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getAggregator(cls, instanceId, name):\n    parent = cls.parentMap.get(instanceId)\n    while parent:\n      stat = cls.getStat(parent, name)\n      if stat:\n        return stat, parent\n      parent = cls.parentMap.get(statsId(parent))", "response": "Gets the aggregate stat for the given stat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating a child value.", "response": "def updateItem(self, instance, subKey, value):\n    \"\"\"Updates a child value.  Must be called before the update has actually occurred.\"\"\"\n    instanceId = statsId(instance)\n\n    container = _Stats.getContainerForObject(instanceId)\n    self._aggregate(instanceId, container, value, subKey)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, instance, oldValue, newValue):\n    self.__set__(instance,\n                 self.__get__(instance, None) + newValue - (oldValue or 0))", "response": "Updates the aggregate based on a change in the child value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the aggregate based on a change in the child value.", "response": "def update(self, instance, oldValue, newValue):\n    \"\"\"Updates the aggregate based on a change in the child value.\"\"\"\n    histogram = self.__get__(instance, None)\n    if oldValue:\n      histogram[oldValue] -= 1\n      if self.autoDelete and histogram[oldValue] == 0:\n        del histogram[oldValue]\n    if newValue:\n      histogram[newValue] += 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, instance, oldValue, newValue, subKey):\n    histogram = self.__get__(instance, None)\n    histogram[subKey] += newValue - oldValue", "response": "Updates the aggregate based on a change in the child value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addValue(self, value):\n    self['count'] += 1\n    self.__sample.update(value)\n    if time.time() > self.__timestamp + 20 and len(self.__sample) > 1:\n      self.__timestamp = time.time()\n      self['min'] = self.__sample.min\n      self['max'] = self.__sample.max\n      self['mean'] = self.__sample.mean\n      self['stddev'] = self.__sample.stddev\n\n      percentiles = self.__sample.percentiles([0.5, 0.75, 0.95, 0.98, 0.99, 0.999])\n      self['median'] = percentiles[0]\n      self['75percentile'] = percentiles[1]\n      self['95percentile'] = percentiles[2]\n      self['98percentile'] = percentiles[3]\n      self['99percentile'] = percentiles[4]\n      self.percentile99 = percentiles[4]\n      self['999percentile'] = percentiles[5]", "response": "Updates the dictionary with the given value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef incr(self, item, value):\n    if item in self:\n      old = UserDict.__getitem__(self, item)\n    else:\n      old = 0.0\n    self[item] = old + value", "response": "Increment a key by the given amount."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef acquire(self):\n    self.parent.__set__(self.instance, self.parent.state + 1)\n    try:\n      yield\n    finally:\n      self.parent.__set__(self.instance, self.parent.state - 1)", "response": "Acquire a new state from the current state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender a GET request by showing this nodes stats and children.", "response": "def render_GET(self, request):\n    \"\"\"Renders a GET request, by showing this nodes stats and children.\"\"\"\n    fullPath = request.path.split('/')\n    if not fullPath[-1]:\n      fullPath = fullPath[:-1]\n    parts = fullPath[2:]\n    statDict = util.lookup(scales.getStats(), parts)\n\n    if statDict is None:\n      request.setResponseCode(404)\n      return \"Path not found.\"\n\n    if 'query' in request.args:\n      query = request.args['query'][0]\n    else:\n      query = None\n\n    if 'format' in request.args and request.args['format'][0] == 'json':\n      request.headers['content-type'] = 'text/javascript; charset=UTF-8'\n      formats.jsonFormat(request, statDict, query)\n    elif 'format' in request.args and request.args['format'][0] == 'prettyjson':\n      request.headers['content-type'] = 'text/javascript; charset=UTF-8'\n      formats.jsonFormat(request, statDict, query, pretty=True)\n    else:\n      formats.htmlHeader(request, '/' + '/'.join(parts), self.serverName, query)\n      formats.htmlFormat(request, tuple(parts), statDict, query)\n\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender a GET request by showing this nodes stats and children.", "response": "def statsHandler(serverName, path=''):\n  \"\"\"Renders a GET request, by showing this nodes stats and children.\"\"\"\n  path = path.lstrip('/')\n  parts = path.split('/')\n  if not parts[0]:\n    parts = parts[1:]\n  statDict = util.lookup(scales.getStats(), parts)\n\n  if statDict is None:\n    abort(404, 'No stats found with path /%s' % '/'.join(parts))\n\n  output = StringIO()\n  outputFormat = request.args.get('format', 'html')\n  query = request.args.get('query', None)\n  if outputFormat == 'json':\n    formats.jsonFormat(output, statDict, query)\n  elif outputFormat == 'prettyjson':\n    formats.jsonFormat(output, statDict, query, pretty=True)\n  else:\n    formats.htmlHeader(output, '/' + path, serverName, query)\n    formats.htmlFormat(output, tuple(parts), statDict, query)\n\n  return output.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering the stats handler with a Flask app serving routes with a given prefix.", "response": "def registerStatsHandler(app, serverName, prefix='/status/'):\n  \"\"\"Register the stats handler with a Flask app, serving routes\n  with a given prefix. The prefix defaults to '/status/', which is\n  generally what you want.\"\"\"\n  if prefix[-1] != '/':\n    prefix += '/'\n  handler = functools.partial(statsHandler, serverName)\n  app.add_url_rule(prefix, 'statsHandler', handler, methods=['GET'])\n  app.add_url_rule(prefix + '<path:path>', 'statsHandler', handler, methods=['GET'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serveInBackground(port, serverName, prefix='/status/'):\n  import flask, threading\n  from wsgiref.simple_server import make_server\n  app = flask.Flask(__name__)\n  registerStatsHandler(app, serverName, prefix)\n  server = threading.Thread(target=make_server('', port, app).serve_forever)\n  server.daemon = True\n  server.start()\n  return server", "response": "Convenience function that spawn a background server thread that will\n serve HTTP requests to get the status. Returns the thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsort strings with numbers in a way that makes sense to humans", "response": "def _humanSortKey(s):\n  \"\"\"Sort strings with numbers in a way that makes sense to humans (e.g., 5 < 20)\"\"\"\n  if isinstance(s, str):\n    return [w.isdigit() and int(w) or w for w in re.split(r'(\\d+)', s)]\n  else:\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a clone of this aggregator.", "response": "def clone(self):\n    \"\"\"Creates a clone of this aggregator.\"\"\"\n    return type(self)(name = self.name, dataFormat = self._dataFormat)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addValue(self, _, value):\n    if value is not None:\n      try:\n        self._count += self._dataFormat.getCount(value)\n        self._total += self._dataFormat.getValue(value) * self._dataFormat.getCount(value)\n      except TypeError:\n        self._count += 1\n        self._total += value", "response": "Adds a value from the given source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a value from the given source.", "response": "def addValue(self, _, value):\n    \"\"\"Adds a value from the given source.\"\"\"\n    self.total += self._dataFormat.getValue(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addValue(self, source, data):\n    self.__result[self._dataFormat.getValue(data)].append(source)", "response": "Adds a value from the given source."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a clone of this aggregator.", "response": "def clone(self):\n    \"\"\"Creates a clone of this aggregator.\"\"\"\n    return type(self)(self.__cmp, self.__key, self.__reverse, name = self.name, dataFormat = self._dataFormat)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addValue(self, source, value):\n    if self.source is None or self.fn(self.value, value):\n      self.value = value\n      self.source = source", "response": "Adds a value from the given source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addSource(self, source, data):\n    self._aggregate(source, self._aggregators, data, self._result)", "response": "Adds the given source s stats to the result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd data from json files in the given directory.", "response": "def addJsonDirectory(self, directory, test=None):\n    \"\"\"Adds data from json files in the given directory.\"\"\"\n\n    for filename in os.listdir(directory):\n      try:\n        fullPath = os.path.join(directory, filename)\n        if not test or test(filename, fullPath):\n          with open(fullPath) as f:\n            jsonData = json.load(f)\n            name, _ = os.path.splitext(filename)\n            self.addSource(name, jsonData)\n\n      except ValueError:\n        continue"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming aggregation at a specific node in the data tree.", "response": "def _aggregate(self, source, aggregators, data, result):\n    \"\"\"Performs aggregation at a specific node in the data/aggregator tree.\"\"\"\n    if data is None:\n      return\n\n    if hasattr(aggregators, 'items'):\n      # Keep walking the tree.\n      for key, value in six.iteritems(aggregators):\n        if isinstance(key, tuple):\n          key, regex = key\n          for dataKey, dataValue in six.iteritems(data):\n            if regex.match(dataKey):\n              result.setdefault(key, {})\n              self._aggregate(source, value, dataValue, result[key])\n        else:\n          if key == '*':\n            for dataKey, dataValue in six.iteritems(data):\n              result.setdefault(dataKey, {})\n              self._aggregate(source, value, dataValue, result[dataKey])\n          elif key in data:\n            result.setdefault(key, {})\n            self._aggregate(source, value, data[key], result[key])\n\n    else:\n      # We found a leaf.\n      for aggregator in aggregators:\n        if aggregator.name not in result:\n          result[aggregator.name] = aggregator.clone()\n        result[aggregator.name].addValue(source, data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the internal state of the internal state of the internal state of the internal state.", "response": "def tick(self):\n    \"\"\"Updates meters\"\"\"\n    for m in self._meters:\n      m.tick()\n    self['m1'] = self._m1.rate\n    self['m5'] = self._m5.rate\n    self['m15'] = self._m15.rate"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mark(self, value=1):\n\n    self['count'] += value\n    for m in self._meters:\n      m.update(value)", "response": "Updates the dictionary with the given value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the sample mean.", "response": "def mean(self):\n    \"\"\"Return the sample mean.\"\"\"\n    if len(self) == 0:\n      return float('NaN')\n    arr = self.samples()\n    return sum(arr) / float(len(arr))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the sample standard deviation.", "response": "def stddev(self):\n    \"\"\"Return the sample standard deviation.\"\"\"\n    if len(self) < 2:\n      return float('NaN')\n    # The stupidest algorithm, but it works fine.\n    try:\n      arr = self.samples()\n      mean = sum(arr) / len(arr)\n      bigsum = 0.0\n      for x in arr:\n        bigsum += (x - mean)**2\n      return sqrt(bigsum / (len(arr) - 1))\n    except ZeroDivisionError:\n      return float('NaN')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of percentiles return a list of the values at those percentiles interpolating if necessary.", "response": "def percentiles(self, percentiles):\n    \"\"\"Given a list of percentiles (floats between 0 and 1), return a\n    list of the values at those percentiles, interpolating if\n    necessary.\"\"\"\n    try:\n      scores = [0.0]*len(percentiles)\n\n      if self.count > 0:\n        values = self.samples()\n        values.sort()\n\n        for i in range(len(percentiles)):\n          p = percentiles[i]\n          pos = p * (len(values) + 1)\n          if pos < 1:\n            scores[i] = values[0]\n          elif pos > len(values):\n            scores[i] = values[-1]\n          else:\n            upper, lower = values[int(pos - 1)], values[int(pos)]\n            scores[i] = lower + (pos - floor(pos)) * (upper - lower)\n\n      return scores\n    except IndexError:\n      return [float('NaN')] * len(percentiles)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an old value to the reservoir with a fixed timestamp.", "response": "def update(self, value):\n    \"\"\"\n      Adds an old value with a fixed timestamp to the reservoir.\n      @param value     the value to be added\n    \"\"\"\n    super(ExponentiallyDecayingReservoir, self).update(value)\n\n    timestamp = self.clock.time()\n\n    self.__rescaleIfNeeded()\n    priority = self.__weight(timestamp - self.startTime) / random.random()\n\n    self.count += 1\n    if (self.count <= self.size):\n      self.values[priority] = value\n    else:\n      first = min(self.values)\n\n      if first < priority and priority not in self.values:\n        self.values[priority] = value\n        while first not in self.values:\n          first = min(self.values)\n\n        del self.values[first]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a value to the sample.", "response": "def update(self, value):\n    \"\"\"Add a value to the sample.\"\"\"\n    super(UniformSample, self).update(value)\n\n    self.count += 1\n    c = self.count\n    if c < len(self.sample):\n      self.sample[c-1] = value\n    else:\n      r = random.randint(0, c)\n      if r < len(self.sample):\n        self.sample[r] = value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _forbidden(self, path, value):\n    if path[0] == '/':\n      path = path[1:]\n    for rule in reversed(self.rules):\n      if isinstance(rule[1], six.string_types):\n        if fnmatch(path, rule[1]):\n          return not rule[0]\n      elif rule[1](path, value):\n        return not rule[0]\n    return True", "response": "Returns True if the stat is forbidden by default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbeing a stat tree node pruned?", "response": "def _pruned(self, path):\n    \"\"\"Is a stat tree node pruned?  Goes through the list of prune rules\n    to find one that applies.  Chronologically newer rules are\n    higher-precedence than older ones. If no rule applies, the stat is\n    not pruned by default.\"\"\"\n    if path[0] == '/':\n      path = path[1:]\n    for rule in reversed(self.pruneRules):\n      if isinstance(rule, six.string_types):\n        if fnmatch(path, rule):\n          return True\n      elif rule(path):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npushes stat values out to Graphite.", "response": "def push(self, statsDict=None, prefix=None, path=None):\n    \"\"\"Push stat values out to Graphite.\"\"\"\n    if statsDict is None:\n      statsDict = scales.getStats()\n    prefix = prefix or self.prefix\n    path = path or '/'\n\n    for name, value in list(statsDict.items()):\n      name = str(name)\n      subpath = os.path.join(path, name)\n\n      if self._pruned(subpath):\n        continue\n\n      if hasattr(value, '__call__'):\n        try:\n          value = value()\n        except:                       # pylint: disable=W0702\n          value = None\n          log.exception('Error when calling stat function for graphite push')\n\n      if hasattr(value, 'items'):\n        self.push(value, '%s%s.' % (prefix, self._sanitize(name)), subpath)\n      elif self._forbidden(subpath, value):\n        continue\n\n      if six.PY3:\n        type_values = (int, float)\n      else:\n        type_values = (int, long, float)\n\n      if type(value) in type_values and len(name) < 500:\n        self.graphite.log(prefix + self._sanitize(name), value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _addRule(self, isWhitelist, rule):\n    if isinstance(rule, six.string_types) or hasattr(rule, '__call__'):\n      self.rules.append((isWhitelist, rule))\n    else:\n      raise TypeError('Graphite logging rules must be glob pattern or callable. Invalid: %r' % rule)", "response": "Add an entry to the rule list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n    self.graphite.start()\n    while True:\n      log.debug('Graphite pusher is sleeping for %d seconds', self.period)\n      time.sleep(self.period)\n      log.debug('Pushing stats to Graphite')\n      try:\n        self.push()\n        log.debug('Done pushing stats to Graphite')\n      except:\n        log.exception('Exception while pushing stats to Graphite')\n        raise", "response": "Loop forever pushing out stats to Graphite."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering a GET request by showing this nodes stats and children.", "response": "def get(self, path): # pylint: disable=W0221\n    \"\"\"Renders a GET request, by showing this nodes stats and children.\"\"\"\n    path = path or ''\n    path = path.lstrip('/')\n    parts = path.split('/')\n    if not parts[0]:\n      parts = parts[1:]\n    statDict = util.lookup(scales.getStats(), parts)\n\n    if statDict is None:\n      self.set_status(404)\n      self.finish('Path not found.')\n      return\n\n    outputFormat = self.get_argument('format', default='html')\n    query = self.get_argument('query', default=None)\n    if outputFormat == 'json':\n      formats.jsonFormat(self, statDict, query)\n    elif outputFormat == 'prettyjson':\n      formats.jsonFormat(self, statDict, query, pretty=True)\n    else:\n      formats.htmlHeader(self, '/' + path, self.serverName, query)\n      formats.htmlFormat(self, tuple(parts), statDict, query)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compose(*decorators):\n    def composed(f):\n        for decor in reversed(decorators):\n            f = decor(f)\n        return f\n    return composed", "response": "Helper to compose a sequence of tokens."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_ipython_extension(ip):\n    ip.register_magics(FortranMagics)\n\n    # enable fortran highlight\n    patch = (\"IPython.config.cell_magic_highlight['magic_fortran'] = \"\n             \"{'reg':[/^%%fortran/]};\")\n    js = display.Javascript(data=patch,\n                            lib=[\"https://raw.github.com/marijnh/CodeMirror/master/mode/\"\n                                 \"fortran/fortran.js\"])\n    display.display_javascript(js)", "response": "Load the extension in IPython."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef installStatsLoop(statsFile, statsDelay):\n\n  def dumpStats():\n    \"\"\"Actual stats dump function.\"\"\"\n    scales.dumpStatsTo(statsFile)\n    reactor.callLater(statsDelay, dumpStats)\n\n  def startStats():\n    \"\"\"Starts the stats dump in \"statsDelay\" seconds.\"\"\"\n    reactor.callLater(statsDelay, dumpStats)\n\n  reactor.callWhenRunning(startStats)", "response": "Installs an interval loop that dumps stats to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef runQuery(statDict, query):\n  parts = [x.strip() for x in OPERATOR.split(query)]\n  assert len(parts) in (1, 3)\n  queryKey = parts[0]\n\n  result = {}\n  for key, value in six.iteritems(statDict):\n    if key == queryKey:\n      if len(parts) == 3:\n        op = OPERATORS[parts[1]]\n        try:\n          queryValue = type(value)(parts[2]) if value else parts[2]\n        except (TypeError, ValueError):\n          continue\n        if not op(value, queryValue):\n          continue\n      result[key] = value\n    elif isinstance(value, scales.StatContainer) or isinstance(value, dict):\n      child = runQuery(value, query)\n      if child:\n        result[key] = child\n  return result", "response": "Filters for the given query."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite an HTML header.", "response": "def htmlHeader(output, path, serverName, query = None):\n  \"\"\"Writes an HTML header.\"\"\"\n  if path and path != '/':\n    output.write('<title>%s - Status: %s</title>' % (serverName, path))\n  else:\n    output.write('<title>%s - Status</title>' % serverName)\n  output.write('''\n<style>\nbody,td { font-family: monospace }\n.level div {\n  padding-bottom: 4px;\n}\n.level .level {\n  margin-left: 2em;\n  padding: 1px 0;\n}\nspan { color: #090; vertical-align: top }\n.key { color: black; font-weight: bold }\n.int, .float { color: #00c }\n</style>\n  ''')\n  output.write('<h1 style=\"margin: 0\">Stats</h1>')\n  output.write('<h3 style=\"margin: 3px 0 18px\">%s</h3>' % serverName)\n  output.write(\n      '<p><form action=\"#\" method=\"GET\">Filter: <input type=\"text\" name=\"query\" size=\"20\" value=\"%s\"></form></p>' %\n      (query or ''))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats as HTML writing to the given object.", "response": "def htmlFormat(output, pathParts = (), statDict = None, query = None):\n  \"\"\"Formats as HTML, writing to the given object.\"\"\"\n  statDict = statDict or scales.getStats()\n  if query:\n    statDict = runQuery(statDict, query)\n  _htmlRenderDict(pathParts, statDict, output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders a dictionary as a table - recursing as necessary.", "response": "def _htmlRenderDict(pathParts, statDict, output):\n  \"\"\"Render a dictionary as a table - recursing as necessary.\"\"\"\n  keys = list(statDict.keys())\n  keys.sort()\n\n  links = []\n\n  output.write('<div class=\"level\">')\n  for key in keys:\n    keyStr = cgi.escape(_utf8str(key))\n    value = statDict[key]\n    if hasattr(value, '__call__'):\n      value = value()\n    if hasattr(value, 'keys'):\n      valuePath = pathParts + (keyStr,)\n      if isinstance(value, scales.StatContainer) and value.isCollapsed():\n        link = '/status/' + '/'.join(valuePath)\n        links.append('<div class=\"key\"><a href=\"%s\">%s</a></div>' % (link, keyStr))\n      else:\n        output.write('<div class=\"key\">%s</div>' % keyStr)\n        _htmlRenderDict(valuePath, value, output)\n    else:\n      output.write('<div><span class=\"key\">%s</span> <span class=\"%s\">%s</span></div>' %\n                   (keyStr, type(value).__name__, cgi.escape(_utf8str(value)).replace('\\n', '<br/>')))\n\n  if links:\n    for link in links:\n      output.write(link)\n\n  output.write('</div>')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nliking str but returns UTF8.", "response": "def _utf8str(x):\n  \"\"\"Like str(x), but returns UTF8.\"\"\"\n  if six.PY3:\n    return str(x)\n  if isinstance(x, six.binary_type):\n    return x\n  elif isinstance(x, six.text_type):\n    return x.encode('utf-8')\n  else:\n    return six.binary_type(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats as JSON writing to the given object.", "response": "def jsonFormat(output, statDict = None, query = None, pretty = False):\n  \"\"\"Formats as JSON, writing to the given object.\"\"\"\n  statDict = statDict or scales.getStats()\n  if query:\n    statDict = runQuery(statDict, query)\n  indent = 2 if pretty else None\n  # At first, assume that strings are in UTF-8. If this fails -- if, for example, we have\n  # crazy binary data -- then in order to get *something* out, we assume ISO-8859-1,\n  # which maps each byte to a unicode code point.\n  try:\n    serialized = json.dumps(statDict, cls=scales.StatContainerEncoder, indent=indent)\n  except UnicodeDecodeError:\n    serialized = json.dumps(statDict, cls=scales.StatContainerEncoder, indent=indent, encoding='iso-8859-1')\n\n  output.write(serialized)\n  output.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrepeat a function in a background thread.", "response": "def RepeatTimer(interval, function, iterations=0, *args, **kwargs):\n  \"\"\"Repeating timer. Returns a thread id.\"\"\"\n\n  def __repeat_timer(interval, function, iterations, args, kwargs):\n    \"\"\"Inner function, run in background thread.\"\"\"\n    count = 0\n    while iterations <= 0 or count < iterations:\n      sleep(interval)\n      function(*args, **kwargs)\n      count += 1\n\n  return start_new_thread(__repeat_timer, (interval, function, iterations, args, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(context):\n\n    conf_vars = ['disqus_developer',\n                 'disqus_identifier',\n                 'disqus_url',\n                 'disqus_title',\n                 'disqus_category_id'\n                 ]\n\n    js = '\\tvar {} = \"{}\";'\n\n    output = [js.format(item, context[item]) for item in conf_vars \\\n              if item in context]\n\n    return '\\n'.join(output)", "response": "Return the formatted javascript for any disqus config variables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disqus_dev(context):\n\n    if settings.DEBUG:\n        disqus_url = '//{}{}'.format(\n            Site.objects.get_current().domain,\n            context['request'].path\n        )\n\n        return {'disqus_url': disqus_url}\n\n    return {}", "response": "Return the HTML and JS code to enable DISQUS comments on a local\n    development server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the HTML code to enable DISQUS SSO - so logged in users on the site can be logged in to disqus seemlessly.", "response": "def disqus_sso(context):\n    \"\"\"\n    Return the HTML/js code to enable DISQUS SSO - so logged in users on\n    your site can be logged in to disqus seemlessly.\n    \"\"\"\n\n    DISQUS_SECRET_KEY = getattr(settings, 'DISQUS_SECRET_KEY', None)\n    if DISQUS_SECRET_KEY is None:\n        return \"<p>You need to set DISQUS_SECRET_KEY before you can use SSO</p>\"\n\n    DISQUS_PUBLIC_KEY = getattr(settings, 'DISQUS_PUBLIC_KEY', None)\n    if DISQUS_PUBLIC_KEY is None:\n        return \"<p>You need to set DISQUS_PUBLIC_KEY before you can use SSO</p>\"\n\n    user = context['user']\n\n    if user.is_anonymous():\n        return \"\"\n\n    # create a JSON packet of our data attributes\n    data = json.dumps({\n        'id': user.id,\n        'username': user.username,\n        'email': user.email,\n    })\n\n    # encode the data to base64\n    message = base64.b64encode(data.encode('utf-8'))\n\n    # generate a timestamp for signing the message\n    timestamp = int(time.time())\n\n    key = DISQUS_SECRET_KEY.encode('utf-8')\n    msg = ('%s %s' % (message, timestamp)).encode('utf-8')\n    digestmod = hashlib.sha1\n\n    # generate our hmac signature\n    sig = hmac.HMAC(key, msg, digestmod).hexdigest()\n\n    return  dict(\n        message=message,\n        timestamp=timestamp,\n        sig=sig,\n        pub_key=DISQUS_PUBLIC_KEY,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the HTML code which transforms links that end with anonymized disqus_thread anchor into the threads comment count.", "response": "def disqus_num_replies(context, shortname=''):\n    \"\"\"\n    Return the HTML/js code which transforms links that end with an\n    #disqus_thread anchor into the threads comment count.\n    \"\"\"\n    shortname = getattr(settings, 'DISQUS_WEBSITE_SHORTNAME', shortname)\n\n    return {\n        'shortname': shortname,\n        'config': get_config(context),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disqus_recent_comments(context, shortname='', num_items=5, excerpt_length=200, hide_avatars=0, avatar_size=32):\n    shortname = getattr(settings, 'DISQUS_WEBSITE_SHORTNAME', shortname)\n\n    return {\n        'shortname': shortname,\n        'num_items': num_items,\n        'hide_avatars': hide_avatars,\n        'avatar_size': avatar_size,\n        'excerpt_length': excerpt_length,\n        'config': get_config(context),\n    }", "response": "Return the HTML code which shows recent comments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the HTML code to display DISQUS comments.", "response": "def disqus_show_comments(context, shortname=''):\n    \"\"\"\n    Return the HTML code to display DISQUS comments.\n    \"\"\"\n    shortname = getattr(settings, 'DISQUS_WEBSITE_SHORTNAME', shortname)\n\n    return {\n        'shortname': shortname,\n        'config': get_config(context),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_item(self, title, link, description, author_email=None,\n        author_name=None, author_link=None, pubdate=None, comments=None,\n        unique_id=None, enclosure=None, categories=(), item_copyright=None,\n        ttl=None, **kwargs):\n        \"\"\"\n        Adds an item to the feed. All args are expected to be Python Unicode\n        objects except pubdate, which is a datetime.datetime object, and\n        enclosure, which is an instance of the Enclosure class.\n        \"\"\"\n        to_unicode = lambda s: force_text(s, strings_only=True)\n        if categories:\n            categories = [to_unicode(c) for c in categories]\n        if ttl is not None:\n            # Force ints to unicode\n            ttl = force_text(ttl)\n        item = {\n            'title': to_unicode(title),\n            'link': iri_to_uri(link),\n            'description': to_unicode(description),\n            'author_email': to_unicode(author_email),\n            'author_name': to_unicode(author_name),\n            'author_link': iri_to_uri(author_link),\n            'pubdate': pubdate,\n            'comments': comments,\n            'unique_id': to_unicode(unique_id),\n            'enclosure': enclosure,\n            'categories': categories or (),\n            'item_copyright': to_unicode(item_copyright),\n            'ttl': ttl,\n        }\n        item.update(kwargs)\n        self.items.append(item)", "response": "Adds an item to the feed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call(method, data, post=False):\n    url = \"%s%s\" % ('http://disqus.com/api/', method)\n    if post:\n        # POST request\n        url += \"/\"\n        data = urlencode(data)\n    else:\n        # GET request\n        url += \"?%s\" % urlencode(data)\n        data = ''\n    res = json.load(urlopen(url, data))\n    if not res['succeeded']:\n        raise CommandError(\"'%s' failed: %s\\nData: %s\" % (method, res['code'], data))\n    return res['message']", "response": "Calls method from the DISQUS API with data either in POST or GET."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_comments_to_export(self, last_export_id=None):\n        qs = comments.get_model().objects.order_by('pk')\\\n                .filter(is_public=True, is_removed=False)\n        if last_export_id is not None:\n            print(\"Resuming after comment %s\" % str(last_export_id))\n            qs = qs.filter(id__gt=last_export_id)\n        return qs", "response": "Return comments which should be exported."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_last_state(self, state_file):\n        state = None\n        fp = open(state_file)\n        try:\n            state = int(fp.read())\n            print(\"Found previous state: %d\" % (state,))\n        finally:\n            fp.close()\n        return state", "response": "Checks the given path for the last exported comment s id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _save_state(self, state_file, last_pk):\n        fp = open(state_file, 'w+')\n        try:\n            fp.write(str(last_pk))\n        finally:\n            fp.close()", "response": "Saves the last_pk into the given state_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Request object that has the GET parameters attached to the url and the POST data attached to the object.", "response": "def _get_request(self, request_url, request_method, **params):\n        \"\"\"\n        Return a Request object that has the GET parameters\n        attached to the url or the POST data attached to the object.\n        \"\"\"\n        if request_method == 'GET':\n            if params:\n                request_url += '&%s' % urlencode(params)\n            request = Request(request_url)\n        elif request_method == 'POST':\n            request = Request(request_url, urlencode(params, doseq=1))\n        return request"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call(self, method, **params):\n        url = self.api_url % method\n        request = self._get_request(url, self.METHODS[method], **params)\n        try:\n            response = urlopen(request)\n        except URLError:\n            raise\n        else:\n            response_json = json.loads(response.read())\n            if not response_json['succeeded']:\n                raise DisqusException(response_json['message'])\n            return response_json['message']", "response": "Call the DISQUS API and return the json response."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(self, smtp=None, **kw):\n        smtp_options = {}\n        smtp_options.update(self.config.smtp_options)\n        if smtp:\n            smtp_options.update(smtp)\n        return super(Message, self).send(smtp=smtp_options, **kw)", "response": "Sends message.\n\n        :param smtp: When set, parameters from this dictionary overwrite\n                     options from config. See `emails.Message.send` for more information.\n\n        :param kwargs: Parameters for `emails.Message.send`\n\n        :return: Response objects from emails backend.\n                 For default `emails.backend.smtp.STMPBackend` returns an `emails.backend.smtp.SMTPResponse` object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads all EMAIL_ options and set default values.", "response": "def options(self):\n        \"\"\"\n        Reads all EMAIL_ options and set default values.\n        \"\"\"\n        config = self._config\n        o = {}\n        o.update(self._default_smtp_options)\n        o.update(self._default_message_options)\n        o.update(self._default_backend_options)\n        o.update(get_namespace(config, 'EMAIL_', valid_keys=o.keys()))\n        o['port'] = int(o['port'])\n        o['timeout'] = float(o['timeout'])\n        return o"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts config namespace to emails. backend. SMTPBackend namespace Returns dict for SMTPFactory", "response": "def smtp_options(self):\n        \"\"\"\n        Convert config namespace to emails.backend.SMTPBackend namespace\n        Returns dict for SMTPFactory\n        \"\"\"\n        o = {}\n        options = self.options\n        for key in self._default_smtp_options:\n            if key in options:\n                o[key] = options[key]\n\n        o['user'] = o.pop('host_user', None)\n        o['password'] = o.pop('host_password', None)\n        o['tls'] = o.pop('use_tls', False)\n        o['ssl'] = o.pop('use_ssl', False)\n        o['debug'] = o.pop('smtp_debug', 0)\n        for k in ('certfile', 'keyfile'):\n            v = o.pop('ssl_'+k, None)\n            if v:\n                o[k] = v\n        return o"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef message_options(self):\n        o = {}\n        options = self.options\n        for key in self._default_message_options:\n            if key in options:\n                o[key] = options[key]\n        return o", "response": "Convert config namespace to emails. Message namespace\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the Spark session and wait for it to be ready.", "response": "def start(self) -> None:\n        \"\"\"Create the remote Spark session and wait for it to be ready.\"\"\"\n\n        session = self.client.create_session(\n            self.kind,\n            self.proxy_user,\n            self.jars,\n            self.py_files,\n            self.files,\n            self.driver_memory,\n            self.driver_cores,\n            self.executor_memory,\n            self.executor_cores,\n            self.num_executors,\n            self.archives,\n            self.queue,\n            self.name,\n            self.spark_conf,\n        )\n        self.session_id = session.session_id\n\n        not_ready = {SessionState.NOT_STARTED, SessionState.STARTING}\n        intervals = polling_intervals([0.1, 0.2, 0.3, 0.5], 1.0)\n\n        while self.state in not_ready:\n            time.sleep(next(intervals))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nkill the managed Spark session.", "response": "def close(self) -> None:\n        \"\"\"Kill the managed Spark session.\"\"\"\n        if self.session_id is not None:\n            self.client.delete_session(self.session_id)\n        self.client.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, code: str) -> Output:\n        output = self._execute(code)\n        if self.echo and output.text:\n            print(output.text)\n        if self.check:\n            output.raise_for_status()\n        return output", "response": "Run some code in the managed Spark session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, dataframe_name: str) -> pandas.DataFrame:\n        code = serialise_dataframe_code(dataframe_name, self.kind)\n        output = self._execute(code)\n        output.raise_for_status()\n        if output.text is None:\n            raise RuntimeError(\"statement had no text output\")\n        return deserialise_dataframe(output.text)", "response": "Evaluate and retrieve a Spark dataframe in the managed session."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_sql(self, code: str) -> pandas.DataFrame:\n        if self.kind != SessionKind.SQL:\n            raise ValueError(\"not a SQL session\")\n        output = self._execute(code)\n        output.raise_for_status()\n        if output.json is None:\n            raise RuntimeError(\"statement had no JSON output\")\n        return dataframe_from_json_output(output.json)", "response": "Evaluate a Spark SQL satatement and retrieve the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the version of Livy running on the server.", "response": "def server_version(self) -> Version:\n        \"\"\"Get the version of Livy running on the server.\"\"\"\n        if self._server_version_cache is None:\n            data = self._client.get(\"/version\")\n            self._server_version_cache = Version(data[\"version\"])\n        return self._server_version_cache"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist all the active sessions in Livy.", "response": "def list_sessions(self) -> List[Session]:\n        \"\"\"List all the active sessions in Livy.\"\"\"\n        data = self._client.get(\"/sessions\")\n        return [Session.from_json(item) for item in data[\"sessions\"]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new session in Livy.", "response": "def create_session(\n        self,\n        kind: SessionKind,\n        proxy_user: str = None,\n        jars: List[str] = None,\n        py_files: List[str] = None,\n        files: List[str] = None,\n        driver_memory: str = None,\n        driver_cores: int = None,\n        executor_memory: str = None,\n        executor_cores: int = None,\n        num_executors: int = None,\n        archives: List[str] = None,\n        queue: str = None,\n        name: str = None,\n        spark_conf: Dict[str, Any] = None,\n    ) -> Session:\n        \"\"\"Create a new session in Livy.\n\n        The py_files, files, jars and archives arguments are lists of URLs,\n        e.g. [\"s3://bucket/object\", \"hdfs://path/to/file\", ...] and must be\n        reachable by the Spark driver process.  If the provided URL has no\n        scheme, it's considered to be relative to the default file system\n        configured in the Livy server.\n\n        URLs in the py_files argument are copied to a temporary staging area\n        and inserted into Python's sys.path ahead of the standard library\n        paths.  This allows you to import .py, .zip and .egg files in Python.\n\n        URLs for jars, py_files, files and archives arguments are all copied\n        to the same working directory on the Spark cluster.\n\n        The driver_memory and executor_memory arguments have the same format\n        as JVM memory strings with a size unit suffix (\"k\", \"m\", \"g\" or \"t\")\n        (e.g. 512m, 2g).\n\n        See https://spark.apache.org/docs/latest/configuration.html for more\n        information on Spark configuration properties.\n\n        :param kind: The kind of session to create.\n        :param proxy_user: User to impersonate when starting the session.\n        :param jars: URLs of jars to be used in this session.\n        :param py_files: URLs of Python files to be used in this session.\n        :param files: URLs of files to be used in this session.\n        :param driver_memory: Amount of memory to use for the driver process\n            (e.g. '512m').\n        :param driver_cores: Number of cores to use for the driver process.\n        :param executor_memory: Amount of memory to use per executor process\n            (e.g. '512m').\n        :param executor_cores: Number of cores to use for each executor.\n        :param num_executors: Number of executors to launch for this session.\n        :param archives: URLs of archives to be used in this session.\n        :param queue: The name of the YARN queue to which submitted.\n        :param name: The name of this session.\n        :param spark_conf: Spark configuration properties.\n        \"\"\"\n        if self.legacy_server():\n            valid_kinds = VALID_LEGACY_SESSION_KINDS\n        else:\n            valid_kinds = VALID_SESSION_KINDS\n\n        if kind not in valid_kinds:\n            raise ValueError(\n                f\"{kind} is not a valid session kind for a Livy server of \"\n                f\"this version (should be one of {valid_kinds})\"\n            )\n\n        body = {\"kind\": kind.value}\n        if proxy_user is not None:\n            body[\"proxyUser\"] = proxy_user\n        if jars is not None:\n            body[\"jars\"] = jars\n        if py_files is not None:\n            body[\"pyFiles\"] = py_files\n        if files is not None:\n            body[\"files\"] = files\n        if driver_memory is not None:\n            body[\"driverMemory\"] = driver_memory\n        if driver_cores is not None:\n            body[\"driverCores\"] = driver_cores\n        if executor_memory is not None:\n            body[\"executorMemory\"] = executor_memory\n        if executor_cores is not None:\n            body[\"executorCores\"] = executor_cores\n        if num_executors is not None:\n            body[\"numExecutors\"] = num_executors\n        if archives is not None:\n            body[\"archives\"] = archives\n        if queue is not None:\n            body[\"queue\"] = queue\n        if name is not None:\n            body[\"name\"] = name\n        if spark_conf is not None:\n            body[\"conf\"] = spark_conf\n\n        data = self._client.post(\"/sessions\", data=body)\n        return Session.from_json(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting information about a session.", "response": "def get_session(self, session_id: int) -> Optional[Session]:\n        \"\"\"Get information about a session.\n\n        :param session_id: The ID of the session.\n        \"\"\"\n        try:\n            data = self._client.get(f\"/sessions/{session_id}\")\n        except requests.HTTPError as e:\n            if e.response.status_code == 404:\n                return None\n            else:\n                raise\n        return Session.from_json(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all the statements in a session.", "response": "def list_statements(self, session_id: int) -> List[Statement]:\n        \"\"\"Get all the statements in a session.\n\n        :param session_id: The ID of the session.\n        \"\"\"\n        response = self._client.get(f\"/sessions/{session_id}/statements\")\n        return [\n            Statement.from_json(session_id, data)\n            for data in response[\"statements\"]\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_statement(\n        self, session_id: int, code: str, kind: StatementKind = None\n    ) -> Statement:\n        \"\"\"Run a statement in a session.\n\n        :param session_id: The ID of the session.\n        :param code: The code to execute.\n        :param kind: The kind of code to execute.\n        \"\"\"\n\n        data = {\"code\": code}\n\n        if kind is not None:\n            if self.legacy_server():\n                LOGGER.warning(\"statement kind ignored on Livy<0.5.0\")\n            data[\"kind\"] = kind.value\n\n        response = self._client.post(\n            f\"/sessions/{session_id}/statements\", data=data\n        )\n        return Statement.from_json(session_id, response)", "response": "Run a statement in a session."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets information about a statement in a session.", "response": "def get_statement(self, session_id: int, statement_id: int) -> Statement:\n        \"\"\"Get information about a statement in a session.\n\n        :param session_id: The ID of the session.\n        :param statement_id: The ID of the statement.\n        \"\"\"\n        response = self._client.get(\n            f\"/sessions/{session_id}/statements/{statement_id}\"\n        )\n        return Statement.from_json(session_id, response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lattice(lattice, filename, directory, render, view, **kwargs):\n    dot = graphviz.Digraph(\n        name=lattice.__class__.__name__,\n        comment=repr(lattice),\n        filename=filename,\n        directory=directory,\n        node_attr=dict(shape='circle', width='.25', style='filled', label=''),\n        edge_attr=dict(dir='none', labeldistance='1.5', minlen='2'),\n        **kwargs\n    )\n\n    sortkey = SORTKEYS[0]\n\n    node_name = NAME_GETTERS[0]\n\n    for concept in lattice._concepts:\n        name = node_name(concept)\n        dot.node(name)\n\n        if concept.objects:\n            dot.edge(name, name,\n                headlabel=' '.join(concept.objects),\n                labelangle='270', color='transparent')\n\n        if concept.properties:\n            dot.edge(name, name,\n                taillabel=' '.join(concept.properties),\n                labelangle='90', color='transparent')\n\n        dot.edges((name, node_name(c))\n            for c in sorted(concept.lower_neighbors, key=sortkey))\n\n    if render or view:\n        dot.render(view=view)  # pragma: no cover\n    return dot", "response": "Return a graphviz source for visualizing the lattice."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(cls, filename, encoding):\n        if encoding is None:\n            encoding = cls.encoding\n\n        with io.open(filename, 'r', encoding=encoding) as fd:\n            source = fd.read()\n\n        if cls.normalize_newlines:\n            source = source.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n        return cls.loads(source)", "response": "Load and parse serialized objects properties bools from file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting serialized objects properties bools to file.", "response": "def dump(cls, filename, objects, properties, bools, encoding):\n        \"\"\"Write serialized objects, properties, bools to file.\"\"\"\n        if encoding is None:\n            encoding = cls.encoding\n\n        source = cls.dumps(objects, properties, bools)\n        if PY2:\n            source = unicode(source)\n\n        with io.open(filename, 'w', encoding=encoding) as fd:\n            fd.write(source)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_csv(filename, dialect='excel', encoding='utf-8'):\n    return Context.fromfile(filename, 'csv', encoding, dialect=dialect)", "response": "Load and return formal context from CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield all pairs where the two objects disagree.", "response": "def conflicting_pairs(left, right):\n    \"\"\"Yield all ``(object, property)`` pairs where the two definitions disagree.\"\"\"\n    objects = left._objects & right._objects\n    properties = left._properties & right._properties\n    difference = left._pairs ^ right._pairs\n    for o in objects:\n        for p in properties:\n            if (o, p) in difference:\n                yield (o, p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nraises an informative ValueError if the two definitions disagree.", "response": "def ensure_compatible(left, right):\n    \"\"\"Raise an informative ``ValueError`` if the two definitions disagree.\"\"\"\n    conflicts = list(conflicting_pairs(left, right))\n    if conflicts:\n        raise ValueError('conflicting values for object/property pairs: %r' % conflicts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing the name of an object by a new one.", "response": "def rename_object(self, old, new):\n        \"\"\"Replace the name of an object by a new one.\"\"\"\n        self._objects.replace(old, new)\n        pairs = self._pairs\n        pairs |= {(new, p) for p in self._properties\n            if (old, p) in pairs and not pairs.remove((old, p))}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rename_property(self, old, new):\n        self._properties.replace(old, new)\n        pairs = self._pairs\n        pairs |= {(o, new) for o in self._objects\n            if (o, old) in pairs and not pairs.remove((o, old))}", "response": "Replace the name of a property by a new one."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an object to the definition and add properties as related.", "response": "def add_object(self, obj, properties=()):\n        \"\"\"Add an object to the definition and add ``properties`` as related.\"\"\"\n        self._objects.add(obj)\n        self._properties |= properties\n        self._pairs.update((obj, p) for p in properties)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a property to the definition and add objects as related.", "response": "def add_property(self, prop, objects=()):\n        \"\"\"Add a property to the definition and add ``objects`` as related.\"\"\"\n        self._properties.add(prop)\n        self._objects |= objects\n        self._pairs.update((o, prop) for o in objects)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an object from the definition.", "response": "def remove_object(self, obj):\n        \"\"\"Remove an object from the definition.\"\"\"\n        self._objects.remove(obj)\n        self._pairs.difference_update((obj, p) for p in self._properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves a property from the definition.", "response": "def remove_property(self, prop):\n        \"\"\"Remove a property from the definition.\"\"\"\n        self._properties.remove(prop)\n        self._pairs.difference_update((o, prop) for o in self._objects)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_object(self, obj, properties):\n        self._objects.add(obj)\n        properties = set(properties)\n        self._properties |= properties\n        pairs = self._pairs\n        for p in self._properties:\n            if p in properties:\n                pairs.add((obj, p))\n            else:\n                pairs.discard((obj, p))", "response": "Add an object to the definition and set its properties."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_property(self, prop, objects):\n        self._properties.add(prop)\n        objects = set(objects)\n        self._objects |= objects\n        pairs = self._pairs\n        for o in self._objects:\n            if o in objects:\n                pairs.add((o, prop))\n            else:\n                pairs.discard((o, prop))", "response": "Add a property to the definition and set its objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the definition with the union of the other.", "response": "def union_update(self, other, ignore_conflicts=False):\n        \"\"\"Update the definition with the union of the ``other``.\"\"\"\n        if not ignore_conflicts:\n            ensure_compatible(self, other)\n        self._objects |= other._objects\n        self._properties |= other._properties\n        self._pairs |= other._pairs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new definition from the union of the definitions.", "response": "def union(self, other, ignore_conflicts=False):\n        \"\"\"Return a new definition from the union of the definitions.\"\"\"\n        result = self.copy()\n        result.union_update(other, ignore_conflicts)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef intersection(self, other, ignore_conflicts=False):\n        result = self.copy()\n        result.intersection_update(other, ignore_conflicts)\n        return result", "response": "Return a new definition from the intersection of the definitions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the len of the longest item in iterable or minimum.", "response": "def max_len(iterable, minimum=0):\n    \"\"\"Return the len() of the longest item in ``iterable`` or ``minimum``.\n\n    >>> max_len(['spam', 'ham'])\n    4\n\n    >>> max_len([])\n    0\n\n    >>> max_len(['ham'], 4)\n    4\n    \"\"\"\n    try:\n        result = max(map(len, iterable))\n    except ValueError:\n        result = minimum\n    return minimum if result < minimum else result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maximal(iterable, comparison=operator.lt, _groupkey=operator.itemgetter(0)):\n    iterable = set(iterable)\n    if len(iterable) < 2:\n        return iterable\n    return (item for item, pairs\n        in groupby(permutations(iterable, 2), key=_groupkey)\n        if not any(starmap(comparison, pairs)))", "response": "Yields the unique maximal elements from iterable using comparison."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing an item preserving order.", "response": "def replace(self, item, new_item):\n        \"\"\"Replace an item preserving order.\n\n        >>> u = Unique([0, 1, 2])\n        >>> u.replace(1, 'spam')\n        >>> u\n        Unique([0, 'spam', 2])\n\n        >>> u.replace('eggs', 1)\n        Traceback (most recent call last):\n            ...\n        ValueError: 'eggs' is not in list\n\n        >>> u.replace('spam', 0)\n        Traceback (most recent call last):\n            ...\n        ValueError: 0 already in list\n        \"\"\"\n        if new_item in self._seen:\n            raise ValueError('%r already in list' % new_item)\n        idx = self._items.index(item)\n        self._seen.remove(item)\n        self._seen.add(new_item)\n        self._items[idx] = new_item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move(self, item, new_index):\n        idx = self._items.index(item)\n        if idx != new_index:\n            item = self._items.pop(idx)\n            self._items.insert(new_index, item)", "response": "Move an item to the given position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning whether this collection contains all items.", "response": "def issuperset(self, items):\n        \"\"\"Return whether this collection contains all items.\n\n        >>> Unique(['spam', 'eggs']).issuperset(['spam', 'spam', 'spam'])\n        True\n        \"\"\"\n        return all(_compat.map(self._seen.__contains__, items))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn order preserving unique items not in this collection.", "response": "def rsub(self, items):\n        \"\"\"Return order preserving unique items not in this collection.\n\n        >>> Unique(['spam']).rsub(['ham', 'spam', 'eggs'])\n        Unique(['ham', 'eggs'])\n        \"\"\"\n        ignore = self._seen\n        seen = set()\n        add = seen.add\n        items = [i for i in items\n                 if i not in ignore and i not in seen and not add(i)]\n        return self._fromargs(seen, items)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning min of val1 or None only if both values are None", "response": "def min_or_none(val1, val2):\n    \"\"\"Returns min(val1, val2) returning None only if both values are None\"\"\"\n    return min(val1, val2, key=lambda x: sys.maxint if x is None else x)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn max of val1 and val2 returning None only if both values are None", "response": "def max_or_none(val1, val2):\n    \"\"\"Returns max(val1, val2) returning None only if both values are None\"\"\"\n    return max(val1, val2, key=lambda x: -sys.maxint if x is None else x)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the result of merging the two given schemas.", "response": "def merge_schema(first, second):\n    \"\"\"Returns the result of merging the two given schemas.\n    \"\"\"\n    if not (type(first) == type(second) == dict):\n        raise ValueError(\"Argument is not a schema\")\n\n    if not (first.get('type') == second.get('type') == 'object'):\n        raise NotImplementedError(\"Unsupported root type\")\n\n    return merge_objects(first, second)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates schemas for the given samples and merges them into a single base base.", "response": "def generate_and_merge_schemas(samples):\n    \"\"\"Iterates through the given samples, generating schemas\n    and merging them, returning the resulting merged schema.\n\n    \"\"\"\n    merged = generate_schema_for_sample(next(iter(samples)))\n\n    for sample in samples:\n        merged = merge_schema(merged, generate_schema_for_sample(sample))\n\n    return merged"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mtspec(data, delta, time_bandwidth, nfft=None, number_of_tapers=None,\n           quadratic=False, adaptive=True, verbose=False,\n           optional_output=False, statistics=False, rshape=False,\n           fcrit=False):\n    \"\"\"\n    Wrapper method for the mtspec subroutine in the library by German A.\n    Prieto.\n\n    This method estimates the adaptive weighted multitaper spectrum, as in\n    Thomson 1982.  This is done by estimating the DPSS (discrete prolate\n    spheroidal sequences), multiplying each of the tapers with the data series,\n    take the FFT, and using the adaptive scheme for a better estimation. It\n    outputs the power spectral density (PSD).\n\n    :param data: :class:`numpy.ndarray`\n         Array with the data.\n    :param delta: float\n         Sample spacing of the data.\n    :param time_bandwidth: float\n         Time-bandwidth product. Common values are 2, 3, 4 and numbers in\n         between.\n    :param nfft: int\n         Number of points for fft. If nfft == None, no zero padding\n         will be applied before the fft\n    :param number_of_tapers: integer, optional\n         Number of tapers to use. Defaults to int(2*time_bandwidth) - 1. This\n         is maximum senseful amount. More tapers will have no great influence\n         on the final spectrum but increase the calculation time. Use fewer\n         tapers for a faster calculation.\n    :param quadratic: bool, optional\n         Whether or not to caluclate a quadratic multitaper. Will only work\n         if nfft is False or equal to the sample count. The nfft parameter\n         will overwrite the quadratic paramter. Defaults to False.\n    :param adaptive: bool, optional\n         Whether to use adaptive or constant weighting of the eigenspectra.\n         Defaults to True(adaptive).\n    :param verbose: bool, optional\n         Passed to the fortran library. Defaults to False.\n    :param optional_output: bool, optional\n         Calculates and returns additional output parameters. See the notes in\n         the docstring for further details.\n    :param statistics: bool, optional\n         Calculates and returns statistics. See the notes in the docstring for\n         further details.\n    :param rshape: integer/None, optional\n         Determines whether or not to perform the F-test for lines. If rshape\n         is 1 or 2, then don't put the lines back. If rshape is 2 only check\n         around 60 Hz. See the fortran source code for more informations.\n         Defaults to None (do not perform the F-test).\n    :param fcrit: float/None, optional\n         The threshold probability for the F-test. If none is given, the mtspec\n         library calculates a default value. See the fortran source code for\n         details. Defaults to None.\n    :return: Returns a list with :class:`numpy.ndarray`. See the note\n         below.\n\n    .. note::\n\n        This method will at return at least two arrays: The calculated spectrum\n        and the corresponding frequencies.  If optional_output is true it will\n        also return (in the given order) (multidimensional) arrays containing\n        the eigenspectra, the corresponding eigencoefficients and an array\n        containing the weights for each eigenspectra normalized so that the sum\n        of squares over the eigenspectra is one.  If statistics is True is will\n        also return (in the given order) (multidimensional) arrays containing\n        the jackknife 5% and 95% confidence intervals, the F statistics for\n        single line and the number of degrees of freedom for each frequency\n        bin.  If both optional_output and statistics are true, the\n        optional_outputs will be returned before the statistics.\n    \"\"\"\n    npts = len(data)\n\n    # Depending if nfft is specified or not initialte MtspecTytpe\n    # for mtspec_pad_ or mtspec_d_\n    if nfft is None or nfft == npts:\n        nfft = npts\n        mt = _MtspecType(\"float64\")  # mtspec_d_\n    else:\n        mt = _MtspecType(\"float32\")  # mtspec_pad_\n        quadratic = False\n    # Use the optimal number of tapers in case no number is specified.\n    if number_of_tapers is None:\n        number_of_tapers = int(2 * time_bandwidth) - 1\n    # Transform the data to work with the library.\n    data = np.require(data, dtype=mt.float, requirements=[mt.order])\n    # Get some information necessary for the call to the Fortran library.\n    number_of_frequency_bins = int(nfft / 2) + 1\n    # Create output arrays.\n    spectrum = mt.empty(number_of_frequency_bins)\n    frequency_bins = mt.empty(number_of_frequency_bins)\n    # Create optional outputs.\n    if optional_output is True:\n        eigenspectra = mt.empty((number_of_frequency_bins, number_of_tapers))\n        eigencoefficients = mt.empty((nfft, number_of_tapers), complex=True)\n        weights = mt.empty((number_of_frequency_bins, number_of_tapers))\n    else:\n        eigenspectra = eigencoefficients = weights = None\n    # Create statistics.\n    if statistics is True:\n        jackknife_interval = mt.empty((number_of_frequency_bins, 2))\n        f_statistics = mt.empty(number_of_frequency_bins)\n        degrees_of_freedom = mt.empty(number_of_frequency_bins)\n    else:\n        jackknife_interval = f_statistics = degrees_of_freedom = None\n    # Verbose mode on or off.\n    if verbose is True:\n        verbose = C.byref(C.c_char('y'))\n    else:\n        verbose = None\n    # Determine whether or not to compute the quadratic multitaper.\n    if quadratic is True:\n        quadratic = C.byref(C.c_int(1))\n    else:\n        quadratic = None\n    # Determine whether to use adaptive or constant weighting of the\n    # eigenspectra.\n    if adaptive is True:\n        adaptive = None\n    else:\n        adaptive = C.byref(C.c_int(1))\n    # Determines whether or not to perform the F-test for lines. If rshape is 1\n    # or 2, then don't put the lines back. If rshape is 2 only check around 60\n    # Hz. See the fortran source code for more informations.\n    if type(rshape) == int:\n        rshape = C.byref(C.c_int(rshape))\n    else:\n        rshape = None\n    # The threshold probability for the F-test. If none is given, the mtspec\n    # library calculates a default value. See the fortran source code for\n    # details.\n    if type(fcrit) == float:\n        fcrit = C.byref(C.c_float(fcrit))\n    else:\n        fcrit = None\n    # Call the library. Fortran passes pointers!\n    args = [C.byref(C.c_int(npts)), C.byref(C.c_int(nfft)),\n            C.byref(mt.c_float(delta)), mt.p(data),\n            C.byref(mt.c_float(time_bandwidth)),\n            C.byref(C.c_int(number_of_tapers)),\n            C.byref(C.c_int(number_of_frequency_bins)), mt.p(frequency_bins),\n            mt.p(spectrum), verbose, quadratic, adaptive,\n            mt.p(eigencoefficients), mt.p(weights),\n            mt.p(jackknife_interval), mt.p(degrees_of_freedom),\n            mt.p(eigenspectra), rshape, mt.p(f_statistics), fcrit, None]\n    # diffrent arguments, depending on mtspec_pad_ or mtspec_d_, adapt\n    if npts == nfft:\n        args.pop(1)\n\n    # finally call the shared library function\n    mt.mtspec(*args)\n\n    # Figure out what to return. See the docstring of this method for details.\n    return_values = [spectrum, frequency_bins]\n    if optional_output is True:\n        return_values.extend([eigenspectra, eigencoefficients, weights])\n    if statistics is True:\n        return_values.extend([jackknife_interval, f_statistics,\n                              degrees_of_freedom])\n    return return_values", "response": "This method is used to estimate the adaptive weighted multitaper spectrum of the data series."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sine_psd(data, delta, number_of_tapers=None, number_of_iterations=2,\n             degree_of_smoothing=1.0, statistics=False, verbose=False):\n    \"\"\"\n    Wrapper method for the sine_psd subroutine in the library by German A.\n    Prieto.\n\n    The subroutine is in charge of estimating the adaptive sine multitaper as\n    in Riedel and Sidorenko (1995). It outputs the power spectral density\n    (PSD).\n\n    This is done by performing a MSE adaptive estimation. First a pilot\n    spectral estimate is used, and S\" is estimated, in order to get te number\n    of tapers to use, using (13) of Riedel and Sidorenko for a min square error\n    spectrum.\n    Unlike the prolate spheroidal multitapers, the sine multitaper adaptive\n    process introduces a variable resolution and error in the frequency domain.\n    Complete error information is contained in the output variables as the\n    corridor of 1-standard-deviation errors, and in the number of tapers used\n    at each frequency.  The errors are estimated in the simplest way, from the\n    number of degrees of freedom (two per taper), not by jack-knifing. The\n    frequency resolution is found from K*fN/Nf where fN is the Nyquist\n    frequency and Nf is the number of frequencies estimated.  The adaptive\n    process used is as follows. A quadratic fit to the log PSD within an\n    adaptively determined frequency band is used to find an estimate of the\n    local second derivative of the spectrum. This is used in an equation like R\n    & S (13) for the MSE taper number, with the difference that a parabolic\n    weighting is applied with increasing taper order. Because the FFTs of the\n    tapered series can be found by resampling the FFT of the original time\n    series (doubled in length and padded with zeros) only one FFT is required\n    per series, no matter how many tapers are used. This makes the program\n    fast. Compared with the Thomson multitaper programs, this code is not only\n    fast but simple and short. The spectra associated with the sine tapers are\n    weighted before averaging with a parabolically varying weight. The\n    expression for the optimal number of tapers given by R & S must be modified\n    since it gives an unbounded result near points where S\" vanishes, which\n    happens at many points in most spectra. This program restricts the rate of\n    growth of the number of tapers so that a neighboring covering interval\n    estimate is never completely contained in the next such interval.\n\n    This method SHOULD not be used for sharp cutoffs or deep valleys, or small\n    sample sizes. Instead use Thomson multitaper in mtspec in this same\n    library.\n\n    :param data: :class:`numpy.ndarray`\n        Array with the data.\n    :param delta: float\n        Sample spacing of the data.\n    :param number_of_tapers: integer/None, optional\n        Number of tapers to use. If none is given, the library will perform an\n        adaptive taper estimation with a varying number of tapers for each\n        frequency. Defaults to None.\n    :param number_of_iterations: integer, optional\n        Number of iterations to perform. Values less than 2 will be set to 2.\n        Defaults to 2.\n    :param degree_of_smoothing: float, optional\n        Degree of smoothing. Defaults to 1.0.\n    :param statistics: bool, optional\n        Calculates and returns statistics. See the notes in the docstring for\n        further details.\n    :param verbose: bool, optional\n        Passed to the fortran library. Defaults to False.\n    :return: Returns a list with :class:`numpy.ndarray`. See the note below\n        for details.\n\n    .. note::\n\n        This method will at return at least two arrays: The calculated\n        spectrum and the corresponding frequencies.  If statistics is True\n        is will also return (in the given order) (multidimensional) arrays\n        containing the 1-std errors (a simple dof estimate) and the number\n        of tapers used for each frequency point.\n    \"\"\"\n    # Verbose mode on or off.\n    if verbose is True:\n        verbose = C.byref(C.c_char('y'))\n    else:\n        verbose = None\n    # Set the number of tapers so it can be read by the library.\n    if number_of_tapers is None:\n        number_of_tapers = 0\n    # initialize _MtspecType to save some space\n    mt = _MtspecType(\"float32\")\n    # Transform the data to work with the library.\n    data = np.require(data, dtype=mt.float, requirements=[mt.order])\n    # Some variables necessary to call the library.\n    npts = len(data)\n    number_of_frequency_bins = int(npts / 2) + 1\n    # Create output arrays.\n    frequency_bins = mt.empty(number_of_frequency_bins)\n    spectrum = mt.empty(number_of_frequency_bins)\n    # Create optional arrays or set to None.\n    if statistics is True:\n        # here an exception, mt sets the type float32, here we need int32\n        # that is do all the type and POINTER definition once by hand\n        tapers_per_freq_point = np.empty(number_of_frequency_bins,\n                                         dtype='int32', order=mt.order)\n        tapers_per_freq_point_p = \\\n            tapers_per_freq_point.ctypes.data_as(C.POINTER(C.c_int))\n        errors = mt.empty((number_of_frequency_bins, 2))\n    else:\n        tapers_per_freq_point_p = errors = None\n    # Call the library. Fortran passes pointers!\n    mtspeclib.sine_psd_(\n        C.byref(C.c_int(npts)),\n        C.byref(C.c_float(delta)), mt.p(data),\n        C.byref(C.c_int(number_of_tapers)),\n        C.byref(C.c_int(number_of_iterations)),\n        C.byref(C.c_float(degree_of_smoothing)),\n        C.byref(C.c_int(number_of_frequency_bins)),\n        mt.p(frequency_bins), mt.p(spectrum),\n        tapers_per_freq_point_p, mt.p(errors), verbose)\n    # Calculate return values.\n    return_values = [spectrum, frequency_bins]\n    if statistics is True:\n        return_values.extend([errors, tapers_per_freq_point])\n    return return_values", "response": "This method is used to compute the power spectral density of a time series using the MSE adaptive estimation algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the DPSS of a series of time - bandwidth values.", "response": "def dpss(npts, fw, number_of_tapers, auto_spline=True, npts_max=None):\n    \"\"\"\n    Calculates DPSS also known as Slepian sequences or Slepian tapers.\n\n    Calculation of the DPSS (Discrete Prolate Spheroidal Sequences) and the\n    correspondent eigenvalues. The (1 - eigenvalue) terms are also calculated.\n\n    Wraps the ``dpss()`` subroutine from the Fortran library.\n\n    By default this routine will use spline interpolation if sequences with\n    more than 200.000 samples are requested.\n\n    .. note::\n\n        The tapers are the eigenvectors of the tridiagonal matrix sigma(i, j)\n        [see Slepian(1978) eq 14 and 25]. They are also the eigenvectors of\n        the Toeplitz matrix, eq. 18.\n\n    :param npts: The number of points in the series.\n    :type npts: int\n    :param fw: The time-bandwidth product (number of Rayleigh bins).\n    :type fw: float\n    :param number_of_tapers: The desired number of tapers.\n    :type number_of_tapers: int\n    :param auto_spline: Whether or not to automatically use spline\n        interpolation for ``npts`` > 200000.\n    :type auto_spline: bool\n    :param npts_max: The number of actual points to calculate the DPSS. If this\n        number is smaller than ``npts``, spline interpolation will be\n        performed, regardless of the value of ``auto_spline``.\n    :type npts_max: None or int\n    :returns: ``(v, lambda, theta)`` with\n        ``v(npts, number_of_tapers)`` the eigenvectors (tapers), ``lambda`` the\n        eigenvalues of the ``v``'s and ``theta`` the 1 - ``lambda``\n        (energy outside the bandwidth) values.\n\n    .. rubric:: Example\n\n    This example demonstrates how to calculate and plot the first five DPSS'.\n\n    >>> import matplotlib.pyplot as plt\n    >>> from mtspec import dpss\n    >>> tapers, lamb, theta = dpss(512, 2.5, 5)\n    >>> for i in range(5):\n    ...     plt.plot(tapers[:, i])\n\n    .. plot ::\n\n        # Same as the code snippet in the docstring, just a bit prettier.\n        import matplotlib.pyplot as plt\n        plt.style.use(\"ggplot\")\n        from mtspec import dpss\n        tapers, lamb, theta = dpss(512, 2.5, 5)\n        for i in range(5):\n            plt.plot(tapers[:, i])\n        plt.xlim(0, 512)\n        plt.ylim(-0.09, 0.09)\n        plt.tight_layout()\n    \"\"\"\n    mt = _MtspecType(\"float64\")\n\n    v = mt.empty((npts, number_of_tapers))\n    lamb = mt.empty(number_of_tapers)\n    theta = mt.empty(number_of_tapers)\n\n    # Set auto_spline to True.\n    if npts_max and npts_max < npts:\n        auto_spline = True\n    # Always set npts_max.\n    else:\n        npts_max = 200000\n\n    # Call either the spline routine or the normal routine.\n    if auto_spline is True and npts > npts_max:\n        mtspeclib.dpss_spline_(\n            C.byref(C.c_int(npts_max)), C.byref(C.c_int(npts)),\n            C.byref(C.c_double(fw)), C.byref(C.c_int(number_of_tapers)),\n            mt.p(v), mt.p(lamb), mt.p(theta))\n    else:\n        mtspeclib.dpss_(C.byref(C.c_int(npts)), C.byref(C.c_double(fw)),\n                        C.byref(C.c_int(number_of_tapers)), mt.p(v),\n                        mt.p(lamb), mt.p(theta))\n\n    return (v, lamb, theta)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to calculate the Wigner - Ville Spectrum of a signal using multitaper spectral estimates.", "response": "def wigner_ville_spectrum(data, delta, time_bandwidth=3.5,\n                          number_of_tapers=None, smoothing_filter=None,\n                          filter_width=100, frequency_divider=1,\n                          verbose=False):\n    \"\"\"\n    Function to calculate the Wigner-Ville Distribution or Wigner-Ville\n    Spectrum of a signal using multitaper spectral estimates.\n\n    In general it gives better temporal and frequency resolution than a\n    spectrogram but introduces many artifacts and possibly negative values\n    which are not physical. This can be alleviated a bit by applying a\n    smoothing kernel which is also known as a reduced interference\n    distribution (RID).\n\n    Wraps the ``wv_spec()`` and ``wv_spec_to_array()`` subroutines of the\n    Fortran library.\n\n    It is very slow for large arrays so try with a small one (< 5000 samples)\n    first.\n\n    :param data: The input signal.\n    :type data: numpy.ndarray\n    :param delta: The sampling interval of the data.\n    :type delta: float\n    :param time_bandwidth: Time bandwidth product for the tapers.\n    :type time_bandwidth: float\n    :param number_of_tapers: Number of tapers to use. If ``None``, the number\n        will be automatically determined from the time bandwidth product\n        which is usually the optimal choice.\n    :type number_of_tapers: int\n    :param smoothing_filter: One of ``\"boxcar\"``, ``\"gauss\"`` or just ``None``\n    :type smoothing_filter: str\n    :param filter_width: Filter width in samples.\n    :type filter_width: int\n    :param frequency_divider: This method will always calculate all\n        frequencies from 0 ... Nyquist frequency. This parameter allows the\n        adjustment of the maximum frequency, so that the frequencies range\n        from 0 .. Nyquist frequency / int(frequency_divider).\n    :type frequency_divider: int\n    :param verbose: Verbose output on/off.\n    :type verbose: bool\n\n    .. rubric:: Example\n\n    This example demonstrates how to plot a signal, its multitaper spectral\n    estimate, and its Wigner-Ville time-frequency distribution. The signal\n    is sinusoidal overlaid with two simple linear chirps.\n\n    >>> import matplotlib.pyplot as plt\n    >>> import numpy as np\n    >>> from mtspec import mtspec, wigner_ville_spectrum\n    >>> from mtspec.util import signal_bursts\n    >>> fig = plt.figure()\n\n    Get the example signal.\n\n    >>> data = signal_bursts()\n\n    Plot the data on the top axes.\n\n    >>> ax1 = fig.add_axes([0.2,0.75, 0.79, 0.23])\n    >>> ax1.plot(data, color=\"0.1\")\n    >>> ax1.set_xlim(0, len(data))\n\n    Plot its spectral estimate on the side.\n\n    >>> ax2 = fig.add_axes([0.06,0.02,0.13,0.69])\n    >>> spec, freq = mtspec(data, 10, 3.5)\n    >>> ax2.plot(spec, freq, color=\"0.1\")\n    >>> ax2.set_xlim(0, spec.max())\n    >>> ax2.set_ylim(freq[0], freq[-1])\n    >>> ax2.set_xticks([])\n\n    Create and plot the Wigner-Ville distribution.\n\n    >>> wv = wigner_ville_spectrum(data, 10, 3.5,\n    ...                            smoothing_filter='gauss')\n    >>> ax3 = fig.add_axes([0.2, 0.02, 0.79, 0.69])\n    >>> ax3.set_yticks([])\n    >>> ax3.set_xticks([])\n    >>> # The square root only serves plotting purposes.\n    >>> ax3.imshow(np.sqrt(abs(wv)), interpolation='lanczos',\n    ...            aspect='auto', cmap=\"magma\")\n\n    .. plot::\n\n        # Same as the above code snippet just a bit prettier.\n        import matplotlib.pyplot as plt\n        plt.style.use(\"ggplot\")\n        from mtspec import mtspec, wigner_ville_spectrum\n        from mtspec.util import signal_bursts\n        import numpy as np\n\n        fig = plt.figure()\n        data = signal_bursts()\n\n        # Plot the data\n        ax1 = fig.add_axes([0.2,0.75, 0.79, 0.23])\n        ax1.plot(data, color=\"0.1\")\n        ax1.set_xlim(0, len(data))\n\n        # Plot multitaper spectrum\n        ax2 = fig.add_axes([0.06,0.02,0.13,0.69])\n        spec, freq = mtspec(data, 10, 3.5)\n        ax2.plot(spec, freq, color=\"0.1\")\n        ax2.set_xlim(0, spec.max())\n        ax2.set_ylim(freq[0], freq[-1])\n        ax2.set_xticks([])\n\n        # Create the wigner ville spectrum\n        wv = wigner_ville_spectrum(data, 10, 3.5, smoothing_filter='gauss')\n\n        # Plot the WV\n        ax3 = fig.add_axes([0.2, 0.02, 0.79, 0.69])\n        ax3.set_yticks([])\n        ax3.set_xticks([])\n        ax3.imshow(np.sqrt(abs(wv)), interpolation='lanczos', aspect='auto',\n                   cmap=\"magma\")\n    \"\"\"\n    data = np.require(data, 'float32')\n    mt = _MtspecType(\"float32\")\n\n    npts = len(data)\n\n    # Use the optimal number of tapers in case no number is specified.\n    if number_of_tapers is None:\n        number_of_tapers = int(2 * time_bandwidth) - 1\n\n    # Determine filter.\n    if not smoothing_filter:\n        smoothing_filter = 0\n    elif smoothing_filter == 'boxcar':\n        smoothing_filter = 1\n    elif smoothing_filter == 'gauss':\n        smoothing_filter = 2\n    else:\n        msg = 'Invalid value for smoothing filter.'\n        raise Exception(msg)\n\n    # Verbose mode on or off.\n    if verbose:\n        verbose = C.byref(C.c_char('y'))\n    else:\n        verbose = None\n\n    # Allocate the output array\n    # f90 code internally pads zeros to 2 * npts. That is we only return\n    # every second frequency point, thus decrease the size of the array\n    output = mt.empty((npts // 2 // int(frequency_divider) + 1, npts))\n\n    mtspeclib.wv_spec_to_array_(C.byref(C.c_int(npts)),\n                                C.byref(C.c_float(delta)),\n                                mt.p(data), mt.p(output),\n                                C.byref(C.c_float(time_bandwidth)),\n                                C.byref(C.c_int(number_of_tapers)),\n                                C.byref(C.c_int(smoothing_filter)),\n                                C.byref(C.c_float(filter_width)),\n                                C.byref(C.c_int(frequency_divider)), verbose)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct the coherence spectrum from the yk s and the weights of the usual multitaper spectrum estimation.", "response": "def mt_coherence(df, xi, xj, tbp, kspec, nf, p, **kwargs):\n    \"\"\"\n    Construct the coherence spectrum from the yk's and the\n    weights of the usual multitaper spectrum estimation.\n    Note this code uses the real(4) multitaper code.\n\n    INPUT\n\n    :param df: float; sampling rate of time series\n    :param xi: numpy.ndarray; data for first series\n    :param xj: numpy.ndarray; data for second series\n    :param tbp: float; the time-bandwidth product\n    :param kspec: integer; number of tapers to use\n    :param nf: integer; number of freq points in spectrum\n    :param p:  float; confidence for null hypothesis test, e.g. .95\n\n\n    OPTIONAL INPUT\n\n    :param iadapt:  integer 0 - adaptive, 1 - constant weights\n             default adapt = 1\n\n    OPTIONAL OUTPUTS, the outputs are returned as dictionary, with keys as\n    specified below and values as numpy.ndarrays. In order to activate the\n    output set the corresponding kwarg in the argument list, e.g.\n    ``mt_coherence(df, xi, xj, tbp, kspec, nf, p, freq=True, cohe=True)``\n\n    :param freq:     the frequency bins\n    :param cohe:     coherence of the two series (0 - 1)\n    :param phase:    the phase at each frequency\n    :param speci:    spectrum of first series\n    :param specj:    spectrum of second series\n    :param conf:     p confidence value for each freq.\n    :param cohe_ci:  95% bounds on coherence (not larger than 1)\n    :param phase_ci: 95% bounds on phase estimates\n\n    If confidence intervals are requested, then both phase and\n    cohe variables need to be requested as well.\n    \"\"\"\n    npts = len(xi)\n    if len(xj) != npts:\n        raise Exception(\"Inpurt ndarrays have mismatching length\")\n    mt = _MtspecType('float32')\n\n    # convert type of input arguments if necessary\n    xi = np.require(xi, dtype=mt.float, requirements=[mt.order])\n    xj = np.require(xj, dtype=mt.float, requirements=[mt.order])\n\n    # fill up optional arguments, if not given set them None\n    args = []\n    for key in ('freq', 'cohe', 'phase', 'speci', 'specj', 'conf',\n                'cohe_ci', 'phase_ci', 'iadapt'):\n        kwargs.setdefault(key, None)\n        if key in ('cohe_ci', 'phase_ci') and kwargs[key]:\n            kwargs[key] = mt.empty(nf, 2)\n            args.append(mt.p(kwargs[key]))\n        elif key == 'iadapt' and kwargs[key]:\n            args.append(C.byref(C.c_int(kwargs[key])))\n        elif kwargs[key]:\n            kwargs[key] = mt.empty(nf)\n            args.append(mt.p(kwargs[key]))\n        else:\n            args.append(kwargs[key])\n\n    mtspeclib.mt_cohe_(C.byref(C.c_int(int(npts))),\n                       C.byref(C.c_float(float(df))),\n                       mt.p(xi), mt.p(xj), C.byref(C.c_float(float(tbp))),\n                       C.byref(C.c_int(int(kspec))), C.byref(C.c_int(int(nf))),\n                       C.byref(C.c_float(float(p))), *args)\n\n    # remove None values from dictionary\n    return dict([(k, v) for k, v in kwargs.items() if v is not None])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a signal with two bursts inside. Useful for testing time - based bursts inside.", "response": "def signal_bursts():\n    \"\"\"\n    Generates a signal with two bursts inside. Useful for testing time\n    frequency distributions.\n\n    :returns: Generated signal\n    :rtype: numpy.ndarray\n    \"\"\"\n    np.random.seed(815)\n    length = 5 * 512\n\n    # Baseline low frequency plus noise.\n    data = np.sin(np.linspace(0, 80 * np.pi, length))\n    noise = np.random.ranf(length)\n    noise /= noise.max()\n    noise /= 15\n    data += noise\n\n    # Double last two fifths of the signal.\n    data[-2 * 512:] *= 2.0\n    chirp1 = 2.5 * np.sin(np.linspace(0, 400 * np.pi, 512))\n    chirp1 *= np.linspace(1, 0, 512)\n    data[512:2 * 512] += chirp1\n\n    # Add second transient signal.\n    chirp2 = 5.0 * np.sin(np.linspace(0, 200 * np.pi, 512))\n    chirp2 *= np.linspace(1, 0, 512)\n    data[3 * 512:4 * 512] += chirp2\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a simple linear chirp.", "response": "def linear_chirp(npts=2000):\n    \"\"\"\n    Generates a simple linear chirp.\n\n    :param npts: Number of samples.\n    :type npts: int\n    :returns: Generated signal\n    :rtype: numpy.ndarray\n    \"\"\"\n    time = np.linspace(0, 20, npts)\n    chirp = np.sin(0.2 * np.pi * (0.1 + 24.0 / 2.0 * time) * time)\n    return chirp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating an exponential chirp.", "response": "def exponential_chirp(npts=2000):\n    \"\"\"\n    Generates an exponential chirp.\n\n    :param npts: Number of samples.\n    :type npts: int\n    :returns: Generated signal\n    :rtype: numpy.ndarray\n    \"\"\"\n    time = np.linspace(0, 20, npts)\n    chirp = np.sin(2 * np.pi * 0.2 * (1.3 ** time - 1) / np.log(1.3))\n    return chirp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new object with the argument as the source prototype.", "response": "def create(cls, obj):\n        \"\"\"\n        Create a new prototype object with the argument as the source\n        prototype.\n\n        .. Note:\n\n            This does not `initialize` the newly created object any\n            more than setting its prototype.\n            Calling the __init__ method is usually unnecessary as all\n            initialization data should be in the original prototype\n            object already.\n\n            If required, call __init__ explicitly:\n\n            >>> proto_obj = MyProtoObj(1, 2, 3)\n            >>> obj = MyProtoObj.create(proto_obj)\n            >>> obj.__init__(1, 2, 3)\n\n        \"\"\"\n        self = cls.__new__(cls)\n        self.__proto__ = obj\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bind(self, func):\n        if self.__methods__ is None:\n            self.__methods__ = {}\n        self.__methods__[func.__name__] = BoundFunction(func)", "response": "Take a function and create a bound method\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning if the property attr is owned by the object.", "response": "def has_own_property(self, attr):\n        \"\"\"\n        Returns if the property\n        \"\"\"\n        try:\n            object.__getattribute__(self, attr)\n        except AttributeError:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef use(self, middleware=None, path='/', method_mask=HTTPMethod.ALL):\n\n        # catch decorator pattern\n        if middleware is None:\n            return lambda mw: self.use(mw, path, method_mask)\n\n        if hasattr(middleware, '__growler_router'):\n            router = getattr(middleware, '__growler_router')\n            if isinstance(router, (types.MethodType,)):\n                router = router()\n            self.add_router(path, router)\n        elif isinstance(type(middleware), RouterMeta):\n            router = middleware._RouterMeta__growler_router()\n            self.add_router(path, router)\n        elif hasattr(middleware, '__iter__'):\n            for mw in middleware:\n                self.use(mw, path, method_mask)\n        else:\n            log.info(\"{} Using {} on path {}\", id(self), middleware, path)\n            self.middleware.add(path=path,\n                                func=middleware,\n                                method_mask=method_mask)\n        return middleware", "response": "Decorator for the use method of the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_router(self, path, router):\n        if self.strict_router_check and not isinstance(router, Router):\n            raise TypeError(\"Expected object of type Router, found %r\" % type(router))\n\n        log.info(\"{} Adding router {} on path {}\", id(self), router, path)\n        self.middleware.add(path=path,\n                            func=router,\n                            method_mask=HTTPMethod.ALL,)", "response": "Adds a router to the list of routers that can be used to route a path on a growler. Router."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_root_router(self):\n        try:\n            mw = self.middleware.last()\n        except IndexError:\n            return False\n\n        return (\n            isinstance(mw.func, Router)\n            and mw.mask == HTTPMethod.ALL\n            and mw.path is MiddlewareChain.ROOT_PATTERN\n        )", "response": "Returns True if the last element in the chain is a root router."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints a unix - tree - like output of the structure of the web application.", "response": "def print_middleware_tree(self, *, EOL=os.linesep, **kwargs):  # noqa pragma: no cover\n        \"\"\"\n        Prints a unix-tree-like output of the structure of the web\n        application to the file specified (stdout by default).\n\n        Args:\n            EOL (str): The character or string that ends the line.\n            **kwargs: Arguments pass to the standard print function.\n                This allows specifying the file to write to and the\n                ability to flush output upon creation.\n        \"\"\"\n\n        def mask_to_method_name(mask):\n            if mask == HTTPMethod.ALL:\n                return 'ALL'\n            methods = set(HTTPMethod) - {HTTPMethod.ALL}\n            names = (method.name for method in methods if method.value & mask)\n            return '+'.join(names)\n\n        def path_to_str(path):\n            if isinstance(path, str):\n                return path\n            return path.pattern.replace('\\\\', '')\n\n        def decend_into_tree(chain, level):\n            lines_ = []\n            for mw in chain:\n                info = (mask_to_method_name(mw.mask),\n                        path_to_str(mw.path),\n                        mw.func)\n                prefix = \"\u2502\u00a0\u00a0 \" * level\n                lines_ += [prefix + \"\u251c\u2500\u2500 %s %s %s\" % info]\n                if mw.is_subchain:\n                    lines_ += decend_into_tree(mw.func, level + 1)\n            if level:\n                lines_[-1] = lines_[-1].replace('\u251c', '\u2514')\n            return lines_\n\n        lines = [self.name]\n        lines += decend_into_tree(self.middleware, 0)\n        lines.append('\u2534')\n        print(EOL.join(lines), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_server_and_run_forever(self, loop=None, **server_config):\n        if loop is None:\n            import asyncio\n            loop = asyncio.get_event_loop()\n\n        self.create_server(loop=loop, **server_config)\n        try:\n            loop.run_forever()\n        except KeyboardInterrupt:\n            pass", "response": "This function creates an HTTP server and listens the event loop forever."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for a file matching the given template name.", "response": "def find_template_filename(self, template_name):\n        \"\"\"\n        Searches for a file matching the given template name.\n\n        If found, this method returns the pathlib.Path object of the found\n        template file.\n\n        Args:\n            template_name (str): Name of the template, with or without a file\n                extension.\n\n        Returns:\n            pathlib.Path: Path to the matching filename.\n        \"\"\"\n\n        def next_file():\n            filename = self.path / template_name\n            yield filename\n            try:\n                exts = self.default_file_extensions\n            except AttributeError:\n                return\n\n            strfilename = str(filename)\n            for ext in exts:\n                yield Path(strfilename + ext)\n\n        for filename in next_file():\n            if filename.is_file():\n                return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_data(self, data):\n\n        # Headers have not been read in yet\n        if len(self.headers) == 0:\n            # forward data to the parser\n            data = self.parser.consume(data)\n\n            # Headers are finished - build the request and response\n            if data is not None:\n\n                # setup the request line attributes\n                self.set_request_line(self.parser.method,\n                                      self.parser.parsed_url,\n                                      self.parser.version)\n\n                # initialize \"content_length\" and \"body_buffer\" attributes\n                self.init_body_buffer(self.method, self.headers)\n\n                # builds request and response out of self.headers and protocol\n                self.req, self.res = self.build_req_and_res()\n\n                # add instruct handler to begin running the application\n                # with the created req and res pairs\n                self._handler.begin_application(self.req, self.res)\n\n        # if truthy, 'data' now holds body data\n        if data:\n            self.validate_and_store_body_data(data)\n\n            # if we have reached end of content - put in the request's body\n            if len(self.body_buffer) == self.content_length:\n                self.set_body_data(bytes(self.body_buffer))", "response": "This function is called by the HTTP client when data is received from the socket. It is called by the HTTP client when the HTTP connection is established."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the request line on the responder.", "response": "def set_request_line(self, method, url, version):\n        \"\"\"\n        Sets the request line on the responder.\n        \"\"\"\n        self.parsed_request = (method, url, version)\n        self.request = {\n            'method': method,\n            'url': url,\n            'version': version\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_body_buffer(self, method, headers):\n        content_length = headers.get(\"CONTENT-LENGTH\", None)\n\n        if method in (HTTPMethod.POST, HTTPMethod.PUT):\n            if content_length is None:\n                raise HTTPErrorBadRequest(\"HTTP Method requires a CONTENT-LENGTH header\")\n            self.content_length = int(content_length)\n            self.body_buffer = bytearray(0)\n\n        elif content_length is not None:\n            raise HTTPErrorBadRequest(\n                \"HTTP method %s may NOT have a CONTENT-LENGTH header\"\n            )", "response": "Initializes the body_buffer and content_length attributes based on the HTTP method and headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a request and response pair.", "response": "def build_req_and_res(self):\n        \"\"\"\n        Simple method which calls the request and response factories\n        the responder was given, and returns the pair.\n        \"\"\"\n        req = self.build_req(self, self.headers)\n        res = self.build_res(self._handler)\n        return req, res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_and_store_body_data(self, data):\n\n        # add data to end of buffer\n        self.body_buffer[-1:] = data\n\n        #\n        if len(self.body_buffer) > self.content_length:\n            problem = \"Content length exceeds expected value (%d > %d)\" % (\n                len(self.body_buffer), self.content_length\n            )\n            raise HTTPErrorBadRequest(phrase=problem)", "response": "Checks if incoming data is valid and stores it into self. _buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef http_responder_factory(proto):\n        return GrowlerHTTPResponder(\n            proto,\n            request_factory=proto.http_application._request_class,\n            response_factory=proto.http_application._response_class,\n        )", "response": "Returns a default factory function which creates a GrowlerHTTPResponder with the same parameters as the parent protocol and the application s req and resizers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling by the HTTP server to start the application processing.", "response": "def begin_application(self, req, res):\n        \"\"\"\n        Entry point for the application middleware chain for an asyncio\n        event loop.\n        \"\"\"\n        # Add the middleware processing to the event loop - this *should*\n        # change the call stack so any server errors do not link back to this\n        # function\n        self.loop.create_task(self.http_application.handle_client_request(req, res))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef body_storage_pair(self):\n        future = Future()\n\n        def send_body():\n            nonlocal future\n            data = yield\n            future.set_result(data)\n            yield\n\n        sender = send_body()\n        next(sender)\n        return future, sender", "response": "Return reader and writer pair for storing receiving body data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connection_made(self, transport):\n        self.transport = transport\n        self.responders = [self.make_responder(self)]\n\n        try:\n            good_func = callable(self.responders[0].on_data)\n        except AttributeError:\n            good_func = False\n\n        if not good_func:\n            err_str = \"Provided responder MUST implement an 'on_data' method\"\n            raise TypeError(err_str)\n\n        log_info = (id(self), self.remote_hostname, self.remote_port)\n        log.info(\"{:d} connection from {}:{}\", *log_info)", "response": "Called upon when a new connection is made."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connection_lost(self, exc):\n        if exc:\n            log.error(\"{:d} connection_lost {}\", id(self), exc)\n        else:\n            log.info(\"{:d} connection_lost\", id(self))", "response": "Called upon when a socket is closed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef data_received(self, data):\n        try:\n            self.responders[-1].on_data(data)\n        except Exception as error:\n            self.handle_error(error)", "response": "Called upon when data is received from the protocol."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_etag(file_path):\n        stat = file_path.stat()\n        etag = \"%x-%x\" % (stat.st_mtime_ns, stat.st_size)\n        return etag", "response": "Calculates an etag value for the file_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_default_headers(self):\n        self.headers.setdefault('Date', self.get_current_time)\n        self.headers.setdefault('Server', self.SERVER_INFO)\n        self.headers.setdefault('Content-Length', \"%d\" % len(self.message))\n        if self.app.enabled('x-powered-by'):\n            self.headers.setdefault('X-Powered-By', 'Growler')", "response": "Set some default headers for the HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends the headers to the client", "response": "def send_headers(self):\n        \"\"\"\n        Sends the headers to the client\n        \"\"\"\n        self.events.sync_emit('headers')\n        self._set_default_headers()\n        header_str = self.status_line + self.EOL + str(self.headers)\n        self.stream.write(header_str.encode())\n        self.events.sync_emit('after_headers')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the first line of response including http version status and phrase.", "response": "def status_line(self):\n        \"\"\"\n        Returns the first line of response, including http version, status\n        and a phrase (OK).\n        \"\"\"\n        if not self.phrase:\n            self.phrase = HttpStatus(self.status_code).phrase\n        return \"{} {} {}\".format(\"HTTP/1.1\", self.status_code, self.phrase)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef end(self):\n        self.send_headers()\n        self.write()\n        self.write_eof()\n        self.has_ended = True", "response": "Ends the response. Useful for quickly ending connection with no data\n        sent"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef redirect(self, url, status=None):\n        self.status_code = 302 if status is None else status\n        self.headers = Headers([('location', url)])\n        self.message = ''\n        self.end()", "response": "Redirect to the specified url optional status code defaults to 302."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the value of the header", "response": "def set(self, header, value=None):\n        \"\"\"Set header to the value\"\"\"\n        if value is None:\n            for k, v in header.items():\n                self.headers[k] = v\n        else:\n            self.headers[header] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a stringified JSON response to the client. Automatically sets the content - type header to application / json.", "response": "def send_json(self, obj, status=200):\n        \"\"\"\n        Sends a stringified JSON response to client. Automatically sets the\n        content-type header to application/json.\n\n        Parameters\n        ----------\n        obj : mixed\n            Any object which will be serialized by the json.dumps module\n            function\n        status : int, optional\n            The HTTP status code, defaults to 200 (OK)\n        \"\"\"\n        self.headers['Content-Type'] = 'application/json'\n        self.status_code = status\n        message = json.dumps(obj)\n        self.send_text(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_html(self, html, status=200):\n        self.headers.setdefault('Content-Type', 'text/html')\n        self.message = html\n        self.status_code = status\n        self.send_headers()\n        self.write()\n        self.write_eof()", "response": "Sends an html response to the client. Automatically sets the content - type to text / html."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_text(self, txt, status=200):\n        self.headers.setdefault('content-type', 'text/plain')\n        if not isinstance(txt, bytes):\n            txt = str(txt).encode()\n        self.message = txt\n        self.status_code = status\n        self.end()", "response": "Sends a plaintext response to the client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread in the file filename and sends bytes to the client.", "response": "def send_file(self, filename, status=200):\n        \"\"\"\n        Reads in the file 'filename' and sends bytes to client\n\n        Parameters\n        ----------\n        filename : str\n            Filename of the file to read\n        status : int, optional\n            The HTTP status code, defaults to 200 (OK)\n        \"\"\"\n        if isinstance(filename, Path) and sys.version_info >= (3, 5):\n            self.message = filename.read_bytes()\n        else:\n            with io.FileIO(str(filename)) as f:\n                self.message = f.read()\n        self.status_code = status\n        self.send_headers()\n        self.write()\n        self.write_eof()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, *args, **kwargs):\n        for next_dict in chain(args, (kwargs, )):\n            for k, v in next_dict.items():\n                self[k] = v", "response": "Equivalent to the python dict update method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a header to the collection.", "response": "def add_header(self, key, value, **params):\n        \"\"\"\n        Add a header to the collection, including potential parameters.\n\n        Args:\n            key (str): The name of the header\n            value (str): The value to store under that key\n            params: Option parameters to be appended to the value,\n                automatically formatting them in a standard way\n        \"\"\"\n\n        key = self.escape(key)\n        ci_key = key.casefold()\n\n        def quoted_params(items):\n            for p in items:\n                param_name = self.escape(p[0])\n                param_val = self.de_quote(self.escape(p[1]))\n                yield param_name, param_val\n\n        sorted_items = sorted(params.items())\n\n        quoted_iter = ('%s=\"%s\"' % p for p in quoted_params(sorted_items))\n        param_str = ' '.join(quoted_iter)\n\n        if param_str:\n            value = \"%s; %s\" % (value, param_str)\n\n        self._header_data[ci_key] = (key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string representation of the HTTP headers as a valid HTTP header string.", "response": "def stringify(self, use_bytes=False):\n        \"\"\"\n        Returns representation of headers as a valid HTTP header string. This\n        is called by __str__.\n\n        Args:\n            use_bytes (bool): Returns a bytes object instead of a str.\n        \"\"\"\n        def _str_value(value):\n            if isinstance(value, (list, tuple)):\n                value = (self.EOL + '\\t').join(map(_str_value, value))\n            elif callable(value):\n                value = _str_value(value())\n            return value\n\n        s = self.EOL.join((\"{key}: {value}\".format(key=key,\n                                                   value=_str_value(value))\n                           for key, value in self._header_data.values()\n                           if value is not None))\n        return s + (self.EOL * 2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning root page of website.", "response": "def index(req, res):\n    \"\"\"\n    Return root page of website.\n    \"\"\"\n    number = req.session.get('counter', -1)\n    req.session['counter'] = int(number) + 1\n    print(\" -- Session '{id}' returned {counter} times\".format(**req.session))\n    msg = \"Hello!! You've been here [[%s]] times\" % (req.session['counter'])\n    res.send_text(msg)\n    req.session.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def body(self):\n        if not isinstance(self._body, bytes):\n            self._body = await self._body\n        return self._body", "response": "A helper function which blocks until the body has been read."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a callback to the event named name.", "response": "def on(self, name, _callback=None):\n        \"\"\"\n        Add a callback to the event named 'name'.\n        Returns callback object for decorationable calls.\n        \"\"\"\n\n        # this is being used as a decorator\n        if _callback is None:\n            return lambda cb: self.on(name, cb)\n\n        if not (callable(_callback) or isawaitable(_callback)):\n            msg = \"Callback not callable: {0!r}\".format(_callback)\n            raise ValueError(msg)\n\n        self._event_list[name].append(_callback)\n        return _callback"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a callback to the event named name. Returns this object for chained on calls.", "response": "async def emit(self, name):\n        \"\"\"\n        Add a callback to the event named 'name'.\n        Returns this object for chained 'on' calls.\n        \"\"\"\n        for cb in self._event_list[name]:\n            if isawaitable(cb):\n                await cb\n            else:\n                cb()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_routeable_attributes(obj, keys):\n    for attr in keys:\n        matches = ROUTABLE_NAME_REGEX.match(attr)\n        if matches is None:\n            continue\n        try:\n            val = getattr(obj, attr)\n        except AttributeError:\n            continue\n\n        if not callable(val) or val.__doc__ is None:\n            continue\n\n        method_name = matches.group(1).upper()\n        yield val, method_name", "response": "Given a set of keys find the attributes that satisfy the routeable requirements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield a list of routing attributes for the object obj.", "response": "def get_routing_attributes(obj, modify_doc=False, keys=None):\n    \"\"\"\n    Loops through the provided object (using the dir() function) and\n    finds any callables which match the name signature (e.g.\n    get_foo()) AND has a docstring beginning with a path-like char\n    string.\n    This does process things in alphabetical order (rather than than\n    the unpredictable __dict__ attribute) so take this into\n    consideration if certain routes should be checked before others.\n    Unfortunately, this is a problem because the 'all' method will\n    always come before others, so there is no capturing one type\n    followed by a catch-all 'all'. Until a solution is found, just\n    make a router by hand.\n    \"\"\"\n    if keys is None:\n        keys = dir(obj)\n\n    for val, method_str in _find_routeable_attributes(obj, keys):\n\n        path, *doc = val.__doc__.split(maxsplit=1) or ('', '')\n\n        if not path:\n            continue\n\n        if modify_doc:\n            val.__doc__ = ''.join(doc)\n\n        method = HTTPMethod[method_str]\n\n        yield method, path, val"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef routerclass(cls):\n    logging.debug(\"Creating a routerclass with the class %s\" % cls)\n    cls.__growler_router = lambda self: routerify(self)\n    return cls", "response": "A class decorator that creates a routerclass for the given class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef routerify(obj):\n    router = Router()\n    for info in get_routing_attributes(obj):\n        router.add_route(*info)\n    obj.__growler_router = router\n    return router", "response": "Create a router from the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a route to this request.", "response": "def add_router(self, path, router):\n        \"\"\"\n        Add a (regex, router) pair to this router. Any req.path that\n        matches the regex will pass the request/response objects to\n        that router.\n        \"\"\"\n        self.add(HTTPMethod.ALL, path, router)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling the provided middleware upon requests matching the path.", "response": "def use(self, middleware, path=None):\n        \"\"\"\n        Call the provided middleware upon requests matching the path.\n        If path is not provided or None, all requests will match.\n\n        Args:\n            middleware (callable): Callable with the signature\n                ``(res, req) -> None``\n            path (Optional[str or regex]): a specific path the\n                request must match for the middleware to be called.\n        Returns:\n            This router\n        \"\"\"\n        self.log.info(\" Using middleware {}\", middleware)\n        if path is None:\n            path = MiddlewareChain.ROOT_PATTERN\n        self.add(HTTPMethod.ALL, path, middleware)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the request should not be skipped based on the middleware path.", "response": "def should_skip_middleware(self, middleware, matching, rest) -> bool:\n        \"\"\"\n        Returns True (i.e. should skip) if request does not match the\n        entire middleware path.\n        This is a simple check if 'rest' is truthy or not.\n        \"\"\"\n        return bool(not matching) or bool(rest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subrouters(self):\n        yield from filter(lambda mw: isinstance(mw.func, Router), self.mw_list)", "response": "Generator of sub - routers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sinatra_path_to_regex(cls, path):\n        # Return the path if already a (compiled) regex\n        if type(path) is cls.regex_type:\n            return path\n\n        # Build a regular expression string which is split on the '/' character\n        regex = [\n            \"(?P<{}>\\w+)\".format(segment[1:])\n            if cls.sinatra_param_regex.match(segment)\n            else segment\n            for segment in path.split('/')\n        ]\n        return re.compile('/'.join(regex))", "response": "Converts a sinatra - style path to a regular expression with named\n        parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _http_parser(self):\n        # first, find first EOL (i.e. get request line)\n        yield from self._receive_eol_token()\n\n        # split the request line and headers (modifies buffer)\n        yield from self._parse_and_store_req_line(self.EOL_TOKEN)\n\n        # completes when all headers have been processed and stored\n        yield from self._parse_and_store_headers()\n\n        # we send back the rest of the body\n        yield self._buffer", "response": "A generator that yields the HTTP headers and the response body."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _receive_eol_token(self):\n        while self.EOL_TOKEN is None:\n            # use yield to have data sent to us - store in buffer\n            self._buffer += yield\n            if len(self._buffer) > MAX_REQUEST_LENGTH:\n                raise HTTPErrorBadRequest(\"Max request length exceeded\")\n            self.EOL_TOKEN = self.determine_newline(self._buffer)", "response": "A simple coroutine that is sent data until the first EOL token is found."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the request line into the internal state of the object.", "response": "def _store_request_line(self, req_line):\n        \"\"\"\n        Splits the request line given into three components.\n        Ensures that the version and method are valid for this server,\n        and uses the urllib.parse function to parse the request URI.\n\n        Note:\n            This method has the additional side effect of updating all\n            request line related attributes of the parser.\n\n        Returns:\n            tuple: Tuple containing the parsed (method, parsed_url,\n                version)\n\n        Raises:\n            HTTPErrorBadRequest: If request line is invalid\n            HTTPErrorNotImplemented: If HTTP method is not recognized\n            HTTPErrorVersionNotSupported: If HTTP version is not\n                recognized.\n        \"\"\"\n        if not isinstance(req_line, str):\n            try:\n                req_line = self.raw_request_line = req_line.decode()\n            except UnicodeDecodeError:\n                raise HTTPErrorBadRequest\n\n        try:\n            self.method_str, self.original_url, self.version = req_line.split()\n        except ValueError:\n            raise HTTPErrorBadRequest()\n\n        if self.version not in ('HTTP/1.1', 'HTTP/1.0'):\n            raise HTTPErrorVersionNotSupported(self.version)\n\n        # allow lowercase methodname?\n        # self.method_str = self.method_str.upper()\n\n        # save 'method' and get the correct function to finish processing\n        try:\n            self.method = HTTPMethod[self.method_str]\n        except KeyError:\n            # Method not found\n            err = \"Unknown HTTP Method '{}'\".format(self.method_str)\n            raise HTTPErrorNotImplemented(err)\n\n        self._process_headers = {\n            HTTPMethod.GET: self.process_get_headers,\n            HTTPMethod.POST: self.process_post_headers\n        }.get(self.method, lambda data: True)\n\n        _, num_str = self.version.split('/', 1)\n        self.HTTP_VERSION = tuple(num_str.split('.'))\n        self.version_number = float(num_str)\n\n        self.parsed_url = urlparse(self.original_url)\n        self.path = unquote(self.parsed_url.path)\n        self.query = parse_qs(self.parsed_url.query)\n\n        return self.method, self.parsed_url, self.version"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef determine_newline(data):\n        line_end_pos = data.find(b'\\n')\n\n        if line_end_pos == -1:\n            return None\n        elif line_end_pos == 0:\n            return b'\\n'\n\n        prev_char = data[line_end_pos - 1]\n\n        return b'\\r\\n' if (prev_char is b'\\r'[0]) else b'\\n'", "response": "Determines if a newline character in bytestring parameter data is found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a byte string and attempts to decode and build a key - value pair for the header.", "response": "def split_header_key_value(self, line):\n        \"\"\"\n        Takes a byte string and attempts to decode and build a key-value\n        pair for the header. Header names are checked for validity. In\n        the event that the string can not be split on a ':' char, an\n        HTTPErrorInvalidHeader exception is raised.\n\n        Parameters:\n            line (bytes): header bytes which will be automatically\n                decoded and split between key + value.\n\n        Returns:\n            tuple of 2 strs: key + value\n\n        Raises:\n            HTTPErrorInvalidHeader: The string cannot be split on a ':'\n                character or the header key is an invalid HTTP header\n                name.\n        \"\"\"\n        try:\n            line = line.decode()\n            key, value = map(str.strip, line.split(':', 1))\n        except ValueError:\n            raise HTTPErrorInvalidHeader\n\n        if self.is_invalid_header_name(key):\n            raise HTTPErrorInvalidHeader\n\n        return key, value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit a path into the parts matching this middleware and the part remaining.", "response": "def path_split(self, path):\n        \"\"\"\n        Splits a path into the part matching this middleware and the part remaining.\n        If path does not exist, it returns a pair of None values.\n        If the regex matches the entire pair, the second item in returned tuple is None.\n\n        Args:\n            path (str): The url to split\n\n        Returns:\n            Tuple\n            matching_path (str or None): The beginning of the path which matches this\n                middleware or None if it does not match\n            remaining_path (str or None): The 'rest' of the path, following the matching part\n        \"\"\"\n\n        match = self.path.match(path)\n        if match is None:\n            return None, None\n\n        # split string at position\n        the_rest = path[match.end():]\n\n        # ensure we split at a '/' character\n        if the_rest:\n            if match.group().endswith('/'):\n                pass\n            elif the_rest.startswith('/'):\n                pass\n            else:\n                return None, None\n\n        if self.IGNORE_TRAILING_SLASH and the_rest == '/':\n            the_rest = ''\n\n        return match, the_rest"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield the middleware and the rest of the url that matches the given method + path.", "response": "def find_matching_middleware(self, method, path):\n        \"\"\"\n        Iterator handling the matching of middleware against a method+path\n        pair. Yields the middleware, and the\n        \"\"\"\n        for mw in self.mw_list:\n            if not mw.matches_method(method):\n                continue\n\n            # get the path matching this middleware and the 'rest' of the url\n            # (i.e. the part that comes AFTER the match) to be potentially\n            # matched later by a subchain\n            path_match, rest_url = mw.path_split(path)\n            if self.should_skip_middleware(mw, path_match, rest_url):\n                continue\n\n            yield mw, path_match, rest_url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over the specified sub - chain and yield the result.", "response": "def iterate_subchain(self, chain):\n        \"\"\"\n        A coroutine used by __call__ to forward all requests to a\n        subchain.\n        \"\"\"\n        for mw in chain:\n            try:\n                yield mw\n            except Exception as err:\n                yield chain.throw(err)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, method_mask, path, func):\n        is_err = len(signature(func).parameters) == 3\n        is_subchain = isinstance(func, MiddlewareChain)\n        tup = MiddlewareNode(func=func,\n                             mask=method_mask,\n                             path=path,\n                             is_errorhandler=is_err,\n                             is_subchain=is_subchain,)\n        self.mw_list.append(tup)", "response": "Adds a function to the middleware chain."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the total number of middleware in this chain and all subchains.", "response": "def count_all(self):\n        \"\"\"\n        Returns the total number of middleware in this chain and subchains.\n        \"\"\"\n        return sum(x.func.count_all() if x.is_subchain else 1 for x in self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the given class is a valid feature writer class.", "response": "def isValidFeatureWriter(klass):\n    \"\"\"Return True if 'klass' is a valid feature writer class.\n    A valid feature writer class is a class (of type 'type'), that has\n    two required attributes:\n    1) 'tableTag' (str), which can be \"GSUB\", \"GPOS\", or other similar tags.\n    2) 'write' (bound method), with the signature matching the same method\n       from the BaseFeatureWriter class:\n\n           def write(self, font, feaFile, compiler=None)\n    \"\"\"\n    if not isclass(klass):\n        logger.error(\"%r is not a class\", klass)\n        return False\n    if not hasattr(klass, \"tableTag\"):\n        logger.error(\"%r does not have required 'tableTag' attribute\", klass)\n        return False\n    if not hasattr(klass, \"write\"):\n        logger.error(\"%r does not have a required 'write' method\", klass)\n        return False\n    if (\n        getargspec(klass.write).args\n        != getargspec(BaseFeatureWriter.write).args\n    ):\n        logger.error(\"%r 'write' method has incorrect signature\", klass)\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the list of feature writers from the UFO lib.", "response": "def loadFeatureWriters(ufo, ignoreErrors=True):\n    \"\"\"Check UFO lib for key \"com.github.googlei18n.ufo2ft.featureWriters\",\n    containing a list of dicts, each having the following key/value pairs:\n    For example:\n\n      {\n        \"module\": \"myTools.featureWriters\",  # default: ufo2ft.featureWriters\n        \"class\": \"MyKernFeatureWriter\",  # required\n        \"options\": {\"doThis\": False, \"doThat\": True},\n      }\n\n    Import each feature writer class from the specified module (default is\n    the built-in ufo2ft.featureWriters), and instantiate it with the given\n    'options' dict.\n\n    Return the list of feature writer objects.\n    If the 'featureWriters' key is missing from the UFO lib, return None.\n\n    If an exception occurs and 'ignoreErrors' is True, the exception message\n    is logged and the invalid writer is skipped, otrherwise it's propagated.\n    \"\"\"\n    if FEATURE_WRITERS_KEY not in ufo.lib:\n        return None\n    writers = []\n    for wdict in ufo.lib[FEATURE_WRITERS_KEY]:\n        try:\n            moduleName = wdict.get(\"module\", __name__)\n            className = wdict[\"class\"]\n            options = wdict.get(\"options\", {})\n            if not isinstance(options, dict):\n                raise TypeError(type(options))\n            module = importlib.import_module(moduleName)\n            klass = getattr(module, className)\n            if not isValidFeatureWriter(klass):\n                raise TypeError(klass)\n            writer = klass(**options)\n        except Exception:\n            if ignoreErrors:\n                logger.exception(\"failed to load feature writer: %r\", wdict)\n                continue\n            raise\n        writers.append(writer)\n    return writers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef loadFeatureWriterFromString(spec):\n    spec = spec.strip()\n    m = _featureWriterSpecRE.match(spec)\n    if not m or (m.end() - m.start()) != len(spec):\n        raise ValueError(spec)\n    moduleName = m.group(1) or \"ufo2ft.featureWriters\"\n    className = m.group(2)\n    kwargs = m.group(3)\n\n    module = importlib.import_module(moduleName)\n    klass = getattr(module, className)\n    if not isValidFeatureWriter(klass):\n        raise TypeError(klass)\n    try:\n        options = _kwargsEval(kwargs) if kwargs else {}\n    except SyntaxError:\n        raise ValueError(\"options have incorrect format: %r\" % kwargs)\n    return klass(**options)", "response": "Load a single feature writer from a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if a certain type of relationship exists between two users.", "response": "def if_relationship(parser, token):\n    \"\"\"\n    Determine if a certain type of relationship exists between two users.\n    The ``status`` parameter must be a slug matching either the from_slug,\n    to_slug or symmetrical_slug of a RelationshipStatus.\n\n    Example::\n\n        {% if_relationship from_user to_user \"friends\" %}\n            Here are pictures of me drinking alcohol\n        {% else %}\n            Sorry coworkers\n        {% endif_relationship %}\n\n        {% if_relationship from_user to_user \"blocking\" %}\n            damn seo experts\n        {% endif_relationship %}\n    \"\"\"\n    bits = list(token.split_contents())\n    if len(bits) != 4:\n        raise TemplateSyntaxError(\"%r takes 3 arguments:\\n%s\" % (bits[0], if_relationship.__doc__))\n    end_tag = 'end' + bits[0]\n    nodelist_true = parser.parse(('else', end_tag))\n    token = parser.next_token()\n    if token.contents == 'else':\n        nodelist_false = parser.parse((end_tag,))\n        parser.delete_first_token()\n    else:\n        nodelist_false = template.NodeList()\n    return IfRelationshipNode(nodelist_true, nodelist_false, *bits[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_relationship_url(user, status):\n    if isinstance(status, RelationshipStatus):\n        status = status.from_slug\n    return reverse('relationship_add', args=[user.username, status])", "response": "Generate a url for adding a relationship on a given user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the UFO file and return a new ufo2ft. Otf file with the new UFO file.", "response": "def process(self, useProductionNames=None, optimizeCFF=True):\n        \"\"\"\n        useProductionNames:\n          By default, when value is None, this will rename glyphs using the\n          'public.postscriptNames' in then UFO lib. If the mapping is not\n          present, no glyph names are renamed.\n          If the value is False, no glyphs are renamed whether or not the\n          'public.postscriptNames' mapping is present.\n          If the value is True, but no 'public.postscriptNames' are present,\n          then uniXXXX names are generated from the glyphs' unicode.\n\n          The 'com.github.googlei18n.ufo2ft.useProductionNames' key can be set\n          in the UFO lib to control this parameter (plist boolean value).\n\n          For legacy reasons, an alias key (with an inverted meaning) is also\n          supported: \"com.schriftgestaltung.Don't use Production Names\";\n          when this is present if the UFO lib and is set to True, this is\n          equivalent to 'useProductionNames' set to False.\n\n        optimizeCFF:\n          Run compreffor to subroubtinize CFF table, if present.\n        \"\"\"\n        if useProductionNames is None:\n            useProductionNames = self.ufo.lib.get(\n                USE_PRODUCTION_NAMES,\n                not self.ufo.lib.get(GLYPHS_DONT_USE_PRODUCTION_NAMES)\n                and self._postscriptNames is not None\n            )\n        if useProductionNames:\n            self._rename_glyphs_from_ufo()\n        if optimizeCFF and 'CFF ' in self.otf:\n            from compreffor import compress\n\n            logger.info(\"Subroutinizing CFF table\")\n            compress(self.otf)\n        if 'OS/2' in self.otf:\n            self.otf['OS/2'].usMaxContext = maxCtxFont(self.otf)\n        return self.otf"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _rename_glyphs_from_ufo(self):\n        rename_map = self._build_production_names()\n\n        otf = self.otf\n        otf.setGlyphOrder([rename_map.get(n, n) for n in otf.getGlyphOrder()])\n\n        # we need to compile format 2 'post' table so that the 'extraNames'\n        # attribute is updated with the list of the names outside the\n        # standard Macintosh glyph order; otherwise, if one dumps the font\n        # to TTX directly before compiling first, the post table will not\n        # contain the extraNames.\n        if 'post' in otf and otf['post'].formatType == 2.0:\n            otf['post'].compile(self.otf)\n\n        if 'CFF ' in otf:\n            cff = otf['CFF '].cff.topDictIndex[0]\n            char_strings = cff.CharStrings.charStrings\n            cff.CharStrings.charStrings = {\n                rename_map.get(n, n): v for n, v in char_strings.items()}\n            cff.charset = [rename_map.get(n, n) for n in cff.charset]", "response": "Rename glyphs using ufo. lib. public. postscriptNames in UFO."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _unique_name(name, seen):\n        if name in seen:\n            n = seen[name]\n            while (name + \".%d\" % n) in seen:\n                n += 1\n            seen[name] = n + 1\n            name += \".%d\" % n\n        seen[name] = 1\n        return name", "response": "Append an incremental. N suffix if glyph is a duplicate."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_production_name(self, glyph):\n\n        # use PostScript names from UFO lib if available\n        if self._postscriptNames:\n            production_name = self._postscriptNames.get(glyph.name)\n            return production_name if production_name else glyph.name\n\n        # use name derived from unicode value\n        unicode_val = glyph.unicode\n        if glyph.unicode is not None:\n            return '%s%04X' % (\n                'u' if unicode_val > 0xffff else 'uni', unicode_val)\n\n        # use production name + last (non-script) suffix if possible\n        parts = glyph.name.rsplit('.', 1)\n        if len(parts) == 2 and parts[0] in self.glyphSet:\n            return '%s.%s' % (\n                self._build_production_name(self.glyphSet[parts[0]]), parts[1])\n\n        # use ligature name, making sure to look up components with suffixes\n        parts = glyph.name.split('.', 1)\n        if len(parts) == 2:\n            liga_parts = ['%s.%s' % (n, parts[1]) for n in parts[0].split('_')]\n        else:\n            liga_parts = glyph.name.split('_')\n        if len(liga_parts) > 1 and all(n in self.glyphSet for n in liga_parts):\n            unicode_vals = [self.glyphSet[n].unicode for n in liga_parts]\n            if all(v and v <= 0xffff for v in unicode_vals):\n                return 'uni' + ''.join('%04X' % v for v in unicode_vals)\n            return '_'.join(\n                self._build_production_name(self.glyphSet[n]) for n in liga_parts)\n\n        return glyph.name", "response": "Build a production name for a single glyph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of tuples ( baseGlyph transform ) of nested component.", "response": "def _flattenComponent(glyphSet, component):\n    \"\"\"Returns a list of tuples (baseGlyph, transform) of nested component.\"\"\"\n\n    glyph = glyphSet[component.baseGlyph]\n    if not glyph.components:\n        transformation = Transform(*component.transformation)\n        return [(component.baseGlyph, transformation)]\n\n    all_flattened_components = []\n    for nested in glyph.components:\n        flattened_components = _flattenComponent(glyphSet, nested)\n        for i, (_, tr) in enumerate(flattened_components):\n            tr = tr.transform(component.transformation)\n            flattened_components[i] = (flattened_components[i][0], tr)\n        all_flattened_components.extend(flattened_components)\n    return all_flattened_components"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getScriptLanguageSystems(feaFile):\n    languagesByScript = collections.OrderedDict()\n    for ls in [\n        st\n        for st in feaFile.statements\n        if isinstance(st, ast.LanguageSystemStatement)\n    ]:\n        if ls.script == \"DFLT\":\n            continue\n        languagesByScript.setdefault(ls.script, []).append(ls.language)\n\n    langSysMap = collections.OrderedDict()\n    for script, languages in languagesByScript.items():\n        sc = unicodedata.ot_tag_to_script(script)\n        langSysMap.setdefault(sc, []).append((script, languages))\n    return langSysMap", "response": "Return dictionary keyed by Unicode script code containing lists of language systems in the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a dictionary of groups and a feaFile returns a dictionary keyed by the original group name and the class name of each group.", "response": "def makeGlyphClassDefinitions(groups, feaFile=None, stripPrefix=\"\"):\n    \"\"\" Given a groups dictionary ({str: list[str]}), create feaLib\n    GlyphClassDefinition objects for each group.\n    Return a dict keyed by the original group name.\n\n    If `stripPrefix` (str) is provided and a group name starts with it,\n    the string will be stripped from the beginning of the class name.\n    \"\"\"\n    classDefs = {}\n    if feaFile is not None:\n        classNames = {cdef.name for cdef in iterClassDefinitions(feaFile)}\n    else:\n        classNames = set()\n    lengthPrefix = len(stripPrefix)\n    for groupName, members in sorted(groups.items()):\n        originalGroupName = groupName\n        if stripPrefix and groupName.startswith(stripPrefix):\n            groupName = groupName[lengthPrefix:]\n        className = makeFeaClassName(groupName, classNames)\n        classNames.add(className)\n        classDef = makeGlyphClassDefinition(className, members)\n        classDefs[originalGroupName] = classDef\n    return classDefs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeFeaClassName(name, existingClassNames=None):\n    name = re.sub(r\"[^A-Za-z0-9._]\", r\"\", name)\n    if existingClassNames is None:\n        return name\n    i = 1\n    origName = name\n    while name in existingClassNames:\n        name = \"%s_%d\" % (origName, i)\n        i += 1\n    return name", "response": "Make a glyph class name which is legal to use in feature text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding references to named lookups to the given feature file s statements.", "response": "def addLookupReferences(\n    feature, lookups, script=None, languages=None, exclude_dflt=False\n):\n    \"\"\"Add references to named lookups to the feature's statements.\n    If `script` (str) and `languages` (sequence of str) are provided,\n    only register the lookup for the given script and languages,\n    optionally with `exclude_dflt` directive.\n    Otherwise add a global reference which will be registered for all\n    the scripts and languages in the feature file's `languagesystems`\n    statements.\n    \"\"\"\n    assert lookups\n    if not script:\n        for lookup in lookups:\n            feature.statements.append(ast.LookupReferenceStatement(lookup))\n        return\n\n    feature.statements.append(ast.ScriptStatement(script))\n    if exclude_dflt:\n        for language in languages or (\"dflt\",):\n            feature.statements.append(\n                ast.LanguageStatement(language, include_default=False)\n            )\n            for lookup in lookups:\n                feature.statements.append(ast.LookupReferenceStatement(lookup))\n    else:\n        feature.statements.append(\n            ast.LanguageStatement(\"dflt\", include_default=True)\n        )\n        for lookup in lookups:\n            feature.statements.append(ast.LookupReferenceStatement(lookup))\n        for language in languages or ():\n            if language == \"dflt\":\n                continue\n            feature.statements.append(\n                ast.LanguageStatement(language, include_default=True)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns GDEF GlyphClassDef base / mark / ligature / component glyphs or None if no GDEF table is defined in the feature file.", "response": "def getGDEFGlyphClasses(feaLib):\n    \"\"\"Return GDEF GlyphClassDef base/mark/ligature/component glyphs, or\n    None if no GDEF table is defined in the feature file.\n    \"\"\"\n    for st in feaLib.statements:\n        if isinstance(st, ast.TableBlock) and st.name == \"GDEF\":\n            for st in st.statements:\n                if isinstance(st, ast.GlyphClassDefStatement):\n                    return _GDEFGlyphClasses(\n                        frozenset(st.baseGlyphs.glyphSet())\n                        if st.baseGlyphs is not None\n                        else frozenset(),\n                        frozenset(st.ligatureGlyphs.glyphSet())\n                        if st.ligatureGlyphs is not None\n                        else frozenset(),\n                        frozenset(st.markGlyphs.glyphSet())\n                        if st.markGlyphs is not None\n                        else frozenset(),\n                        frozenset(st.componentGlyphs.glyphSet())\n                        if st.componentGlyphs is not None\n                        else frozenset(),\n                    )\n    return _GDEFGlyphClasses(None, None, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an anchor name and return a tuple that specifies whether the anchor is a mark anchor or not.", "response": "def parseAnchorName(\n    anchorName,\n    markPrefix=MARK_PREFIX,\n    ligaSeparator=LIGA_SEPARATOR,\n    ligaNumRE=LIGA_NUM_RE,\n    ignoreRE=None,\n):\n    \"\"\"Parse anchor name and return a tuple that specifies:\n    1) whether the anchor is a \"mark\" anchor (bool);\n    2) the \"key\" name of the anchor, i.e. the name after stripping all the\n       prefixes and suffixes, which identifies the class it belongs to (str);\n    3) An optional number (int), starting from 1, which identifies that index\n       of the ligature component the anchor refers to.\n\n    The 'ignoreRE' argument is an optional regex pattern (str) identifying\n    sub-strings in the anchor name that should be ignored when parsing the\n    three elements above.\n    \"\"\"\n    number = None\n    if ignoreRE is not None:\n        anchorName = re.sub(ignoreRE, \"\", anchorName)\n\n    m = ligaNumRE.match(anchorName)\n    if not m:\n        key = anchorName\n    else:\n        number = m.group(1)\n        key = anchorName.rstrip(number)\n        separator = ligaSeparator\n        if key.endswith(separator):\n            assert separator\n            key = key[: -len(separator)]\n            number = int(number)\n        else:\n            # not a valid ligature anchor name\n            key = anchorName\n            number = None\n\n    if anchorName.startswith(markPrefix) and key:\n        if number is not None:\n            raise ValueError(\"mark anchor cannot be numbered: %r\" % anchorName)\n        isMark = True\n        key = key[len(markPrefix) :]\n        if not key:\n            raise ValueError(\"mark anchor key is nil: %r\" % anchorName)\n    else:\n        isMark = False\n\n    return isMark, key, number"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef styleMapStyleNameFallback(info):\n    styleName = getAttrWithFallback(info, \"openTypeNamePreferredSubfamilyName\")\n    if styleName is None:\n        styleName = \"regular\"\n    elif styleName.strip().lower() not in _styleMapStyleNames:\n        styleName = \"regular\"\n    else:\n        styleName = styleName.strip().lower()\n    return styleName", "response": "Fallback to *openTypeNamePreferredSubfamilyName* if\n    it is one of *regular*, *bold*, *italic*, *bold italic*, otherwise\n    fallback to *regular*."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef openTypeNameUniqueIDFallback(info):\n    version = getAttrWithFallback(info, \"openTypeNameVersion\").replace(\"Version \", \"\")\n    vendor = getAttrWithFallback(info, \"openTypeOS2VendorID\")\n    fontName = getAttrWithFallback(info, \"postscriptFontName\")\n    return \"%s;%s;%s\" % (version, vendor, fontName)", "response": "Fallback to openTypeNameVersion ; openTypeOS2VendorID ; postscriptFontName."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAttrWithFallback(info, attr):\n    if hasattr(info, attr) and getattr(info, attr) is not None:\n        value = getattr(info, attr)\n    else:\n        if attr in specialFallbacks:\n            value = specialFallbacks[attr](info)\n        else:\n            value = staticFallbackData[attr]\n    return value", "response": "Get the value for the attribute from the object info."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef preflightInfo(info):\n    missingRequired = set()\n    missingRecommended = set()\n    for attr in requiredAttributes:\n        if not hasattr(info, attr) or getattr(info, attr) is None:\n            missingRequired.add(attr)\n    for attr in recommendedAttributes:\n        if not hasattr(info, attr) or getattr(info, attr) is None:\n            missingRecommended.add(attr)\n    return dict(missingRequired=missingRequired, missingRecommended=missingRecommended)", "response": "Returns a dictionary containing two items for each\n   . The value for each\n    item will be a list of info attribute names. The value for each\n    item will be a list of info attribute names."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, user, status=None, symmetrical=False):\n        if not status:\n            status = RelationshipStatus.objects.following()\n\n        relationship, created = Relationship.objects.get_or_create(\n            from_user=self.instance,\n            to_user=user,\n            status=status,\n            site=Site.objects.get_current()\n        )\n\n        if symmetrical:\n            return (relationship, user.relationships.add(self.instance, status, False))\n        else:\n            return relationship", "response": "Add a relationship from one user to another with the given status."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a relationship from one user to another with the same caveats and behavior as adding a relationship.", "response": "def remove(self, user, status=None, symmetrical=False):\n        \"\"\"\n        Remove a relationship from one user to another, with the same caveats\n        and behavior as adding a relationship.\n        \"\"\"\n        if not status:\n            status = RelationshipStatus.objects.following()\n\n        res = Relationship.objects.filter(\n            from_user=self.instance,\n            to_user=user,\n            status=status,\n            site__pk=settings.SITE_ID\n        ).delete()\n\n        if symmetrical:\n            return (res, user.relationships.remove(self.instance, status, False))\n        else:\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_relationships(self, status, symmetrical=False):\n        query = self._get_from_query(status)\n\n        if symmetrical:\n            query.update(self._get_to_query(status))\n\n        return User.objects.filter(**query)", "response": "Returns a QuerySet of user objects with which the given user has a relationship."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef only_to(self, status):\n        from_relationships = self.get_relationships(status)\n        to_relationships = self.get_related_to(status)\n        return to_relationships.exclude(pk__in=from_relationships.values_list('pk'))", "response": "Returns a QuerySet of users who have created a relationship to\n        the given user but which the given user has not reciprocated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning whether or not a relationship exists between the given user and the given status.", "response": "def exists(self, user, status=None, symmetrical=False):\n        \"\"\"\n        Returns boolean whether or not a relationship exists between the given\n        users.  An optional :class:`RelationshipStatus` instance can be specified.\n        \"\"\"\n        query = dict(\n            to_users__from_user=self.instance,\n            to_users__to_user=user,\n            to_users__site__pk=settings.SITE_ID,\n        )\n\n        if status:\n            query.update(to_users__status=status)\n\n        if symmetrical:\n            query.update(\n                from_users__to_user=self.instance,\n                from_users__from_user=user,\n                from_users__site__pk=settings.SITE_ID\n            )\n\n            if status:\n                query.update(from_users__status=status)\n\n        return User.objects.filter(**query).exists()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking the final glyph order for a font.", "response": "def makeOfficialGlyphOrder(font, glyphOrder=None):\n    \"\"\" Make the final glyph order for 'font'.\n\n    If glyphOrder is None, try getting the font.glyphOrder list.\n    If not explicit glyphOrder is defined, sort glyphs alphabetically.\n\n    If \".notdef\" glyph is present in the font, force this to always be\n    the first glyph (at index 0).\n    \"\"\"\n    if glyphOrder is None:\n        glyphOrder = getattr(font, \"glyphOrder\", ())\n    names = set(font.keys())\n    order = []\n    if \".notdef\" in names:\n        names.remove(\".notdef\")\n        order.append(\".notdef\")\n    for name in glyphOrder:\n        if name not in names:\n            continue\n        names.remove(name)\n        order.append(name)\n    order.extend(sorted(names))\n    return order"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies contours from parent to composite including nested components.", "response": "def deepCopyContours(\n    glyphSet, parent, composite, transformation, specificComponents=None\n):\n    \"\"\"Copy contours from component to parent, including nested components.\n\n    specificComponent: an optional list of glyph name strings. If not passed or\n    None, decompose all components of a glyph unconditionally and completely. If\n    passed, only completely decompose components whose baseGlyph is in the list.\n    \"\"\"\n\n    for nestedComponent in composite.components:\n        # Because this function works recursively, test at each turn if we are going to\n        # recurse into a specificComponent. If so, set the specificComponents argument\n        # to None so we unconditionally decompose the possibly nested component\n        # completely.\n        specificComponentsEffective = specificComponents\n        if specificComponentsEffective:\n            if nestedComponent.baseGlyph not in specificComponentsEffective:\n                continue\n            else:\n                specificComponentsEffective = None\n\n        try:\n            nestedBaseGlyph = glyphSet[nestedComponent.baseGlyph]\n        except KeyError:\n            logger.warning(\n                \"dropping non-existent component '%s' in glyph '%s'\",\n                nestedComponent.baseGlyph,\n                parent.name,\n            )\n        else:\n            deepCopyContours(\n                glyphSet,\n                parent,\n                nestedBaseGlyph,\n                transformation.transform(nestedComponent.transformation),\n                specificComponents=specificComponentsEffective,\n            )\n\n    # Check if there are any contours to copy before instantiating pens.\n    if composite != parent and len(composite):\n        if transformation == Identity:\n            pen = parent.getPen()\n        else:\n            pen = TransformPen(parent.getPen(), transformation)\n            # if the transformation has a negative determinant, it will\n            # reverse the contour direction of the component\n            xx, xy, yx, yy = transformation[:4]\n            if xx * yy - xy * yx < 0:\n                pen = ReverseContourPen(pen)\n\n        for contour in composite:\n            contour.draw(pen)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeUnicodeToGlyphNameMapping(font, glyphOrder=None):\n    if glyphOrder is None:\n        glyphOrder = makeOfficialGlyphOrder(font)\n    mapping = {}\n    for glyphName in glyphOrder:\n        glyph = font[glyphName]\n        unicodes = glyph.unicodes\n        for uni in unicodes:\n            if uni not in mapping:\n                mapping[uni] = glyphName\n            else:\n                from ufo2ft.errors import InvalidFontData\n\n                InvalidFontData(\n                    \"cannot map '%s' to U+%04X; already mapped to '%s'\"\n                    % (glyphName, uni, mapping[uni])\n                )\n    return mapping", "response": "Make a unicode glyph name mapping for this font."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compileGSUB(featureFile, glyphOrder):\n    font = ttLib.TTFont()\n    font.setGlyphOrder(glyphOrder)\n    addOpenTypeFeatures(font, featureFile, tables={\"GSUB\"})\n    return font.get(\"GSUB\")", "response": "Compile and return a GSUB table from featureFile using the given glyphOrder."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef closeGlyphsOverGSUB(gsub, glyphs):\n    subsetter = subset.Subsetter()\n    subsetter.glyphs = glyphs\n    gsub.closure_glyphs(subsetter)", "response": "Use the GSUB subsetter to perform a closure over the GSUB table\n    given the set of glyphs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclassify the Unicode codepoints of a Unicode character and returns a dictionary of glyph sets associated with that character.", "response": "def classifyGlyphs(unicodeFunc, cmap, gsub=None):\n    \"\"\" 'unicodeFunc' is a callable that takes a Unicode codepoint and\n    returns a string denoting some Unicode property associated with the\n    given character (or None if a character is considered 'neutral').\n    'cmap' is a dictionary mapping Unicode codepoints to glyph names.\n    'gsub' is an (optional) fonttools GSUB table object, used to find all\n    the glyphs that are \"reachable\" via substitutions from the initial\n    sets of glyphs defined in the cmap.\n\n    Returns a dictionary of glyph sets associated with the given Unicode\n    properties.\n    \"\"\"\n    glyphSets = {}\n    neutralGlyphs = set()\n    for uv, glyphName in cmap.items():\n        key = unicodeFunc(uv)\n        if key is None:\n            neutralGlyphs.add(glyphName)\n        else:\n            glyphSets.setdefault(key, set()).add(glyphName)\n\n    if gsub is not None:\n        if neutralGlyphs:\n            closeGlyphsOverGSUB(gsub, neutralGlyphs)\n\n        for glyphs in glyphSets.values():\n            s = glyphs | neutralGlyphs\n            closeGlyphsOverGSUB(gsub, s)\n            glyphs.update(s - neutralGlyphs)\n\n    return glyphSets"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks Unicode s ScriptExtension property for unicode codepoint uv and return True if it intersects with the set of scripts provided.", "response": "def unicodeInScripts(uv, scripts):\n    \"\"\" Check UnicodeData's ScriptExtension property for unicode codepoint\n    'uv' and return True if it intersects with the set of 'scripts' provided,\n    False if it does not intersect.\n    Return None for 'Common' script ('Zyyy').\n    \"\"\"\n    sx = unicodedata.script_extension(unichr(uv))\n    if \"Zyyy\" in sx:\n        return None\n    return not sx.isdisjoint(scripts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the codepage ranges for a set of Unicode codepoints.", "response": "def calcCodePageRanges(unicodes):\n    \"\"\" Given a set of Unicode codepoints (integers), calculate the\n    corresponding OS/2 CodePage range bits.\n    This is a direct translation of FontForge implementation:\n    https://github.com/fontforge/fontforge/blob/7b2c074/fontforge/tottf.c#L3158\n    \"\"\"\n    codepageRanges = set()\n\n    chars = [unichr(u) for u in unicodes]\n\n    hasAscii = set(range(0x20, 0x7E)).issubset(unicodes)\n    hasLineart = \"\u2524\" in chars\n\n    for char in chars:\n        if char == \"\u00de\" and hasAscii:\n            codepageRanges.add(0)           # Latin 1\n        elif char == \"\u013d\" and hasAscii:\n            codepageRanges.add(1)           # Latin 2: Eastern Europe\n            if hasLineart:\n                codepageRanges.add(58)      # Latin 2\n        elif char == \"\u0411\":\n            codepageRanges.add(2)           # Cyrillic\n            if \"\u0405\" in chars and hasLineart:\n                codepageRanges.add(57)      # IBM Cyrillic\n            if \"\u255c\" in chars and hasLineart:\n                codepageRanges.add(49)      # MS-DOS Russian\n        elif char == \"\u0386\":\n            codepageRanges.add(3)           # Greek\n            if hasLineart and \"\u00bd\" in chars:\n                codepageRanges.add(48)      # IBM Greek\n            if hasLineart and \"\u221a\" in chars:\n                codepageRanges.add(60)      # Greek, former 437 G\n        elif char == \"\u0130\" and hasAscii:\n            codepageRanges.add(4)           # Turkish\n            if hasLineart:\n                codepageRanges.add(56)      # IBM turkish\n        elif char == \"\u05d0\":\n            codepageRanges.add(5)           # Hebrew\n            if hasLineart and \"\u221a\" in chars:\n                codepageRanges.add(53)      # Hebrew\n        elif char == \"\u0631\":\n            codepageRanges.add(6)           # Arabic\n            if \"\u221a\" in chars:\n                codepageRanges.add(51)      # Arabic\n            if hasLineart:\n                codepageRanges.add(61)      # Arabic; ASMO 708\n        elif char == \"\u0157\" and hasAscii:\n            codepageRanges.add(7)           # Windows Baltic\n            if hasLineart:\n                codepageRanges.add(59)      # MS-DOS Baltic\n        elif char == \"\u20ab\" and hasAscii:\n            codepageRanges.add(8)           # Vietnamese\n        elif char == \"\u0e45\":\n            codepageRanges.add(16)          # Thai\n        elif char == \"\u30a8\":\n            codepageRanges.add(17)          # JIS/Japan\n        elif char == \"\u3105\":\n            codepageRanges.add(18)          # Chinese: Simplified chars\n        elif char == \"\u3131\":\n            codepageRanges.add(19)          # Korean wansung\n        elif char == \"\u592e\":\n            codepageRanges.add(20)          # Chinese: Traditional chars\n        elif char == \"\uacf4\":\n            codepageRanges.add(21)          # Korean Johab\n        elif char == \"\u2665\" and hasAscii:\n            codepageRanges.add(30)          # OEM Character Set\n        # TODO: Symbol bit has a special meaning (check the spec), we need\n        # to confirm if this is wanted by default.\n        # elif unichr(0xF000) <= char <= unichr(0xF0FF):\n        #    codepageRanges.add(31)          # Symbol Character Set\n        elif char == \"\u00fe\" and hasAscii and hasLineart:\n            codepageRanges.add(54)          # MS-DOS Icelandic\n        elif char == \"\u255a\" and hasAscii:\n            codepageRanges.add(62)          # WE/Latin 1\n            codepageRanges.add(63)          # US\n        elif hasAscii and hasLineart and \"\u221a\" in chars:\n            if char == \"\u00c5\":\n                codepageRanges.add(50)      # MS-DOS Nordic\n            elif char == \"\u00e9\":\n                codepageRanges.add(52)      # MS-DOS Canadian French\n            elif char == \"\u00f5\":\n                codepageRanges.add(55)      # MS-DOS Portuguese\n\n    if hasAscii and \"\u2030\" in chars and \"\u2211\" in chars:\n        codepageRanges.add(29)              # Macintosh Character Set (US Roman)\n\n    # when no codepage ranges can be enabled, fall back to enabling bit 0\n    # (Latin 1) so that the font works in MS Word:\n    # https://github.com/googlei18n/fontmake/issues/468\n    if not codepageRanges:\n        codepageRanges.add(0)\n\n    return codepageRanges"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_layer(cls, font, layerName=None, copy=False, skipExportGlyphs=None):\n        if layerName is not None:\n            layer = font.layers[layerName]\n        else:\n            layer = font.layers.defaultLayer\n\n        if copy:\n            self = _copyLayer(layer, obj_type=cls)\n            self.lib = deepcopy(layer.lib)\n        else:\n            self = cls((g.name, g) for g in layer)\n            self.lib = layer.lib\n\n        # If any glyphs in the skipExportGlyphs list are used as components, decompose\n        # them in the containing glyphs...\n        if skipExportGlyphs:\n            for glyph in self.values():\n                if any(c.baseGlyph in skipExportGlyphs for c in glyph.components):\n                    deepCopyContours(self, glyph, glyph, Transform(), skipExportGlyphs)\n                    if hasattr(glyph, \"removeComponent\"):  # defcon\n                        for c in [\n                            component\n                            for component in glyph.components\n                            if component.baseGlyph in skipExportGlyphs\n                        ]:\n                            glyph.removeComponent(c)\n                    else:  # ufoLib2\n                        glyph.components[:] = [\n                            c\n                            for c in glyph.components\n                            if c.baseGlyph not in skipExportGlyphs\n                        ]\n            # ... and then remove them from the glyph set, if even present.\n            for glyph_name in skipExportGlyphs:\n                if glyph_name in self:\n                    del self[glyph_name]\n\n        self.name = layer.name if layerName is not None else None\n        return self", "response": "Return a mapping of glyph names to glyph objects from font."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses OpenType layout features in the UFO and return a feaLib. ast. FeatureFile instance.", "response": "def parseLayoutFeatures(font):\n    \"\"\" Parse OpenType layout features in the UFO and return a\n    feaLib.ast.FeatureFile instance.\n    \"\"\"\n    featxt = tounicode(font.features.text or \"\", \"utf-8\")\n    if not featxt:\n        return ast.FeatureFile()\n    buf = UnicodeIO(featxt)\n    # the path is used by the lexer to resolve 'include' statements\n    # and print filename in error messages. For the UFO spec, this\n    # should be the path of the UFO, not the inner features.fea:\n    # https://github.com/unified-font-object/ufo-spec/issues/55\n    ufoPath = font.path\n    if ufoPath is not None:\n        buf.name = ufoPath\n    glyphNames = set(font.keys())\n    try:\n        parser = Parser(buf, glyphNames)\n        doc = parser.parse()\n    except IncludedFeaNotFound as e:\n        if ufoPath and os.path.exists(os.path.join(ufoPath, e.args[0])):\n            logger.warning(\n                \"Please change the file name in the include(...); \"\n                \"statement to be relative to the UFO itself, \"\n                \"instead of relative to the 'features.fea' file \"\n                \"contained in it.\"\n            )\n        raise\n    return doc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the list of feature writers as specified in the UFO lib.", "response": "def initFeatureWriters(self, featureWriters=None):\n        \"\"\" Initialize feature writer classes as specified in the UFO lib.\n        If none are defined in the UFO, the default feature writers are used:\n        currently, KernFeatureWriter and MarkFeatureWriter.\n        The 'featureWriters' argument can be used to override these.\n        The method sets the `self.featureWriters` attribute with the list of\n        writers.\n\n        Note that the writers that generate GSUB features are placed first in\n        this list, before all others. This is because the GSUB table may be\n        used in the subsequent feature writers to resolve substitutions from\n        glyphs with unicodes to their alternates.\n        \"\"\"\n        if featureWriters is None:\n            featureWriters = loadFeatureWriters(self.ufo)\n            if featureWriters is None:\n                featureWriters = self.defaultFeatureWriters\n\n        gsubWriters = []\n        others = []\n        for writer in featureWriters:\n            if isclass(writer):\n                writer = writer()\n            if writer.tableTag == \"GSUB\":\n                gsubWriters.append(writer)\n            else:\n                others.append(writer)\n\n        self.featureWriters = gsubWriters + others"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes the features source. **This should not be called externally.** Subclasses may override this method to handle the file creation in a different way if desired.", "response": "def setupFeatures(self):\n        \"\"\"\n        Make the features source.\n\n        **This should not be called externally.** Subclasses\n        may override this method to handle the file creation\n        in a different way if desired.\n        \"\"\"\n        if self.featureWriters:\n            featureFile = parseLayoutFeatures(self.ufo)\n\n            for writer in self.featureWriters:\n                writer.write(self.ufo, featureFile, compiler=self)\n\n            # stringify AST to get correct line numbers in error messages\n            self.features = featureFile.asFea()\n        else:\n            # no featureWriters, simply read existing features' text\n            self.features = tounicode(self.ufo.features.text or \"\", \"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef buildTables(self):\n\n        if not self.features:\n            return\n\n        # the path is used by the lexer to follow 'include' statements;\n        # if we generated some automatic features, includes have already been\n        # resolved, and we work from a string which does't exist on disk\n        path = self.ufo.path if not self.featureWriters else None\n        try:\n            addOpenTypeFeaturesFromString(\n                self.ttFont, self.features, filename=path\n            )\n        except FeatureLibError:\n            if path is None:\n                # if compilation fails, create temporary file for inspection\n                data = tobytes(self.features, encoding=\"utf-8\")\n                with NamedTemporaryFile(delete=False) as tmp:\n                    tmp.write(data)\n                logger.error(\n                    \"Compilation failed! Inspect temporary file: %r\", tmp.name\n                )\n            raise", "response": "Builds OpenType feature tables from the source file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maxCtxFont(font):\n\n    maxCtx = 0\n    for tag in ('GSUB', 'GPOS'):\n        if tag not in font:\n            continue\n        table = font[tag].table\n        if table.LookupList is None:\n            continue\n        for lookup in table.LookupList.Lookup:\n            for st in lookup.SubTable:\n                maxCtx = maxCtxSubtable(maxCtx, tag, lookup.LookupType, st)\n    return maxCtx", "response": "Calculate the usMaxContext value for an entire font."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef maxCtxSubtable(maxCtx, tag, lookupType, st):\n\n    # single positioning, single / multiple substitution\n    if (tag == 'GPOS' and lookupType == 1) or (\n        tag == 'GSUB' and lookupType in (1, 2, 3)):\n        maxCtx = max(maxCtx, 1)\n\n    # pair positioning\n    elif tag == 'GPOS' and lookupType == 2:\n        maxCtx = max(maxCtx, 2)\n\n    # ligatures\n    elif tag == 'GSUB' and lookupType == 4:\n        for ligatures in st.ligatures.values():\n            for ligature in ligatures:\n                maxCtx = max(maxCtx, ligature.CompCount)\n\n    # context\n    elif (tag == 'GPOS' and lookupType == 7) or (\n          tag == 'GSUB' and lookupType == 5):\n        maxCtx = maxCtxContextualSubtable(\n            maxCtx, st, 'Pos' if tag == 'GPOS' else 'Sub')\n\n    # chained context\n    elif (tag == 'GPOS' and lookupType == 8) or (\n          tag == 'GSUB' and lookupType == 6):\n        maxCtx = maxCtxContextualSubtable(\n            maxCtx, st, 'Pos' if tag == 'GPOS' else 'Sub', 'Chain')\n\n    # extensions\n    elif (tag == 'GPOS' and lookupType == 9) or (\n          tag == 'GSUB' and lookupType == 7):\n        maxCtx = maxCtxSubtable(\n            maxCtx, tag, st.ExtensionLookupType, st.ExtSubTable)\n\n    # reverse-chained context\n    elif tag == 'GSUB' and lookupType == 8:\n        maxCtx = maxCtxContextualRule(maxCtx, st, 'Reverse')\n\n    return maxCtx", "response": "Calculate the max context for a single or multiple lookup tables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate usMaxContext based on a contextual feature subtable.", "response": "def maxCtxContextualSubtable(maxCtx, st, ruleType, chain=''):\n    \"\"\"Calculate usMaxContext based on a contextual feature subtable.\"\"\"\n\n    if st.Format == 1:\n        for ruleset in getattr(st, '%s%sRuleSet' % (chain, ruleType)):\n            if ruleset is None:\n                continue\n            for rule in getattr(ruleset, '%s%sRule' % (chain, ruleType)):\n                if rule is None:\n                    continue\n                maxCtx = maxCtxContextualRule(maxCtx, rule, chain)\n\n    elif st.Format == 2:\n        for ruleset in getattr(st, '%s%sClassSet' % (chain, ruleType)):\n            if ruleset is None:\n                continue\n            for rule in getattr(ruleset, '%s%sClassRule' % (chain, ruleType)):\n                if rule is None:\n                    continue\n                maxCtx = maxCtxContextualRule(maxCtx, rule, chain)\n\n    elif st.Format == 3:\n        maxCtx = maxCtxContextualRule(maxCtx, st, chain)\n\n    return maxCtx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maxCtxContextualRule(maxCtx, st, chain):\n\n    if not chain:\n        return max(maxCtx, st.GlyphCount)\n    elif chain == 'Reverse':\n        return max(maxCtx, st.GlyphCount + st.LookAheadGlyphCount)\n    return max(maxCtx, st.InputGlyphCount + st.LookAheadGlyphCount)", "response": "Calculate usMaxContext based on a contextual feature rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compileOTF(\n    ufo,\n    preProcessorClass=OTFPreProcessor,\n    outlineCompilerClass=OutlineOTFCompiler,\n    featureCompilerClass=None,\n    featureWriters=None,\n    glyphOrder=None,\n    useProductionNames=None,\n    optimizeCFF=CFFOptimization.SUBROUTINIZE,\n    roundTolerance=None,\n    removeOverlaps=False,\n    overlapsBackend=None,\n    inplace=False,\n    layerName=None,\n    skipExportGlyphs=None,\n    _tables=None,\n):\n    \"\"\"Create FontTools CFF font from a UFO.\n\n    *removeOverlaps* performs a union operation on all the glyphs' contours.\n\n    *optimizeCFF* (int) defines whether the CFF charstrings should be\n      specialized and subroutinized. By default both optimization are enabled.\n      A value of 0 disables both; 1 only enables the specialization; 2 (default)\n      does both specialization and subroutinization.\n\n    *roundTolerance* (float) controls the rounding of point coordinates.\n      It is defined as the maximum absolute difference between the original\n      float and the rounded integer value.\n      By default, all floats are rounded to integer (tolerance 0.5); a value\n      of 0 completely disables rounding; values in between only round floats\n      which are close to their integral part within the tolerated range.\n\n    *featureWriters* argument is a list of BaseFeatureWriter subclasses or\n      pre-initialized instances. Features will be written by each feature\n      writer in the given order. If featureWriters is None, the default\n      feature writers [KernFeatureWriter, MarkFeatureWriter] are used.\n\n    *useProductionNames* renames glyphs in TrueType 'post' or OpenType 'CFF '\n      tables based on the 'public.postscriptNames' mapping in the UFO lib,\n      if present. Otherwise, uniXXXX names are generated from the glyphs'\n      unicode values. The default value (None) will first check if the UFO lib\n      has the 'com.github.googlei18n.ufo2ft.useProductionNames' key. If this\n      is missing or True (default), the glyphs are renamed. Set to False\n      to keep the original names.\n\n    **inplace** (bool) specifies whether the filters should modify the input\n      UFO's glyphs, a copy should be made first.\n\n    *layerName* specifies which layer should be compiled. When compiling something\n    other than the default layer, feature compilation is skipped.\n\n    *skipExportGlyphs* is a list or set of glyph names to not be exported to the\n    final font. If these glyphs are used as components in any other glyph, those\n    components get decomposed. If the parameter is not passed in, the UFO's\n    \"public.skipExportGlyphs\" lib key will be consulted. If it doesn't exist,\n    all glyphs are exported. UFO groups and kerning will be pruned of skipped\n    glyphs.\n    \"\"\"\n    logger.info(\"Pre-processing glyphs\")\n\n    if skipExportGlyphs is None:\n        skipExportGlyphs = ufo.lib.get(\"public.skipExportGlyphs\", [])\n\n    preProcessor = preProcessorClass(\n        ufo,\n        inplace=inplace,\n        removeOverlaps=removeOverlaps,\n        overlapsBackend=overlapsBackend,\n        layerName=layerName,\n        skipExportGlyphs=skipExportGlyphs,\n    )\n    glyphSet = preProcessor.process()\n\n    logger.info(\"Building OpenType tables\")\n    optimizeCFF = CFFOptimization(optimizeCFF)\n    outlineCompiler = outlineCompilerClass(\n        ufo,\n        glyphSet=glyphSet,\n        glyphOrder=glyphOrder,\n        roundTolerance=roundTolerance,\n        optimizeCFF=optimizeCFF >= CFFOptimization.SPECIALIZE,\n        tables=_tables,\n    )\n    otf = outlineCompiler.compile()\n\n    # Only the default layer is likely to have all glyphs used in feature code.\n    if layerName is None:\n        compileFeatures(\n            ufo,\n            otf,\n            glyphSet=glyphSet,\n            featureWriters=featureWriters,\n            featureCompilerClass=featureCompilerClass,\n        )\n\n    postProcessor = PostProcessor(otf, ufo, glyphSet=glyphSet)\n    otf = postProcessor.process(\n        useProductionNames,\n        optimizeCFF=optimizeCFF >= CFFOptimization.SUBROUTINIZE,\n    )\n\n    return otf", "response": "Create FontTools CFF font from a UFO."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compileTTF(\n    ufo,\n    preProcessorClass=TTFPreProcessor,\n    outlineCompilerClass=OutlineTTFCompiler,\n    featureCompilerClass=None,\n    featureWriters=None,\n    glyphOrder=None,\n    useProductionNames=None,\n    convertCubics=True,\n    cubicConversionError=None,\n    reverseDirection=True,\n    rememberCurveType=True,\n    removeOverlaps=False,\n    overlapsBackend=None,\n    inplace=False,\n    layerName=None,\n    skipExportGlyphs=None,\n):\n    \"\"\"Create FontTools TrueType font from a UFO.\n\n    *removeOverlaps* performs a union operation on all the glyphs' contours.\n\n    *convertCubics* and *cubicConversionError* specify how the conversion from cubic\n    to quadratic curves should be handled.\n\n    *layerName* specifies which layer should be compiled. When compiling something\n    other than the default layer, feature compilation is skipped.\n\n    *skipExportGlyphs* is a list or set of glyph names to not be exported to the\n    final font. If these glyphs are used as components in any other glyph, those\n    components get decomposed. If the parameter is not passed in, the UFO's\n    \"public.skipExportGlyphs\" lib key will be consulted. If it doesn't exist,\n    all glyphs are exported. UFO groups and kerning will be pruned of skipped\n    glyphs.\n    \"\"\"\n    logger.info(\"Pre-processing glyphs\")\n\n    if skipExportGlyphs is None:\n        skipExportGlyphs = ufo.lib.get(\"public.skipExportGlyphs\", [])\n\n    preProcessor = preProcessorClass(\n        ufo,\n        inplace=inplace,\n        removeOverlaps=removeOverlaps,\n        overlapsBackend=overlapsBackend,\n        convertCubics=convertCubics,\n        conversionError=cubicConversionError,\n        reverseDirection=reverseDirection,\n        rememberCurveType=rememberCurveType,\n        layerName=layerName,\n        skipExportGlyphs=skipExportGlyphs,\n    )\n    glyphSet = preProcessor.process()\n\n    logger.info(\"Building OpenType tables\")\n    outlineCompiler = outlineCompilerClass(\n        ufo, glyphSet=glyphSet, glyphOrder=glyphOrder\n    )\n    otf = outlineCompiler.compile()\n\n    # Only the default layer is likely to have all glyphs used in feature code.\n    if layerName is None:\n        compileFeatures(\n            ufo,\n            otf,\n            glyphSet=glyphSet,\n            featureWriters=featureWriters,\n            featureCompilerClass=featureCompilerClass,\n        )\n\n    postProcessor = PostProcessor(otf, ufo, glyphSet=glyphSet)\n    otf = postProcessor.process(useProductionNames)\n\n    return otf", "response": "Compile a TrueType font from a UFO."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate FontTools TrueType fonts from a list of UFOs with interpolatable outlines.", "response": "def compileInterpolatableTTFs(\n    ufos,\n    preProcessorClass=TTFInterpolatablePreProcessor,\n    outlineCompilerClass=OutlineTTFCompiler,\n    featureCompilerClass=None,\n    featureWriters=None,\n    glyphOrder=None,\n    useProductionNames=None,\n    cubicConversionError=None,\n    reverseDirection=True,\n    inplace=False,\n    layerNames=None,\n    skipExportGlyphs=None,\n):\n    \"\"\"Create FontTools TrueType fonts from a list of UFOs with interpolatable\n    outlines. Cubic curves are converted compatibly to quadratic curves using\n    the Cu2Qu conversion algorithm.\n\n    Return an iterator object that yields a TTFont instance for each UFO.\n\n    *layerNames* refers to the layer names to use glyphs from in the order of\n    the UFOs in *ufos*. By default, this is a list of `[None]` times the number\n    of UFOs, i.e. using the default layer from all the UFOs.\n\n    When the layerName is not None for a given UFO, the corresponding TTFont object\n    will contain only a minimum set of tables (\"head\", \"hmtx\", \"glyf\", \"loca\", \"maxp\",\n    \"post\" and \"vmtx\"), and no OpenType layout tables.\n\n    *skipExportGlyphs* is a list or set of glyph names to not be exported to the\n    final font. If these glyphs are used as components in any other glyph, those\n    components get decomposed. If the parameter is not passed in, the union of\n    all UFO's \"public.skipExportGlyphs\" lib keys will be used. If they don't\n    exist, all glyphs are exported. UFO groups and kerning will be pruned of\n    skipped glyphs.\n    \"\"\"\n    from ufo2ft.util import _LazyFontName\n\n    if layerNames is None:\n        layerNames = [None] * len(ufos)\n    assert len(ufos) == len(layerNames)\n\n    if skipExportGlyphs is None:\n        skipExportGlyphs = set()\n        for ufo in ufos:\n            skipExportGlyphs.update(ufo.lib.get(\"public.skipExportGlyphs\", []))\n\n    logger.info(\"Pre-processing glyphs\")\n    preProcessor = preProcessorClass(\n        ufos,\n        inplace=inplace,\n        conversionError=cubicConversionError,\n        reverseDirection=reverseDirection,\n        layerNames=layerNames,\n        skipExportGlyphs=skipExportGlyphs,\n    )\n    glyphSets = preProcessor.process()\n\n    for ufo, glyphSet, layerName in zip(ufos, glyphSets, layerNames):\n        fontName = _LazyFontName(ufo)\n        if layerName is not None:\n            logger.info(\"Building OpenType tables for %s-%s\", fontName, layerName)\n        else:\n            logger.info(\"Building OpenType tables for %s\", fontName)\n\n        outlineCompiler = outlineCompilerClass(\n            ufo,\n            glyphSet=glyphSet,\n            glyphOrder=glyphOrder,\n            tables=SPARSE_TTF_MASTER_TABLES if layerName else None,\n        )\n        ttf = outlineCompiler.compile()\n\n        # Only the default layer is likely to have all glyphs used in feature\n        # code.\n        if layerName is None:\n            compileFeatures(\n                ufo,\n                ttf,\n                glyphSet=glyphSet,\n                featureWriters=featureWriters,\n                featureCompilerClass=featureCompilerClass,\n            )\n\n        postProcessor = PostProcessor(ttf, ufo, glyphSet=glyphSet)\n        ttf = postProcessor.process(useProductionNames)\n\n        if layerName is not None:\n            # for sparse masters (i.e. containing only a subset of the glyphs), we\n            # need to include the post table in order to store glyph names, so that\n            # fontTools.varLib can interpolate glyphs with same name across masters.\n            # However we want to prevent the underlinePosition/underlineThickness\n            # fields in such sparse masters to be included when computing the deltas\n            # for the MVAR table. Thus, we set them to this unlikely, limit value\n            # (-36768) which is a signal varLib should ignore them when building MVAR.\n            ttf[\"post\"].underlinePosition = -0x8000\n            ttf[\"post\"].underlineThickness = -0x8000\n\n        yield ttf"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compileInterpolatableTTFsFromDS(\n    designSpaceDoc,\n    preProcessorClass=TTFInterpolatablePreProcessor,\n    outlineCompilerClass=OutlineTTFCompiler,\n    featureCompilerClass=None,\n    featureWriters=None,\n    glyphOrder=None,\n    useProductionNames=None,\n    cubicConversionError=None,\n    reverseDirection=True,\n    inplace=False,\n):\n    \"\"\"Create FontTools TrueType fonts from the DesignSpaceDocument UFO sources\n    with interpolatable outlines. Cubic curves are converted compatibly to\n    quadratic curves using the Cu2Qu conversion algorithm.\n\n    If the Designspace contains a \"public.skipExportGlyphs\" lib key, these\n    glyphs will not be exported to the final font. If these glyphs are used as\n    components in any other glyph, those components get decomposed. If the lib\n    key doesn't exist in the Designspace, all glyphs are exported (keys in\n    individual UFOs are ignored). UFO groups and kerning will be pruned of\n    skipped glyphs.\n\n    The DesignSpaceDocument should contain SourceDescriptor objects with 'font'\n    attribute set to an already loaded defcon.Font object (or compatible UFO\n    Font class). If 'font' attribute is unset or None, an AttributeError exception\n    is thrown.\n\n    Return a copy of the DesignSpaceDocument object (or the same one if\n    inplace=True) with the source's 'font' attribute set to the corresponding\n    TTFont instance.\n\n    For sources that have the 'layerName' attribute defined, the corresponding TTFont\n    object will contain only a minimum set of tables (\"head\", \"hmtx\", \"glyf\", \"loca\",\n    \"maxp\", \"post\" and \"vmtx\"), and no OpenType layout tables.\n    \"\"\"\n    ufos, layerNames = [], []\n    for source in designSpaceDoc.sources:\n        if source.font is None:\n            raise AttributeError(\n                \"designspace source '%s' is missing required 'font' attribute\"\n                % getattr(source, \"name\", \"<Unknown>\")\n            )\n        ufos.append(source.font)\n        # 'layerName' is None for the default layer\n        layerNames.append(source.layerName)\n\n    skipExportGlyphs = designSpaceDoc.lib.get(\"public.skipExportGlyphs\", [])\n\n    ttfs = compileInterpolatableTTFs(\n        ufos,\n        preProcessorClass=preProcessorClass,\n        outlineCompilerClass=outlineCompilerClass,\n        featureCompilerClass=featureCompilerClass,\n        featureWriters=featureWriters,\n        glyphOrder=glyphOrder,\n        useProductionNames=useProductionNames,\n        cubicConversionError=cubicConversionError,\n        reverseDirection=reverseDirection,\n        inplace=inplace,\n        layerNames=layerNames,\n        skipExportGlyphs=skipExportGlyphs,\n    )\n\n    if inplace:\n        result = designSpaceDoc\n    else:\n        # TODO try a more efficient copy method that doesn't involve (de)serializing\n        result = designSpaceDoc.__class__.fromstring(designSpaceDoc.tostring())\n    for source, ttf in zip(result.sources, ttfs):\n        source.font = ttf\n    return result", "response": "Compiles TrueType FontTools TrueType fonts from a DesignSpaceDocument UFO."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compileInterpolatableOTFsFromDS(\n    designSpaceDoc,\n    preProcessorClass=OTFPreProcessor,\n    outlineCompilerClass=OutlineOTFCompiler,\n    featureCompilerClass=None,\n    featureWriters=None,\n    glyphOrder=None,\n    useProductionNames=None,\n    roundTolerance=None,\n    inplace=False,\n):\n    \"\"\"Create FontTools CFF fonts from the DesignSpaceDocument UFO sources\n    with interpolatable outlines.\n\n    Interpolatable means without subroutinization and specializer optimizations\n    and no removal of overlaps.\n\n    If the Designspace contains a \"public.skipExportGlyphs\" lib key, these\n    glyphs will not be exported to the final font. If these glyphs are used as\n    components in any other glyph, those components get decomposed. If the lib\n    key doesn't exist in the Designspace, all glyphs are exported (keys in\n    individual UFOs are ignored). UFO groups and kerning will be pruned of\n    skipped glyphs.\n\n    The DesignSpaceDocument should contain SourceDescriptor objects with 'font'\n    attribute set to an already loaded defcon.Font object (or compatible UFO\n    Font class). If 'font' attribute is unset or None, an AttributeError exception\n    is thrown.\n\n    Return a copy of the DesignSpaceDocument object (or the same one if\n    inplace=True) with the source's 'font' attribute set to the corresponding\n    TTFont instance.\n\n    For sources that have the 'layerName' attribute defined, the corresponding TTFont\n    object will contain only a minimum set of tables (\"head\", \"hmtx\", \"CFF \", \"maxp\",\n    \"vmtx\" and \"VORG\"), and no OpenType layout tables.\n    \"\"\"\n    for source in designSpaceDoc.sources:\n        if source.font is None:\n            raise AttributeError(\n                \"designspace source '%s' is missing required 'font' attribute\"\n                % getattr(source, \"name\", \"<Unknown>\")\n            )\n\n    skipExportGlyphs = designSpaceDoc.lib.get(\"public.skipExportGlyphs\", [])\n\n    otfs = []\n    for source in designSpaceDoc.sources:\n        otfs.append(\n            compileOTF(\n                ufo=source.font,\n                layerName=source.layerName,\n                preProcessorClass=preProcessorClass,\n                outlineCompilerClass=outlineCompilerClass,\n                featureCompilerClass=featureCompilerClass,\n                featureWriters=featureWriters,\n                glyphOrder=glyphOrder,\n                useProductionNames=useProductionNames,\n                optimizeCFF=CFFOptimization.NONE,\n                roundTolerance=roundTolerance,\n                removeOverlaps=False,\n                overlapsBackend=None,\n                inplace=inplace,\n                skipExportGlyphs=skipExportGlyphs,\n                _tables=SPARSE_OTF_MASTER_TABLES if source.layerName else None,\n            )\n        )\n\n    if inplace:\n        result = designSpaceDoc\n    else:\n        # TODO try a more efficient copy method that doesn't involve (de)serializing\n        result = designSpaceDoc.__class__.fromstring(designSpaceDoc.tostring())\n\n    for source, otf in zip(result.sources, otfs):\n        source.font = otf\n\n    return result", "response": "Compiles Interpolatable UFOs from the given DesignSpaceDocument and returns a new DesignSpaceDocument object with the interpolated UFOs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compileFeatures(\n    ufo,\n    ttFont=None,\n    glyphSet=None,\n    featureWriters=None,\n    featureCompilerClass=None,\n):\n    \"\"\" Compile OpenType Layout features from `ufo` into FontTools OTL tables.\n    If `ttFont` is None, a new TTFont object is created containing the new\n    tables, else the provided `ttFont` is updated with the new tables.\n\n    If no explicit `featureCompilerClass` is provided, the one used will\n    depend on whether the ufo contains any MTI feature files in its 'data'\n    directory (thus the `MTIFeatureCompiler` is used) or not (then the\n    default FeatureCompiler for Adobe FDK features is used).\n\n    If skipExportGlyphs is provided (see description in the ``compile*``\n    functions), the feature compiler will prune groups (removing them if empty)\n    and kerning of the UFO of these glyphs. The feature file is left untouched.\n    \"\"\"\n    if featureCompilerClass is None:\n        if any(\n            fn.startswith(MTI_FEATURES_PREFIX) and fn.endswith(\".mti\")\n            for fn in ufo.data.fileNames\n        ):\n            featureCompilerClass = MtiFeatureCompiler\n        else:\n            featureCompilerClass = FeatureCompiler\n    featureCompiler = featureCompilerClass(\n        ufo, ttFont, glyphSet=glyphSet, featureWriters=featureWriters\n    )\n    return featureCompiler.compile()", "response": "Compile OpenType Layout features from UFO into FontTools OTL tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _propagate_glyph_anchors(glyphSet, composite, processed):\n\n    if composite.name in processed:\n        return\n    processed.add(composite.name)\n\n    if not composite.components:\n        return\n\n    base_components = []\n    mark_components = []\n    anchor_names = set()\n    to_add = {}\n    for component in composite.components:\n        try:\n            glyph = glyphSet[component.baseGlyph]\n        except KeyError:\n            logger.warning(\n                'Anchors not propagated for inexistent component {} '\n                'in glyph {}'.format(component.baseGlyph, composite.name))\n        else:\n            _propagate_glyph_anchors(glyphSet, glyph, processed)\n            if any(a.name.startswith('_') for a in glyph.anchors):\n                mark_components.append(component)\n            else:\n                base_components.append(component)\n                anchor_names |= {a.name for a in glyph.anchors}\n\n    if mark_components and not base_components and _is_ligature_mark(composite):\n        # The composite is a mark that is composed of other marks (E.g.\n        # \"circumflexcomb_tildecomb\"). Promote the mark that is positioned closest\n        # to the origin to a base.\n        try:\n            component = _component_closest_to_origin(mark_components, glyphSet)\n        except Exception as e:\n            raise Exception(\n                \"Error while determining which component of composite \"\n                \"'{}' is the lowest: {}\".format(composite.name, str(e))\n            )\n        mark_components.remove(component)\n        base_components.append(component)\n        glyph = glyphSet[component.baseGlyph]\n        anchor_names |= {a.name for a in glyph.anchors}\n\n    for anchor_name in anchor_names:\n        # don't add if composite glyph already contains this anchor OR any\n        # associated ligature anchors (e.g. \"top_1, top_2\" for \"top\")\n        if not any(a.name.startswith(anchor_name) for a in composite.anchors):\n            _get_anchor_data(to_add, glyphSet, base_components, anchor_name)\n\n    for component in mark_components:\n        _adjust_anchors(to_add, glyphSet, component)\n\n    # we sort propagated anchors to append in a deterministic order\n    for name, (x, y) in sorted(to_add.items()):\n        anchor_dict = {'name': name, 'x': x, 'y': y}\n        try:\n            composite.appendAnchor(anchor_dict)\n        except TypeError:  # pragma: no cover\n            # fontParts API\n            composite.appendAnchor(name, (x, y))", "response": "Propagate anchors from base glyphs to a given composite ArcGIS glyph and to all composite Glyphs used in between."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting data for an anchor from a list of components.", "response": "def _get_anchor_data(anchor_data, glyphSet, components, anchor_name):\n    \"\"\"Get data for an anchor from a list of components.\"\"\"\n\n    anchors = []\n    for component in components:\n        for anchor in glyphSet[component.baseGlyph].anchors:\n            if anchor.name == anchor_name:\n                anchors.append((anchor, component))\n                break\n    if len(anchors) > 1:\n        for i, (anchor, component) in enumerate(anchors):\n            t = Transform(*component.transformation)\n            name = '%s_%d' % (anchor.name, i + 1)\n            anchor_data[name] = t.transformPoint((anchor.x, anchor.y))\n    elif anchors:\n        anchor, component = anchors[0]\n        t = Transform(*component.transformation)\n        anchor_data[anchor.name] = t.transformPoint((anchor.x, anchor.y))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the component closest to the origin.", "response": "def _component_closest_to_origin(components, glyph_set):\n    \"\"\"Return the component whose (xmin, ymin) bounds are closest to origin.\n\n    This ensures that a component that is moved below another is\n    actually recognized as such. Looking only at the transformation\n    offset can be misleading.\n    \"\"\"\n    return min(components, key=lambda comp: _distance((0, 0), _bounds(comp, glyph_set)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _bounds(component, glyph_set):\n    if hasattr(component, \"bounds\"):  # e.g. defcon\n        return component.bounds[:2]\n    elif hasattr(component, \"draw\"):  # e.g. ufoLib2\n        pen = fontTools.pens.boundsPen.BoundsPen(glyphSet=glyph_set)\n        component.draw(pen)\n        return pen.bounds[:2]\n    else:\n        raise ValueError(\n            \"Don't know to to compute the bounds of component '{}' \".format(component)\n        )", "response": "Return the xmin ymin of the bounds of component."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setContext(self, font, feaFile, compiler=None):\n        todo = set(self.features)\n        if self.mode == \"skip\":\n            existing = ast.findFeatureTags(feaFile)\n            todo.difference_update(existing)\n\n        self.context = SimpleNamespace(\n            font=font, feaFile=feaFile, compiler=compiler, todo=todo\n        )\n\n        return self.context", "response": "Populate a temporary context namespace which is resetby default."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecide whether to continue generating features or return early.", "response": "def shouldContinue(self):\n        \"\"\" Decide whether to start generating features or return early.\n        Returns a boolean: True to proceed, False to skip.\n\n        Sublcasses may override this to skip generation based on the presence\n        or lack of other required pieces of font data.\n        \"\"\"\n        if not self.context.todo:\n            self.log.debug(\"No features to be generated; skipped\")\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, font, feaFile, compiler=None):\n        self.setContext(font, feaFile, compiler=compiler)\n        try:\n            if self.shouldContinue():\n                return self._write()\n            else:\n                return False\n        finally:\n            del self.context", "response": "Write features and class definitions for this font to a feaLib\n        FeatureFile object. Returns True if the feature file was modified False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makeUnicodeToGlyphNameMapping(self):\n        # Try to get the \"best\" Unicode cmap subtable if this writer is running\n        # in the context of a FeatureCompiler, else create a new mapping from\n        # the UFO glyphs\n        compiler = self.context.compiler\n        cmap = None\n        if compiler is not None:\n            table = compiler.ttFont.get(\"cmap\")\n            if table is not None:\n                cmap = table.getBestCmap()\n        if cmap is None:\n            from ufo2ft.util import makeUnicodeToGlyphNameMapping\n\n            if compiler is not None:\n                glyphSet = compiler.glyphSet\n            else:\n                glyphSet = self.context.font\n            cmap = makeUnicodeToGlyphNameMapping(glyphSet)\n        return cmap", "response": "Return the Unicode to glyph name mapping for the current font."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning OrderedDict [ glyphName glyph ] sorted by glyphOrder.", "response": "def getOrderedGlyphSet(self):\n        \"\"\"Return OrderedDict[glyphName, glyph] sorted by glyphOrder.\n        \"\"\"\n        compiler = self.context.compiler\n        if compiler is not None:\n            return compiler.glyphSet\n\n        from ufo2ft.util import makeOfficialGlyphOrder\n\n        glyphSet = self.context.font\n        glyphOrder = makeOfficialGlyphOrder(self.context.font)\n        return OrderedDict((gn, glyphSet[gn]) for gn in glyphOrder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compileGSUB(self):\n        from ufo2ft.util import compileGSUB\n\n        compiler = self.context.compiler\n        if compiler is not None:\n            # The result is cached in the compiler instance, so if another\n            # writer requests one it is not compiled again.\n            if hasattr(compiler, \"_gsub\"):\n                return compiler._gsub\n\n            glyphOrder = compiler.ttFont.getGlyphOrder()\n        else:\n            # the 'real' glyph order doesn't matter because the table is not\n            # compiled to binary, only the glyph names are used\n            glyphOrder = sorted(self.context.font.keys())\n\n        gsub = compileGSUB(self.context.feaFile, glyphOrder)\n\n        if compiler and not hasattr(compiler, \"_gsub\"):\n            compiler._gsub = gsub\n        return gsub", "response": "Compile a temporary GSUB table from the current feature file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compile(self):\n        self.otf = TTFont(sfntVersion=self.sfntVersion)\n\n        # only compile vertical metrics tables if vhea metrics a defined\n        vertical_metrics = [\n            \"openTypeVheaVertTypoAscender\",\n            \"openTypeVheaVertTypoDescender\",\n            \"openTypeVheaVertTypoLineGap\",\n            \"openTypeVheaCaretSlopeRise\",\n            \"openTypeVheaCaretSlopeRun\",\n            \"openTypeVheaCaretOffset\",\n        ]\n        self.vertical = all(\n            getAttrWithFallback(self.ufo.info, metric) is not None\n            for metric in vertical_metrics\n        )\n\n        # write the glyph order\n        self.otf.setGlyphOrder(self.glyphOrder)\n\n        # populate basic tables\n        self.setupTable_head()\n        self.setupTable_hmtx()\n        self.setupTable_hhea()\n        self.setupTable_name()\n        self.setupTable_maxp()\n        self.setupTable_cmap()\n        self.setupTable_OS2()\n        self.setupTable_post()\n        if self.vertical:\n            self.setupTable_vmtx()\n            self.setupTable_vhea()\n        self.setupOtherTables()\n        self.importTTX()\n\n        return self.otf", "response": "Compile the OpenType binary and return the OpenType font."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makeGlyphsBoundingBoxes(self):\n\n        def getControlPointBounds(glyph):\n            pen.init()\n            glyph.draw(pen)\n            return pen.bounds\n\n        glyphBoxes = {}\n        pen = ControlBoundsPen(self.allGlyphs)\n        for glyphName, glyph in self.allGlyphs.items():\n            bounds = None\n            if glyph or glyph.components:\n                bounds = getControlPointBounds(glyph)\n                if bounds:\n                    bounds = BoundingBox(*(otRound(v) for v in bounds))\n            glyphBoxes[glyphName] = bounds\n        return glyphBoxes", "response": "Make bounding boxes for all the glyphs and return a dictionary of keyed by glyph names and BoundingBox objects keyed by glyph names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a bounding box for the font.", "response": "def makeFontBoundingBox(self):\n        \"\"\"\n        Make a bounding box for the font.\n\n        **This should not be called externally.** Subclasses\n        may override this method to handle the bounds creation\n        in a different way if desired.\n        \"\"\"\n        if not hasattr(self, \"glyphBoundingBoxes\"):\n            self.glyphBoundingBoxes = self.makeGlyphsBoundingBoxes()\n        fontBox = None\n        for glyphName, glyphBox in self.glyphBoundingBoxes.items():\n            if glyphBox is None:\n                continue\n            if fontBox is None:\n                fontBox = glyphBox\n            else:\n                fontBox = unionRect(fontBox, glyphBox)\n        if fontBox is None:  # unlikely\n            fontBox = BoundingBox(0, 0, 0, 0)\n        return fontBox"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds .notdef to the glyph set if it is not present. **This should not be called externally.** Subclasses may override this method to handle the glyph creation in a different way if desired.", "response": "def makeMissingRequiredGlyphs(font, glyphSet):\n        \"\"\"\n        Add .notdef to the glyph set if it is not present.\n\n        **This should not be called externally.** Subclasses\n        may override this method to handle the glyph creation\n        in a different way if desired.\n        \"\"\"\n        if \".notdef\" in glyphSet:\n            return\n\n        unitsPerEm = otRound(getAttrWithFallback(font.info, \"unitsPerEm\"))\n        ascender = otRound(getAttrWithFallback(font.info, \"ascender\"))\n        descender = otRound(getAttrWithFallback(font.info, \"descender\"))\n        defaultWidth = otRound(unitsPerEm * 0.5)\n        glyphSet[\".notdef\"] = StubGlyph(name=\".notdef\",\n                                        width=defaultWidth,\n                                        unitsPerEm=unitsPerEm,\n                                        ascender=ascender,\n                                        descender=descender)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setupTable_head(self):\n        if \"head\" not in self.tables:\n            return\n\n        self.otf[\"head\"] = head = newTable(\"head\")\n        font = self.ufo\n        head.checkSumAdjustment = 0\n        head.tableVersion = 1.0\n        head.magicNumber = 0x5F0F3CF5\n\n        # version numbers\n        # limit minor version to 3 digits as recommended in OpenType spec:\n        # https://www.microsoft.com/typography/otspec/recom.htm\n        versionMajor = getAttrWithFallback(font.info, \"versionMajor\")\n        versionMinor = getAttrWithFallback(font.info, \"versionMinor\")\n        fullFontRevision = float(\"%d.%03d\" % (versionMajor, versionMinor))\n        head.fontRevision = round(fullFontRevision, 3)\n        if head.fontRevision != fullFontRevision:\n            logger.warning(\n                \"Minor version in %s has too many digits and won't fit into \"\n                \"the head table's fontRevision field; rounded to %s.\",\n                fullFontRevision, head.fontRevision)\n\n        # upm\n        head.unitsPerEm = otRound(getAttrWithFallback(font.info, \"unitsPerEm\"))\n\n        # times\n        head.created = dateStringToTimeValue(getAttrWithFallback(font.info, \"openTypeHeadCreated\")) - mac_epoch_diff\n        head.modified = dateStringToTimeValue(dateStringForNow()) - mac_epoch_diff\n\n        # bounding box\n        xMin, yMin, xMax, yMax = self.fontBoundingBox\n        head.xMin = otRound(xMin)\n        head.yMin = otRound(yMin)\n        head.xMax = otRound(xMax)\n        head.yMax = otRound(yMax)\n\n        # style mapping\n        styleMapStyleName = getAttrWithFallback(font.info, \"styleMapStyleName\")\n        macStyle = []\n        if styleMapStyleName == \"bold\":\n            macStyle = [0]\n        elif styleMapStyleName == \"bold italic\":\n            macStyle = [0, 1]\n        elif styleMapStyleName == \"italic\":\n            macStyle = [1]\n        head.macStyle = intListToNum(macStyle, 0, 16)\n\n        # misc\n        head.flags = intListToNum(getAttrWithFallback(font.info, \"openTypeHeadFlags\"), 0, 16)\n        head.lowestRecPPEM = otRound(getAttrWithFallback(font.info, \"openTypeHeadLowestRecPPEM\"))\n        head.fontDirectionHint = 2\n        head.indexToLocFormat = 0\n        head.glyphDataFormat = 0", "response": "This method sets up the head table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setupTable_name(self):\n        if \"name\" not in self.tables:\n            return\n\n        font = self.ufo\n        self.otf[\"name\"] = name = newTable(\"name\")\n        name.names = []\n\n        # Set name records from font.info.openTypeNameRecords\n        for nameRecord in getAttrWithFallback(\n                font.info, \"openTypeNameRecords\"):\n            nameId = nameRecord[\"nameID\"]\n            platformId = nameRecord[\"platformID\"]\n            platEncId = nameRecord[\"encodingID\"]\n            langId = nameRecord[\"languageID\"]\n            # on Python 2, plistLib (used by ufoLib) returns unicode strings\n            # only when plist data contain non-ascii characters, and returns\n            # ascii-encoded bytes when it can. On the other hand, fontTools's\n            # name table `setName` method wants unicode strings, so we must\n            # decode them first\n            nameVal = tounicode(nameRecord[\"string\"], encoding='ascii')\n            name.setName(nameVal, nameId, platformId, platEncId, langId)\n\n        # Build name records\n        familyName = getAttrWithFallback(font.info, \"styleMapFamilyName\")\n        styleName = getAttrWithFallback(font.info, \"styleMapStyleName\").title()\n        preferredFamilyName = getAttrWithFallback(\n            font.info, \"openTypeNamePreferredFamilyName\")\n        preferredSubfamilyName = getAttrWithFallback(\n            font.info, \"openTypeNamePreferredSubfamilyName\")\n        fullName = \"%s %s\" % (preferredFamilyName, preferredSubfamilyName)\n\n        nameVals = {\n            0: getAttrWithFallback(font.info, \"copyright\"),\n            1: familyName,\n            2: styleName,\n            3: getAttrWithFallback(font.info, \"openTypeNameUniqueID\"),\n            4: fullName,\n            5: getAttrWithFallback(font.info, \"openTypeNameVersion\"),\n            6: getAttrWithFallback(font.info, \"postscriptFontName\"),\n            7: getAttrWithFallback(font.info, \"trademark\"),\n            8: getAttrWithFallback(font.info, \"openTypeNameManufacturer\"),\n            9: getAttrWithFallback(font.info, \"openTypeNameDesigner\"),\n            10: getAttrWithFallback(font.info, \"openTypeNameDescription\"),\n            11: getAttrWithFallback(font.info, \"openTypeNameManufacturerURL\"),\n            12: getAttrWithFallback(font.info, \"openTypeNameDesignerURL\"),\n            13: getAttrWithFallback(font.info, \"openTypeNameLicense\"),\n            14: getAttrWithFallback(font.info, \"openTypeNameLicenseURL\"),\n            16: preferredFamilyName,\n            17: preferredSubfamilyName,\n        }\n\n        # don't add typographic names if they are the same as the legacy ones\n        if nameVals[1] == nameVals[16]:\n            del nameVals[16]\n        if nameVals[2] == nameVals[17]:\n            del nameVals[17]\n        # postscript font name\n        if nameVals[6]:\n            nameVals[6] = normalizeStringForPostscript(nameVals[6])\n\n        for nameId in sorted(nameVals.keys()):\n            nameVal = nameVals[nameId]\n            if not nameVal:\n                continue\n            nameVal = tounicode(nameVal, encoding='ascii')\n            platformId = 3\n            platEncId = 10 if _isNonBMP(nameVal) else 1\n            langId = 0x409\n            # Set built name record if not set yet\n            if name.getName(nameId, platformId, platEncId, langId):\n                continue\n            name.setName(nameVal, nameId, platformId, platEncId, langId)", "response": "Setup the name table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake the cmap table. **This should not be called externally.** Subclasses may override or supplement this method to handle the table creation in a different way if desired.", "response": "def setupTable_cmap(self):\n        \"\"\"\n        Make the cmap table.\n\n        **This should not be called externally.** Subclasses\n        may override or supplement this method to handle the\n        table creation in a different way if desired.\n        \"\"\"\n        if \"cmap\" not in self.tables:\n            return\n\n        from fontTools.ttLib.tables._c_m_a_p import cmap_format_4\n\n        nonBMP = dict((k,v) for k,v in self.unicodeToGlyphNameMapping.items() if k > 65535)\n        if nonBMP:\n            mapping = dict((k,v) for k,v in self.unicodeToGlyphNameMapping.items() if k <= 65535)\n        else:\n            mapping = dict(self.unicodeToGlyphNameMapping)\n        # mac\n        cmap4_0_3 = cmap_format_4(4)\n        cmap4_0_3.platformID = 0\n        cmap4_0_3.platEncID = 3\n        cmap4_0_3.language = 0\n        cmap4_0_3.cmap = mapping\n        # windows\n        cmap4_3_1 = cmap_format_4(4)\n        cmap4_3_1.platformID = 3\n        cmap4_3_1.platEncID = 1\n        cmap4_3_1.language = 0\n        cmap4_3_1.cmap = mapping\n        # store\n        self.otf[\"cmap\"] = cmap = newTable(\"cmap\")\n        cmap.tableVersion = 0\n        cmap.tables = [cmap4_0_3, cmap4_3_1]\n        # If we have glyphs outside Unicode BMP, we must set another\n        # subtable that can hold longer codepoints for them.\n        if nonBMP:\n            from fontTools.ttLib.tables._c_m_a_p import cmap_format_12\n            nonBMP.update(mapping)\n            # mac\n            cmap12_0_4 = cmap_format_12(12)\n            cmap12_0_4.platformID = 0\n            cmap12_0_4.platEncID = 4\n            cmap12_0_4.language = 0\n            cmap12_0_4.cmap = nonBMP\n            # windows\n            cmap12_3_10 = cmap_format_12(12)\n            cmap12_3_10.platformID = 3\n            cmap12_3_10.platEncID = 10\n            cmap12_3_10.language = 0\n            cmap12_3_10.cmap = nonBMP\n            # update tables registry\n            cmap.tables = [cmap4_0_3, cmap4_3_1, cmap12_0_4, cmap12_3_10]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake the OS/2 table. **This should not be called externally.** Subclasses may override or supplement this method to handle the table creation in a different way if desired.", "response": "def setupTable_OS2(self):\n        \"\"\"\n        Make the OS/2 table.\n\n        **This should not be called externally.** Subclasses\n        may override or supplement this method to handle the\n        table creation in a different way if desired.\n        \"\"\"\n        if \"OS/2\" not in self.tables:\n            return\n\n        self.otf[\"OS/2\"] = os2 = newTable(\"OS/2\")\n        font = self.ufo\n        os2.version = 0x0004\n        # average glyph width\n        os2.xAvgCharWidth = 0\n        hmtx = self.otf.get(\"hmtx\")\n        if hmtx is not None:\n            widths = [width for width, _ in hmtx.metrics.values() if width > 0]\n            if widths:\n                os2.xAvgCharWidth = otRound(sum(widths) / len(widths))\n        # weight and width classes\n        os2.usWeightClass = getAttrWithFallback(font.info, \"openTypeOS2WeightClass\")\n        os2.usWidthClass = getAttrWithFallback(font.info, \"openTypeOS2WidthClass\")\n        # embedding\n        os2.fsType = intListToNum(getAttrWithFallback(font.info, \"openTypeOS2Type\"), 0, 16)\n\n        # subscript, superscript, strikeout values, taken from AFDKO:\n        # FDK/Tools/Programs/makeotf/makeotf_lib/source/hotconv/hot.c\n        unitsPerEm = getAttrWithFallback(font.info, \"unitsPerEm\")\n        italicAngle = getAttrWithFallback(font.info, \"italicAngle\")\n        xHeight = getAttrWithFallback(font.info, \"xHeight\")\n        def adjustOffset(offset, angle):\n            \"\"\"Adjust Y offset based on italic angle, to get X offset.\"\"\"\n            return offset * math.tan(math.radians(-angle)) if angle else 0\n\n        v = getAttrWithFallback(font.info, \"openTypeOS2SubscriptXSize\")\n        if v is None:\n            v = unitsPerEm * 0.65\n        os2.ySubscriptXSize = otRound(v)\n        v = getAttrWithFallback(font.info, \"openTypeOS2SubscriptYSize\")\n        if v is None:\n            v = unitsPerEm * 0.6\n        os2.ySubscriptYSize = otRound(v)\n        v = getAttrWithFallback(font.info, \"openTypeOS2SubscriptYOffset\")\n        if v is None:\n            v = unitsPerEm * 0.075\n        os2.ySubscriptYOffset = otRound(v)\n        v = getAttrWithFallback(font.info, \"openTypeOS2SubscriptXOffset\")\n        if v is None:\n            v = adjustOffset(-os2.ySubscriptYOffset, italicAngle)\n        os2.ySubscriptXOffset = otRound(v)\n\n        v = getAttrWithFallback(font.info, \"openTypeOS2SuperscriptXSize\")\n        if v is None:\n            v = os2.ySubscriptXSize\n        os2.ySuperscriptXSize = otRound(v)\n        v = getAttrWithFallback(font.info, \"openTypeOS2SuperscriptYSize\")\n        if v is None:\n            v = os2.ySubscriptYSize\n        os2.ySuperscriptYSize = otRound(v)\n        v = getAttrWithFallback(font.info, \"openTypeOS2SuperscriptYOffset\")\n        if v is None:\n            v = unitsPerEm * 0.35\n        os2.ySuperscriptYOffset = otRound(v)\n        v = getAttrWithFallback(font.info, \"openTypeOS2SuperscriptXOffset\")\n        if v is None:\n            v = adjustOffset(os2.ySuperscriptYOffset, italicAngle)\n        os2.ySuperscriptXOffset = otRound(v)\n\n        v = getAttrWithFallback(font.info, \"openTypeOS2StrikeoutSize\")\n        if v is None:\n            v = getAttrWithFallback(font.info, \"postscriptUnderlineThickness\")\n        os2.yStrikeoutSize = otRound(v)\n        v = getAttrWithFallback(font.info, \"openTypeOS2StrikeoutPosition\")\n        if v is None:\n            v = xHeight * 0.6 if xHeight else unitsPerEm * 0.22\n        os2.yStrikeoutPosition = otRound(v)\n\n        # family class\n        ibmFontClass, ibmFontSubclass = getAttrWithFallback(\n            font.info, \"openTypeOS2FamilyClass\")\n        os2.sFamilyClass = (ibmFontClass << 8) + ibmFontSubclass\n        # panose\n        data = getAttrWithFallback(font.info, \"openTypeOS2Panose\")\n        panose = Panose()\n        panose.bFamilyType = data[0]\n        panose.bSerifStyle = data[1]\n        panose.bWeight = data[2]\n        panose.bProportion = data[3]\n        panose.bContrast = data[4]\n        panose.bStrokeVariation = data[5]\n        panose.bArmStyle = data[6]\n        panose.bLetterForm = data[7]\n        panose.bMidline = data[8]\n        panose.bXHeight = data[9]\n        os2.panose = panose\n        # Unicode ranges\n        uniRanges = getAttrWithFallback(font.info, \"openTypeOS2UnicodeRanges\")\n        if uniRanges is not None:\n            os2.ulUnicodeRange1 = intListToNum(uniRanges, 0, 32)\n            os2.ulUnicodeRange2 = intListToNum(uniRanges, 32, 32)\n            os2.ulUnicodeRange3 = intListToNum(uniRanges, 64, 32)\n            os2.ulUnicodeRange4 = intListToNum(uniRanges, 96, 32)\n        else:\n            os2.recalcUnicodeRanges(self.otf)\n\n        # codepage ranges\n        codepageRanges = getAttrWithFallback(font.info, \"openTypeOS2CodePageRanges\")\n        if codepageRanges is None:\n            unicodes = self.unicodeToGlyphNameMapping.keys()\n            codepageRanges = calcCodePageRanges(unicodes)\n        os2.ulCodePageRange1 = intListToNum(codepageRanges, 0, 32)\n        os2.ulCodePageRange2 = intListToNum(codepageRanges, 32, 32)\n\n        # vendor id\n        os2.achVendID = tounicode(\n            getAttrWithFallback(font.info, \"openTypeOS2VendorID\"),\n            encoding=\"ascii\", errors=\"ignore\")\n        # vertical metrics\n        os2.sxHeight = otRound(getAttrWithFallback(font.info, \"xHeight\"))\n        os2.sCapHeight = otRound(getAttrWithFallback(font.info, \"capHeight\"))\n        os2.sTypoAscender = otRound(getAttrWithFallback(font.info, \"openTypeOS2TypoAscender\"))\n        os2.sTypoDescender = otRound(getAttrWithFallback(font.info, \"openTypeOS2TypoDescender\"))\n        os2.sTypoLineGap = otRound(getAttrWithFallback(font.info, \"openTypeOS2TypoLineGap\"))\n        os2.usWinAscent = otRound(getAttrWithFallback(font.info, \"openTypeOS2WinAscent\"))\n        os2.usWinDescent = otRound(getAttrWithFallback(font.info, \"openTypeOS2WinDescent\"))\n        # style mapping\n        selection = list(getAttrWithFallback(font.info, \"openTypeOS2Selection\"))\n        styleMapStyleName = getAttrWithFallback(font.info, \"styleMapStyleName\")\n        if styleMapStyleName == \"regular\":\n            selection.append(6)\n        elif styleMapStyleName == \"bold\":\n            selection.append(5)\n        elif styleMapStyleName == \"italic\":\n            selection.append(0)\n        elif styleMapStyleName == \"bold italic\":\n            selection += [0, 5]\n        os2.fsSelection = intListToNum(selection, 0, 16)\n        # characetr indexes\n        unicodes = [i for i in self.unicodeToGlyphNameMapping.keys() if i is not None]\n        if unicodes:\n            minIndex = min(unicodes)\n            maxIndex = max(unicodes)\n        else:\n            # the font may have *no* unicode values (it really happens!) so\n            # there needs to be a fallback. use 0xFFFF, as AFDKO does:\n            # FDK/Tools/Programs/makeotf/makeotf_lib/source/hotconv/map.c\n            minIndex = 0xFFFF\n            maxIndex = 0xFFFF\n        if maxIndex > 0xFFFF:\n            # the spec says that 0xFFFF should be used\n            # as the max if the max exceeds 0xFFFF\n            maxIndex = 0xFFFF\n        os2.fsFirstCharIndex = minIndex\n        os2.fsLastCharIndex = maxIndex\n        os2.usBreakChar = 32\n        os2.usDefaultChar = 0\n        # maximum contextual lookup length\n        os2.usMaxContex = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setupTable_hmtx(self):\n        if \"hmtx\" not in self.tables:\n            return\n\n        self.otf[\"hmtx\"] = hmtx = newTable(\"hmtx\")\n        hmtx.metrics = {}\n        for glyphName, glyph in self.allGlyphs.items():\n            width = otRound(glyph.width)\n            if width < 0:\n                raise ValueError(\n                    \"The width should not be negative: '%s'\" % (glyphName))\n            bounds = self.glyphBoundingBoxes[glyphName]\n            left = bounds.xMin if bounds else 0\n            hmtx[glyphName] = (width, left)", "response": "Setup the hmtx table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking the hhea table or the vhea table. This assume the hmtx or the vmtx were respectively made first.", "response": "def _setupTable_hhea_or_vhea(self, tag):\n        \"\"\"\n        Make the hhea table or the vhea table. This assume the hmtx or\n        the vmtx were respectively made first.\n        \"\"\"\n        if tag not in self.tables:\n            return\n\n        if tag == \"hhea\":\n            isHhea = True\n        else:\n            isHhea = False\n        self.otf[tag] = table = newTable(tag)\n        mtxTable = self.otf.get(tag[0] + \"mtx\")\n        font = self.ufo\n        if isHhea:\n            table.tableVersion = 0x00010000\n        else:\n            table.tableVersion = 0x00011000\n        # Vertical metrics in hhea, horizontal metrics in vhea\n        # and caret info.\n        # The hhea metrics names are formed as:\n        #   \"openType\" + tag.title() + \"Ascender\", etc.\n        # While vhea metrics names are formed as:\n        #   \"openType\" + tag.title() + \"VertTypo\" + \"Ascender\", etc.\n        # Caret info names only differ by tag.title().\n        commonPrefix = \"openType%s\" % tag.title()\n        if isHhea:\n            metricsPrefix = commonPrefix\n        else:\n            metricsPrefix = \"openType%sVertTypo\" % tag.title()\n        metricsDict = {\n            \"ascent\": \"%sAscender\" % metricsPrefix,\n            \"descent\": \"%sDescender\" % metricsPrefix,\n            \"lineGap\": \"%sLineGap\" % metricsPrefix,\n            \"caretSlopeRise\": \"%sCaretSlopeRise\" % commonPrefix,\n            \"caretSlopeRun\": \"%sCaretSlopeRun\" % commonPrefix,\n            \"caretOffset\": \"%sCaretOffset\" % commonPrefix,\n        }\n        for otfName, ufoName in metricsDict.items():\n            setattr(table, otfName,\n                    otRound(getAttrWithFallback(font.info, ufoName)))\n        # Horizontal metrics in hhea, vertical metrics in vhea\n        advances = []  # width in hhea, height in vhea\n        firstSideBearings = []  # left in hhea, top in vhea\n        secondSideBearings = []  # right in hhea, bottom in vhea\n        extents = []\n        if mtxTable is not None:\n            for glyphName in self.allGlyphs:\n                advance, firstSideBearing = mtxTable[glyphName]\n                advances.append(advance)\n                bounds = self.glyphBoundingBoxes[glyphName]\n                if bounds is None:\n                    continue\n                if isHhea:\n                    boundsAdvance = (bounds.xMax - bounds.xMin)\n                    # equation from the hhea spec for calculating xMaxExtent:\n                    #   Max(lsb + (xMax - xMin))\n                    extent = firstSideBearing + boundsAdvance\n                else:\n                    boundsAdvance = (bounds.yMax - bounds.yMin)\n                    # equation from the vhea spec for calculating yMaxExtent:\n                    #   Max(tsb + (yMax - yMin)).\n                    extent = firstSideBearing + boundsAdvance\n                secondSideBearing = advance - firstSideBearing - boundsAdvance\n\n                firstSideBearings.append(firstSideBearing)\n                secondSideBearings.append(secondSideBearing)\n                extents.append(extent)\n        setattr(table,\n                \"advance%sMax\" % (\"Width\" if isHhea else \"Height\"),\n                max(advances) if advances else 0)\n        setattr(table,\n                \"min%sSideBearing\" % (\"Left\" if isHhea else \"Top\"),\n                min(firstSideBearings) if firstSideBearings else 0)\n        setattr(table,\n                \"min%sSideBearing\" % (\"Right\" if isHhea else \"Bottom\"),\n                min(secondSideBearings) if secondSideBearings else 0)\n        setattr(table,\n                \"%sMaxExtent\" % (\"x\" if isHhea else \"y\"),\n                max(extents) if extents else 0)\n        if isHhea:\n            reserved = range(4)\n        else:\n            # vhea.reserved0 is caretOffset for legacy reasons\n            reserved = range(1, 5)\n        for i in reserved:\n            setattr(table, \"reserved%i\" % i, 0)\n        table.metricDataFormat = 0\n        # glyph count\n        setattr(table,\n                \"numberOf%sMetrics\" % (\"H\" if isHhea else \"V\"),\n                len(self.allGlyphs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setupTable_vmtx(self):\n        if \"vmtx\" not in self.tables:\n            return\n\n        self.otf[\"vmtx\"] = vmtx = newTable(\"vmtx\")\n        vmtx.metrics = {}\n        for glyphName, glyph in self.allGlyphs.items():\n            height = otRound(glyph.height)\n            if height < 0:\n                raise ValueError(\n                    \"The height should not be negative: '%s'\" % (glyphName))\n            verticalOrigin = _getVerticalOrigin(self.otf, glyph)\n            bounds = self.glyphBoundingBoxes[glyphName]\n            top = bounds.yMax if bounds else 0\n            vmtx[glyphName] = (height, verticalOrigin - top)", "response": "Setup the vmtx table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setupTable_VORG(self):\n        if \"VORG\" not in self.tables:\n            return\n\n        self.otf[\"VORG\"] = vorg = newTable(\"VORG\")\n        vorg.majorVersion = 1\n        vorg.minorVersion = 0\n        vorg.VOriginRecords = {}\n        # Find the most frequent verticalOrigin\n        vorg_count = Counter(_getVerticalOrigin(self.otf, glyph)\n                             for glyph in self.allGlyphs.values())\n        vorg.defaultVertOriginY = vorg_count.most_common(1)[0][0]\n        if len(vorg_count) > 1:\n            for glyphName, glyph in self.allGlyphs.items():\n                vorg.VOriginRecords[glyphName] = _getVerticalOrigin(\n                    self.otf, glyph)\n        vorg.numVertOriginYMetrics = len(vorg.VOriginRecords)", "response": "Setup the VORG table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setupTable_post(self):\n        if \"post\" not in self.tables:\n            return\n\n        self.otf[\"post\"] = post = newTable(\"post\")\n        font = self.ufo\n        post.formatType = 3.0\n        # italic angle\n        italicAngle = getAttrWithFallback(font.info, \"italicAngle\")\n        post.italicAngle = italicAngle\n        # underline\n        underlinePosition = getAttrWithFallback(font.info, \"postscriptUnderlinePosition\")\n        post.underlinePosition = otRound(underlinePosition)\n        underlineThickness = getAttrWithFallback(font.info, \"postscriptUnderlineThickness\")\n        post.underlineThickness = otRound(underlineThickness)\n        post.isFixedPitch = getAttrWithFallback(font.info, \"postscriptIsFixedPitch\")\n        # misc\n        post.minMemType42 = 0\n        post.maxMemType42 = 0\n        post.minMemType1 = 0\n        post.maxMemType1 = 0", "response": "Setup the post table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef importTTX(self):\n        import os\n        import re\n\n        prefix = \"com.github.fonttools.ttx\"\n        sfntVersionRE = re.compile('(^<ttFont\\s+)(sfntVersion=\".*\"\\s+)(.*>$)',\n                                   flags=re.MULTILINE)\n        if not hasattr(self.ufo, \"data\"):\n            return\n        if not self.ufo.data.fileNames:\n            return\n        for path in self.ufo.data.fileNames:\n            foldername, filename = os.path.split(path)\n            if (foldername == prefix and filename.endswith(\".ttx\")):\n                ttx = self.ufo.data[path].decode('utf-8')\n                # strip 'sfntVersion' attribute from ttFont element,\n                # if present, to avoid overwriting the current value\n                ttx = sfntVersionRE.sub(r'\\1\\3', ttx)\n                fp = BytesIO(ttx.encode('utf-8'))\n                self.otf.importXML(fp)", "response": "Merge TTX files from data directory com. github. fonttools. ttx"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makeGlyphsBoundingBoxes(self):\n\n        def getControlPointBounds(glyph):\n            pen.init()\n            glyph.draw(pen)\n            return pen.bounds\n\n        def toInt(value, else_callback):\n            rounded = otRound(value)\n            if tolerance >= 0.5 or abs(rounded - value) <= tolerance:\n                return rounded\n            else:\n                return int(else_callback(value))\n\n        tolerance = self.roundTolerance\n        glyphBoxes = {}\n        pen = ControlBoundsPen(self.allGlyphs)\n        for glyphName, glyph in self.allGlyphs.items():\n            bounds = None\n            if glyph or glyph.components:\n                bounds = getControlPointBounds(glyph)\n                if bounds:\n                    rounded = []\n                    for value in bounds[:2]:\n                        rounded.append(toInt(value, math.floor))\n                    for value in bounds[2:]:\n                        rounded.append(toInt(value, math.ceil))\n                    bounds = BoundingBox(*rounded)\n            glyphBoxes[glyphName] = bounds\n        return glyphBoxes", "response": "Make bounding boxes for all the glyphs and return a dictionary of glyph names xMin yMin yMax namedtuples keyed by glyph names."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a Type2CharString for the glyph.", "response": "def getCharStringForGlyph(self, glyph, private, globalSubrs):\n        \"\"\"\n        Get a Type2CharString for the *glyph*\n\n        **This should not be called externally.** Subclasses\n        may override this method to handle the charstring creation\n        in a different way if desired.\n        \"\"\"\n        width = glyph.width\n        defaultWidth = private.defaultWidthX\n        nominalWidth = private.nominalWidthX\n        if width == defaultWidth:\n            # if width equals the default it can be omitted from charstring\n            width = None\n        else:\n            # subtract the nominal width\n            width -= nominalWidth\n        if width is not None:\n            width = otRound(width)\n        pen = T2CharStringPen(width, self.allGlyphs,\n                              roundTolerance=self.roundTolerance)\n        glyph.draw(pen)\n        charString = pen.getCharString(\n            private, globalSubrs, optimize=self.optimizeCFF\n        )\n        return charString"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake the maxp table.", "response": "def setupTable_maxp(self):\n        \"\"\"Make the maxp table.\"\"\"\n        if \"maxp\" not in self.tables:\n            return\n\n        self.otf[\"maxp\"] = maxp = newTable(\"maxp\")\n        maxp.tableVersion = 0x00005000\n        maxp.numGlyphs = len(self.glyphOrder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setupTable_CFF(self):\n        if not {\"CFF\", \"CFF \"}.intersection(self.tables):\n            return\n\n        self.otf[\"CFF \"] = cff = newTable(\"CFF \")\n        cff = cff.cff\n        # set up the basics\n        cff.major = 1\n        cff.minor = 0\n        cff.hdrSize = 4\n        cff.offSize = 4\n        cff.fontNames = []\n        strings = IndexedStrings()\n        cff.strings = strings\n        private = PrivateDict(strings=strings)\n        private.rawDict.update(private.defaults)\n        globalSubrs = GlobalSubrsIndex(private=private)\n        topDict = TopDict(GlobalSubrs=globalSubrs, strings=strings)\n        topDict.Private = private\n        charStrings = topDict.CharStrings = CharStrings(file=None, charset=None,\n            globalSubrs=globalSubrs, private=private, fdSelect=None, fdArray=None)\n        charStrings.charStringsAreIndexed = True\n        topDict.charset = []\n        charStringsIndex = charStrings.charStringsIndex = SubrsIndex(private=private, globalSubrs=globalSubrs)\n        cff.topDictIndex = topDictIndex = TopDictIndex()\n        topDictIndex.append(topDict)\n        topDictIndex.strings = strings\n        cff.GlobalSubrs = globalSubrs\n        # populate naming data\n        info = self.ufo.info\n        psName = getAttrWithFallback(info, \"postscriptFontName\")\n        cff.fontNames.append(psName)\n        topDict = cff.topDictIndex[0]\n        topDict.version = \"%d.%d\" % (getAttrWithFallback(info, \"versionMajor\"), getAttrWithFallback(info, \"versionMinor\"))\n        trademark = getAttrWithFallback(info, \"trademark\")\n        if trademark:\n            trademark = normalizeStringForPostscript(trademark.replace(\"\\u00A9\", \"Copyright\"))\n        if trademark != self.ufo.info.trademark:\n            logger.info(\"The trademark was normalized for storage in the \"\n                        \"CFF table and consequently some characters were \"\n                        \"dropped: '%s'\", trademark)\n        if trademark is None:\n            trademark = \"\"\n        topDict.Notice = trademark\n        copyright = getAttrWithFallback(info, \"copyright\")\n        if copyright:\n            copyright = normalizeStringForPostscript(copyright.replace(\"\\u00A9\", \"Copyright\"))\n        if copyright != self.ufo.info.copyright:\n            logger.info(\"The copyright was normalized for storage in the \"\n                        \"CFF table and consequently some characters were \"\n                        \"dropped: '%s'\", copyright)\n        if copyright is None:\n            copyright = \"\"\n        topDict.Copyright = copyright\n        topDict.FullName = getAttrWithFallback(info, \"postscriptFullName\")\n        topDict.FamilyName = getAttrWithFallback(info, \"openTypeNamePreferredFamilyName\")\n        topDict.Weight = getAttrWithFallback(info, \"postscriptWeightName\")\n        # populate various numbers\n        topDict.isFixedPitch = getAttrWithFallback(info, \"postscriptIsFixedPitch\")\n        topDict.ItalicAngle = getAttrWithFallback(info, \"italicAngle\")\n        underlinePosition = getAttrWithFallback(info, \"postscriptUnderlinePosition\")\n        topDict.UnderlinePosition = otRound(underlinePosition)\n        underlineThickness = getAttrWithFallback(info, \"postscriptUnderlineThickness\")\n        topDict.UnderlineThickness = otRound(underlineThickness)\n        # populate font matrix\n        unitsPerEm = otRound(getAttrWithFallback(info, \"unitsPerEm\"))\n        topDict.FontMatrix = [1.0 / unitsPerEm, 0, 0, 1.0 / unitsPerEm, 0, 0]\n        # populate the width values\n        if (not any(hasattr(info, attr) and getattr(info, attr) is not None\n                    for attr in (\"postscriptDefaultWidthX\",\n                                 \"postscriptNominalWidthX\"))\n                and \"hmtx\" in self.otf):\n            # no custom values set in fontinfo.plist; compute optimal ones\n            from fontTools.cffLib.width import optimizeWidths\n            hmtx = self.otf['hmtx']\n            widths = [m[0] for m in hmtx.metrics.values()]\n            defaultWidthX, nominalWidthX = optimizeWidths(widths)\n        else:\n            defaultWidthX = otRound(getAttrWithFallback(info, \"postscriptDefaultWidthX\"))\n            nominalWidthX = otRound(getAttrWithFallback(info, \"postscriptNominalWidthX\"))\n        if defaultWidthX:\n            private.rawDict[\"defaultWidthX\"] = defaultWidthX\n        if nominalWidthX:\n            private.rawDict[\"nominalWidthX\"] = nominalWidthX\n        # populate hint data\n        blueFuzz = otRound(getAttrWithFallback(info, \"postscriptBlueFuzz\"))\n        blueShift = otRound(getAttrWithFallback(info, \"postscriptBlueShift\"))\n        blueScale = getAttrWithFallback(info, \"postscriptBlueScale\")\n        forceBold = getAttrWithFallback(info, \"postscriptForceBold\")\n        blueValues = getAttrWithFallback(info, \"postscriptBlueValues\")\n        if isinstance(blueValues, list):\n            blueValues = [otRound(i) for i in blueValues]\n        otherBlues = getAttrWithFallback(info, \"postscriptOtherBlues\")\n        if isinstance(otherBlues, list):\n            otherBlues = [otRound(i) for i in otherBlues]\n        familyBlues = getAttrWithFallback(info, \"postscriptFamilyBlues\")\n        if isinstance(familyBlues, list):\n            familyBlues = [otRound(i) for i in familyBlues]\n        familyOtherBlues = getAttrWithFallback(info, \"postscriptFamilyOtherBlues\")\n        if isinstance(familyOtherBlues, list):\n            familyOtherBlues = [otRound(i) for i in familyOtherBlues]\n        stemSnapH = getAttrWithFallback(info, \"postscriptStemSnapH\")\n        if isinstance(stemSnapH, list):\n            stemSnapH = [otRound(i) for i in stemSnapH]\n        stemSnapV = getAttrWithFallback(info, \"postscriptStemSnapV\")\n        if isinstance(stemSnapV, list):\n            stemSnapV = [otRound(i) for i in stemSnapV]\n        # only write the blues data if some blues are defined.\n        if any((blueValues, otherBlues, familyBlues, familyOtherBlues)):\n            private.rawDict[\"BlueFuzz\"] = blueFuzz\n            private.rawDict[\"BlueShift\"] = blueShift\n            private.rawDict[\"BlueScale\"] = blueScale\n            private.rawDict[\"ForceBold\"] = forceBold\n            if blueValues:\n                private.rawDict[\"BlueValues\"] = blueValues\n            if otherBlues:\n                private.rawDict[\"OtherBlues\"] = otherBlues\n            if familyBlues:\n                private.rawDict[\"FamilyBlues\"] = familyBlues\n            if familyOtherBlues:\n                private.rawDict[\"FamilyOtherBlues\"] = familyOtherBlues\n        # only write the stems if both are defined.\n        if (stemSnapH and stemSnapV):\n            private.rawDict[\"StemSnapH\"] = stemSnapH\n            private.rawDict[\"StdHW\"] = stemSnapH[0]\n            private.rawDict[\"StemSnapV\"] = stemSnapV\n            private.rawDict[\"StdVW\"] = stemSnapV[0]\n        # populate glyphs\n        for glyphName in self.glyphOrder:\n            glyph = self.allGlyphs[glyphName]\n            charString = self.getCharStringForGlyph(glyph, private, globalSubrs)\n            # add to the font\n            if glyphName in charStrings:\n                # XXX a glyph already has this name. should we choke?\n                glyphID = charStrings.charStrings[glyphName]\n                charStringsIndex.items[glyphID] = charString\n            else:\n                charStringsIndex.append(charString)\n                glyphID = len(topDict.charset)\n                charStrings.charStrings[glyphName] = glyphID\n                topDict.charset.append(glyphName)\n        topDict.FontBBox = self.fontBoundingBox", "response": "Make the CFF table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setupTable_maxp(self):\n        if \"maxp\" not in self.tables:\n            return\n\n        self.otf[\"maxp\"] = maxp = newTable(\"maxp\")\n        maxp.tableVersion = 0x00010000\n        maxp.numGlyphs = len(self.glyphOrder)\n        maxp.maxZones = 1\n        maxp.maxTwilightPoints = 0\n        maxp.maxStorage = 0\n        maxp.maxFunctionDefs = 0\n        maxp.maxInstructionDefs = 0\n        maxp.maxStackElements = 0\n        maxp.maxSizeOfInstructions = 0\n        maxp.maxComponentElements = max(len(g.components)\n                                        for g in self.allGlyphs.values())", "response": "Make the maxp table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a format 2 post table with the compiler s glyph order.", "response": "def setupTable_post(self):\n        \"\"\"Make a format 2 post table with the compiler's glyph order.\"\"\"\n        super(OutlineTTFCompiler, self).setupTable_post()\n        if \"post\" not in self.otf:\n            return\n\n        post = self.otf[\"post\"]\n        post.formatType = 2.0\n        post.extraNames = []\n        post.mapping = {}\n        post.glyphOrder = self.glyphOrder"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes the glyf table.", "response": "def setupTable_glyf(self):\n        \"\"\"Make the glyf table.\"\"\"\n        if not {\"glyf\", \"loca\"}.issubset(self.tables):\n            return\n\n        self.otf[\"loca\"] = newTable(\"loca\")\n        self.otf[\"glyf\"] = glyf = newTable(\"glyf\")\n        glyf.glyphs = {}\n        glyf.glyphOrder = self.glyphOrder\n\n        hmtx = self.otf.get(\"hmtx\")\n        allGlyphs = self.allGlyphs\n        for name in self.glyphOrder:\n            glyph = allGlyphs[name]\n            pen = TTGlyphPen(allGlyphs)\n            try:\n                glyph.draw(pen)\n            except NotImplementedError:\n                logger.error(\"%r has invalid curve format; skipped\", name)\n                ttGlyph = Glyph()\n            else:\n                ttGlyph = pen.glyph()\n                if (\n                    ttGlyph.isComposite()\n                    and hmtx is not None\n                    and self.autoUseMyMetrics\n                ):\n                    self.autoUseMyMetrics(ttGlyph, name, hmtx)\n            glyf[name] = ttGlyph"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the \"USE_MY_METRICS\" flag on the first component having the same advance width as the composite glyph, no transform and no horizontal shift (but allow it to shift vertically). This forces the composite glyph to use the possibly hinted horizontal metrics of the sub-glyph, instead of those from the \"hmtx\" table.", "response": "def autoUseMyMetrics(ttGlyph, glyphName, hmtx):\n        \"\"\" Set the \"USE_MY_METRICS\" flag on the first component having the\n        same advance width as the composite glyph, no transform and no\n        horizontal shift (but allow it to shift vertically).\n        This forces the composite glyph to use the possibly hinted horizontal\n        metrics of the sub-glyph, instead of those from the \"hmtx\" table.\n        \"\"\"\n        width = hmtx[glyphName][0]\n        for component in ttGlyph.components:\n            try:\n                baseName, transform = component.getComponentInfo()\n            except AttributeError:\n                # component uses '{first,second}Pt' instead of 'x' and 'y'\n                continue\n            try:\n                baseMetrics = hmtx[baseName]\n            except KeyError:\n                continue  # ignore missing components\n            else:\n                if (baseMetrics[0] == width and\n                        transform[:-1] == (1, 0, 0, 1, 0)):\n                    component.flags |= USE_MY_METRICS\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef syllabify(language, word) :\n\t'''Syllabifies the word, given a language configuration loaded with loadLanguage.\n\t   word is either a string of phonemes from the CMU pronouncing dictionary set\n\t   (with optional stress numbers after vowels), or a Python list of phonemes,\n\t   e.g. \"B AE1 T\" or [\"B\", \"AE1\", \"T\"]'''\n\t   \n\tif type(word) == str :\n\t\tword = word.split()\n\t\t\n\tsyllables = [] # This is the returned data structure.\n\n\tinternuclei = [] # This maintains a list of phonemes between nuclei.\n\t\n\tfor phoneme in word :\n\t\n\t\tphoneme = phoneme.strip()\n\t\tif phoneme == \"\" :\n\t\t\tcontinue\n\t\tstress = None\n\t\tif phoneme[-1].isdigit() :\n\t\t\tstress = int(phoneme[-1])\n\t\t\tphoneme = phoneme[0:-1]\n\t\t\n\t\tif phoneme in language[\"vowels\"] :\n\t\t\t# Split the consonants seen since the last nucleus into coda and onset.\n\t\t\t\n\t\t\tcoda = None\n\t\t\tonset = None\n\t\t\t\n\t\t\t# If there is a period in the input, split there.\n\t\t\tif \".\" in internuclei :\n\t\t\t\tperiod = internuclei.index(\".\")\n\t\t\t\tcoda = internuclei[:period]\n\t\t\t\tonset = internuclei[period+1:]\n\t\t\t\n\t\t\telse :\n\t\t\t\t# Make the largest onset we can. The 'split' variable marks the break point.\n\t\t\t\tfor split in range(0, len(internuclei)+1) :\n\t\t\t\t\tcoda = internuclei[:split]\n\t\t\t\t\tonset = internuclei[split:]\n\t\t\t\t\t\n\t\t\t\t\t# If we are looking at a valid onset, or if we're at the start of the word\n\t\t\t\t\t# (in which case an invalid onset is better than a coda that doesn't follow\n\t\t\t\t\t# a nucleus), or if we've gone through all of the onsets and we didn't find\n\t\t\t\t\t# any that are valid, then split the nonvowels we've seen at this location.\n\t\t\t\t\tif \" \".join(onset) in language[\"onsets\"] \\\n\t\t\t\t\t   or len(syllables) == 0 \\\n\t\t\t\t\t   or len(onset) == 0 :\n\t\t\t\t\t   break\n\t\t\t   \n\t\t\t# Tack the coda onto the coda of the last syllable. Can't do it if this\n\t\t\t# is the first syllable.\n\t\t\tif len(syllables) > 0 :\n\t\t\t\tsyllables[-1][3].extend(coda)\n\t\t\t\n\t\t\t# Make a new syllable out of the onset and nucleus.\n\t\t\tsyllables.append( (stress, onset, [phoneme], []) )\n\t\t\t\t\n\t\t\t# At this point we've processed the internuclei list.\n\t\t\tinternuclei = []\n\n\t\telif not phoneme in language[\"consonants\"] and phoneme != \".\" :\n\t\t\traise ValueError, \"Invalid phoneme: \" + phoneme\n\t\t\t\n\t\telse : # a consonant\n\t\t\tinternuclei.append(phoneme)\n\t\n\t# Done looping through phonemes. We may have consonants left at the end.\n\t# We may have even not found a nucleus.\n\tif len(internuclei) > 0 :\n\t\tif len(syllables) == 0 :\n\t\t\tsyllables.append( (None, internuclei, [], []) )\n\t\telse :\n\t\t\tsyllables[-1][3].extend(internuclei)\n\n\treturn syllables", "response": "Syllabifies the word given a language configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stringify(syllables) :\n\t'''This function takes a syllabification returned by syllabify and\n\t   turns it into a string, with phonemes spearated by spaces and\n\t   syllables spearated by periods.'''\n\tret = []\n\tfor syl in syllables :\n\t\tstress, onset, nucleus, coda = syl\n\t\tif stress != None and len(nucleus) != 0 :\n\t\t\tnucleus[0] += str(stress)\n\t\tret.append(\" \".join(onset + nucleus + coda))\n\treturn \" . \".join(ret)", "response": "This function takes a syllabification returned by syllabify and\n\t   turns it into a string with phonemes spearated by spaces and\n\t   syllables spearated by periods."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd alternative pronunciations to the end of a syllabified atom.", "response": "def add_elisions(_ipa):\n\t\"\"\"\n\tAdd alternative pronunciations: those that have elided syllables\n\t\"\"\"\n\treplace={}\n\n\t# -OWER\n\t# e.g. tower, hour, bower, etc\n\treplace[u'a\u028a.\u025b\u02d0']=u'a\u028ar'\n\n\n\t# -INOUS\n\t# e.g. ominous, etc\n\treplace[u'\u0259.n\u0259s']=u'n\u0259s'\n\n\t# -EROUS\n\t# e.g. ponderous, adventurous\n\treplace[u'\u025b\u02d0.\u0259s']=u'r\u0259s'\n\n\t# -IA-\n\t# e.g. plutonian, indian, assyrian, idea, etc\n\treplace[u'i\u02d0.\u0259']=u'j\u0259'\n\t# -IOUS\n\t# e.g. studious, tedious, etc\n\t#replace[u'i\u02d0.\u0259s']=u'i\u02d0\u0259s'\n\n\n\t# -IER\n\t# e.g. happier\n\treplace[u'i\u02d0.\u025b\u02d0']=u'\u026ar'\n\n\t# -ERING\n\t# e.g. scattering, wondering, watering\n\treplace[u'\u025b\u02d0.\u026a\u014b']=u'r\u026a\u014b'\n\n\t# -ERY\n\t# e.g. memory\n\t# QUESTIONABLE\n\t#replace[u'\u025b\u02d0.i\u02d0']=u'ri\u02d0'\n\n\t# -ENING\n\t# e.g. opening\n\treplace[u'\u0259.n\u026a\u014b']=u'n\u026a\u014b'\n\n\t# -ENER\n\t# e.g. gardener\n\treplace[u'\u0259.n\u025b\u02d0']=u'n\u025b\u02d0'\n\n\t# -EL- (-ELLER, -ELLING, -ELLY)\n\t# e.g. traveller, dangling, gravelly\n\t# QUESTIONABLE\n\t#replace[u'\u0259.l']=u'l'\n\n\t# -IRE-\n\t# e.g. fire, fiery, attire, hired\n\treplace[u'\u026a.\u025b\u02d0']=u'\u026ar'\n\n\t# -EL, -UAL\n\t# e.g. jewel\n\treplace[u'u\u02d0.\u0259l']=u'u\u02d0l'\n\n\t# -EVN\n\t# e.g. heaven, seven\n\treplace[u'\u025b.v\u0259n']=u'\u025bvn'\n\n\t# -IOUS, -EER\n\t# e.g. sincerest, dear, incommodiously\n\t# QUESTIONABLE\n\t#replace[u'.\u028c.']=u'\u028c.'\n\treplace[u'e\u026a.\u028c']=u'e\u026a\u028c'\n\n\tnew=[_ipa]\n\tfor k,v in replace.items():\n\t\tif k in _ipa:\n\t\t\tnew+=[_ipa.replace(k,v)]\n\treturn new"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef slice(l,num_slices=None,slice_length=None,runts=True,random=False):\n\tif random:\n\t\timport random\n\t\trandom.shuffle(l)\n\tif not num_slices and not slice_length: return l\n\tif not slice_length: slice_length=int(len(l)/num_slices)\n\tnewlist=[l[i:i+slice_length] for i in range(0, len(l), slice_length)]\n\tif runts: return newlist\n\treturn [lx for lx in newlist if len(lx)==slice_length]", "response": "Returns a new list of n evenly - sized segments of the original list l."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self,meter=None,init=None):\n\t\t#print '>> LINE PARSING',meter,init\n\t\t#if not meter:\n\t\t#\tfrom Meter import Meter,genDefault\n\t\t#\tmeter = genDefault()\n\n\t\t\"\"\"\n\t\twords=self.ents(cls='Word',flattenList=False)\n\t\t#print words\n\t\tnumSyll=0\n\t\tif not words: return None\n\t\tfor word in words:\n\t\t\tif type(word)==type([]):\n\t\t\t\tfor wrd in word:\n\t\t\t\t\tif wrd.isBroken():\n\t\t\t\t\t\t#print wrd\n\t\t\t\t\t\treturn None\n\t\t\t\tnumSyll+=word[0].getNumSyll()\n\t\t\telse:\n\t\t\t\tif word.isBroken():\n\t\t\t\t\treturn None\n\t\t\t\tnumSyll+=word.getNumSyll()\n\n\t\t## PARSE\n\t\tself.__parses[meter.id],self.__boundParses[meter.id]=meter.parse(words,numSyll)\n\t\t####\n\t\t\"\"\"\n\t\twordtoks=self.wordtokens(include_punct=False)\n\t\t#print words\n\t\tnumSyll=0\n\t\tif not wordtoks: return None\n\t\tfor wordtok in wordtoks:\n\t\t\twordtok_words = wordtok.children\n\t\t\tif not wordtok_words or True in [word.isBroken() for word in wordtok_words]:\n\t\t\t\treturn None\n\t\t\tnumSyll+=wordtok_words[0].getNumSyll()\n\n\t\t## PARSE\n\t\tself.__parses[meter.id],self.__boundParses[meter.id]=meter.parse(wordtoks,numSyll)\n\t\t####\n\n\t\t### SORT\n\t\tself.__parses[meter.id].sort(key=lambda p: (p.totalScore,p.num_non_monosyllabic_positions))\n\t\t###\n\n\n\t\tself.__bestparse[meter.id]=None\n\t\ttry:\n\t\t\tself.__bestparse[meter.id]=self.__parses[meter.id][0]\n\t\texcept (KeyError,IndexError) as e:\n\t\t\ttry:\n\t\t\t\tself.__bestparse[meter.id]=self.__boundParses[meter.id][0]\n\t\t\texcept (KeyError,IndexError) as e:\n\t\t\t\tself.__bestparse[meter.id]=None\n\n\n\t\t## Re-sort words within wordtoken\n\t\tbp = self.__bestparse[meter.id]\n\t\tif bp: bp.set_wordtokens_to_best_word_options()", "response": "Parses the wordlist and returns the best parse for the given meter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the list of descendants of this object", "response": "def descendants(self,collapseLists=True):\n\t\t\"\"\"\n\t\tA more genteel method of accessing an object's children (than self.children)\n\t\tIf collapseLists==True, the returned list is guaranteed not to be a list of lists.\n\t\tIf collapseLists==False, the returned list *may* be a list of lists, [cf. self.children]\n\t\t\tin the case of optionality among children\n\t\t\teg, a line's children is a list of lists of Words.\n\t\t\"\"\"\n\n\t\tif not collapseLists:\n\t\t\treturn self.children\n\n\t\tif not self.children:\n\t\t\treturn []\n\n\t\tif type(self.children[0])==type([]):\n\t\t\treturn [x[0] for x in self.children]\n\n\t\treturn self.children"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef om(self,breath,conscious=True):\n\n\t\t#import prosodic\n\t\tif (not conscious) and bool(being.config['print_to_screen']):\n\t\t\tif not type(breath) in [str,unicode]:\n\t\t\t\tbreath=unicode(breath)\n\t\t\tbeing.om+=breath+\"\\n\"\n\t\t\tprint self.u2s(breath)\n\t\treturn breath", "response": "print the string passed via argument breath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a value v to the feature list self. featpaths.", "response": "def feat(self,k,v):\n\t\t\"\"\"\n\t\tStore value 'v' as a feature name 'k' for this object.\n\t\tFeatures are stored in the dictionary self.feats.\n\t\t[IMPORTANT NOTE:]\n\t\tIf the feature 'k' is not yet defined, then:\n\t\t\tself.feats[k]=v\n\t\tOTHERWISE:\n\t\t\tself.feats[k] becomes a list (if not already)\n\t\t\t'v' is added to this list\n\t\t\"\"\"\n\n\n\t\tif (not hasattr(self,'feats')):\n\t\t\tself.feats = {}\n\t\t\tif (not hasattr(self,'featpaths')):\n\t\t\t\tself.featpaths={}\n\t\tif (not k in self.feats):\n\t\t\tself.feats[k] = v\n\t\telse:\n\t\t\tif type(self.feats[k])==type([]):\n\t\t\t\tself.feats[k].append(v)\n\t\t\telse:\n\t\t\t\tobj=self.feats[k]\n\t\t\t\tself.feats[k]=[obj,v]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef feature(self,feat=None,searchforit=False,init=None):\n\t\tif feat==None:\n\t\t\treturn self.feats\n\n\t\tif not init:\n\t\t\tinit=self\n\t\t\tinit.tick=0\n\t\t\tinit._matches=[]\n\t\t\tfeat=feat.strip()\n\t\t\tif feat.startswith(\"+\"):\n\t\t\t\tinit._eval=True\n\t\t\t\tfeat=feat[1:]\n\t\t\telif feat.startswith(\"-\"):\n\t\t\t\tinit._eval=False\n\t\t\t\tfeat=feat[1:]\n\t\t\telse:\n\t\t\t\tinit._eval=None\n\n\t\tif (hasattr(self,'feats')) and (feat in self.feats):\n\t\t\tif type(self.feats[feat]) == type([]):\n\t\t\t\tif len(self.feats[feat]) > 1:\n\t\t\t\t\treturn self.feats[feat]\n\t\t\t\telse:\n\t\t\t\t\treturn self.feats[feat][0]\n\t\t\telse:\n\t\t\t\treturn self.feats[feat]\n\t\telse:\n\t\t\tif searchforit:\n\n\t\t\t\tfor child in self.descendants():\n\t\t\t\t\tinit.tick+=1\n\t\t\t\t\tx=child.feature(feat,searchforit,init)\n\t\t\t\t\t#print init.tick, self.classname(), child.classname(), x\n\t\t\t\t\tif x==None: continue\n\t\t\t\t\tinit._matches.append ( (child,x) )\n\n\n\t\t\t\t#return [child.feature(feat,searchforit) for child in self.descendants()]\n\t\t\telse:\n\t\t\t\treturn None\n\t\t\t#if searchforit:\n\t\t\t#\treturn self.search(SearchTerm(feat))\n\t\t\t#else:\n\t\t\t#\tNone\n\n\t\tif self==init:\n\n\t\t\tif init._eval==None:\n\t\t\t\treturn init._matches\n\t\t\telse:\n\t\t\t\treturn [ x for (x,y) in init._matches if bool(y)==init._eval ]", "response": "returns value of self.feats [ feat ] if found returns None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef u2s(self,u):\n\n\t\ttry:\n\t\t\treturn u.encode('utf-8',errors='ignore')\n\t\texcept (UnicodeDecodeError,AttributeError) as e:\n\t\t\ttry:\n\t\t\t\treturn str(u)\n\t\t\texcept UnicodeEncodeError:\n\t\t\t\treturn unicode(u).encode('utf-8',errors='ignore')", "response": "Returns an ASCII representation of the Unicode string u."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlike givebirth but appends the new child to the list of children.", "response": "def newchild(self,chld=False):\n\t\t\"\"\"Like givebirth(), but also appends the new child to the list of children.\"\"\"\n\n\t\tif not chld:\n\t\t\tchld = self.givebirth()\n\n\t\tlchld=[chld] if type(chld)!=list else chld\n\t\tfor chldx in lchld: chldx.parent=self\n\t\tself.children.append(chld)\n\n\t\treturn chld"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ents(self,cls=\"Word\",flattenList=True):\n\n\t\tents = []\n\t\t\"\"\"\n\t\tprint 'getting entities',self.classname()\n\t\tif self.classname() == cls:\n\t\t\treturn [self]\n\t\telse:\n\t\t\tfor child in self.children:\n\t\t\t\tif type(child)==type([]):\n\t\t\t\t\tif flattenList:\n\t\t\t\t\t\tents+=child[0].ents(cls=cls,flattenList=flattenList)\n\t\t\t\t\telse:\n\t\t\t\t\t\tents_list2ndlevel=[]\n\t\t\t\t\t\tfor chld in child:\n\t\t\t\t\t\t\tif chld:\n\t\t\t\t\t\t\t\tents_list2ndlevel+=chld.ents(cls=cls,flattenList=flattenList)\n\t\t\t\t\t\tents+=[ents_list2ndlevel]\n\n\t\t\t\telse:\n\t\t\t\t\tif child:\n\t\t\t\t\t\tents += child.ents(cls=cls,flattenList=flattenList)\n\t\t\"\"\"\n\t\t#print 'getting entities',self.classname()\n\t\tif self.classname() == cls:\n\t\t\treturn [self]\n\t\telse:\n\t\t\tfor child in self.children:\n\t\t\t\t#print child,child.classname()\n\t\t\t\tif child.classname()=='WordToken':\n\t\t\t\t\tif cls=='WordToken':\n\t\t\t\t\t\tents+=[child]\n\t\t\t\t\telif not child.children:\n\t\t\t\t\t\tpass\n\t\t\t\t\telif cls=='Word':\n\t\t\t\t\t\tif flattenList:\n\t\t\t\t\t\t\tents+=[child.children[0]]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tents+=[child.children]\n\t\t\t\t\telse:\n\t\t\t\t\t\tif child:\n\t\t\t\t\t\t\tents += child.children[0].ents(cls=cls,flattenList=flattenList)\n\t\t\t\telse:\n\t\t\t\t\tif child:\n\t\t\t\t\t\tents += child.ents(cls=cls,flattenList=flattenList)\n\n\n\t\treturn ents", "response": "Returns a list of entities of the specified class specified the second argument."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wordtokens(self,include_punct=True):\n\t\tws=self.ents('WordToken')\n\t\tif not include_punct: return [w for w in ws if not w.is_punct]\n\t\treturn ws", "response": "Returns a list of this object s Words in order of their appearance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint the basic stats about the loaded text.", "response": "def show(self):\n\t\t\"\"\"'Prints' (using self.om()) the basic stats about the loaded text. Eg:\n\t\t\t001776\tecstatic            \tP:'ecs.ta.tic                      \tS:PUU\tW:HLH\n\t\t\t001777\tbreath              \tP:'bre.ath                         \tS:PU\tW:LH\n\t\t\"\"\"\n\n\n\t\tif self.classname()==\"Corpus\":\n\t\t\tfor text in self.children:\n\t\t\t\ttext.om(\"## showing Text \" + text.name)\n\t\t\t\ttext.show()\n\t\telse:\n\t\t\twords=self.words()\n\t\t\tfor i in range(len(words)):\n\t\t\t\tword=words[i]\n\t\t\t\tword.om(str(i+1).zfill(6)+\"\\t\"+str(word.output_minform()),conscious=False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow this object s attributes and methods.", "response": "def dir(self,methods=True,showall=True):\n\t\t\"\"\"Show this object's attributes and methods.\"\"\"\n\t\timport inspect\n\t\t#print \"[attributes]\"\n\t\tfor k,v in sorted(self.__dict__.items()):\n\t\t\tif k.startswith(\"_\"): continue\n\t\t\tprint makeminlength(\".\"+k,being.linelen),\"\\t\",v\n\n\t\tif not methods:\n\t\t\treturn\n\n\t\tentmethods=dir(entity)\n\n\t\tprint\n\t\t#print \"[methods]\"\n\t\tfor x in [x for x in dir(self) if (\"bound method \"+self.classname() in str(getattr(self,x))) and not x.startswith(\"_\")]:\n\t\t\tif (not showall) and (x in entmethods): continue\n\n\t\t\tattr=getattr(self,x)\n\n\t\t\t#print attr.__dict__\n\t\t\t#print dir(attr)\n\n\t\t\t#doc=inspect.getdoc(attr)\n\t\t\tdoc = attr.__doc__\n\t\t\tif not doc:\n\t\t\t\tdoc=\"\"\n\t\t\t#else:\n\t\t\t#\tdocsplit=[z for z in doc.replace(\"\\r\",\"\\n\").split(\"\\n\") if z]\n\t\t\t#\tif len(docsplit)>1:\n\t\t\t#\t\tdoc = docsplit[0] + \"\\n\" + makeminlength(\" \",being.linelen) + \"\\n\".join( makeminlength(\" \",being.linelen)+\"\\t\"+z for z in docsplit[1:])\n\t\t\t#\telse:\n\t\t\t#\t\tdoc = docsplit[0]\n\t\t\ty=describe_func(attr)\n\t\t\tif not y:\n\t\t\t\ty=\"\"\n\t\t\telse:\n\t\t\t\ty=\", \".join(a+\"=\"+str(b) for (a,b) in y)\n\t\t\tprint makeminlength(\".\"+x+\"(\"+y+\")\",being.linelen),\"\\t\", doc\n\t\t\tif showall: print"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef str_ipasyllstress(self):\n\n\t\tphonsylls=[\"\".join([str(x) for x in child.phonemes()]) for child in self.children]\n\t\ttry:\n\t\t\tif hasattr(self,'stress'):\n\t\t\t\tfor i in range(len(self.stress)):\n\t\t\t\t\tphonsylls[i]=entity.stress_str2strikes[self.stress[i]]+phonsylls[i]\n\t\texcept IndexError:\n\t\t\tprint \"<\"+self.classname()+\" creation failed on:\\n\"+str(self)+\"\\n\"\n\t\treturn \".\".join(phonsylls)", "response": "Returns a string representation of the self - object\n\t in a syllabified stressed IPA form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findattr(self,attr,connector='parent'):\n\t\tif (not hasattr(self,attr)):\n\t\t\tif (not hasattr(self,connector)):\n\t\t\t\treturn None\n\t\t\telse:\n\t\t\t\tcon=getattr(self,connector)\n\t\t\t\tif not con:\n\t\t\t\t\treturn None\n\t\t\t\tif type(con)==type([]):\n\t\t\t\t\treturn [x.findattr(attr,connector) for x in con]\n\t\t\t\telse:\n\t\t\t\t\treturn con.findattr(attr,connector)\n\t\telse:\n\t\t\treturn getattr(self,attr)", "response": "Returns the attribute named attr from either the self or the parents ( recursively )."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot(self,init=None):\n\n\t\tif not init:\n\t\t\tinit=self\n\t\t\tinit.plotstats={}\n\t\t\tinit.unitfeats=self._getFeatValDict()\n\n\t\t\tsels=[]\n\t\t\tfor k,v in sorted(init.unitfeats.items()):\n\t\t\t\tfor kk,vv in v.items():\n\t\t\t\t\tsels.append((k,kk))\n\n\t\t\tconditions={'x':[],'y':[]}\n\t\t\ttargets={'x':[],'y':[]}\n\t\t\tpkey={'x':[],'y':[]}\n\n\t\t\tprint str(0)+\"\\t[no condition]\"\n\t\t\tfor selnum in range(len(sels)):\n\t\t\t\tprint str(selnum+1)+\"\\t\"+str(sels[selnum][0])+\" = \"+str(sels[selnum][1])\n\t\t\tprint\n\n\t\t\tstepnum=0\n\t\t\tfor a in sorted(conditions.keys()):\n\t\t\t\tstepnum+=1\n\t\t\t\tsel=raw_input(\">> [step \"+str(stepnum)+\"/4] [\"+a+\" coord] [conditions of population] please type in the number (or numbers separated by commas)\\n\\tof the conditions determining the total population from which the percentage of the sample is taken:\\n\").strip()\n\t\t\t\tfor x in sel.split(\",\"):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tconditions[a].append(sels[int(x)-1])\n\t\t\t\t\t\tpkey[a]+=[\"of_\"+str(sels[int(x)-1][0])+\"_is_\"+str(sels[int(x)-1][1])]\n\t\t\t\t\texcept:\n\t\t\t\t\t\tpass\n\n\t\t\t\tstepnum+=1\n\t\t\t\tsel=raw_input(\">> [step \"+str(stepnum)+\"/4] [\"+a+\" coord] [conditions of sample] please type in the number (or numbers separated by commas)\\n\\tof the conditions determining the sample:\\n\").strip()\n\t\t\t\tfor x in sel.split(\",\"):\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttargets[a].append(sels[int(x)-1])\n\t\t\t\t\t\tpkey[a]+=[str(sels[int(x)-1][0])+\"_is_\"+str(sels[int(x)-1][1])]\n\t\t\t\t\texcept:\n\t\t\t\t\t\tpass\n\n\n\t\t\t# print \">> POPULATION:\"\n\t\t\t# print conditions\n\t\t\t#\n\t\t\t# print \">> SAMPLE:\"\n\t\t\t# print targets\n\n\t\t\tinit.population=conditions\n\t\t\tinit.sample=targets\n\n\t\t\tinit.pkey=\"X_\"+\"-\".join(pkey['x'])+\".\"+\"Y_\"+\"-\".join(pkey['y'])\n\t\t\tprint \">> plotting: \"+ init.pkey\n\n\n\t\tif not hasattr(self,'bestParses'):\n\t\t\tfor child in self.children:\n\t\t\t\tchild.plot(init)\n\t\telse:\n\t\t\tposdict={}\n\t\t\tminparselen = min([len(parse.positions) for parse in self.bestParses()])\n\t\t\tmaxparselen = max([len(parse.positions) for parse in self.bestParses()])\n\t\t\t#if not hasattr(self,'minparselen'): return None\n\t\t\tfor posnum in range(maxparselen):\n\t\t\t\tposdict[posnum]={'x':[],'y':[]}\n\t\t\t\tfor parse in self.bestParses():\n\t\t\t\t\ttry:\n\t\t\t\t\t\tposfeats=parse.positions[posnum].posfeats()\n\t\t\t\t\texcept IndexError:\n\t\t\t\t\t\t# there is no position number `posnum` in this parse `parse`\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tfor a in ['x','y']:\n\t\t\t\t\t\tconds=init.population[a]\n\t\t\t\t\t\ttargs=init.sample[a]\n\n\t\t\t\t\t\tcondsHold=True\n\t\t\t\t\t\tif len(conds):\n\t\t\t\t\t\t\tfor cond in conds:\n\t\t\t\t\t\t\t\tcondK=cond[0]\n\t\t\t\t\t\t\t\tcondV=cond[1]\n\n\t\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t\tif posfeats[condK]==condV:\n\t\t\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\t\tcondsHold=False\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\tcondsHold=False\n\t\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\tif condsHold:\n\t\t\t\t\t\t\ttargsHold=True\n\t\t\t\t\t\t\tfor targ in targs:\n\t\t\t\t\t\t\t\ttargK=targ[0]\n\t\t\t\t\t\t\t\ttargV=targ[1]\n\n\t\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t\tif posfeats[targK]==targV:\n\t\t\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\t\ttargsHold=False\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\ttargsHold=False\n\t\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\t\tif targsHold:\n\t\t\t\t\t\t\t\tposdict[posnum][a].append(1)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tposdict[posnum][a].append(0)\n\n\t\t\tfor a in ['x','y']:\n\t\t\t\tif not posdict[posnum][a]:\n\t\t\t\t\tprint \"<< not enough data: position number (\"+str(posnum)+\") empty on dimension [\"+str(a)+\"]\"\n\t\t\t\t\treturn None\n\n\t\t\tinit.plotstats[self.getName()]=posdict\n\t\t\tif (self!=init): return None\n\n\t\ttotalstrs=[]\n\t\ttotaltsvs=[]\n\t\tfor textname,posdict in sorted(init.plotstats.items()):\n\n\t\t\ttsv=\"posnum\\tnumobs\\tx_mean\\ty_mean\\tx_std\\ty_std\\n\"\n\t\t\txs=[]\n\t\t\tys=[]\n\t\t\tfor posnum,xydict in posdict.items():\n\t\t\t\tx_avg,x_std=mean_stdev(xydict['x'])\n\t\t\t\ty_avg,y_std=mean_stdev(xydict['y'])\n\n\t\t\t\tassert len(xydict['x'])==len(xydict['y'])\n\n\t\t\t\txs.append(x_avg)\n\t\t\t\tys.append(y_avg)\n\t\t\t\ttsv+=\"\\t\".join(str(bb) for bb in [(posnum+1),len(xydict['x']),x_avg,y_avg,x_std,y_std])+\"\\n\"\n\n\n\t\t\tccmsg=\"\"\n\t\t\tcc=None\n\t\t\tp=None\n\t\t\ttry:\n\t\t\t\tfrom statlib import stats\n\t\t\t\t(cc,p)=stats.pearsonr(xs,ys)\n\n\t\t\t\taa=makeminlength(\"    correlation coefficient: \",int(being.linelen/1.4))+str(cc)\n\t\t\t\tbb=makeminlength(\"    p-value: \",int(being.linelen/1.4))+str(p)\n\n\t\t\t\ttsv+=aa.strip().replace(\":\",\":\\t\")+\"\\n\"\n\t\t\t\ttsv+=bb.strip().replace(\":\",\":\\t\")+\"\\n\"\n\n\t\t\t\tfor l in tsv.split(\"\\n\"):\n\t\t\t\t\ttotaltsvs.append(textname+\"_\"+l)\n\n\t\t\t\tccmsg+=aa+\"\\n\"+bb+\"\\n\"\n\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\t\twriteToFile(textname,init.pkey,tsv,extension=\"tsv\")\n\t\t\ttotaltsvs.append(tsv)\n\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\tstrtowrite=self.makeBubbleChart(posdict,\".\".join([textname,init.pkey]),(cc,p))\n\t\t\t\ttotalstrs+=[strtowrite]\n\t\t\t\twriteToFile(textname,init.pkey,self._getBubbleHeader()+strtowrite+self._getBubbleFooter(),extension=\"htm\")\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\"\"\"\n\n\t\t\tif ccmsg:\n\t\t\t\tprint ccmsg\n\n\t\tif not self.classname()==\"Corpus\": return None\n\n\t\t\"\"\"\n\t\twriteToFile(self.getName(),\n\t\t\tinit.pkey,\n\t\t\tself._getBubbleHeader()+\"\\n<br/><br/><br/><br/><br/><br/><br/><br/>\\n\".join(totalstrs)+self._getBubbleFooter(),\n\t\t\tiscorpus=True,\n\t\t\textension=\"htm\")\n\t\t\"\"\"\n\n\t\twriteToFile(self.getName(),init.pkey,\"\\n\\n\\n\\n\".join(totaltsvs),iscorpus=True,extension=\"tsv\")", "response": "Interactive plotting of parsing features."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns HTML for a bubble chart of the positin dictionary.", "response": "def makeBubbleChart(self,posdict,name,stattup=None):\n\t\t\"\"\"Returns HTML for a bubble chart of the positin dictionary.\"\"\"\n\n\t\txname=[x for x in name.split(\".\") if x.startswith(\"X_\")][0]\n\t\tyname=[x for x in name.split(\".\") if x.startswith(\"Y_\")][0]\n\t\t#elsename=name.replace(xname,'').replace(yname,'').replace('..','.').replace('..','.')\n\n\n\n\t\to='<div id=\"'+name+'\"><h2>'+name+'</h2>'\n\n\t\tif stattup:\n\t\t\tcc=stattup[0]\n\t\t\tp=stattup[1]\n\t\t\to+='<h3>corr.coef='+str(cc)+' / p-value='+str(p)+'</h3>'\n\n\t\to+='<br/><script type=\"text/javascript\">\\nvar myChart = new Chart.Bubble(\"'+name+'\", {\\nwidth: 400,\\nheight: 400,\\n bubbleSize: 10,\\nxlabel:\"'+xname+'\",\\nylabel:\"'+yname+'\"});\\n'\n\n\t\tfor posnum,xydict in posdict.items():\n\t\t\tx_avg,x_std=mean_stdev(xydict['x'])\n\t\t\ty_avg,y_std=mean_stdev(xydict['y'])\n\n\t\t\tz=1/(x_std+y_std)\n\n\t\t\to+='myChart.addBubble('+str(x_avg*100)+', '+str(y_avg*100)+', '+str(z)+', \"#666\", \"'+str(posnum+1)+' [%'+str(x_avg*100)[0:5]+', %'+str(y_avg*100)[0:5]+']\");\\n'\n\t\to+='myChart.redraw();\\n</script>\\n</div>'\n\t\treturn o"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Name string for this object.", "response": "def getName(self):\n\t\t\"\"\"Return a Name string for this object.\"\"\"\n\n\t\tname=self.findattr('name')\n\t\tif not name:\n\t\t\tname=\"_directinput_\"\n\t\t\tif self.classname().lower()==\"line\":\n\t\t\t\tname+=\".\"+str(self).replace(\" \",\"_\").lower()\n\t\telse:\n\t\t\tname=name.replace('.txt','')\n\n\t\twhile name.startswith(\".\"):\n\t\t\tname=name[1:]\n\n\t\treturn name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting out the header column for line - scansions for a given meter.", "response": "def scansion_prepare(self,meter=None,conscious=False):\n\t\t\"\"\"Print out header column for line-scansions for a given meter. \"\"\"\n\t\timport prosodic\n\t\tconfig=prosodic.config\n\n\t\tif not meter:\n\t\t\tif not hasattr(self,'_Text__bestparses'): return\n\t\t\tx=getattr(self,'_Text__bestparses')\n\t\t\tif not x.keys(): return\n\t\t\tmeter=x.keys()[0]\n\n\t\tckeys=\"\\t\".join(sorted([str(x) for x in meter.constraints]))\n\t\tself.om(\"\\t\".join([makeminlength(str(\"text\"),config['linelen']), makeminlength(str(\"parse\"),config['linelen']),\"meter\",\"num_parses\",\"num_viols\",\"score_viols\",ckeys]),conscious=conscious)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints all parses and their violations in a structured format.", "response": "def report(self,meter=None,include_bounded=False,reverse=True):\n\t\t\"\"\" Print all parses and their violations in a structured format. \"\"\"\n\n\t\tReportStr = ''\n\t\tif not meter:\n\t\t\tfrom Meter import Meter\n\t\t\tmeter=Meter.genDefault()\n\t\tif (hasattr(self,'allParses')):\n\t\t\tself.om(unicode(self))\n\t\t\tallparses=self.allParses(meter=meter,include_bounded=include_bounded)\n\t\t\tnumallparses=len(allparses)\n\t\t\t#allparses = reversed(allparses) if reverse else allparses\n\t\t\tfor pi,parseList in enumerate(allparses):\n\t\t\t\tline=self.iparse2line(pi).txt\n\t\t\t\t#parseList.sort(key = lambda P: P.score())\n\t\t\t\thdr=\"\\n\\n\"+'='*30+'\\n[line #'+str(pi+1)+' of '+str(numallparses)+']: '+line+'\\n\\n\\t'\n\t\t\t\tftr='='*30+'\\n'\n\t\t\t\tReportStr+=self.om(hdr+meter.printParses(parseList,reverse=reverse).replace('\\n','\\n\\t')[:-1]+ftr,conscious=False)\n\t\telse:\n\t\t\tfor child in self.children:\n\t\t\t\tif type(child)==type([]): continue\n\t\t\t\tReportStr+=child.report()\n\n\t\treturn ReportStr"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a tree - structure of this object s phonological representation.", "response": "def tree(self,offset=0,prefix_inherited=\"\",nofeatsplease=['Phoneme']):\n\t\t\"\"\"Print a tree-structure of this object's phonological representation.\"\"\"\n\n\t\ttree = \"\"\n\t\tnumchild=0\n\t\tfor child in self.children:\n\t\t\tif type(child)==type([]):\n\t\t\t\tchild=child[0]\n\t\t\tnumchild+=1\n\n\t\t\tclassname=child.classname()\n\t\t\tif classname==\"Word\":\n\t\t\t\ttree+=\"\\n\\n\"\n\t\t\telif classname==\"Line\":\n\t\t\t\ttree+=\"\\n\\n\\n\"\n\t\t\telif classname==\"Stanza\":\n\t\t\t\ttree+=\"\\n\\n\\n\\n\"\n\n\t\t\tif offset!=0:\n\t\t\t\ttree+=\"\\n\"\n\t\t\t\tfor i in range(0,offset):\n\t\t\t\t\ttree+=\"      \"\n\t\t\t\t#if not len(child.feats):\n\t\t\t\t#\ttree+=\"\t  \"\n\t\t\t\ttree+=\"|\"\n\n\n\t\t\ttree+=\"\\n\"\n\t\t\tnewline=\"\"\n\t\t\tfor i in range(0,offset):\n\t\t\t\tnewline+=\"      \"\n\t\t\tnewline+=\"|\"\n\n\n\t\t\tcname=\"\"\n\t\t\tfor letter in classname:\n\t\t\t\tif letter==letter.upper():\n\t\t\t\t\tcname+=letter\n\n\t\t\tprefix=prefix_inherited+cname+str(numchild) + \".\"\n\t\t\tnewline+=\"-----| (\"+prefix[:-1]+\") <\"+classname+\">\"\n\t\t\tif child.isBroken():\n\t\t\t\tnewline+=\"<<broken>>\"\n\t\t\telse:\n\t\t\t\tstring=self.u2s(child)\n\t\t\t\tif (not \"<\" in string):\n\t\t\t\t\tnewline=makeminlength(newline,99)\n\t\t\t\t\tnewline+=\"[\"+string+\"]\"\n\t\t\t\telif string[0]!=\"<\":\n\t\t\t\t\tnewline+=\"\\t\"+string\n\t\t\t\tif len(child.feats):\n\t\t\t\t\tif (not child.classname() in nofeatsplease):\n\t\t\t\t\t\tfor k,v in sorted(child.feats.items()):\n\t\t\t\t\t\t\tif v==None:\n\t\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t\t\tnewline+=\"\\n\"\n\t\t\t\t\t\t\tfor i in range(0,offset+1):\n\t\t\t\t\t\t\t\tnewline+=\"      \"\n\t\t\t\t\t\t\tnewline+=\"|     \"\n\t\t\t\t\t\t\tnewline+=self.showFeat(k,v)\n\n\n\t\t\ttree+=newline\n\t\t\ttree+=child.tree(offset+1,prefix)\n\n\t\treturn tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self,query_string,toprint=False):\n\t\tqq=SearchTerm(query_string)\n\t\tmatchcount=0\n\t\tmatches=[]\n\t\tfor word in self.words():\n\t\t\tfor match in word.search(qq):\n\t\t\t\tmatches.append(match)\n\t\t\t\tmatchcount+=1\n\t\t\t\tif \"Word\" in str(type(match)):\n\t\t\t\t\tmatchstr=\"\"\n\t\t\t\telse:\n\t\t\t\t\tmatchstr=str(match)\n\t\t\t\tif toprint:\n\t\t\t\t\tword.om(makeminlength(str(matchcount),int(being.linelen/6))+\"\\t\"+makeminlength(str(word),int(being.linelen))+\"\\t\"+matchstr)\n\t\treturn matches", "response": "Prints words matching the given query string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns objects matching the query.", "response": "def search(self, searchTerm):\n\t\t\"\"\"Returns objects matching the query.\"\"\"\n\t\tif type(searchTerm)==type(''):\n\t\t\tsearchTerm=SearchTerm(searchTerm)\n\n\t\tif searchTerm not in self.featpaths:\n\t\t\tmatches = None\n\t\t\tif searchTerm.type != None and searchTerm.type != self.classname():\n\t\t\t\tmatches = self._searchInChildren(searchTerm)\n\t\t\telif searchTerm.isAtomic():\n\t\t\t\tmatches = self._searchSingleTerm(searchTerm)\n\t\t\telse:\n\t\t\t\tmatches = self._searchMultipleTerms(searchTerm)\n\t\t\t\tif matches == True:\n\t\t\t\t\tmatches = [self]\n\t\t\t\tif matches == False:\n\t\t\t\t\tmatches = []\n\t\t\tself.featpaths[searchTerm] = matches\n\n\t\treturn self.featpaths[searchTerm]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stats_positions(self,meter=None,all_parses=False):\n\n\t\t\"\"\"Produce statistics from the parser\"\"\"\n\n\t\t\"\"\"Positions\n\t\tAll feats of slots\n\t\tAll constraint violations\n\n\n\t\t\"\"\"\n\t\tparses = self.allParses(meter=meter) if all_parses else [[parse] for parse in self.bestParses(meter=meter)]\n\n\t\tdx={}\n\t\tfor parselist in parses:\n\n\t\t\tfor parse in parselist:\n\t\t\t\tif not parse: continue\n\t\t\t\tslot_i=0\n\t\t\t\tfor pos in parse.positions:\n\t\t\t\t\tfor slot in pos.slots:\n\t\t\t\t\t\tslot_i+=1\n\n\t\t\t\t\t\tfeat_dicts = [slot.feats, pos.constraintScores, pos.feats]\n\t\t\t\t\t\tfor feat_dict in feat_dicts:\n\t\t\t\t\t\t\tfor k,v in feat_dict.items():\n\t\t\t\t\t\t\t\tdk = (slot_i,str(k))\n\t\t\t\t\t\t\t\tif not dk in dx: dx[dk]=[]\n\t\t\t\t\t\t\t\tdx[dk]+=[v]\n\n\n\t\tdef _writegen():\n\t\t\tfor ((slot_i,k),l) in sorted(dx.items()):\n\t\t\t\tl2=[]\n\t\t\t\tfor x in l:\n\t\t\t\t\tif type(x)==bool:\n\t\t\t\t\t\tx=1 if x else 0\n\t\t\t\t\telif type(x)==type(None):\n\t\t\t\t\t\tx=0\n\t\t\t\t\telif type(x) in [str,unicode]:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\tx=float(x)\n\t\t\t\t\tif x>1: x=1\n\t\t\t\t\tl2+=[x]\n\t\t\t\t#print k, l2\n\t\t\t\t#try:\n\t\t\t\tif not l2: continue\n\n\t\t\t\tavg=sum(l2) / float(len(l2))\n\t\t\t\tcount=sum(l2)\n\t\t\t\tchances=len(l2)\n\t\t\t\t#except TypeError:\n\t\t\t\t#\tcontinue\n\n\t\t\t\todx={'slot_num':slot_i, 'statistic':k, 'average':avg, 'count':count, 'chances':chances, 'text':self.name}\n\t\t\t\todx['header']=['slot_num', 'statistic','count','chances','average']\n\t\t\t\t#print odx\n\t\t\t\tyield odx\n\n\t\tname=self.name.replace('.txt','')\n\t\tofn=os.path.join(self.dir_results, 'stats','texts',name, name+'.positions.csv')\n\t\t#print ofn\n\t\tif not os.path.exists(os.path.split(ofn)[0]): os.makedirs(os.path.split(ofn)[0])\n\t\tfor dx in writegengen(ofn, _writegen):\n\t\t\tyield dx\n\t\tprint '>> saved:',ofn", "response": "Produce statistics for all positions of the parser."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iparse(self,meter=None,num_processes=1,arbiter='Line',line_lim=None):\n\t\tfrom Meter import Meter,genDefault,parse_ent,parse_ent_mp\n\t\timport multiprocessing as mp\n\t\tmeter=self.get_meter(meter)\n\n\t\t# set internal attributes\n\t\tself.__parses[meter.id]=[]\n\t\tself.__bestparses[meter.id]=[]\n\t\tself.__boundParses[meter.id]=[]\n\t\tself.__parsed_ents[meter.id]=[]\n\n\t\tlines = self.lines()\n\t\tlines=lines[:line_lim]\n\t\tnumlines = len(lines)\n\n\t\tinit=self\n\t\tents=self.ents(arbiter)\n\t\tsmax=self.config.get('line_maxsylls',100)\n\t\tsmin=self.config.get('line_minsylls',0)\n\t\t#print '>> # of lines to parse:',len(ents)\n\t\tents = [e for e in ents if e.num_syll >= smin and e.num_syll<=smax]\n\t\t#print '>> # of lines to parse after applying min/max line settings:',len(ents)\n\n\t\tself.scansion_prepare(meter=meter,conscious=True)\n\n\t\tnuments=len(ents)\n\n\t\t#pool=mp.Pool(1)\n\t\ttoprint=self.config['print_to_screen']\n\t\tobjects = [(ent,meter,init,False) for ent in ents]\n\n\t\tif num_processes>1:\n\t\t\tprint '!! MULTIPROCESSING PARSING IS NOT WORKING YET !!'\n\t\t\tpool = mp.Pool(num_processes)\n\t\t\tjobs = [pool.apply_async(parse_ent_mp,(x,)) for x in objects]\n\t\t\tfor j in jobs:\n\t\t\t\tprint j.get()\n\t\t\t\tyield j.get()\n\t\telse:\n\t\t\tnow=time.time()\n\t\t\tclock_snum=0\n\t\t\t#for ei,ent in enumerate(pool.imap(parse_ent_mp,objects)):\n\t\t\tfor ei,objectx in enumerate(objects):\n\t\t\t\tclock_snum+=ent.num_syll\n\t\t\t\tif ei and not ei%100:\n\t\t\t\t\tnownow=time.time()\n\t\t\t\t\tif self.config['print_to_screen']:\n\t\t\t\t\t\tprint '>> parsing line #',ei,'of',numents,'lines','[',round(float(clock_snum/(nownow-now)),2),'syllables/second',']'\n\t\t\t\t\tnow=nownow\n\t\t\t\t\tclock_snum=0\n\n\t\t\t\tyield parse_ent_mp(objectx)\n\n\t\tif self.config['print_to_screen']:\n\t\t\tprint '>> parsing complete in:',time.time()-now,'seconds'", "response": "Parse this text metrically yielding it line by line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting out the parses and their violations in scansion format.", "response": "def scansion(self,meter=None,conscious=False):\n\t\t\"\"\"Print out the parses and their violations in scansion format.\"\"\"\n\t\tmeter=self.get_meter(meter)\n\t\tself.scansion_prepare(meter=meter,conscious=conscious)\n\t\tfor line in self.lines():\n\t\t\ttry:\n\t\t\t\tline.scansion(meter=meter,conscious=conscious)\n\t\t\texcept AttributeError:\n\t\t\t\tprint \"!!! Line skipped [Unknown word]:\"\n\t\t\t\tprint line\n\t\t\t\tprint line.words()\n\t\t\t\tprint"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of lists of parses.", "response": "def allParses(self,meter=None,include_bounded=False,one_per_meter=True):\n\t\t\"\"\"Return a list of lists of parses.\"\"\"\n\n\t\tmeter=self.get_meter(meter)\n\t\ttry:\n\t\t\tparses=self.__parses[meter.id]\n\n\t\t\tif one_per_meter:\n\t\t\t\ttoreturn=[]\n\t\t\t\tfor _parses in parses:\n\t\t\t\t\tsofar=set()\n\t\t\t\t\t_parses2=[]\n\t\t\t\t\tfor _p in _parses:\n\t\t\t\t\t\t_pm=_p.str_meter()\n\t\t\t\t\t\tif not _pm in sofar:\n\t\t\t\t\t\t\tsofar|={_pm}\n\t\t\t\t\t\t\tif _p.isBounded and _p.boundedBy.str_meter() == _pm:\n\t\t\t\t\t\t\t\tpass\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t_parses2+=[_p]\n\t\t\t\t\ttoreturn+=[_parses2]\n\t\t\t\tparses=toreturn\n\n\t\t\tif include_bounded:\n\t\t\t\tboundedParses=self.boundParses(meter)\n\t\t\t\treturn [bp+boundp for bp,boundp in zip(toreturn,boundedParses)]\n\t\t\telse:\n\t\t\t\treturn parses\n\n\t\texcept (KeyError,IndexError) as e:\n\t\t\treturn []"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of the best parse per line.", "response": "def bestParses(self,meter=None):\n\t\t\"\"\"Return a list of the best parse per line.\"\"\"\n\t\tmeter=self.get_meter(meter)\n\t\ttry:\n\t\t\treturn self.__bestparses[meter.id]\n\t\texcept (KeyError,IndexError) as e:\n\t\t\treturn []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of the best parse per line.", "response": "def boundParses(self,meter=None,include_stressbounds=False):\n\t\t\"\"\"Return a list of the best parse per line.\"\"\"\n\t\tmeter=self.get_meter(meter)\n\t\ttry:\n\t\t\ttoreturn=[]\n\t\t\tfor _parses in self.__boundParses[meter.id]:\n\t\t\t\tsofar=set()\n\t\t\t\t_parses2=[]\n\t\t\t\tfor _p in _parses:\n\t\t\t\t\t_pm=_p.str_meter()\n\t\t\t\t\tif not _pm in sofar:\n\t\t\t\t\t\tif _p.isBounded and _p.boundedBy.str_meter() == _pm:\n\t\t\t\t\t\t\tpass\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsofar|={_pm}\n\t\t\t\t\t\t\t_parses2+=[_p]\n\t\t\t\ttoreturn+=[_parses2]\n\t\t\treturn toreturn\n\n\t\texcept (KeyError,IndexError) as e:\n\t\t\treturn [[]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validlines(self):\n\n\t\treturn [ln for ln in self.lines() if (not ln.isBroken() and not ln.ignoreMe)]", "response": "Return all lines within which Prosodic understood all words."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has(self,word):\n\t\tif not word: return False\n\t\tword=unicode(word)\n\t\t(p0,word,p1)=gleanPunc2(word)\n\t\tword_l = word.lower()\n\n\n\t\t## if not there, but a contractino\n\t\t# if already there, say yes\n\t\tif word_l in self.dict['Word'] and self.dict['Word'][word_l]: return True\n\t\t\"\"\"\n\t\tfor contr,add_ipa in [(\"'s\",\"z\"), (\"'d\",\"d\")]:\n\t\t\tif word_l.endswith(contr):\n\t\t\t\tword_l_unc = word_l[:-2]\n\t\t\t\t# if the uncontracted in the dictionary\n\t\t\t\tif word_l_unc in self.dict['Word'] and self.dict['Word'][word_l_unc]:\n\t\t\t\t\tfor obj in self.dict['Word'][word_l_unc]:\n\t\t\t\t\t\tif type(obj) in [tuple]:\n\t\t\t\t\t\t\tipa,sylls_text=obj\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tipa=obj.ipa\n\t\t\t\t\t\t\tsylls_text=obj.sylls_text\n\n\t\t\t\t\t\tipa+=add_ipa\n\t\t\t\t\t\t#sylls_text[-1]+=contr\n\n\t\t\t\t\t\t## save new word\n\t\t\t\t\t\tif not word_l in self.dict['Word']: self.dict['Word'][word_l]=[]\n\t\t\t\t\t\tself.dict['Word'][word_l]+=[(ipa,sylls_text)]\n\t\t\"\"\"\n\n\t\treturn (word_l in self.dict['Word'] and self.dict['Word'][word_l])", "response": "check if word is in the dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing the specified class and key.", "response": "def use(self,classtype,key):\n\t\t\"\"\"\n\t\tHACKED 9/29/16: No longer caching SyllableBodies. Reuse was causing bugs. More thorough solution would be helpful.\n\t\t\"\"\"\n\n\t\tif type(key)==type([]):\n\t\t\tkey=tuple(key)\n\t\tif (not key in self.dict[classtype]):\n\t\t\tif classtype in ['Phoneme','Onset','Nucleus','Coda','Rime','Syllable']:\n\t\t\t\tself.dict[classtype][key]=get_class(classtype+'.'+classtype)(key,self.lang)\n\t\t\t\t#return get_class(classtype+'.'+classtype)(key,self.lang)\n\t\t\telif classtype==\"SyllableBody\":\n\t\t\t\t#self.dict[classtype][key]=self.syllphon2syll(key,self.lang)\n\t\t\t\treturn self.syllphon2syll(key,self.lang)\n\n\t\treturn self.dict[classtype][key]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the human readable title of a given filter name.", "response": "def _get_filter_title(self, filter_name):\n        \"\"\"\n        Returns the human readable title of a given filter_name. If no title\n        attribute is set, the filter_name is used, where underscores are\n        replaced with whitespaces and the first character of each word is\n        uppercased. Example:\n\n        >>> MarkupFormatter._get_title('markdown')\n        'Markdown'\n\n        >>> MarkupFormatter._get_title('a_cool_filter_name')\n        'A Cool Filter Name'\n        \"\"\"\n        title = getattr(self.filter_list[filter_name], 'title', None)\n        if not title:\n            title = ' '.join([w.title() for w in filter_name.split('_')])\n        return title"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the filter list as a tuple. Useful for model choices.", "response": "def choices(self):\n        \"\"\"\n        Returns the filter list as a tuple. Useful for model choices.\n        \"\"\"\n        choice_list = getattr(\n            settings, 'MARKUP_CHOICES', DEFAULT_MARKUP_CHOICES\n        )\n        return [(f, self._get_filter_title(f)) for f in choice_list]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls this on a MeterPosition to return an integer representing the violation value for this Constraint in this MPos", "response": "def violationScore(self,meterPos,pos_i=None,slot_i=None,num_slots=None,all_positions=None,parse=None):\n\t\t\"\"\"call this on a MeterPosition to return an integer representing the violation value\n\t\tfor this Constraint in this MPos (0 represents no violation)\"\"\"\n\t\tviolation = None\n\t\tif self.constr != None:\n\t\t\tviolation = self.constr.parse(meterPos)\n\t\telse:\n\t\t\tviolation = self.__hardparse(meterPos,pos_i=pos_i,slot_i=slot_i,num_slots=num_slots,all_positions=all_positions,parse=parse)\n\t\t#violation = self.__hardparse(meterPos)\n\t\tif violation != \"*\":\n\t\t\tmeterPos.constraintScores[self] += violation\n\n\n\t\t\"\"\"\n\t\tprint\n\t\tprint '>>',slot_i,num_slots, self.name, meterPos, violation, all_positions\n\t\tfor slot in meterPos.slots:\n\t\t\tprint slot, slot.feature('prom.stress')\n\t\tprint\"\"\"\n\n\t\treturn violation"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __hardparse(self,meterPos,pos_i=None,slot_i=None,num_slots=None,all_positions=None,parse=None):\n\t\timport prosodic as p\n\t\t#if meterPos.slots[0].i<2:\n\t\t#\tprint meterPos.slots[0].word\n\n\t\t#print meterPos,pos_i,slot_i,num_slots,all_positions\n\t\t#prevpos=all_positions[pos_i-1]\n\t\t#print pos_i, meterPos, prevpos, pos_i,pos_i-1,all_positions, len(meterPos.slots)\n\n\t\tif '.' in self.name:\t# kiparsky self.names\n\t\t\t## load variables\n\n\t\t\t#exception for first foot\n\t\t\t#if 'skip_initial_foot' in parse.constraintNames:\n\t\t\t#\tif meterPos.slots[0].i<2:\n\t\t\t#\t\treturn 0\n\n\t\t\tif 'extrametrical-first-pos' in parse.constraintNames and pos_i==0:\n\t\t\t\treturn 0\n\t\t\telif 'skip_initial_foot' in parse.constraintNames and pos_i in [0,1]:\n\t\t\t\treturn 0\n\n\n\t\t\tpromSite = self.name.split(\".\")[1]\n\t\t\tpromType = self.name.split(\".\")[0]\n\t\t\tpromSite_meter = promSite.split(\"=>\")[0].strip()\t# s/w\n\t\t\tpromSite_prom = promSite.split(\"=>\")[1].strip()\t\t# +- u/p\n\n\t\t\tif meterPos.meterVal != promSite_meter:\t# then this constraint does not apply\n\t\t\t\treturn 0\n\n\t\t\tif promSite_prom[0:1] == \"-\":\t\t\t\t\t\t# -u or -p: eg, if s=>-u, then NOT EVEN ONE s can be u(nprom)\n\t\t\t\tpromSite_isneg = True\n\t\t\t\tpromSite_prom = promSite_prom[1:]\t\t\t\t# u or p\n\t\t\telse:\n\t\t\t\tpromSite_isneg = False\t\t\t\t\t\t\t# u or p: eg, if s=>p, then AT LEAST ONE s must be p(rom)\n\n\t\t\t\"\"\"\n\t\t\tRemoved 4/12/2017: apparently there was an option to restrict just 'P'rimary stresses\n\t\t\tBut required using an uppercase P in the meter config. This was nowhere stated elsewhere\n\t\t\tand has never been used. I'm disabling it. Let's just use a separate prominence type\n\t\t\tif we want to restrict only primary stresses.\n\n\t\t\tif promSite_prom.lower()==promSite_prom:\n\t\t\t\tpromSite_prom = (promSite_prom == 'p')\t\t\t\t# string 2 boolean: p:True, u:False\n\t\t\telse:\n\t\t\t\tif promSite_prom==\"P\":\n\t\t\t\t\tpromSite_prom=1.0\n\t\t\t\t#elif promSite_prom==\"U\":\n\t\t\t\telse:\n\t\t\t\t\tpromSite_prom=0.0\n\t\t\t\"\"\"\n\n\t\t\tpromSite_prom = (promSite_prom == 'p')\t\t\t\t# string 2 boolean: p:True, u:False\n\n\n\n\n\t\t\t# NOT EVEN ONE unit_prom can be promSite_prom:\n\t\t\tif promSite_isneg:\n\t\t\t\tnumtrue=0\n\t\t\t\tfor slot in meterPos.slots:\n\t\t\t\t\tslot_prom=slot.feature('prom.'+promType,True)\n\t\t\t\t\tif slot_prom==None: continue\n\n\t\t\t\t\t#if type(promSite_prom)==type(True):\n\t\t\t\t\t#\tslot_prom=bool(slot_prom)\n\t\t\t\t\tpstress_thresh=self.meter.config.get('phrasal_stress_threshold',PSTRESS_THRESH_DEFAULT)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tpstress_thresh=float(pstress_thresh)\n\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\tpstress_thresh=PSTRESS_THRESH_DEFAULT\n\n\t\t\t\t\tbool_prom_type = bool(slot_prom) if promType!='phrasal_stress' else slot_prom<=pstress_thresh\n\n\t\t\t\t\tif bool_prom_type == promSite_prom:\n\t\t\t\t\t\t#numtrue+=float(slot_prom)\n\t\t\t\t\t\treturn self.weight\n\t\t\t\t#return 2 if numtrue else 0\n\t\t\t\t#print self.weight, numtrue\n\t\t\t\t## CHANGED 10/10/2016: This constraint returns its weight\n\t\t\t\t## *times* the number of slots/syllables that violated it.\n\t\t\t\t## CHANGED 4/12/2017: numtrue is actually float of the prominence\n\t\t\t\t## so for phrasal stress is its p-stress value, for seconday stress is 0.5, etc.\n\n\t\t\t\treturn self.weight * numtrue\n\t\t\t\t#return 0\n\n\t\t\t# AT LEAST ONE unit_prom must be promSite_prom (or else, violate):\n\t\t\telse:\n\t\t\t\tviolated=True\n\t\t\t\tran=False\n\t\t\t\tfor slot in meterPos.slots:\n\t\t\t\t\tslot_prom=slot.feature('prom.'+promType,True)\n\t\t\t\t\tif slot_prom==None:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tran=True\n\t\t\t\t\tif bool(slot_prom)==promSite_prom:\n\t\t\t\t\t\tviolated=False\n\t\t\t\tif ran and violated:\n\t\t\t\t\treturn self.weight\n\t\t\t\telse:\n\t\t\t\t\treturn 0\n\n\t\telif self.name.lower().startswith('initialstrong'):\n\t\t\t#if meterPos.slots[0].i==0:\n\t\t\tif pos_i==0:\n\t\t\t\tif meterPos.meterVal == 's':\n\t\t\t\t\treturn self.weight\n\t\t\treturn 0\n\n\t\telif self.name.lower().startswith('functiontow'):\n\t\t\t#exception for first foot\n\t\t\tif p.config.get('skip_initial_foot',0):\n\t\t\t\tif meterPos.slots[0].i<2:\n\t\t\t\t\treturn 0\n\n\t\t\tif meterPos.meterVal != 's':\t# then this constraint does not apply\n\t\t\t\treturn 0\n\n\t\t\tvio = 0\n\t\t\tfor slot in meterPos.slots:\n\t\t\t\tif slot.word.feature('functionword'):\n\t\t\t\t\tvio += self.weight\n\t\t\treturn vio\n\n\t\telif self.name.lower().startswith('footmin'):\n\t\t\tif len(meterPos.slots) < 2:\n\t\t\t\treturn 0\n\t\t\telif len(meterPos.slots) > 2:\n\t\t\t\treturn self.weight\n\t\t\tname=self.name.lower()\n\t\t\ta = meterPos.slots[0]\n\t\t\tb = meterPos.slots[1]\n\n\t\t\t## should this apply to ALL foomin constraints?\n\t\t\t#if ( bool(a.feature('prom.stress',True)) and bool(b.feature('prom.stress',True))):\n\t\t\t#\treturn self.weight\n\t\t\t##\n\n\t\t\tif name=='footmin-nohx':\n\t\t\t\tif (bool(a.feature('prom.weight',True))):\n\t\t\t\t\treturn self.weight\n\n\t\t\tif name=='footmin-w-resolution':\n\t\t\t\tif a.word != b.word: return 0 # only applies within word-boundaries\n\t\t\t\tfirstsyll_islight=bool(a.feature('prom.weight',True)) == False\n\t\t\t\tfirstsyll_isstressed=bool(a.feature('prom.stress',True)) == True\n\t\t\t\tif not (firstsyll_islight and firstsyll_isstressed):\n\t\t\t\t\treturn self.weight\n\n\t\t\tif name=='footmin-f-resolution':\n\t\t\t\tif a.word == b.word: return 0 # only applies to word-boundaries\n\t\t\t\tif meterPos.meterVal=='s': return self.weight # cannot apply to strong positions\n\t\t\t\ta_is_fw = bool(a.word.feature('functionword'))\n\t\t\t\tb_is_fw = bool(b.word.feature('functionword'))\n\t\t\t\tif not (a_is_fw and b_is_fw): return self.weight\n\n\t\t\tif name=='footmin-s-nohx':\n\t\t\t\tif meterPos.meterVal=='s':\n\t\t\t\t\tif bool(a.feature('prom.weight',True)) or a.word!=b.word:\n\t\t\t\t\t\treturn self.weight\n\n\t\t\tif \"nolh\" in name:\n\t\t\t\tif ( (bool(b.feature('prom.weight',True))) ):\n\t\t\t\t\treturn self.weight\n\n\t\t\tif \"strongconstraint\" in name:\n\t\t\t\tif bool(b.feature('prom.strength',True)):\n\t\t\t\t\treturn self.weight\n\n\t\t\t\tif bool(a.feature('prom.strength',True)):\n\t\t\t\t\tif not bool(a.feature('prom.weight',True)):\n\t\t\t\t\t\t if a.word==b.word and not a.wordpos[0]==a.wordpos[1]:\n\t\t\t\t\t\t \tif not bool(b.feature('prom.stress',True)):\n\t\t\t\t\t\t \t\treturn 0\n\t\t\t\t\treturn self.weight\n\n\t\t\tif name=='footmin-none':\n\t\t\t\treturn self.weight\n\n\t\t\tif name=='footmin-none-unless-in-first-two-positions':\n\t\t\t\tif pos_i!=0 and pos_i!=1:\n\t\t\t\t\treturn self.weight\n\n\t\t\tif name=='footmin-none-unless-in-second-position':\n\t\t\t\tif pos_i!=1:\n\t\t\t\t\treturn self.weight\n\n\t\t\tif name=='footmin-no-s': return self.weight * int(meterPos.meterVal=='s')\n\t\t\tif name=='footmin-no-w': return self.weight * int(meterPos.meterVal=='w')\n\n\t\t\tif name=='footmin-no-s-unless-preceded-by-ww':\n\t\t\t\t# @TODO: bug when number of syllables in maxW is > 2 ?\n\t\t\t\tif meterPos.meterVal!='s': return 0\n\t\t\t\tif pos_i==0: return self.weight\n\t\t\t\tprevpos=all_positions[pos_i-1]\n\t\t\t\t#print pos_i, meterPos, prevpos, pos_i,pos_i-1,all_positions\n\t\t\t\tif len(prevpos.slots)>1 and prevpos.meterVal=='w':\n\t\t\t\t\treturn 0\n\t\t\t\treturn self.weight\n\n\n\n\n\t\t\tif \"wordbound\" in name:\n\t\t\t\tif name=='footmin-wordbound':\n\t\t\t\t\tif a.word!=b.word:\n\t\t\t\t\t\treturn self.weight\n\n\t\t\t\tif \"nomono\" in name:\n\t\t\t\t\tif (a.word.numSyll==1 or b.word.numSyll==1):\n\t\t\t\t\t\treturn self.weight\n\n\t\t\t\tif 'lexmono' in name:\n\t\t\t\t\t#if a.word.numSyll==1 and a.word.stress==\"P\"\n\t\t\t\t\tif a.word.isLexMono() or b.word.isLexMono():\n\t\t\t\t\t\treturn self.weight\n\n\n\n\t\t\t\t## everyone is happy if both are function words\n\t\t\t\tif a.word.feature('functionword') and b.word.feature('functionword'):\n\t\t\t\t\treturn 0\n\n\t\t\t\tif a.word!=b.word:\n\t\t\t\t\tif \"bothnotfw\" in name:\n\t\t\t\t\t\tif not (a.word.feature('functionword') and b.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\t\t\t\t\telif \"neitherfw\":\n\t\t\t\t\t\tif not (a.word.feature('functionword') or b.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\t\t\t\t\telif \"leftfw\":\n\t\t\t\t\t\tif not (a.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\t\t\t\t\telif \"rightfw\":\n\t\t\t\t\t\tif not (b.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\n\n\n\t\t\t\t# only remaining possibilities:\n\t\t\t\t#\ti) slots a,b are from the same word\n\t\t\t\t#   ii) slots a,b are from contiguous words which are the same (haPPY HAppy)\n\n\t\t\t\tif a.wordpos[0]==a.wordpos[1]:\t# in the firs slot's (start,end) wordpos : if (start==end) :  then poss. (ii) above\n\t\t\t\t\tif \"bothnotfw\" in name:\n\t\t\t\t\t\tif not (a.word.feature('functionword') and b.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\t\t\t\t\telif \"neitherfw\":\n\t\t\t\t\t\tif not (a.word.feature('functionword') or b.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\t\t\t\t\telif \"leftfw\":\n\t\t\t\t\t\tif not (a.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\t\t\t\t\telif \"rightfw\":\n\t\t\t\t\t\tif not (b.word.feature('functionword')):\n\t\t\t\t\t\t\treturn self.weight\n\n\t\t\t\t# poss. (i) remains\n\t\t\t\treturn 0\n\n\n\t\t## Constraints about words\n\t\tif self.name=='word-elision':\n\t\t\twords=set([slot.word for slot in meterPos.slots if hasattr(slot.word,'is_elision') and slot.word.is_elision])\n\t\t\tsylls=[]\n\t\t\tfor slot in meterPos.slots: sylls+=slot.children\n\n\t\t\tfor word in words:\n\t\t\t\tlastsyll=word.children[-1]\n\t\t\t\tif lastsyll in sylls: # only break if this position contains the word's final syllable\n\t\t\t\t\treturn self.weight\n\n\n\t\t# is this the end?\n\t\tis_end = slot_i+1==num_slots and meterPos.slots==all_positions[-1].slots\n\n\t\t## CONSTRAINTS ON PREVIOUS POSITIONS\n\t\t\"\"\"\n\t\tABANDONED TEMPORARILY AS NOT POSSIBLE GIVEN THAT PARSES ARE BOUNDED AS PARSING GOES ON\n\t\t\"\"\"\n\n\t\tif self.name=='attridge-ss-not-by-ww':\n\t\t\t#if meterPos.meterVal!='s': return 0\n\t\t\t#if not is_end and meterPos.meterVal2 == 'ss':\n\t\t\t#\tparse.pauseComparisons=True\n\n\t\t\tif pos_i==0: return 0\n\t\t\tprevpos=all_positions[pos_i-1]\n\t\t\tprevprevpos=all_positions[pos_i-2] if (pos_i-2)>=0 else None\n\t\t\t#print prevprevpos,prevpos,meterPos\n\t\t\t#print prevprevpos.meterVal2 if prevprevpos else None,prevpos.meterVal2, meterPos.meterVal2\n\n\t\t\t#print prevprevpos,prevpos,meterPos\n\t\t\t#print prevprevpos.meterVal2 if prevprevpos else None,prevpos.meterVal2, meterPos.meterVal2\n\t\t\t#print dir(prevprevpos) if prevprevpos else None\n\t\t\t#print dir(prevpos) if prevprevpos else None\n\t\t\t#print dir(meterPos)\n\t\t\t#print\n\n\t\t\tif prevpos.meterVal2 == 'ss':\n\n\t\t\t\t#if (prevprevpos and prevprevpos.meterVal2=='ww')\n\n\t\t\t\tif (prevprevpos and prevprevpos.meterVal2=='ww') and (not hasattr(prevprevpos,'_flag_already_served_as_ww')):\n\t\t\t\t\tprevprevpos._flag_already_served_as_ww=True\n\t\t\t\t\tpass\n\t\t\t\telif meterPos.meterVal2=='ww' and (not hasattr(meterPos,'_flag_already_served_as_ww')):\n\t\t\t\t\tmeterPos._flag_already_served_as_ww=True\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\t#print 'ERROR!'\n\t\t\t\t\tfor cnstr in prevpos.constraintScores:\n\t\t\t\t\t\tif cnstr.name==self.name:\n\t\t\t\t\t\t\tprevpos.constraintScores[cnstr]=self.weight\n\t\t\t\t\t\t\tparse.constraintScores[cnstr]+=self.weight\n\n\t\t\t\t#parse.pauseComparisons=False\n\t\t\telif is_end and meterPos.meterVal2=='ss':\n\t\t\t\t#parse.pauseComparisons=False\n\t\t\t\tif prevpos.meterVal2=='ww':\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\t#print 'ERROR!'\n\t\t\t\t\treturn self.weight\n\t\t\t#print\n\t\t#\"\"\"\n\n\n\n\t\t## POST HOC CONSTRAINTS\n\t\tif is_end:\n\t\t\tfinal_meter_str=''.join([''.join(pos.meterVal for slot in pos.slots) for pos in all_positions])\n\t\t\t#print final_meter_str\n\n\t\t\t# headedness\n\t\t\tif self.name.startswith('headedness'):\n\t\t\t\tshouldbe = self.name.split('!=')[-1]\n\n\t\t\t\t\"\"\"\n\t\t\t\tApproach 1: This approach doesn't really work on individual lines:\n\n\t\t\t\t# binary or ternary?\n\t\t\t\tweak_pos = [pos for pos in all_positions if pos.meterVal=='w']\n\t\t\t\tif len(weak_pos)<2: return 0\n\t\t\t\tweak_pos_types = [''.join('w' for slot in pos.slots) for pos in weak_pos]\n\n\t\t\t\tif weak_pos_types.count('ww')>weak_pos_types.count('w'): # ternary\n\t\t\t\t\tif final_meter_str[3]=='w': # anapestic\n\t\t\t\t\t\theadedness = 'rising'\n\t\t\t\t\telse: # dactylic\n\t\t\t\t\t\theadedness = 'falling'\n\t\t\t\telse: # binary\n\t\t\t\t\tif final_meter_str[3]=='w':\n\t\t\t\t\t\theadedness = 'falling' # trochaic\n\t\t\t\t\telse:\n\t\t\t\t\t\theadedness = 'rising'\n\n\t\t\t\tif shouldbe != headedness:\n\t\t\t\t\treturn self.weight\n\t\t\t\t\"\"\"\n\n\t\t\t\t\"\"\"\n\t\t\t\tApproach 2: count 'ws' vs 'sw' pairs and give categorical violation\n\t\t\t\t\"\"\"\n\t\t\t\tquasi_feet=[''.join(x) for x in tools.slice([pos.meterVal for pos in all_positions],slice_length=2,runts=False)]\n\t\t\t\theadedness = 'rising' if quasi_feet.count('ws')>=quasi_feet.count('sw') else 'falling'\n\t\t\t\t#print final_meter_str\n\t\t\t\t#print quasi_feet\n\t\t\t\t#print headedness\n\t\t\t\t#print\n\n\t\t\t\tif shouldbe != headedness:\n\t\t\t\t\treturn self.weight\n\t\t\t\t#\"\"\"\n\n\t\t\t\t\"\"\"\n\t\t\t\tApproach 3: count 'ws' vs 'sw' pairs and give violation/num-pos per off foot\n\n\t\t\t\tquasi_feet=[''.join(x) for x in tools.slice([pos.meterVal for pos in all_positions],slice_length=2,runts=True)]\n\t\t\t\tif shouldbe == 'rising':\n\t\t\t\t\tnum_not_rising = float(len([ft for ft in quasi_feet if ft!='ws']))\n\t\t\t\t\treturn num_not_rising / float(len(all_positions)) * float(self.weight)\n\t\t\t\telif shouldbe == 'falling':\n\t\t\t\t\tnum_not_falling = float(len([ft for ft in quasi_feet if ft!='sw']))\n\t\t\t\t\treturn num_not_falling / float(len(all_positions)) * float(self.weight)\n\t\t\t\t\"\"\"\n\n\n\t\t\t# number of feet\n\t\t\tif self.name.startswith('number_feet'):\n\t\t\t\tshouldbe = int(self.name.split('!=')[-1])\n\t\t\t\tstrong_pos = [pos for pos in all_positions if pos.meterVal=='s']\n\t\t\t\tnum_feet = len(strong_pos) # debatable\n\t\t\t\tif shouldbe != num_feet:\n\t\t\t\t\treturn self.weight\n\n\t\t\t# other posthoc constraints\n\t\t\tif self.name.startswith('posthoc'):\n\t\t\t\tif self.name=='posthoc-no-final-ww':\n\t\t\t\t\tif len(all_positions[-1].slots)>1 and all_positions[-1].meterVal=='w':\n\t\t\t\t\t\treturn self.weight\n\n\t\t\t\tif self.name=='posthoc-no-final-w':\n\t\t\t\t\tif all_positions[-1].meterVal=='w':\n\t\t\t\t\t\treturn self.weight\n\n\t\t\t\tif self.name=='posthoc-standardize-weakpos':\n\t\t\t\t\tweak_pos = [pos for pos in all_positions if pos.meterVal=='w']\n\t\t\t\t\tif len(weak_pos)<2: return 0\n\t\t\t\t\tweak_pos_types = [''.join('w' for slot in pos.slots) for pos in weak_pos]\n\t\t\t\t\tmaxcount = max([weak_pos_types.count(wtype) for wtype in set(weak_pos_types)])\n\t\t\t\t\tdiff = len(weak_pos) - maxcount\n\t\t\t\t\treturn self.weight*diff\n\n\t\t# made it through this minefield, eh?\n\t\treturn 0", "response": "hardparse the meter position and parse"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the paths for the Stanford Parser and EJML files.", "response": "def set_paths(dir_root='.'):\n    global DIR_ROOT,sylcmu\n    DIR_ROOT = dir_root\n    os.environ['STANFORD_PARSER'] = os.path.join(dir_root,'StanfordLibrary/stanford-parser-full-%s/stanford-parser.jar' % DATE)\n    os.environ['STANFORD_MODELS'] = os.path.join(dir_root,'StanfordLibrary/stanford-parser-full-%s/stanford-parser-%s-models.jar' % (DATE, MODELS_VERSION))\n    os.environ['STANFORD_EJML']   = os.path.join(dir_root,'StanfordLibrary/stanford-parser-full-%s/ejml-%s.jar' % (DATE, EJML_VERSION))\n\n    \"\"\"\n    then=time.time()\n    print '>> loading CMU for MetricalParser...'\n    sylcmu = pkl.load(open(os.path.join(dir_root,'Pickle Jar/sylcmu.pkl')))\n    now=time.time()\n    print '>> done loading in',round(now-then,2),'seconds'\n    \"\"\"\n    sylcmu = {}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert(cls, tree):\n\n        if isinstance(tree, Tree):\n            children = [cls.convert(child) for child in tree]\n            if isinstance(tree, MetricalTree):\n                return cls(tree._cat, children, tree._dep, tree._lstress)\n            elif isinstance(tree, DependencyTree):\n                return cls(tree._cat, children, tree._dep)\n            else:\n                return cls(tree._label, children)\n        else:\n            return tree", "response": "Convert a tree between different subtypes of Tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_meter(id=None, name=None, maxS=2, maxW=2, splitheavies=0, constraints=DEFAULT_CONSTRAINTS,return_dict=False):\n\n\t\"\"\"\n\t{'constraints': ['footmin-w-resolution/1',\n\t'footmin-f-resolution/1',\n\t'strength.w=>-p/1',\n\t'headedness!=rising/1',\n\t'number_feet!=5/1'],\n\t'id': 'iambic_pentameter',\n\t'maxS': 2,\n\t'maxW': 2,\n\t'name': 'Iambic Pentameter',\n\t'splitheavies': 0}\"\"\"\n\tif 'Meter.Meter' in str(id.__class__): return id\n\n\tif not id: id='Meter_%s' % now()\n\tif not name: name = id + '['+' '.join(constraints)+']'\n\n\tconfig = locals()\n\n\timport prosodic\n\tif id in prosodic.config['meters']:\n\t\treturn prosodic.config['meters'][id]\n\n\n\tif return_dict: return config\n\treturn Meter(config)", "response": "get a meter by id name maxS maxW splitheavies"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self,wordlist,numSyll=0,numTopBounded=10):\n\t\tnumTopBounded = self.prosodic_config.get('num_bounded_parses_to_store',numTopBounded)\n\t\tmaxsec = self.prosodic_config.get('parse_maxsec',None)\n\t\t#print '>> NTB!',numTopBounded\n\t\tfrom Parse import Parse\n\t\tif not numSyll:\n\t\t\treturn []\n\n\n\t\tslotMatrix = self.genSlotMatrix(wordlist)\n\t\tif not slotMatrix: return None\n\n\t\tconstraints = self.constraints\n\n\n\t\tallParses = []\n\t\tallBoundedParses=[]\n\n\t\timport time\n\t\tclockstart=time.time()\n\t\tfor slots_i,slots in enumerate(slotMatrix):\n\t\t\t#for slot in slots:\n\t\t\t\t#print slot\n\t\t\t\t#print slot.feats\n\t\t\t\t#print\n\n\t\t\t## give up?\n\t\t\tif maxsec and time.time()-clockstart > maxsec:\n\t\t\t\tprint '!! Time limit ({0}s) elapsed in trying to parse line:'.format(maxsec), ' '.join(wtok.token for wtok in wordlist)\n\t\t\t\treturn [],[]\n\n\t\t\t_parses,_boundedParses = self.parseLine(slots)\n\n\t\t\t\"\"\"\n\t\t\tfor prs in _parses:\n\t\t\t\tprint 'UNBOUNDED:'\n\t\t\t\tprint prs.__report__()\n\t\t\t\tprint\n\n\t\t\tfor prs in _parses:\n\t\t\t\tprint 'BOUNDED:'\n\t\t\t\tprint prs.__report__()\n\t\t\t\tprint\n\n\t\t\tprint\n\t\t\tprint\n\t\t\t\"\"\"\n\n\t\t\tallParses.append(_parses)\n\t\t\tallBoundedParses+=_boundedParses\n\n\t\tparses,_boundedParses = self.boundParses(allParses)\n\n\t\tparses.sort()\n\n\t\tallBoundedParses+=_boundedParses\n\n\t\tallBoundedParses.sort(key=lambda _p: (-_p.numSlots, _p.score()))\n\t\tallBoundedParses=allBoundedParses[:numTopBounded]\n\t\t#allBoundedParses=[]\n\n\t\t\"\"\"print parses\n\t\tprint\n\t\tprint allBoundedParses\n\t\tfor parse in allBoundedParses:\n\t\t\tprint parse.__report__()\n\t\t\tprint\n\t\t\tprint parse.boundedBy if type(parse.boundedBy) in [str,unicode] else parse.boundedBy.__report__()\n\t\t\tprint\n\t\t\tprint\n\t\t\tprint\n\t\t\"\"\"\n\n\t\treturn parses,allBoundedParses", "response": "parse a list of words into a list of parse objects"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raw(self, raw):\n        if raw is None:\n            raise ValueError(\"Invalid value for `raw`, must not be `None`\")  # noqa: E501\n        if raw is not None and not re.search(r'^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$', raw):  # noqa: E501\n            raise ValueError(r\"Invalid value for `raw`, must be a follow pattern or equal to `/^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$/`\")  # noqa: E501\n\n        self._raw = raw", "response": "Sets the raw of this RuntimeRawExtension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a namespaced CronJob.", "response": "def delete_namespaced_cron_job(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_cron_job  # noqa: E501\n\n        delete a CronJob  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_cron_job(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CronJob (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_cron_job_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_cron_job_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_namespaced_cron_job(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_cron_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_cron_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a CronJob with the given namespace with the given body."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npatch the status of a CronJob in the given namespace.", "response": "def patch_namespaced_cron_job_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_cron_job_status  # noqa: E501\n\n        partially update status of the specified CronJob  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_cron_job_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CronJob (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V2alpha1CronJob\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_cron_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_cron_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreplacing a namespace with a new one.", "response": "def replace_namespaced_cron_job(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_cron_job  # noqa: E501\n\n        replace the specified CronJob  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_cron_job(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CronJob (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V2alpha1CronJob body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V2alpha1CronJob\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_cron_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_cron_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace the status of a CronJob with a new status.", "response": "def replace_namespaced_cron_job_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_cron_job_status  # noqa: E501\n\n        replace status of the specified CronJob  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_cron_job_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CronJob (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V2alpha1CronJob body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V2alpha1CronJob\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_cron_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_cron_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a PodPreset in the specified namespace.", "response": "def delete_namespaced_pod_preset(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_pod_preset  # noqa: E501\n\n        delete a PodPreset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_pod_preset(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodPreset (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_pod_preset_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_pod_preset_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_namespaced_pod_preset(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_pod_preset_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_pod_preset_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "List all pods in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_namespaced_pod_preset(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_pod_preset_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_pod_preset_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a PodPreset in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_namespaced_pod_preset(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_pod_preset_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_pod_preset_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read a PodPreset by name and namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef replace_namespaced_pod_preset(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_pod_preset_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_pod_preset_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a PodPreset with a new one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the K8s response `data` in JSON format.", "response": "def unmarshal_event(self, data: str, response_type):\n        \"\"\"Return the K8s response `data` in JSON format.\n\n        \"\"\"\n        js = json.loads(data)\n\n        # Make a copy of the original object and save it under the\n        # `raw_object` key because we will replace the data under `object` with\n        # a Python native type shortly.\n        js['raw_object'] = js['object']\n\n        # Something went wrong. A typical example would be that the user\n        # supplied a resource version that was too old. In that case K8s would\n        # not send a conventional ADDED/DELETED/... event but an error. Turn\n        # this error into a Python exception to save the user the hassle.\n        if js['type'].lower() == 'error':\n            return js\n\n        # If possible, compile the JSON response into a Python native response\n        # type, eg `V1Namespace` or `V1Pod`,`ExtensionsV1beta1Deployment`, ...\n        if response_type is not None:\n            js['object'] = self._api_client.deserialize(\n                response=SimpleNamespace(data=json.dumps(js['raw_object'])),\n                response_type=response_type\n            )\n\n            # decode and save resource_version to continue watching\n            if hasattr(js['object'], 'metadata'):\n                self.resource_version = js['object'].metadata.resource_version\n\n            # For custom objects that we don't have model defined, json\n            # deserialization results in dictionary\n            elif (isinstance(js['object'], dict) and\n                  'metadata' in js['object'] and\n                  'resourceVersion' in js['object']['metadata']):\n\n                self.resource_version = js['object']['metadata']['resourceVersion']\n\n        return js"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stream(self, func, *args, **kwargs):\n        self.close()\n        self._stop = False\n        self.return_type = self.get_return_type(func)\n        kwargs['watch'] = True\n        kwargs['_preload_content'] = False\n\n        self.func = partial(func, *args, **kwargs)\n\n        return self", "response": "Stream an API resource and stream the result back via a generator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_collection_custom_resource_definition(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_custom_resource_definition_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_custom_resource_definition_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "Delete collection of CustomResourceDefinition"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef patch_custom_resource_definition(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_custom_resource_definition_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_custom_resource_definition_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a custom resource definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_custom_resource_definition_status(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_custom_resource_definition_status_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_custom_resource_definition_status_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the status of a specific resource definition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef replace_custom_resource_definition(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_custom_resource_definition_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_custom_resource_definition_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a custom resource definition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace_custom_resource_definition_status(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_custom_resource_definition_status_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_custom_resource_definition_status_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace status of a specific CustomResourceDefinition"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a given response header.", "response": "def getheader(self, name, default=None):\n        \"\"\"Returns a given response header.\"\"\"\n        return self.aiohttp_response.headers.get(name, default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def request(self, method, url, query_params=None, headers=None,\n                      body=None, post_params=None, _preload_content=True,\n                      _request_timeout=None):\n        \"\"\"Execute request\n\n        :param method: http request method\n        :param url: http request url\n        :param query_params: query parameters in the url\n        :param headers: http request headers\n        :param body: request json body, for `application/json`\n        :param post_params: request post parameters,\n                            `application/x-www-form-urlencoded`\n                            and `multipart/form-data`\n        :param _preload_content: this is a non-applicable field for\n                                 the AiohttpClient.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        \"\"\"\n        method = method.upper()\n        assert method in ['GET', 'HEAD', 'DELETE', 'POST', 'PUT',\n                          'PATCH', 'OPTIONS']\n\n        if post_params and body:\n            raise ValueError(\n                \"body parameter cannot be used with post_params parameter.\"\n            )\n\n        post_params = post_params or {}\n        headers = headers or {}\n        timeout = _request_timeout or 5 * 60\n\n        if 'Content-Type' not in headers:\n            headers['Content-Type'] = 'application/json'\n\n        args = {\n            \"method\": method,\n            \"url\": url,\n            \"timeout\": timeout,\n            \"headers\": headers\n        }\n\n        if query_params:\n            args[\"url\"] += '?' + urlencode(query_params)\n\n        # For `POST`, `PUT`, `PATCH`, `OPTIONS`, `DELETE`\n        if method in ['POST', 'PUT', 'PATCH', 'OPTIONS', 'DELETE']:\n            if re.search('json', headers['Content-Type'], re.IGNORECASE):\n                if headers['Content-Type'] == 'application/json-patch+json':\n                    if not isinstance(body, list):\n                        headers['Content-Type'] = 'application/strategic-merge-patch+json'\n                if body is not None:\n                    body = json.dumps(body)\n                args[\"data\"] = body\n            elif headers['Content-Type'] == 'application/x-www-form-urlencoded':  # noqa: E501\n                args[\"data\"] = aiohttp.FormData(post_params)\n            elif headers['Content-Type'] == 'multipart/form-data':\n                # must del headers['Content-Type'], or the correct\n                # Content-Type which generated by aiohttp\n                del headers['Content-Type']\n                data = aiohttp.FormData()\n                for param in post_params:\n                    k, v = param\n                    if isinstance(v, tuple) and len(v) == 3:\n                        data.add_field(k,\n                                       value=v[1],\n                                       filename=v[0],\n                                       content_type=v[2])\n                    else:\n                        data.add_field(k, v)\n                args[\"data\"] = data\n\n            # Pass a `bytes` parameter directly in the body to support\n            # other content types than Json when `body` argument is provided\n            # in serialized form\n            elif isinstance(body, bytes):\n                args[\"data\"] = body\n            else:\n                # Cannot generate the request from given parameters\n                msg = \"\"\"Cannot prepare a request message for provided\n                         arguments. Please check that your arguments match\n                         declared content type.\"\"\"\n                raise ApiException(status=0, reason=msg)\n\n        r = await self.pool_manager.request(**args)\n        if _preload_content:\n\n            data = await r.text()\n            r = RESTResponse(r, data)\n\n            # log response body\n            logger.debug(\"response body: %s\", r.data)\n\n            if not 200 <= r.status <= 299:\n                raise ApiException(http_resp=r)\n\n        return r", "response": "Execute a request to the Aiohttp server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that user is authenticated otherwise return 401 Unauthorized.", "response": "def authentication_required(req, resp, resource, uri_kwargs):\n    \"\"\"Ensure that user is authenticated otherwise return ``401 Unauthorized``.\n\n    If request fails to authenticate this authorization hook will also\n    include list of ``WWW-Athenticate`` challenges.\n\n    Args:\n        req (falcon.Request): the request object.\n        resp (falcon.Response): the response object.\n        resource (object): the resource object.\n        uri_kwargs (dict): keyword arguments from the URI template.\n\n    .. versionadded:: 0.4.0\n    \"\"\"\n    if 'user' not in req.context:\n        args = [\"Unauthorized\", \"This resource requires authentication\"]\n\n        # compat: falcon >= 1.0.0 requires the list of challenges\n        if FALCON_VERSION >= (1, 0, 0):\n            args.append(req.context.get('challenges', []))\n\n        raise HTTPUnauthorized(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_namespaced_deployment(self, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_deployment_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_deployment_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a new Deployment with the specified namespace and body."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a rollback of a Deployment", "response": "def create_namespaced_deployment_rollback(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_deployment_rollback  # noqa: E501\n\n        create rollback of a Deployment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_deployment_rollback(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the DeploymentRollback (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param ExtensionsV1beta1DeploymentRollback body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param bool include_uninitialized: If IncludeUninitialized is specified, the object may be returned without completing initialization.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_deployment_rollback_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_deployment_rollback_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an ingress with the specified namespace and body.", "response": "def create_namespaced_ingress(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_ingress  # noqa: E501\n\n        create an Ingress  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_ingress(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1beta1Ingress body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1Ingress\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_ingress_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_ingress_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_namespaced_replica_set(self, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_replica_set_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_replica_set_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a new ReplicaSet with the specified namespace and body."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_collection_namespaced_ingress(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_ingress_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_ingress_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete all items in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_collection_namespaced_network_policy(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_network_policy_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_network_policy_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a collection of NetworkPolicy objects in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes all resources in the specified namespace", "response": "def delete_collection_namespaced_replica_set(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_collection_namespaced_replica_set  # noqa: E501\n\n        delete collection of ReplicaSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_collection_namespaced_replica_set(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_replica_set_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_replica_set_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a daemon set with the specified name and namespace.", "response": "def delete_namespaced_daemon_set(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_daemon_set  # noqa: E501\n\n        delete a DaemonSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_daemon_set(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the DaemonSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_daemon_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_daemon_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a namespaced Deployment", "response": "def delete_namespaced_deployment(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_deployment  # noqa: E501\n\n        delete a Deployment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_deployment(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Deployment (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_deployment_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_deployment_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a Namespaced ReplicaSet", "response": "def delete_namespaced_replica_set(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_replica_set  # noqa: E501\n\n        delete a ReplicaSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_replica_set(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicaSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_replica_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_replica_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_ingress_for_all_namespaces(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_ingress_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_ingress_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "List all namespaces in the current namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the daemon sets in the specified namespace.", "response": "def list_namespaced_daemon_set(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"list_namespaced_daemon_set  # noqa: E501\n\n        list or watch objects of kind DaemonSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_namespaced_daemon_set(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1beta1DaemonSetList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_daemon_set_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_daemon_set_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_namespaced_network_policy(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_network_policy_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_network_policy_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "List all objects in a namespace that are in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist replica sets for all namespaces", "response": "def list_replica_set_for_all_namespaces(self, **kwargs):  # noqa: E501\n        \"\"\"list_replica_set_for_all_namespaces  # noqa: E501\n\n        list or watch objects of kind ReplicaSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_replica_set_for_all_namespaces(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1beta1ReplicaSetList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_replica_set_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_replica_set_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch_namespaced_daemon_set(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_daemon_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_daemon_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a daemon set with the given name and namespace with the given body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npatching the status of a daemon set.", "response": "def patch_namespaced_daemon_set_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_daemon_set_status  # noqa: E501\n\n        partially update status of the specified DaemonSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_daemon_set_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the DaemonSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1DaemonSet\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_daemon_set_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_daemon_set_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npatch an existing ExtensionsV1beta1Scale.", "response": "def patch_namespaced_deployment_scale(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_deployment_scale  # noqa: E501\n\n        partially update scale of the specified Deployment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_deployment_scale(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Scale (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: ExtensionsV1beta1Scale\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_deployment_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_deployment_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_namespaced_deployment_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_deployment_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_deployment_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the status of a specific Kubernetes Deployment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch_namespaced_replica_set(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_replica_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_replica_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the properties of a ReplicaSet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npatches a ReplicationControllerDummy with the given body.", "response": "def patch_namespaced_replication_controller_dummy_scale(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_replication_controller_dummy_scale  # noqa: E501\n\n        partially update scale of the specified ReplicationControllerDummy  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_replication_controller_dummy_scale(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Scale (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: ExtensionsV1beta1Scale\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_replication_controller_dummy_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_replication_controller_dummy_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npatch an existing PodSecurityPolicy.", "response": "def patch_pod_security_policy(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"patch_pod_security_policy  # noqa: E501\n\n        partially update the specified PodSecurityPolicy  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_pod_security_policy(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodSecurityPolicy (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: ExtensionsV1beta1PodSecurityPolicy\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_pod_security_policy_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_pod_security_policy_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the status of the specified Ingress in a namespace.", "response": "def read_namespaced_ingress_status(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_ingress_status  # noqa: E501\n\n        read status of the specified Ingress  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_ingress_status(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Ingress (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1Ingress\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_ingress_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_ingress_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the specified ReplicaSet", "response": "def read_namespaced_replica_set(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_replica_set  # noqa: E501\n\n        read the specified ReplicaSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_replica_set(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicaSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1beta1ReplicaSet\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_replica_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_replica_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the status of a ReplicaSet in a namespace.", "response": "def read_namespaced_replica_set_status(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_replica_set_status  # noqa: E501\n\n        read status of the specified ReplicaSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_replica_set_status(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicaSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1ReplicaSet\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_replica_set_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_replica_set_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_namespaced_replication_controller_dummy_scale(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_replication_controller_dummy_scale_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_replication_controller_dummy_scale_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read a ReplicationControllerDummy with optional namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace the status of a daemon set with a new status.", "response": "def replace_namespaced_daemon_set_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_daemon_set_status  # noqa: E501\n\n        replace status of the specified DaemonSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_daemon_set_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the DaemonSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1beta1DaemonSet body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1DaemonSet\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_daemon_set_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_daemon_set_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_pod_security_policy(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_pod_security_policy_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_pod_security_policy_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a PodSecurityPolicy in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def refresh_token(self, refresh_token):\n        async with self._client_session() as client:\n            well_known = await self._get_well_known(client)\n\n            try:\n                return await self._post(\n                    client,\n                    well_known['token_endpoint'],\n                    data={\n                        'grant_type': GRANT_TYPE_REFRESH_TOKEN,\n                        'refresh_token': refresh_token,\n                    }\n                )\n            except aiohttp.ClientResponseError as e:\n                raise ConfigException('oidc: failed to refresh access token')", "response": "Refresh an openid access token"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef describe(self, **kwargs):\n        description = {\n            'label': self.label,\n            'details': inspect.cleandoc(self.details),\n            'type': \"list of {}\".format(self.type) if self.many else self.type,\n            'spec': self.spec,\n            'read_only': self.read_only,\n            'write_only': self.write_only,\n            'allow_null': self.allow_null,\n        }\n        description.update(kwargs)\n        return description", "response": "Describes this field instance for purpose of self - documentation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts representation value to bool.", "response": "def from_representation(self, data):\n        \"\"\"Convert representation value to ``bool`` if it has expected form.\"\"\"\n        if data in self._TRUE_VALUES:\n            return True\n        elif data in self._FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\n                \"{type} type value must be one of {values}\".format(\n                    type=self.type,\n                    values=self._TRUE_VALUES.union(self._FALSE_VALUES)\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete an Event with the specified name and namespace.", "response": "def delete_namespaced_event(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_event  # noqa: E501\n\n        delete an Event  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_event(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Event (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_event_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_event_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_event_for_all_namespaces(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_event_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_event_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "List all namespaces in the current namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads an Event in a namespace.", "response": "def read_namespaced_event(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_event  # noqa: E501\n\n        read the specified Event  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_event(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Event (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1beta1Event\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_event_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_event_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreplacing an existing Event with a new body.", "response": "def replace_namespaced_event(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_event  # noqa: E501\n\n        replace the specified Event  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_event(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Event (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1beta1Event body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1Event\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_event_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_event_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace a priority class in a resource.", "response": "def replace_priority_class(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"replace_priority_class  # noqa: E501\n\n        replace the specified PriorityClass  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_priority_class(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PriorityClass (required)\n        :param V1beta1PriorityClass body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1PriorityClass\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_priority_class_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_priority_class_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a HorizontalPodAutoscaler with the specified namespace and body.", "response": "def create_namespaced_horizontal_pod_autoscaler(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_horizontal_pod_autoscaler  # noqa: E501\n\n        create a HorizontalPodAutoscaler  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_horizontal_pod_autoscaler(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V2beta1HorizontalPodAutoscaler body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V2beta1HorizontalPodAutoscaler\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_horizontal_pod_autoscaler_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_horizontal_pod_autoscaler_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a collection of HorizontalPodAutoscaler objects in the specified namespace.", "response": "def delete_collection_namespaced_horizontal_pod_autoscaler(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_collection_namespaced_horizontal_pod_autoscaler  # noqa: E501\n\n        delete collection of HorizontalPodAutoscaler  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_collection_namespaced_horizontal_pod_autoscaler(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_horizontal_pod_autoscaler_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_horizontal_pod_autoscaler_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_horizontal_pod_autoscaler_for_all_namespaces(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_horizontal_pod_autoscaler_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_horizontal_pod_autoscaler_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "List all namespaces in the horizontal pod."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_namespaced_horizontal_pod_autoscaler(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_horizontal_pod_autoscaler_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_horizontal_pod_autoscaler_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "List all HorizontalPodAutoscaler objects in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_namespaced_horizontal_pod_autoscaler(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_horizontal_pod_autoscaler_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_horizontal_pod_autoscaler_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read the specified HorizontalPodAutoscaler with the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the status of a specific HorizontalPodAutoscaler in a namespace", "response": "def read_namespaced_horizontal_pod_autoscaler_status(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_horizontal_pod_autoscaler_status  # noqa: E501\n\n        read status of the specified HorizontalPodAutoscaler  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_horizontal_pod_autoscaler_status(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the HorizontalPodAutoscaler (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V2beta1HorizontalPodAutoscaler\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_horizontal_pod_autoscaler_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_horizontal_pod_autoscaler_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a cluster scoped Custom object", "response": "def create_cluster_custom_object(self, group, version, plural, body, **kwargs):  # noqa: E501\n        \"\"\"create_cluster_custom_object  # noqa: E501\n\n        Creates a cluster scoped Custom object  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_cluster_custom_object(group, version, plural, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: The custom resource's group name (required)\n        :param str version: The custom resource's version (required)\n        :param str plural: The custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param UNKNOWN_BASE_TYPE body: The JSON schema of the Resource to create. (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_cluster_custom_object_with_http_info(group, version, plural, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_cluster_custom_object_with_http_info(group, version, plural, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_namespaced_custom_object(self, group, version, namespace, plural, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_custom_object_with_http_info(group, version, namespace, plural, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_custom_object_with_http_info(group, version, namespace, plural, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a namespace scoped Custom object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_cluster_custom_object(self, group, version, plural, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_cluster_custom_object_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_cluster_custom_object_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n            return data", "response": "Delete a cluster scoped custom object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cluster_custom_object(self, group, version, plural, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_cluster_custom_object_with_http_info(group, version, plural, name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_cluster_custom_object_with_http_info(group, version, plural, name, **kwargs)  # noqa: E501\n            return data", "response": "Get a specific cluster scoped custom object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cluster_custom_object_scale(self, group, version, plural, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_cluster_custom_object_scale_with_http_info(group, version, plural, name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_cluster_custom_object_scale_with_http_info(group, version, plural, name, **kwargs)  # noqa: E501\n            return data", "response": "Get the cluster custom object scale"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cluster_custom_object_status(self, group, version, plural, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_cluster_custom_object_status_with_http_info(group, version, plural, name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_cluster_custom_object_status_with_http_info(group, version, plural, name, **kwargs)  # noqa: E501\n            return data", "response": "Get the status of a specific cluster scoped custom object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a custom object scale", "response": "def get_namespaced_custom_object_scale(self, group, version, namespace, plural, name, **kwargs):  # noqa: E501\n        \"\"\"get_namespaced_custom_object_scale  # noqa: E501\n\n        read scale of the specified namespace scoped custom object  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_namespaced_custom_object_scale(group, version, namespace, plural, name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: the custom resource's group (required)\n        :param str version: the custom resource's version (required)\n        :param str namespace: The custom resource's namespace (required)\n        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param str name: the custom object's name (required)\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the status of a specific namespace scoped custom object", "response": "def get_namespaced_custom_object_status(self, group, version, namespace, plural, name, **kwargs):  # noqa: E501\n        \"\"\"get_namespaced_custom_object_status  # noqa: E501\n\n        read status of the specified namespace scoped custom object  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_namespaced_custom_object_status(group, version, namespace, plural, name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: the custom resource's group (required)\n        :param str version: the custom resource's version (required)\n        :param str namespace: The custom resource's namespace (required)\n        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param str name: the custom object's name (required)\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_namespaced_custom_object_status_with_http_info(group, version, namespace, plural, name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_namespaced_custom_object_status_with_http_info(group, version, namespace, plural, name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_cluster_custom_object(self, group, version, plural, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_cluster_custom_object_with_http_info(group, version, plural, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_cluster_custom_object_with_http_info(group, version, plural, **kwargs)  # noqa: E501\n            return data", "response": "List cluster scoped custom objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef patch_cluster_custom_object(self, group, version, plural, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_cluster_custom_object_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_cluster_custom_object_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a cluster scoped custom object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npatching a cluster scoped custom object scale", "response": "def patch_cluster_custom_object_scale(self, group, version, plural, name, body, **kwargs):  # noqa: E501\n        \"\"\"patch_cluster_custom_object_scale  # noqa: E501\n\n        partially update scale of the specified cluster scoped custom object  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_cluster_custom_object_scale(group, version, plural, name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: the custom resource's group (required)\n        :param str version: the custom resource's version (required)\n        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param str name: the custom object's name (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_cluster_custom_object_scale_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_cluster_custom_object_scale_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npatches the status of a cluster scoped custom object.", "response": "def patch_cluster_custom_object_status(self, group, version, plural, name, body, **kwargs):  # noqa: E501\n        \"\"\"patch_cluster_custom_object_status  # noqa: E501\n\n        partially update status of the specified cluster scoped custom object  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_cluster_custom_object_status(group, version, plural, name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: the custom resource's group (required)\n        :param str version: the custom resource's version (required)\n        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param str name: the custom object's name (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_cluster_custom_object_status_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_cluster_custom_object_status_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch_namespaced_custom_object_scale(self, group, version, namespace, plural, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a namespace scoped custom object scale"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npatching the status of a specific namespace scoped custom object.", "response": "def patch_namespaced_custom_object_status(self, group, version, namespace, plural, name, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_custom_object_status  # noqa: E501\n\n        partially update status of the specified namespace scoped custom object  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_custom_object_status(group, version, namespace, plural, name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: the custom resource's group (required)\n        :param str version: the custom resource's version (required)\n        :param str namespace: The custom resource's namespace (required)\n        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param str name: the custom object's name (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_custom_object_status_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_custom_object_status_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef replace_cluster_custom_object(self, group, version, plural, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_cluster_custom_object_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_cluster_custom_object_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a cluster scoped custom object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_cluster_custom_object_status(self, group, version, plural, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_cluster_custom_object_status_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_cluster_custom_object_status_with_http_info(group, version, plural, name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the status of a cluster scoped specified custom object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_namespaced_custom_object(self, group, version, namespace, plural, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_custom_object_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_custom_object_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a namespace scoped custom object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_namespaced_custom_object_scale(self, group, version, namespace, plural, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a namespace scoped custom object scale."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_fields(mcs, bases, namespace):\n        fields = [\n            (name, namespace.pop(name))\n            for name, attribute\n            in list(namespace.items())\n            if isinstance(attribute, BaseField)\n        ]\n\n        for base in reversed(bases):\n            if hasattr(base, mcs._fields_storage_key):\n                fields = list(\n                    getattr(base, mcs._fields_storage_key).items()\n                ) + fields\n\n        return OrderedDict(fields)", "response": "Create fields dictionary to be used in resource class namespace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_representation(self, obj):\n        representation = {}\n\n        for name, field in self.fields.items():\n            if field.write_only:\n                continue\n\n            # note fields do not know their names in source representation\n            # but may know what attribute they target from source object\n            attribute = self.get_attribute(obj, field.source or name)\n\n            if attribute is None:\n                # Skip none attributes so fields do not have to deal with them\n                representation[name] = [] if field.many else None\n            elif field.many:\n                representation[name] = [\n                    field.to_representation(item) for item in attribute\n                ]\n            else:\n                representation[name] = field.to_representation(attribute)\n\n        return representation", "response": "Convert given internal object instance into representation dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_representation(self, representation):\n        object_dict = {}\n        failed = {}\n\n        for name, field in self.fields.items():\n            if name not in representation:\n                continue\n\n            try:\n                if (\n                    # note: we cannot check for any sequence or iterable\n                    #       because of strings and nested dicts.\n                    not isinstance(representation[name], (list, tuple)) and\n                    field.many\n                ):\n                    raise ValueError(\"field should be sequence\")\n\n                source = _source(name, field)\n                value = representation[name]\n\n                if field.many:\n                    if not field.allow_null:\n                        object_dict[source] = [\n                            field.from_representation(single_value)\n                            for single_value in value\n                        ]\n                    else:\n                        object_dict[source] = [\n                            field.from_representation(single_value)\n                            if single_value is not None else None\n                            for single_value in value\n                        ]\n                else:\n                    if not field.allow_null:\n                        object_dict[source] = field.from_representation(value)\n                    else:\n                        object_dict[source] = field.from_representation(\n                            value) if value else None\n            except ValueError as err:\n                failed[name] = str(err)\n\n        if failed:\n            # if failed to parse we eagerly perform validation so full\n            # information about what is wrong will be returned\n            try:\n                self.validate(object_dict)\n                # note: this exception can be reached with partial==True\n                # since do not support partial updates yet this has 'no cover'\n                raise DeserializationError()  # pragma: no cover\n            except DeserializationError as err:\n                err.failed = failed\n                raise\n\n        return object_dict", "response": "Convert given representation dict into internal object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self, object_dict, partial=False):\n        # we are working on object_dict not an representation so there\n        # is a need to annotate sources differently\n        sources = {\n            _source(name, field): field\n            for name, field in self.fields.items()\n        }\n\n        # note: we are checking for all mising and invalid fields so we can\n        # return exception with all fields that are missing and should\n        # exist instead of single one\n        missing = [\n            name for name, field in sources.items()\n            if all((not partial, name not in object_dict, not field.read_only))\n        ]\n\n        forbidden = [\n            name for name in object_dict\n            if any((name not in sources, sources[name].read_only))\n        ]\n\n        invalid = {}\n        for name, value in object_dict.items():\n            try:\n                field = sources[name]\n\n                if field.many:\n                    for single_value in value:\n                        field.validate(single_value)\n                else:\n                    field.validate(value)\n\n            except ValueError as err:\n                invalid[name] = str(err)\n\n        if any([missing, forbidden, invalid]):\n            # note: We have validated internal object instance but need to\n            #       inform the user about problems with his representation.\n            #       This is why we have to do this dirty transformation.\n            # note: This will be removed in 1.0.0 where we change how\n            #       validation works and where we remove star-like fields.\n            # refs: #42 (https://github.com/swistakm/graceful/issues/42)\n            sources_to_field_names = {\n                _source(name, field): name\n                for name, field in self.fields.items()\n            }\n\n            def _(names):\n                if isinstance(names, list):\n                    return [\n                        sources_to_field_names.get(name, name)\n                        for name in names\n                    ]\n                elif isinstance(names, dict):\n                    return {\n                        sources_to_field_names.get(name, name): value\n                        for name, value in names.items()\n                    }\n                else:\n                    return names  # pragma: nocover\n\n            raise DeserializationError(_(missing), _(forbidden), _(invalid))", "response": "Validate given internal object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_attribute(self, obj, attr):\n        # '*' is a special wildcard character that means whole object\n        # is passed\n        if attr == '*':\n            return obj\n\n        # if this is any mapping then instead of attributes use keys\n        if isinstance(obj, Mapping):\n            return obj.get(attr, None)\n\n        return getattr(obj, attr, None)", "response": "Get attribute of given object instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_attribute(self, obj, attr, value):\n        # if this is any mutable mapping then instead of attributes use keys\n        if isinstance(obj, MutableMapping):\n            obj[attr] = value\n        else:\n            setattr(obj, attr, value)", "response": "Set value of attribute in given object instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndescribe all serialized fields.", "response": "def describe(self):\n        \"\"\"Describe all serialized fields.\n\n        It returns dictionary of all fields description defined for this\n        serializer using their own ``describe()`` methods with respect to order\n        in which they are defined as class attributes.\n\n        Returns:\n            OrderedDict: serializer description\n\n        \"\"\"\n        return OrderedDict([\n            (name, field.describe())\n            for name, field in self.fields.items()\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadapts golang s net. JoinHostPort", "response": "def _join_host_port(host, port):\n    \"\"\"Adapted golang's net.JoinHostPort\"\"\"\n    template = \"%s:%s\"\n    host_requires_bracketing = ':' in host or '%' in host\n    if host_requires_bracketing:\n        template = \"[%s]:%s\"\n    return template % (host, port)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new ControllerRevision with the given namespace and body.", "response": "def create_namespaced_controller_revision(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_controller_revision  # noqa: E501\n\n        create a ControllerRevision  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_controller_revision(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ControllerRevision body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ControllerRevision\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_controller_revision_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_controller_revision_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_namespaced_stateful_set(self, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_stateful_set_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_stateful_set_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a new StatefulSet with the specified namespace and body."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a namespace from the stateful set", "response": "def delete_collection_namespaced_stateful_set(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_collection_namespaced_stateful_set  # noqa: E501\n\n        delete collection of StatefulSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_collection_namespaced_stateful_set(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_stateful_set_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_stateful_set_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_namespaced_controller_revision(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_controller_revision_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_controller_revision_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a namespaced ControllerRevision."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting all namespaces in the current namespace.", "response": "def list_controller_revision_for_all_namespaces(self, **kwargs):  # noqa: E501\n        \"\"\"list_controller_revision_for_all_namespaces  # noqa: E501\n\n        list or watch objects of kind ControllerRevision  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_controller_revision_for_all_namespaces(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1ControllerRevisionList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_controller_revision_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_controller_revision_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_namespaced_stateful_set(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_stateful_set_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_stateful_set_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "List all the stateful resources in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npatching a ControllerRevision with the given body.", "response": "def patch_namespaced_controller_revision(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_controller_revision  # noqa: E501\n\n        partially update the specified ControllerRevision  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_controller_revision(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ControllerRevision (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ControllerRevision\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_controller_revision_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_controller_revision_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef patch_namespaced_stateful_set(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_stateful_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_stateful_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a namespace of a stateful set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npatch a namespace in a StatefulSet", "response": "def patch_namespaced_stateful_set_scale(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_stateful_set_scale  # noqa: E501\n\n        partially update scale of the specified StatefulSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_stateful_set_scale(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Scale (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Scale\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_stateful_set_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_stateful_set_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_namespaced_controller_revision(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_controller_revision_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_controller_revision_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read a V1 ControllerRevision"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the named StatefulSet and return the corresponding V1StatefulSet object.", "response": "def read_namespaced_stateful_set(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_stateful_set  # noqa: E501\n\n        read the specified StatefulSet  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_stateful_set(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the StatefulSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1StatefulSet\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_stateful_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_stateful_set_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_namespaced_stateful_set_scale(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_stateful_set_scale_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_stateful_set_scale_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read a namespace for a StatefulSet."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreplaces the specified ControllerRevision with the specified body.", "response": "def replace_namespaced_controller_revision(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_controller_revision  # noqa: E501\n\n        replace the specified ControllerRevision  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_controller_revision(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ControllerRevision (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ControllerRevision body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ControllerRevision\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_controller_revision_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_controller_revision_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace_namespaced_stateful_set(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_stateful_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_stateful_set_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the named StatefulSet with the specified body."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace_namespaced_stateful_set_scale(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_stateful_set_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_stateful_set_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a namespace with a new scale."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle(self, handler, req, resp, **kwargs):\n        params = self.require_params(req)\n\n        # future: remove in 1.x\n        if getattr(self, '_with_context', False):\n            handler = partial(handler, context=req.context)\n\n        meta, content = self.require_meta_and_content(\n            handler, params, **kwargs\n        )\n        self.make_body(resp, params, meta, content)\n        return content", "response": "Handle given resource manipulation flow in consistent manner."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresponds on GET HTTP request assuming that single resource instance retrieval flow.", "response": "def on_get(self, req, resp, handler=None, **kwargs):\n        \"\"\"Respond on GET HTTP request assuming resource retrieval flow.\n\n        This request handler assumes that GET requests are associated with\n        single resource instance retrieval. Thus default flow for such requests\n        is:\n\n        * Retrieve single resource instance of prepare its representation by\n          calling retrieve method handler.\n\n        Args:\n            req (falcon.Request): request object instance.\n            resp (falcon.Response): response object instance to be modified\n            handler (method): list method handler to be called. Defaults\n                to ``self.list``.\n            **kwargs: additional keyword arguments retrieved from url template.\n        \"\"\"\n        self.handle(\n            handler or self.retrieve, req, resp, **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_get(self, req, resp, handler=None, **kwargs):\n        self.handle(\n            handler or self.list, req, resp, **kwargs\n        )", "response": "Respond on GET HTTP request assuming that resource list retrieval flow is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresponds on DELETE HTTP request assuming that resource deletion flow is used.", "response": "def on_delete(self, req, resp, handler=None, **kwargs):\n        \"\"\"Respond on DELETE HTTP request assuming resource deletion flow.\n\n        This request handler assumes that DELETE requests are associated with\n        resource deletion. Thus default flow for such requests is:\n\n        * Delete existing resource instance.\n        * Set response status code to ``202 Accepted``.\n\n        Args:\n            req (falcon.Request): request object instance.\n            resp (falcon.Response): response object instance to be modified\n            handler (method): deletion method handler to be called. Defaults\n                to ``self.delete``.\n            **kwargs: additional keyword arguments retrieved from url template.\n        \"\"\"\n        self.handle(\n            handler or self.delete, req, resp, **kwargs\n        )\n\n        resp.status = falcon.HTTP_ACCEPTED"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_put(self, req, resp, handler=None, **kwargs):\n        self.handle(\n            handler or self.update, req, resp, **kwargs\n        )\n        resp.status = falcon.HTTP_ACCEPTED", "response": "Respond on PUT HTTP request assuming that resource update flow is performed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_post(self, req, resp, handler=None, **kwargs):\n        obj = self.handle(\n            handler or self.create, req, resp, **kwargs\n        )\n        try:\n            resp.location = self.get_object_location(obj)\n        except NotImplementedError:\n            pass\n\n        resp.status = falcon.HTTP_CREATED", "response": "Respond on POST HTTP request assuming resource creation flow."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresponding on PATCH HTTP request assuming that resource creation flow is performed.", "response": "def on_patch(self, req, resp, handler=None, **kwargs):\n        \"\"\"Respond on POST HTTP request assuming resource creation flow.\n\n        This request handler assumes that POST requests are associated with\n        resource creation. Thus default flow for such requests is:\n\n        * Create new resource instances and prepare their representation by\n          calling its bulk creation method handler.\n        * Set response status code to ``201 Created``.\n\n        **Note:** this handler does not set ``Location`` header by default as\n        it would be valid only for single resource creation.\n\n        Args:\n            req (falcon.Request): request object instance.\n            resp (falcon.Response): response object instance to be modified\n            handler (method): creation method handler to be called. Defaults\n                to ``self.create``.\n            **kwargs: additional keyword arguments retrieved from url template.\n        \"\"\"\n        self.handle(\n            handler or self.create_bulk, req, resp, **kwargs\n        )\n\n        resp.status = falcon.HTTP_CREATED"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_pagination_meta(self, params, meta):\n        meta['page_size'] = params['page_size']\n        meta['page'] = params['page']\n\n        meta['prev'] = \"page={0}&page_size={1}\".format(\n            params['page'] - 1, params['page_size']\n        ) if meta['page'] > 0 else None\n\n        meta['next'] = \"page={0}&page_size={1}\".format(\n            params['page'] + 1, params['page_size']\n        ) if meta.get('has_more', True) else None", "response": "Extend default meta dictionary value with pagination hints."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect_delete_namespaced_pod_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_delete_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_delete_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Connect delete the pod proxy of a namespace"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects delete the pod proxy with the given path", "response": "def connect_delete_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        \"\"\"connect_delete_namespaced_pod_proxy_with_path  # noqa: E501\n\n        connect DELETE requests to proxy of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_delete_namespaced_pod_proxy_with_path(name, namespace, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the URL path to use for the current proxy request to pod.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_delete_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_delete_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect delete the named service proxy with the specified namespace.", "response": "def connect_delete_namespaced_service_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_delete_namespaced_service_proxy  # noqa: E501\n\n        connect DELETE requests to proxy of Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_delete_namespaced_service_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_delete_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_delete_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting Delete Node Proxy", "response": "def connect_delete_node_proxy(self, name, **kwargs):  # noqa: E501\n        \"\"\"connect_delete_node_proxy  # noqa: E501\n\n        connect DELETE requests to proxy of Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_delete_node_proxy(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_delete_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_delete_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect_delete_node_proxy_with_path(self, name, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_delete_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_delete_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect Delete Node Proxy with Path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to the specified pod and return the corresponding object.", "response": "def connect_get_namespaced_pod_attach(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_get_namespaced_pod_attach  # noqa: E501\n\n        connect GET requests to attach of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_get_namespaced_pod_attach(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodAttachOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str container: The container in which to execute the command. Defaults to only container if there is only one container in the pod.\n        :param bool stderr: Stderr if true indicates that stderr is to be redirected for the attach call. Defaults to true.\n        :param bool stdin: Stdin if true, redirects the standard input stream of the pod for this call. Defaults to false.\n        :param bool stdout: Stdout if true indicates that stdout is to be redirected for the attach call. Defaults to true.\n        :param bool tty: TTY if true indicates that a tty will be allocated for the attach call. This is passed through the container runtime so the tty is allocated on the worker node by the container runtime. Defaults to false.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_get_namespaced_pod_attach_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_get_namespaced_pod_attach_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to a pod with portforward", "response": "def connect_get_namespaced_pod_portforward(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_get_namespaced_pod_portforward  # noqa: E501\n\n        connect GET requests to portforward of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_get_namespaced_pod_portforward(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodPortForwardOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param int ports: List of ports to forward Required when using WebSockets\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_get_namespaced_pod_portforward_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_get_namespaced_pod_portforward_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to a pod proxy of a namespace", "response": "def connect_get_namespaced_pod_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_get_namespaced_pod_proxy  # noqa: E501\n\n        connect GET requests to proxy of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_get_namespaced_pod_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the URL path to use for the current proxy request to pod.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_get_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_get_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_get_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_get_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_get_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect to a pod with a given path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_get_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_get_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_get_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect get the named service proxy with the specified path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect Get Node Proxy", "response": "def connect_get_node_proxy(self, name, **kwargs):  # noqa: E501\n        \"\"\"connect_get_node_proxy  # noqa: E501\n\n        connect GET requests to proxy of Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_get_node_proxy(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_get_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_get_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect_get_node_proxy_with_path(self, name, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_get_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_get_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect Get Node Proxy with Path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_head_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_head_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_head_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect the given pod with the given path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting the HTTP request to the service proxy of a namespace.", "response": "def connect_head_namespaced_service_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_head_namespaced_service_proxy  # noqa: E501\n\n        connect HEAD requests to proxy of Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_head_namespaced_service_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_head_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_head_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect_head_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_head_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_head_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect the HTTP request to the service proxy with the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects HEAD requests to proxy of Node", "response": "def connect_head_node_proxy_with_path(self, name, path, **kwargs):  # noqa: E501\n        \"\"\"connect_head_node_proxy_with_path  # noqa: E501\n\n        connect HEAD requests to proxy of Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_head_node_proxy_with_path(name, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_head_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_head_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect_options_namespaced_pod_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_options_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_options_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Connect OPTIONS requests to proxy of a Pod"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect_options_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_options_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_options_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect Options requests to proxy of a Pod with a given path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect_options_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_options_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_options_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data", "response": "This method connects OPTIONS requests to proxy of Service with the given path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect Options Node Proxy", "response": "def connect_options_node_proxy(self, name, **kwargs):  # noqa: E501\n        \"\"\"connect_options_node_proxy  # noqa: E501\n\n        connect OPTIONS requests to proxy of Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_options_node_proxy(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_options_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_options_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect OPTIONS requests to proxy of Node", "response": "def connect_options_node_proxy_with_path(self, name, path, **kwargs):  # noqa: E501\n        \"\"\"connect_options_node_proxy_with_path  # noqa: E501\n\n        connect OPTIONS requests to proxy of Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_options_node_proxy_with_path(name, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_options_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_options_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting PATCH requests to proxy of Pod with given path", "response": "def connect_patch_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        \"\"\"connect_patch_namespaced_pod_proxy_with_path  # noqa: E501\n\n        connect PATCH requests to proxy of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_patch_namespaced_pod_proxy_with_path(name, namespace, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the URL path to use for the current proxy request to pod.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_patch_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_patch_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects PATCH requests to proxy of Service", "response": "def connect_patch_namespaced_service_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_patch_namespaced_service_proxy  # noqa: E501\n\n        connect PATCH requests to proxy of Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_patch_namespaced_service_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_patch_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_patch_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting PATCH requests to proxy of Service with given path.", "response": "def connect_patch_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        \"\"\"connect_patch_namespaced_service_proxy_with_path  # noqa: E501\n\n        connect PATCH requests to proxy of Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_patch_namespaced_service_proxy_with_path(name, namespace, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_patch_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_patch_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect_patch_node_proxy_with_path(self, name, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_patch_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_patch_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect PATCH requests to proxy of Node"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting to a pod with a given namespace.", "response": "def connect_post_namespaced_pod_attach(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_post_namespaced_pod_attach  # noqa: E501\n\n        connect POST requests to attach of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_post_namespaced_pod_attach(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodAttachOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str container: The container in which to execute the command. Defaults to only container if there is only one container in the pod.\n        :param bool stderr: Stderr if true indicates that stderr is to be redirected for the attach call. Defaults to true.\n        :param bool stdin: Stdin if true, redirects the standard input stream of the pod for this call. Defaults to false.\n        :param bool stdout: Stdout if true indicates that stdout is to be redirected for the attach call. Defaults to true.\n        :param bool tty: TTY if true indicates that a tty will be allocated for the attach call. This is passed through the container runtime so the tty is allocated on the worker node by the container runtime. Defaults to false.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_namespaced_pod_attach_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_post_namespaced_pod_attach_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to an exec of a Pod in a namespace", "response": "def connect_post_namespaced_pod_exec(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_post_namespaced_pod_exec  # noqa: E501\n\n        connect POST requests to exec of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_post_namespaced_pod_exec(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodExecOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str command: Command is the remote command to execute. argv array. Not executed within a shell.\n        :param str container: Container in which to execute the command. Defaults to only container if there is only one container in the pod.\n        :param bool stderr: Redirect the standard error stream of the pod for this call. Defaults to true.\n        :param bool stdin: Redirect the standard input stream of the pod for this call. Defaults to false.\n        :param bool stdout: Redirect the standard output stream of the pod for this call. Defaults to true.\n        :param bool tty: TTY if true indicates that a tty will be allocated for the exec call. Defaults to false.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_namespaced_pod_exec_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_post_namespaced_pod_exec_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_post_namespaced_pod_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_post_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Connect to proxy of a Pod"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects to proxy of a pod with a given path", "response": "def connect_post_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        \"\"\"connect_post_namespaced_pod_proxy_with_path  # noqa: E501\n\n        connect POST requests to proxy of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_post_namespaced_pod_proxy_with_path(name, namespace, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the URL path to use for the current proxy request to pod.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_post_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to the service proxy with the specified namespace.", "response": "def connect_post_namespaced_service_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_post_namespaced_service_proxy  # noqa: E501\n\n        connect POST requests to proxy of Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_post_namespaced_service_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_post_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect_post_node_proxy(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_post_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "Connect to a node proxy"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect_post_node_proxy_with_path(self, name, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_post_node_proxy_with_path_with_http_info(name, path, **kwargs)  # noqa: E501\n            return data", "response": "Connect to a node proxy with a path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting PUT requests to proxy of a Pod", "response": "def connect_put_namespaced_pod_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_put_namespaced_pod_proxy  # noqa: E501\n\n        connect PUT requests to proxy of Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_put_namespaced_pod_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the URL path to use for the current proxy request to pod.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_put_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_put_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect PUT requests to proxy of Service", "response": "def connect_put_namespaced_service_proxy(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"connect_put_namespaced_service_proxy  # noqa: E501\n\n        connect PUT requests to proxy of Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_put_namespaced_service_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_put_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_put_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect_put_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_put_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_put_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  # noqa: E501\n            return data", "response": "This method allows you to add a new path to a service proxy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting PUT requests to proxy of Node", "response": "def connect_put_node_proxy(self, name, **kwargs):  # noqa: E501\n        \"\"\"connect_put_node_proxy  # noqa: E501\n\n        connect PUT requests to proxy of Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_put_node_proxy(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_put_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.connect_put_node_proxy_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_namespace(self, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespace_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespace_with_http_info(body, **kwargs)  # noqa: E501\n            return data", "response": "Create a new namespace. This method creates a new namespace and returns the created namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Binding in the specified namespace.", "response": "def create_namespaced_binding(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_binding  # noqa: E501\n\n        create a Binding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_binding(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Binding body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param bool include_uninitialized: If IncludeUninitialized is specified, the object may be returned without completing initialization.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Binding\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_binding_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_binding_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_namespaced_config_map(self, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_config_map_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_config_map_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a ConfigMap with the specified namespace and body."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_namespaced_limit_range(self, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_limit_range_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_limit_range_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a LimitRange object in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_namespaced_persistent_volume_claim(self, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_persistent_volume_claim_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_persistent_volume_claim_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a new persistent volume claim with the specified namespace and body."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new NamespacedPodBinding", "response": "def create_namespaced_pod_binding(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_pod_binding  # noqa: E501\n\n        create binding of a Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_pod_binding(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Binding (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Binding body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param bool include_uninitialized: If IncludeUninitialized is specified, the object may be returned without completing initialization.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Binding\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_binding_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_pod_binding_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new object in the specified namespace with the specified body.", "response": "def create_namespaced_pod_eviction(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_pod_eviction  # noqa: E501\n\n        create eviction of a Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_pod_eviction(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Eviction (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1beta1Eviction body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param bool include_uninitialized: If IncludeUninitialized is specified, the object may be returned without completing initialization.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1Eviction\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_eviction_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_pod_eviction_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_namespaced_pod_template(self, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_template_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_pod_template_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Create a PodTemplate with the specified namespace and body."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a ResourceQuota object in the specified namespace.", "response": "def create_namespaced_resource_quota(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_resource_quota  # noqa: E501\n\n        create a ResourceQuota  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_resource_quota(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ResourceQuota body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ResourceQuota\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_resource_quota_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_resource_quota_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new Secret in the specified namespace.", "response": "def create_namespaced_secret(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_secret  # noqa: E501\n\n        create a Secret  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_secret(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Secret body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Secret\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_secret_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_secret_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new Service in the specified namespace.", "response": "def create_namespaced_service(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_service  # noqa: E501\n\n        create a Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_service(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Service body: (required)\n        :param bool include_uninitialized: If IncludeUninitialized is specified, the object may be returned without completing initialization.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Service\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_service_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_service_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a node in the cluster.", "response": "def create_node(self, body, **kwargs):  # noqa: E501\n        \"\"\"create_node  # noqa: E501\n\n        create a Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_node(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1Node body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Node\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_node_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_node_with_http_info(body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_persistent_volume(self, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_persistent_volume_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_persistent_volume_with_http_info(body, **kwargs)  # noqa: E501\n            return data", "response": "Create a persistent volume."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_collection_namespaced_config_map(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_config_map_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_config_map_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a collection of ConfigMap objects in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_collection_namespaced_limit_range(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_limit_range_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_limit_range_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a collection of LimitRanges"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a collection of persistent volume claims in the specified namespace.", "response": "def delete_collection_namespaced_persistent_volume_claim(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_collection_namespaced_persistent_volume_claim  # noqa: E501\n\n        delete collection of PersistentVolumeClaim  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_collection_namespaced_persistent_volume_claim(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_persistent_volume_claim_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_persistent_volume_claim_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_collection_namespaced_replication_controller(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_replication_controller_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_replication_controller_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a collection of ReplicationController objects in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting collection of ResourceQuota objects in the specified namespace.", "response": "def delete_collection_namespaced_resource_quota(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_collection_namespaced_resource_quota  # noqa: E501\n\n        delete collection of ResourceQuota  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_collection_namespaced_resource_quota(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_resource_quota_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_resource_quota_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_collection_node(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_node_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_node_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "Delete collection of nodes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting collection of persistent volumes", "response": "def delete_collection_persistent_volume(self, **kwargs):  # noqa: E501\n        \"\"\"delete_collection_persistent_volume  # noqa: E501\n\n        delete collection of PersistentVolume  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_collection_persistent_volume(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_persistent_volume_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_persistent_volume_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the named Endpoints in the specified namespace.", "response": "def delete_namespaced_endpoints(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_endpoints  # noqa: E501\n\n        delete Endpoints  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_endpoints(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Endpoints (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_endpoints_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_endpoints_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_namespaced_pod(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_pod_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_pod_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a pod with the specified name and namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a pod template in the specified namespace.", "response": "def delete_namespaced_pod_template(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_pod_template  # noqa: E501\n\n        delete a PodTemplate  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_pod_template(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodTemplate (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_pod_template_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_pod_template_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_namespaced_secret(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_secret_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_secret_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a namespaced Secret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_namespaced_service(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_service_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_service_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a namespaced service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_persistent_volume(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_persistent_volume_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_persistent_volume_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "Delete a persistent volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all endpoints in a namespace", "response": "def list_namespaced_endpoints(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"list_namespaced_endpoints  # noqa: E501\n\n        list or watch objects of kind Endpoints  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_namespaced_endpoints(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1EndpointsList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_endpoints_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_endpoints_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npatches the LimitRange with the specified body.", "response": "def patch_namespaced_limit_range(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_limit_range  # noqa: E501\n\n        partially update the specified LimitRange  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_limit_range(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the LimitRange (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1LimitRange\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_limit_range_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_limit_range_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npatch a namespace s persistent volume claim.", "response": "def patch_namespaced_persistent_volume_claim(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_persistent_volume_claim  # noqa: E501\n\n        partially update the specified PersistentVolumeClaim  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_persistent_volume_claim(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PersistentVolumeClaim (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1PersistentVolumeClaim\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_persistent_volume_claim_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_persistent_volume_claim_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch_namespaced_persistent_volume_claim_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_persistent_volume_claim_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_persistent_volume_claim_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the status of a namespace s persistent volume claim."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npatch a Pod with the given namespace with the given body.", "response": "def patch_namespaced_pod(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_pod  # noqa: E501\n\n        partially update the specified Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_pod(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Pod (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Pod\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_pod_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_pod_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npatches the status of a Pod in a namespace", "response": "def patch_namespaced_pod_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_pod_status  # noqa: E501\n\n        partially update status of the specified Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_pod_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Pod (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Pod\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_pod_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_pod_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_namespaced_pod_template(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_pod_template_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_pod_template_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a PodTemplate with the given name and namespace with the given body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npatching a ReplicationController with the given namespace with the given body.", "response": "def patch_namespaced_replication_controller(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_replication_controller  # noqa: E501\n\n        partially update the specified ReplicationController  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_replication_controller(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicationController (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ReplicationController\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_replication_controller_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_replication_controller_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_namespaced_replication_controller_scale(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_replication_controller_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_replication_controller_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a ReplicationController in the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npatch the status of a ReplicationController in a namespace.", "response": "def patch_namespaced_replication_controller_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_replication_controller_status  # noqa: E501\n\n        partially update status of the specified ReplicationController  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_replication_controller_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicationController (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ReplicationController\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_replication_controller_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_replication_controller_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_namespaced_resource_quota(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_resource_quota_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_resource_quota_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the specified ResourceQuota."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npatching the status of a resource quota.", "response": "def patch_namespaced_resource_quota_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_resource_quota_status  # noqa: E501\n\n        partially update status of the specified ResourceQuota  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_resource_quota_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ResourceQuota (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ResourceQuota\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_namespaced_secret(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_secret_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_secret_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a Secret in a Namespace"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef patch_namespaced_service(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_service_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_service_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a Service in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef patch_namespaced_service_account(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_service_account_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_service_account_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch a namespace of a service account."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npatch the status of a Service in the given Namespace", "response": "def patch_namespaced_service_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_service_status  # noqa: E501\n\n        partially update status of the specified Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_service_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Service (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Service\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_service_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_service_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npatching a node s attributes.", "response": "def patch_node(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"patch_node  # noqa: E501\n\n        partially update the specified Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_node(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Node (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Node\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_node_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_node_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch_node_status(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_node_status_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_node_status_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the status of a specific Node in the V1 cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef patch_persistent_volume(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_persistent_volume_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_persistent_volume_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch an existing persistent volume."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch_persistent_volume_status(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_persistent_volume_status_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_persistent_volume_status_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the status of a specific persistent volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_component_status(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_component_status_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_component_status_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "Read the specified ComponentStatus"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_namespace(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespace_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespace_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "Read a specific namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the status of a specific namespace.", "response": "def read_namespace_status(self, name, **kwargs):  # noqa: E501\n        \"\"\"read_namespace_status  # noqa: E501\n\n        read status of the specified Namespace  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespace_status(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Namespace (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Namespace\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespace_status_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespace_status_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the specified ConfigMap", "response": "def read_namespaced_config_map(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_config_map  # noqa: E501\n\n        read the specified ConfigMap  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_config_map(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ConfigMap (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1ConfigMap\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_config_map_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_config_map_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_namespaced_endpoints(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_endpoints_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_endpoints_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read the specified Endpoints in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the specified LimitRange.", "response": "def read_namespaced_limit_range(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_limit_range  # noqa: E501\n\n        read the specified LimitRange  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_limit_range(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the LimitRange (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1LimitRange\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_limit_range_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_limit_range_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_namespaced_persistent_volume_claim(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_persistent_volume_claim_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_persistent_volume_claim_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read a namespaced persistent volume claim."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_namespaced_pod(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_pod_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_pod_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read the specified Pod"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the logs of a Pod in a namespace.", "response": "def read_namespaced_pod_log(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_pod_log  # noqa: E501\n\n        read log of the specified Pod  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_pod_log(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Pod (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str container: The container for which to stream logs. Defaults to only container if there is one container in the pod.\n        :param bool follow: Follow the log stream of the pod. Defaults to false.\n        :param int limit_bytes: If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool previous: Return previous terminated container logs. Defaults to false.\n        :param int since_seconds: A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified.\n        :param int tail_lines: If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime\n        :param bool timestamps: If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_namespaced_pod_status(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_pod_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_pod_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read the status of a Pod in a namespace"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a ReplicationController in a namespace.", "response": "def read_namespaced_replication_controller(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_replication_controller  # noqa: E501\n\n        read the specified ReplicationController  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_replication_controller(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicationController (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1ReplicationController\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_replication_controller_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_replication_controller_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a ReplicationController in a namespace.", "response": "def read_namespaced_replication_controller_scale(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_replication_controller_scale  # noqa: E501\n\n        read scale of the specified ReplicationController  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_replication_controller_scale(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Scale (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Scale\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_replication_controller_scale_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_replication_controller_scale_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_namespaced_replication_controller_status(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_replication_controller_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_replication_controller_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read the status of a ReplicationController in a namespace"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_namespaced_resource_quota_status(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_resource_quota_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_resource_quota_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Get the status of the specified ResourceQuota in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_namespaced_secret(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_secret_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_secret_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read a V1 Secret in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_namespaced_service(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_service_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_service_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read a named service in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_namespaced_service_status(self, name, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_service_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_service_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data", "response": "Read the status of the specified Service in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a node from the cluster.", "response": "def read_node(self, name, **kwargs):  # noqa: E501\n        \"\"\"read_node  # noqa: E501\n\n        read the specified Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_node(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Node (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1Node\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_node_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_node_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the status of a specific Node in the V1 cluster.", "response": "def read_node_status(self, name, **kwargs):  # noqa: E501\n        \"\"\"read_node_status  # noqa: E501\n\n        read status of the specified Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_node_status(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Node (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Node\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_node_status_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_node_status_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the status of a specific persistent volume.", "response": "def read_persistent_volume_status(self, name, **kwargs):  # noqa: E501\n        \"\"\"read_persistent_volume_status  # noqa: E501\n\n        read status of the specified PersistentVolume  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_persistent_volume_status(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PersistentVolume (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1PersistentVolume\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_persistent_volume_status_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_persistent_volume_status_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace_namespace(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespace_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespace_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the specified Namespace"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef replace_namespace_finalize(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespace_finalize_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespace_finalize_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace finalize of the specified Namespace"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the specified ConfigMap with the specified body.", "response": "def replace_namespaced_config_map(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_config_map  # noqa: E501\n\n        replace the specified ConfigMap  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_config_map(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ConfigMap (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ConfigMap body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ConfigMap\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_config_map_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_config_map_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace_namespaced_endpoints(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_endpoints_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_endpoints_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the specified Endpoints with the specified body."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces a namespaced persistent volume claim.", "response": "def replace_namespaced_persistent_volume_claim(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_persistent_volume_claim  # noqa: E501\n\n        replace the specified PersistentVolumeClaim  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_persistent_volume_claim(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PersistentVolumeClaim (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1PersistentVolumeClaim body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1PersistentVolumeClaim\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_persistent_volume_claim_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_persistent_volume_claim_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace_namespaced_persistent_volume_claim_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_persistent_volume_claim_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_persistent_volume_claim_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the status of a namespace with a new status."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef replace_namespaced_pod(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_pod_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_pod_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the specified Pod with the specified body."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces a PodTemplate with a new one.", "response": "def replace_namespaced_pod_template(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_pod_template  # noqa: E501\n\n        replace the specified PodTemplate  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_pod_template(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodTemplate (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1PodTemplate body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1PodTemplate\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_pod_template_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_pod_template_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces a ReplicationController with a new one.", "response": "def replace_namespaced_replication_controller(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_replication_controller  # noqa: E501\n\n        replace the specified ReplicationController  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_replication_controller(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicationController (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ReplicationController body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ReplicationController\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_replication_controller_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_replication_controller_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces a ReplicationController with a new V1Scale.", "response": "def replace_namespaced_replication_controller_scale(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_replication_controller_scale  # noqa: E501\n\n        replace scale of the specified ReplicationController  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_replication_controller_scale(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Scale (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Scale body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Scale\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_replication_controller_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_replication_controller_scale_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_namespaced_resource_quota(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_resource_quota_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_resource_quota_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the specified ResourceQuota with the specified body."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the status of a resource quota with a new status.", "response": "def replace_namespaced_resource_quota_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_resource_quota_status  # noqa: E501\n\n        replace status of the specified ResourceQuota  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_resource_quota_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ResourceQuota (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ResourceQuota body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ResourceQuota\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace the specified Secret with the specified body.", "response": "def replace_namespaced_secret(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_secret  # noqa: E501\n\n        replace the specified Secret  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_secret(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Secret (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Secret body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Secret\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_secret_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_secret_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_namespaced_service_account(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_service_account_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_service_account_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the specified ServiceAccount with a new one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace the status of a Service in a Namespace", "response": "def replace_namespaced_service_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_service_status  # noqa: E501\n\n        replace status of the specified Service  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_service_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Service (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Service body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Service\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_service_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_service_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace a node in the cluster.", "response": "def replace_node(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"replace_node  # noqa: E501\n\n        replace the specified Node  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_node(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Node (required)\n        :param V1Node body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Node\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_node_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_node_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace_persistent_volume(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_persistent_volume_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_persistent_volume_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a persistent volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the status of a persistent volume.", "response": "def replace_persistent_volume_status(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"replace_persistent_volume_status  # noqa: E501\n\n        replace status of the specified PersistentVolume  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_persistent_volume_status(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PersistentVolume (required)\n        :param V1PersistentVolume body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1PersistentVolume\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_persistent_volume_status_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_persistent_volume_status_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_params(mcs, bases, namespace):\n        params = [\n            (name, namespace.pop(name))\n            for name, attribute\n            in list(namespace.items())\n            if isinstance(attribute, BaseParam)\n        ]\n\n        for base in reversed(bases):\n            if hasattr(base, mcs._params_storage_key):\n                params = list(\n                    getattr(base, mcs._params_storage_key).items()\n                ) + params\n\n        return OrderedDict(params)", "response": "Create params dictionary to be used in resource class namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_body(self, resp, params, meta, content):\n        response = {\n            'meta': meta,\n            'content': content\n        }\n        resp.content_type = 'application/json'\n        resp.body = json.dumps(\n            response,\n            indent=params['indent'] or None if 'indent' in params else None\n        )", "response": "Construct the response body in resp using JSON serialization."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn list of allowed HTTP methods on this resource.", "response": "def allowed_methods(self):\n        \"\"\"Return list of allowed HTTP methods on this resource.\n\n        This is only for purpose of making resource description.\n\n        Returns:\n            list: list of allowed HTTP method names (uppercase)\n\n        \"\"\"\n        return [\n            method\n            for method, allowed in (\n                ('GET', hasattr(self, 'on_get')),\n                ('POST', hasattr(self, 'on_post')),\n                ('PUT', hasattr(self, 'on_put')),\n                ('PATCH', hasattr(self, 'on_patch')),\n                ('DELETE', hasattr(self, 'on_delete')),\n                ('HEAD', hasattr(self, 'on_head')),\n                ('OPTIONS', hasattr(self, 'on_options')),\n            ) if allowed\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndescribing API resource using resource introspection.", "response": "def describe(self, req=None, resp=None, **kwargs):\n        \"\"\"Describe API resource using resource introspection.\n\n        Additional description on derrived resource class can be added using\n        keyword arguments and calling ``super().decribe()`` method call\n        like following:\n\n        .. code-block:: python\n\n             class SomeResource(BaseResource):\n                 def describe(req, resp, **kwargs):\n                     return super().describe(\n                         req, resp, type='list', **kwargs\n                      )\n\n        Args:\n            req (falcon.Request): request object\n            resp (falcon.Response): response object\n            kwargs (dict): dictionary of values created from resource url\n                template\n\n        Returns:\n            dict: dictionary with resource descritpion information\n\n        .. versionchanged:: 0.2.0\n           The `req` and `resp` parameters became optional to ease the\n           implementation of application-level documentation generators.\n        \"\"\"\n        description = {\n            'params': OrderedDict([\n                (name, param.describe())\n                for name, param in self.params.items()\n            ]),\n            'details':\n                inspect.cleandoc(\n                    self.__class__.__doc__ or\n                    \"This resource does not have description yet\"\n                ),\n            'name': self.__class__.__name__,\n            'methods': self.allowed_methods()\n        }\n        # note: add path to resource description only if request object was\n        #       provided in order to make auto-documentation engines simpler\n        if req:\n            description['path'] = req.path\n\n        description.update(**kwargs)\n        return description"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresponding with JSON formatted resource description on OPTIONS request.", "response": "def on_options(self, req, resp, **kwargs):\n        \"\"\"Respond with JSON formatted resource description on OPTIONS request.\n\n        Args:\n            req (falcon.Request): Optional request object. Defaults to None.\n            resp (falcon.Response): Optional response object. Defaults to None.\n            kwargs (dict): Dictionary of values created by falcon from\n                resource uri template.\n\n        Returns:\n            None\n\n\n        .. versionchanged:: 0.2.0\n           Default ``OPTIONS`` responses include ``Allow`` header with list of\n           allowed HTTP methods.\n        \"\"\"\n        resp.set_header('Allow', ', '.join(self.allowed_methods()))\n        resp.body = json.dumps(self.describe(req, resp))\n        resp.content_type = 'application/json'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequiring all defined parameters from request query string.", "response": "def require_params(self, req):\n        \"\"\"Require all defined parameters from request query string.\n\n        Raises ``falcon.errors.HTTPMissingParam`` exception if any of required\n        parameters is missing and ``falcon.errors.HTTPInvalidParam`` if any\n        of parameters could not be understood (wrong format).\n\n        Args:\n            req (falcon.Request): request object\n\n        \"\"\"\n        params = {}\n\n        for name, param in self.params.items():\n            if name not in req.params and param.required:\n                # we could simply raise with this single param or use get_param\n                # with required=True parameter but for client convenience\n                # we prefer to list all missing params that are required\n                missing = set(\n                    p for p in self.params\n                    if self.params[p].required\n                ) - set(req.params.keys())\n\n                raise errors.HTTPMissingParam(\", \".join(missing))\n\n            elif name in req.params or param.default:\n                # Note: lack of key in req.params means it was not specified\n                # so unless there is default value it will not be included in\n                # output params dict.\n                # This way we have explicit information that param was\n                # not specified. Using None would not be as good because param\n                # class can also return None from `.value()` method as a valid\n                # translated value.\n                try:\n                    if param.many:\n                        # params with \"many\" enabled need special care\n                        values = req.get_param_as_list(\n                            # note: falcon allows to pass value handler using\n                            #       `transform` param so we do not need to\n                            #       iterate through list manually\n                            name, param.validated_value\n                        ) or [\n                            param.default and\n                            param.validated_value(param.default)\n                        ]\n                        params[name] = param.container(values)\n                    else:\n                        # note that if many==False and query parameter\n                        # occurs multiple times in qs then it is\n                        # **unspecified** which one will be used. See:\n                        # http://falcon.readthedocs.org/en/latest/api/request_and_response.html#falcon.Request.get_param  # noqa\n                        params[name] = param.validated_value(\n                            req.get_param(name, default=param.default)\n                        )\n\n                except ValidationError as err:\n                    # ValidationError allows to easily translate itself to\n                    # to falcon's HTTPInvalidParam (Bad Request HTTP response)\n                    raise err.as_invalid_param(name)\n\n                except ValueError as err:\n                    # Other parsing issues are expected to raise ValueError\n                    raise errors.HTTPInvalidParam(str(err), name)\n\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef require_meta_and_content(self, content_handler, params, **kwargs):\n        meta = {\n            'params': params\n        }\n        content = content_handler(params, meta, **kwargs)\n        meta['params'] = params\n        return meta, content", "response": "Require meta and content dictionaries using proper hander."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrequire raw dictionary of representation supplied in request object.", "response": "def require_representation(self, req):\n        \"\"\"Require raw representation dictionary from falcon request object.\n\n        This does not perform any field parsing or validation but only uses\n        allowed content-encoding handler to decode content body.\n\n        Note:\n            Currently only JSON is allowed as content type.\n\n        Args:\n            req (falcon.Request): request object\n\n        Returns:\n            dict: raw dictionary of representation supplied in request body\n\n        \"\"\"\n        try:\n            type_, subtype, _ = parse_mime_type(req.content_type)\n            content_type = '/'.join((type_, subtype))\n        except:\n            raise falcon.HTTPUnsupportedMediaType(\n                description=\"Invalid Content-Type header: {}\".format(\n                    req.content_type\n                )\n            )\n\n        if content_type == 'application/json':\n            body = req.stream.read()\n            return json.loads(body.decode('utf-8'))\n        else:\n            raise falcon.HTTPUnsupportedMediaType(\n                description=\"only JSON supported, got: {}\".format(content_type)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef require_validated(self, req, partial=False, bulk=False):\n        representations = [\n            self.require_representation(req)\n        ] if not bulk else self.require_representation(req)\n\n        if bulk and not isinstance(representations, list):\n            raise ValidationError(\n                \"Request payload should represent a list of resources.\"\n            ).as_bad_request()\n\n        object_dicts = []\n\n        try:\n            for representation in representations:\n                object_dict = self.serializer.from_representation(\n                    representation\n                )\n                self.serializer.validate(object_dict, partial)\n                object_dicts.append(object_dict)\n\n        except DeserializationError as err:\n            # when working on Resource we know that we can finally raise\n            # bad request exceptions\n            raise err.as_bad_request()\n\n        except ValidationError as err:\n            # ValidationError is a suggested way to validate whole resource\n            # so we also are prepared to catch it\n            raise err.as_bad_request()\n\n        return object_dicts if bulk else object_dicts[0]", "response": "Require fully validated internal object dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the logger file path.", "response": "def logger_file(self, value):\n        \"\"\"The logger file.\n\n        If the logger_file is None, then add stream handler and remove file\n        handler. Otherwise, add file handler and remove stream handler.\n\n        :param value: The logger_file path.\n        :type: str\n        \"\"\"\n        self.__logger_file = value\n        if self.__logger_file:\n            # If set logging file,\n            # then add file handler and remove stream handler.\n            self.logger_file_handler = logging.FileHandler(self.__logger_file)\n            self.logger_file_handler.setFormatter(self.logger_formatter)\n            for _, logger in six.iteritems(self.logger):\n                logger.addHandler(self.logger_file_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_file_list_handler(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.log_file_list_handler_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.log_file_list_handler_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "Log File List Handler"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_cluster_role_binding(self, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_cluster_role_binding_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_cluster_role_binding_with_http_info(body, **kwargs)  # noqa: E501\n            return data", "response": "create_cluster_role_binding  # noqa: E501\n\n        create a ClusterRoleBinding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_cluster_role_binding(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1ClusterRoleBinding body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1ClusterRoleBinding\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_cluster_role_binding(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_cluster_role_binding_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_cluster_role_binding_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "delete_cluster_role_binding  # noqa: E501\n\n        delete a ClusterRoleBinding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_cluster_role_binding(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ClusterRoleBinding (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_collection_namespaced_role_binding(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_role_binding_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_role_binding_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete a collection of RoleBinding objects in the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_cluster_role_binding(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_cluster_role_binding_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_cluster_role_binding_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "list_cluster_role_binding  # noqa: E501\n\n        list or watch objects of kind ClusterRoleBinding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_cluster_role_binding(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1ClusterRoleBindingList\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting all namespaces in the cluster.", "response": "def list_role_for_all_namespaces(self, **kwargs):  # noqa: E501\n        \"\"\"list_role_for_all_namespaces  # noqa: E501\n\n        list or watch objects of kind Role  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_role_for_all_namespaces(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1RoleList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_role_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_role_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npatch a V1 RoleBinding.", "response": "def patch_namespaced_role_binding(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_role_binding  # noqa: E501\n\n        partially update the specified RoleBinding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_role_binding(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the RoleBinding (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1RoleBinding\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_role_binding_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_role_binding_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a V1 RoleBinding in a namespace.", "response": "def read_namespaced_role_binding(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_role_binding  # noqa: E501\n\n        read the specified RoleBinding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_role_binding(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the RoleBinding (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1RoleBinding\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_role_binding_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_role_binding_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing a V1 RoleBinding with a new body.", "response": "def replace_namespaced_role_binding(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_role_binding  # noqa: E501\n\n        replace the specified RoleBinding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_role_binding(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the RoleBinding (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1RoleBinding body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1RoleBinding\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_role_binding_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_role_binding_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_api_versions(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_api_versions_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_api_versions_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "Get available API versions of a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the code version of an antenna entry.", "response": "def get_code(self, **kwargs):  # noqa: E501\n        \"\"\"get_code  # noqa: E501\n\n        get the code version  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_code(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: VersionInfo\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_code_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_code_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ca_bundle(self, ca_bundle):\n        if ca_bundle is not None and not re.search(r'^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$', ca_bundle):  # noqa: E501\n            raise ValueError(r\"Invalid value for `ca_bundle`, must be a follow pattern or equal to `/^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$/`\")  # noqa: E501\n\n        self._ca_bundle = ca_bundle", "response": "Sets the ca_bundle of this AdmissionregistrationV1beta1WebhookClientConfig."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the certificate of this V1beta1CertificateSigningRequestStatus.", "response": "def certificate(self, certificate):\n        \"\"\"Sets the certificate of this V1beta1CertificateSigningRequestStatus.\n\n        If request was approved, the controller will place the issued certificate here.  # noqa: E501\n\n        :param certificate: The certificate of this V1beta1CertificateSigningRequestStatus.  # noqa: E501\n        :type: str\n        \"\"\"\n        if certificate is not None and not re.search(r'^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$', certificate):  # noqa: E501\n            raise ValueError(r\"Invalid value for `certificate`, must be a follow pattern or equal to `/^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$/`\")  # noqa: E501\n\n        self._certificate = certificate"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_get(self, req, resp, **kwargs):\n        return super().on_get(\n            req, resp, handler=self._retrieve, **kwargs\n        )", "response": "Respond on GET requests using self. retrieve."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresponding on PUT requests using self. _update method.", "response": "def on_put(self, req, resp, **kwargs):\n        \"\"\"Respond on PUT requests using ``self.update()`` handler.\"\"\"\n        validated = self.require_validated(req)\n        return super().on_put(\n            req, resp,\n            handler=partial(self._update, validated=validated),\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextend default endpoint description with serializer description.", "response": "def describe(self, req=None, resp=None, **kwargs):\n        \"\"\"Extend default endpoint description with serializer description.\"\"\"\n        return super().describe(\n            req, resp,\n            type='list',\n            fields=self.serializer.describe() if self.serializer else None,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresponding on GET requests using self. _list method.", "response": "def on_get(self, req, resp, **kwargs):\n        \"\"\"Respond on GET requests using ``self.list()`` handler.\"\"\"\n        return super().on_get(req, resp, handler=self._list, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating items in bulk by reusing existing. create method handler.", "response": "def create_bulk(self, params, meta, **kwargs):\n        \"\"\"Create items in bulk by reusing existing ``.create()`` handler.\n\n        .. note::\n            This is default create_bulk implementation that may not be safe\n            to use in production environment depending on your implementation\n            of ``.create()`` method handler.\n        \"\"\"\n        validated = kwargs.pop('validated')\n        return [\n            self.create(params, meta, validated=item)\n            for item in validated\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrespond on POST requests using self. _create method.", "response": "def on_post(self, req, resp, **kwargs):\n        \"\"\"Respond on POST requests using ``self.create()`` handler.\"\"\"\n        validated = self.require_validated(req)\n\n        return super().on_post(\n            req, resp,\n            handler=partial(self._create, validated=validated),\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresponding on PATCH requests using self. _create_bulk method.", "response": "def on_patch(self, req, resp, **kwargs):\n        \"\"\"Respond on PATCH requests using ``self.create_bulk()`` handler.\"\"\"\n        validated = self.require_validated(req, bulk=True)\n\n        return super().on_patch(\n            req, resp,\n            handler=partial(self._create_bulk, validated=validated),\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user(\n        self, identified_with, identifier, req, resp, resource, uri_kwargs\n    ):\n        \"\"\"Return default user object.\"\"\"\n        return self.user", "response": "Return default user object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_storage_key(self, identified_with, identifier):\n        return ':'.join((\n            self.key_prefix, identified_with.name,\n            self.hash_identifier(identified_with, identifier),\n        ))", "response": "Get key string for given user identifier in consistent manner."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user(\n        self, identified_with, identifier, req, resp, resource, uri_kwargs\n    ):\n        \"\"\"Get user object for given identifier.\n\n        Args:\n            identified_with (object): authentication middleware used\n                to identify the user.\n            identifier: middleware specifix user identifier (string or tuple\n                in case of all built in authentication middleware classes).\n\n        Returns:\n            dict: user object stored in Redis if it exists, otherwise ``None``\n        \"\"\"\n        stored_value = self.kv_store.get(\n            self._get_storage_key(identified_with, identifier)\n        )\n        if stored_value is not None:\n            user = self.serialization.loads(stored_value.decode())\n        else:\n            user = None\n\n        return user", "response": "Get user object for given identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters new key for given client identifier.", "response": "def register(self, identified_with, identifier, user):\n        \"\"\"Register new key for given client identifier.\n\n        This is only a helper method that allows to register new\n        user objects for client identities (keys, tokens, addresses etc.).\n\n        Args:\n            identified_with (object): authentication middleware used\n                to identify the user.\n            identifier (str): user identifier.\n            user (str): user object to be stored in the backend.\n        \"\"\"\n        self.kv_store.set(\n            self._get_storage_key(identified_with, identifier),\n            self.serialization.dumps(user).encode(),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_resource(self, req, resp, resource, uri_kwargs=None):\n        if 'user' in req.context:\n            return\n\n        identifier = self.identify(req, resp, resource, uri_kwargs)\n        user = self.try_storage(identifier, req, resp, resource, uri_kwargs)\n\n        if user is not None:\n            req.context['user'] = user\n\n        # if did not succeed then we need to add this to list of available\n        # challenges.\n        elif self.challenge is not None:\n            req.context.setdefault(\n                'challenges', list()\n            ).append(self.challenge)", "response": "This is basic falcon middleware handler. It handles the resource after routing to it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef try_storage(self, identifier, req, resp, resource, uri_kwargs):\n        if identifier is None:\n            user = None\n\n        # note: if user_storage is defined, always use it in order to\n        #       authenticate user.\n        elif self.user_storage is not None:\n            user = self.user_storage.get_user(\n                self, identifier, req, resp, resource, uri_kwargs\n            )\n\n        # note: some authentication middleware classes may not require\n        #       to be initialized with their own user_storage. In such\n        #       case this will always authenticate with \"syntetic user\"\n        #       if there is a valid indentity.\n        elif self.user_storage is None and not self.only_with_storage:\n            user = {\n                'identified_with': self,\n                'identifier': identifier\n            }\n\n        else:  # pragma: nocover\n            # note: this should not happen if the base class is properly\n            #       initialized. Still, user can skip super().__init__() call.\n            user = None\n\n        return user", "response": "Try to find user in configured user storage object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef identify(self, req, resp, resource, uri_kwargs):\n        header = req.get_header(\"Authorization\", False)\n        auth = header.split(\" \") if header else None\n\n        if auth is None or auth[0].lower() != 'basic':\n            return None\n\n        if len(auth) != 2:\n            raise HTTPBadRequest(\n                \"Invalid Authorization header\",\n                \"The Authorization header for Basic auth should be in form:\\n\"\n                \"Authorization: Basic <base64-user-pass>\"\n            )\n\n        user_pass = auth[1]\n\n        try:\n            decoded = base64.b64decode(user_pass).decode()\n\n        except (TypeError, UnicodeDecodeError, binascii.Error):\n            raise HTTPBadRequest(\n                \"Invalid Authorization header\",\n                \"Credentials for Basic auth not correctly base64 encoded.\"\n            )\n\n        username, _, password = decoded.partition(\":\")\n        return username, password", "response": "Identify user using Basic auth."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef identify(self, req, resp, resource, uri_kwargs):\n        try:\n            return req.get_header('X-Api-Key', True)\n        except (KeyError, HTTPMissingHeader):\n            pass", "response": "Initialize X - Api - Key authentication middleware."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nidentify user using Authenticate header with Token auth.", "response": "def identify(self, req, resp, resource, uri_kwargs):\n        \"\"\"Identify user using Authenticate header with Token auth.\"\"\"\n        header = req.get_header('Authorization', False)\n        auth = header.split(' ') if header else None\n\n        if auth is None or auth[0].lower() != 'token':\n            return None\n\n        if len(auth) != 2:\n            raise HTTPBadRequest(\n                \"Invalid Authorization header\",\n                \"The Authorization header for Token auth should be in form:\\n\"\n                \"Authorization: Token <token_value>\"\n            )\n\n        return auth[1]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_client_address(self, req):\n        try:\n            forwarded_for = req.get_header('X-Forwarded-For', True)\n            return forwarded_for.split(',')[0].strip()\n        except (KeyError, HTTPMissingHeader):\n            return (\n                req.env.get('REMOTE_ADDR') if self.remote_address_fallback\n                else None\n            )", "response": "Get client address from X - Forwarded - For header or use remote address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_collection_mutating_webhook_configuration(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_mutating_webhook_configuration_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_mutating_webhook_configuration_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "Delete a collection of MutatingWebhookConfigurations"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_mutating_webhook_configuration(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_mutating_webhook_configuration_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_mutating_webhook_configuration_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "List or watch objects of kind MutatingWebhookConfiguration"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_mutating_webhook_configuration(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_mutating_webhook_configuration_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_mutating_webhook_configuration_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "read_mutating_webhook_configuration  # noqa: E501\n\n        read the specified MutatingWebhookConfiguration  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_mutating_webhook_configuration(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the MutatingWebhookConfiguration (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1beta1MutatingWebhookConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a namespaced lease.", "response": "def delete_namespaced_lease(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_lease  # noqa: E501\n\n        delete a Lease  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_lease(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Lease (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_lease_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_lease_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_lease_for_all_namespaces(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_lease_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_lease_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "List all namespaces in the lease."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a specific lease from the given namespace.", "response": "def read_namespaced_lease(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_lease  # noqa: E501\n\n        read the specified Lease  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_lease(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Lease (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1beta1Lease\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_lease_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_lease_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace_namespaced_lease(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_lease_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_lease_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a Lease with a new body."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a PodDisruptionBudget object in the specified namespace.", "response": "def create_namespaced_pod_disruption_budget(self, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"create_namespaced_pod_disruption_budget  # noqa: E501\n\n        create a PodDisruptionBudget  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_pod_disruption_budget(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1beta1PodDisruptionBudget body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1PodDisruptionBudget\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_disruption_budget_with_http_info(namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_namespaced_pod_disruption_budget_with_http_info(namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_collection_namespaced_pod_disruption_budget(self, namespace, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_namespaced_pod_disruption_budget_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_namespaced_pod_disruption_budget_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data", "response": "Delete all PodDisruptionBudgets for a namespace"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all PodDisruptionBudgets for the specified namespace.", "response": "def list_namespaced_pod_disruption_budget(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"list_namespaced_pod_disruption_budget  # noqa: E501\n\n        list or watch objects of kind PodDisruptionBudget  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_namespaced_pod_disruption_budget(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1beta1PodDisruptionBudgetList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_pod_disruption_budget_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_pod_disruption_budget_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npatches a PodDisruptionBudget in a namespace.", "response": "def patch_namespaced_pod_disruption_budget(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_pod_disruption_budget  # noqa: E501\n\n        partially update the specified PodDisruptionBudget  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_pod_disruption_budget(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodDisruptionBudget (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1PodDisruptionBudget\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_pod_disruption_budget_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_pod_disruption_budget_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef patch_namespaced_pod_disruption_budget_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_pod_disruption_budget_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_pod_disruption_budget_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the PodDisruptionBudget status for a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces the specified PodDisruptionBudget with the specified body.", "response": "def replace_namespaced_pod_disruption_budget(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"replace_namespaced_pod_disruption_budget  # noqa: E501\n\n        replace the specified PodDisruptionBudget  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_pod_disruption_budget(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodDisruptionBudget (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1beta1PodDisruptionBudget body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1PodDisruptionBudget\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_pod_disruption_budget_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_pod_disruption_budget_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_namespaced_pod_disruption_budget_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_pod_disruption_budget_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_pod_disruption_budget_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the status of a PodDisruptionBudget object in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a StorageClass. This method creates a StorageClass and returns the response.", "response": "def create_storage_class(self, body, **kwargs):  # noqa: E501\n        \"\"\"create_storage_class  # noqa: E501\n\n        create a StorageClass  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_storage_class(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1StorageClass body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1StorageClass\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_storage_class_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_storage_class_with_http_info(body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_collection_storage_class(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_storage_class_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_storage_class_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "Delete collection of StorageClass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_storage_class(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_storage_class_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_storage_class_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "delete_storage_class  # noqa: E501\n\n        delete a StorageClass  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_storage_class(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the StorageClass (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_volume_attachment(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_volume_attachment_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_volume_attachment_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "delete_volume_attachment  # noqa: E501\n\n        delete a VolumeAttachment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_volume_attachment(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the VolumeAttachment (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch_storage_class(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_storage_class_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_storage_class_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch an existing V1 Storage Class"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npatching an existing VolumeAttachment", "response": "def patch_volume_attachment(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"patch_volume_attachment  # noqa: E501\n\n        partially update the specified VolumeAttachment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_volume_attachment(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the VolumeAttachment (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1VolumeAttachment\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_volume_attachment_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_volume_attachment_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a specific volume attachment.", "response": "def read_volume_attachment(self, name, **kwargs):  # noqa: E501\n        \"\"\"read_volume_attachment  # noqa: E501\n\n        read the specified VolumeAttachment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_volume_attachment(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the VolumeAttachment (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1VolumeAttachment\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_volume_attachment_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_volume_attachment_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the status of a specific VolumeAttachment", "response": "def read_volume_attachment_status(self, name, **kwargs):  # noqa: E501\n        \"\"\"read_volume_attachment_status  # noqa: E501\n\n        read status of the specified VolumeAttachment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_volume_attachment_status(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the VolumeAttachment (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1VolumeAttachment\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_volume_attachment_status_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_volume_attachment_status_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces the status of a specific VolumeAttachment.", "response": "def replace_volume_attachment_status(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"replace_volume_attachment_status  # noqa: E501\n\n        replace status of the specified VolumeAttachment  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_volume_attachment_status(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the VolumeAttachment (required)\n        :param V1VolumeAttachment body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1VolumeAttachment\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_volume_attachment_status_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_volume_attachment_status_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning human readable description of the exception.", "response": "def _get_description(self):\n        \"\"\"Return human readable description error description.\n\n        This description should explain everything that went wrong during\n        deserialization.\n\n        \"\"\"\n        return \", \".join([\n            part for part in [\n                \"missing: {}\".format(self.missing) if self.missing else \"\",\n                (\n                    \"forbidden: {}\".format(self.forbidden)\n                    if self.forbidden else \"\"\n                ),\n                \"invalid: {}:\".format(self.invalid) if self.invalid else \"\",\n                (\n                    \"failed to parse: {}\".format(self.failed)\n                    if self.failed else \"\"\n                )\n            ] if part\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def load_kube_config(config_file=None, context=None,\n                           client_configuration=None,\n                           persist_config=True):\n    \"\"\"Loads authentication and cluster information from kube-config file\n    and stores them in kubernetes.client.configuration.\n\n    :param config_file: Name of the kube-config file.\n    :param context: set the active context. If is set to None, current_context\n        from config file will be used.\n    :param client_configuration: The kubernetes.client.Configuration to\n        set configs to.\n    :param persist_config: If True, config file will be updated when changed\n        (e.g GCP token refresh).\n    \"\"\"\n\n    if config_file is None:\n        config_file = KUBE_CONFIG_DEFAULT_LOCATION\n\n    loader = _get_kube_config_loader_for_yaml_file(\n        config_file, active_context=context,\n        persist_config=persist_config)\n    if client_configuration is None:\n        config = type.__call__(Configuration)\n        await loader.load_and_set(config)\n        Configuration.set_default(config)\n    else:\n        await loader.load_and_set(client_configuration)\n\n    return loader", "response": "Loads authentication and cluster information from kube - config file and stores them in kubernetes. client. configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload configuration and returns an ApiClient object.", "response": "async def new_client_from_config(config_file=None, context=None, persist_config=True):\n    \"\"\"Loads configuration the same as load_kube_config but returns an ApiClient\n    to be used with any API object. This will allow the caller to concurrently\n    talk with multiple clusters.\"\"\"\n    client_config = type.__call__(Configuration)\n\n    await load_kube_config(config_file=config_file, context=context,\n                           client_configuration=client_config,\n                           persist_config=persist_config)\n\n    return ApiClient(configuration=client_config)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the name of a file with base64 decoded obj[%data_key_name content otherwise raise a ConfigException", "response": "def as_file(self):\n        \"\"\"If obj[%data_key_name] exists, return name of a file with base64\n        decoded obj[%data_key_name] content otherwise obj[%file_key_name].\"\"\"\n        use_data_if_no_file = not self._file and self._data\n        if use_data_if_no_file:\n            if self._base64_file_content:\n                if isinstance(self._data, str):\n                    content = self._data.encode()\n                else:\n                    content = self._data\n                self._file = _create_temp_file_with_content(\n                    base64.standard_b64decode(content))\n            else:\n                self._file = _create_temp_file_with_content(self._data)\n        if self._file and not os.path.isfile(self._file):\n            raise ConfigException(\"File does not exists: %s\" % self._file)\n        return self._file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns obj[%data_key_name ] otherwise obj[%file_key_name ] base64 encoded string of file content.", "response": "def as_data(self):\n        \"\"\"If obj[%data_key_name] exists, Return obj[%data_key_name] otherwise\n        base64 encoded string of obj[%file_key_name] file content.\"\"\"\n        use_file_if_no_data = not self._data and self._file\n        if use_file_if_no_data:\n            with open(self._file) as f:\n                if self._base64_file_content:\n                    self._data = bytes.decode(\n                        base64.standard_b64encode(str.encode(f.read())))\n                else:\n                    self._data = f.read()\n        return self._data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading authentication from kube - config user section if exists.", "response": "async def _load_authentication(self):\n        \"\"\"Read authentication from kube-config user section if exists.\n\n        This function goes through various authentication methods in user\n        section of kube-config and stops if it finds a valid authentication\n        method. The order of authentication methods is:\n\n            1. GCP auth-provider\n            2. token field (point to a token file)\n            3. oidc auth-provider\n            4. exec provided plugin\n            5. username/password\n        \"\"\"\n\n        if not self._user:\n            logging.debug('No user section in current context.')\n            return\n\n        if self.provider == 'gcp':\n            await self.load_gcp_token()\n            return\n\n        if self.provider == PROVIDER_TYPE_OIDC:\n            await self._load_oid_token()\n            return\n\n        if 'exec' in self._user:\n            logging.debug('Try to use exec provider')\n            res_exec_plugin = await self._load_from_exec_plugin()\n            if res_exec_plugin:\n                return\n\n        logging.debug('Try to load user token')\n        if self._load_user_token():\n            return\n\n        logging.debug('Try to use username and password')\n        self._load_user_pass_token()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef min_validator(min_value):\n    def validator(value):\n        if value < min_value:\n            raise ValidationError(\"{} is not >= {}\".format(value, min_value))\n\n    return validator", "response": "Returns a validator function that ensures the minimum value of a resource\n    instance field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef max_validator(max_value):\n    def validator(value):\n        if value > max_value:\n            raise ValidationError(\"{} is not <= {}\".format(value, max_value))\n\n    return validator", "response": "Returns a validator function that ensures upper bound of a number."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a validator function that will check if value in choices.", "response": "def choices_validator(choices):\n    \"\"\"Return validator function that will check if ``value in choices``.\n\n    Args:\n        max_value (list, set, tuple): allowed choices for new validator\n\n    \"\"\"\n    def validator(value):\n        if value not in choices:\n            # note: make it a list for consistent representation\n            raise ValidationError(\n                \"{} is not in {}\".format(value, list(choices))\n            )\n\n    return validator"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns validator function that will check if given expression matches given object.", "response": "def match_validator(expression):\n    \"\"\"Return validator function that will check if matches given expression.\n\n    Args:\n        match: if string then this will be converted to regular expression\n           using ``re.compile``. Can be also any object that has ``match()``\n           method like already compiled regular regular expression or custom\n           matching object/class.\n\n    \"\"\"\n    if isinstance(expression, str):\n        compiled = re.compile(expression)\n    elif hasattr(expression, 'match'):\n        # check it early so we could say something is wrong early\n        compiled = expression\n    else:\n        raise TypeError(\n            'Provided match is nor a string nor has a match method '\n            '(like re expressions)'\n        )\n\n    def validator(value):\n        if not compiled.match(value):\n            # note: make it a list for consistent representation\n            raise ValidationError(\n                \"{} does not match pattern: {}\".format(\n                    value,\n                    compiled.pattern\n                    if hasattr(compiled, 'pattern')\n                    else compiled\n                )\n            )\n\n    return validator"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_certificate_signing_request(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_certificate_signing_request_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_certificate_signing_request_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "Read a specific certificate signing request."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces the status of a certificate signing request.", "response": "def replace_certificate_signing_request_status(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"replace_certificate_signing_request_status  # noqa: E501\n\n        replace status of the specified CertificateSigningRequest  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_certificate_signing_request_status(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CertificateSigningRequest (required)\n        :param V1beta1CertificateSigningRequest body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_certificate_signing_request_status_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_certificate_signing_request_status_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an API Service", "response": "def create_api_service(self, body, **kwargs):  # noqa: E501\n        \"\"\"create_api_service  # noqa: E501\n\n        create an APIService  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_api_service(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1beta1APIService body: (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1beta1APIService\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_api_service_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_api_service_with_http_info(body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch_api_service(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_api_service_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_api_service_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch an existing API Service."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_api_service(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_api_service_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_api_service_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "Read an API Service by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_api_service_status(self, name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_api_service_status_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_api_service_status_with_http_info(name, **kwargs)  # noqa: E501\n            return data", "response": "Read the status of the specified APIService"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_api_service(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_api_service_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_api_service_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace an API Service with a new one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validated_value(self, raw_value):\n        value = self.value(raw_value)\n        try:\n            for validator in self.validators:\n                validator(value)\n        except:\n            raise\n        else:\n            return value", "response": "Return parsed parameter value and run validation handlers."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndescribe this parameter instance for purpose of self - documentation.", "response": "def describe(self, **kwargs):\n        \"\"\"Describe this parameter instance for purpose of self-documentation.\n\n        Args:\n            kwargs (dict): dictionary of additional description items for\n                extending default description\n\n        Returns:\n            dict: dictionary of description items\n\n\n        Suggested way for overriding description fields or extending it with\n        additional items is calling super class method with new/overriden\n        fields passed as keyword arguments like following:\n\n        .. code-block:: python\n\n            class DummyParam(BaseParam):\n                def description(self, **kwargs):\n                    super().describe(is_dummy=True, **kwargs)\n\n        \"\"\"\n        description = {\n            'label': self.label,\n            # note: details are expected to be large so it should\n            #       be reformatted\n            'details': inspect.cleandoc(self.details),\n            'required': self.required,\n            'many': self.many,\n            'spec': self.spec,\n            'default': self.default,\n            'type': self.type or 'unspecified'\n        }\n\n        description.update(kwargs)\n        return description"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef value(self, raw_value):\n        try:\n            return base64.b64decode(bytes(raw_value, 'utf-8')).decode('utf-8')\n        except binascii.Error as err:\n            raise ValueError(str(err))", "response": "Decode param with Base64."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding param as decimal value.", "response": "def value(self, raw_value):\n        \"\"\"Decode param as decimal value.\"\"\"\n        try:\n            return decimal.Decimal(raw_value)\n        except decimal.InvalidOperation:\n            raise ValueError(\n                \"Could not parse '{}' value as decimal\".format(raw_value)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef value(self, raw_value):\n        if raw_value in self._FALSE_VALUES:\n            return False\n        elif raw_value in self._TRUE_VALUES:\n            return True\n        else:\n            raise ValueError(\n                \"Could not parse '{}' value as boolean\".format(raw_value)\n            )", "response": "Decode param as bool value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a namespaced Job", "response": "def delete_namespaced_job(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"delete_namespaced_job  # noqa: E501\n\n        delete a Job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_job(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Job (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_job_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_namespaced_job_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all namespaces in the cluster.", "response": "def list_job_for_all_namespaces(self, **kwargs):  # noqa: E501\n        \"\"\"list_job_for_all_namespaces  # noqa: E501\n\n        list or watch objects of kind Job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_job_for_all_namespaces(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1JobList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_job_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_job_for_all_namespaces_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists all the resources in the specified namespace.", "response": "def list_namespaced_job(self, namespace, **kwargs):  # noqa: E501\n        \"\"\"list_namespaced_job  # noqa: E501\n\n        list or watch objects of kind Job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.list_namespaced_job(namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1JobList\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_namespaced_job_with_http_info(namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.list_namespaced_job_with_http_info(namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npatching a Job with the given namespace with the given body.", "response": "def patch_namespaced_job(self, name, namespace, body, **kwargs):  # noqa: E501\n        \"\"\"patch_namespaced_job  # noqa: E501\n\n        partially update the specified Job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_job(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Job (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param UNKNOWN_BASE_TYPE body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1Job\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef patch_namespaced_job_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_namespaced_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch the status of a Job in the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a single Job from the given namespace.", "response": "def read_namespaced_job(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_job  # noqa: E501\n\n        read the specified Job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_job(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Job (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1Job\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_job_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_job_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the status of a Job in a namespace", "response": "def read_namespaced_job_status(self, name, namespace, **kwargs):  # noqa: E501\n        \"\"\"read_namespaced_job_status  # noqa: E501\n\n        read status of the specified Job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_job_status(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Job (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Job\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_job_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_namespaced_job_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace_namespaced_job(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_job_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace a Job with a new body."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace_namespaced_job_status(self, name, namespace, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_namespaced_job_status_with_http_info(name, namespace, body, **kwargs)  # noqa: E501\n            return data", "response": "Replace the status of a Job in a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting an initialization configuration.", "response": "def delete_initializer_configuration(self, name, **kwargs):  # noqa: E501\n        \"\"\"delete_initializer_configuration  # noqa: E501\n\n        delete an InitializerConfiguration  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_initializer_configuration(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the InitializerConfiguration (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :param V1DeleteOptions body:\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_initializer_configuration_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_initializer_configuration_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_initializer_configuration(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.list_initializer_configuration_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.list_initializer_configuration_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "List or watch objects of kind InitializerConfiguration"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_initializer_configuration(self, name, body, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_initializer_configuration_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.patch_initializer_configuration_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data", "response": "Patch an existing InitializerConfiguration."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads an initialization configuration.", "response": "def read_initializer_configuration(self, name, **kwargs):  # noqa: E501\n        \"\"\"read_initializer_configuration  # noqa: E501\n\n        read the specified InitializerConfiguration  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_initializer_configuration(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the InitializerConfiguration (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.\n        :return: V1alpha1InitializerConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_initializer_configuration_with_http_info(name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.read_initializer_configuration_with_http_info(name, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace an existing InitializerConfiguration in a specific resource.", "response": "def replace_initializer_configuration(self, name, body, **kwargs):  # noqa: E501\n        \"\"\"replace_initializer_configuration  # noqa: E501\n\n        replace the specified InitializerConfiguration  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_initializer_configuration(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the InitializerConfiguration (required)\n        :param V1alpha1InitializerConfiguration body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :return: V1alpha1InitializerConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_initializer_configuration_with_http_info(name, body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.replace_initializer_configuration_with_http_info(name, body, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the request of this V1beta1CertificateSigningRequestSpec.", "response": "def request(self, request):\n        \"\"\"Sets the request of this V1beta1CertificateSigningRequestSpec.\n\n        Base64-encoded PKCS#10 CSR data  # noqa: E501\n\n        :param request: The request of this V1beta1CertificateSigningRequestSpec.  # noqa: E501\n        :type: str\n        \"\"\"\n        if request is None:\n            raise ValueError(\"Invalid value for `request`, must not be `None`\")  # noqa: E501\n        if request is not None and not re.search(r'^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$', request):  # noqa: E501\n            raise ValueError(r\"Invalid value for `request`, must be a follow pattern or equal to `/^(?:[A-Za-z0-9+\\/]{4})*(?:[A-Za-z0-9+\\/]{2}==|[A-Za-z0-9+\\/]{3}=)?$/`\")  # noqa: E501\n\n        self._request = request"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting collection of AuditSinks", "response": "def delete_collection_audit_sink(self, **kwargs):  # noqa: E501\n        \"\"\"delete_collection_audit_sink  # noqa: E501\n\n        delete collection of AuditSink  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_collection_audit_sink(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param bool include_uninitialized: If true, partially initialized resources are included in the response.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \\\"next key\\\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\n        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.\n        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\n        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_collection_audit_sink_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_collection_audit_sink_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef project(dataIn, projectionScript):\n    '''Programs may make use of data in the `dataIn` variable and should\n    produce data on the `dataOut` variable.'''\n    # We don't really need to initialize it, but we do it to avoid linter errors\n    dataOut = {}\n    try:\n        projectionScript = str(projectionScript)\n        program = makeProgramFromString(projectionScript)\n        if PY3:\n            loc = {\n                'dataIn': dataIn,\n                'dataOut': dataOut\n            }\n            exec(program, {}, loc)\n            dataOut = loc['dataOut']\n        else:\n            exec(program)\n    except Exception as e:\n        glogger.error(\"Error while executing SPARQL projection\")\n        glogger.error(projectionScript)\n        glogger.error(\"Encountered exception: \")\n        glogger.error(e)\n        dataOut = {\n            'status': 'error',\n            'message': e.message\n        }\n    return dataOut", "response": "Executes a SPARQL projection and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_prov_graph(self):\n\n        try:\n            # Use git2prov to get prov on the repo\n            repo_prov = check_output(\n                ['node_modules/git2prov/bin/git2prov', 'https://github.com/{}/{}/'.format(self.user, self.repo),\n                 'PROV-O']).decode(\"utf-8\")\n            repo_prov = repo_prov[repo_prov.find('@'):]\n            # glogger.debug('Git2PROV output: {}'.format(repo_prov))\n            glogger.debug('Ingesting Git2PROV output into RDF graph')\n            with open('temp.prov.ttl', 'w') as temp_prov:\n                temp_prov.write(repo_prov)\n\n            self.prov_g.parse('temp.prov.ttl', format='turtle')\n        except Exception as e:\n            glogger.error(e)\n            glogger.error(\"Couldn't parse Git2PROV graph, continuing without repo PROV\")\n            pass\n\n        self.prov_g.add((self.agent, RDF.type, self.prov.Agent))\n        self.prov_g.add((self.entity_d, RDF.type, self.prov.Entity))\n        self.prov_g.add((self.activity, RDF.type, self.prov.Activity))\n\n        # entity_d\n        self.prov_g.add((self.entity_d, self.prov.wasGeneratedBy, self.activity))\n        self.prov_g.add((self.entity_d, self.prov.wasAttributedTo, self.agent))\n        # later: entity_d genereated at time (when we know the end time)\n\n        # activity\n        self.prov_g.add((self.activity, self.prov.wasAssociatedWith, self.agent))\n        self.prov_g.add((self.activity, self.prov.startedAtTime, Literal(datetime.now())))", "response": "Initialize the PROV graph with all we know at the start of the recording."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_used_entity(self, entity_uri):\n        entity_o = URIRef(entity_uri)\n        self.prov_g.add((entity_o, RDF.type, self.prov.Entity))\n        self.prov_g.add((self.activity, self.prov.used, entity_o))", "response": "Add the provided URI as a used entity by the logged activity"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinalizing prov recording with end time", "response": "def end_prov_graph(self):\n        \"\"\"\n        Finalize prov recording with end time\n        \"\"\"\n        endTime = Literal(datetime.now())\n        self.prov_g.add((self.entity_d, self.prov.generatedAtTime, endTime))\n        self.prov_g.add((self.activity, self.prov.endedAtTime, endTime))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog provenance graph so far", "response": "def log_prov_graph(self):\n        \"\"\"\n        Log provenance graph so far\n        \"\"\"\n        glogger.debug(\"Spec generation provenance graph:\")\n        glogger.debug(self.prov_g.serialize(format='turtle'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing provenance graph in the specified format", "response": "def serialize(self, format):\n        \"\"\"\n        Serialize provenance graph in the specified format\n        \"\"\"\n        if PY3:\n            return self.prov_g.serialize(format=format).decode('utf-8')\n        else:\n            return self.prov_g.serialize(format=format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the grlc cache", "response": "def init_cache():\n    '''\n    Initializes the grlc cache (json file)\n    '''\n    cache_obj = json.loads(\"{}\")\n    try:\n        with open(CACHE_NAME, 'r') as cache_file:\n            try:\n                cache_obj = json.load(cache_file)\n            except ValueError:\n                print(\"The cache file seems to be empty, starting with flushed cache\")\n    except IOError:\n        print(\"The cache file seems to be empty, starting with flushed cache\")\n\n    print(\"Loaded JSON cache\")\n\n    return cache_obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef guess_endpoint_uri(rq, gh_repo):\n    auth = (static.DEFAULT_ENDPOINT_USER, static.DEFAULT_ENDPOINT_PASSWORD)\n    if auth == ('none', 'none'):\n        auth = None\n\n    if has_request_context() and \"endpoint\" in request.args:\n        endpoint = request.args['endpoint']\n        glogger.info(\"Endpoint provided in request: \" + endpoint)\n        return endpoint, auth\n\n    # Decorator\n    try:\n        decorators = get_yaml_decorators(rq)\n        endpoint = decorators['endpoint']\n        auth = None\n        glogger.info(\"Decorator guessed endpoint: \" + endpoint)\n    except (TypeError, KeyError):\n        # File\n        try:\n            endpoint_content = gh_repo.getTextFor({'download_url': 'endpoint.txt'})\n            endpoint = endpoint_content.strip().splitlines()[0]\n            auth = None\n            glogger.info(\"File guessed endpoint: \" + endpoint)\n        # TODO: except all is really ugly\n        except:\n            # Default\n            endpoint = static.DEFAULT_ENDPOINT\n            auth = (static.DEFAULT_ENDPOINT_USER, static.DEFAULT_ENDPOINT_PASSWORD)\n            if auth == ('none', 'none'):\n                auth = None\n            glogger.warning(\"No endpoint specified, using default ({})\".format(endpoint))\n\n    return endpoint, auth", "response": "Guesses the endpoint URI from the request and gh_repo."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_parameters(rq, variables, endpoint, query_metadata, auth=None):\n\n    # variables = translateQuery(Query.parseString(rq, parseAll=True)).algebra['_vars']\n\n    ## Aggregates\n    internal_matcher = re.compile(\"__agg_\\d+__\")\n    ## Basil-style variables\n    variable_matcher = re.compile(\n        \"(?P<required>[_]{1,2})(?P<name>[^_]+)_?(?P<type>[a-zA-Z0-9]+)?_?(?P<userdefined>[a-zA-Z0-9]+)?.*$\")\n\n    parameters = {}\n    for v in variables:\n        if internal_matcher.match(v):\n            continue\n\n        match = variable_matcher.match(v)\n        # TODO: currently only one parameter per triple pattern is supported\n        if match:\n            vname = match.group('name')\n            vrequired = True if match.group('required') == '_' else False\n            vtype = 'string'\n            # All these can be None\n            vcodes = get_enumeration(rq, vname, endpoint, query_metadata, auth)\n            vdefault = get_defaults(rq, vname, query_metadata)\n            vlang = None\n            vdatatype = None\n            vformat = None\n\n            mtype = match.group('type')\n            muserdefined = match.group('userdefined')\n\n            if mtype in ['number', 'literal', 'string']:\n                vtype = mtype\n            elif mtype in ['iri']:  # TODO: proper form validation of input parameter uris\n                vtype = 'string'\n                vformat = 'iri'\n            elif mtype:\n                vtype = 'string'\n\n                if mtype in static.XSD_DATATYPES:\n                    vdatatype = 'xsd:{}'.format(mtype)\n                elif len(mtype) == 2:\n                    vlang = mtype\n                elif muserdefined:\n                    vdatatype = '{}:{}'.format(mtype, muserdefined)\n\n            parameters[vname] = {\n                'original': '?{}'.format(v),\n                'required': vrequired,\n                'name': vname,\n                'type': vtype\n            }\n\n            # Possibly None parameter attributes\n            if vcodes is not None:\n                parameters[vname]['enum'] = sorted(vcodes)\n            if vlang is not None:\n                parameters[vname]['lang'] = vlang\n            if vdatatype is not None:\n                parameters[vname]['datatype'] = vdatatype\n            if vformat is not None:\n                parameters[vname]['format'] = vformat\n            if vdefault is not None:\n                parameters[vname]['default'] = vdefault\n\n            glogger.info('Finished parsing the following parameters: {}'.format(parameters))\n\n    return parameters", "response": "Get the parameters from the query string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_defaults(rq, v, metadata):\n    glogger.debug(\"Metadata with defaults: {}\".format(metadata))\n    if 'defaults' not in metadata:\n        return None\n    defaultsDict = _getDictWithKey(v, metadata['defaults'])\n    if defaultsDict:\n        return defaultsDict[v]\n    return None", "response": "Returns the default value for a parameter or None if no default value is set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_enumeration(rq, v, endpoint, metadata={}, auth=None):\n    # glogger.debug(\"Metadata before processing enums: {}\".format(metadata))\n    # We only fire the enum filling queries if indicated by the query metadata\n    if 'enumerate' not in metadata:\n        return None\n    enumDict = _getDictWithKey(v, metadata['enumerate'])\n    if enumDict:\n        return enumDict[v]\n    if v in metadata['enumerate']:\n        return get_enumeration_sparql(rq, v, endpoint, auth)\n    return None", "response": "Returns a list of enumerated values for variable v in query rq"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of enumerated values for a variable v in query rq", "response": "def get_enumeration_sparql(rq, v, endpoint, auth=None):\n    \"\"\"\n    Returns a list of enumerated values for variable 'v' in query 'rq'\n    \"\"\"\n    glogger.info('Retrieving enumeration for variable {}'.format(v))\n    vcodes = []\n    # tpattern_matcher = re.compile(\".*(FROM\\s+)?(?P<gnames>.*)\\s+WHERE.*[\\.\\{][\\n\\t\\s]*(?P<tpattern>.*\\?\" + re.escape(v) + \".*?\\.).*\", flags=re.DOTALL)\n    # tpattern_matcher = re.compile(\".*?((FROM\\s*)(?P<gnames>(\\<.*\\>)+))?\\s*WHERE\\s*\\{(?P<tpattern>.*)\\}.*\", flags=re.DOTALL)\n\n    # WHERE is optional too!!\n    tpattern_matcher = re.compile(\".*?(FROM\\s*(?P<gnames>\\<.*\\>+))?\\s*(WHERE\\s*)?\\{(?P<tpattern>.*)\\}.*\",\n                                  flags=re.DOTALL)\n\n    glogger.debug(rq)\n    tp_match = tpattern_matcher.match(rq)\n    if tp_match:\n        vtpattern = tp_match.group('tpattern')\n        gnames = tp_match.group('gnames')\n        glogger.debug(\"Detected graph names: {}\".format(gnames))\n        glogger.debug(\"Detected BGP: {}\".format(vtpattern))\n        glogger.debug(\"Matched triple pattern with parameter\")\n        if gnames:\n            codes_subquery = re.sub(\"SELECT.*\\{.*\\}.*\",\n                                    \"SELECT DISTINCT ?\" + v + \" FROM \" + gnames + \" WHERE { \" + vtpattern + \" }\", rq,\n                                    flags=re.DOTALL)\n        else:\n            codes_subquery = re.sub(\"SELECT.*\\{.*\\}.*\",\n                                    \"SELECT DISTINCT ?\" + v + \" WHERE { \" + vtpattern + \" }\", rq,\n                                    flags=re.DOTALL)\n        glogger.debug(\"Codes subquery: {}\".format(codes_subquery))\n        glogger.debug(endpoint)\n        codes_json = requests.get(endpoint, params={'query': codes_subquery},\n                                  headers={'Accept': static.mimetypes['json'],\n                                           'Authorization': 'token {}'.format(static.ACCESS_TOKEN)}, auth=auth).json()\n        for code in codes_json['results']['bindings']:\n            vcodes.append(list(code.values())[0][\"value\"])\n    else:\n        glogger.debug(\"No match between variable name and query.\")\n\n    return vcodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the yaml decorator metadata only", "response": "def get_yaml_decorators(rq):\n    \"\"\"\n    Returns the yaml decorator metadata only (this is needed by triple pattern fragments)\n    \"\"\"\n    # glogger.debug('Guessing decorators for query {}'.format(rq))\n    if not rq:\n        return None\n\n    if isinstance(rq, dict) and 'grlc' in rq:  # json query (sparql transformer)\n        yaml_string = rq['grlc']\n        query_string = rq\n    else:  # classic query\n        yaml_string = \"\\n\".join([row.lstrip('#+') for row in rq.split('\\n') if row.startswith('#+')])\n        query_string = \"\\n\".join([row for row in rq.split('\\n') if not row.startswith('#+')])\n\n    query_metadata = None\n    if type(yaml_string) == dict:\n        query_metadata = yaml_string\n    elif type(yaml_string) == str:\n        try:  # Invalid YAMLs will produce empty metadata\n            query_metadata = yaml.load(yaml_string)\n        except (yaml.parser.ParserError, yaml.scanner.ScannerError) as e:\n            try:\n                query_metadata = json.loads(yaml_string)\n            except json.JSONDecodeError:\n                glogger.warning(\"Query decorators could not be parsed; check your YAML syntax\")\n\n    # If there is no YAML string\n    if query_metadata is None:\n        query_metadata = {}\n    query_metadata['query'] = query_string\n\n    # glogger.debug(\"Parsed query decorators: {}\".format(query_metadata))\n\n    return query_metadata"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the metadata from the raw query file rq", "response": "def get_metadata(rq, endpoint):\n    \"\"\"\n    Returns the metadata 'exp' parsed from the raw query file 'rq'\n    'exp' is one of: 'endpoint', 'tags', 'summary', 'request', 'pagination', 'enumerate'\n    \"\"\"\n    query_metadata = get_yaml_decorators(rq)\n    query_metadata['type'] = 'UNKNOWN'\n    query_metadata['original_query'] = rq\n\n    if isinstance(rq, dict):  # json query (sparql transformer)\n        rq, proto, opt = SPARQLTransformer.pre_process(rq)\n        rq = rq.strip()\n        query_metadata['proto'] = proto\n        query_metadata['opt'] = opt\n        query_metadata['query'] = rq\n\n    rq = enable_custom_function_prefix(rq, 'bif')\n    rq = enable_custom_function_prefix(rq, 'sql')\n\n    try:\n        # THE PARSING\n        # select, describe, construct, ask\n        parsed_query = translateQuery(Query.parseString(rq, parseAll=True))\n        query_metadata['type'] = parsed_query.algebra.name\n        if query_metadata['type'] == 'SelectQuery':\n            # Projection variables\n            query_metadata['variables'] = parsed_query.algebra['PV']\n            # Parameters\n            query_metadata['parameters'] = get_parameters(rq, parsed_query.algebra['_vars'], endpoint, query_metadata)\n        elif query_metadata['type'] == 'ConstructQuery':\n            # Parameters\n            query_metadata['parameters'] = get_parameters(rq, parsed_query.algebra['_vars'], endpoint, query_metadata)\n        else:\n            glogger.warning(\n                \"Query type {} is currently unsupported and no metadata was parsed!\".format(query_metadata['type']))\n    except ParseException as pe:\n        glogger.warning(pe)\n        glogger.warning(\"Could not parse regular SELECT, CONSTRUCT, DESCRIBE or ASK query\")\n        # glogger.warning(traceback.print_exc())\n\n        # insert queries won't parse, so we regex\n        # glogger.info(\"Trying to parse INSERT query\")\n        # if static.INSERT_PATTERN in rq:\n        #     query_metadata['type'] = 'InsertQuery'\n        #     query_metadata['parameters'] = [u'_g_iri']\n\n        try:\n            # update query\n            glogger.info(\"Trying to parse UPDATE query\")\n            parsed_query = UpdateUnit.parseString(rq, parseAll=True)\n            glogger.info(parsed_query)\n            query_metadata['type'] = parsed_query[0]['request'][0].name\n            if query_metadata['type'] == 'InsertData':\n                query_metadata['parameters'] = {\n                    'g': {'datatype': None, 'enum': [], 'lang': None, 'name': 'g', 'original': '?_g_iri',\n                          'required': True, 'type': 'iri'},\n                    'data': {'datatype': None, 'enum': [], 'lang': None, 'name': 'data', 'original': '?_data',\n                             'required': True, 'type': 'literal'}}\n\n            glogger.info(\"Update query parsed with {}\".format(query_metadata['type']))\n            # if query_metadata['type'] == 'InsertData':\n            #     query_metadata['variables'] = parsed_query.algebra['PV']\n        except:\n            glogger.error(\"Could not parse query\")\n            glogger.error(query_metadata['query'])\n            glogger.error(traceback.print_exc())\n            pass\n\n    glogger.debug(\"Finished parsing query of type {}\".format(query_metadata['type']))\n    glogger.debug(\"All parsed query metadata (from decorators and content): \")\n    glogger.debug(pformat(query_metadata, indent=32))\n\n    return query_metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the projection for a given query name.", "response": "def getProjectionForQueryName(self, query_name):\n        \"\"\" TODO: DOCUMENT !!\n        Returns None if no such projection exists\n        \"\"\"\n        projectionFileName = query_name + '.pyql'\n        projectionText = self._getText(projectionFileName)\n        return projectionText"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of file items contained on the local repo.", "response": "def fetchFiles(self):\n        \"\"\"Returns a list of file items contained on the local repo.\"\"\"\n        print(\"Fetching files from {}\".format(self.baseDir))\n        files = glob(path.join(self.baseDir, '*'))\n        filesDef = []\n        for f in files:\n            print(\"Found SPARQL file {}\".format(f))\n            relative = f.replace(self.baseDir, '')\n            filesDef.append({\n                'download_url': relative,\n                'name': relative\n            })\n        return filesDef"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the swagger information from the repo being used.", "response": "def get_repo_info(loader, sha, prov_g):\n    \"\"\"Generate swagger information from the repo being used.\"\"\"\n    user_repo = loader.getFullName()\n    repo_title = loader.getRepoTitle()\n    contact_name = loader.getContactName()\n    contact_url = loader.getContactUrl()\n    commit_list = loader.getCommitList()\n    licence_url = loader.getLicenceURL()\n\n    # Add the API URI as a used entity by the activity\n    if prov_g:\n        prov_g.add_used_entity(loader.getRepoURI())\n\n    prev_commit = None\n    next_commit = None\n    version = sha if sha else commit_list[0]\n    if commit_list.index(version) < len(commit_list) - 1:\n        prev_commit = commit_list[commit_list.index(version) + 1]\n    if commit_list.index(version) > 0:\n        next_commit = commit_list[commit_list.index(version) - 1]\n\n    info = {\n        'version': version,\n        'title': repo_title,\n        'contact': {\n            'name': contact_name,\n            'url': contact_url\n        },\n        'license': {\n            'name': 'License',\n            'url': licence_url\n        }\n    }\n\n    basePath = '/api/' + user_repo + '/'\n    basePath += ('commit/' + sha + '/') if sha else ''\n\n    return prev_commit, next_commit, info, basePath"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_spec(user, repo, sha=None, prov=None, extraMetadata=[]):\n    loader = grlc.utils.getLoader(user, repo, sha=sha, prov=prov)\n\n    files = loader.fetchFiles()\n    raw_repo_uri = loader.getRawRepoUri()\n\n    # Fetch all .rq files\n    items = []\n\n    allowed_ext = [\"rq\", \"sparql\", \"json\", \"tpf\"]\n    for c in files:\n        glogger.debug('>>>>>>>>>>>>>>>>>>>>>>>>>c_name: {}'.format(c['name']))\n        extension = c['name'].split('.')[-1]\n        if extension in allowed_ext:\n            call_name = c['name'].split('.')[0]\n\n            # Retrieve extra metadata from the query decorators\n            query_text = loader.getTextFor(c)\n\n            item = None\n            if extension == \"json\":\n                query_text = json.loads(query_text)\n\n            if extension in [\"rq\", \"sparql\", \"json\"]:\n                glogger.debug(\"===================================================================\")\n                glogger.debug(\"Processing SPARQL query: {}\".format(c['name']))\n                glogger.debug(\"===================================================================\")\n                item = process_sparql_query_text(query_text, loader, call_name, extraMetadata)\n            elif \"tpf\" == extension:\n                glogger.debug(\"===================================================================\")\n                glogger.debug(\"Processing TPF query: {}\".format(c['name']))\n                glogger.debug(\"===================================================================\")\n                item = process_tpf_query_text(query_text, raw_repo_uri, call_name, extraMetadata)\n            else:\n                glogger.info(\"Ignoring unsupported source call name: {}\".format(c['name']))\n\n            if item:\n                items.append(item)\n    return items", "response": "Builds a grlc specification for the given github user / repo."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getResponseText(endpoint, query, requestedMimeType):\n    '''\n    endpoint - URL of sparql endpoint\n    query    - SPARQL query to be executed\n    requestedMimeType  Type of content requested. can be:\n                'text/csv; q=1.0, */*; q=0.1'\n                'application/json'\n                etc.\n\n    Returns result + mimetype\n    '''\n    retFormat = _mimeTypeToSparqlFormat(requestedMimeType)\n\n    client = SPARQLWrapper(endpoint)\n    client.setQuery(query)\n    client.setReturnFormat(retFormat)\n    client.setCredentials(static.DEFAULT_ENDPOINT_USER, static.DEFAULT_ENDPOINT_PASSWORD)\n    result = client.queryAndConvert()\n\n    if retFormat==JSON:\n        result = jsonify(result)\n\n    return result, MIME_FORMAT[retFormat]", "response": "Returns the response text of the query"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getLoader(user, repo, sha=None, prov=None):\n    if user is None and repo is None:\n        loader = LocalLoader()\n    else:\n        loader = GithubLoader(user, repo, sha, prov)\n    return loader", "response": "Build a fileLoader for the given user repository and sha."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_swagger_spec(user, repo, sha, serverName):\n    if user and repo:\n        # Init provenance recording\n        prov_g = grlcPROV(user, repo)\n    else:\n        prov_g = None\n\n    swag = swagger.get_blank_spec()\n    swag['host'] = serverName\n\n    try:\n        loader = getLoader(user, repo, sha, prov_g)\n    except Exception as e:\n        # If repo does not exits\n        swag['info'] = {\n            'title': 'ERROR!',\n            'description': str(e)\n        }\n        swag['paths'] = {}\n        return swag\n\n    prev_commit, next_commit, info, basePath = \\\n        swagger.get_repo_info(loader, sha, prov_g)\n    swag['prev_commit'] = prev_commit\n    swag['next_commit'] = next_commit\n    swag['info'] = info\n    swag['basePath'] = basePath\n\n    # TODO: can we pass loader to build_spec ?\n    spec = swagger.build_spec(user, repo, sha, prov_g)\n    for item in spec:\n        swag['paths'][item['call_name']] = swagger.get_path_for_item(item)\n\n    if prov_g:\n        prov_g.end_prov_graph()\n        swag['prov'] = prov_g.serialize(format='turtle')\n    return swag", "response": "Build a grlc specification for the given github user / repo and sha."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef buildPaginationHeader(resultCount, resultsPerPage, pageArg, url):\n    '''Build link header for result pagination'''\n    lastPage = resultCount / resultsPerPage\n\n    if pageArg:\n        page = int(pageArg)\n        next_url = re.sub(\"page=[0-9]+\", \"page={}\".format(page + 1), url)\n        prev_url = re.sub(\"page=[0-9]+\", \"page={}\".format(page - 1), url)\n        first_url = re.sub(\"page=[0-9]+\", \"page=1\", url)\n        last_url = re.sub(\"page=[0-9]+\", \"page={}\".format(lastPage), url)\n    else:\n        page = 1\n        next_url = url + \"?page=2\"\n        prev_url = \"\"\n        first_url = url + \"?page=1\"\n        last_url = url + \"?page={}\".format(lastPage)\n\n    if page == 1:\n        headerLink = \"<{}>; rel=next, <{}>; rel=last\".format(next_url, last_url)\n    elif page == lastPage:\n        headerLink = \"<{}>; rel=prev, <{}>; rel=first\".format(prev_url, first_url)\n    else:\n        headerLink = \"<{}>; rel=next, <{}>; rel=prev, <{}>; rel=first, <{}>; rel=last\".format(next_url, prev_url, first_url, last_url)\n    return headerLink", "response": "Build link header for result pagination"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_directive(module, package=None):\n    # type: (unicode, unicode) -> unicode\n    \"\"\"Create the automodule directive and add the options.\"\"\"\n    directive = '.. automodule:: %s\\n' % makename(package, module)\n    for option in OPTIONS:\n        directive += '    :%s:\\n' % option\n    return directive", "response": "Create the automodule directive and add the options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating RST for a top - level module.", "response": "def create_module_file(package, module, opts):\n    # type: (unicode, unicode, Any) -> None\n    \"\"\"Generate RST for a top-level module (i.e., not part of a package)\"\"\"\n    if not opts.noheadings:\n        text = format_heading(1, '%s module' % module)\n    else:\n        text = ''\n    # text += format_heading(2, ':mod:`%s` Module' % module)\n    text += format_directive(module, package)\n\n    if opts.templates:\n        template_loader = FileSystemLoader(opts.templates)\n        template_env = SandboxedEnvironment(loader=template_loader)\n        try:\n            mod_ns = _get_mod_ns(\n                name=module, fullname=module,\n                includeprivate=opts.includeprivate)\n            template = template_env.get_template('module.rst')\n            text = template.render(**mod_ns)\n        except ImportError as e:\n            _warn('failed to import %r: %s' % (module, e))\n        add_get_members_to_template_env(template_env, module, opts)\n    write_file(makename(package, module), text, opts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget public and private members of the module or package mod.", "response": "def _get_members(\n        mod, typ=None, include_imported=False, out_format='names',\n        in_list=None, known_refs=None):\n    \"\"\"Get (filtered) public/total members of the module or package `mod`.\n\n\n    Returns:\n        lists `public` and `items`. The lists contains the public and private +\n        public members, as strings.\n    \"\"\"\n    roles = {'function': 'func', 'module': 'mod', 'class': 'class',\n             'exception': 'exc', 'data': 'data'}\n    # not included, because they cannot occur at modul level:\n    #   'method': 'meth', 'attribute': 'attr', 'instanceattribute': 'attr'\n\n    out_formats = ['names', 'fullnames', 'refs', 'table']\n    if out_format not in out_formats:\n        raise ValueError(\"out_format %s not in %r\" % (out_format, out_formats))\n\n    def check_typ(typ, mod, member):\n        \"\"\"Check if mod.member is of the desired typ\"\"\"\n        if inspect.ismodule(member):\n            return False\n        documenter = _get_documenter(APP, member, mod)\n        if typ is None:\n            return True\n        if typ == getattr(documenter, 'objtype', None):\n            return True\n        if hasattr(documenter, 'directivetype'):\n            return roles[typ] == getattr(documenter, 'directivetype')\n\n    def is_local(mod, member, name):\n        \"\"\"Check whether mod.member is defined locally in module mod\"\"\"\n        if hasattr(member, '__module__'):\n            return getattr(member, '__module__') == mod.__name__\n        else:\n            # we take missing __module__ to mean the member is a data object\n            # it is recommended to filter data by e.g. __all__\n            return True\n\n    if typ is not None and typ not in roles:\n        raise ValueError(\"typ must be None or one of %s\"\n                         % str(list(roles.keys())))\n    items = []\n    public = []\n    if known_refs is None:\n        known_refs = {}\n    elif isinstance(known_refs, str):\n        known_refs = getattr(mod, known_refs)\n    if in_list is not None:\n        try:\n            in_list = getattr(mod, in_list)\n        except AttributeError:\n            in_list = []\n    for name in dir(mod):\n        if name.startswith('__'):\n            continue\n        try:\n            member = safe_getattr(mod, name)\n        except AttributeError:\n            continue\n        if check_typ(typ, mod, member):\n            if in_list is not None:\n                if name not in in_list:\n                    continue\n            if not (include_imported or is_local(mod, member, name)):\n                continue\n            if out_format in ['table', 'refs']:\n                documenter = _get_documenter(APP, member, mod)\n                role = roles.get(documenter.objtype, 'obj')\n                ref = _get_member_ref_str(\n                        name, obj=member, role=role,\n                        known_refs=known_refs)\n            if out_format == 'table':\n                docsummary = extract_summary(member)\n                items.append((ref, docsummary))\n                if not name.startswith('_'):\n                    public.append((ref, docsummary))\n            elif out_format == 'refs':\n                items.append(ref)\n                if not name.startswith('_'):\n                    public.append(ref)\n            elif out_format == 'fullnames':\n                fullname = _get_fullname(name, obj=member)\n                items.append(fullname)\n                if not name.startswith('_'):\n                    public.append(fullname)\n            else:\n                assert out_format == 'names', str(out_format)\n                items.append(name)\n                if not name.startswith('_'):\n                    public.append(name)\n    if out_format == 'table':\n        return _assemble_table(public), _assemble_table(items)\n    else:\n        return public, items"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_summary(obj):\n    # type: (List[unicode], Any) -> unicode\n    \"\"\"Extract summary from docstring.\"\"\"\n\n    try:\n        doc = inspect.getdoc(obj).split(\"\\n\")\n    except AttributeError:\n        doc = ''\n\n    # Skip a blank lines at the top\n    while doc and not doc[0].strip():\n        doc.pop(0)\n\n    # If there's a blank line, then we can assume the first sentence /\n    # paragraph has ended, so anything after shouldn't be part of the\n    # summary\n    for i, piece in enumerate(doc):\n        if not piece.strip():\n            doc = doc[:i]\n            break\n\n    # Try to find the \"first sentence\", which may span multiple lines\n    sentences = periods_re.split(\" \".join(doc))  # type: ignore\n    if len(sentences) == 1:\n        summary = sentences[0].strip()\n    else:\n        summary = ''\n        state_machine = RSTStateMachine(state_classes, 'Body')\n        while sentences:\n            summary += sentences.pop(0) + '.'\n            node = new_document('')\n            node.reporter = NullReporter('', 999, 4)\n            node.settings.pep_references = None\n            node.settings.rfc_references = None\n            state_machine.run([summary], node)\n            if not node.traverse(nodes.system_message):\n                # considered as that splitting by period does not break inline\n                # markups\n                break\n\n    return summary", "response": "Extract summary from docstring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a ReST - formmated reference link to the given object of type role using the name as the link text", "response": "def _get_member_ref_str(name, obj, role='obj', known_refs=None):\n    \"\"\"generate a ReST-formmated reference link to the given `obj` of type\n    `role`, using `name` as the link text\"\"\"\n    if known_refs is not None:\n        if name in known_refs:\n            return known_refs[name]\n    ref = _get_fullname(name, obj)\n    return \":%s:`%s <%s>`\" % (role, name, ref)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the template context of module identified by name as a dict", "response": "def _get_mod_ns(name, fullname, includeprivate):\n    \"\"\"Return the template context of module identified by `fullname` as a\n    dict\"\"\"\n    ns = {  # template variables\n        'name': name, 'fullname': fullname, 'members': [], 'functions': [],\n        'classes': [], 'exceptions': [], 'subpackages': [], 'submodules': [],\n        'doc': None}\n    p = 0\n    if includeprivate:\n        p = 1\n    mod = importlib.import_module(fullname)\n    ns['members'] = _get_members(mod)[p]\n    ns['functions'] = _get_members(mod, typ='function')[p]\n    ns['classes'] = _get_members(mod, typ='class')[p]\n    ns['exceptions'] = _get_members(mod, typ='exception')[p]\n    ns['data'] = _get_members(mod, typ='data')[p]\n    ns['doc'] = mod.__doc__\n    return ns"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the text of the file and write it to the file.", "response": "def create_package_file(root, master_package, subroot, py_files, opts, subs, is_namespace):\n    # type: (unicode, unicode, unicode, List[unicode], Any, List[unicode], bool) -> None\n    \"\"\"Build the text of the file and write the file.\"\"\"\n\n    use_templates = False\n    fullname = makename(master_package, subroot)\n\n    if opts.templates:\n        use_templates = True\n        template_loader = FileSystemLoader(opts.templates)\n        template_env = SandboxedEnvironment(loader=template_loader)\n\n    text = format_heading(\n        1, ('%s package' if not is_namespace else \"%s namespace\") % fullname)\n\n    if opts.modulefirst and not is_namespace:\n        text += format_directive(subroot, master_package)\n        text += '\\n'\n\n    # build a list of directories that are szvpackages (contain an INITPY file)\n    subs = [sub for sub in subs if path.isfile(path.join(root, sub, INITPY))]\n    # if there are some package directories, add a TOC for theses subpackages\n    if subs:\n        text += format_heading(2, 'Subpackages')\n        text += '.. toctree::\\n\\n'\n        for sub in subs:\n            text += '    %s.%s\\n' % (makename(master_package, subroot), sub)\n        text += '\\n'\n\n    submods = [path.splitext(sub)[0] for sub in py_files\n               if not shall_skip(path.join(root, sub), opts) and\n               sub != INITPY]\n\n    if use_templates:\n        try:\n            package_ns = _get_mod_ns(name=subroot, fullname=fullname,\n                                     includeprivate=opts.includeprivate)\n            package_ns['subpackages'] = subs\n            package_ns['submodules'] = submods\n        except ImportError as e:\n            _warn('failed to import %r: %s' % (fullname, e))\n\n    if submods:\n        text += format_heading(2, 'Submodules')\n        if opts.separatemodules:\n            text += '.. toctree::\\n\\n'\n            for submod in submods:\n                modfile = makename(master_package, makename(subroot, submod))\n                text += '   %s\\n' % modfile\n\n                # generate separate file for this module\n                if not opts.noheadings:\n                    filetext = format_heading(1, '%s module' % modfile)\n                else:\n                    filetext = ''\n                filetext += format_directive(makename(subroot, submod),\n                                             master_package)\n                if use_templates:\n                    try:\n                        mod_ns = _get_mod_ns(\n                            name=submod, fullname=modfile,\n                            includeprivate=opts.includeprivate)\n                        template = template_env.get_template('module.rst')\n                        add_get_members_to_template_env(\n                            template_env, modfile, opts)\n                        filetext = template.render(**mod_ns)\n                    except ImportError as e:\n                        _warn('failed to import %r: %s' % (modfile, e))\n                write_file(modfile, filetext, opts)\n        else:\n            for submod in submods:\n                modfile = makename(master_package, makename(subroot, submod))\n                if not opts.noheadings:\n                    text += format_heading(2, '%s module' % modfile)\n                text += format_directive(makename(subroot, submod),\n                                         master_package)\n                text += '\\n'\n        text += '\\n'\n\n    if use_templates:\n        template = template_env.get_template('package.rst')\n        add_get_members_to_template_env(template_env, fullname, opts)\n        text = template.render(**package_ns)\n    else:\n        if not opts.modulefirst and not is_namespace:\n            text += format_heading(2, 'Module contents')\n            text += format_directive(subroot, master_package)\n\n    write_file(makename(master_package, subroot), text, opts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if we want to skip this module.", "response": "def shall_skip(module, opts):\n    # type: (unicode, Any) -> bool\n    \"\"\"Check if we want to skip this module.\"\"\"\n    # skip if the file doesn't exist and not using implicit namespaces\n    if not opts.implicit_namespaces and not path.exists(module):\n        return True\n\n    # skip it if there is nothing (or just \\n or \\r\\n) in the file\n    if path.exists(module) and path.getsize(module) <= 2:\n        return True\n\n    # skip if it has a \"private\" name and this is selected\n    filename = path.basename(module)\n    if filename != '__init__.py' and filename.startswith('_') and \\\n       not opts.includeprivate:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_excluded(root, excludes):\n    # type: (unicode, List[unicode]) -> bool\n    \"\"\"Check if the directory is in the exclude list.\n\n    Note: by having trailing slashes, we avoid common prefix issues, like\n          e.g. an exlude \"foo\" also accidentally excluding \"foobar\".\n    \"\"\"\n    for exclude in excludes:\n        if fnmatch(root, exclude):\n            return True\n    return False", "response": "Check if the directory is in the exclude list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and check the command line arguments and run the main function.", "response": "def main(argv=sys.argv):\n    # type: (List[str]) -> int\n    \"\"\"Parse and check the command line arguments.\"\"\"\n    parser = optparse.OptionParser(\n        usage=\"\"\"\\\nusage: %prog [options] -o <output_path> <module_path> [exclude_pattern, ...]\n\nLook recursively in <module_path> for Python modules and packages and create\none reST file with automodule directives per package in the <output_path>.\n\nThe <exclude_pattern>s can be file and/or directory patterns that will be\nexcluded from generation.\n\nNote: By default this script will not overwrite already created files.\"\"\")\n\n    parser.add_option('-o', '--output-dir', action='store', dest='destdir',\n                      help='Directory to place all output', default='')\n    parser.add_option('-d', '--maxdepth', action='store', dest='maxdepth',\n                      help='Maximum depth of submodules to show in the TOC '\n                      '(default: 4)', type='int', default=4)\n    parser.add_option('-f', '--force', action='store_true', dest='force',\n                      help='Overwrite existing files')\n    parser.add_option('-l', '--follow-links', action='store_true',\n                      dest='followlinks', default=False,\n                      help='Follow symbolic links. Powerful when combined '\n                      'with collective.recipe.omelette.')\n    parser.add_option('-n', '--dry-run', action='store_true', dest='dryrun',\n                      help='Run the script without creating files')\n    parser.add_option('-e', '--separate', action='store_true',\n                      dest='separatemodules',\n                      help='Put documentation for each module on its own page')\n    parser.add_option('-P', '--private', action='store_true',\n                      dest='includeprivate',\n                      help='Include \"_private\" modules')\n    parser.add_option('-T', '--no-toc', action='store_true', dest='notoc',\n                      help='Don\\'t create a table of contents file')\n    parser.add_option('-E', '--no-headings', action='store_true',\n                      dest='noheadings',\n                      help='Don\\'t create headings for the module/package '\n                           'packages (e.g. when the docstrings already contain '\n                           'them). No effect in combination with -t')\n    parser.add_option('-M', '--module-first', action='store_true',\n                      dest='modulefirst',\n                      help='Put module documentation before submodule '\n                      'documentation (no effect in combination with -t)')\n    parser.add_option('--implicit-namespaces', action='store_true',\n                      dest='implicit_namespaces',\n                      help='Interpret module paths according to PEP-0420 '\n                           'implicit namespaces specification')\n    parser.add_option('-s', '--suffix', action='store', dest='suffix',\n                      help='file suffix (default: rst)', default='rst')\n    parser.add_option('-F', '--full', action='store_true', dest='full',\n                      help='Generate a full project with sphinx-quickstart')\n    parser.add_option('-a', '--append-syspath', action='store_true',\n                      dest='append_syspath',\n                      help='Append module_path to sys.path, used when --full is given')\n    parser.add_option(\"-t\", \"--templates\", action=\"store\", type=\"string\",\n                      dest=\"templates\", default=None,\n                      help=\"Custom template directory (default: %default). \"\n                      \"Must contain template files package.rst and/or \"\n                      \"module.rst\")\n    parser.add_option('-H', '--doc-project', action='store', dest='header',\n                      help='Project name (default: root module name)')\n    parser.add_option('-A', '--doc-author', action='store', dest='author',\n                      type='str',\n                      help='Project author(s), used when --full is given')\n    parser.add_option('-V', '--doc-version', action='store', dest='version',\n                      help='Project version, used when --full is given')\n    parser.add_option('-R', '--doc-release', action='store', dest='release',\n                      help='Project release, used when --full is given, '\n                      'defaults to --doc-version')\n    parser.add_option('--version', action='store_true', dest='show_version',\n                      help='Show version information and exit')\n    group = parser.add_option_group('Extension options')\n    for ext in EXTENSIONS:\n        group.add_option('--ext-' + ext, action='store_true',\n                         dest='ext_' + ext, default=False,\n                         help='enable %s extension' % ext)\n\n    (opts, args) = parser.parse_args(argv[1:])\n\n    if opts.show_version:\n        #print('Sphinx (sphinx-apidoc) %s' % __display_version__)\n        print('better-apidoc %s' % __display_version__)\n        return 0\n\n    if not args:\n        parser.error('A package path is required.')\n\n    rootpath, excludes = args[0], args[1:]\n    if not opts.destdir:\n        parser.error('An output directory is required.')\n    if opts.header is None:\n        opts.header = path.abspath(rootpath).split(path.sep)[-1]\n    if opts.suffix.startswith('.'):\n        opts.suffix = opts.suffix[1:]\n    if not path.isdir(rootpath):\n        print('%s is not a directory.' % rootpath, file=sys.stderr)\n        sys.exit(1)\n    if not path.isdir(opts.destdir):\n        if not opts.dryrun:\n            os.makedirs(opts.destdir)\n    rootpath = path.abspath(rootpath)\n    excludes = normalize_excludes(rootpath, excludes)\n    try:\n        modules = recurse_tree(rootpath, excludes, opts)\n    except TemplateNotFound as e:\n        print('Cannot find template in %s: %s' %\n              (opts.templates, e), file=sys.stderr)\n        sys.exit(1)\n\n    if opts.full:\n        raise NotImplementedError(\"--full not supported\")\n        # This would only make sense if this script was integrated in Sphinx\n        from sphinx import quickstart as qs\n        modules.sort()\n        prev_module = ''  # type: unicode\n        text = ''\n        for module in modules:\n            if module.startswith(prev_module + '.'):\n                continue\n            prev_module = module\n            text += '   %s\\n' % module\n        d = dict(\n            path = opts.destdir,\n            sep = False,\n            dot = '_',\n            project = opts.header,\n            author = opts.author or 'Author',\n            version = opts.version or '',\n            release = opts.release or opts.version or '',\n            suffix = '.' + opts.suffix,\n            master = 'index',\n            epub = True,\n            ext_autodoc = True,\n            ext_viewcode = True,\n            ext_todo = True,\n            makefile = True,\n            batchfile = True,\n            mastertocmaxdepth = opts.maxdepth,\n            mastertoctree = text,\n            language = 'en',\n            module_path = rootpath,\n            append_syspath = opts.append_syspath,\n        )\n        enabled_exts = {'ext_' + ext: getattr(opts, 'ext_' + ext)\n                        for ext in EXTENSIONS if getattr(opts, 'ext_' + ext)}\n        d.update(enabled_exts)\n\n        if isinstance(opts.header, binary_type):\n            d['project'] = d['project'].decode('utf-8')\n        if isinstance(opts.author, binary_type):\n            d['author'] = d['author'].decode('utf-8')\n        if isinstance(opts.version, binary_type):\n            d['version'] = d['version'].decode('utf-8')\n        if isinstance(opts.release, binary_type):\n            d['release'] = d['release'].decode('utf-8')\n\n        if not opts.dryrun:\n            qs.generate(d, silent=True, overwrite=opts.force)\n    elif not opts.notoc:\n        create_modules_toc_file(modules, opts)\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all values of the given key.", "response": "def get_all_for(self, key):\n        \"\"\" Returns all values of the given key \"\"\"\n        if not isinstance(key, _string_type):\n            raise TypeError(\"Key needs to be a string.\")\n        return [self[(idx, key)] for idx in _range(self.__kcount[key])]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_all_for(self, key):\n        if not isinstance(key, _string_type):\n            raise TypeError(\"Key need to be a string.\")\n\n        for idx in _range(self.__kcount[key]):\n            super(VDFDict, self).__delitem__((idx, key))\n\n        self.__omap = list(filter(lambda x: x[1] != key, self.__omap))\n\n        del self.__kcount[key]", "response": "Removes all items with the given key from the VDFDict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the dict contains any keys with duplicates.", "response": "def has_duplicates(self):\n        \"\"\"\n        Returns ``True`` if the dict contains keys with duplicates.\n        Recurses through any all keys with value that is ``VDFDict``.\n        \"\"\"\n        for n in getattr(self.__kcount, _iter_values)():\n            if n != 1:\n                return True\n\n        def dict_recurse(obj):\n            for v in getattr(obj, _iter_values)():\n                if isinstance(v, VDFDict) and v.has_duplicates():\n                    return True\n                elif isinstance(v, dict):\n                    return dict_recurse(v)\n            return False\n\n        return dict_recurse(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(fp, mapper=dict, merge_duplicate_keys=True, escaped=True):\n    if not issubclass(mapper, dict):\n        raise TypeError(\"Expected mapper to be subclass of dict, got %s\" % type(mapper))\n    if not hasattr(fp, 'readline'):\n        raise TypeError(\"Expected fp to be a file-like object supporting line iteration\")\n\n    stack = [mapper()]\n    expect_bracket = False\n\n    re_keyvalue = re.compile(r'^(\"(?P<qkey>(?:\\\\.|[^\\\\\"])+)\"|(?P<key>#?[a-z0-9\\-\\_\\\\\\?]+))'\n                             r'([ \\t]*('\n                             r'\"(?P<qval>(?:\\\\.|[^\\\\\"])*)(?P<vq_end>\")?'\n                             r'|(?P<val>[a-z0-9\\-\\_\\\\\\?\\*\\.]+)'\n                             r'))?',\n                             flags=re.I)\n\n    for idx, line in enumerate(fp):\n        if idx == 0:\n            line = strip_bom(line)\n\n        line = line.lstrip()\n\n        # skip empty and comment lines\n        if line == \"\" or line[0] == '/':\n            continue\n\n        # one level deeper\n        if line[0] == \"{\":\n            expect_bracket = False\n            continue\n\n        if expect_bracket:\n            raise SyntaxError(\"vdf.parse: expected openning bracket (line %d)\" % (idx + 1))\n\n        # one level back\n        if line[0] == \"}\":\n            if len(stack) > 1:\n                stack.pop()\n                continue\n\n            raise SyntaxError(\"vdf.parse: one too many closing parenthasis (line %d)\" % (idx + 1))\n\n        # parse keyvalue pairs\n        while True:\n            match = re_keyvalue.match(line)\n\n            if not match:\n                try:\n                    line += next(fp)\n                    continue\n                except StopIteration:\n                    raise SyntaxError(\"vdf.parse: unexpected EOF (open key quote?)\")\n\n            key = match.group('key') if match.group('qkey') is None else match.group('qkey')\n            val = match.group('val') if match.group('qval') is None else match.group('qval')\n\n            if escaped:\n                key = _unescape(key)\n\n            # we have a key with value in parenthesis, so we make a new dict obj (level deeper)\n            if val is None:\n                if merge_duplicate_keys and key in stack[-1]:\n                    _m = stack[-1][key]\n                else:\n                    _m = mapper()\n                    stack[-1][key] = _m\n\n                stack.append(_m)\n                expect_bracket = True\n\n            # we've matched a simple keyvalue pair, map it to the last dict obj in the stack\n            else:\n                # if the value is line consume one more line and try to match again,\n                # until we get the KeyValue pair\n                if match.group('vq_end') is None and match.group('qval') is not None:\n                    try:\n                        line += next(fp)\n                        continue\n                    except StopIteration:\n                        raise SyntaxError(\"vdf.parse: unexpected EOF (open value quote?)\")\n\n                stack[-1][key] = _unescape(val) if escaped else val\n\n            # exit the loop\n            break\n\n    if len(stack) != 1:\n        raise SyntaxError(\"vdf.parse: unclosed parenthasis or quotes (EOF)\")\n\n    return stack.pop()", "response": "Deserialize a file - like object containing a VDF into a Python object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dumps(obj, pretty=False, escaped=True):\n    if not isinstance(obj, dict):\n        raise TypeError(\"Expected data to be an instance of``dict``\")\n    if not isinstance(pretty, bool):\n        raise TypeError(\"Expected pretty to be of type bool\")\n    if not isinstance(escaped, bool):\n        raise TypeError(\"Expected escaped to be of type bool\")\n\n    return ''.join(_dump_gen(obj, pretty, escaped))", "response": "Serialize obj to a VDF formatted str."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize obj as a VDF formatted stream to fp.", "response": "def dump(obj, fp, pretty=False, escaped=True):\n    \"\"\"\n    Serialize ``obj`` as a VDF formatted stream to ``fp`` (a\n    ``.write()``-supporting file-like object).\n    \"\"\"\n    if not isinstance(obj, dict):\n        raise TypeError(\"Expected data to be an instance of``dict``\")\n    if not hasattr(fp, 'write'):\n        raise TypeError(\"Expected fp to have write() method\")\n    if not isinstance(pretty, bool):\n        raise TypeError(\"Expected pretty to be of type bool\")\n    if not isinstance(escaped, bool):\n        raise TypeError(\"Expected escaped to be of type bool\")\n\n    for chunk in _dump_gen(obj, pretty, escaped):\n        fp.write(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserializing obj to a binary VDF formatted bytes.", "response": "def binary_dumps(obj, alt_format=False):\n    \"\"\"\n    Serialize ``obj`` to a binary VDF formatted ``bytes``.\n    \"\"\"\n    return b''.join(_binary_dump_gen(obj, alt_format=alt_format))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vbkv_loads(s, mapper=dict, merge_duplicate_keys=True):\n    if s[:4] != b'VBKV':\n        raise ValueError(\"Invalid header\")\n\n    checksum, = struct.unpack('<i', s[4:8])\n\n    if checksum != crc32(s[8:]):\n        raise ValueError(\"Invalid checksum\")\n\n    return binary_loads(s[8:], mapper, merge_duplicate_keys, alt_format=True)", "response": "Deserialize a VBKV string into a Python object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserializes obj to a VBKV formatted bytes.", "response": "def vbkv_dumps(obj):\n    \"\"\"\n    Serialize ``obj`` to a VBKV formatted ``bytes``.\n    \"\"\"\n    data =  b''.join(_binary_dump_gen(obj, alt_format=True))\n    checksum = crc32(data)\n\n    return b'VBKV' + struct.pack('<i', checksum) + data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data_key(data) :\n    \"returns a unique value that allows data to be used as a dict/set key.\"\n    if isinstance(data, (bytes, float, frozenset, int, str, tuple)) :\n        result = data\n    else :\n        # data itself is non-hashable\n        result = id(data)\n    #end if\n    return \\\n        result", "response": "returns a unique value that allows data to be used as a dict / set key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the libdbus library version as a tuple of integers major minor micro.", "response": "def get_version() :\n    \"returns the libdbus library version as a tuple of integers (major, minor, micro).\"\n    major = ct.c_int()\n    minor = ct.c_int()\n    micro = ct.c_int()\n    dbus.dbus_get_version(ct.byref(major), ct.byref(minor), ct.byref(micro))\n    return \\\n        (major.value, minor.value, micro.value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing Message message match against the specified rule.", "response": "def matches_rule(message, rule, destinations = None) :\n    \"does Message message match against the specified rule.\"\n    if not isinstance(message, Message) :\n        raise TypeError(\"message must be a Message\")\n    #end if\n    rule = unformat_rule(rule)\n    eavesdrop = rule.get(\"eavesdrop\", \"false\") == \"true\"\n\n    def match_message_type(expect, actual) :\n        return \\\n            actual == Message.type_from_string(expect)\n    #end match_message_type\n\n    def match_path_namespace(expect, actual) :\n        return \\\n            (\n                actual != None\n            and\n                (\n                    expect == actual\n                or\n                    actual.startswith(expect) and (expect == \"/\" or actual[len(expect)] == \"/\")\n                )\n            )\n    #end match_path_namespace\n\n    def match_dotted_namespace(expect, actual) :\n        return \\\n            (\n                actual != None\n            and\n                (\n                    expect == actual\n                or\n                    actual.startswith(expect) and actual[len(expect)] == \".\"\n                )\n            )\n    #end match_dotted_namespace\n\n    def get_nth_arg(msg, n, expect_types) :\n        msg_signature = parse_signature(msg.signature)\n        if n >= len(msg_signature) :\n            raise IndexError(\"arg nr %d beyond nr args %d\" % (n, len(msg_signature)))\n        #end if\n        val = msg.all_objects[n]\n        valtype = msg_signature[n]\n        if valtype not in expect_types :\n            if False :\n                raise TypeError \\\n                  (\n                        \"expecting one of types %s, not %s for arg %d val %s\"\n                    %\n                        ((repr(expect_types), repr(valtype), n, repr(val)))\n                  )\n            #end if\n            val = None # never match\n        #end if\n        return \\\n            val\n    #end get_nth_arg\n\n    def get_arg_0_str(message) :\n        return \\\n            get_nth_arg(message, 0, [BasicType(TYPE.STRING)])\n    #end get_arg_0_str\n\n    def match_arg_paths(expect, actual) :\n        return \\\n            (\n                actual != None\n            and\n                (\n                    expect == actual\n                or\n                    expect.endswith(\"/\") and actual.startswith(expect)\n                or\n                    actual.endswith(\"/\") and expect.startswith(actual)\n                )\n            )\n    #end match_arg_paths\n\n    match_types = \\\n        ( # note that message attribute value of None will fail to match\n          # any expected string value, which is exactly what we want\n            (\"type\", None, match_message_type, None),\n            (\"sender\", None, operator.eq, None),\n            (\"interface\", None, operator.eq, None),\n            (\"member\", None, operator.eq, None),\n            (\"path\", None, operator.eq, None),\n            (\"destination\", None, operator.eq, None),\n            (\"path_namespace\", \"path\", match_path_namespace, None),\n            (\"arg0namespace\", None, match_dotted_namespace, get_arg_0_str),\n            # \u201carg\u00abn\u00bbpath\u201d handled specially below\n        )\n\n#begin matches_rule\n    keys_used = set(rule.keys()) - {\"eavesdrop\"}\n    matches = \\\n        (\n            eavesdrop\n        or\n            destinations == None\n        or\n            message.destination == None\n        or\n            message.destination in destinations\n        )\n    if matches :\n        try_matching = iter(match_types)\n        while True :\n            try_rule = next(try_matching, None)\n            if try_rule == None :\n                break\n            rulekey, attrname, action, accessor = try_rule\n            if attrname == None :\n                attrname = rulekey\n            #end if\n            if rulekey in rule :\n                if accessor != None :\n                    val = accessor(message)\n                else :\n                    val = getattr(message, attrname)\n                #end if\n                keys_used.remove(rulekey)\n                if not action(rule[rulekey], val) :\n                    matches = False\n                    break\n                #end if\n            #end if\n        #end while\n    #end if\n    if matches :\n        try_matching = iter(rule.keys())\n        while True :\n            try_key = next(try_matching, None)\n            if try_key == None :\n                break\n            if try_key.startswith(\"arg\") and not try_key.endswith(\"namespace\") :\n                argnr = try_key[3:]\n                is_path = argnr.endswith(\"path\")\n                if is_path :\n                    argnr = argnr[:-4]\n                #end if\n                argnr = int(argnr)\n                if not (0 <= argnr < 64) :\n                    raise ValueError(\"argnr %d out of range\" % argnr)\n                #end if\n                argval = get_nth_arg \\\n                  (\n                    message,\n                    argnr,\n                    [BasicType(TYPE.STRING)] + ([], [BasicType(TYPE.OBJECT_PATH)])[is_path]\n                  )\n                keys_used.remove(try_key)\n                if not (operator.eq, match_arg_paths)[is_path](rule[try_key], argval) :\n                    matches = False\n                    break\n                #end if\n            #end if\n        #end while\n    #end if\n    if matches and len(keys_used) != 0 :\n        # fixme: not checking for unrecognized rule keys if I didn\u2019t try matching them all\n        raise KeyError(\"unrecognized rule keywords: %s\" % \", \".join(sorted(keys_used)))\n    #end if\n    return \\\n        matches"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signature_validate(signature, error = None) :\n    \"is signature a valid sequence of zero or more complete types.\"\n    error, my_error = _get_error(error)\n    result = dbus.dbus_signature_validate(signature.encode(), error._dbobj) != 0\n    my_error.raise_if_set()\n    return \\\n        result", "response": "is signature a valid sequence of zero or more complete types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a signature from parsed form to string form.", "response": "def unparse_signature(signature) :\n    \"converts a signature from parsed form to string form.\"\n    signature = parse_signature(signature)\n    if not isinstance(signature, (tuple, list)) :\n        signature = [signature]\n    #end if\n    return \\\n        DBUS.Signature(\"\".join(t.signature for t in signature))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbeing signature a single valid type.", "response": "def signature_validate_single(signature, error = None) :\n    \"is signature a single valid type.\"\n    error, my_error = _get_error(error)\n    result = dbus.dbus_signature_validate_single(signature.encode(), error._dbobj) != 0\n    my_error.raise_if_set()\n    return \\\n        result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_path(path) :\n    \"convenience routine for splitting a path into a list of components.\"\n    if isinstance(path, (tuple, list)) :\n        result = path # assume already split\n    elif path == \"/\" :\n        result = []\n    else :\n        if not path.startswith(\"/\") or path.endswith(\"/\") :\n            raise DBusError(DBUS.ERROR_INVALID_ARGS, \"invalid path %s\" % repr(path))\n        #end if\n        result = path.split(\"/\")[1:]\n    #end if\n    return \\\n        result", "response": "convenience routine for splitting a path into a list of components."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_utf8(alleged_utf8, error = None) :\n    \"alleged_utf8 must be null-terminated bytes.\"\n    error, my_error = _get_error(error)\n    result = dbus.dbus_validate_utf8(alleged_utf8, error._dbobj) != 0\n    my_error.raise_if_set()\n    return \\\n        result", "response": "alleged_utf8 must be null - terminated bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn integer i after checking that it fits in the given number of bits.", "response": "def int_subtype(i, bits, signed) :\n        \"returns integer i after checking that it fits in the given number of bits.\"\n        if not isinstance(i, int) :\n            raise TypeError(\"value is not int: %s\" % repr(i))\n        #end if\n        if signed :\n            lo = - 1 << bits - 1\n            hi = (1 << bits - 1) - 1\n        else :\n            lo = 0\n            hi = (1 << bits) - 1\n        #end if\n        if i < lo or i > hi :\n            raise ValueError \\\n              (\n                \"%d not in range of %s %d-bit value\" % (i, (\"unsigned\", \"signed\")[signed], bits)\n              )\n        #end if\n        return \\\n            i"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init(celf, *, loop = None, unregister = None, message = None) :\n        \"for consistency with other classes that don\u2019t want caller to instantiate directly.\"\n        return \\\n            celf \\\n              (\n                loop = loop,\n                unregister = unregister,\n                message = message,\n              )", "response": "for consistency with other classes that don t want caller to instantiate directly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nasking the server at the other end for its unique id.", "response": "def server_id(self) :\n        \"asks the server at the other end for its unique id.\"\n        c_result = dbus.dbus_connection_get_server_id(self._dbobj)\n        result = ct.cast(c_result, ct.c_char_p).value.decode()\n        dbus.dbus_free(c_result)\n        return \\\n            result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, message) :\n        \"puts a message in the outgoing queue.\"\n        if not isinstance(message, Message) :\n            raise TypeError(\"message must be a Message\")\n        #end if\n        serial = ct.c_uint()\n        if not dbus.dbus_connection_send(self._dbobj, message._dbobj, ct.byref(serial)) :\n            raise CallFailed(\"dbus_connection_send\")\n        #end if\n        return \\\n            serial.value", "response": "puts a message in the outgoing queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_with_reply_and_block(self, message, timeout = DBUS.TIMEOUT_USE_DEFAULT, error = None) :\n        \"sends a message, blocks the thread until the reply is available, and returns it.\"\n        if not isinstance(message, Message) :\n            raise TypeError(\"message must be a Message\")\n        #end if\n        error, my_error = _get_error(error)\n        reply = dbus.dbus_connection_send_with_reply_and_block(self._dbobj, message._dbobj, _get_timeout(timeout), error._dbobj)\n        my_error.raise_if_set()\n        if reply != None :\n            result = Message(reply)\n        else :\n            result = None\n        #end if\n        return \\\n            result", "response": "sends a message blocks the thread until the reply is available and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_registered(self, parent_path) :\n        \"lists all the object paths for which you have ObjectPathVTable handlers registered.\"\n        child_entries = ct.POINTER(ct.c_char_p)()\n        if not dbus.dbus_connection_list_registered(self._dbobj, parent_path.encode(), ct.byref(child_entries)) :\n            raise CallFailed(\"dbus_connection_list_registered\")\n        #end if\n        result = []\n        i = 0\n        while True :\n            entry = child_entries[i]\n            if entry == None :\n                break\n            result.append(entry.decode())\n            i += 1\n        #end while\n        dbus.dbus_free_string_array(child_entries)\n        return \\\n            result", "response": "lists all the object paths for which you have ObjectPathVTable handlers registered."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bus_get(celf, type, private, error = None) :\n        \"returns a Connection to one of the predefined D-Bus buses; type is a BUS_xxx value.\"\n        error, my_error = _get_error(error)\n        result = (dbus.dbus_bus_get, dbus.dbus_bus_get_private)[private](type, error._dbobj)\n        my_error.raise_if_set()\n        if result != None :\n            result = celf(result)\n        #end if\n        return \\\n            result", "response": "returns a Connection to one of the predefined D - Bus buses ; type is a BUS_xxx value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a message filter previously installed with bus_add_match_action.", "response": "def bus_remove_match_action(self, rule, func, user_data, error = None) :\n        \"removes a message filter previously installed with bus_add_match_action.\"\n        rulekey = format_rule(rule)\n        rule = unformat_rule(rule)\n        self._match_actions[rulekey].actions.remove(_MatchActionEntry._Action(func, user_data))\n        if len(self._match_actions[rulekey].actions) == 0 :\n            self.bus_remove_match(rulekey, error) # shouldn\u2019t fail!\n            del self._match_actions[rulekey]\n            if len(self._match_actions) == 0 :\n                self.remove_filter(self._rule_action_match, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove a message filter previously installed with bus_add_match_action.", "response": "async def bus_remove_match_action_async(self, rule, func, user_data, error = None, timeout = DBUS.TIMEOUT_USE_DEFAULT) :\n        \"removes a message filter previously installed with bus_add_match_action.\"\n        rulekey = format_rule(rule)\n        rule = unformat_rule(rule)\n        self._match_actions[rulekey].actions.remove(_MatchActionEntry._Action(func, user_data))\n        if len(self._match_actions[rulekey].actions) == 0 :\n            await self.bus_remove_match_async(rulekey, error, timeout) # shouldn\u2019t fail!\n            del self._match_actions[rulekey]\n            if len(self._match_actions) == 0 :\n                self.remove_filter(self._rule_action_match, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef become_monitor(self, rules) :\n        \"turns the connection into one that can only receive monitoring messages.\"\n        message = Message.new_method_call \\\n          (\n            destination = DBUS.SERVICE_DBUS,\n            path = DBUS.PATH_DBUS,\n            iface = DBUS.INTERFACE_MONITORING,\n            method = \"BecomeMonitor\"\n          )\n        message.append_objects(\"asu\", (list(format_rule(rule) for rule in rules)), 0)\n        self.send(message)", "response": "turns the connection into one that can only receive monitoring messages."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_error(self, name, message) :\n        \"creates a new DBUS.MESSAGE_TYPE_ERROR message that is a reply to this Message.\"\n        result = dbus.dbus_message_new_error(self._dbobj, name.encode(), (lambda : None, lambda : message.encode())[message != None]())\n        if result == None :\n            raise CallFailed(\"dbus_message_new_error\")\n        #end if\n        return \\\n            type(self)(result)", "response": "creates a new DBUS. MESSAGE_TYPE_ERROR message that is a reply to this Message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_method_call(celf, destination, path, iface, method) :\n        \"creates a new DBUS.MESSAGE_TYPE_METHOD_CALL message.\"\n        result = dbus.dbus_message_new_method_call \\\n          (\n            (lambda : None, lambda : destination.encode())[destination != None](),\n            path.encode(),\n            (lambda : None, lambda : iface.encode())[iface != None](),\n            method.encode(),\n          )\n        if result == None :\n            raise CallFailed(\"dbus_message_new_method_call\")\n        #end if\n        return \\\n            celf(result)", "response": "creates a new DBUS. MESSAGE_TYPE_METHOD_CALL message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new DBUS. MESSAGE_TYPE_METHOD_RETURN that is a reply to this Message.", "response": "def new_method_return(self) :\n        \"creates a new DBUS.MESSAGE_TYPE_METHOD_RETURN that is a reply to this Message.\"\n        result = dbus.dbus_message_new_method_return(self._dbobj)\n        if result == None :\n            raise CallFailed(\"dbus_message_new_method_return\")\n        #end if\n        return \\\n            type(self)(result)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new DBUS. MESSAGE_TYPE_SIGNAL message.", "response": "def new_signal(celf, path, iface, name) :\n        \"creates a new DBUS.MESSAGE_TYPE_SIGNAL message.\"\n        result = dbus.dbus_message_new_signal(path.encode(), iface.encode(), name.encode())\n        if result == None :\n            raise CallFailed(\"dbus_message_new_signal\")\n        #end if\n        return \\\n            celf(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy(self) :\n        \"creates a copy of this Message.\"\n        result = dbus.dbus_message_copy(self._dbobj)\n        if result == None :\n            raise CallFailed(\"dbus_message_copy\")\n        #end if\n        return \\\n            type(self)(result)", "response": "creates a copy of this Message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an iterator for extracting the arguments of the Message.", "response": "def iter_init(self) :\n        \"creates an iterator for extracting the arguments of the Message.\"\n        iter = self.ExtractIter(None)\n        if dbus.dbus_message_iter_init(self._dbobj, iter._dbobj) == 0 :\n            iter._nulliter = True\n        #end if\n        return \\\n             iter"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_init_append(self) :\n        \"creates a Message.AppendIter for appending arguments to the Message.\"\n        iter = self.AppendIter(None)\n        dbus.dbus_message_iter_init_append(self._dbobj, iter._dbobj)\n        return \\\n            iter", "response": "creates a Message. AppendIter for appending arguments to the Message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef error_name(self) :\n        \"the error name for a DBUS.MESSAGE_TYPE_ERROR message.\"\n        result = dbus.dbus_message_get_error_name(self._dbobj)\n        if result != None :\n            result = result.decode()\n        #end if\n        return \\\n            result", "response": "the error name for a DBUS. MESSAGE_TYPE_ERROR message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserializes this Message into the wire protocol format and returns a bytes object.", "response": "def marshal(self) :\n        \"serializes this Message into the wire protocol format and returns a bytes object.\"\n        buf = ct.POINTER(ct.c_ubyte)()\n        nr_bytes = ct.c_int()\n        if not dbus.dbus_message_marshal(self._dbobj, ct.byref(buf), ct.byref(nr_bytes)) :\n            raise CallFailed(\"dbus_message_marshal\")\n        #end if\n        result = bytearray(nr_bytes.value)\n        ct.memmove \\\n          (\n            ct.addressof((ct.c_ubyte * nr_bytes.value).from_buffer(result)),\n            buf,\n            nr_bytes.value\n          )\n        dbus.dbus_free(buf)\n        return \\\n            result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntells libdbus you no longer care about the pending incoming message.", "response": "def cancel(self) :\n        \"tells libdbus you no longer care about the pending incoming message.\"\n        dbus.dbus_pending_call_cancel(self._dbobj)\n        if self._awaiting != None :\n            # This probably shouldn\u2019t occur. Looking at the source of libdbus,\n            # it doesn\u2019t keep track of any \u201ccancelled\u201d state for the PendingCall,\n            # it just detaches it from any notifications about an incoming reply.\n            self._awaiting.cancel()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfill in the error name and message.", "response": "def set(self, name, msg) :\n        \"fills in the error name and message.\"\n        dbus.dbus_set_error(self._dbobj, name.encode(), b\"%s\", msg.encode())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a match rule string from the standard syntax to a dict of { key : value entries.", "response": "def unformat_rule(celf, rule) :\n        \"converts a match rule string from the standard syntax to a dict of {key : value} entries.\"\n        if isinstance(rule, dict) :\n            pass\n        elif isinstance(rule, str) :\n            PARSE = celf.PARSE\n            parsed = {}\n            chars = iter(rule)\n            state = PARSE.EXPECT_NAME\n            curname = None\n            curval = None\n            while True :\n                ch = next(chars, None)\n                if ch == None :\n                    if state == PARSE.EXPECT_ESCAPED :\n                        raise SyntaxError(\"missing character after backslash\")\n                    elif state == PARSE.EXPECT_QUOTED_VALUE :\n                        raise SyntaxError(\"missing closing apostrophe\")\n                    else : # state in (PARSE.EXPECT_NAME, PARSE.EXPECT_UNQUOTED_VALUE)\n                        if curname != None :\n                            if curval != None :\n                                if curname in parsed :\n                                    raise SyntaxError(\"duplicated attribute \u201c%s\u201d\" % curname)\n                                #end if\n                                parsed[curname] = curval\n                            else :\n                                raise SyntaxError(\"missing value for attribute \u201c%s\u201d\" % curname)\n                            #end if\n                        #end if\n                    #end if\n                    break\n                #end if\n                if state == PARSE.EXPECT_ESCAPED :\n                    if ch == \"'\" :\n                        usech = ch\n                        nextch = None\n                    else :\n                        usech = \"\\\\\"\n                        nextch = ch\n                    #end if\n                    ch = usech\n                    if curval == None :\n                        curval = ch\n                    else :\n                        curval += ch\n                    #end if\n                    ch = nextch # None indicates already processed\n                    state = PARSE.EXPECT_UNQUOTED_VALUE\n                #end if\n                if ch != None :\n                    if ch == \",\" and state != PARSE.EXPECT_QUOTED_VALUE :\n                        if state == PARSE.EXPECT_UNQUOTED_VALUE :\n                            if curname in parsed :\n                                raise SyntaxError(\"duplicated attribute \u201c%s\u201d\" % curname)\n                            #end if\n                            if curval == None :\n                                curval = \"\"\n                            #end if\n                            parsed[curname] = curval\n                            curname = None\n                            curval = None\n                            state = PARSE.EXPECT_NAME\n                        else :\n                            raise SyntaxError(\"unexpected comma\")\n                        #end if\n                    elif ch == \"\\\\\" and state != PARSE.EXPECT_QUOTED_VALUE :\n                        if state == PARSE.EXPECT_UNQUOTED_VALUE :\n                            state = PARSE.EXPECT_ESCAPED\n                        else :\n                            raise SyntaxError(\"unexpected backslash\")\n                        #end if\n                    elif ch == \"=\" and state != PARSE.EXPECT_QUOTED_VALUE :\n                        if curname == None :\n                            raise SyntaxError(\"empty attribute name\")\n                        #end if\n                        if state == PARSE.EXPECT_NAME :\n                            state = PARSE.EXPECT_UNQUOTED_VALUE\n                        else :\n                            raise SyntaxError(\"unexpected equals sign\")\n                        #end if\n                    elif ch == \"'\" :\n                        if state == PARSE.EXPECT_UNQUOTED_VALUE :\n                            state = PARSE.EXPECT_QUOTED_VALUE\n                        elif state == PARSE.EXPECT_QUOTED_VALUE :\n                            state = PARSE.EXPECT_UNQUOTED_VALUE\n                        else :\n                            raise SyntaxError(\"unexpected apostrophe\")\n                        #end if\n                    else :\n                        if state == PARSE.EXPECT_NAME :\n                            if curname == None :\n                                curname = ch\n                            else :\n                                curname += ch\n                            #end if\n                        elif state in (PARSE.EXPECT_QUOTED_VALUE, PARSE.EXPECT_UNQUOTED_VALUE) :\n                            if curval == None :\n                                curval = ch\n                            else :\n                                curval += ch\n                            #end if\n                        else :\n                            raise AssertionError(\"shouldn\u2019t occur: parse state %s\" % repr(state))\n                        #end if\n                    #end if\n                #end if\n            #end while\n            rule = parsed\n        else :\n            raise TypeError(\"rule \u201c%s\u201d must be a dict or string\" % repr(rule))\n        #end if\n        return \\\n            rule"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate an Introspection tree from the given XML string description.", "response": "def parse(celf, s) :\n        \"generates an Introspection tree from the given XML string description.\"\n\n        def from_string_elts(celf, attrs, tree) :\n            elts = dict((k, attrs[k]) for k in attrs)\n            child_tags = dict \\\n              (\n                (childclass.tag_name, childclass)\n                for childclass in tuple(celf.tag_elts.values()) + (Introspection.Annotation,)\n              )\n            children = []\n            for child in tree :\n                if child.tag not in child_tags :\n                    raise KeyError(\"unrecognized tag %s\" % child.tag)\n                #end if\n                childclass = child_tags[child.tag]\n                childattrs = {}\n                for attrname in childclass.tag_attrs :\n                    if hasattr(childclass, \"tag_attrs_optional\") and attrname in childclass.tag_attrs_optional :\n                        childattrs[attrname] = child.attrib.get(attrname, None)\n                    else :\n                        if attrname not in child.attrib :\n                            raise ValueError(\"missing %s attribute for %s tag\" % (attrname, child.tag))\n                        #end if\n                        childattrs[attrname] = child.attrib[attrname]\n                    #end if\n                #end for\n                if hasattr(childclass, \"attr_convert\") :\n                    for attr in childclass.attr_convert :\n                        if attr in childattrs :\n                            childattrs[attr] = childclass.attr_convert[attr](childattrs[attr])\n                        #end if\n                    #end for\n                #end if\n                children.append(from_string_elts(childclass, childattrs, child))\n            #end for\n            for child_tag, childclass in tuple(celf.tag_elts.items()) + ((), ((\"annotations\", Introspection.Annotation),))[tree.tag != \"annotation\"] :\n                for child in children :\n                    if isinstance(child, childclass) :\n                        if child_tag not in elts :\n                            elts[child_tag] = []\n                        #end if\n                        elts[child_tag].append(child)\n                    #end if\n                #end for\n            #end for\n            return \\\n                celf(**elts)\n        #end from_string_elts\n\n    #begin parse\n        tree = XMLElementTree.fromstring(s)\n        assert tree.tag == \"node\", \"root of introspection tree must be <node> tag\"\n        return \\\n            from_string_elts(Introspection, {}, tree)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unparse(self, indent_step = 4, max_linelen = 72) :\n        \"returns an XML string description of this Introspection tree.\"\n\n        out = io.StringIO()\n\n        def to_string(obj, indent) :\n            tag_name = obj.tag_name\n            attrs = []\n            for attrname in obj.tag_attrs :\n                attr = getattr(obj, attrname)\n                if attr != None :\n                    if isinstance(attr, enum.Enum) :\n                        attr = attr.value\n                    elif isinstance(attr, Type) :\n                        attr = unparse_signature(attr)\n                    elif not isinstance(attr, str) :\n                        raise TypeError(\"unexpected attribute type %s for %s\" % (type(attr).__name__, repr(attr)))\n                    #end if\n                    attrs.append(\"%s=%s\" % (attrname, quote_xml_attr(attr)))\n                #end if\n            #end for\n            has_elts = \\\n              (\n                    sum\n                      (\n                        len(getattr(obj, attrname))\n                        for attrname in\n                                tuple(obj.tag_elts.keys())\n                            +\n                                ((), (\"annotations\",))\n                                    [not isinstance(obj, Introspection.Annotation)]\n                      )\n                !=\n                    0\n              )\n            out.write(\" \" * indent + \"<\" + tag_name)\n            if (\n                    max_linelen != None\n                and\n                            indent\n                        +\n                            len(tag_name)\n                        +\n                            sum((len(s) + 1) for s in attrs)\n                        +\n                            2\n                        +\n                            int(has_elts)\n                    >\n                        max_linelen\n            ) :\n                out.write(\"\\n\")\n                for attr in attrs :\n                    out.write(\" \" * (indent + indent_step))\n                    out.write(attr)\n                    out.write(\"\\n\")\n                #end for\n                out.write(\" \" * indent)\n            else :\n                for attr in attrs :\n                    out.write(\" \")\n                    out.write(attr)\n                #end for\n            #end if\n            if not has_elts :\n                out.write(\"/\")\n            #end if\n            out.write(\">\\n\")\n            if has_elts :\n                for attrname in sorted(obj.tag_elts.keys()) + [\"annotations\"] :\n                    for elt in getattr(obj, attrname) :\n                        to_string(elt, indent + indent_step)\n                    #end for\n                #end for\n                out.write(\" \" * indent + \"</\" + tag_name + \">\\n\")\n            #end if\n        #end to_string\n\n    #begin unparse\n        out.write(DBUS.INTROSPECT_1_0_XML_DOCTYPE_DECL_NODE)\n        out.write(\"<node\")\n        if self.name != None :\n            out.write(\" name=%s\" % quote_xml_attr(self.name))\n        #end if\n        out.write(\">\\n\")\n        for elt in self.interfaces :\n            to_string(elt, indent_step)\n        #end for\n        for elt in self.nodes :\n            to_string(elt, indent_step)\n        #end for\n        out.write(\"</node>\\n\")\n        return \\\n            out.getvalue()", "response": "returns an XML string description of this Introspection tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Connection object for the current D - Bus session bus.", "response": "def session_bus(**kwargs) :\n    \"returns a Connection object for the current D-Bus session bus.\"\n    return \\\n        Connection(dbus.Connection.bus_get(DBUS.BUS_SESSION, private = False)) \\\n        .register_additional_standard(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef system_bus(**kwargs) :\n    \"returns a Connection object for the D-Bus system bus.\"\n    return \\\n        Connection(dbus.Connection.bus_get(DBUS.BUS_SYSTEM, private = False)) \\\n        .register_additional_standard(**kwargs)", "response": "returns a Connection object for the D - Bus system bus."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Connection object for the D - Bus starter bus.", "response": "def starter_bus(**kwargs) :\n    \"returns a Connection object for the D-Bus starter bus.\"\n    return \\\n        Connection(dbus.Connection.bus_get(DBUS.BUS_STARTER, private = False)) \\\n        .register_additional_standard(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Connection object for the current D - Bus session bus.", "response": "async def session_bus_async(loop = None, **kwargs) :\n    \"returns a Connection object for the current D-Bus session bus.\"\n    return \\\n        Connection \\\n          (\n            await dbus.Connection.bus_get_async(DBUS.BUS_SESSION, private = False, loop = loop)\n          ) \\\n        .register_additional_standard(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Connection object for the D - Bus system bus.", "response": "async def system_bus_async(loop = None, **kwargs) :\n    \"returns a Connection object for the D-Bus system bus.\"\n    return \\\n        Connection \\\n          (\n            await dbus.Connection.bus_get_async(DBUS.BUS_SYSTEM, private = False, loop = loop)\n          ) \\\n        .register_additional_standard(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Connection object for the D - Bus starter bus.", "response": "async def starter_bus_async(loop = None, **kwargs) :\n    \"returns a Connection object for the D-Bus starter bus.\"\n    return \\\n        Connection \\\n          (\n            await dbus.Connection.bus_get_async(DBUS.BUS_STARTER, private = False, loop = loop)\n          ) \\\n        .register_additional_standard(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_error(self) :\n        \"fills in and returns an Error object that reports the specified error name and message.\"\n        result = dbus.Error.init()\n        result.set(self.args[0], self.args[1])\n        return \\\n            result", "response": "fills in and returns an Error object that reports the specified error name and message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_name(self, bus_name, flags) :\n        \"registers a bus name.\"\n        if not self._registered_bus_names_listeners :\n            self.connection.bus_add_match_action \\\n              (\n                rule = \"type=signal,interface=org.freedesktop.DBus,member=NameAcquired\",\n                func = self._bus_name_acquired,\n                user_data = self\n              )\n            self.connection.bus_add_match_action \\\n              (\n                rule = \"type=signal,interface=org.freedesktop.DBus,member=NameLost\",\n                func = self._bus_name_lost,\n                user_data = self\n              )\n            self._registered_bus_names_listeners = True\n        #end if\n        return \\\n            self.connection.bus_request_name(bus_name, flags)", "response": "registers a bus name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a bus name.", "response": "async def request_name_async(self, bus_name, flags, error = None, timeout = DBUS.TIMEOUT_USE_DEFAULT) :\n        \"registers a bus name.\"\n        assert self.loop != None, \"no event loop to attach coroutine to\"\n        if not self._registered_bus_names_listeners :\n            self._registered_bus_names_listeners = True # do first in case of reentrant call\n            await self.connection.bus_add_match_action_async \\\n              (\n                rule = \"type=signal,interface=org.freedesktop.DBus,member=NameAcquired\",\n                func = self._bus_name_acquired,\n                user_data = self\n              )\n            await self.connection.bus_add_match_action_async \\\n              (\n                rule = \"type=signal,interface=org.freedesktop.DBus,member=NameLost\",\n                func = self._bus_name_lost,\n                user_data = self\n              )\n        #end if\n        is_acquired = bus_name in self.bus_names_acquired\n        is_pending = bus_name in self.bus_names_pending\n        if not (is_acquired or is_pending) :\n            self.bus_names_pending.add(bus_name)\n            result = await self.connection.bus_request_name_async(bus_name, flags, error = error, timeout = timeout)\n            if error != None and error.is_set or result != DBUS.REQUEST_NAME_REPLY_IN_QUEUE :\n                self.bus_names_pending.discard(bus_name)\n            #end if\n        elif is_pending :\n            result = DBUS.REQUEST_NAME_REPLY_IN_QUEUE\n        else :\n            result = DBUS.REQUEST_NAME_REPLY_ALREADY_OWNER\n        #end if\n        return \\\n            result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrelease a registered bus name.", "response": "async def release_name_async(self, bus_name, error = None, timeout = DBUS.TIMEOUT_USE_DEFAULT) :\n        \"releases a registered bus name.\"\n        assert self.loop != None, \"no event loop to attach coroutine to\"\n        return \\\n            await self.connection.bus_release_name_async(bus_name, error = error, timeout = timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DefaultAdapter(self):\n    '''Retrieve the default adapter\n    '''\n    default_adapter = None\n\n    for obj in mockobject.objects.keys():\n        if obj.startswith('/org/bluez/') and 'dev_' not in obj:\n            default_adapter = obj\n\n    if default_adapter:\n        return dbus.ObjectPath(default_adapter, variant_level=1)\n    else:\n        raise dbus.exceptions.DBusException(\n            'No such adapter.', name='org.bluez.Error.NoSuchAdapter')", "response": "Retrieve the default adapter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ListAdapters(self):\n    '''List all known adapters\n    '''\n    adapters = []\n\n    for obj in mockobject.objects.keys():\n        if obj.startswith('/org/bluez/') and 'dev_' not in obj:\n            adapters.append(dbus.ObjectPath(obj, variant_level=1))\n\n    return dbus.Array(adapters, variant_level=1)", "response": "List all known adapters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new device.", "response": "def CreateDevice(self, device_address):\n    '''Create a new device '''\n    device_name = 'dev_' + device_address.replace(':', '_').upper()\n    adapter_path = self.path\n    path = adapter_path + '/' + device_name\n\n    if path not in mockobject.objects:\n        raise dbus.exceptions.DBusException(\n            'Could not create device for %s.' % device_address,\n            name='org.bluez.Error.Failed')\n\n    adapter = mockobject.objects[self.path]\n    adapter.EmitSignal(ADAPTER_IFACE, 'DeviceCreated',\n                       'o', [dbus.ObjectPath(path, variant_level=1)])\n\n    return dbus.ObjectPath(path, variant_level=1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all known devices", "response": "def ListDevices(self):\n    '''List all known devices\n    '''\n    devices = []\n\n    for obj in mockobject.objects.keys():\n        if obj.startswith('/org/bluez/') and 'dev_' in obj:\n            devices.append(dbus.ObjectPath(obj, variant_level=1))\n\n    return dbus.Array(devices, variant_level=1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef FindDevice(self, address):\n    '''Find a specific device by bluetooth address.\n    '''\n    for obj in mockobject.objects.keys():\n        if obj.startswith('/org/bluez/') and 'dev_' in obj:\n            o = mockobject.objects[obj]\n            if o.props[DEVICE_IFACE]['Address'] \\\n                    == dbus.String(address, variant_level=1):\n                return obj\n\n    raise dbus.exceptions.DBusException('No such device.',\n                                        name='org.bluez.Error.NoSuchDevice')", "response": "Find a specific device by bluetooth address."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CreatePairedDevice(self, device_address, agent, capability):\n    '''Convenience method to mark an existing device as paired.\n    '''\n    device_name = 'dev_' + device_address.replace(':', '_').upper()\n    device_path = DefaultAdapter(self) + '/' + device_name\n\n    if device_path not in mockobject.objects:\n        raise dbus.exceptions.DBusException('No such device.',\n                                            name='org.bluez.Error.NoSuchDevice')\n\n    device = mockobject.objects[device_path]\n\n    # Based off pairing with a Sennheise headset.\n    uuids = [\n        '00001108-0000-1000-8000-00805f9b34fb',\n        '0000110b-0000-1000-8000-00805f9b34fb',\n        '0000110e-0000-1000-8000-00805f9b34fb',\n        '0000111e-0000-1000-8000-00805f9b34fb',\n    ]\n\n    device.props[DEVICE_IFACE]['UUIDs'] = dbus.Array(uuids, variant_level=1)\n    device.props[DEVICE_IFACE]['Paired'] = dbus.Boolean(True, variant_level=1)\n    device.props[DEVICE_IFACE]['LegacyPairing'] = dbus.Boolean(True,\n                                                               variant_level=1)\n    device.props[DEVICE_IFACE]['Trusted'] = dbus.Boolean(False,\n                                                         variant_level=1)\n    device.props[DEVICE_IFACE]['Connected'] = dbus.Boolean(True,\n                                                           variant_level=1)\n\n    adapter = mockobject.objects[self.path]\n    adapter.EmitSignal(ADAPTER_IFACE, 'DeviceCreated',\n                       'o', [dbus.ObjectPath(device_path, variant_level=1)])\n\n    for prop in device.props[DEVICE_IFACE]:\n        try:\n            device.EmitSignal(DEVICE_IFACE, 'PropertyChanged', 'sv', [\n                prop, device.props[prop]\n            ])\n        except KeyError:\n            pass\n\n    return dbus.ObjectPath(device_path, variant_level=1)", "response": "Convenience method to create a paired device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Connect(self):\n    '''Connect a device '''\n    device_path = self.path\n\n    if device_path not in mockobject.objects:\n        raise dbus.exceptions.DBusException('No such device.',\n                                            name='org.bluez.Error.NoSuchDevice')\n\n    device = mockobject.objects[device_path]\n\n    device.props[AUDIO_IFACE]['State'] = dbus.String(\"connected\",\n                                                     variant_level=1)\n    device.EmitSignal(AUDIO_IFACE, 'PropertyChanged', 'sv', [\n        'State', dbus.String(\"connected\", variant_level=1),\n    ])\n\n    device.props[DEVICE_IFACE]['Connected'] = dbus.Boolean(True,\n                                                           variant_level=1)\n    device.EmitSignal(DEVICE_IFACE, 'PropertyChanged', 'sv', [\n        'Connected', dbus.Boolean(True, variant_level=1),\n    ])", "response": "Connect a resource to a resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisconnecting a resource from a resource.", "response": "def Disconnect(self):\n    '''Disconnect a device '''\n    device_path = self.path\n\n    if device_path not in mockobject.objects:\n        raise dbus.exceptions.DBusException('No such device.',\n                                            name='org.bluez.Error.NoSuchDevice')\n\n    device = mockobject.objects[device_path]\n\n    try:\n        device.props[AUDIO_IFACE]['State'] = dbus.String(\"disconnected\",\n                                                         variant_level=1)\n\n        device.EmitSignal(AUDIO_IFACE, 'PropertyChanged', 'sv', [\n            'State', dbus.String(\"disconnected\", variant_level=1),\n        ])\n    except KeyError:\n        pass\n\n    device.props[DEVICE_IFACE]['Connected'] = dbus.Boolean(False,\n                                                           variant_level=1)\n    device.EmitSignal(DEVICE_IFACE, 'PropertyChanged', 'sv', [\n        'Connected', dbus.Boolean(False, variant_level=1),\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a device from the object list.", "response": "def RemoveDevice(self, object_path):\n    '''Remove (forget) a device '''\n\n    adapter = mockobject.objects[self.path]\n    adapter.EmitSignal(ADAPTER_IFACE, 'DeviceRemoved',\n                       'o', [object_path])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddEthernetDevice(self, device_name, iface_name, state):\n    '''Add an ethernet device.\n\n    You have to specify device_name, device interface name (e. g. eth0), and\n    state. You can use the predefined DeviceState values (e. g.\n    DeviceState.ACTIVATED) or supply a numeric value. For valid state values\n    please visit\n    http://projects.gnome.org/NetworkManager/developers/api/09/spec.html#type-NM_DEVICE_STATE\n\n    Please note that this does not set any global properties.\n\n    Returns the new object path.\n    '''\n    path = '/org/freedesktop/NetworkManager/Devices/' + device_name\n    wired_props = {'Carrier': False,\n                   'HwAddress': dbus.String('78:DD:08:D2:3D:43'),\n                   'PermHwAddress': dbus.String('78:DD:08:D2:3D:43'),\n                   'Speed': dbus.UInt32(0)}\n    self.AddObject(path,\n                   'org.freedesktop.NetworkManager.Device.Wired',\n                   wired_props,\n                   [])\n\n    props = {'DeviceType': dbus.UInt32(1),\n             'State': dbus.UInt32(state),\n             'Interface': iface_name,\n             'ActiveConnection': dbus.ObjectPath('/'),\n             'AvailableConnections': dbus.Array([], signature='o'),\n             'AutoConnect': False,\n             'Managed': True,\n             'Driver': 'dbusmock',\n             'IpInterface': ''}\n\n    obj = dbusmock.get_object(path)\n    obj.AddProperties(DEVICE_IFACE, props)\n\n    self.object_manager_emit_added(path)\n\n    NM = dbusmock.get_object(MANAGER_OBJ)\n    devices = NM.Get(MANAGER_IFACE, 'Devices')\n    devices.append(path)\n    NM.Set(MANAGER_IFACE, 'Devices', devices)\n    NM.EmitSignal('org.freedesktop.NetworkManager', 'DeviceAdded', 'o', [path])\n\n    return path", "response": "Add an ethernet device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a WiFi Device.", "response": "def AddWiFiDevice(self, device_name, iface_name, state):\n    '''Add a WiFi Device.\n\n    You have to specify device_name, device interface name (e. g.  wlan0) and\n    state. You can use the predefined DeviceState values (e. g.\n    DeviceState.ACTIVATED) or supply a numeric value. For valid state values,\n    please visit\n    http://projects.gnome.org/NetworkManager/developers/api/09/spec.html#type-NM_DEVICE_STATE\n\n    Please note that this does not set any global properties.\n\n    Returns the new object path.\n    '''\n\n    path = '/org/freedesktop/NetworkManager/Devices/' + device_name\n    self.AddObject(path,\n                   WIRELESS_DEVICE_IFACE,\n                   {\n                       'HwAddress': dbus.String('11:22:33:44:55:66'),\n                       'PermHwAddress': dbus.String('11:22:33:44:55:66'),\n                       'Bitrate': dbus.UInt32(5400),\n                       'Mode': dbus.UInt32(2),\n                       'WirelessCapabilities': dbus.UInt32(255),\n                       'AccessPoints': dbus.Array([], signature='o'),\n                   },\n                   [\n                       ('GetAccessPoints', '', 'ao',\n                        'ret = self.access_points'),\n                       ('GetAllAccessPoints', '', 'ao',\n                        'ret = self.access_points'),\n                       ('RequestScan', 'a{sv}', '', ''),\n                   ])\n\n    dev_obj = dbusmock.get_object(path)\n    dev_obj.access_points = []\n    dev_obj.AddProperties(DEVICE_IFACE,\n                          {\n                              'ActiveConnection': dbus.ObjectPath('/'),\n                              'AvailableConnections': dbus.Array([], signature='o'),\n                              'AutoConnect': False,\n                              'Managed': True,\n                              'Driver': 'dbusmock',\n                              'DeviceType': dbus.UInt32(2),\n                              'State': dbus.UInt32(state),\n                              'Interface': iface_name,\n                              'IpInterface': iface_name,\n                          })\n\n    self.object_manager_emit_added(path)\n\n    NM = dbusmock.get_object(MANAGER_OBJ)\n    devices = NM.Get(MANAGER_IFACE, 'Devices')\n    devices.append(path)\n    NM.Set(MANAGER_IFACE, 'Devices', devices)\n    NM.EmitSignal('org.freedesktop.NetworkManager', 'DeviceAdded', 'o', [path])\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddAccessPoint(self, dev_path, ap_name, ssid, hw_address,\n                   mode, frequency, rate, strength, security):\n    '''Add an access point to an existing WiFi device.\n\n    You have to specify WiFi Device path, Access Point object name,\n    ssid, hw_address, mode, frequency, rate, strength and security.\n    For valid access point property values, please visit\n    http://projects.gnome.org/NetworkManager/developers/api/09/spec.html#org.freedesktop.NetworkManager.AccessPoint\n\n    Please note that this does not set any global properties.\n\n    Returns the new object path.\n    '''\n    dev_obj = dbusmock.get_object(dev_path)\n    ap_path = '/org/freedesktop/NetworkManager/AccessPoint/' + ap_name\n    if ap_path in dev_obj.access_points:\n        raise dbus.exceptions.DBusException(\n            'Access point %s on device %s already exists' % (ap_name, dev_path),\n            name=MANAGER_IFACE + '.AlreadyExists')\n\n    flags = NM80211ApFlags.NM_802_11_AP_FLAGS_PRIVACY\n    if security == NM80211ApSecurityFlags.NM_802_11_AP_SEC_NONE:\n        flags = NM80211ApFlags.NM_802_11_AP_FLAGS_NONE\n\n    self.AddObject(ap_path,\n                   ACCESS_POINT_IFACE,\n                   {'Ssid': dbus.ByteArray(ssid.encode('UTF-8')),\n                    'HwAddress': dbus.String(hw_address),\n                    'Flags': dbus.UInt32(flags),\n                    'LastSeen': dbus.Int32(1),\n                    'Frequency': dbus.UInt32(frequency),\n                    'MaxBitrate': dbus.UInt32(rate),\n                    'Mode': dbus.UInt32(mode),\n                    'RsnFlags': dbus.UInt32(security),\n                    'WpaFlags': dbus.UInt32(security),\n                    'Strength': dbus.Byte(strength)},\n                   [])\n    self.object_manager_emit_added(ap_path)\n\n    dev_obj.access_points.append(ap_path)\n\n    aps = dev_obj.Get(WIRELESS_DEVICE_IFACE, 'AccessPoints')\n    aps.append(ap_path)\n    dev_obj.Set(WIRELESS_DEVICE_IFACE, 'AccessPoints', aps)\n\n    dev_obj.EmitSignal(WIRELESS_DEVICE_IFACE, 'AccessPointAdded', 'o', [ap_path])\n\n    return ap_path", "response": "This method adds an access point to an existing WiFi device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an available connection to an existing WiFi device and access point.", "response": "def AddWiFiConnection(self, dev_path, connection_name, ssid_name, key_mgmt):\n    '''Add an available connection to an existing WiFi device and access point.\n\n    You have to specify WiFi Device path, Connection object name,\n    SSID and key management.\n\n    The SSID must match one of the previously created access points.\n\n    Please note that this does not set any global properties.\n\n    Returns the new object path.\n    '''\n\n    dev_obj = dbusmock.get_object(dev_path)\n    connection_path = '/org/freedesktop/NetworkManager/Settings/' + connection_name\n    connections = dev_obj.Get(DEVICE_IFACE, 'AvailableConnections')\n\n    settings_obj = dbusmock.get_object(SETTINGS_OBJ)\n    main_connections = settings_obj.ListConnections()\n\n    ssid = ssid_name.encode('UTF-8')\n\n    # Find the access point by ssid\n    access_point = None\n    access_points = dev_obj.access_points\n    for ap_path in access_points:\n        ap = dbusmock.get_object(ap_path)\n        if ap.Get(ACCESS_POINT_IFACE, 'Ssid') == ssid:\n            access_point = ap\n            break\n\n    if not access_point:\n        raise dbus.exceptions.DBusException(\n            'Access point with SSID [%s] could not be found' % (ssid_name),\n            name=MANAGER_IFACE + '.DoesNotExist')\n\n    hw_address = access_point.Get(ACCESS_POINT_IFACE, 'HwAddress')\n    mode = access_point.Get(ACCESS_POINT_IFACE, 'Mode')\n    security = access_point.Get(ACCESS_POINT_IFACE, 'WpaFlags')\n\n    if connection_path in connections or connection_path in main_connections:\n        raise dbus.exceptions.DBusException(\n            'Connection %s on device %s already exists' % (connection_name, dev_path),\n            name=MANAGER_IFACE + '.AlreadyExists')\n\n    # Parse mac address string into byte array\n    mac_bytes = binascii.unhexlify(hw_address.replace(':', ''))\n\n    settings = {\n        '802-11-wireless': {\n            'seen-bssids': [hw_address],\n            'ssid': dbus.ByteArray(ssid),\n            'mac-address': dbus.ByteArray(mac_bytes),\n            'mode': InfrastructureMode.NAME_MAP[mode]\n        },\n        'connection': {\n            'timestamp': dbus.UInt64(1374828522),\n            'type': '802-11-wireless',\n            'id': ssid_name,\n            'uuid': str(uuid.uuid4())\n        },\n    }\n\n    if security != NM80211ApSecurityFlags.NM_802_11_AP_SEC_NONE:\n        settings['802-11-wireless']['security'] = '802-11-wireless-security'\n        settings['802-11-wireless-security'] = NM80211ApSecurityFlags.NAME_MAP[security]\n\n    self.AddObject(connection_path,\n                   CSETTINGS_IFACE,\n                   {\n                       'Unsaved': False\n                   },\n                   [\n                       ('Delete', '', '', 'self.ConnectionDelete(self)'),\n                       ('GetSettings', '', 'a{sa{sv}}', 'ret = self.ConnectionGetSettings(self)'),\n                       ('GetSecrets', 's', 'a{sa{sv}}', 'ret = self.ConnectionGetSecrets(self, args[0])'),\n                       ('Update', 'a{sa{sv}}', '', 'self.ConnectionUpdate(self, args[0])'),\n                   ])\n    self.object_manager_emit_added(connection_path)\n\n    connection_obj = dbusmock.get_object(connection_path)\n    connection_obj.settings = settings\n    connection_obj.connection_path = connection_path\n    connection_obj.ConnectionDelete = ConnectionDelete\n    connection_obj.ConnectionGetSettings = ConnectionGetSettings\n    connection_obj.ConnectionGetSecrets = ConnectionGetSecrets\n    connection_obj.ConnectionUpdate = ConnectionUpdate\n\n    connections.append(dbus.ObjectPath(connection_path))\n    dev_obj.Set(DEVICE_IFACE, 'AvailableConnections', connections)\n\n    main_connections.append(connection_path)\n    settings_obj.Set(SETTINGS_IFACE, 'Connections', main_connections)\n\n    settings_obj.EmitSignal(SETTINGS_IFACE, 'NewConnection', 'o', [ap_path])\n\n    return connection_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an active connection to an existing WiFi device.", "response": "def AddActiveConnection(self, devices, connection_device, specific_object, name, state):\n    '''Add an active connection to an existing WiFi device.\n\n    You have to a list of the involved WiFi devices, the connection path,\n    the access point path, ActiveConnection object name and connection\n    state.\n\n    Please note that this does not set any global properties.\n\n    Returns the new object path.\n    '''\n\n    conn_obj = dbusmock.get_object(connection_device)\n    settings = conn_obj.settings\n    conn_uuid = settings['connection']['uuid']\n    conn_type = settings['connection']['type']\n\n    device_objects = [dbus.ObjectPath(dev) for dev in devices]\n\n    active_connection_path = '/org/freedesktop/NetworkManager/ActiveConnection/' + name\n    self.AddObject(active_connection_path,\n                   ACTIVE_CONNECTION_IFACE,\n                   {\n                       'Devices': dbus.Array(device_objects, signature='o'),\n                       'Default6': False,\n                       'Default': True,\n                       'Type': conn_type,\n                       'Vpn': (conn_type == 'vpn'),\n                       'Connection': dbus.ObjectPath(connection_device),\n                       'Master': dbus.ObjectPath('/'),\n                       'SpecificObject': dbus.ObjectPath(specific_object),\n                       'Uuid': conn_uuid,\n                       'State': dbus.UInt32(state),\n                   },\n                   [])\n\n    for dev_path in devices:\n        self.SetDeviceActive(dev_path, active_connection_path)\n\n    self.object_manager_emit_added(active_connection_path)\n\n    NM = dbusmock.get_object(MANAGER_OBJ)\n    active_connections = NM.Get(MANAGER_IFACE, 'ActiveConnections')\n    active_connections.append(dbus.ObjectPath(active_connection_path))\n    NM.SetProperty(MANAGER_OBJ, MANAGER_IFACE, 'ActiveConnections', active_connections)\n\n    return active_connection_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the specified access point from the specified device.", "response": "def RemoveAccessPoint(self, dev_path, ap_path):\n    '''Remove the specified access point.\n\n    You have to specify the device to remove the access point from, and the\n    path of the access point.\n\n    Please note that this does not set any global properties.\n    '''\n\n    dev_obj = dbusmock.get_object(dev_path)\n\n    aps = dev_obj.Get(WIRELESS_DEVICE_IFACE, 'AccessPoints')\n    aps.remove(ap_path)\n    dev_obj.Set(WIRELESS_DEVICE_IFACE, 'AccessPoints', aps)\n\n    dev_obj.access_points.remove(ap_path)\n\n    dev_obj.EmitSignal(WIRELESS_DEVICE_IFACE, 'AccessPointRemoved', 'o', [ap_path])\n\n    self.object_manager_emit_removed(ap_path)\n    self.RemoveObject(ap_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef RemoveWifiConnection(self, dev_path, connection_path):\n    '''Remove the specified WiFi connection.\n\n    You have to specify the device to remove the connection from, and the\n    path of the Connection.\n\n    Please note that this does not set any global properties.\n    '''\n\n    dev_obj = dbusmock.get_object(dev_path)\n    settings_obj = dbusmock.get_object(SETTINGS_OBJ)\n\n    connections = dev_obj.Get(DEVICE_IFACE, 'AvailableConnections')\n    main_connections = settings_obj.ListConnections()\n\n    if connection_path not in connections and connection_path not in main_connections:\n        return\n\n    connections.remove(dbus.ObjectPath(connection_path))\n    dev_obj.Set(DEVICE_IFACE, 'AvailableConnections', connections)\n\n    main_connections.remove(connection_path)\n    settings_obj.Set(SETTINGS_IFACE, 'Connections', main_connections)\n\n    settings_obj.EmitSignal(SETTINGS_IFACE, 'ConnectionRemoved', 'o', [connection_path])\n\n    connection_obj = dbusmock.get_object(connection_path)\n    connection_obj.EmitSignal(CSETTINGS_IFACE, 'Removed', '', [])\n\n    self.object_manager_emit_removed(connection_path)\n    self.RemoveObject(connection_path)", "response": "Remove the specified WiFi connection from the specified device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the specified ActiveConnection from the specified device.", "response": "def RemoveActiveConnection(self, dev_path, active_connection_path):\n    '''Remove the specified ActiveConnection.\n\n    You have to specify the device to remove the connection from, and the\n    path of the ActiveConnection.\n\n    Please note that this does not set any global properties.\n    '''\n    self.SetDeviceDisconnected(dev_path)\n\n    NM = dbusmock.get_object(MANAGER_OBJ)\n    active_connections = NM.Get(MANAGER_IFACE, 'ActiveConnections')\n\n    if active_connection_path not in active_connections:\n        return\n\n    active_connections.remove(dbus.ObjectPath(active_connection_path))\n    NM.SetProperty(MANAGER_OBJ, MANAGER_IFACE, 'ActiveConnections', active_connections)\n\n    self.object_manager_emit_removed(active_connection_path)\n    self.RemoveObject(active_connection_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef SettingsAddConnection(self, connection_settings):\n    '''Add a connection.\n\n    connection_settings is a String String Variant Map Map. See\n    https://developer.gnome.org/NetworkManager/0.9/spec.html\n        #type-String_String_Variant_Map_Map\n\n    If you omit uuid, this method adds one for you.\n    '''\n\n    if 'uuid' not in connection_settings['connection']:\n        connection_settings['connection']['uuid'] = str(uuid.uuid4())\n\n    NM = dbusmock.get_object(MANAGER_OBJ)\n    settings_obj = dbusmock.get_object(SETTINGS_OBJ)\n    main_connections = settings_obj.ListConnections()\n\n    # Mimic how NM names connections\n    count = 0\n    while True:\n        connection_obj_path = dbus.ObjectPath(SETTINGS_OBJ + '/' + str(count))\n        if connection_obj_path not in main_connections:\n            break\n        count += 1\n    connection_path = str(connection_obj_path)\n\n    self.AddObject(connection_path,\n                   CSETTINGS_IFACE,\n                   {\n                       'Unsaved': False\n                   },\n                   [\n                       ('Delete', '', '', 'self.ConnectionDelete(self)'),\n                       ('GetSettings', '', 'a{sa{sv}}', 'ret = self.ConnectionGetSettings(self)'),\n                       ('GetSecrets', 's', 'a{sa{sv}}', 'ret = self.ConnectionGetSecrets(self, args[0])'),\n                       ('Update', 'a{sa{sv}}', '', 'self.ConnectionUpdate(self, args[0])'),\n                   ])\n    self.object_manager_emit_added(connection_path)\n\n    connection_obj = dbusmock.get_object(connection_path)\n    connection_obj.settings = connection_settings\n    connection_obj.connection_path = connection_path\n    connection_obj.ConnectionDelete = ConnectionDelete\n    connection_obj.ConnectionGetSettings = ConnectionGetSettings\n    connection_obj.ConnectionGetSecrets = ConnectionGetSecrets\n    connection_obj.ConnectionUpdate = ConnectionUpdate\n\n    main_connections.append(connection_path)\n    settings_obj.Set(SETTINGS_IFACE, 'Connections', main_connections)\n\n    settings_obj.EmitSignal(SETTINGS_IFACE, 'NewConnection', 'o', [connection_path])\n\n    auto_connect = False\n    if 'autoconnect' in connection_settings['connection']:\n        auto_connect = connection_settings['connection']['autoconnect']\n\n    if auto_connect:\n        dev = None\n        devices = NM.GetDevices()\n\n        # Grab the first device.\n        if len(devices) > 0:\n            dev = devices[0]\n\n        if dev:\n            activate_connection(NM, connection_path, dev, connection_path)\n\n    return connection_path", "response": "This method adds a connection to the NM."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ConnectionUpdate(self, settings):\n    '''Update settings on a connection.\n\n    settings is a String String Variant Map Map. See\n    https://developer.gnome.org/NetworkManager/0.9/spec.html\n        #type-String_String_Variant_Map_Map\n    '''\n    connection_path = self.connection_path\n\n    NM = dbusmock.get_object(MANAGER_OBJ)\n    settings_obj = dbusmock.get_object(SETTINGS_OBJ)\n\n    main_connections = settings_obj.ListConnections()\n\n    if connection_path not in main_connections:\n        raise dbus.exceptions.DBusException(\n            'Connection %s does not exist' % connection_path,\n            name=MANAGER_IFACE + '.DoesNotExist',)\n\n    # Take care not to overwrite the secrets\n    for setting_name in settings:\n        setting = settings[setting_name]\n        for k in setting:\n            if setting_name not in self.settings:\n                self.settings[setting_name] = {}\n            self.settings[setting_name][k] = setting[k]\n\n    self.EmitSignal(CSETTINGS_IFACE, 'Updated', '', [])\n\n    auto_connect = False\n    if 'autoconnect' in settings['connection']:\n        auto_connect = settings['connection']['autoconnect']\n\n    if auto_connect:\n        dev = None\n        devices = NM.GetDevices()\n\n        # Grab the first device.\n        if len(devices) > 0:\n            dev = devices[0]\n\n        if dev:\n            activate_connection(NM, connection_path, dev, connection_path)\n\n    return connection_path", "response": "Update settings on a connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a connection from the NM.", "response": "def ConnectionDelete(self):\n    '''Deletes a connection.\n\n    This also\n        * removes the deleted connection from any device,\n        * removes any active connection(s) it might be associated with,\n        * removes it from the Settings interface,\n        * as well as deletes the object from the mock.\n\n    Note: If this was the only active connection, we change the global\n    connection state.\n    '''\n    connection_path = self.connection_path\n\n    NM = dbusmock.get_object(MANAGER_OBJ)\n    settings_obj = dbusmock.get_object(SETTINGS_OBJ)\n\n    # Find the associated active connection(s).\n    active_connections = NM.Get(MANAGER_IFACE, 'ActiveConnections')\n    associated_active_connections = []\n    for ac in active_connections:\n        ac_obj = dbusmock.get_object(ac)\n        ac_con = ac_obj.Get(ACTIVE_CONNECTION_IFACE, 'Connection')\n        if ac_con == connection_path:\n            associated_active_connections.append(ac)\n\n    # We found that the connection we are deleting are associated to all\n    # active connections and subsequently set the global state to\n    # disconnected.\n    if len(active_connections) == len(associated_active_connections):\n        self.SetGlobalConnectionState(NMState.NM_STATE_DISCONNECTED)\n\n    # Remove the connection from all associated devices.\n    # We also remove all associated active connections.\n    for dev_path in NM.GetDevices():\n        dev_obj = dbusmock.get_object(dev_path)\n        connections = dev_obj.Get(DEVICE_IFACE, 'AvailableConnections')\n\n        for ac in associated_active_connections:\n            NM.RemoveActiveConnection(dev_path, ac)\n\n        if connection_path not in connections:\n            continue\n\n        connections.remove(dbus.ObjectPath(connection_path))\n        dev_obj.Set(DEVICE_IFACE, 'AvailableConnections', connections)\n\n    # Remove the connection from the settings interface\n    main_connections = settings_obj.ListConnections()\n    if connection_path not in main_connections:\n        return\n    main_connections.remove(connection_path)\n    settings_obj.Set(SETTINGS_IFACE, 'Connections', main_connections)\n    settings_obj.EmitSignal(SETTINGS_IFACE, 'ConnectionRemoved', 'o', [connection_path])\n\n    # Remove the connection from the mock\n    connection_obj = dbusmock.get_object(connection_path)\n    connection_obj.EmitSignal(CSETTINGS_IFACE, 'Removed', '', [])\n\n    self.object_manager_emit_removed(connection_path)\n    self.RemoveObject(connection_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef AddDischargingBattery(self, device_name, model_name, percentage, seconds_to_empty):\n    '''Convenience method to add a discharging battery object\n\n    You have to specify a device name which must be a valid part of an object\n    path, e. g. \"mock_ac\", an arbitrary model name, the charge percentage, and\n    the seconds until the battery is empty.\n\n    Please note that this does not set any global properties such as\n    \"on-battery\".\n\n    Returns the new object path.\n    '''\n    path = '/org/freedesktop/UPower/devices/' + device_name\n    self.AddObject(path,\n                   DEVICE_IFACE,\n                   {\n                       'PowerSupply': dbus.Boolean(True, variant_level=1),\n                       'IsPresent': dbus.Boolean(True, variant_level=1),\n                       'Model': dbus.String(model_name, variant_level=1),\n                       'Percentage': dbus.Double(percentage, variant_level=1),\n                       'TimeToEmpty': dbus.Int64(seconds_to_empty, variant_level=1),\n                       'EnergyFull': dbus.Double(100.0, variant_level=1),\n                       'Energy': dbus.Double(percentage, variant_level=1),\n                       # UP_DEVICE_STATE_DISCHARGING\n                       'State': dbus.UInt32(2, variant_level=1),\n                       # UP_DEVICE_KIND_BATTERY\n                       'Type': dbus.UInt32(2, variant_level=1),\n                   },\n                   [])\n    self.EmitSignal(MAIN_IFACE, 'DeviceAdded', self.device_sig_type, [path])\n    return path", "response": "Convenience method to add a discharging battery object to the Unreal system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SetupDisplayDevice(self, type, state, percentage, energy, energy_full,\n                       energy_rate, time_to_empty, time_to_full, is_present,\n                       icon_name, warning_level):\n    '''Convenience method to configure DisplayDevice properties\n\n    This calls Set() for all properties that the DisplayDevice is defined to\n    have, and is shorter if you have to completely set it up instead of\n    changing just one or two properties.\n\n    This is only available when mocking the 1.0 API.\n    '''\n    if not self.api1:\n        raise dbus.exceptions.DBusException(\n            'SetupDisplayDevice() can only be used with the 1.0 API',\n            name=MOCK_IFACE + '.APIVersion')\n\n    display_props = mockobject.objects[self.p_display_dev]\n    display_props.Set(DEVICE_IFACE, 'Type',\n                      dbus.UInt32(type))\n    display_props.Set(DEVICE_IFACE, 'State',\n                      dbus.UInt32(state))\n    display_props.Set(DEVICE_IFACE, 'Percentage',\n                      percentage)\n    display_props.Set(DEVICE_IFACE, 'Energy', energy)\n    display_props.Set(DEVICE_IFACE, 'EnergyFull',\n                      energy_full)\n    display_props.Set(DEVICE_IFACE, 'EnergyRate',\n                      energy_rate)\n    display_props.Set(DEVICE_IFACE, 'TimeToEmpty',\n                      dbus.Int64(time_to_empty))\n    display_props.Set(DEVICE_IFACE, 'TimeToFull',\n                      dbus.Int64(time_to_full))\n    display_props.Set(DEVICE_IFACE, 'IsPresent',\n                      is_present)\n    display_props.Set(DEVICE_IFACE, 'IconName',\n                      icon_name)\n    display_props.Set(DEVICE_IFACE, 'WarningLevel',\n                      dbus.UInt32(warning_level))", "response": "Convenience method to configure DisplayDevice properties."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetDeviceProperties(self, object_path, properties):\n    '''Convenience method to Set a device's properties.\n\n    object_path: the device to update\n    properties: dictionary of keys to dbus variants.\n\n    If the 1.0 API is being mocked, changing this property will trigger\n    the device's PropertiesChanged signal; otherwise, the older\n    org.freedesktop.UPower DeviceChanged signal will be emitted.\n    '''\n    device = dbusmock.get_object(object_path)\n\n    # set the properties\n    for key, value in properties.items():\n        device.Set(DEVICE_IFACE, key, value)\n\n    # notify the listeners\n    if not self.api1:\n        self.EmitSignal(MAIN_IFACE, 'DeviceChanged', 's', [object_path])", "response": "Convenience method to set a device s properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AddSeat(self, seat):\n    '''Convenience method to add a seat.\n\n    Return the object path of the new seat.\n    '''\n    seat_path = '/org/freedesktop/login1/seat/' + seat\n    if seat_path in mockobject.objects:\n        raise dbus.exceptions.DBusException('Seat %s already exists' % seat,\n                                            name=MOCK_IFACE + '.SeatExists')\n\n    self.AddObject(seat_path,\n                   'org.freedesktop.login1.Seat',\n                   {\n                       'Sessions': dbus.Array([], signature='(so)'),\n                       'CanGraphical': False,\n                       'CanMultiSession': True,\n                       'CanTTY': False,\n                       'IdleHint': False,\n                       'ActiveSession': ('', dbus.ObjectPath('/')),\n                       'Id': seat,\n                       'IdleSinceHint': dbus.UInt64(0),\n                       'IdleSinceHintMonotonic': dbus.UInt64(0),\n                   },\n                   [\n                       ('ActivateSession', 's', '', ''),\n                       ('Terminate', '', '', '')\n                   ])\n\n    return seat_path", "response": "Convenience method to add a seat.\n    Return the object path of the new seat. Seat exists if it does not exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets up this mock object as a D - Bus ObjectManager.", "response": "def _set_up_object_manager(self):\n        '''Set up this mock object as a D-Bus ObjectManager.'''\n        if self.path == '/':\n            cond = 'k != \\'/\\''\n        else:\n            cond = 'k.startswith(\\'%s/\\')' % self.path\n\n        self.AddMethod(OBJECT_MANAGER_IFACE,\n                       'GetManagedObjects', '', 'a{oa{sa{sv}}}',\n                       'ret = {dbus.ObjectPath(k): objects[k].props ' +\n                       '  for k in objects.keys() if ' + cond + '}')\n        self.object_manager = self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a property value in an interface.", "response": "def Set(self, interface_name, property_name, value, *args, **kwargs):\n        '''Standard D-Bus API for setting a property value'''\n\n        self.log('Set %s.%s%s' % (interface_name,\n                                  property_name,\n                                  self.format_args((value,))))\n\n        try:\n            iface_props = self.props[interface_name]\n        except KeyError:\n            raise dbus.exceptions.DBusException(\n                'no such interface ' + interface_name,\n                name=self.interface + '.UnknownInterface')\n\n        if property_name not in iface_props:\n            raise dbus.exceptions.DBusException(\n                'no such property ' + property_name,\n                name=self.interface + '.UnknownProperty')\n\n        iface_props[property_name] = value\n\n        self.EmitSignal('org.freedesktop.DBus.Properties',\n                        'PropertiesChanged',\n                        'sa{sv}as',\n                        [interface_name,\n                         dbus.Dictionary({property_name: value}, signature='sv'),\n                         dbus.Array([], signature='s')\n                        ])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddObject(self, path, interface, properties, methods):\n        '''Add a new D-Bus object to the mock\n\n        path: D-Bus object path\n        interface: Primary D-Bus interface name of this object (where\n                   properties and methods will be put on)\n        properties: A property_name (string) \u2192 value map with initial\n                    properties on \"interface\"\n        methods: An array of 4-tuples (name, in_sig, out_sig, code) describing\n                 methods to add to \"interface\"; see AddMethod() for details of\n                 the tuple values\n\n        If this is a D-Bus ObjectManager instance, the InterfacesAdded signal\n        will *not* be emitted for the object automatically; it must be emitted\n        manually if desired. This is because AddInterface may be called after\n        AddObject, but before the InterfacesAdded signal should be emitted.\n\n        Example:\n        dbus_proxy.AddObject('/com/example/Foo/Manager',\n                             'com.example.Foo.Control',\n                             {\n                                 'state': dbus.String('online', variant_level=1),\n                             },\n                             [\n                                 ('Start', '', '', ''),\n                                 ('EchoInt', 'i', 'i', 'ret = args[0]'),\n                                 ('GetClients', '', 'ao', 'ret = [\"/com/example/Foo/Client1\"]'),\n                             ])\n        '''\n        if path in objects:\n            raise dbus.exceptions.DBusException(\n                'object %s already exists' % path,\n                name='org.freedesktop.DBus.Mock.NameError')\n\n        obj = DBusMockObject(self.bus_name,\n                             path,\n                             interface,\n                             properties)\n        # make sure created objects inherit the log file stream\n        obj.logfile = self.logfile\n        obj.object_manager = self.object_manager\n        obj.is_logfile_owner = False\n        obj.AddMethods(interface, methods)\n\n        objects[path] = obj", "response": "Add a new object to the mock object manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef RemoveObject(self, path):\n        '''Remove a D-Bus object from the mock\n\n        As with AddObject, this will *not* emit the InterfacesRemoved signal if\n        it\u2019s an ObjectManager instance.\n        '''\n        try:\n            objects[path].remove_from_connection()\n            del objects[path]\n        except KeyError:\n            raise dbus.exceptions.DBusException(\n                'object %s does not exist' % path,\n                name='org.freedesktop.DBus.Mock.NameError')", "response": "Remove an object from the mock."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the state of the object.", "response": "def Reset(self):\n        '''Reset the mock object state.\n\n        Remove all mock objects from the bus and tidy up so the state is as if\n        python-dbusmock had just been restarted. If the mock object was\n        originally created with a template (from the command line, the Python\n        API or by calling AddTemplate over D-Bus), it will be\n        re-instantiated with that template.\n        '''\n        # Clear other existing objects.\n        for obj_name, obj in objects.items():\n            if obj_name != self.path:\n                obj.remove_from_connection()\n        objects.clear()\n\n        # Reinitialise our state. Carefully remove new methods from our dict;\n        # they don't not actually exist if they are a statically defined\n        # template function\n        for method_name in self.methods[self.interface]:\n            try:\n                delattr(self.__class__, method_name)\n            except AttributeError:\n                pass\n\n        self._reset({})\n\n        if self._template is not None:\n            self.AddTemplate(self._template, self._template_parameters)\n\n        objects[self.path] = self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddMethod(self, interface, name, in_sig, out_sig, code):\n        '''Add a method to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the method to the object's main\n                   interface (as specified on construction).\n        name: Name of the method\n        in_sig: Signature of input arguments; for example \"ias\" for a method\n                that takes an int32 and a string array as arguments; see\n                http://dbus.freedesktop.org/doc/dbus-specification.html#message-protocol-signatures\n        out_sig: Signature of output arguments; for example \"s\" for a method\n                 that returns a string; use '' for methods that do not return\n                 anything.\n        code: Python 3 code to run in the method call; you have access to the\n              arguments through the \"args\" list, and can set the return value\n              by assigning a value to the \"ret\" variable. You can also read the\n              global \"objects\" variable, which is a dictionary mapping object\n              paths to DBusMockObject instances.\n\n              For keeping state across method calls, you are free to use normal\n              Python members of the \"self\" object, which will be persistent for\n              the whole mock's life time. E. g. you can have a method with\n              \"self.my_state = True\", and another method that returns it with\n              \"ret = self.my_state\".\n\n              When specifying '', the method will not do anything (except\n              logging) and return None.\n        '''\n        if not interface:\n            interface = self.interface\n        n_args = len(dbus.Signature(in_sig))\n\n        # we need to have separate methods for dbus-python, so clone\n        # mock_method(); using message_keyword with this dynamic approach fails\n        # because inspect cannot handle those, so pass on interface and method\n        # name as first positional arguments\n        method = lambda self, *args, **kwargs: DBusMockObject.mock_method(\n            self, interface, name, in_sig, *args, **kwargs)\n\n        # we cannot specify in_signature here, as that trips over a consistency\n        # check in dbus-python; we need to set it manually instead\n        dbus_method = dbus.service.method(interface,\n                                          out_signature=out_sig)(method)\n        dbus_method.__name__ = str(name)\n        dbus_method._dbus_in_signature = in_sig\n        dbus_method._dbus_args = ['arg%i' % i for i in range(1, n_args + 1)]\n\n        # for convenience, add mocked methods on the primary interface as\n        # callable methods\n        if interface == self.interface:\n            setattr(self.__class__, name, dbus_method)\n\n        self.methods.setdefault(interface, {})[str(name)] = (in_sig, out_sig, code, dbus_method)", "response": "Add a method to this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AddMethods(self, interface, methods):\n        '''Add several methods to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the method to the object's main\n                   interface (as specified on construction).\n        methods: list of 4-tuples (name, in_sig, out_sig, code) describing one\n                 method each. See AddMethod() for details of the tuple values.\n        '''\n        for method in methods:\n            self.AddMethod(interface, *method)", "response": "Add several methods to this object s main\n                   interface."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AddProperty(self, interface, name, value):\n        '''Add property to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the property to the object's main\n                   interface (as specified on construction).\n        name: Property name.\n        value: Property value.\n        '''\n        if not interface:\n            interface = self.interface\n        try:\n            self.props[interface][name]\n            raise dbus.exceptions.DBusException(\n                'property %s already exists' % name,\n                name=self.interface + '.PropertyExists')\n        except KeyError:\n            # this is what we expect\n            pass\n\n        # copy.copy removes one level of variant-ness, which means that the\n        # types get exported in introspection data correctly, but we can't do\n        # this for container types.\n        if not (isinstance(value, dbus.Dictionary) or isinstance(value, dbus.Array)):\n            value = copy.copy(value)\n\n        self.props.setdefault(interface, {})[name] = value", "response": "Add property to this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds several properties to this object.", "response": "def AddProperties(self, interface, properties):\n        '''Add several properties to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the property to the object's main\n                   interface (as specified on construction).\n        properties: A property_name (string) \u2192 value map\n        '''\n        for k, v in properties.items():\n            self.AddProperty(interface, k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddTemplate(self, template, parameters):\n        '''Load a template into the mock.\n\n        python-dbusmock ships a set of standard mocks for common system\n        services such as UPower and NetworkManager. With these the actual tests\n        become a lot simpler, as they only have to set up the particular\n        properties for the tests, and not the skeleton of common properties,\n        interfaces, and methods.\n\n        template: Name of the template to load or the full path to a *.py file\n                  for custom templates. See \"pydoc dbusmock.templates\" for a\n                  list of available templates from python-dbusmock package, and\n                  \"pydoc dbusmock.templates.NAME\" for documentation about\n                  template NAME.\n        parameters: A parameter (string) \u2192 value (variant) map, for\n                    parameterizing templates. Each template can define their\n                    own, see documentation of that particular template for\n                    details.\n        '''\n        try:\n            module = load_module(template)\n        except ImportError as e:\n            raise dbus.exceptions.DBusException('Cannot add template %s: %s' % (template, str(e)),\n                                                name='org.freedesktop.DBus.Mock.TemplateError')\n\n        # If the template specifies this is an ObjectManager, set that up\n        if hasattr(module, 'IS_OBJECT_MANAGER') and module.IS_OBJECT_MANAGER:\n            self._set_up_object_manager()\n\n        # pick out all D-Bus service methods and add them to our interface\n        for symbol in dir(module):\n            fn = getattr(module, symbol)\n            if ('_dbus_interface' in dir(fn) and ('_dbus_is_signal' not in dir(fn) or not fn._dbus_is_signal)):\n                # for dbus-python compatibility, add methods as callables\n                setattr(self.__class__, symbol, fn)\n                self.methods.setdefault(fn._dbus_interface, {})[str(symbol)] = (\n                    fn._dbus_in_signature,\n                    fn._dbus_out_signature, '', fn\n                )\n\n        if parameters is None:\n            parameters = {}\n\n        module.load(self, parameters)\n        # save the given template and parameters for re-instantiation on\n        # Reset()\n        self._template = template\n        self._template_parameters = parameters", "response": "Add a template into the mock."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef EmitSignal(self, interface, name, signature, args):\n        '''Emit a signal from the object.\n\n        interface: D-Bus interface to send the signal from. For convenience you\n                   can specify '' here to add the method to the object's main\n                   interface (as specified on construction).\n        name: Name of the signal\n        signature: Signature of input arguments; for example \"ias\" for a signal\n                that takes an int32 and a string array as arguments; see\n                http://dbus.freedesktop.org/doc/dbus-specification.html#message-protocol-signatures\n        args: variant array with signal arguments; must match order and type in\n              \"signature\"\n        '''\n        if not interface:\n            interface = self.interface\n\n        # convert types of arguments according to signature, using\n        # MethodCallMessage.append(); this will also provide type/length\n        # checks, except for the case of an empty signature\n        if signature == '' and len(args) > 0:\n            raise TypeError('Fewer items found in D-Bus signature than in Python arguments')\n        m = dbus.connection.MethodCallMessage('a.b', '/a', 'a.b', 'a')\n        m.append(signature=signature, *args)\n        args = m.get_args_list()\n\n        fn = lambda self, *args: self.log('emit %s.%s%s' % (interface, name, self.format_args(args)))\n        fn.__name__ = str(name)\n        dbus_fn = dbus.service.signal(interface)(fn)\n        dbus_fn._dbus_signature = signature\n        dbus_fn._dbus_args = ['arg%i' % i for i in range(1, len(args) + 1)]\n\n        dbus_fn(self, *args)", "response": "Emits a signal from the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetMethodCalls(self, method):\n        '''List all the logged calls of a particular method.\n\n        Return a list of (timestamp, args_list) tuples.\n        '''\n        return [(row[0], row[2]) for row in self.call_log if row[1] == method]", "response": "List all the logged calls of a particular method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmaster mock method. This gets instantiated in AddMethod.", "response": "def mock_method(self, interface, dbus_method, in_signature, *args, **kwargs):\n        '''Master mock method.\n\n        This gets \"instantiated\" in AddMethod(). Execute the code snippet of\n        the method and return the \"ret\" variable if it was set.\n        '''\n        # print('mock_method', dbus_method, self, in_signature, args, kwargs, file=sys.stderr)\n\n        # convert types of arguments according to signature, using\n        # MethodCallMessage.append(); this will also provide type/length\n        # checks, except for the case of an empty signature\n        if in_signature == '' and len(args) > 0:\n            raise TypeError('Fewer items found in D-Bus signature than in Python arguments')\n        m = dbus.connection.MethodCallMessage('a.b', '/a', 'a.b', 'a')\n        m.append(signature=in_signature, *args)\n        args = m.get_args_list()\n\n        self.log(dbus_method + self.format_args(args))\n        self.call_log.append((int(time.time()), str(dbus_method), args))\n        self.MethodCalled(dbus_method, args)\n\n        # The code may be a Python 3 string to interpret, or may be a function\n        # object (if AddMethod was called from within Python itself, rather than\n        # over D-Bus).\n        code = self.methods[interface][dbus_method][2]\n        if code and isinstance(code, types.FunctionType):\n            return code(self, *args)\n        elif code:\n            loc = locals().copy()\n            exec(code, globals(), loc)\n            if 'ret' in loc:\n                return loc['ret']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat a D - Bus argument tuple into an appropriate logging string.", "response": "def format_args(self, args):\n        '''Format a D-Bus argument tuple into an appropriate logging string.'''\n\n        def format_arg(a):\n            if isinstance(a, dbus.Boolean):\n                return str(bool(a))\n            if isinstance(a, dbus.Byte):\n                return str(int(a))\n            if isinstance(a, int) or isinstance(a, long):\n                return str(a)\n            if isinstance(a, str):\n                return '\"' + str(a) + '\"'\n            if isinstance(a, unicode):  # Python 2 only\n                return '\"' + repr(a.encode('UTF-8'))[1:-1] + '\"'\n            if isinstance(a, list):\n                return '[' + ', '.join([format_arg(x) for x in a]) + ']'\n            if isinstance(a, dict):\n                fmta = '{'\n                first = True\n                for k, v in a.items():\n                    if first:\n                        first = False\n                    else:\n                        fmta += ', '\n                    fmta += format_arg(k) + ': ' + format_arg(v)\n                return fmta + '}'\n\n            # fallback\n            return repr(a)\n\n        s = ''\n        for a in args:\n            if s:\n                s += ' '\n            s += format_arg(a)\n        if s:\n            s = ' ' + s\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log(self, msg):\n        '''Log a message, prefixed with a timestamp.\n\n        If a log file was specified in the constructor, it is written there,\n        otherwise it goes to stdout.\n        '''\n        if self.logfile:\n            fd = self.logfile.fileno()\n        else:\n            fd = sys.stdout.fileno()\n\n        os.write(fd, ('%.3f %s\\n' % (time.time(), msg)).encode('UTF-8'))", "response": "Log a message prefixed with a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns XML description of this object s interfaces methods and signals.", "response": "def Introspect(self, object_path, connection):\n        '''Return XML description of this object's interfaces, methods and signals.\n\n        This wraps dbus-python's Introspect() method to include the dynamic\n        methods and properties.\n        '''\n        # temporarily add our dynamic methods\n        cls = self.__class__.__module__ + '.' + self.__class__.__name__\n        orig_interfaces = self._dbus_class_table[cls]\n\n        mock_interfaces = orig_interfaces.copy()\n        for interface, methods in self.methods.items():\n            for method in methods:\n                mock_interfaces.setdefault(interface, {})[method] = self.methods[interface][method][3]\n        self._dbus_class_table[cls] = mock_interfaces\n\n        xml = dbus.service.Object.Introspect(self, object_path, connection)\n\n        tree = ElementTree.fromstring(xml)\n\n        for name in self.props:\n            # We might have properties for new interfaces we don't know about\n            # yet. Try to find an existing <interface> node named after our\n            # interface to append to, and create one if we can't.\n            interface = tree.find(\".//interface[@name='%s']\" % name)\n            if interface is None:\n                interface = ElementTree.Element(\"interface\", {\"name\": name})\n                tree.append(interface)\n\n            for prop, val in self.props[name].items():\n                if val is None:\n                    # can't guess type from None, skip\n                    continue\n                elem = ElementTree.Element(\"property\", {\n                    \"name\": prop,\n                    # We don't store the signature anywhere, so guess it.\n                    \"type\": dbus.lowlevel.Message.guess_signature(val),\n                    \"access\": \"readwrite\"})\n\n                interface.append(elem)\n\n        xml = ElementTree.tostring(tree, encoding='utf8', method='xml').decode('utf8')\n\n        # restore original class table\n        self._dbus_class_table[cls] = orig_interfaces\n\n        return xml"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef RemoveSession(self, session_path):\n    '''OBEX method to remove an existing transfer session.\n\n    This takes the path of the transfer Session object and removes it.\n    '''\n\n    manager = mockobject.objects['/']\n\n    # Remove all the session's transfers.\n    transfer_id = 0\n    while session_path + '/transfer' + str(transfer_id) in mockobject.objects:\n        transfer_path = session_path + '/transfer' + str(transfer_id)\n        transfer_id += 1\n\n        self.RemoveObject(transfer_path)\n\n        manager.EmitSignal(OBJECT_MANAGER_IFACE, 'InterfacesRemoved',\n                           'oas', [\n                               dbus.ObjectPath(transfer_path),\n                               [TRANSFER_IFACE],\n                           ])\n\n    # Remove the session itself.\n    self.RemoveObject(session_path)\n\n    manager.EmitSignal(OBJECT_MANAGER_IFACE, 'InterfacesRemoved',\n                       'oas', [\n                           dbus.ObjectPath(session_path),\n                           [SESSION_IFACE, PHONEBOOK_ACCESS_IFACE],\n                       ])", "response": "OBEX method to remove an existing transfer session. This method removes an existing session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef PullAll(self, target_file, filters):\n    '''OBEX method to start a pull transfer of a phone book.\n\n    This doesn't complete the transfer; code to mock up activating and\n    completing the transfer must be provided by the test driver, as it\u2019s\n    too complex and test-specific to put here.\n\n    The target_file is the absolute path for a file which will have zero or\n    more vCards, separated by new-line characters, written to it if the method\n    is successful (and the transfer is completed). This target_file is actually\n    emitted in a TransferCreated signal, which is a special part of the mock\n    interface designed to be handled by the test driver, which should then\n    populate that file and call UpdateStatus on the Transfer object. The test\n    driver is responsible for deleting the file once the test is complete.\n\n    The filters parameter is a map of filters to be applied to the results\n    device-side before transmitting them back to the adapter.\n\n    Returns a tuple containing the path for a new Transfer D-Bus object\n    representing the transfer, and a map of the initial properties of that\n    Transfer object.\n    '''\n\n    # Find the first unused session ID.\n    session_path = self.path\n    transfer_id = 0\n    while session_path + '/transfer' + str(transfer_id) in mockobject.objects:\n        transfer_id += 1\n\n    transfer_path = session_path + '/transfer' + str(transfer_id)\n\n    # Create a new temporary file to transfer to.\n    temp_file = tempfile.NamedTemporaryFile(suffix='.vcf',\n                                            prefix='tmp-bluez5-obex-PullAll_',\n                                            delete=False)\n    filename = os.path.abspath(temp_file.name)\n\n    props = {\n        'Status': dbus.String('queued', variant_level=1),\n        'Session': dbus.ObjectPath(session_path,\n                                   variant_level=1),\n        'Name': dbus.String(target_file, variant_level=1),\n        'Filename': dbus.String(filename, variant_level=1),\n        'Transferred': dbus.UInt64(0, variant_level=1),\n    }\n\n    self.AddObject(transfer_path,\n                   TRANSFER_IFACE,\n                   # Properties\n                   props,\n                   # Methods\n                   [\n                       ('Cancel', '', '', ''),  # Currently a no-op\n                   ])\n\n    transfer = mockobject.objects[transfer_path]\n    transfer.AddMethods(TRANSFER_MOCK_IFACE, [\n        ('UpdateStatus', 'b', '', UpdateStatus),\n    ])\n\n    manager = mockobject.objects['/']\n    manager.EmitSignal(OBJECT_MANAGER_IFACE, 'InterfacesAdded',\n                       'oa{sa{sv}}', [\n                           dbus.ObjectPath(transfer_path),\n                           {TRANSFER_IFACE: props},\n                       ])\n\n    # Emit a behind-the-scenes signal that a new transfer has been created.\n    manager.EmitSignal(OBEX_MOCK_IFACE, 'TransferCreated', 'sa{sv}s',\n                       [transfer_path, filters, filename])\n\n    return (transfer_path, props)", "response": "OBEX method to start a pull all of the phone books from the target file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmocking method to update the status of the current node.", "response": "def UpdateStatus(self, is_complete):\n    '''Mock method to update the transfer status.\n\n    If is_complete is False, this marks the transfer is active; otherwise it\n    marks the transfer as complete. It is an error to call this method after\n    calling it with is_complete as True.\n\n    In both cases, it updates the number of bytes transferred to be the current\n    size of the transfer file (whose filename was emitted in the\n    TransferCreated signal).\n    '''\n    status = 'complete' if is_complete else 'active'\n    transferred = os.path.getsize(self.props[TRANSFER_IFACE]['Filename'])\n\n    self.props[TRANSFER_IFACE]['Status'] = status\n    self.props[TRANSFER_IFACE]['Transferred'] = dbus.UInt64(transferred, variant_level=1)\n\n    self.EmitSignal(dbus.PROPERTIES_IFACE, 'PropertiesChanged', 'sa{sv}as', [\n        TRANSFER_IFACE,\n        {\n            'Status': dbus.String(status, variant_level=1),\n            'Transferred': dbus.UInt64(transferred, variant_level=1),\n        },\n        [],\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddModem(self, name, properties):\n    '''Convenience method to add a modem\n\n    You have to specify a device name which must be a valid part of an object\n    path, e. g. \"mock_ac\". For future extensions you can specify a \"properties\"\n    array, but no extra properties are supported for now.\n\n    Returns the new object path.\n    '''\n    path = '/' + name\n    self.AddObject(path,\n                   'org.ofono.Modem',\n                   {\n                       'Online': dbus.Boolean(True, variant_level=1),\n                       'Powered': dbus.Boolean(True, variant_level=1),\n                       'Lockdown': dbus.Boolean(False, variant_level=1),\n                       'Emergency': dbus.Boolean(False, variant_level=1),\n                       'Manufacturer': dbus.String('Fakesys', variant_level=1),\n                       'Model': dbus.String('Mock Modem', variant_level=1),\n                       'Revision': dbus.String('0815.42', variant_level=1),\n                       'Serial': dbus.String(new_modem_serial(self), variant_level=1),\n                       'Type': dbus.String('hardware', variant_level=1),\n                       'Interfaces': ['org.ofono.CallVolume',\n                                      'org.ofono.VoiceCallManager',\n                                      'org.ofono.NetworkRegistration',\n                                      'org.ofono.SimManager',\n                                      # 'org.ofono.MessageManager',\n                                      'org.ofono.ConnectionManager',\n                                      # 'org.ofono.NetworkTime'\n                                     ],\n                       # 'Features': ['sms', 'net', 'gprs', 'sim']\n                       'Features': ['gprs', 'net'],\n                   },\n                   [\n                       ('GetProperties', '', 'a{sv}', 'ret = self.GetAll(\"org.ofono.Modem\")'),\n                       ('SetProperty', 'sv', '', 'self.Set(\"org.ofono.Modem\", args[0], args[1]); '\n                                                 'self.EmitSignal(\"org.ofono.Modem\", \"PropertyChanged\",'\n                                                 ' \"sv\", [args[0], args[1]])'),\n                   ]\n                  )\n    obj = dbusmock.mockobject.objects[path]\n    obj.name = name\n    add_voice_call_api(obj)\n    add_netreg_api(obj)\n    add_simmanager_api(self, obj)\n    add_connectionmanager_api(obj)\n    self.modems.append(path)\n    props = obj.GetAll('org.ofono.Modem', dbus_interface=dbus.PROPERTIES_IFACE)\n    self.EmitSignal(MAIN_IFACE, 'ModemAdded', 'oa{sv}', [path, props])\n    return path", "response": "Convenience method to add a modem to the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_voice_call_api(mock):\n    '''Add org.ofono.VoiceCallManager API to a mock'''\n\n    # also add an emergency number which is not a real one, in case one runs a\n    # test case against a production ofono :-)\n    mock.AddProperty('org.ofono.VoiceCallManager', 'EmergencyNumbers', ['911', '13373'])\n\n    mock.calls = []  # object paths\n\n    mock.AddMethods('org.ofono.VoiceCallManager', [\n        ('GetProperties', '', 'a{sv}', 'ret = self.GetAll(\"org.ofono.VoiceCallManager\")'),\n        ('Transfer', '', '', ''),\n        ('SwapCalls', '', '', ''),\n        ('ReleaseAndAnswer', '', '', ''),\n        ('ReleaseAndSwap', '', '', ''),\n        ('HoldAndAnswer', '', '', ''),\n        ('SendTones', 's', '', ''),\n        ('PrivateChat', 'o', 'ao', NOT_IMPLEMENTED),\n        ('CreateMultiparty', '', 'o', NOT_IMPLEMENTED),\n        ('HangupMultiparty', '', '', NOT_IMPLEMENTED),\n        ('GetCalls', '', 'a(oa{sv})', 'ret = [(c, objects[c].GetAll(\"org.ofono.VoiceCall\")) for c in self.calls]')\n    ])", "response": "Add org. ofono. VoiceCallManager API to a mock"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds org. ofono. NetworkRegistration API to a mock", "response": "def add_netreg_api(mock):\n    '''Add org.ofono.NetworkRegistration API to a mock'''\n\n    # also add an emergency number which is not a real one, in case one runs a\n    # test case against a production ofono :-)\n    mock.AddProperties('org.ofono.NetworkRegistration', {\n        'Mode': 'auto',\n        'Status': 'registered',\n        'LocationAreaCode': _parameters.get('LocationAreaCode', 987),\n        'CellId': _parameters.get('CellId', 10203),\n        'MobileCountryCode': _parameters.get('MobileCountryCode', '777'),\n        'MobileNetworkCode': _parameters.get('MobileNetworkCode', '11'),\n        'Technology': _parameters.get('Technology', 'gsm'),\n        'Name': _parameters.get('Name', 'fake.tel'),\n        'Strength': _parameters.get('Strength', dbus.Byte(80)),\n        'BaseStation': _parameters.get('BaseStation', ''),\n    })\n\n    mock.AddObject('/%s/operator/op1' % mock.name,\n                   'org.ofono.NetworkOperator',\n                   {\n                       'Name': _parameters.get('Name', 'fake.tel'),\n                       'Status': 'current',\n                       'MobileCountryCode': _parameters.get('MobileCountryCode', '777'),\n                       'MobileNetworkCode': _parameters.get('MobileNetworkCode', '11'),\n                       'Technologies': [_parameters.get('Technology', 'gsm')],\n                   },\n                   [\n                       ('GetProperties', '', 'a{sv}', 'ret = self.GetAll(\"org.ofono.NetworkOperator\")'),\n                       ('Register', '', '', ''),\n                   ]  # noqa: silly pep8 error here about hanging indent\n                  )\n\n    mock.AddMethods('org.ofono.NetworkRegistration', [\n        ('GetProperties', '', 'a{sv}', 'ret = self.GetAll(\"org.ofono.NetworkRegistration\")'),\n        ('SetProperty', 'sv', '', 'self.Set(\"%(i)s\", args[0], args[1]); '\n         'self.EmitSignal(\"%(i)s\", \"PropertyChanged\", \"sv\", [args[0], args[1]])' % {'i': 'org.ofono.NetworkRegistration'}),\n        ('Register', '', '', ''),\n        ('GetOperators', '', 'a(oa{sv})', get_all_operators(mock)),\n        ('Scan', '', 'a(oa{sv})', get_all_operators(mock)),\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_simmanager_api(self, mock):\n    '''Add org.ofono.SimManager API to a mock'''\n\n    iface = 'org.ofono.SimManager'\n    mock.AddProperties(iface, {\n        'BarredDialing': _parameters.get('BarredDialing', False),\n        'CardIdentifier': _parameters.get('CardIdentifier', new_iccid(self)),\n        'FixedDialing': _parameters.get('FixedDialing', False),\n        'LockedPins': _parameters.get('LockedPins', dbus.Array([], signature='s')),\n        'MobileCountryCode': _parameters.get('MobileCountryCode', '310'),\n        'MobileNetworkCode': _parameters.get('MobileNetworkCode', '150'),\n        'PreferredLanguages': _parameters.get('PreferredLanguages', ['en']),\n        'Present': _parameters.get('Present', dbus.Boolean(True)),\n        'Retries': _parameters.get('Retries', dbus.Dictionary([[\"pin\", dbus.Byte(3)], [\"puk\", dbus.Byte(10)]])),\n        'PinRequired': _parameters.get('PinRequired', \"none\"),\n        'SubscriberNumbers': _parameters.get('SubscriberNumbers', ['123456789', '234567890']),\n        'SubscriberIdentity': _parameters.get('SubscriberIdentity', new_imsi(self)),\n    })\n    mock.AddMethods(iface, [\n        ('GetProperties', '', 'a{sv}', 'ret = self.GetAll(\"%s\")' % iface),\n        ('SetProperty', 'sv', '', 'self.Set(\"%(i)s\", args[0], args[1]); '\n         'self.EmitSignal(\"%(i)s\", \"PropertyChanged\", \"sv\", [args[0], args[1]])' % {'i': iface}),\n        ('ChangePin', 'sss', '', ''),\n\n        ('EnterPin', 'ss', '',\n         'correctPin = \"1234\"\\n'\n         'newRetries = self.Get(\"%(i)s\", \"Retries\")\\n'\n         'if args[0] == \"pin\" and args[1] != correctPin:\\n'\n         '    newRetries[\"pin\"] = dbus.Byte(newRetries[\"pin\"] - 1)\\n'\n         'elif args[0] == \"pin\":\\n'\n         '    newRetries[\"pin\"] = dbus.Byte(3)\\n'\n\n         'self.Set(\"%(i)s\", \"Retries\", newRetries)\\n'\n         'self.EmitSignal(\"%(i)s\", \"PropertyChanged\", \"sv\", [\"Retries\", newRetries])\\n'\n\n         'if args[0] == \"pin\" and args[1] != correctPin:\\n'\n         '    class Failed(dbus.exceptions.DBusException):\\n'\n         '        _dbus_error_name = \"org.ofono.Error.Failed\"\\n'\n         '    raise Failed(\"Operation failed\")' % {'i': iface}),\n\n        ('ResetPin', 'sss', '',\n         'correctPuk = \"12345678\"\\n'\n         'newRetries = self.Get(\"%(i)s\", \"Retries\")\\n'\n         'if args[0] == \"puk\" and args[1] != correctPuk:\\n'\n         '    newRetries[\"puk\"] = dbus.Byte(newRetries[\"puk\"] - 1)\\n'\n         'elif args[0] == \"puk\":\\n'\n         '    newRetries[\"pin\"] = dbus.Byte(3)\\n'\n         '    newRetries[\"puk\"] = dbus.Byte(10)\\n'\n\n         'self.Set(\"%(i)s\", \"Retries\", newRetries)\\n'\n         'self.EmitSignal(\"%(i)s\", \"PropertyChanged\", \"sv\", [\"Retries\", newRetries])\\n'\n\n         'if args[0] == \"puk\" and args[1] != correctPuk:\\n'\n         '    class Failed(dbus.exceptions.DBusException):\\n'\n         '        _dbus_error_name = \"org.ofono.Error.Failed\"\\n'\n         '    raise Failed(\"Operation failed\")' % {'i': iface}),\n\n        ('LockPin', 'ss', '', ''),\n        ('UnlockPin', 'ss', '', ''),\n    ])", "response": "Add org. ofono. SimManager API to a mock"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd org. ofono. ConnectionManager API to a mock", "response": "def add_connectionmanager_api(mock):\n    '''Add org.ofono.ConnectionManager API to a mock'''\n\n    iface = 'org.ofono.ConnectionManager'\n    mock.AddProperties(iface, {\n        'Attached': _parameters.get('Attached', True),\n        'Bearer': _parameters.get('Bearer', 'gprs'),\n        'RoamingAllowed': _parameters.get('RoamingAllowed', False),\n        'Powered': _parameters.get('ConnectionPowered', True),\n    })\n    mock.AddMethods(iface, [\n        ('GetProperties', '', 'a{sv}', 'ret = self.GetAll(\"%s\")' % iface),\n        ('SetProperty', 'sv', '', 'self.Set(\"%(i)s\", args[0], args[1]); '\n         'self.EmitSignal(\"%(i)s\", \"PropertyChanged\", \"sv\", [args[0], args[1]])' % {'i': iface}),\n        ('AddContext', 's', 'o', 'ret = \"/\"'),\n        ('RemoveContext', 'o', '', ''),\n        ('DeactivateAll', '', '', ''),\n        ('GetContexts', '', 'a(oa{sv})', 'ret = dbus.Array([])'),\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddAdapter(self, device_name, system_name):\n    '''Convenience method to add a Bluetooth adapter\n\n    You have to specify a device name which must be a valid part of an object\n    path, e. g. \"hci0\", and an arbitrary system name (pretty hostname).\n\n    Returns the new object path.\n    '''\n    path = '/org/bluez/' + device_name\n    adapter_properties = {\n        'UUIDs': dbus.Array([\n            # Reference:\n            # http://git.kernel.org/cgit/bluetooth/bluez.git/tree/lib/uuid.h\n            # PNP\n            '00001200-0000-1000-8000-00805f9b34fb',\n            # Generic Access Profile\n            '00001800-0000-1000-8000-00805f9b34fb',\n            # Generic Attribute Profile\n            '00001801-0000-1000-8000-00805f9b34fb',\n            # Audio/Video Remote Control Profile (remote)\n            '0000110e-0000-1000-8000-00805f9b34fb',\n            # Audio/Video Remote Control Profile (target)\n            '0000110c-0000-1000-8000-00805f9b34fb',\n        ], variant_level=1),\n        'Discoverable': dbus.Boolean(True, variant_level=1),\n        'Discovering': dbus.Boolean(True, variant_level=1),\n        'Pairable': dbus.Boolean(True, variant_level=1),\n        'Powered': dbus.Boolean(True, variant_level=1),\n        'Address': dbus.String('00:01:02:03:04:05', variant_level=1),\n        'Alias': dbus.String(system_name, variant_level=1),\n        'Modalias': dbus.String('usb:v1D6Bp0245d050A', variant_level=1),\n        'Name': dbus.String(system_name, variant_level=1),\n        # Reference:\n        # http://bluetooth-pentest.narod.ru/software/\n        # bluetooth_class_of_device-service_generator.html\n        'Class': dbus.UInt32(268, variant_level=1),  # Computer, Laptop\n        'DiscoverableTimeout': dbus.UInt32(180, variant_level=1),\n        'PairableTimeout': dbus.UInt32(180, variant_level=1),\n    }\n\n    self.AddObject(path,\n                   ADAPTER_IFACE,\n                   # Properties\n                   adapter_properties,\n                   # Methods\n                   [\n                       ('RemoveDevice', 'o', '', ''),\n                       ('StartDiscovery', '', '', ''),\n                       ('StopDiscovery', '', '', ''),\n                   ])\n\n    adapter = mockobject.objects[path]\n    adapter.AddMethods(MEDIA_IFACE, [\n        ('RegisterEndpoint', 'oa{sv}', '', ''),\n        ('UnregisterEndpoint', 'o', '', ''),\n    ])\n    adapter.AddMethods(NETWORK_SERVER_IFACE, [\n        ('Register', 'ss', '', ''),\n        ('Unregister', 's', '', ''),\n    ])\n\n    manager = mockobject.objects['/']\n    manager.EmitSignal(OBJECT_MANAGER_IFACE, 'InterfacesAdded',\n                       'oa{sa{sv}}', [\n                           dbus.ObjectPath(path),\n                           {ADAPTER_IFACE: adapter_properties},\n                       ])\n\n    return path", "response": "Convenience method to add a Bluetooth adapter to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an instance of a model instance.", "response": "def create(self, instance, errors):\n        \"\"\"\n        Create an instance of a model.\n\n        :param instance: The created model instance.\n        :param errors: Any errors.\n        :return: The created model instance, or a dictionary of errors.\n        \"\"\"\n        if errors:\n            return self.errors(errors)\n        return self.created(instance)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch(self, instance, errors):\n        if errors:\n            return self.errors(errors)\n        return self.updated(instance)", "response": "Partially update a model instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put(self, instance, errors):\n        if errors:\n            return self.errors(errors)\n        return self.updated(instance)", "response": "Update a model instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef created(self, instance, commit=True):\n        if commit:\n            self.session_manager.save(instance, commit=True)\n        return instance, HTTPStatus.CREATED", "response": "Returns the object and HTTPStatus. CREATED status code"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deleted(self, instance):\n        self.session_manager.delete(instance, commit=True)\n        return '', HTTPStatus.NO_CONTENT", "response": "This method deletes a record from the database"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef updated(self, instance):\n        self.session_manager.save(instance, commit=True)\n        return instance", "response": "Returns the object with status 200 if the object was updated"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef before_init_app(self, app: FlaskUnchained):\n        from .templates import (UnchainedJinjaEnvironment,\n                                UnchainedJinjaLoader)\n        app.jinja_environment = UnchainedJinjaEnvironment\n        app.jinja_options = {**app.jinja_options,\n                             'loader': UnchainedJinjaLoader(app)}\n        app.jinja_env.globals['url_for'] = url_for\n\n        for name in ['string', 'str']:\n            app.url_map.converters[name] = StringConverter", "response": "Configure the Jinja environment and template loader."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef after_init_app(self, app: FlaskUnchained):\n\n        from flask_wtf.csrf import generate_csrf\n\n        # send CSRF token in the cookie\n        @app.after_request\n        def set_csrf_cookie(response):\n            if response:\n                response.set_cookie('csrf_token', generate_csrf())\n            return response", "response": "Configure an after request hook to send the CSRF token in the cookie."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shell():\n    ctx = _get_shell_ctx()\n    try:\n        import IPython\n        IPython.embed(header=_get_shell_banner(), user_ns=ctx)\n    except ImportError:\n        import code\n        code.interact(banner=_get_shell_banner(verbose=True), local=ctx)", "response": "Runs a shell in the app context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nviews function to log a user in. Supports html and json requests. Supports json requests. Supports html and json requests. Supports json requests. Supports json requests.", "response": "def login(self):\n        \"\"\"\n        View function to log a user in. Supports html and json requests.\n        \"\"\"\n        form = self._get_form('SECURITY_LOGIN_FORM')\n        if form.validate_on_submit():\n            try:\n                self.security_service.login_user(form.user, form.remember.data)\n            except AuthenticationError as e:\n                form._errors = {'_error': [str(e)]}\n            else:\n                self.after_this_request(self._commit)\n                if request.is_json:\n                    return self.jsonify({'token': form.user.get_auth_token(),\n                                         'user': form.user})\n                self.flash(_('flask_unchained.bundles.security:flash.login'),\n                           category='success')\n                return self.redirect('SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')\n        else:\n            # FIXME-identity\n            identity_attrs = app.config.SECURITY_USER_IDENTITY_ATTRIBUTES\n            msg = f\"Invalid {', '.join(identity_attrs)} and/or password.\"\n\n            # we just want a single top-level form error\n            form._errors = {'_error': [msg]}\n            for field in form._fields.values():\n                field.errors = None\n\n        if form.errors and request.is_json:\n            return self.jsonify({'error': form.errors.get('_error')[0]},\n                                code=HTTPStatus.UNAUTHORIZED)\n\n        return self.render('login',\n                           login_user_form=form,\n                           **self.security.run_ctx_processor('login'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logout(self):\n        if current_user.is_authenticated:\n            self.security_service.logout_user()\n\n        if request.is_json:\n            return '', HTTPStatus.NO_CONTENT\n\n        self.flash(_('flask_unchained.bundles.security:flash.logout'),\n                   category='success')\n        return self.redirect('SECURITY_POST_LOGOUT_REDIRECT_ENDPOINT')", "response": "View function to log a user out. Supports html and json requests. Supports html and json requests. Supports html and json requests. Supports json requests."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nview function which sends confirmation token and instructions to a user.", "response": "def send_confirmation_email(self):\n        \"\"\"\n        View function which sends confirmation token and instructions to a user.\n        \"\"\"\n        form = self._get_form('SECURITY_SEND_CONFIRMATION_FORM')\n        if form.validate_on_submit():\n            self.security_service.send_email_confirmation_instructions(form.user)\n            self.flash(_('flask_unchained.bundles.security:flash.confirmation_request',\n                         email=form.user.email), category='info')\n            if request.is_json:\n                return '', HTTPStatus.NO_CONTENT\n\n        elif form.errors and request.is_json:\n            return self.errors(form.errors)\n\n        return self.render('send_confirmation_email',\n                           send_confirmation_form=form,\n                           **self.security.run_ctx_processor('send_confirmation_email'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef confirm_email(self, token):\n        expired, invalid, user = \\\n            self.security_utils_service.confirm_email_token_status(token)\n        if not user or invalid:\n            invalid = True\n            self.flash(\n                _('flask_unchained.bundles.security:flash.invalid_confirmation_token'),\n                category='error')\n\n        already_confirmed = user is not None and user.confirmed_at is not None\n        if expired and not already_confirmed:\n            self.security_service.send_email_confirmation_instructions(user)\n            self.flash(_('flask_unchained.bundles.security:flash.confirmation_expired',\n                         email=user.email,\n                         within=app.config.SECURITY_CONFIRM_EMAIL_WITHIN),\n                       category='error')\n\n        if invalid or (expired and not already_confirmed):\n            return self.redirect('SECURITY_CONFIRM_ERROR_REDIRECT_ENDPOINT',\n                                 'security_controller.send_confirmation_email')\n\n        if self.security_service.confirm_user(user):\n            self.after_this_request(self._commit)\n            self.flash(_('flask_unchained.bundles.security:flash.email_confirmed'),\n                       category='success')\n        else:\n            self.flash(_('flask_unchained.bundles.security:flash.already_confirmed'),\n                       category='info')\n\n        if user != current_user:\n            self.security_service.logout_user()\n            self.security_service.login_user(user)\n\n        return self.redirect('SECURITY_POST_CONFIRM_REDIRECT_ENDPOINT',\n                             'SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')", "response": "View function to confirm a user s token from the confirmation email send to them."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nviews function to request a password recovery email with a reset token.", "response": "def forgot_password(self):\n        \"\"\"\n        View function to request a password recovery email with a reset token.\n        Supports html and json requests.\n        \"\"\"\n        form = self._get_form('SECURITY_FORGOT_PASSWORD_FORM')\n        if form.validate_on_submit():\n            self.security_service.send_reset_password_instructions(form.user)\n            self.flash(_('flask_unchained.bundles.security:flash.password_reset_request',\n                         email=form.user.email),\n                       category='info')\n            if request.is_json:\n                return '', HTTPStatus.NO_CONTENT\n\n        elif form.errors and request.is_json:\n            return self.errors(form.errors)\n\n        return self.render('forgot_password',\n                           forgot_password_form=form,\n                           **self.security.run_ctx_processor('forgot_password'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset_password(self, token):\n        expired, invalid, user = \\\n            self.security_utils_service.reset_password_token_status(token)\n        if invalid:\n            self.flash(\n                _('flask_unchained.bundles.security:flash.invalid_reset_password_token'),\n                category='error')\n            return self.redirect('SECURITY_INVALID_RESET_TOKEN_REDIRECT')\n        elif expired:\n            self.security_service.send_reset_password_instructions(user)\n            self.flash(_('flask_unchained.bundles.security:flash.password_reset_expired',\n                         email=user.email,\n                         within=app.config.SECURITY_RESET_PASSWORD_WITHIN),\n                       category='error')\n            return self.redirect('SECURITY_EXPIRED_RESET_TOKEN_REDIRECT')\n\n        spa_redirect = app.config.SECURITY_API_RESET_PASSWORD_HTTP_GET_REDIRECT\n        if request.method == 'GET' and spa_redirect:\n            return self.redirect(spa_redirect, token=token, _external=True)\n\n        form = self._get_form('SECURITY_RESET_PASSWORD_FORM')\n        if form.validate_on_submit():\n            self.security_service.reset_password(user, form.password.data)\n            self.security_service.login_user(user)\n            self.after_this_request(self._commit)\n            self.flash(_('flask_unchained.bundles.security:flash.password_reset'),\n                       category='success')\n            if request.is_json:\n                return self.jsonify({'token': user.get_auth_token(),\n                                     'user': user})\n            return self.redirect('SECURITY_POST_RESET_REDIRECT_ENDPOINT',\n                                 'SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')\n\n        elif form.errors and request.is_json:\n            return self.errors(form.errors)\n\n        return self.render('reset_password',\n                           reset_password_form=form,\n                           reset_password_token=token,\n                           **self.security.run_ctx_processor('reset_password'))", "response": "View function verify a users reset password token from the email we sent to them. It handles the form for the user to reset a new password. It handles the form for the user to set a new password. It handles the form for the user to reset a new password."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nviews function for a user to change their password.", "response": "def change_password(self):\n        \"\"\"\n        View function for a user to change their password.\n        Supports html and json requests.\n        \"\"\"\n        form = self._get_form('SECURITY_CHANGE_PASSWORD_FORM')\n        if form.validate_on_submit():\n            self.security_service.change_password(\n                current_user._get_current_object(),\n                form.new_password.data)\n            self.after_this_request(self._commit)\n            self.flash(_('flask_unchained.bundles.security:flash.password_change'),\n                       category='success')\n            if request.is_json:\n                return self.jsonify({'token': current_user.get_auth_token()})\n            return self.redirect('SECURITY_POST_CHANGE_REDIRECT_ENDPOINT',\n                                 'SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')\n\n        elif form.errors and request.is_json:\n            return self.errors(form.errors)\n\n        return self.render('change_password',\n                           change_password_form=form,\n                           **self.security.run_ctx_processor('change_password'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the token hashing context.", "response": "def _get_hashing_context(self, app: FlaskUnchained) -> CryptContext:\n        \"\"\"\n        Get the token hashing (and verifying) context.\n        \"\"\"\n        return CryptContext(schemes=app.config.SECURITY_HASHING_SCHEMES,\n                            deprecated=app.config.SECURITY_DEPRECATED_HASHING_SCHEMES)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets an initialized instance of Flask Login s nanomsg.", "response": "def _get_login_manager(self,\n                           app: FlaskUnchained,\n                           anonymous_user: AnonymousUser,\n                           ) -> LoginManager:\n        \"\"\"\n        Get an initialized instance of Flask Login's\n        :class:`~flask_login.LoginManager`.\n        \"\"\"\n        login_manager = LoginManager()\n        login_manager.anonymous_user = anonymous_user or AnonymousUser\n        login_manager.localize_callback = _\n        login_manager.request_loader(self._request_loader)\n        login_manager.user_loader(\n            lambda *a, **kw: self.security_utils_service.user_loader(*a, **kw))\n        login_manager.login_view = 'security_controller.login'\n        login_manager.login_message = _(\n            'flask_unchained.bundles.security:error.login_required')\n        login_manager.login_message_category = 'info'\n        login_manager.needs_refresh_message = _(\n            'flask_unchained.bundles.security:error.fresh_login_required')\n        login_manager.needs_refresh_message_category = 'info'\n        login_manager.init_app(app)\n        return login_manager"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_principal(self, app: FlaskUnchained) -> Principal:\n        principal = Principal(app, use_sessions=False)\n        principal.identity_loader(self._identity_loader)\n        return principal", "response": "Get an initialized instance of Flask Principal s.\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the password hashing context.", "response": "def _get_pwd_context(self, app: FlaskUnchained) -> CryptContext:\n        \"\"\"\n        Get the password hashing context.\n        \"\"\"\n        pw_hash = app.config.SECURITY_PASSWORD_HASH\n        schemes = app.config.SECURITY_PASSWORD_SCHEMES\n        if pw_hash not in schemes:\n            allowed = (', '.join(schemes[:-1]) + ' and ' + schemes[-1])\n            raise ValueError(f'Invalid password hashing scheme {pw_hash}. '\n                             f'Allowed values are {allowed}.')\n        return CryptContext(schemes=schemes, default=pw_hash,\n                            deprecated=app.config.SECURITY_DEPRECATED_PASSWORD_SCHEMES)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a URLSafeTimedSerializer for the given serialization context name.", "response": "def _get_serializer(self, app: FlaskUnchained, name: str) -> URLSafeTimedSerializer:\n        \"\"\"\n        Get a URLSafeTimedSerializer for the given serialization context name.\n\n        :param app: the :class:`FlaskUnchained` instance\n        :param name: Serialization context. One of ``confirm``, ``login``,\n          ``remember``, or ``reset``\n        :return: URLSafeTimedSerializer\n        \"\"\"\n        salt = app.config.get('SECURITY_%s_SALT' % name.upper())\n        return URLSafeTimedSerializer(secret_key=app.config.SECRET_KEY, salt=salt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _identity_loader(self) -> Union[Identity, None]:\n        if not isinstance(current_user._get_current_object(), AnonymousUser):\n            return Identity(current_user.id)", "response": "This is the identity loading function that is used to load the user s identity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _request_loader(self, request: Request) -> Union[User, AnonymousUser]:\n        header_key = self.token_authentication_header\n        args_key = self.token_authentication_key\n        token = request.args.get(args_key, request.headers.get(header_key, None))\n        if request.is_json:\n            data = request.get_json(silent=True) or {}\n            token = data.get(args_key, token)\n\n        try:\n            data = self.remember_token_serializer.loads(token, max_age=self.token_max_age)\n            user = self.user_manager.get(data[0])\n            if user and self.security_utils_service.verify_hash(data[1], user.password):\n                return user\n        except:\n            pass\n\n        return self.login_manager.anonymous_user()", "response": "Attempt to load the user from the request token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a column that can be used to associate a foreign key with the current model.", "response": "def foreign_key(*args,\n                fk_col: Optional[str] = None,\n                primary_key: bool = False,\n                nullable: bool = False,\n                **kwargs,\n                ) -> Column:\n    \"\"\"\n    Helper method to add a foreign key column to a model.\n\n    For example::\n\n        class Post(db.Model):\n            category_id = db.foreign_key('Category')\n            category = db.relationship('Category', back_populates='posts')\n\n    Is equivalent to::\n\n        class Post(db.Model):\n            category_id = db.Column(db.BigInteger, db.ForeignKey('category.id'),\n                                    nullable=False)\n            category = db.relationship('Category', back_populates='posts')\n\n    Customizing all the things::\n\n        class Post(db.Model):\n            _category_id = db.foreign_key('category_id',  # db column name\n                                          db.String,      # db column type\n                                          'categories',   # foreign table name\n                                          fk_col='pk')    # foreign key col name\n\n    Is equivalent to::\n\n        class Post(db.Model):\n            _category_id = db.Column('category_id',\n                                     db.String,\n                                     db.ForeignKey('categories.pk'),\n                                     nullable=False)\n\n    :param args: :func:`foreign_key` takes up to three positional arguments.\n    Most commonly, you will only pass one argument, which should be the model\n    name, the model class, or table name you're linking to.\n    If you want to customize the column name the foreign key gets stored in\n    the database under, then *it must be the first string argument*, and you must\n    *also* supply the model name, class or table name. You can also customize the\n    column type (eg ``sa.Integer`` or ``sa.String(36)``) by passing it as an arg.\n\n    :param str fk_col: The column name of the primary key on the *opposite* side\n      of the relationship (defaults to\n      :attr:`sqlalchemy_unchained._ModelRegistry.default_primary_key_column`).\n    :param bool primary_key: Whether or not this :class:`~sqlalchemy.Column` is\n                             a primary key.\n    :param bool nullable: Whether or not this :class:`~sqlalchemy.Column` should\n                          be nullable.\n    :param kwargs: Any other kwargs to pass the :class:`~sqlalchemy.Column`\n                   constructor.\n    \"\"\"\n    return Column(*_get_fk_col_args(args, fk_col, _default_col_type=BigInteger),\n                  primary_key=primary_key, nullable=nullable, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef macro(name):\n    def wrapper(view, context, model, column):\n        if '.' in name:\n            macro_import_name, macro_name = name.split('.')\n            m = getattr(context.get(macro_import_name), macro_name, None)\n        else:\n            m = context.resolve(name)\n\n        if not m:\n            return m\n\n        return m(model=model, column=column)\n\n    return wrapper", "response": "Adds support for using\n    macros imported from another file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist registered hooks (in the order they run).", "response": "def hooks(ctx):\n    \"\"\"\n    List registered hooks (in the order they run).\n    \"\"\"\n    from ..hooks.run_hooks_hook import RunHooksHook\n\n    bundles = _get_bundles(ctx.obj.data['env'])\n    hooks = RunHooksHook(None).collect_from_bundles(bundles)\n    print_table(('Hook Name',\n                 'Default Bundle Module',\n                 'Bundle Module Override Attr',\n                 'Description'),\n                [(hook.name,\n                 hook.bundle_module_name or '(None)',\n                 hook.bundle_override_module_name_attr or '(None)',\n                 format_docstring(hook.__doc__) or '(None)') for hook in hooks])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef command(name=None, cls=None, **attrs):\n    return click.command(name=name, cls=cls or Command, **attrs)", "response": "A command line interface for the command line parser."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef option(*param_decls, cls=None, **attrs):\n    return click.option(*param_decls, cls=cls or Option, **attrs)", "response": "Option decorator that returns a click. Option that will be shown on the command line and the user will prompt for the option to be displayed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef command(self, *args, **kwargs):\n        return super().command(\n            *args, cls=kwargs.pop('cls', Command) or click.Command, **kwargs)", "response": "A command that will be used to dispatch the command line to the command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef group(self, *args, **kwargs):\n        return super().group(\n            *args, cls=kwargs.pop('cls', Group) or Group, **kwargs)", "response": "A base method for the group method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprompt the user for a value.", "response": "def prompt_for_value(self, ctx):\n        \"\"\"This is an alternative flow that can be activated in the full\n        value processing if a value does not exist.  It will prompt the\n        user until a valid value exists and then returns the processed\n        value as result.\n        \"\"\"\n        # Calculate the default before prompting anything to be stable.\n        default = self.get_default(ctx)\n        if isinstance(default, AutoDefault):\n            return self.type_cast_value(ctx, default.value)\n\n        # If this is a prompt for a flag we need to handle this\n        # differently.\n        if self.is_bool_flag:\n            return confirm(self.prompt, default)\n\n        return prompt(self.prompt, default=default,\n                      hide_input=self.hide_input,\n                      confirmation_prompt=self.confirmation_prompt,\n                      value_proc=lambda x: self.process_value(ctx, x))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserve JSON spec file", "response": "def _openapi_json(self):\n        \"\"\"Serve JSON spec file\"\"\"\n        # We don't use Flask.jsonify here as it would sort the keys\n        # alphabetically while we want to preserve the order.\n        from pprint import pprint\n        pprint(self.to_dict())\n        return current_app.response_class(json.dumps(self.to_dict(), indent=4),\n                                          mimetype='application/json')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the OpenAPI Redoc page.", "response": "def _openapi_redoc(self):\n        \"\"\"\n        Expose OpenAPI spec with ReDoc\n\n        The ReDoc script URL can be specified as ``API_REDOC_SOURCE_URL``\n        \"\"\"\n        return render_template('openapi/redoc.html',\n                               title=self.app.config.API_TITLE or self.app.name,\n                               redoc_url=self.app.config.API_REDOC_SOURCE_URL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters custom path parameter converter", "response": "def register_converter(self, converter, conv_type, conv_format=None):\n        \"\"\"\n        Register custom path parameter converter\n\n        :param BaseConverter converter: Converter.\n            Subclass of werkzeug's BaseConverter\n        :param str conv_type: Parameter type\n        :param str conv_format: Parameter format (optional)\n        \"\"\"\n        self.flask_plugin.register_converter(converter, conv_type, conv_format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_loader(*decorator_args, serializer):\n    def wrapped(fn):\n        @wraps(fn)\n        def decorated(*args, **kwargs):\n            result = serializer.load(request.get_json(),\n                                     instance=kwargs.pop('instance'),\n                                     partial=True)\n            if not result.errors and not result.data.id:\n                abort(HTTPStatus.NOT_FOUND)\n            return fn(*result)\n        return decorated\n\n    if decorator_args and callable(decorator_args[0]):\n        return wrapped(decorator_args[0])\n    return wrapped", "response": "Decorator to automatically load and update a model from json request data\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_loader(*decorator_args, serializer):\n    def wrapped(fn):\n        @wraps(fn)\n        def decorated(*args, **kwargs):\n            return fn(*serializer.load(request.get_json()))\n        return decorated\n\n    if decorator_args and callable(decorator_args[0]):\n        return wrapped(decorator_args[0])\n    return wrapped", "response": "Decorator to automatically instantiate a model from json request data\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the database tables and run migrations.", "response": "def reset_command(force):\n    \"\"\"Drop database tables and run migrations.\"\"\"\n    if not force:\n        exit('Cancelled.')\n\n    click.echo('Dropping DB tables.')\n    drop_all()\n\n    click.echo('Running DB migrations.')\n    alembic.upgrade(migrate.get_config(None), 'head')\n\n    click.echo('Done.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, loader, pathname, recursive=False, encoding=None):\n        if not encoding:\n            encoding = self._encoding or self.DEFAULT_ENCODING\n        if self._base_dir:\n            pathname = os.path.join(self._base_dir, pathname)\n        if re.match(WILDCARDS_REGEX, pathname):\n            result = []\n            if PYTHON_MAYOR_MINOR >= '3.5':\n                iterator = iglob(pathname, recursive=recursive)\n            else:\n                iterator = iglob(pathname)\n            for path in iterator:\n                if os.path.isfile(path):\n                    with io.open(path, encoding=encoding) as fp:  # pylint:disable=invalid-name\n                        result.append(yaml.load(fp, type(loader)))\n            return result\n        with io.open(pathname, encoding=encoding) as fp:  # pylint:disable=invalid-name\n            return yaml.load(fp, type(loader))", "response": "Loads a YAML file and returns it as a Python object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_to_loader_class(cls, loader_class=None, tag=None, **kwargs):\n        # type: (type(yaml.Loader), str, **str)-> YamlIncludeConstructor\n        \"\"\"\n        Create an instance of the constructor, and add it to the YAML `Loader` class\n\n        :param loader_class: The `Loader` class add constructor to.\n\n            .. attention:: This parameter **SHOULD** be a **class type**, **NOT** object.\n\n            It's one of following:\n\n                - :class:`yaml.BaseLoader`\n                - :class:`yaml.UnSafeLoader`\n                - :class:`yaml.SafeLoader`\n                - :class:`yaml.Loader`\n                - :class:`yaml.FullLoader`\n                - :class:`yaml.CBaseLoader`\n                - :class:`yaml.CUnSafeLoader`\n                - :class:`yaml.CSafeLoader`\n                - :class:`yaml.CLoader`\n                - :class:`yaml.CFullLoader`\n\n            :default: ``None``:\n\n                - When :mod:`pyyaml` 3.*: Add to PyYAML's default `Loader`\n                - When :mod:`pyyaml` 5.*: Add to `FullLoader`\n\n        :type loader_class: type\n\n        :param str tag: Tag's name of the include constructor.\n\n          :default: ``\"\"``: Use :attr:`DEFAULT_TAG_NAME` as tag name.\n\n        :param kwargs: Arguments passed to construct function\n\n        :return: New created object\n        :rtype: YamlIncludeConstructor\n        \"\"\"\n        if tag is None:\n            tag = ''\n        tag = tag.strip()\n        if not tag:\n            tag = cls.DEFAULT_TAG_NAME\n        if not tag.startswith('!'):\n            raise ValueError('`tag` argument should start with character \"!\"')\n        instance = cls(**kwargs)\n        if loader_class is None:\n            if FullLoader:\n                yaml.add_constructor(tag, instance, FullLoader)\n            else:\n                yaml.add_constructor(tag, instance)\n        else:\n            yaml.add_constructor(tag, instance, loader_class)\n        return instance", "response": "Add a constructor to the YAML Loader class and add it to the YAML Loader class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_table(column_names: IterableOfStrings,\n                rows: IterableOfTuples,\n                column_alignments: Optional[IterableOfStrings] = None,\n                primary_column_idx: int = 0,\n                ) -> None:\n    \"\"\"\n    Prints a table of information to the console. Automatically determines if the\n    console is wide enough, and if not, displays the information in list form.\n\n    :param column_names: The heading labels\n    :param rows: A list of lists\n    :param column_alignments: An optional list of strings, using either '<' or '>'\n        to specify left or right alignment respectively\n    :param primary_column_idx: Used when displaying information in list form,\n        to determine which label should be the top-most one. Defaults to the first\n        label in ``column_names``.\n    \"\"\"\n    header_template = ''\n    row_template = ''\n    table_width = 0\n    type_formatters = {int: 'd', float: 'f', str: 's'}\n    types = [type_formatters.get(type(x), 'r') for x in rows[0]]\n\n    alignments = {int: '>', float: '>'}\n    column_alignments = (column_alignments or\n                         [alignments.get(type(x), '<') for x in rows[0]])\n\n    def get_column_width(idx):\n        header_length = len(column_names[idx])\n        content_length = max(len(str(row[idx])) for row in rows)\n        return (content_length if content_length > header_length\n                else header_length)\n\n    for i in range(0, len(column_names)):\n        col_width = get_column_width(i)\n        header_col_template = f'{{:{col_width}}}'\n        col_template = f'{{:{column_alignments[i]}{col_width}{types[i]}}}'\n        if i == 0:\n            header_template += header_col_template\n            row_template += col_template\n            table_width += col_width\n        else:\n            header_template += '  ' + header_col_template\n            row_template += '  ' + col_template\n            table_width += 2 + col_width\n\n    # check if we can format the table horizontally\n    if table_width < get_terminal_width():\n        click.echo(header_template.format(*column_names))\n        click.echo('-' * table_width)\n\n        for row in rows:\n            try:\n                click.echo(row_template.format(*row))\n            except TypeError as e:\n                raise TypeError(f'{e}: {row!r}')\n\n    # otherwise format it vertically\n    else:\n        max_label_width = max(*[len(label) for label in column_names])\n        non_primary_columns = [(i, col) for i, col in enumerate(column_names)\n                               if i != primary_column_idx]\n        for row in rows:\n            type_ = types[primary_column_idx]\n            row_template = f'{{:>{max_label_width}s}}: {{:{type_}}}'\n            click.echo(row_template.format(column_names[primary_column_idx],\n                                           row[primary_column_idx]))\n            for i, label in non_primary_columns:\n                row_template = f'{{:>{max_label_width}s}}: {{:{types[i]}}}'\n                click.echo(row_template.format(label, row[i]))\n            click.echo()", "response": "Prints a table of information in the list form of the modules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef route(rule=None, blueprint=None, defaults=None, endpoint=None,\n          is_member=False, methods=None, only_if=None, **rule_options):\n    \"\"\"\n    Decorator to set default route rules for a view function. The arguments this\n    function accepts are very similar to Flask's :meth:`~flask.Flask.route`,\n    however, the ``is_member`` perhaps deserves an example::\n\n        class UserResource(ModelResource):\n            class Meta:\n                model = User\n                member_param = '<int:id>'\n                include_methods = ['list', 'get']\n\n            @route(is_member=True, methods=['POST'])\n            def set_profile_pic(user):\n                # do stuff\n\n        # registered like so in your ``app_bundle/routes.py``:\n        routes = lambda: [\n            resource(UserResource),\n        ]\n\n        # results in the following routes:\n        # UserResource.list             => GET  /users\n        # UserResource.get              => GET  /users/<int:id>\n        # UserResource.set_profile_pic  => POST /users/<int:id>/set-profile-pic\n\n    :param rule: The URL rule.\n    :param defaults: Any default values for parameters in the URL rule.\n    :param endpoint: The endpoint name of this view. Determined automatically if left\n                     unspecified.\n    :param is_member: Whether or not this view is for a\n                      :class:`~flask_unchained.bundles.resource.resource.Resource`\n                      member method.\n    :param methods: A list of HTTP methods supported by this view. Defaults to\n                    ``['GET']``.\n    :param only_if: A boolean or callable to dynamically determine whether or not to\n                    register this route with the app.\n    :param rule_options: Other kwargs passed on to :class:`~werkzeug.routing.Rule`.\n    \"\"\"\n    def wrapper(fn):\n        fn_routes = getattr(fn, FN_ROUTES_ATTR, [])\n        route = Route(rule, fn, blueprint=blueprint, defaults=defaults,\n                      endpoint=endpoint, is_member=is_member, methods=methods,\n                      only_if=only_if, **rule_options)\n        setattr(fn, FN_ROUTES_ATTR, fn_routes + [route])\n        return fn\n\n    if callable(rule):\n        fn = rule\n        rule = None\n        return wrapper(fn)\n    return wrapper", "response": "Decorator to set default route rules for a view function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef no_route(arg=None):\n    def wrapper(fn):\n        setattr(fn, NO_ROUTES_ATTR, True)\n        return fn\n\n    if callable(arg):\n        return wrapper(arg)\n    return wrapper", "response": "Decorator to mark a resource method as not route."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the hook for the application.", "response": "def run_hook(self,\n                 app: FlaskUnchained,\n                 bundles: List[Bundle],\n                 _config_overrides: Optional[Dict[str, Any]] = None,\n                 ) -> None:\n        \"\"\"\n        For each bundle in ``unchained_config.BUNDLES``, iterate through that\n        bundle's class hierarchy, starting from the base-most bundle. For each\n        bundle in that order, look for a ``config`` module, and if it exists,\n        update ``app.config`` with the options first from a base ``Config`` class,\n        if it exists, and then also if it exists, from an env-specific config class:\n        one of ``DevConfig``, ``ProdConfig``, ``StagingConfig``, or ``TestConfig``.\n        \"\"\"\n        self.apply_default_config(app, bundles and bundles[-1] or None)\n        BundleConfig._set_current_app(app)\n        for bundle_ in bundles:\n            for bundle in bundle_._iter_class_hierarchy():\n                app.config.from_mapping(self.get_bundle_config(bundle, app.env))\n\n        if _config_overrides and isinstance(_config_overrides, dict):\n            app.config.from_mapping(_config_overrides)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef force_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, text_type):\n        return s\n\n    try:\n        if not isinstance(s, string_types):\n            if PY3:\n                if isinstance(s, bytes):\n                    s = text_type(s, encoding, errors)\n                else:\n                    s = text_type(s)\n            elif hasattr(s, '__unicode__'):\n                s = s.__unicode__()\n            else:\n                s = text_type(bytes(s), encoding, errors)\n        else:\n            s = s.decode(encoding, errors)\n    except UnicodeDecodeError as e:\n        if not isinstance(s, Exception):\n            raise FlaskMailUnicodeDecodeError(s, *e.args)\n        else:\n            s = ' '.join([force_text(arg, encoding, errors)\n                          for arg in s])\n    return s", "response": "Force text to a lazy object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify and sends a message.", "response": "def send(self, message, envelope_from=None):\n        \"\"\"Verifies and sends message.\n\n        :param message: Message instance.\n        :param envelope_from: Email address to be used in MAIL FROM command.\n        \"\"\"\n        assert message.send_to, \"No recipients have been added\"\n\n        assert message.sender, (\n            \"The message does not specify a sender and a default sender \"\n            \"has not been configured\")\n\n        if message.has_bad_headers():\n            raise BadHeaderError\n\n        if message.date is None:\n            message.date = time.time()\n\n        ret = None\n        if self.host:\n            ret = self.host.sendmail(\n                sanitize_address(envelope_from or message.sender),\n                list(sanitize_addresses(message.send_to)),\n                message.as_bytes() if PY3 else message.as_string(),\n                message.mail_options,\n                message.rcpt_options\n            )\n\n        email_dispatched.send(message, app=current_app._get_current_object())\n\n        self.num_emails += 1\n\n        if self.num_emails == self.mail.max_emails:\n            self.num_emails = 0\n            if self.host:\n                self.host.quit()\n                self.host = self.configure_host()\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _message(self):\n        ascii_attachments = current_app.extensions['mail'].ascii_attachments\n        encoding = self.charset or 'utf-8'\n\n        attachments = self.attachments or []\n\n        if len(attachments) == 0 and not self.alts:\n            # No html content and zero attachments means plain text\n            msg = self._mimetext(self.body, self.subtype)\n        elif len(attachments) > 0 and not self.alts:\n            # No html and at least one attachment means multipart\n            subtype = self.subtype or 'mixed'\n            msg = MIMEMultipart(_subtype=subtype)\n            msg.attach(self._mimetext(self.body))\n        else:\n            # Anything else\n            subtype = self.subtype or 'mixed'\n            msg = MIMEMultipart(_subtype=subtype)\n            alternative = MIMEMultipart(_subtype='alternative')\n            alternative.attach(self._mimetext(self.body))\n            for mimetype, content in self.alts.items():\n                alternative.attach(self._mimetext(content, mimetype))\n            msg.attach(alternative)\n\n        if self.subject:\n            msg['Subject'] = sanitize_subject(force_text(self.subject),\n                                              encoding)\n\n        msg['From'] = sanitize_address(self.sender, encoding)\n        msg['To'] = ', '.join(\n            list(set(sanitize_addresses(self.recipients, encoding)))\n        )\n\n        msg['Date'] = formatdate(self.date, localtime=True)\n        # see RFC 5322 section 3.6.4.\n        msg['Message-ID'] = self.msgId\n\n        if self.cc:\n            msg['Cc'] = ', '.join(\n                list(set(sanitize_addresses(self.cc, encoding)))\n            )\n\n        if self.reply_to:\n            msg['Reply-To'] = sanitize_address(self.reply_to, encoding)\n\n        if self.extra_headers:\n            for k, v in self.extra_headers.items():\n                msg[k] = v\n\n        SPACES = re.compile(r'[\\s]+', re.UNICODE)\n        for attachment in attachments:\n            f = MIMEBase(*attachment.content_type.split('/'))\n            f.set_payload(attachment.data)\n            encode_base64(f)\n\n            filename = attachment.filename\n            if filename and ascii_attachments:\n                # force filename to ascii\n                filename = unicodedata.normalize('NFKD', filename)\n                filename = filename.encode('ascii', 'ignore').decode('ascii')\n                filename = SPACES.sub(u' ', filename).strip()\n\n            try:\n                filename and filename.encode('ascii')\n            except UnicodeEncodeError:\n                if not PY3:\n                    filename = filename.encode('utf8')\n                filename = ('UTF8', '', filename)\n\n            f.add_header('Content-Disposition',\n                         attachment.disposition,\n                         filename=filename)\n\n            for key, value in attachment.headers.items():\n                f.add_header(key, value)\n\n            if attachment.content_id:\n                try:\n                    f.replace_header('Content-ID', attachment.content_id)\n                except KeyError:\n                    f.add_header('Content-ID', attachment.content_id)\n\n            msg.attach(f)\n        if message_policy:\n            msg.policy = message_policy\n\n        return msg", "response": "Creates the email object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self):\n        app = getattr(self, \"app\", None) or current_app\n        try:\n            return Connection(app.extensions['mail'])\n        except KeyError:\n            raise RuntimeError(\"The curent application was\"\n                               \" not configured with Flask-Mail\")", "response": "Connects to the mail host."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_app(self, app):\n        state = self.init_mail(app.config, app.debug, app.testing)\n\n        # register extension with app\n        app.extensions = getattr(app, 'extensions', {})\n        app.extensions['mail'] = state\n        return state", "response": "Initializes your mail settings from the application settings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef context_processor(self, fn):\n        self._defer(lambda bp: bp.context_processor(fn))\n        return fn", "response": "Decorator for the Flask. ContextProcessor class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef url_value_preprocessor(self, fn):\n        self._defer(lambda bp: bp.url_value_preprocessor(fn))\n        return fn", "response": "Registers a function as URL value preprocessor for this bundle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister an error handler that becomes active for this bundle only. Please be aware that routing does not happen local to a bundle so an error handler for 404 usually is not handled by a bundle unless it is caused inside a view function. Another special case is the 500 internal server error which is always looked up from the application. Otherwise works as the :meth:`flask.Blueprint.errorhandler` decorator.", "response": "def errorhandler(self, code_or_exception):\n        \"\"\"\n        Registers an error handler that becomes active for this bundle\n        only.  Please be aware that routing does not happen local to a\n        bundle so an error handler for 404 usually is not handled by\n        a bundle unless it is caused inside a view function.  Another\n        special case is the 500 internal server error which is always looked\n        up from the application.\n\n        Otherwise works as the :meth:`flask.Blueprint.errorhandler` decorator.\n        \"\"\"\n        def decorator(fn):\n            self._defer(lambda bp: bp.register_error_handler(code_or_exception, fn))\n            return fn\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _iter_class_hierarchy(self, include_self=True, reverse=True):\n        supers = self.__class__.__mro__[(0 if include_self else 1):]\n        for bundle in (supers if not reverse else reversed(supers)):\n            if issubclass(bundle, Bundle) and bundle not in {AppBundle, Bundle}:\n                if bundle == self.__class__:\n                    yield self\n                else:\n                    yield bundle()", "response": "Iterate over the class hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a route generator for a given controller class or url prefix.", "response": "def controller(url_prefix_or_controller_cls: Union[str, Type[Controller]],\n               controller_cls: Optional[Type[Controller]] = None,\n               *,\n               rules: Optional[Iterable[Union[Route, RouteGenerator]]] = None,\n               ) -> RouteGenerator:\n    \"\"\"\n    This function is used to register a controller class's routes.\n\n    Example usage::\n\n        routes = lambda: [\n            controller(SiteController),\n        ]\n\n    Or with the optional prefix argument::\n\n        routes = lambda: [\n            controller('/products', ProductController),\n        ]\n\n    Specify ``rules`` to only include those routes from the controller::\n\n        routes = lambda: [\n            controller(SecurityController, rules=[\n               rule('/login', SecurityController.login),\n               rule('/logout', SecurityController.logout),\n               rule('/sign-up', SecurityController.register),\n            ]),\n        ]\n\n    :param url_prefix_or_controller_cls: The controller class, or a url prefix for\n                                         all of the rules from the controller class\n                                         passed as the second argument\n    :param controller_cls: If a url prefix was given as the first argument, then\n                           the controller class must be passed as the second argument\n    :param rules: An optional list of rules to limit/customize the routes included\n                  from the controller\n    \"\"\"\n    url_prefix, controller_cls = _normalize_args(\n        url_prefix_or_controller_cls, controller_cls, _is_controller_cls)\n    url_prefix = url_prefix or controller_cls.Meta.url_prefix\n\n    routes = []\n    controller_routes = getattr(controller_cls, CONTROLLER_ROUTES_ATTR)\n    if rules is None:\n        routes = controller_routes.values()\n    else:\n        for route in _reduce_routes(rules):\n            existing = controller_routes.get(route.method_name)\n            if existing:\n                routes.append(_inherit_route_options(route, existing[0]))\n            else:\n                routes.append(route)\n\n    yield from _normalize_controller_routes(routes, controller_cls,\n                                            url_prefix=url_prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlike :func:`rule`, except specifically for HTTP DELETE requests. :param rule: The url rule for this route. :param cls_method_name_or_view_fn: The view function for this route. :param is_member: Whether or not this route is a member function. :param only_if: An optional function to decide at runtime whether or not to register the route with Flask. It gets passed the configured app as a single argument, and should return a boolean. :param rule_options: Keyword arguments that ultimately end up getting passed on to :class:`~werkzeug.routing.Rule`", "response": "def delete(rule: str,\n           cls_method_name_or_view_fn: Optional[Union[str, Callable]] = None,\n           *,\n           defaults: Optional[Defaults] = _missing,\n           endpoint: Optional[str] = _missing,\n           is_member: Optional[bool] = _missing,\n           only_if: Optional[Union[bool, Callable[[FlaskUnchained], bool]]] = _missing,\n           **rule_options) -> RouteGenerator:\n    \"\"\"\n    Like :func:`rule`, except specifically for HTTP DELETE requests.\n\n    :param rule: The url rule for this route.\n    :param cls_method_name_or_view_fn: The view function for this route.\n    :param is_member: Whether or not this route is a member function.\n    :param only_if: An optional function to decide at runtime whether or not to register\n                    the route with Flask. It gets passed the configured app as a single\n                    argument, and should return a boolean.\n    :param rule_options: Keyword arguments that ultimately end up getting passed on to\n                         :class:`~werkzeug.routing.Rule`\n    \"\"\"\n    rule_options.pop('methods', None)\n    yield Route(rule, cls_method_name_or_view_fn, defaults=defaults,\n                endpoint=endpoint, is_member=is_member, methods=['DELETE'],\n                only_if=only_if, **rule_options)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef func(rule_or_view_func: Union[str, Callable],\n         view_func: Optional[Callable] = _missing,\n         blueprint: Optional[Blueprint] = _missing,\n         defaults: Optional[Defaults] = _missing,\n         endpoint: Optional[str] = _missing,\n         methods: Optional[Methods] = _missing,\n         only_if: Optional[Union[bool, Callable[[FlaskUnchained], bool]]] = _missing,\n         **rule_options,\n         ) -> RouteGenerator:\n    \"\"\"\n    This function allows to register legacy view functions as routes, eg::\n\n        @route('/')\n        def index():\n            return render_template('site/index.html')\n\n        routes = lambda: [\n            func(index),\n        ]\n\n    It accepts an optional url rule argument::\n\n        routes = lambda: [\n            func('/products', product_list_view),\n        ]\n\n    As well as supporting the same kwargs as Werkzeug's :class:`~werkzeug.routing.Rule`,\n    eg::\n\n        routes = lambda: [\n            func('/', index, endpoint='home', methods=['GET', 'POST']),\n        ]\n\n    :param rule_or_view_func: The view function, or an optional url rule for the view\n                              function given as the second argument\n    :param view_func: The view function if passed a url rule as the first argument\n    :param only_if: An optional function to decide at runtime whether or not to register\n                    the route with Flask. It gets passed the configured app as a single\n                    argument, and should return a boolean.\n    :param rule_options: Keyword arguments that ultimately end up getting passed on to\n                         :class:`~werkzeug.routing.Rule`\n    \"\"\"\n    rule, view_func = _normalize_args(\n        rule_or_view_func, view_func, _is_view_func)\n\n    route = Route(rule, view_func, blueprint=blueprint, defaults=defaults,\n                  endpoint=endpoint, methods=methods, only_if=only_if,\n                  **rule_options)\n\n    existing_routes = getattr(view_func, FN_ROUTES_ATTR, [])\n    if len(existing_routes) == 1:\n        existing_route = existing_routes[0]\n    else:\n        routes_by_rule = {route.rule: route for route in existing_routes}\n        lookup_rule = (rule if isinstance(rule, str)\n                       else method_name_to_url(view_func.__name__))\n        existing_route = routes_by_rule.get(lookup_rule, None)\n\n    if not existing_route:\n        yield route\n    else:\n        yield _inherit_route_options(route, existing_route)", "response": "A function that returns a route that can be used to render a view function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a route generator that includes routes from another module at that point in the tree.", "response": "def include(url_prefix_or_module_name: str,\n            module_name: Optional[str] = None,\n            *,\n            attr: str = 'routes',\n            exclude: Optional[Endpoints] = None,\n            only: Optional[Endpoints] = None,\n            ) -> RouteGenerator:\n    \"\"\"\n    Include the routes from another module at that point in the tree. For example::\n\n        # project-root/bundles/primes/routes.py\n        routes = lambda: [\n            controller('/two', TwoController),\n            controller('/three', ThreeController),\n            controller('/five', FiveController),\n        ]\n\n\n        # project-root/bundles/blog/routes.py\n        routes = lambda: [\n            func('/', index),\n            controller('/authors', AuthorController),\n            controller('/posts', PostController),\n        ]\n\n\n        # project-root/your_app_bundle/routes.py\n        routes = lambda: [\n            include('some_bundle.routes'),\n\n            # these last two are equivalent\n            include('/blog', 'bundles.blog.routes'),\n            prefix('/blog', [\n                include('bundles.blog.routes'),\n            ]),\n        ]\n\n    :param url_prefix_or_module_name: The module name, or a url prefix for all\n                                      of the included routes in the module name\n                                      passed as the second argument.\n    :param module_name: The module name of the routes to include if a url prefix\n                        was given as the first argument.\n    :param attr: The attribute name in the module, if different from ``routes``.\n    :param exclude: An optional list of endpoints to exclude.\n    :param only: An optional list of endpoints to only include.\n    \"\"\"\n    url_prefix = None\n    if module_name is None:\n        module_name = url_prefix_or_module_name\n    else:\n        url_prefix = url_prefix_or_module_name\n\n    module = importlib.import_module(module_name)\n    try:\n        routes = getattr(module, attr)()\n    except AttributeError:\n        raise AttributeError(f'Could not find a variable named `{attr}` '\n                             f'in the {module_name} module!')\n\n    routes = _reduce_routes(routes, exclude=exclude, only=only)\n    if url_prefix:\n        yield from prefix(url_prefix, routes)\n    else:\n        yield from routes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prefix(url_prefix: str,\n           children: Iterable[Union[Route, RouteGenerator]],\n           ) -> RouteGenerator:\n    \"\"\"\n    Sets a prefix on all of the child routes passed to it. It also supports nesting, eg::\n\n        routes = lambda: [\n            prefix('/foobar', [\n                controller('/one', OneController),\n                controller('/two', TwoController),\n                prefix('/baz', [\n                    controller('/three', ThreeController),\n                    controller('/four', FourController),\n                ])\n            ])\n        ]\n\n    :param url_prefix: The url prefix to set on the child routes\n    :param children:\n    \"\"\"\n    for route in _reduce_routes(children):\n        route = route.copy()\n        route.rule = join(url_prefix, route.rule,\n                          trailing_slash=route.rule.endswith('/'))\n        yield route", "response": "Sets a prefix on all of the child routes passed to it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resource(url_prefix_or_resource_cls: Union[str, Type[Resource]],\n             resource_cls: Optional[Type[Resource]] = None,\n             *,\n             member_param: Optional[str] = None,\n             unique_member_param: Optional[str] = None,\n             rules: Optional[Iterable[Union[Route, RouteGenerator]]] = None,\n             subresources: Optional[Iterable[RouteGenerator]] = None,\n             ) -> RouteGenerator:\n    \"\"\"\n    This function is used to register a :class:`Resource`'s routes.\n\n    Example usage::\n\n        routes = lambda: [\n            prefix('/api/v1', [\n                resource('/products', ProductResource),\n            ])\n        ]\n\n    Or with the optional prefix argument::\n\n        routes = lambda: [\n            resource('/products', ProductResource),\n        ]\n\n    Specify ``rules`` to only include those routes from the resource::\n\n        routes = lambda: [\n            resource('/users', UserResource, rules=[\n               get('/', UserResource.list),\n               get('/<int:id>', UserResource.get),\n            ]),\n        ]\n\n    Specify ``subresources`` to nest resource routes::\n\n        routes = lambda: [\n            resource('/users', UserResource, subresources=[\n               resource('/roles', RoleResource)\n            ]),\n        ]\n\n    Subresources can be nested as deeply as you want, however it's not recommended\n    to go more than two or three levels deep at the most, otherwise your URLs will\n    become unwieldy.\n\n    :param url_prefix_or_resource_cls: The resource class, or a url prefix for\n                                       all of the rules from the resource class\n                                       passed as the second argument.\n    :param resource_cls: If a url prefix was given as the first argument, then\n                         the resource class must be passed as the second argument.\n    :param member_param: Optionally override the controller's member_param attribute.\n    :param rules: An optional list of rules to limit/customize the routes included\n                  from the resource.\n    :param subresources: An optional list of subresources.\n    \"\"\"\n    url_prefix, resource_cls = _normalize_args(\n        url_prefix_or_resource_cls, resource_cls, _is_resource_cls)\n    member_param = member_param or resource_cls.Meta.member_param\n    unique_member_param = unique_member_param or resource_cls.Meta.unique_member_param\n    url_prefix = url_prefix or resource_cls.Meta.url_prefix\n\n    routes = getattr(resource_cls, CONTROLLER_ROUTES_ATTR)\n    if rules is not None:\n        routes = {method_name: method_routes\n                  for method_name, method_routes in routes.items()\n                  if method_name in resource_cls.resource_methods}\n        for route in rules:\n            routes[route.method_name] = route\n\n    yield from _normalize_controller_routes(routes.values(), resource_cls,\n                                            url_prefix=url_prefix,\n                                            member_param=member_param,\n                                            unique_member_param=unique_member_param)\n\n    for subroute in _reduce_routes(subresources):\n        subroute._parent_resource_cls = resource_cls\n        subroute._parent_member_param = member_param\n        subroute._unique_member_param = unique_member_param\n        subroute = subroute.copy()\n        subroute.rule = rename_parent_resource_param_name(\n            subroute, rule=join(url_prefix, member_param, subroute.rule,\n                                trailing_slash=subroute.rule.endswith('/')))\n        yield subroute", "response": "This function returns a route generator that can be used to generate a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a LocalProxy to the extension or service with the given name as registered with the current app.", "response": "def get_local_proxy(self, name):\n        \"\"\"\n        Returns a :class:`~werkzeug.local.LocalProxy` to the extension or service\n        with ``name`` as registered with the current app.\n        \"\"\"\n        def get_extension_or_service_by_name():\n            if name in current_app.unchained.extensions:\n                return current_app.unchained.extensions[name]\n            elif name in current_app.unchained.services:\n                return current_app.unchained.services[name]\n            raise KeyError(f'No extension or service was found with the name {name}.')\n\n        return LocalProxy(get_extension_or_service_by_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef service(self, name: str = None):\n        if self._services_initialized:\n            from warnings import warn\n            warn('Services have already been initialized. Please register '\n                 f'{name} sooner.')\n            return lambda x: x\n\n        def wrapper(service):\n            self.register_service(name, service)\n            return service\n        return wrapper", "response": "Decorator to mark something as a service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inject(self, *args):\n        used_without_parenthesis = len(args) and callable(args[0])\n        has_explicit_args = len(args) and all(isinstance(x, str) for x in args)\n\n        def wrapper(fn):\n            cls = None\n            if isinstance(fn, type):\n                cls = fn\n                fn = cls.__init__\n\n            # check if the fn has already been wrapped with inject\n            if hasattr(fn, '__signature__'):\n                if cls and not hasattr(cls, '__signature__'):\n                    # this happens when both the class and its __init__ method\n                    # where decorated with @inject. which would be silly, but,\n                    # it should still work regardless\n                    cls.__signature__ = fn.__signature__\n\n                if not cls:\n                    return fn\n\n            sig = inspect.signature(fn)\n\n            # create a new function wrapping the original to inject params\n            @functools.wraps(fn)\n            def new_fn(*fn_args, **fn_kwargs):\n                # figure out which params we need to inject (we don't want to\n                # interfere with any params the user has passed manually)\n                bound_args = sig.bind_partial(*fn_args, **fn_kwargs)\n                required = set(sig.parameters.keys())\n                have = set(bound_args.arguments.keys())\n                need = required.difference(have)\n                to_inject = (args if has_explicit_args\n                             else set([k for k, v in sig.parameters.items()\n                                       if v.default == injectable]))\n\n                # try to inject needed params from extensions or services\n                for param_name in to_inject:\n                    if param_name not in need:\n                        continue\n                    if param_name in self.extensions:\n                        fn_kwargs[param_name] = self.extensions[param_name]\n                    elif param_name in self.services:\n                        fn_kwargs[param_name] = self.services[param_name]\n\n                # check to make sure we we're not missing anything required\n                bound_args = sig.bind_partial(*fn_args, **fn_kwargs)\n                bound_args.apply_defaults()\n                for k, v in bound_args.arguments.items():\n                    if isinstance(v, str) and v == injectable:\n                        di_name = new_fn.__di_name__\n                        is_constructor = ('.' not in di_name\n                                          and di_name != di_name.lower())\n                        action = 'initialized' if is_constructor else 'called'\n                        msg = f'{di_name} was {action} without the ' \\\n                              f'{k} parameter. Please supply it manually, or '\\\n                               'make sure it gets injected.'\n                        raise ServiceUsageError(msg)\n\n                if cls and not getattr(cls, _DI_AUTOMATICALLY_HANDLED, False):\n                    cls_attrs_to_inject = getattr(cls, _INJECT_CLS_ATTRS, [])\n                    for attr, value in vars(cls).items():\n                        if value == injectable:\n                            cls_attrs_to_inject.append(attr)\n\n                    if cls_attrs_to_inject:\n                        setattr(cls, _INJECT_CLS_ATTRS, cls_attrs_to_inject)\n                        _inject_cls_attrs()(cls)\n\n                return fn(*bound_args.args, **bound_args.kwargs)\n\n            new_fn.__signature__ = sig\n            new_fn.__di_name__ = getattr(fn, '__di_name__', fn.__name__)\n\n            if cls:\n                cls.__init__ = new_fn\n                cls.__signature__ = sig\n                for attr, meth in vars(cls).items():\n                    if (attr.startswith('__')\n                            or not callable(meth)\n                            or hasattr(meth, '__signature__')):\n                        continue\n                    setattr(cls, attr, self.inject()(meth))\n                return cls\n            return new_fn\n\n        if used_without_parenthesis:\n            return wrapper(args[0])\n        return wrapper", "response": "Decorator to mark a class method or function as needing dependencies injected."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a new url rule. Acts the same as flask. add_url_rule.", "response": "def add_url_rule(self, rule, endpoint=None, view_func=None, **options):\n        \"\"\"\n        Register a new url rule. Acts the same as :meth:`flask.Flask.add_url_rule`.\n        \"\"\"\n        self._defer(lambda app: app.add_url_rule(rule,\n                                                 endpoint=endpoint,\n                                                 view_func=view_func,\n                                                 **options))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a function to run before each request.", "response": "def before_request(self, fn):\n        \"\"\"\n        Registers a function to run before each request.\n\n        For example, this can be used to open a database connection, or to load\n        the logged in user from the session.\n\n        The function will be called without any arguments. If it returns a\n        non-None value, the value is handled as if it was the return value from\n        the view, and further request handling is stopped.\n        \"\"\"\n        self._defer(lambda app: app.before_request(fn))\n        return fn"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef before_first_request(self, fn):\n        self._defer(lambda app: app.before_first_request(fn))\n        return fn", "response": "Registers a function to be run before the first request to this application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef after_request(self, fn):\n        self._defer(lambda app: app.after_request(fn))\n        return fn", "response": "Register a function to be run after each request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef teardown_request(self, fn):\n        self._defer(lambda app: app.teardown_request(fn))\n        return fn", "response": "Register a function to be run at the end of each request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a function to be called when the application context ends.", "response": "def teardown_appcontext(self, fn):\n        \"\"\"\n        Registers a function to be called when the application context\n        ends.  These functions are typically also called when the request\n        context is popped.\n\n        Example::\n\n            ctx = app.app_context()\n            ctx.push()\n            ...\n            ctx.pop()\n\n        When ``ctx.pop()`` is executed in the above example, the teardown\n        functions are called just before the app context moves from the\n        stack of active contexts.  This becomes relevant if you are using\n        such constructs in tests.\n\n        Since a request context typically also manages an application\n        context it would also be called when you pop a request context.\n\n        When a teardown function was called because of an unhandled exception\n        it will be passed an error object. If an :meth:`errorhandler` is\n        registered, it will handle the exception and the teardown will not\n        receive it.\n\n        The return values of teardown functions are ignored.\n        \"\"\"\n        self._defer(lambda app: app.teardown_appcontext(fn))\n        return fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef context_processor(self, fn):\n        self._defer(lambda app: app.context_processor(fn))\n        return fn", "response": "Registers a template context processor function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shell_context_processor(self, fn):\n        self._defer(lambda app: app.shell_context_processor(fn))\n        return fn", "response": "Registers a shell context processor function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a function to be called before the url_value_preprocessor function for all view functions in the application.", "response": "def url_value_preprocessor(self, fn):\n        \"\"\"\n        Register a URL value preprocessor function for all view\n        functions in the application. These functions will be called before the\n        :meth:`before_request` functions.\n\n        The function can modify the values captured from the matched url before\n        they are passed to the view. For example, this can be used to pop a\n        common language code value and place it in ``g`` rather than pass it to\n        every view.\n\n        The function is passed the endpoint name and values dict. The return\n        value is ignored.\n        \"\"\"\n        self._defer(lambda app: app.url_value_preprocessor(fn))\n        return fn"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef errorhandler(self, code_or_exception):\n        def decorator(fn):\n            self._defer(lambda app: app.register_error_handler(code_or_exception, fn))\n            return fn\n        return decorator", "response": "Register a function to handle errors by code or exception class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef template_filter(self,\n                        arg: Optional[Callable] = None,\n                        *,\n                        name: Optional[str] = None,\n                        pass_context: bool = False,\n                        inject: Optional[Union[bool, Iterable[str]]] = None,\n                        safe: bool = False,\n                        ) -> Callable:\n        \"\"\"\n        Decorator to mark a function as a Jinja template filter.\n\n        :param name: The name of the filter, if different from the function name.\n        :param pass_context: Whether or not to pass the template context into the filter.\n            If ``True``, the first argument must be the context.\n        :param inject: Whether or not this filter needs any dependencies injected.\n        :param safe: Whether or not to mark the output of this filter as html-safe.\n        \"\"\"\n        def wrapper(fn):\n            fn = _inject(fn, inject)\n            if safe:\n                fn = _make_safe(fn)\n            if pass_context:\n                fn = jinja2.contextfilter(fn)\n            self._defer(lambda app: app.add_template_filter(fn, name=name))\n            return fn\n\n        if callable(arg):\n            return wrapper(arg)\n        return wrapper", "response": "Decorator to mark a function as a Jinja template filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naliasing for :meth:`template_global`. :param name: The name of the tag, if different from the function name. :param pass_context: Whether or not to pass the template context into the tag. If ``True``, the first argument must be the context. :param inject: Whether or not this tag needs any dependencies injected. :param safe: Whether or not to mark the output of this tag as html-safe.", "response": "def template_tag(self,\n                     arg: Optional[Callable] = None,\n                     *,\n                     name: Optional[str] = None,\n                     pass_context: bool = False,\n                     inject: Optional[Union[bool, Iterable[str]]] = None,\n                     safe: bool = False,\n                     ) -> Callable:\n        \"\"\"\n        Alias for :meth:`template_global`.\n\n        :param name: The name of the tag, if different from the function name.\n        :param pass_context: Whether or not to pass the template context into the tag.\n            If ``True``, the first argument must be the context.\n        :param inject: Whether or not this tag needs any dependencies injected.\n        :param safe: Whether or not to mark the output of this tag as html-safe.\n        \"\"\"\n        return self.template_global(arg, name=name, pass_context=pass_context,\n                                    inject=inject, safe=safe)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reset(self):\n        self.bundles = AttrDict()\n        self._bundles = _DeferredBundleFunctionsStore()\n        self.babel_bundle = None\n        self.env = None\n        self.extensions = AttrDict()\n        self.services = AttrDict()\n\n        self._deferred_functions = []\n        self._initialized = False\n        self._models_initialized = False\n        self._services_initialized = False\n        self._services_registry = {}\n        self._shell_ctx = {}", "response": "Reset the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fields2jsonschema(self, fields, schema=None, use_refs=True, dump=True, name=None):\n        Meta = getattr(schema, 'Meta', None)\n        if getattr(Meta, 'additional', None):\n            declared_fields = set(schema._declared_fields.keys())\n            if set(getattr(Meta, 'additional', set())) > declared_fields:\n                import warnings\n                warnings.warn(\n                    'Only explicitly-declared fields will be included in the Schema Object. '\n                    'Fields defined in Meta.fields or Meta.additional are ignored.',\n                )\n\n        jsonschema = {\n            'type': 'object',\n            'properties': (OrderedLazyDict() if getattr(Meta, 'ordered', None)\n                           else LazyDict()),\n        }\n\n        exclude = set(getattr(Meta, 'exclude', []))\n\n        for field_name, field_obj in iteritems(fields):\n            if field_name in exclude or (field_obj.dump_only and not dump):\n                continue\n\n            observed_field_name = self._observed_name(field_obj, field_name)\n            prop_func = lambda field_obj=field_obj: self.field2property(  # flake8: noqa\n                field_obj, use_refs=use_refs, dump=dump, name=name,\n            )\n            jsonschema['properties'][observed_field_name] = prop_func\n\n            partial = getattr(schema, 'partial', None)\n            if field_obj.required:\n                if not partial or (is_collection(partial) and field_name not in partial):\n                    jsonschema.setdefault('required', []).append(observed_field_name)\n\n        if 'required' in jsonschema:\n            jsonschema['required'].sort()\n\n        if Meta is not None:\n            if hasattr(Meta, 'title'):\n                jsonschema['title'] = Meta.title\n            if hasattr(Meta, 'description'):\n                jsonschema['description'] = Meta.description\n\n        if getattr(schema, 'many', False):\n            jsonschema = {\n                'type': 'array',\n                'items': jsonschema,\n            }\n\n        return jsonschema", "response": "Return a JSON Schema Object for a given marshmallow. Schema instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef model_fields(model, db_session=None, only=None, exclude=None,\n                 field_args=None, converter=None, exclude_pk=False,\n                 exclude_fk=False):\n    \"\"\"\n    Generate a dictionary of fields for a given SQLAlchemy model.\n\n    See `model_form` docstring for description of parameters.\n    \"\"\"\n    mapper = model._sa_class_manager.mapper\n    converter = converter or _ModelConverter()\n    field_args = field_args or {}\n    properties = []\n\n    for prop in mapper.iterate_properties:\n        if getattr(prop, 'columns', None):\n            if exclude_fk and prop.columns[0].foreign_keys:\n                continue\n            elif exclude_pk and prop.columns[0].primary_key:\n                continue\n\n        properties.append((prop.key, prop))\n\n    # the following if condition is modified:\n    if only is not None:\n        properties = (x for x in properties if x[0] in only)\n    elif exclude:\n        properties = (x for x in properties if x[0] not in exclude)\n\n    field_dict = {}\n    for name, prop in properties:\n        field = converter.convert(\n            model, mapper, prop,\n            field_args.get(name), db_session\n        )\n        if field is not None:\n            field_dict[name] = field\n\n    return field_dict", "response": "Generate a dictionary of fields for a given SQLAlchemy model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send_from_directories(directories, filename, **options):\n    for directory in directories:\n        filename = safe_join(directory, filename)\n        if not os.path.isabs(filename):\n            filename = os.path.join(current_app.root_path, filename)\n        try:\n            if not os.path.isfile(filename):\n                continue\n        except (TypeError, ValueError):\n            raise BadRequest()\n        options.setdefault('conditional', True)\n        return send_file(filename, **options)\n    raise NotFound()", "response": "Send a file from a list of directories."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_url_rule(self, rule, endpoint=None, view_func=None, **options):\n        if self.url_prefix:\n            rule = self.url_prefix + rule\n        options.setdefault('subdomain', self.subdomain)\n        if endpoint is None:\n            endpoint = _endpoint_from_view_func(view_func)\n        defaults = self.url_defaults\n        if 'defaults' in options:\n            defaults = dict(defaults, **options.pop('defaults'))\n        self.app.add_url_rule(rule, endpoint, view_func, defaults=defaults, **options)", "response": "A helper method to register a url rule and optionally a view function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a url rule to the blueprint.", "response": "def add_url_rule(self, rule, endpoint=None, view_func=None, **options):\n        \"\"\"\n        Like :meth:`~flask.Flask.add_url_rule` but for a blueprint.  The endpoint for\n        the :func:`url_for` function is prefixed with the name of the blueprint.\n\n        Overridden to allow dots in endpoint names\n        \"\"\"\n        self.record(lambda s: s.add_url_rule(rule, endpoint, view_func,\n                                             register_with_babel=False, **options))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling by flask. Flask. register_blueprint to register a new blueprint on the application.", "response": "def register(self, app, options, first_registration=False):\n        \"\"\"\n        Called by :meth:`~flask.Flask.register_blueprint` to register a blueprint\n        on the application.  This can be overridden to customize the register\n        behavior.  Keyword arguments from\n        :func:`~flask.Flask.register_blueprint` are directly forwarded to this\n        method in the `options` dictionary.\n        \"\"\"\n        self._got_registered_once = True\n        state = self.make_setup_state(app, options, first_registration)\n        if self.has_static_folder:\n            state.add_url_rule(self.static_url_path + '/<path:filename>',\n                               view_func=self.send_static_file,\n                               endpoint=f'{self.bundle._blueprint_name}.static',\n                               register_with_babel=False)\n\n        for deferred in self.bundle._deferred_functions:\n            deferred(self)\n\n        for deferred in self.deferred_functions:\n            deferred(state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef url(url: str, method: str):\n    try:\n        url_rule, params = (current_app.url_map.bind('localhost')\n                            .match(url, method=method, return_rule=True))\n    except (NotFound, MethodNotAllowed)\\\n            as e:\n        click.secho(str(e), fg='white', bg='red')\n    else:\n        headings = ('Method(s)', 'Rule', 'Params', 'Endpoint', 'View', 'Options')\n        print_table(headings,\n                    [(_get_http_methods(url_rule),\n                      url_rule.rule if url_rule.strict_slashes\n                                    else url_rule.rule + '[/]',\n                      _format_dict(params),\n                      url_rule.endpoint,\n                      _get_rule_view(url_rule),\n                      _format_rule_options(url_rule))],\n                    ['<' if i > 0 else '>' for i, col in enumerate(headings)],\n                    primary_column_idx=1)", "response": "Show details for a specific URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all URLs registered with the app.", "response": "def urls(order_by: Optional[str] = None):\n    \"\"\"List all URLs registered with the app.\"\"\"\n    url_rules: List[Rule] = current_app.url_map._rules\n\n    # sort the rules. by default they're sorted by priority,\n    # ie in the order they were registered with the app\n    if order_by == 'view':\n        url_rules = sorted(url_rules, key=lambda rule: _get_rule_view(rule))\n    elif order_by != 'priority':\n        url_rules = sorted(url_rules, key=lambda rule: getattr(rule, order_by))\n\n    headings = ('Method(s)', 'Rule', 'Endpoint', 'View', 'Options')\n    print_table(headings,\n                [(_get_http_methods(url_rule),\n                  url_rule.rule if url_rule.strict_slashes\n                                else url_rule.rule.rstrip('/') + '[/]',\n                  url_rule.endpoint,\n                  _get_rule_view(url_rule),\n                  _format_rule_options(url_rule),\n                  ) for url_rule in url_rules],\n                ['<' if i > 0 else '>' for i, col in enumerate(headings)],\n                primary_column_idx=1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_serializer(self, serializer, name=None, **kwargs):\n        name = name or serializer.Meta.model.__name__\n        self.spec.definition(name, schema=serializer, **kwargs)", "response": "Method to manually register a serializer with APISpec."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering custom path parameter converter.", "response": "def register_converter(self, converter, conv_type, conv_format=None, *, name=None):\n        \"\"\"\n        Register custom path parameter converter.\n\n        :param BaseConverter converter: Converter\n            Subclass of werkzeug's BaseConverter\n        :param str conv_type: Parameter type\n        :param str conv_format: Parameter format (optional)\n        :param str name: Name of the converter. If not None, this name is used\n            to register the converter in the Flask app.\n            Example::\n\n                api.register_converter(\n                    UUIDConverter, 'string', 'UUID', name='uuid')\n                @blp.route('/pets/{uuid:pet_id}')\n                    # ...\n                api.register_blueprint(blp)\n\n        This registers the converter in the Flask app and in the internal\n        APISpec instance.\n\n        Once the converter is registered, all paths using it will have\n        corresponding path parameter documented with the right type and format.\n        The `name` parameter need not be passed if the converter is already\n        registered in the app, for instance if it belongs to a Flask extension\n        that already registers it in the app.\n        \"\"\"\n        if name:\n            self.app.url_map.converters[name] = converter\n        self.spec.register_converter(converter, conv_type, conv_format)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhook entry point. Override to disable standard behavior of iterating over bundles to discover objects and processing them.", "response": "def run_hook(self, app: FlaskUnchained, bundles: List[Bundle]):\n        \"\"\"\n        Hook entry point. Override to disable standard behavior of iterating\n        over bundles to discover objects and processing them.\n        \"\"\"\n        self.process_objects(app, self.collect_from_bundles(bundles))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collect_from_bundles(self, bundles: List[Bundle]) -> Dict[str, Any]:\n        all_objects = {}  # all discovered objects\n        key_bundles = {}  # lookup of which bundle a key came from\n        object_keys = set()  # keys in all_objects, used to ensure uniqueness\n        for bundle in bundles:\n            from_bundle = self.collect_from_bundle(bundle)\n            if isinstance(bundle, AppBundle):\n                all_objects.update(from_bundle)\n                break  # app_bundle is last, no need to update keys\n\n            from_bundle_keys = set(from_bundle.keys())\n            conflicts = object_keys.intersection(from_bundle_keys)\n            if conflicts:\n                msg = [f'{self.name} from {bundle.name} conflict with '\n                       f'previously registered {self.name}:']\n                for key in conflicts:\n                    msg.append(f'{key} from {key_bundles[key].name}')\n                raise NameCollisionError('\\n'.join(msg))\n\n            all_objects.update(from_bundle)\n            object_keys = object_keys.union(from_bundle_keys)\n            key_bundles.update({k: bundle for k in from_bundle_keys})\n\n        return all_objects", "response": "Collect objects where type_check returns True from bundles."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects objects where type_check returns True from bundles.", "response": "def collect_from_bundle(self, bundle: Bundle) -> Dict[str, Any]:\n        \"\"\"\n        Collect objects where :meth:`type_check` returns ``True`` from bundles.\n        Bundle subclasses can override objects discovered in superclass bundles.\n        \"\"\"\n        members = {}\n        hierarchy = ([bundle] if not self.discover_from_bundle_superclasses\n                     else bundle._iter_class_hierarchy())\n        for bundle in hierarchy:\n            module = self.import_bundle_module(bundle)\n            if not module:\n                continue\n            members.update(self._collect_from_package(module))\n        return members"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _collect_from_package(self, module,\n                              type_checker: Optional[FunctionType] = None,\n                              ) -> Dict[str, Any]:\n        \"\"\"\n        Discover objects passing :meth:`type_check` by walking through all the\n        child modules/packages in the given module (ie, do not require modules\n        to import everything into their ``__init__.py`` for it to be discovered)\n        \"\"\"\n        type_checker = type_checker or self.type_check\n        members = dict(self._get_members(module, type_checker))\n\n        if pkgutil.get_loader(module).is_package(module.__name__):\n            for loader, name, is_pkg in pkgutil.walk_packages(module.__path__):\n                child_module_name = f'{module.__package__}.{name}'\n                child_module = importlib.import_module(child_module_name)\n                for key, obj in self._get_members(child_module, type_checker):\n                    if key not in members:\n                        members[key] = obj\n\n        return members", "response": "Collect all the objects in the given module and its subpackages."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef de_camel(s, separator=\"_\", _lowercase=True):\n    s = re.sub(r\"([a-z0-9])([A-Z])\", \"\\\\1%s\\\\2\" % separator, s)\n    s = re.sub(r\"([A-Z])([A-Z][a-z])\", \"\\\\1%s\\\\2\" % separator, s)\n    return s.lower() if _lowercase else s", "response": "Returns the string with CamelCase converted to underscores e. g. tom - de - smedt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the plural of a given word.", "response": "def pluralize(word, pos=NOUN, custom=None, classical=True):\n    \"\"\" Returns the plural of a given word, e.g., child => children.\n        Handles nouns and adjectives, using classical inflection by default\n        (i.e., where \"matrix\" pluralizes to \"matrices\" and not \"matrixes\").\n        The custom dictionary is for user-defined replacements.\n    \"\"\"\n    if custom and word in custom:\n        return custom[word]\n    # Recurse genitives.\n    # Remove the apostrophe and any trailing -s,\n    # form the plural of the resultant noun, and then append an apostrophe (dog's => dogs').\n    if word.endswith((\"'\", \"'s\")):\n        w = word.rstrip(\"'s\")\n        w = pluralize(w, pos, custom, classical)\n        if w.endswith(\"s\"):\n            return w + \"'\"\n        else:\n            return w + \"'s\"\n    # Recurse compound words\n    # (e.g., Postmasters General, mothers-in-law, Roman deities).\n    w = word.replace(\"-\", \" \").split(\" \")\n    if len(w) > 1:\n        if w[1] == \"general\" or \\\n           w[1] == \"General\" and \\\n           w[0] not in plural_categories[\"general-generals\"]:\n            return word.replace(w[0], pluralize(w[0], pos, custom, classical))\n        elif w[1] in plural_prepositions:\n            return word.replace(w[0], pluralize(w[0], pos, custom, classical))\n        else:\n            return word.replace(w[-1], pluralize(w[-1], pos, custom, classical))\n    # Only a very few number of adjectives inflect.\n    n = range(len(plural_rules))\n    if pos.startswith(ADJECTIVE):\n        n = [0, 1]\n    # Apply pluralization rules.\n    for i in n:\n        for suffix, inflection, category, classic in plural_rules[i]:\n            # A general rule, or a classic rule in classical mode.\n            if category is None:\n                if not classic or (classic and classical):\n                    if suffix.search(word) is not None:\n                        return suffix.sub(inflection, word)\n            # A rule pertaining to a specific category of words.\n            if category is not None:\n                if word in plural_categories[category] and (not classic or (classic and classical)):\n                    if suffix.search(word) is not None:\n                        return suffix.sub(inflection, word)\n    return word"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef singularize(word, pos=NOUN, custom=None):\n    if custom and word in custom:\n        return custom[word]\n    # Recurse compound words (e.g. mothers-in-law).\n    if \"-\" in word:\n        w = word.split(\"-\")\n        if len(w) > 1 and w[1] in plural_prepositions:\n            return singularize(w[0], pos, custom) + \"-\" + \"-\".join(w[1:])\n    # dogs' => dog's\n    if word.endswith(\"'\"):\n        return singularize(word[:-1]) + \"'s\"\n    w = word.lower()\n    for x in singular_uninflected:\n        if x.endswith(w):\n            return word\n    for x in singular_uncountable:\n        if x.endswith(w):\n            return word\n    for x in singular_ie:\n        if w.endswith(x + \"s\"):\n            return w[:-1]\n    for x in singular_irregular:\n        if w.endswith(x):\n            return re.sub('(?i)' + x + '$', singular_irregular[x], word)\n    for suffix, inflection in singular_rules:\n        m = suffix.search(word)\n        g = m and m.groups() or []\n        if m:\n            for k in range(len(g)):\n                if g[k] is None:\n                    inflection = inflection.replace('\\\\' + str(k + 1), '')\n            return suffix.sub(inflection, word)\n    return word", "response": "Returns the singular of a given word."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an application instance of the Unchained application class.", "response": "def create_app(cls,\n                   env: Union[DEV, PROD, STAGING, TEST],\n                   bundles: Optional[List[str]] = None,\n                   _config_overrides: Optional[Dict[str, Any]] = None,\n                   **flask_kwargs) -> FlaskUnchained:\n        \"\"\"\n        Flask Unchained Application Factory. Returns an instance of\n        :class:`~flask_unchained.FlaskUnchained`.\n\n        Example Usage::\n\n            app = AppFactory.create_app(DEV)\n\n        :param env: Which environment the app should run in. Should be one of\n                    \"development\", \"production\", \"staging\", or \"test\" (you can import\n                    them: ``from flask_unchained import DEV, PROD, STAGING, TEST``)\n        :param bundles: An optional list of bundle modules names to use (mainly\n                        useful for testing)\n        :param flask_kwargs: keyword argument overrides for the :class:`FlaskUnchained`\n                             constructor\n        :param _config_overrides: a dictionary of config option overrides; meant for\n                                  test fixtures.\n        :return: The :class:`~flask_unchained.FlaskUnchained` application instance\n        \"\"\"\n        unchained_config = _load_unchained_config(env)\n        app_bundle, bundles = _load_bundles(\n            bundles or getattr(unchained_config, 'BUNDLES', []))\n\n        if app_bundle is None and env != TEST:\n            return cls.create_basic_app(bundles, _config_overrides=_config_overrides)\n\n        for k in ['TEMPLATE_FOLDER', 'STATIC_FOLDER', 'STATIC_URL_PATH']:\n            flask_kwargs.setdefault(k.lower(), getattr(unchained_config, k, None))\n\n        app_import_name = (app_bundle.module_name.split('.')[0]\n                           if app_bundle else 'tests')\n        app = FlaskUnchained(app_import_name, **flask_kwargs)\n\n        # Flask assumes the root_path is based on the app_import_name, but\n        # we want it to be the project root, not the app bundle root\n        app.root_path = os.path.dirname(app.root_path)\n        app.static_folder = flask_kwargs['static_folder']\n\n        for bundle in bundles:\n            bundle.before_init_app(app)\n\n        unchained.init_app(app, env, bundles, _config_overrides=_config_overrides)\n\n        for bundle in bundles:\n            bundle.after_init_app(app)\n\n        return app"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a fake app for use while developing", "response": "def create_basic_app(cls, bundles=None, _config_overrides=None):\n        \"\"\"\n        Creates a \"fake\" app for use while developing\n        \"\"\"\n        bundles = bundles or []\n        name = bundles[-1].module_name if bundles else 'basic_app'\n        app = FlaskUnchained(name, template_folder=os.path.join(\n            os.path.dirname(__file__), 'templates'))\n\n        for bundle in bundles:\n            bundle.before_init_app(app)\n\n        unchained.init_app(app, DEV, bundles, _config_overrides=_config_overrides)\n\n        for bundle in bundles:\n            bundle.after_init_app(app)\n\n        return app"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a blueprint with Babel.", "response": "def register_blueprint(self, blueprint, register_with_babel=True, **options):\n        \"\"\"\n        Like :meth:`~flask.Flask.register_blueprint`, but if ``register_with_babel``\n        is True, then we also allow the Babel Bundle an opportunity to register language\n        code prefixed URLs.\n        \"\"\"\n        if self.unchained.babel_bundle and register_with_babel:\n            self.unchained.babel_bundle.register_blueprint(self, blueprint, **options)\n        return super().register_blueprint(blueprint, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_url_rule(self, rule, endpoint=None, view_func=None,\n                     provide_automatic_options=None, register_with_babel=False,\n                     **options):\n        \"\"\"\n        Like :meth:`~flask.Flask.add_url_rule`, but if ``register_with_babel`` is True,\n        then we also allow the Babel Bundle an opportunity to register a language code\n        prefixed URL.\n        \"\"\"\n        if self.unchained.babel_bundle and register_with_babel:\n            self.unchained.babel_bundle.add_url_rule(\n                self, rule, endpoint=endpoint, view_func=view_func,\n                provide_automatic_options=provide_automatic_options, **options)\n        return super().add_url_rule(rule, endpoint, view_func,\n                                    provide_automatic_options, **options)", "response": "Add a url rule to the set of unlisted modules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the localized translation of the message based on the language specified in the translation key.", "response": "def gettext(*args, **kwargs):\n    \"\"\"\n    Return the localized translation of message, based on the language, and\n    locale directory of the domain specified in the translation key (or the\n    current global domain). This function is usually aliased as ``_``.\n    \"\"\"\n    key = args[0]\n    key_match = TRANSLATION_KEY_RE.match(key)\n    translation = _gettext(*args, **kwargs)\n    if not key_match or translation != key:\n        return translation\n\n    return _get_domain(key_match).gettext(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ngettext(*args, **kwargs):\n    is_plural = args[2] > 1\n    if not is_plural:\n        key = args[0]\n        key_match = TRANSLATION_KEY_RE.match(key)\n    else:\n        key = args[1]\n        key_match = PLURAL_TRANSLATION_KEY_RE.match(key)\n\n    translation = _ngettext(*args, **kwargs)\n    if not key_match or translation != key:\n        return translation\n\n    return _get_domain(key_match).ngettext(*args, **kwargs)", "response": "Like _ngettext except it supports pluralization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new role.", "response": "def create_role(name):\n    \"\"\"\n    Create a new role.\n    \"\"\"\n    role = role_manager.create(name=name)\n    if click.confirm(f'Are you sure you want to create {role!r}?'):\n        role_manager.save(role, commit=True)\n        click.echo(f'Successfully created {role!r}')\n    else:\n        click.echo('Cancelled.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclass decorator for SQLAlchemy models to attach listeners on class methods.", "response": "def attach_events(*args):\n    \"\"\"Class decorator for SQLAlchemy models to attach listeners on class\n    methods decorated with :func:`.on`\n\n    Usage::\n\n        @attach_events\n        class User(Model):\n            email = Column(String(50))\n\n            @on('email', 'set')\n            def lowercase_email(self, new_value, old_value, initiating_event):\n                self.email = new_value.lower()\n    \"\"\"\n    def wrapper(cls):\n        for name, fn in cls.__dict__.items():\n            if not name.startswith('__') and hasattr(fn, _SQLAlchemyEvent.ATTR):\n                e = getattr(fn, _SQLAlchemyEvent.ATTR)\n                if e.field_name:\n                    event.listen(getattr(cls, e.field_name), e.event_name, fn,\n                                 **e.listen_kwargs)\n                else:\n                    event.listen(cls, e.event_name, fn, **e.listen_kwargs)\n        return cls\n    if args and callable(args[0]):\n        return wrapper(args[0])\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on(*args, **listen_kwargs):\n    if len(args) == 1:\n        field_name, event_name = (None, args[0])\n    elif len(args) == 2:\n        field_name, event_name = args\n    else:\n        raise NotImplementedError('@on accepts only one or two positional arguments')\n\n    def wrapper(fn):\n        setattr(fn, _SQLAlchemyEvent.ATTR,\n                _SQLAlchemyEvent(field_name, event_name, listen_kwargs))\n        return fn\n    return wrapper", "response": "Decorator for SQLAlchemy models."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef slugify(field_name, slug_field_name=None, mutable=False):\n    slug_field_name = slug_field_name or 'slug'\n\n    def _set_slug(target, value, old_value, _, mutable=False):\n        existing_slug = getattr(target, slug_field_name)\n        if existing_slug and not mutable:\n            return\n        if value and (not existing_slug or value != old_value):\n            setattr(target, slug_field_name, _slugify(value))\n\n    def wrapper(cls):\n        event.listen(getattr(cls, field_name), 'set',\n                     partial(_set_slug, mutable=mutable))\n        return cls\n    return wrapper", "response": "A class decorator that creates a slugified version of the content of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_message_plain_text(msg: Message):\n    if msg.body:\n        return msg.body\n\n    if BeautifulSoup is None or not msg.html:\n        return msg.html\n\n    plain_text = '\\n'.join(line.strip() for line in\n                           BeautifulSoup(msg.html, 'lxml').text.splitlines())\n    return re.sub(r'\\n\\n+', '\\n\\n', plain_text).strip()", "response": "Converts an HTML message to plain text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_message(subject_or_message: Union[str, Message],\n                 to: Union[str, List[str]],\n                 template: Optional[str] = None,\n                 **kwargs):\n    \"\"\"\n    Creates a new :class:`~flask_mail.Message` from the given arguments.\n\n    :param subject_or_message: A subject string, or for backwards compatibility with\n                               stock Flask-Mail, a :class:`~flask_mail.Message` instance\n    :param to: An email address, or a list of email addresses\n    :param template: Which template to render.\n    :param kwargs: Extra kwargs to pass on to :class:`~flask_mail.Message`\n    :return: The created :class:`~flask_mail.Message`\n    \"\"\"\n    if isinstance(subject_or_message, Message):\n        return subject_or_message\n\n    if isinstance(to, tuple):\n        to = list(to)\n    elif not isinstance(to, list):\n        to = [to]\n    msg = Message(subject=subject_or_message, recipients=to, **{\n        k: kwargs[k] for k in message_kwargs & set(kwargs)})\n\n    if not msg.html and template:\n        msg.html = render_template(template, **kwargs)\n    if not msg.body:\n        msg.body = get_message_plain_text(msg)\n    return msg", "response": "Creates a new Message instance from the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefault function used for sending emails.", "response": "def _send_mail(subject_or_message: Optional[Union[str, Message]] = None,\n               to: Optional[Union[str, List[str]]] = None,\n               template: Optional[str] = None,\n               **kwargs):\n    \"\"\"\n    The default function used for sending emails.\n\n    :param subject_or_message: A subject string, or for backwards compatibility with\n                               stock Flask-Mail, a :class:`~flask_mail.Message` instance\n    :param to: An email address, or a list of email addresses\n    :param template: Which template to render.\n    :param kwargs: Extra kwargs to pass on to :class:`~flask_mail.Message`\n    \"\"\"\n    subject_or_message = subject_or_message or kwargs.pop('subject')\n    to = to or kwargs.pop('recipients', [])\n    msg = make_message(subject_or_message, to, template, **kwargs)\n    with mail.connect() as connection:\n        connection.send(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_role(self, role):\n        if isinstance(role, str):\n            return role in (role.name for role in self.roles)\n        else:\n            return role in self.roles", "response": "Returns True if the user identifies with the specified role."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract(domain):\n    translations_dir = _get_translations_dir()\n    domain = _get_translations_domain(domain)\n    babel_cfg = _get_babel_cfg()\n    pot = os.path.join(translations_dir, f'{domain}.pot')\n    return _run(f'extract -F {babel_cfg} -o {pot} .')", "response": "Extract newly added translations keys from source code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init(lang, domain):\n    translations_dir = _get_translations_dir()\n    domain = _get_translations_domain(domain)\n    pot = os.path.join(translations_dir, f'{domain}.pot')\n    return _run(f'init -i {pot} -d {translations_dir} -l {lang} --domain={domain}')", "response": "Initialize translations for a language code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(domain):\n    translations_dir = _get_translations_dir()\n    domain = _get_translations_domain(domain)\n    pot = os.path.join(translations_dir, f'{domain}.pot')\n    return _run(f'update -i {pot} -d {translations_dir} --domain={domain}')", "response": "Update language - specific translations files with new keys discovered by\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_value(self, meta, base_model_meta, mcs_args: McsArgs):\n        if mcs_args.Meta.abstract:\n            return None\n        value = getattr(base_model_meta, self.name, {}) or {}\n        value.update(getattr(meta, self.name, {}))\n        return value", "response": "overridden to merge with inherited value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflash the message with the given category.", "response": "def flash(self, msg: str, category: Optional[str] = None):\n        \"\"\"\n        Convenience method for flashing messages.\n\n        :param msg: The message to flash.\n        :param category: The category of the message.\n        \"\"\"\n        if not request.is_json and app.config.FLASH_MESSAGES:\n            flash(msg, category)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, template_name: str, **ctx):\n        if '.' not in template_name:\n            template_file_extension = (self.Meta.template_file_extension\n                                       or app.config.TEMPLATE_FILE_EXTENSION)\n            template_name = f'{template_name}{template_file_extension}'\n        if self.Meta.template_folder_name and os.sep not in template_name:\n            template_name = os.path.join(self.Meta.template_folder_name,\n                                         template_name)\n        return render_template(template_name, **ctx)", "response": "Convenience method for rendering a template."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nredirect to a URL or URL rule.", "response": "def redirect(self,\n                 where: Optional[str] = None,\n                 default: Optional[str] = None,\n                 override: Optional[str] = None,\n                 **url_kwargs):\n        \"\"\"\n        Convenience method for returning redirect responses.\n\n        :param where: A URL, endpoint, or config key name to redirect to.\n        :param default: A URL, endpoint, or config key name to redirect to if\n                        ``where`` is invalid.\n        :param override: explicitly redirect to a URL, endpoint, or config key name\n                         (takes precedence over the ``next`` value in query strings\n                         or forms)\n        :param url_kwargs: the variable arguments of the URL rule\n        :param _anchor: if provided this is added as anchor to the URL.\n        :param _external: if set to ``True``, an absolute URL is generated. Server\n                          address can be changed via ``SERVER_NAME`` configuration\n                          variable which defaults to `localhost`.\n        :param _external_host: if specified, the host of an external server to\n                               generate urls for (eg https://example.com or\n                               localhost:8888)\n        :param _method: if provided this explicitly specifies an HTTP method.\n        :param _scheme: a string specifying the desired URL scheme. The `_external`\n                        parameter must be set to ``True`` or a :exc:`ValueError`\n                        is raised. The default behavior uses the same scheme as\n                        the current request, or ``PREFERRED_URL_SCHEME`` from the\n                        :ref:`app configuration <config>` if no request context is\n                        available. As of Werkzeug 0.10, this also can be set\n                        to an empty string to build protocol-relative URLs.\n        \"\"\"\n        return redirect(where, default, override, _cls=self, **url_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef jsonify(self,\n                data: Any,\n                code: Union[int, Tuple[int, str, str]] = HTTPStatus.OK,\n                headers: Optional[Dict[str, str]] = None,\n                ):\n        \"\"\"\n        Convenience method to return json responses.\n\n        :param data: The python data to jsonify.\n        :param code: The HTTP status code to return.\n        :param headers: Any optional headers.\n        \"\"\"\n        return jsonify(data), code, headers or {}", "response": "Convenience method to return json responses."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef errors(self,\n               errors: List[str],\n               code: Union[int, Tuple[int, str, str]] = HTTPStatus.BAD_REQUEST,\n               key: str = 'errors',\n               headers: Optional[Dict[str, str]] = None,\n               ):\n        \"\"\"\n        Convenience method to return errors as json.\n\n        :param errors: The list of errors.\n        :param code: The HTTP status code.\n        :param key: The key to return the errors under.\n        :param headers: Any optional headers.\n        \"\"\"\n        return jsonify({key: errors}), code, headers or {}", "response": "Returns a json representation of the errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_user(email, password, active, confirmed_at, send_email):\n    if confirmed_at == 'now':\n        confirmed_at = security.datetime_factory()\n    user = user_manager.create(email=email, password=password, active=active,\n                               confirmed_at=confirmed_at)\n    if click.confirm(f'Are you sure you want to create {user!r}?'):\n        security_service.register_user(user, allow_login=False, send_email=send_email)\n        user_manager.save(user, commit=True)\n        click.echo(f'Successfully created {user!r}')\n    else:\n        click.echo('Cancelled.')", "response": "Create a new user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_password(query, password, send_email):\n    user = _query_to_user(query)\n    if click.confirm(f'Are you sure you want to change {user!r}\\'s password?'):\n        security_service.change_password(user, password, send_email=send_email)\n        user_manager.save(user, commit=True)\n        click.echo(f'Successfully updated password for {user!r}')\n    else:\n        click.echo('Cancelled.')", "response": "Set a user s password."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfirming a user account.", "response": "def confirm_user(query):\n    \"\"\"\n    Confirm a user account.\n    \"\"\"\n    user = _query_to_user(query)\n    if click.confirm(f'Are you sure you want to confirm {user!r}?'):\n        if security_service.confirm_user(user):\n            click.echo(f'Successfully confirmed {user!r} at '\n                       f'{user.confirmed_at.strftime(\"%Y-%m-%d %H:%M:%S%z\")}')\n            user_manager.save(user, commit=True)\n        else:\n            click.echo(f'{user!r} has already been confirmed.')\n    else:\n        click.echo('Cancelled.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a role to a user.", "response": "def add_role_to_user(user, role):\n    \"\"\"\n    Add a role to a user.\n    \"\"\"\n    user = _query_to_user(user)\n    role = _query_to_role(role)\n    if click.confirm(f'Are you sure you want to add {role!r} to {user!r}?'):\n        user.roles.append(role)\n        user_manager.save(user, commit=True)\n        click.echo(f'Successfully added {role!r} to {user!r}')\n    else:\n        click.echo('Cancelled.')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_role_from_user(user, role):\n    user = _query_to_user(user)\n    role = _query_to_role(role)\n    if click.confirm(f'Are you sure you want to remove {role!r} from {user!r}?'):\n        user.roles.remove(role)\n        user_manager.save(user, commit=True)\n        click.echo(f'Successfully removed {role!r} from {user!r}')\n    else:\n        click.echo('Cancelled.')", "response": "Remove a role from a user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef anonymous_user_required(*decorator_args, msg=None, category=None, redirect_url=None):\n    def wrapper(fn):\n        @wraps(fn)\n        def decorated(*args, **kwargs):\n            if current_user.is_authenticated:\n                if request.is_json:\n                    abort(HTTPStatus.FORBIDDEN)\n                else:\n                    if msg:\n                        flash(msg, category)\n                    return redirect('SECURITY_POST_LOGIN_REDIRECT_ENDPOINT',\n                                    override=redirect_url)\n            return fn(*args, **kwargs)\n        return decorated\n\n    if decorator_args and callable(decorator_args[0]):\n        return wrapper(decorator_args[0])\n    return wrapper", "response": "Decorator that allows anonymous user to access the resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef login_user(self,\n                   user: User,\n                   remember: Optional[bool] = None,\n                   duration: Optional[timedelta] = None,\n                   force: bool = False,\n                   fresh: bool = True,\n                   ) -> bool:\n        \"\"\"\n        Logs a user in. You should pass the actual user object to this. If the\n        user's `active` property is ``False``, they will not be logged in\n        unless `force` is ``True``.\n\n        This will return ``True`` if the log in attempt succeeds, and ``False`` if\n        it fails (i.e. because the user is inactive).\n\n        :param user: The user object to log in.\n        :type user: object\n        :param remember: Whether to remember the user after their session expires.\n            Defaults to ``False``.\n        :type remember: bool\n        :param duration: The amount of time before the remember cookie expires. If\n            ``None`` the value set in the settings is used. Defaults to ``None``.\n        :type duration: :class:`datetime.timedelta`\n        :param force: If the user is inactive, setting this to ``True`` will log\n            them in regardless. Defaults to ``False``.\n        :type force: bool\n        :param fresh: setting this to ``False`` will log in the user with a session\n            marked as not \"fresh\". Defaults to ``True``.\n        :type fresh: bool\n        \"\"\"\n        if not force:\n            if not user.active:\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.disabled_account'))\n\n            if (not user.confirmed_at\n                    and self.security.confirmable\n                    and not self.security.login_without_confirmation):\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.confirmation_required'))\n\n            if not user.password:\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.password_not_set'))\n\n        session['user_id'] = getattr(user, user.Meta.pk)\n        session['_fresh'] = fresh\n        session['_id'] = app.login_manager._session_identifier_generator()\n\n        if remember is None:\n            remember = app.config.SECURITY_DEFAULT_REMEMBER_ME\n        if remember:\n            session['remember'] = 'set'\n            if duration is not None:\n                try:\n                    session['remember_seconds'] = duration.total_seconds()\n                except AttributeError:\n                    raise Exception('duration must be a datetime.timedelta, '\n                                    'instead got: {0}'.format(duration))\n\n        _request_ctx_stack.top.user = user\n        user_logged_in.send(app._get_current_object(), user=user)\n        identity_changed.send(app._get_current_object(),\n                              identity=Identity(user.id))\n        return True", "response": "Logs a user in the application."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logout_user(self):\n\n        for key in ('identity.name', 'identity.auth_type'):\n            session.pop(key, None)\n        _logout_user()\n        identity_changed.send(app._get_current_object(),\n                              identity=AnonymousIdentity())", "response": "Logs out the current user and cleans up the remember me cookie."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_user(self, user, allow_login=None, send_email=None,\n                      _force_login_without_confirmation=False):\n        \"\"\"\n        Service method to register a user.\n\n        Sends signal `user_registered`.\n\n        Returns True if the user has been logged in, False otherwise.\n        \"\"\"\n        should_login_user = (not self.security.confirmable\n                             or self.security.login_without_confirmation\n                             or _force_login_without_confirmation)\n        should_login_user = (should_login_user if allow_login is None\n                             else allow_login and should_login_user)\n        if should_login_user:\n            user.active = True\n\n        # confirmation token depends on having user.id set, which requires\n        # the user be committed to the database\n        self.user_manager.save(user, commit=True)\n\n        confirmation_link, token = None, None\n        if self.security.confirmable and not _force_login_without_confirmation:\n            token = self.security_utils_service.generate_confirmation_token(user)\n            confirmation_link = url_for('security_controller.confirm_email',\n                                        token=token, _external=True)\n\n        user_registered.send(app._get_current_object(),\n                             user=user, confirm_token=token)\n\n        if (send_email or (\n                send_email is None\n                and app.config.SECURITY_SEND_REGISTER_EMAIL)):\n            self.send_mail(_('flask_unchained.bundles.security:email_subject.register'),\n                           to=user.email,\n                           template='security/email/welcome.html',\n                           user=user,\n                           confirmation_link=confirmation_link)\n\n        if should_login_user:\n            return self.login_user(user, force=_force_login_without_confirmation)\n        return False", "response": "Service method to register a user.\n\n        Sends signal `user_registered`.\n\n        Returns True if the user has been logged in, False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_password(self, user, password, send_email=None):\n        user.password = password\n        self.user_manager.save(user)\n        if send_email or (app.config.SECURITY_SEND_PASSWORD_CHANGED_EMAIL\n                          and send_email is None):\n            self.send_mail(\n                _('flask_unchained.bundles.security:email_subject.password_changed_notice'),\n                to=user.email,\n                template='security/email/password_changed_notice.html',\n                user=user)\n        password_changed.send(app._get_current_object(), user=user)", "response": "Changes a user s password."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nservice method to reset a user's password. The same as :meth:`change_password` except we this method sends a different notification email. Sends signal `password_reset`. :param user: :param password: :return:", "response": "def reset_password(self, user, password):\n        \"\"\"\n        Service method to reset a user's password. The same as :meth:`change_password`\n        except we this method sends a different notification email.\n\n        Sends signal `password_reset`.\n\n        :param user:\n        :param password:\n        :return:\n        \"\"\"\n        user.password = password\n        self.user_manager.save(user)\n        if app.config.SECURITY_SEND_PASSWORD_RESET_NOTICE_EMAIL:\n            self.send_mail(\n                _('flask_unchained.bundles.security:email_subject.password_reset_notice'),\n                to=user.email,\n                template='security/email/password_reset_notice.html',\n                user=user)\n        password_reset.send(app._get_current_object(), user=user)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_email_confirmation_instructions(self, user):\n        token = self.security_utils_service.generate_confirmation_token(user)\n        confirmation_link = url_for('security_controller.confirm_email',\n                                    token=token, _external=True)\n        self.send_mail(\n            _('flask_unchained.bundles.security:email_subject.email_confirmation_instructions'),\n            to=user.email,\n            template='security/email/email_confirmation_instructions.html',\n            user=user,\n            confirmation_link=confirmation_link)\n        confirm_instructions_sent.send(app._get_current_object(), user=user,\n                                       token=token)", "response": "Sends the email confirmation instructions for the specified user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_reset_password_instructions(self, user):\n        token = self.security_utils_service.generate_reset_password_token(user)\n        reset_link = url_for('security_controller.reset_password',\n                             token=token, _external=True)\n        self.send_mail(\n            _('flask_unchained.bundles.security:email_subject.reset_password_instructions'),\n            to=user.email,\n            template='security/email/reset_password_instructions.html',\n            user=user,\n            reset_link=reset_link)\n        reset_password_instructions_sent.send(app._get_current_object(),\n                                              user=user, token=token)", "response": "Sends the reset password instructions email for the specified user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef confirm_user(self, user):\n        if user.confirmed_at is not None:\n            return False\n        user.confirmed_at = self.security.datetime_factory()\n        user.active = True\n        self.user_manager.save(user)\n\n        user_confirmed.send(app._get_current_object(), user=user)\n        return True", "response": "Confirm the specified user. Returns False if the user has already been confirmed True otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fields_for_model(self, model, include_fk=False, fields=None,\n                         exclude=None, base_fields=None, dict_cls=dict):\n        \"\"\"\n        Overridden to correctly name hybrid_property fields, eg given::\n\n            class User(db.Model):\n                _password = db.Column('password', db.String)\n\n                @db.hybrid_property\n                def password(self):\n                    return self._password\n\n                @password.setter\n                def password(self, password):\n                    self._password = hash_password(password)\n\n        In this case upstream marshmallow_sqlalchemy uses '_password' for the\n        field name, but we use 'password', as would be expected because it's\n        the attribute name used for the public interface of the Model. In order\n        for this logic to work, the column name must be specified and it must be\n        the same as the hybrid property name. Otherwise we just fallback to the\n        upstream naming convention.\n        \"\"\"\n        # this prevents an error when building the docs\n        if not hasattr(model, '__mapper__'):\n            return\n\n        result = dict_cls()\n        base_fields = base_fields or {}\n        for prop in model.__mapper__.iterate_properties:\n            if self._should_exclude_field(prop, fields=fields, exclude=exclude):\n                continue\n\n            attr_name = prop.key\n            if hasattr(prop, 'columns'):\n                if not include_fk:\n                    # Only skip a column if there is no overridden column\n                    # which does not have a Foreign Key.\n                    for column in prop.columns:\n                        if not column.foreign_keys:\n                            break\n                    else:\n                        continue\n\n                col_name = prop.columns[0].name\n                if attr_name != col_name and hasattr(model, col_name):\n                    attr_name = col_name\n\n            field = base_fields.get(attr_name) or self.property2field(prop)\n            if field:\n                result[attr_name] = field\n        return result", "response": "Returns a dictionary of all the fields that are set for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef property2field(self, prop, instance=True, field_class=None, **kwargs):\n        field = super().property2field(prop, instance, field_class, **kwargs)\n        # when a column is not nullable, mark the field as required\n        if hasattr(prop, 'columns'):\n            col = prop.columns[0]\n            if not col.primary_key and not col.nullable:\n                field.required = True\n        return field", "response": "Override to mark non - nullable model columns as required."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the declared fields for the given class.", "response": "def get_declared_fields(mcs, klass, cls_fields, inherited_fields, dict_cls):\n        \"\"\"\n        Updates declared fields with fields converted from the SQLAlchemy model\n        passed as the `model` class Meta option.\n        \"\"\"\n        opts = klass.opts\n        converter = opts.model_converter(schema_cls=klass)\n        base_fields = _BaseModelSerializerMetaclass.get_declared_fields(\n            klass, cls_fields, inherited_fields, dict_cls)\n        declared_fields = mcs.get_fields(converter, opts, base_fields, dict_cls)\n        if declared_fields is not None:  # prevents sphinx from blowing up\n            declared_fields.update(base_fields)\n        return declared_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_error(self, error, data):\n        required_messages = {'Missing data for required field.',\n                             'Field may not be null.'}\n        for field_name in error.field_names:\n            for i, msg in enumerate(error.messages[field_name]):\n                if isinstance(msg, _LazyString):\n                    msg = str(msg)\n                if msg in required_messages:\n                    label = title_case(field_name)\n                    error.messages[field_name][i] = f'{label} is required.'", "response": "Customize the error messages for required and not - null validators."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noverriding to automatically convert snake - cased field names to snake - cased counterparts to snake - cased field names backto snake - cased counterparts", "response": "def _update_fields(self, obj=None, many=False):\n        \"\"\"\n        Overridden to automatically convert snake-cased field names to\n        camel-cased (when dumping) and to load camel-cased field names back\n        to their snake-cased counterparts\n        \"\"\"\n        fields = super()._update_fields(obj, many)\n        new_fields = self.dict_class()\n        for name, field in fields.items():\n            if (field.dump_to is None\n                    and not name.startswith('_')\n                    and '_' in name):\n                camel_cased_name = camel_case(name)\n                field.dump_to = camel_cased_name\n                field.load_from = camel_cased_name\n            if name in READ_ONLY_FIELDS:\n                field.dump_only = True\n            new_fields[name] = field\n\n        # validate id\n        if 'id' in new_fields:\n            new_fields['id'].validators = [self.validate_id]\n\n        self.fields = new_fields\n        return new_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef qtconsole():\n    connection_file = os.path.join(tempfile.gettempdir(),\n                                   f'connection-{os.getpid()}.json')\n    p = multiprocessing.Process(target=launch_qtconsole,\n                                args=(connection_file,))\n    try:\n        p.start()\n        embed_kernel(local_ns=_get_shell_ctx(),\n                     connection_file=connection_file)\n        p.join()\n    finally:\n        try:\n            os.unlink(connection_file)\n        except OSError as exc:\n            if exc.errno != errno.ENOENT:\n                raise", "response": "Starts the qtconsole in the app context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the internal state of the current instance of the class.", "response": "def reset(self, clear=False):\n        \"\"\"\n        Overridden to customize the order that the banners are printed\n        \"\"\"\n        if self._executing:\n            self._executing = False\n            self._request_info['execute'] = {}\n        self._reading = False\n        self._highlighter.highlighting_on = False\n\n        if clear:\n            self._control.clear()\n            if self._display_banner:\n                if self.kernel_banner:\n                    self._append_plain_text(self.kernel_banner)\n                self._append_plain_text(self.banner)\n\n        # update output marker for stdout/stderr, so that startup\n        # messages appear after banner:\n        self._show_interpreter_prompt()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlog the connection information to the terminal.", "response": "def log_connection_info(self):\n        \"\"\"\n        Overridden to customize the start-up message printed to the terminal\n        \"\"\"\n        _ctrl_c_lines = [\n            'NOTE: Ctrl-C does not work to exit from the command line.',\n            'To exit, just close the window, type \"exit\" or \"quit\" at the '\n            'qtconsole prompt, or use Ctrl-\\\\ in UNIX-like environments '\n            '(at the command prompt).']\n\n        for line in _ctrl_c_lines:\n            io.rprint(line)\n\n        # upstream has this here, even though it seems like a silly place for it\n        self.ports = dict(shell=self.shell_port, iopub=self.iopub_port,\n                          stdin=self.stdin_port, hb=self.hb_port,\n                          control=self.control_port)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the snake - cased name for a controller or resource class.", "response": "def controller_name(cls, _remove_suffixes=None) -> str:\n    \"\"\"\n    Returns the snake-cased name for a controller/resource class. Automatically\n    strips ``Controller``, ``View``, and ``MethodView`` suffixes, eg::\n\n        SiteController -> site\n        FooBarBazView -> foo_bar_baz\n        UsersMethodView -> users\n    \"\"\"\n    name = cls if isinstance(cls, str) else cls.__name__\n    remove_suffixes = _remove_suffixes or getattr(cls, REMOVE_SUFFIXES_ATTR)\n    for suffix in remove_suffixes:\n        if name.endswith(suffix):\n            name = right_replace(name, suffix, '')\n            break\n    return snake_case(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of parameter tuples in a URL rule eg :: CTYPE", "response": "def get_param_tuples(url_rule) -> List[Tuple[str, str]]:\n    \"\"\"\n    Returns a list of parameter tuples in a URL rule, eg::\n\n        url_rule = '/users/<string:username>/roles/<int:id>'\n        param_tuples = get_param_tuples(url_rule)\n        assert param_tuples == [('string', 'username'), ('int', 'id')]\n    \"\"\"\n    if not url_rule:\n        return []\n    return [(type_[:-1], name) for type_, name\n            in re.findall(PARAM_NAME_RE, url_rule)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the name of the last parameter in a URL rule eg.", "response": "def get_last_param_name(url_rule) -> Union[str, None]:\n    \"\"\"\n    Returns the name of the last parameter in a URL rule, eg::\n\n        assert get_last_param_name('/foo/<int:id>/roles') is None\n\n        url_rule = '/foo/<int:id>/bar/<any:something>/baz/<string:spam>'\n        assert get_last_param_name(url_rule) == 'spam'\n    \"\"\"\n    if not url_rule:\n        return None\n    match = re.search(LAST_PARAM_NAME_RE, url_rule)\n    return match.group('param_name') if match else None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef url_for(endpoint_or_url_or_config_key: str,\n            _anchor: Optional[str] = None,\n            _cls: Optional[Union[object, type]] = None,\n            _external: Optional[bool] = False,\n            _external_host: Optional[str] = None,\n            _method: Optional[str] = None,\n            _scheme: Optional[str] = None,\n            **values,\n            ) -> Union[str, None]:\n    \"\"\"\n    An improved version of flask's url_for function\n\n    :param endpoint_or_url_or_config_key: what to lookup. it can be an endpoint\n      name, an app config key, or an already-formed url. if _cls is specified,\n      it also accepts a method name.\n    :param values: the variable arguments of the URL rule\n    :param _anchor: if provided this is added as anchor to the URL.\n    :param _cls: if specified, can also pass a method name as the first argument\n    :param _external: if set to ``True``, an absolute URL is generated. Server\n      address can be changed via ``SERVER_NAME`` configuration variable which\n      defaults to `localhost`.\n    :param _external_host: if specified, the host of an external server\n        to generate urls for (eg https://example.com or localhost:8888)\n    :param _method: if provided this explicitly specifies an HTTP method.\n    :param _scheme: a string specifying the desired URL scheme. The `_external`\n      parameter must be set to ``True`` or a :exc:`ValueError` is raised. The\n      default behavior uses the same scheme as the current request, or\n      ``PREFERRED_URL_SCHEME`` from the :ref:`app configuration <config>` if no\n      request context is available. As of Werkzeug 0.10, this also can be set\n      to an empty string to build protocol-relative URLs.\n    \"\"\"\n    what = endpoint_or_url_or_config_key\n\n    # if what is a config key\n    if what and what.isupper():\n        what = current_app.config.get(what)\n\n    if isinstance(what, LocalProxy):\n        what = what._get_current_object()\n\n    # if we already have a url (or an invalid value, eg None)\n    if not what or '/' in what:\n        return what\n\n    flask_url_for_kwargs = dict(_anchor=_anchor, _external=_external,\n                                _external_host=_external_host, _method=_method,\n                                _scheme=_scheme, **values)\n\n    # check if it's a class method name, and try that endpoint\n    if _cls and '.' not in what:\n        controller_routes = getattr(_cls, CONTROLLER_ROUTES_ATTR)\n        method_routes = controller_routes.get(what)\n        try:\n            return _url_for(method_routes[0].endpoint, **flask_url_for_kwargs)\n        except (\n            BuildError,  # url not found\n            IndexError,  # method_routes[0] is out-of-range\n            TypeError,   # method_routes is None\n        ):\n            pass\n\n    # what must be an endpoint\n    return _url_for(what, **flask_url_for_kwargs)", "response": "A function that returns the URL for the given endpoint or url or config key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a url path joined from the arguments.", "response": "def join(*args, trailing_slash=False):\n    \"\"\"\n    Return a url path joined from the arguments. It correctly handles blank/None\n    arguments, and removes back-to-back slashes, eg::\n\n        assert join('/', 'foo', None, 'bar', '', 'baz') == '/foo/bar/baz'\n        assert join('/', '/foo', '/', '/bar/') == '/foo/bar'\n\n    Note that it removes trailing slashes by default, so if you want to keep those,\n    then you need to pass the ``trailing_slash`` keyword argument::\n\n        assert join('/foo', 'baz', None, trailing_slash=True) == '/foo/baz/'\n    \"\"\"\n    dirty_path = '/'.join(map(lambda x: x and x or '', args))\n    path = re.sub(r'/+', '/', dirty_path)\n    if path in {'', '/'}:\n        return '/'\n    path = path.rstrip('/')\n    return path if not trailing_slash else path + '/'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef roles_required(*roles):\n    def wrapper(fn):\n        @wraps(fn)\n        def decorated_view(*args, **kwargs):\n            perms = [Permission(RoleNeed(role)) for role in roles]\n            for perm in perms:\n                if not perm.can():\n                    abort(HTTPStatus.FORBIDDEN)\n            return fn(*args, **kwargs)\n        return decorated_view\n    return wrapper", "response": "Decorator that returns a view that checks that the current user has all the specified roles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert environment variables to boolean values where truthy is defined as true or false.", "response": "def get_boolean_env(name, default):\n    \"\"\"\n    Converts environment variables to boolean values, where truthy is defined as:\n    ``value.lower() in {'true', 'yes', 'y', '1'}`` (everything else is falsy).\n    \"\"\"\n    default = 'true' if default else 'false'\n    return os.getenv(name, default).lower() in {'true', 'yes', 'y', '1'}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef safe_import_module(module_name):\n    try:\n        return import_module(module_name)\n    except ImportError as e:\n        m = re.match(r\"No module named '([\\w\\.]+)'\", str(e))\n        if not m or not module_name.startswith(m.group(1)):\n            raise e", "response": "Like import_module except it does not raise ImportError."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a string to class case. For example :: AttributeNames class_case = OneTwoThree", "response": "def class_case(string):\n    \"\"\"\n    Converts a string to class case. For example::\n\n        class_case('one_two_three') -> 'OneTwoThree'\n    \"\"\"\n    if not string:\n        return string\n    string = string.replace(' ', '_').replace('-', '_')\n    parts = de_camel(string, '_', _lowercase=False).split('_')\n    rv = ''\n    while parts:\n        part = parts.pop(0)\n        rv += part.title() or '_'\n        if part:\n            break\n    return rv + ''.join([part if part.isupper() else part.title()\n                         for part in parts])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kebab_case(string):\n    if not string:\n        return string\n    string = string.replace('_', '-').replace(' ', '-')\n    return de_camel(string, '-')", "response": "Converts a string to kebab case. For example ::\n        kebab_case = one - two - three"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrighting replaces count occurrences of old with new in string.", "response": "def right_replace(string, old, new, count=1):\n    \"\"\"\n    Right replaces ``count`` occurrences of ``old`` with ``new`` in ``string``.\n    For example::\n\n        right_replace('one_two_two', 'two', 'three') -> 'one_two_three'\n    \"\"\"\n    if not string:\n        return string\n    return new.join(string.rsplit(old, count))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a string to a valid slug. For example :: is a valid slug. For example :: is a valid slug.", "response": "def slugify(string):\n    \"\"\"\n    Converts a string to a valid slug. For example::\n\n        slugify('Hello World') -> 'hello-world'\n    \"\"\"\n    if not string:\n        return string\n    string = re.sub(r'[^\\w\\s-]', '',\n                    unicodedata.normalize('NFKD', de_camel(string, '-'))\n                    .encode('ascii', 'ignore')\n                    .decode('ascii')).strip()\n    return re.sub(r'[-_\\s]+', '-', string).strip('-').lower()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef snake_case(string):\n    if not string:\n        return string\n    string = string.replace('-', '_').replace(' ', '_')\n    return de_camel(string)", "response": "Converts a string to snake case. For example ::\n    = OneTwoThree - > OneTwoThree"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef title_case(string):\n    if not string:\n        return string\n    string = string.replace('_', ' ').replace('-', ' ')\n    parts = de_camel(string, ' ', _lowercase=False).strip().split(' ')\n    return ' '.join([part if part.isupper() else part.title()\n                     for part in parts])", "response": "Converts a string to title case. For example ::\n        title_case ( one_two_three ) -> One Two Three"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_template(template):\n    m = TEMPLATE_OVERRIDE_RE.match(template)\n    if not m:\n        return template, 0\n    return m.group('template'), int(m.group('depth'))", "response": "returns a 2 - tuple of template_name number_of_priors"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexplaining what failed loading of a template.", "response": "def explain_template_loading_attempts(app, template, attempts):\n    \"\"\"\n    This should help developers understand what failed. Mostly the same as\n    :func:`flask.debughelpers.explain_template_loading_attempts`, except here we've\n    extended it to support showing what :class:`UnchainedJinjaLoader` is doing.\n    \"\"\"\n    from flask import Flask, Blueprint\n    from flask.debughelpers import _dump_loader_info\n    from flask.globals import _request_ctx_stack\n\n    template, expected_priors = parse_template(template)\n    info = [f'Locating {pretty_num(expected_priors + 1)} template \"{template}\":']\n\n    total_found = 0\n    blueprint = None\n    reqctx = _request_ctx_stack.top\n    if reqctx is not None and reqctx.request.blueprint is not None:\n        blueprint = reqctx.request.blueprint\n\n    for idx, (loader, srcobj, triple) in enumerate(attempts):\n        if isinstance(srcobj, Flask):\n            src_info = 'application \"%s\"' % srcobj.import_name\n        elif isinstance(srcobj, Blueprint):\n            src_info = 'blueprint \"%s\" (%s)' % (srcobj.name,\n                                                srcobj.import_name)\n        else:\n            src_info = repr(srcobj)\n\n        info.append('% 5d: trying loader of %s' % (\n            idx + 1, src_info))\n\n        for line in _dump_loader_info(loader):\n            info.append('       %s' % line)\n\n        if triple is None:\n            detail = 'no match'\n        else:\n            if total_found < expected_priors:\n                action = 'skipping'\n            elif total_found == expected_priors:\n                action = 'using'\n            else:\n                action = 'ignoring'\n            detail = '%s (%r)' % (action, triple[1] or '<string>')\n            total_found += 1\n\n        info.append('       -> %s' % detail)\n\n    seems_fishy = False\n    if total_found < expected_priors:\n        info.append('Error: the template could not be found.')\n        seems_fishy = True\n\n    if blueprint is not None and seems_fishy:\n        info.append('  The template was looked up from an endpoint that '\n                    'belongs to the blueprint \"%s\".' % blueprint)\n        info.append('  Maybe you did not place a template in the right folder?')\n        info.append('  See http://flask.pocoo.org/docs/blueprints/#templates')\n\n    app.logger.info('\\n'.join(info))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Base64 encoded HMAC + SHA512 of the password signed with the salt specified by SECURITY_PASSWORD_SALT.", "response": "def get_hmac(self, password):\n        \"\"\"\n        Returns a Base64 encoded HMAC+SHA512 of the password signed with\n        the salt specified by ``SECURITY_PASSWORD_SALT``.\n\n        :param password: The password to sign.\n        \"\"\"\n        salt = current_app.config.SECURITY_PASSWORD_SALT\n\n        if salt is None:\n            raise RuntimeError(\n                'The configuration value `SECURITY_PASSWORD_SALT` must '\n                'not be None when the value of `SECURITY_PASSWORD_HASH` is '\n                'set to \"%s\"' % self.security.password_hash)\n\n        h = hmac.new(encode_string(salt), encode_string(password), hashlib.sha512)\n        return base64.b64encode(h.digest())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the user s authentication token.", "response": "def get_auth_token(self, user):\n        \"\"\"\n        Returns the user's authentication token.\n        \"\"\"\n        data = [str(user.id),\n                self.security.hashing_context.hash(encode_string(user._password))]\n        return self.security.remember_token_serializer.dumps(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_password(self, user, password):\n        if self.use_double_hash(user.password):\n            verified = self.security.pwd_context.verify(\n                self.get_hmac(password), user.password)\n        else:\n            # Try with original password.\n            verified = self.security.pwd_context.verify(password, user.password)\n\n        if verified and self.security.pwd_context.needs_update(user.password):\n            user.password = password\n            self.user_manager.save(user)\n        return verified", "response": "Verify the password against the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhashes the specified plaintext password.", "response": "def hash_password(self, password):\n        \"\"\"\n        Hash the specified plaintext password.\n\n        It uses the configured hashing options.\n\n        :param password: The plaintext password to hash\n        \"\"\"\n        if self.use_double_hash():\n            password = self.get_hmac(password).decode('ascii')\n\n        return self.security.pwd_context.hash(\n            password,\n            **current_app.config.SECURITY_PASSWORD_HASH_OPTIONS.get(\n                current_app.config.SECURITY_PASSWORD_HASH, {}))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify_hash(self, hashed_data, compare_data):\n        return self.security.hashing_context.verify(\n            encode_string(compare_data), hashed_data)", "response": "Verify a hash in the security token hashing context."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a bool indicating whether a password should be hashed twice.", "response": "def use_double_hash(self, password_hash=None):\n        \"\"\"\n        Return a bool indicating whether a password should be hashed twice.\n        \"\"\"\n        single_hash = current_app.config.SECURITY_PASSWORD_SINGLE_HASH\n        if single_hash and self.security.password_salt:\n            raise RuntimeError('You may not specify a salt with '\n                               'SECURITY_PASSWORD_SINGLE_HASH')\n\n        if password_hash is None:\n            is_plaintext = self.security.password_hash == 'plaintext'\n        else:\n            is_plaintext = \\\n                self.security.pwd_context.identify(password_hash) == 'plaintext'\n\n        return not (is_plaintext or single_hash)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a unique confirmation token for the specified user.", "response": "def generate_confirmation_token(self, user):\n        \"\"\"\n        Generates a unique confirmation token for the specified user.\n\n        :param user: The user to work with\n        \"\"\"\n        data = [str(user.id), self.hash_data(user.email)]\n        return self.security.confirm_serializer.dumps(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a unique reset password token for the specified user.", "response": "def generate_reset_password_token(self, user):\n        \"\"\"\n        Generates a unique reset password token for the specified user.\n\n        :param user: The user to work with\n        \"\"\"\n        password_hash = self.hash_data(user.password) if user.password else None\n        data = [str(user.id), password_hash]\n        return self.security.reset_serializer.dumps(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the expired invalid status invalid user and a password reset token. For example :: expired invalid user expired user invalid user", "response": "def reset_password_token_status(self, token):\n        \"\"\"\n        Returns the expired status, invalid status, and user of a password reset\n        token. For example::\n\n            expired, invalid, user, data = reset_password_token_status('...')\n\n        :param token: The password reset token\n        \"\"\"\n        expired, invalid, user, data = self.get_token_status(\n            token, 'reset', 'SECURITY_RESET_PASSWORD_WITHIN', return_data=True)\n\n        if (not invalid\n                and user.password\n                and not self.verify_hash(data[1], user.password)):\n            invalid = True\n\n        return expired, invalid, user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the status of a token.", "response": "def get_token_status(self, token, serializer, max_age=None, return_data=False):\n        \"\"\"\n        Get the status of a token.\n\n        :param token: The token to check\n        :param serializer: The name of the serializer. Can be one of the\n                           following: ``confirm``, ``login``, ``reset``\n        :param max_age: The name of the max age config option. Can be one of\n                        the following: ``SECURITY_CONFIRM_EMAIL_WITHIN`` or\n                        ``SECURITY_RESET_PASSWORD_WITHIN``\n        \"\"\"\n        serializer = getattr(self.security, serializer + '_serializer')\n\n        td = self.get_within_delta(max_age)\n        max_age = td.seconds + td.days * 24 * 3600\n        user, data = None, None\n        expired, invalid = False, False\n\n        try:\n            data = serializer.loads(token, max_age=max_age)\n        except SignatureExpired:\n            d, data = serializer.loads_unsafe(token)\n            expired = True\n        except (BadSignature, TypeError, ValueError):\n            invalid = True\n\n        if data:\n            user = self.user_manager.get(data[0])\n\n        expired = expired and (user is not None)\n\n        if return_data:\n            return expired, invalid, user, data\n        else:\n            return expired, invalid, user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_within_delta(self, key):\n        txt = current_app.config.get(key)\n        values = txt.split()\n        return timedelta(**{values[1]: int(values[0])})", "response": "Get a timedelta object from the application configuration following\n            the internal convention of the internal convention of ::\n\n            5 days\n            10 minutes\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean():\n    for dirpath, dirnames, filenames in os.walk('.'):\n        for filename in filenames:\n            if filename.endswith('.pyc') or filename.endswith('.pyo'):\n                filepath = os.path.join(dirpath, filename)\n                click.echo(f'Removing {filepath}')\n                os.remove(filepath)", "response": "Recursively remove all. pyc and. pyo files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends an email using the send function defined by the configuration by the get_message_or_subject method.", "response": "def send_message(self,\n                     subject_or_message: Optional[Union[Message, str]] = None,\n                     to: Optional[Union[str, List[str]]] = None,\n                     **kwargs):\n        \"\"\"\n        Send an email using the send function as configured by\n        :attr:`~flask_unchained.bundles.mail.config.Config.MAIL_SEND_FN`.\n\n        :param subject_or_message: The subject line, or an instance of\n                                   :class:`flask_mail.Message`.\n        :param to: The message recipient(s).\n        :param kwargs: Extra values to pass on to :class:`~flask_mail.Message`\n        \"\"\"\n        to = to or kwargs.pop('recipients', [])\n        return self.send(subject_or_message, to, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef after_init_app(self, app: FlaskUnchained):\n        self.set_json_encoder(app)\n        app.before_first_request(self.register_model_resources)", "response": "Configure the JSON encoder for Flask to be able to serialize Enums LocalProxy objects and SQLAlchemy models."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine whether or not this route should be registered with the app.", "response": "def should_register(self, app):\n        \"\"\"\n        Determines whether or not this route should be registered with the app,\n        based on :attr:`only_if`.\n        \"\"\"\n        if self.only_if is None:\n            return True\n        elif callable(self.only_if):\n            return self.only_if(app)\n        return bool(self.only_if)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the name of the view function.", "response": "def method_name(self):\n        \"\"\"\n        The string name of this route's view function.\n        \"\"\"\n        if isinstance(self.view_func, str):\n            return self.view_func\n        return self.view_func.__name__"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef module_name(self):\n        if not self.view_func:\n            return None\n        elif self._controller_cls:\n            rv = inspect.getmodule(self._controller_cls).__name__\n            return rv\n        return inspect.getmodule(self.view_func).__name__", "response": "Returns the module name of the view function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the url rule for this route.", "response": "def rule(self):\n        \"\"\"\n        The (partial) url rule for this route.\n        \"\"\"\n        if self._rule:\n            return self._rule\n        return self._make_rule(member_param=self._member_param,\n                               unique_member_param=self._unique_member_param)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef full_rule(self):\n        return join(self.bp_prefix, self.rule, trailing_slash=self.rule.endswith('/'))", "response": "Returns the full url rule for this route including any blueprint prefix."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the full name of this route s view function including the module path and the controller name.", "response": "def full_name(self):\n        \"\"\"\n        The full name of this route's view function, including the module path\n        and controller name, if any.\n        \"\"\"\n        if not self.view_func:\n            return None\n\n        prefix = self.view_func.__module__\n        if self._controller_cls:\n            prefix = f'{prefix}.{self._controller_cls.__name__}'\n        return f'{prefix}.{self.method_name}'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new Flask Unchained project.", "response": "def project(dest, app_bundle, force, dev,\n            admin, api, celery, graphene, mail, oauth,\n            security, session, sqlalchemy, webpack):\n    \"\"\"\n    Create a new Flask Unchained project.\n    \"\"\"\n    if os.path.exists(dest) and os.listdir(dest) and not force:\n        if not click.confirm(f'WARNING: Project directory {dest!r} exists and is '\n                             f'not empty. It will be DELETED!!! Continue?'):\n            click.echo(f'Exiting.')\n            sys.exit(1)\n\n    # build up a list of dependencies\n    # IMPORTANT: keys here must match setup.py's `extra_requires` keys\n    ctx = dict(dev=dev, admin=admin, api=api, celery=celery, graphene=graphene,\n               mail=mail, oauth=oauth, security=security or oauth, session=security or session,\n               sqlalchemy=security or sqlalchemy, webpack=webpack)\n    ctx['requirements'] = [k for k, v in ctx.items() if v]\n\n    # remaining ctx vars\n    ctx['app_bundle_module_name'] = app_bundle\n\n    # copy the project template into place\n    copy_file_tree(PROJECT_TEMPLATE, dest, ctx, [\n        (option, files)\n        for option, files\n        in [('api', ['app/serializers']),\n            ('celery', ['app/tasks',\n                        'celery_app.py']),\n            ('graphene', ['app/graphql']),\n            ('mail', ['templates/email']),\n            ('security', ['app/models/role.py',\n                          'app/models/user.py',\n                          'db/fixtures/Role.yaml',\n                          'db/fixtures/User.yaml']),\n            ('webpack', ['assets',\n                         'package.json',\n                         'webpack']),\n            ]\n        if not ctx[option]\n    ])\n\n    click.echo(f'Successfully created a new project at: {dest}')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy the file tree under src to dest.", "response": "def copy_file_tree(src: str,\n                   dest: str,\n                   ctx: Optional[Dict[str, Any]] = None,\n                   option_locations: Optional[List[Tuple[str, List[str]]]] = None):\n    \"\"\"\n    Copy the file tree under the :param:`src` directory to the :param:`dest`\n    directory. Pass :param:`ctx` to support rendering the files, and pass\n    :param:`option_locations` to support deleting optional files/folders.\n    \"\"\"\n    if os.path.exists(dest):\n        shutil.rmtree(dest, ignore_errors=True)\n    shutil.copytree(src, dest)\n\n    if option_locations:\n        for option, paths in option_locations:\n            for path in paths:\n                path = os.path.join(dest, path)\n                if os.path.isfile(path):\n                    os.remove(path)\n                else:\n                    shutil.rmtree(path, ignore_errors=True)\n\n    if 'app_bundle_module_name' in ctx:\n        shutil.move(os.path.join(dest, 'app'),\n                    os.path.join(dest, ctx['app_bundle_module_name']))\n        shutil.move(os.path.join(dest, 'tests', 'app'),\n                    os.path.join(dest, 'tests', ctx['app_bundle_module_name']))\n\n    _render_file_tree(dest, ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nproduce a new set of plugins treating the current set as plugin factories.", "response": "def produce(self, *args, **kwargs):\n        \"\"\"Produce a new set of plugins, treating the current set as plugin\n        factories.\n        \"\"\"\n\n        new_plugins = []\n        for p in self._plugins:\n            r = p(*args, **kwargs)\n            new_plugins.append(r)\n        return PluginManager(new_plugins)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling a common method on all the plugins.", "response": "def call(self, methodname, *args, **kwargs):\n        \"\"\"Call a common method on all the plugins, if it exists.\"\"\"\n\n        for plugin in self._plugins:\n            method = getattr(plugin, methodname, None)\n            if method is None:\n                continue\n            yield method(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall a common method on all the plugins if it exists. Return the first non - None value.", "response": "def first(self, methodname, *args, **kwargs):\n        \"\"\"Call a common method on all the plugins, if it exists. Return the\n        first result (the first non-None)\n        \"\"\"\n\n        for r in self.call(methodname, *args, **kwargs):\n            if r is not None:\n                return r\n\n        raise ValueError(\"No plugins returned a non-None value\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pipe(self, methodname, first_arg, *args, **kwargs):\n\n        for plugin in self._plugins:\n            method = getattr(plugin, methodname, None)\n            if method is None:\n                continue\n            r = method(first_arg, *args, **kwargs)\n            if r is not None:\n                first_arg = r\n        return r", "response": "Calls a common method on all the plugins and returns the result of the first argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides a unified interface to both the module and class loaders and a unified module loader.", "response": "def unified_load(namespace, subclasses=None, recurse=False):\n    \"\"\"Provides a unified interface to both the module and class loaders,\n    finding modules by default or classes if given a ``subclasses`` parameter.\n    \"\"\"\n\n    if subclasses is not None:\n        return ClassLoader(recurse=recurse).load(namespace, subclasses=subclasses)\n    else:\n        return ModuleLoader(recurse=recurse).load(namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fill_cache(self, namespace):\n\n        modules = self._findPluginModules(namespace)\n\n        self._cache = list(modules)", "response": "Load all modules found in a namespace"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_tree(self):\n        if self.built:\n            return\n        self.doc_root = self.root.element()\n        for key in self.sorted_fields():\n            if key not in self._fields:\n                continue\n            field = self._fields[key]\n            if field != self.root:\n                if isinstance(field, XmlModel):\n                    field.build_tree()\n                    if (self.drop_empty and field.drop_empty\n                        and len(field.doc_root) == 0):\n                        continue\n                    self.doc_root.append(field.doc_root)\n                elif isinstance(field, list):\n                    # we just allow XmlFields and XmlModels\n                    # Also xml as str for memory management\n                    for item in field:\n                        if isinstance(item, XmlField):\n                            ele = item.element()\n                            if self.drop_empty and len(ele) == 0:\n                                continue\n                            self.doc_root.append(ele)\n                        elif isinstance(item, XmlModel):\n                            item.build_tree()\n                            if self.drop_empty and len(item.doc_root) == 0:\n                                continue\n                            self.doc_root.append(item.doc_root)\n                        elif isinstance(item, (six.text_type, six.string_types)):\n                            ele = etree.fromstring(clean_xml(item))\n                            self.doc_root.append(ele)\n                        item = None\n                elif (field.parent or self.root.name) == self.root.name:\n                    ele = field.element()\n                    if self.drop_empty and len(ele) == 0 and not ele.text:\n                        continue\n                    ele = field.element(parent=self.doc_root)\n                else:\n                    nodes = [n for n in self.doc_root.iterdescendants(\n                                            tag=field.parent)]\n                    if nodes:\n                        ele = field.element()\n                        if (self.drop_empty and len(ele) == 0 and not ele.text):\n                            continue\n                        ele = field.element(parent=nodes[0])\n                    #else:\n                    #    raise RuntimeError(\"No parent found!\")\n        self.built = True", "response": "Builds the XML tree with all the fields converted to Elements"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _preserve_settings(method: T.Callable) -> T.Callable:\n\n    @functools.wraps(method)\n    def _wrapper(\n            old: \"ObservableProperty\", handler: T.Callable\n    ) -> \"ObservableProperty\":\n        new = method(old, handler)  # type: ObservableProperty\n        new.event = old.event\n        new.observable = old.observable\n        return new\n\n    return _wrapper", "response": "Decorator that ensures ObservableProperty - specific attributes\n    are kept when using methods to change deleter getter or setter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _trigger_event(\n            self, holder: T.Any, alt_name: str, action: str, *event_args: T.Any\n    ) -> None:\n        \"\"\"Triggers an event on the associated Observable object.\n        The Holder is the object this property is a member of, alt_name\n        is used as the event name when self.event is not set, action is\n        prepended to the event name and event_args are passed through\n        to the registered event handlers.\"\"\"\n\n        if isinstance(self.observable, Observable):\n            observable = self.observable\n        elif isinstance(self.observable, str):\n            observable = getattr(holder, self.observable)\n        elif isinstance(holder, Observable):\n            observable = holder\n        else:\n            raise TypeError(\n                \"This ObservableProperty is no member of an Observable \"\n                \"object. Specify where to find the Observable object for \"\n                \"triggering events with the observable keyword argument \"\n                \"when initializing the ObservableProperty.\"\n            )\n\n        name = alt_name if self.event is None else self.event\n        event = \"{}_{}\".format(action, name)\n        observable.trigger(event, *event_args)", "response": "Triggers an event on the associated Observable object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_with(\n            cls, event: str = None, observable: T.Union[str, Observable] = None\n    ) -> T.Callable[..., \"ObservableProperty\"]:\n        \"\"\"Creates a partial application of ObservableProperty with\n        event and observable preset.\"\"\"\n\n        return functools.partial(cls, event=event, observable=observable)", "response": "Creates a partial application of ObservableProperty with the given event and observable preset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_handlers(self) -> T.Dict[str, T.List[T.Callable]]:\n\n        events = {}\n        for event, handlers in self._events.items():\n            events[event] = list(handlers)\n        return events", "response": "Returns a dict with event names as keys and lists of all registered handlers as values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of handlers registered for the given event.", "response": "def get_handlers(self, event: str) -> T.List[T.Callable]:\n        \"\"\"Returns a list of handlers registered for the given event.\"\"\"\n\n        return list(self._events.get(event, []))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_registered(self, event: str, handler: T.Callable) -> bool:\n\n        return handler in self._events.get(event, [])", "response": "Returns whether the given handler is registered for the given event."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering one or more handlers to a specified event.", "response": "def on(  # pylint: disable=invalid-name\n            self, event: str, *handlers: T.Callable\n    ) -> T.Callable:\n        \"\"\"Registers one or more handlers to a specified event.\n        This method may as well be used as a decorator for the handler.\"\"\"\n\n        def _on_wrapper(*handlers: T.Callable) -> T.Callable:\n            \"\"\"wrapper for on decorator\"\"\"\n            self._events[event].extend(handlers)\n            return handlers[0]\n\n        if handlers:\n            return _on_wrapper(*handlers)\n        return _on_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister one or more handlers to a specified event.", "response": "def once(self, event: str, *handlers: T.Callable) -> T.Callable:\n        \"\"\"Registers one or more handlers to a specified event, but\n        removes them when the event is first triggered.\n        This method may as well be used as a decorator for the handler.\"\"\"\n\n        def _once_wrapper(*handlers: T.Callable) -> T.Callable:\n            \"\"\"Wrapper for 'once' decorator\"\"\"\n\n            def _wrapper(*args: T.Any, **kw: T.Any) -> None:\n                \"\"\"Wrapper that unregisters itself before executing\n                the handlers\"\"\"\n\n                self.off(event, _wrapper)\n                for handler in handlers:\n                    handler(*args, **kw)\n\n            return _wrapper\n\n        if handlers:\n            return self.on(event, _once_wrapper(*handlers))\n        return lambda x: self.on(event, _once_wrapper(x))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trigger(self, event: str, *args: T.Any, **kw: T.Any) -> bool:\n\n        callbacks = list(self._events.get(event, []))\n        if not callbacks:\n            return False\n\n        for callback in callbacks:\n            callback(*args, **kw)\n        return True", "response": "Triggers all handlers which are subscribed to an event. Returns True when there were callbacks to execute False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connection(profile_name='default', api_key=None):\n    if api_key is None:\n        profile_fname = datapoint.profile.API_profile_fname(profile_name)\n        if not os.path.exists(profile_fname):\n            raise ValueError('Profile not found in {}. Please install your API \\n'\n                             'key with datapoint.profile.install_API_key('\n                             '\"<YOUR-KEY>\")'.format(profile_fname))\n        with open(profile_fname) as fh:\n            api_key = fh.readlines()\n    return Manager(api_key=api_key)", "response": "Connect to DataPoint with the given API key profile name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of the elements which are not None", "response": "def elements(self):\n        \"\"\"Return a list of the elements which are not None\"\"\"\n\n        elements = []\n\n        for el in ct:\n            if isinstance(el[1], datapoint.Element.Element):\n                elements.append(el[1])\n\n        return elements"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretry the connection using requests. Session", "response": "def __retry_session(self, retries=10, backoff_factor=0.3,\n                        status_forcelist=(500, 502, 504),\n                        session=None):\n        \"\"\"\n        Retry the connection using requests if it fails. Use this as a wrapper\n        to request from datapoint\n        \"\"\"\n\n        # requests.Session allows finer control, which is needed to use the\n        # retrying code\n        the_session = session or requests.Session()\n\n        # The Retry object manages the actual retrying\n        retry = Retry(total=retries, read=retries, connect=retries,\n                      backoff_factor=backoff_factor,\n                      status_forcelist=status_forcelist)\n\n        adapter = HTTPAdapter(max_retries=retry)\n\n        the_session.mount('http://', adapter)\n        the_session.mount('https://', adapter)\n\n        return the_session"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __call_api(self, path, params=None, api_url=FORECAST_URL):\n        if not params:\n            params = dict()\n        payload = {'key': self.api_key}\n        payload.update(params)\n        url = \"%s/%s\" % (api_url, path)\n\n        # Add a timeout to the request.\n        # The value of 1 second is based on attempting 100 connections to\n        # datapoint and taking ten times the mean connection time (rounded up).\n        # Could expose to users in the functions which need to call the api.\n        #req = requests.get(url, params=payload, timeout=1)\n        # The wrapper function __retry_session returns a requests.Session\n        # object. This has a .get() function like requests.get(), so the use\n        # doesn't change here.\n\n        sess = self.__retry_session()\n        req = sess.get(url, params=payload, timeout=1)\n\n        try:\n            data = req.json()\n        except ValueError:\n            raise APIException(\"DataPoint has not returned any data, this could be due to an incorrect API key\")\n        self.call_response = data\n        if req.status_code != 200:\n            msg = [data[m] for m in (\"message\", \"error_message\", \"status\") \\\n                      if m in data][0]\n            raise Exception(msg)\n        return data", "response": "Call the datapoint api using the requests module"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the great circle distance between two points on the earth.", "response": "def _distance_between_coords(self, lon1, lat1, lon2, lat2):\n        \"\"\"\n        Calculate the great circle distance between two points\n        on the earth (specified in decimal degrees).\n        Haversine formula states that:\n\n        d = 2 * r * arcsin(sqrt(sin^2((lat1 - lat2) / 2 +\n        cos(lat1)cos(lat2)sin^2((lon1 - lon2) / 2))))\n\n        where r is the radius of the sphere. This assumes the earth is spherical.\n        \"\"\"\n\n        # Convert the coordinates of the points to radians.\n        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n        r = 6371\n\n        d_hav = 2 * r * asin(sqrt((sin((lat1 - lat2) / 2))**2 + \\\n                                  cos(lat1) * cos(lat2) * (sin((lon1 - lon2) / 2)**2 )))\n\n        return d_hav"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_wx_units(self, params, name):\n        units = \"\"\n        for param in params:\n            if str(name) == str(param['name']):\n                units = param['units']\n        return units", "response": "Get the units for a given element from the Wx array and a name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting observed visibility in metres to text used in forecast", "response": "def _visibility_to_text(self, distance):\n        \"\"\"\n        Convert observed visibility in metres to text used in forecast\n        \"\"\"\n\n        if not isinstance(distance, (int, long)):\n            raise ValueError(\"Distance must be an integer not\", type(distance))\n        if distance < 0:\n            raise ValueError(\"Distance out of bounds, should be 0 or greater\")\n\n        if 0 <= distance < 1000:\n            return 'VP'\n        elif 1000 <= distance < 4000:\n            return 'PO'\n        elif 4000 <= distance < 10000:\n            return 'MO'\n        elif 10000 <= distance < 20000:\n            return 'GO'\n        elif 20000 <= distance < 40000:\n            return 'VG'\n        else:\n            return 'EX'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_forecast_sites(self):\n\n        time_now = time()\n        if (time_now - self.forecast_sites_last_update) > self.forecast_sites_update_time or self.forecast_sites_last_request is None:\n\n            data = self.__call_api(\"sitelist/\")\n            sites = list()\n            for jsoned in data['Locations']['Location']:\n                site = Site()\n                site.name = jsoned['name']\n                site.id = jsoned['id']\n                site.latitude = jsoned['latitude']\n                site.longitude = jsoned['longitude']\n\n                if 'region' in jsoned:\n                    site.region = jsoned['region']\n\n                if 'elevation' in jsoned:\n                    site.elevation = jsoned['elevation']\n\n                if 'unitaryAuthArea' in jsoned:\n                    site.unitaryAuthArea = jsoned['unitaryAuthArea']\n\n                if 'nationalPark' in jsoned:\n                    site.nationalPark = jsoned['nationalPark']\n\n                site.api_key = self.api_key\n\n                sites.append(site)\n            self.forecast_sites_last_request = sites\n            # Only set self.sites_last_update once self.sites_last_request has\n            # been set\n            self.forecast_sites_last_update = time_now\n        else:\n            sites = self.forecast_sites_last_request\n\n        return sites", "response": "This function returns a list of Site objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nearest_site(self, latitude=None,  longitude=None):\n        warning_message = 'This function is deprecated. Use get_nearest_forecast_site() instead'\n        warn(warning_message, DeprecationWarning, stacklevel=2)\n\n        return self.get_nearest_forecast_site(latitude, longitude)", "response": "Deprecated. This function returns nearest Site object to the specified coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_forecast_for_site(self, site_id, frequency=\"daily\"):\n        data = self.__call_api(site_id, {\"res\":frequency})\n        params = data['SiteRep']['Wx']['Param']\n        forecast = Forecast()\n        forecast.data_date = data['SiteRep']['DV']['dataDate']\n        forecast.data_date = datetime.strptime(data['SiteRep']['DV']['dataDate'], DATA_DATE_FORMAT).replace(tzinfo=pytz.UTC)\n        forecast.continent = data['SiteRep']['DV']['Location']['continent']\n        forecast.country = data['SiteRep']['DV']['Location']['country']\n        forecast.name = data['SiteRep']['DV']['Location']['name']\n        forecast.longitude = data['SiteRep']['DV']['Location']['lon']\n        forecast.latitude = data['SiteRep']['DV']['Location']['lat']\n        forecast.id = data['SiteRep']['DV']['Location']['i']\n        forecast.elevation = data['SiteRep']['DV']['Location']['elevation']\n\n        for day in data['SiteRep']['DV']['Location']['Period']:\n            new_day = Day()\n            new_day.date = datetime.strptime(day['value'], DATE_FORMAT).replace(tzinfo=pytz.UTC)\n\n            for timestep in day['Rep']:\n                new_timestep = Timestep()\n\n                if timestep['$'] == \"Day\":\n                    cur_elements = ELEMENTS['Day']\n                    new_timestep.date = datetime.strptime(day['value'], DATE_FORMAT).replace(tzinfo=pytz.UTC) \\\n                                        + timedelta(hours=12)\n                elif timestep['$'] == \"Night\":\n                    cur_elements = ELEMENTS['Night']\n                    new_timestep.date = datetime.strptime(day['value'], DATE_FORMAT).replace(tzinfo=pytz.UTC)\n                else:\n                    cur_elements = ELEMENTS['Default']\n                    new_timestep.date = datetime.strptime(day['value'], DATE_FORMAT).replace(tzinfo=pytz.UTC) \\\n                                        + timedelta(minutes=int(timestep['$']))\n\n                if frequency == 'daily':\n                    new_timestep.name = timestep['$']\n                elif frequency == '3hourly':\n                    new_timestep.name = int(timestep['$'])\n\n                new_timestep.weather = \\\n                    Element(cur_elements['W'],\n                            timestep[cur_elements['W']],\n                            self._get_wx_units(params, cur_elements['W']))\n                new_timestep.weather.text = self._weather_to_text(int(timestep[cur_elements['W']]))\n\n                new_timestep.temperature = \\\n                    Element(cur_elements['T'],\n                            int(timestep[cur_elements['T']]),\n                            self._get_wx_units(params, cur_elements['T']))\n\n                new_timestep.feels_like_temperature = \\\n                    Element(cur_elements['F'],\n                            int(timestep[cur_elements['F']]),\n                            self._get_wx_units(params, cur_elements['F']))\n\n                new_timestep.wind_speed = \\\n                    Element(cur_elements['S'],\n                            int(timestep[cur_elements['S']]),\n                            self._get_wx_units(params, cur_elements['S']))\n\n                new_timestep.wind_direction = \\\n                    Element(cur_elements['D'],\n                            timestep[cur_elements['D']],\n                            self._get_wx_units(params, cur_elements['D']))\n\n\n                new_timestep.wind_gust = \\\n                    Element(cur_elements['G'],\n                            int(timestep[cur_elements['G']]),\n                            self._get_wx_units(params, cur_elements['G']))\n\n                new_timestep.visibility = \\\n                    Element(cur_elements['V'],\n                            timestep[cur_elements['V']],\n                            self._get_wx_units(params, cur_elements['V']))\n\n                new_timestep.precipitation = \\\n                    Element(cur_elements['Pp'],\n                            int(timestep[cur_elements['Pp']]),\n                            self._get_wx_units(params, cur_elements['Pp']))\n\n                new_timestep.humidity = \\\n                    Element(cur_elements['H'],\n                            int(timestep[cur_elements['H']]),\n                            self._get_wx_units(params, cur_elements['H']))\n\n                if 'U' in cur_elements and cur_elements['U'] in timestep:\n                    new_timestep.uv = \\\n                        Element(cur_elements['U'],\n                                timestep[cur_elements['U']],\n                                self._get_wx_units(params, cur_elements['U']))\n\n                new_day.timesteps.append(new_timestep)\n            forecast.days.append(new_day)\n\n        return forecast", "response": "Get a forecast for the provided site."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_observations_for_site(self, site_id, frequency='hourly'):\n\n            data = self.__call_api(site_id,{\"res\":frequency}, OBSERVATION_URL)\n\n            params = data['SiteRep']['Wx']['Param']\n            observation = Observation()\n            observation.data_date = data['SiteRep']['DV']['dataDate']\n            observation.data_date = datetime.strptime(data['SiteRep']['DV']['dataDate'], DATA_DATE_FORMAT).replace(tzinfo=pytz.UTC)\n            observation.continent = data['SiteRep']['DV']['Location']['continent']\n            observation.country = data['SiteRep']['DV']['Location']['country']\n            observation.name = data['SiteRep']['DV']['Location']['name']\n            observation.longitude = data['SiteRep']['DV']['Location']['lon']\n            observation.latitude = data['SiteRep']['DV']['Location']['lat']\n            observation.id = data['SiteRep']['DV']['Location']['i']\n            observation.elevation = data['SiteRep']['DV']['Location']['elevation']\n\n            for day in data['SiteRep']['DV']['Location']['Period']:\n                new_day = Day()\n                new_day.date = datetime.strptime(day['value'], DATE_FORMAT).replace(tzinfo=pytz.UTC)\n\n                # If the day only has 1 timestep, put it into a list by itself so it can be treated\n                # the same as a day with multiple timesteps\n                if type(day['Rep']) is not list:\n                        day['Rep'] = [day['Rep']]\n\n                for timestep in day['Rep']:\n                    # As stated in\n                    # https://www.metoffice.gov.uk/datapoint/product/uk-hourly-site-specific-observations,\n                    # some sites do not have all parameters available for\n                    # observations. The documentation does not state which\n                    # fields may be absent. If the parameter is not available,\n                    # nothing is returned from the API. If this happens the\n                    # value of the element is set to 'Not reported'. This may\n                    # change to the element not being assigned to the timestep.\n\n                    new_timestep = Timestep()\n                    # Assume the '$' field is always present.\n                    new_timestep.name = int(timestep['$'])\n\n                    cur_elements = ELEMENTS['Observation']\n\n                    new_timestep.date = datetime.strptime(day['value'], DATE_FORMAT).replace(tzinfo=pytz.UTC) + timedelta(minutes=int(timestep['$']))\n\n                    if cur_elements['W'] in timestep:\n                        new_timestep.weather = \\\n                            Element(cur_elements['W'],\n                                    timestep[cur_elements['W']],\n                                    self._get_wx_units(params, cur_elements['W']))\n                        new_timestep.weather.text = \\\n                            self._weather_to_text(int(timestep[cur_elements['W']]))\n                    else:\n                        new_timestep.weather = \\\n                            Element(cur_elements['W'],\n                                    'Not reported')\n\n                    if cur_elements['T'] in timestep:\n                        new_timestep.temperature = \\\n                            Element(cur_elements['T'],\n                                    float(timestep[cur_elements['T']]),\n                                    self._get_wx_units(params, cur_elements['T']))\n                    else:\n                        new_timestep.temperature = \\\n                            Element(cur_elements['T'],\n                                    'Not reported')\n\n                    if 'S' in timestep:\n                        new_timestep.wind_speed = \\\n                            Element(cur_elements['S'],\n                                    int(timestep[cur_elements['S']]),\n                                    self._get_wx_units(params, cur_elements['S']))\n                    else:\n                        new_timestep.wind_speed = \\\n                            Element(cur_elements['S'],\n                                    'Not reported')\n\n                    if 'D' in timestep:\n                        new_timestep.wind_direction = \\\n                            Element(cur_elements['D'],\n                                    timestep[cur_elements['D']],\n                                    self._get_wx_units(params, cur_elements['D']))\n                    else:\n                        new_timestep.wind_direction = \\\n                            Element(cur_elements['D'],\n                                    'Not reported')\n\n                    if cur_elements['V'] in timestep:\n                        new_timestep.visibility = \\\n                            Element(cur_elements['V'],\n                                    int(timestep[cur_elements['V']]),\n                                    self._get_wx_units(params, cur_elements['V']))\n                        new_timestep.visibility.text = self._visibility_to_text(int(timestep[cur_elements['V']]))\n                    else:\n                        new_timestep.visibility = \\\n                            Element(cur_elements['V'],\n                                    'Not reported')\n\n                    if cur_elements['H'] in timestep:\n                        new_timestep.humidity = \\\n                            Element(cur_elements['H'],\n                                    float(timestep[cur_elements['H']]),\n                                    self._get_wx_units(params, cur_elements['H']))\n                    else:\n                        new_timestep.humidity = \\\n                            Element(cur_elements['H'],\n                                    'Not reported')\n\n                    if cur_elements['Dp'] in timestep:\n                        new_timestep.dew_point = \\\n                            Element(cur_elements['Dp'],\n                                    float(timestep[cur_elements['Dp']]),\n                                    self._get_wx_units(params,\n                                                       cur_elements['Dp']))\n                    else:\n                        new_timestep.dew_point = \\\n                            Element(cur_elements['Dp'],\n                                    'Not reported')\n\n                    if cur_elements['P'] in timestep:\n                        new_timestep.pressure = \\\n                            Element(cur_elements['P'],\n                                    float(timestep[cur_elements['P']]),\n                                    self._get_wx_units(params, cur_elements['P']))\n                    else:\n                        new_timestep.pressure = \\\n                            Element(cur_elements['P'],\n                                    'Not reported')\n\n                    if cur_elements['Pt'] in timestep:\n                        new_timestep.pressure_tendency = \\\n                            Element(cur_elements['Pt'],\n                                    timestep[cur_elements['Pt']],\n                                    self._get_wx_units(params, cur_elements['Pt']))\n                    else:\n                        new_timestep.pressure_tendency = \\\n                            Element(cur_elements['Pt'],\n                                    'Not reported')\n\n                    new_day.timesteps.append(new_timestep)\n                observation.days.append(new_day)\n\n            return observation", "response": "Get hourly observations for the provided site."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrequest a list of regions from Datapoint. Returns each Region as a Site object.", "response": "def get_all_regions(self):\n        '''\n        Request a list of regions from Datapoint. Returns each Region\n        as a Site object. Regions rarely change, so we cache the response\n        for one hour to minimise requests to API.\n        '''\n        if (time() - self.regions_last_update) < self.regions_update_time:\n            return self.regions_last_request\n\n        response = self.call_api(self.all_regions_path)\n        regions = []\n        for location in response['Locations']['Location']:\n            region = Site()\n            region.id = location['@id']\n            region.region = location['@name']\n            region.name = REGION_NAMES[location['@name']]\n            regions.append(region)\n\n        self.regions_last_update = time()\n        self.regions_last_request = regions\n        return regions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef now(self):\n\n        # From the comments in issue 19: forecast.days[0] is dated for the\n        # previous day shortly after midnight\n\n        now = None\n        # Set the time now to be in the same time zone as the first timestep in\n        # the forecast. This shouldn't cause problems with daylight savings as\n        # the change is far enough after midnight.\n        d = datetime.datetime.now(tz=self.days[0].date.tzinfo)\n        # d is something like datetime.datetime(2019, 1, 19, 17, 5, 28, 337439)\n        # d.replace(...) is datetime.datetime(2019, 1, 19, 0, 0)\n        # for_total_seconds is then: datetime.timedelta(seconds=61528,\n        #                                               microseconds=337439)\n        # In this example, this is (17*60*60) + (5*60) + 28 = 61528\n        # this is the number of seconds through the day\n        for_total_seconds = d - \\\n            d.replace(hour=0, minute=0, second=0, microsecond=0)\n\n        # In the example time,\n        # for_total_seconds.total_seconds() = 61528 + 0.337439\n        # This is the number of seconds after midnight\n        # msm is then the number of minutes after midnight\n        msm = for_total_seconds.total_seconds() / 60\n\n        # If the date now and the date in the forecast are the same, proceed\n        if self.days[0].date.strftime(\"%Y-%m-%dZ\") == d.strftime(\"%Y-%m-%dZ\"):\n            # We have determined that the date in the forecast and the date now\n            # are the same.\n            #\n            # Now, test if timestep.name is larger than the number of minutes\n            # since midnight for each timestep.\n            # The timestep we keep is the one with the largest timestep.name\n            # which is less than the number of minutes since midnight\n            for timestep in self.days[0].timesteps:\n                if timestep.name > msm:\n\n                    # break here stops the for loop\n                    break\n                # now is assigned to the last timestep that did not break the\n                # loop\n                now = timestep\n            return now\n        # Bodge to get around problems near midnight:\n        # Previous method does not account for the end of the month. The test\n        # trying to be evaluated is that the absolute difference between the\n        # last timestep of the first day and the current time is less than 4\n        # hours. 4 hours is because the final timestep of the previous day is\n        # for 21:00\n        elif abs(self.days[0].timesteps[-1].date - d).total_seconds() < 14400:\n            # This is verbose to check that the returned data makes sense\n            timestep_to_return = self.days[0].timesteps[-1]\n\n            return timestep_to_return\n        else:\n            return False", "response": "Function to return just the current timestep from this forecast\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to return a future timestep", "response": "def future(self,in_days=None,in_hours=None,in_minutes=None,in_seconds=None):\n        \"\"\"\n        Function to return a future timestep\n        \"\"\"\n        future = None\n\n        # Initialize variables to 0\n        dd, hh, mm, ss = [0 for i in range(4)]\n        if (in_days != None):\n            dd = dd + in_days\n        if (in_hours != None):\n            hh = hh + in_hours\n        if (in_minutes != None):\n            mm = mm + in_minutes\n        if (in_seconds != None):\n            ss = ss + in_seconds\n\n        # Set the hours, minutes and seconds from now (minus the days)\n        dnow = datetime.datetime.utcnow()  # Now\n        d = dnow + \\\n            datetime.timedelta(hours=hh, minutes=mm, seconds = ss)\n        # Time from midnight\n        for_total_seconds = d - \\\n            d.replace(hour=0, minute=0, second=0, microsecond=0)\n\n        # Convert into minutes since midnight\n        try:\n            msm = for_total_seconds.total_seconds()/60.\n        except:\n            # For versions before 2.7\n            msm = self.timedelta_total_seconds(for_total_seconds)/60.\n\n        if (dd<len(self.days)):\n            for timestep in self.days[dd].timesteps:\n                if timestep.name >= msm:\n                    future = timestep\n                    return future\n        else:\n            print('ERROR: requested date is outside the forecast range selected,' + str(len(self.days)))\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install_API_key(api_key, profile_name='default'):\n    fname = API_profile_fname(profile_name)\n    if not os.path.isdir(os.path.dirname(fname)):\n        os.makedirs(os.path.dirname(fname))\n    with open(fname, 'w') as fh:\n        fh.write(api_key)", "response": "Installs the given API key into the given profile name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the given type is a tuple.", "response": "def is_tuple(type_: Type[Any]) -> bool:\n    '''\n    Tuple[int, str]\n    Tuple\n    '''\n    if HAS_TUPLEARGS:\n        # The tuple, Tuple thing is a difference between 3.6 and 3.7\n        # In 3.6 and before, Tuple had an __extra__ field, while Tuple[something]\n        # would have the normal __origin__ field.\n        #\n        # Those apply for Dict, List, Set, Tuple\n        return _generic_type_check(type_, tuple, Tuple)\n    else:\n        # Old python\n        return _issubclass(type_, Tuple) and _issubclass(type_, tuple) == False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_union(type_: Type[Any]) -> bool:\n    '''\n    Union[A, B]\n    Union\n    Optional[A]\n    '''\n    if HAS_UNIONSUBCLASS:\n        # Old python\n        return _issubclass(type_, Union)\n    else:\n        return getattr(type_, '__origin__', None) == Union", "response": "Check if type_ is a Union."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_namedtuple(type_: Type[Any]) -> bool:\n    '''\n    Generated with typing.NamedTuple\n    '''\n    return _issubclass(type_, tuple) and hasattr(type_, '_field_types') and hasattr(type_, '_fields')", "response": "Returns true if the given type is a namedtuple."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uniontypes(type_: Type[Any]) -> Set[Type[Any]]:\n    '''\n    Returns the types of a Union.\n\n    Raises ValueError if the argument is not a Union\n    and AttributeError when running on an unsupported\n    Python version.\n    '''\n    if not is_union(type_):\n        raise ValueError('Not a Union: ' + str(type_))\n\n    if hasattr(type_, '__args__'):\n        return set(type_.__args__)\n    elif hasattr(type_, '__union_params__'):\n        return set(type_.__union_params__)\n    raise AttributeError('The typing API for this Python version is unknown')", "response": "Returns the types of a Union."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index(self, value: Any) -> int:\n        for i, cond in ((j[0], j[1][0]) for j in enumerate(self.handlers)):\n            try:\n                match = cond(value)\n            except:\n                if self.raiseconditionerrors:\n                    raise\n                match = False\n            if match:\n                return i\n        raise TypedloadValueError('Unable to dump %s' % value, value=value)", "response": "Returns the index in the handlers list that matches the given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump(self, value: Any) -> Any:\n        index = self.index(value)\n        func = self.handlers[index][1]\n        return func(self, value)", "response": "Dump the typed data structure into its\n        untyped equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a value using a ForwardRef.", "response": "def _forwardrefload(l: Loader, value: Any, type_: type) -> Any:\n    \"\"\"\n    This resolves a ForwardRef.\n\n    It just looks up the type in the dictionary of known types\n    and loads the value using that.\n    \"\"\"\n    if l.frefs is None:\n        raise TypedloadException('ForwardRef resolving is disabled for the loader', value=value, type_=type_)\n    tname = type_.__forward_arg__  # type: ignore\n    t = l.frefs.get(tname)\n    if t is None:\n        raise TypedloadValueError(\n            \"ForwardRef '%s' unknown\" % tname,\n            value=value,\n            type_=type_\n        )\n    return l.load(value, t, annotation=Annotation(AnnotationType.FORWARDREF, tname))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _basicload(l: Loader, value: Any, type_: type) -> Any:\n\n    if type(value) != type_:\n        if l.basiccast:\n            try:\n                return type_(value)\n            except ValueError as e:\n                raise TypedloadValueError(str(e), value=value, type_=type_)\n            except TypeError as e:\n                raise TypedloadTypeError(str(e), value=value, type_=type_)\n            except Exception as e:\n                raise TypedloadException(str(e), value=value, type_=type_)\n        else:\n            raise TypedloadValueError('Not of type %s' % type_, value=value, type_=type_)\n    return value", "response": "This function converts a value into a basic type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _dictload(l: Loader, value, type_) -> Dict:\n    key_type, value_type = type_.__args__\n    try:\n        return {\n            l.load(k, key_type, annotation=Annotation(AnnotationType.KEY, k)): l.load(v, value_type, annotation=Annotation(AnnotationType.VALUE, v))\n            for k, v in value.items()}\n    except AttributeError as e:\n        raise TypedloadAttributeError(str(e), type_=type_, value=value)", "response": "This loads into something like Dict [ str ]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setload(l: Loader, value, type_) -> Set:\n    t = type_.__args__[0]\n    return {l.load(i, t) for i in value}", "response": "This loads into something like Set [ int ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tupleload(l: Loader, value, type_) -> Tuple:\n    if HAS_TUPLEARGS:\n        args = type_.__args__\n    else:\n        args = type_.__tuple_params__\n\n    if len(args) == 2 and args[1] == ...: # Tuple[something, ...]\n        return tuple(l.load(i, args[0]) for i in value)\n    else: # Tuple[something, something, somethingelse]\n        if l.failonextra and len(value) > len(args):\n            raise TypedloadValueError('Value is too long for type %s' % type_, value=value, type_=type_)\n        elif len(value) < len(args):\n            raise TypedloadValueError('Value is too short for type %s' % type_, value=value, type_=type_)\n        return tuple(l.load(v, t, annotation=Annotation(AnnotationType.INDEX, i)) for i, (v, t) in enumerate(zip(value, args)))", "response": "This loads into something like Tuple [ int str ]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _namedtupleload(l: Loader, value: Dict[str, Any], type_) -> Tuple:\n    if not hasattr(type_, '__dataclass_fields__'):\n        fields = set(type_._fields)\n        optional_fields = set(getattr(type_, '_field_defaults', {}).keys())\n        type_hints = type_._field_types\n    else:\n        #dataclass\n        import dataclasses\n        fields = set(type_.__dataclass_fields__.keys())\n        optional_fields = {k for k,v in type_.__dataclass_fields__.items() if not (isinstance(getattr(v, 'default', dataclasses._MISSING_TYPE()), dataclasses._MISSING_TYPE) and isinstance(getattr(v, 'default_factory', dataclasses._MISSING_TYPE()), dataclasses._MISSING_TYPE))}\n        type_hints = {k: v.type for k,v in type_.__dataclass_fields__.items()}\n\n        #Name mangling\n\n        # Prepare the list of the needed name changes\n        transforms = []  # type: List[Tuple[str, str]]\n        for field in fields:\n            if type_.__dataclass_fields__[field].metadata:\n                name = type_.__dataclass_fields__[field].metadata.get('name')\n                if name:\n                    transforms.append((field, name))\n        # Do the needed name changes\n        if transforms:\n            value = value.copy()\n            for pyname, dataname in transforms:\n                if dataname in value:\n                    tmp = value[dataname]\n                    del value[dataname]\n                    value[pyname] = tmp\n\n    necessary_fields = fields.difference(optional_fields)\n    try:\n        vfields = set(value.keys())\n    except AttributeError as e:\n        raise TypedloadAttributeError(str(e), value=value, type_=type_)\n\n    if necessary_fields.intersection(vfields) != necessary_fields:\n        raise TypedloadValueError(\n            'Value does not contain fields: %s which are necessary for type %s' % (\n                necessary_fields.difference(vfields),\n                type_\n            ),\n            value=value,\n            type_=type_,\n        )\n\n    fieldsdiff = vfields.difference(fields)\n    if l.failonextra and len(fieldsdiff):\n        extra = ', '.join(fieldsdiff)\n        raise TypedloadValueError(\n            'Dictionary has unrecognized fields: %s and cannot be loaded into %s' % (extra, type_),\n            value=value,\n            type_=type_,\n        )\n\n    params = {}\n    for k, v in value.items():\n        if k not in fields:\n            continue\n        params[k] = l.load(\n            v,\n            type_hints[k],\n            annotation=Annotation(AnnotationType.FIELD, k),\n        )\n    return type_(**params)", "response": "This loads a Dict[str Any ] into a NamedTuple."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a value into a union.", "response": "def _unionload(l: Loader, value, type_) -> Any:\n    \"\"\"\n    Loads a value into a union.\n\n    Basically this iterates all the types inside the\n    union, until one that doesn't raise an exception\n    is found.\n\n    If no suitable type is found, an exception is raised.\n    \"\"\"\n    try:\n        args = uniontypes(type_)\n    except AttributeError:\n        raise TypedloadAttributeError('The typing API for this Python version is unknown')\n\n    # Do not convert basic types, if possible\n    if type(value) in args.intersection(l.basictypes):\n        return value\n\n    exceptions = []\n\n    # Try all types\n    for t in args:\n        try:\n            return l.load(value, t, annotation=Annotation(AnnotationType.UNION, t))\n        except Exception as e:\n            exceptions.append(e)\n    raise TypedloadValueError(\n        'Value could not be loaded into %s' % type_,\n            value=value,\n            type_=type_,\n            exceptions=exceptions\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload an object into an Enum.", "response": "def _enumload(l: Loader, value, type_) -> Enum:\n    \"\"\"\n    This loads something into an Enum.\n\n    It tries with basic types first.\n\n    If that fails, it tries to look for type annotations inside the\n    Enum, and tries to use those to load the value into something\n    that is compatible with the Enum.\n\n    Of course if that fails too, a ValueError is raised.\n    \"\"\"\n    try:\n        # Try na\u00efve conversion\n        return type_(value)\n    except:\n        pass\n\n    # Try with the typing hints\n    for _, t in get_type_hints(type_).items():\n        try:\n            return type_(l.load(value, t))\n        except:\n            pass\n    raise TypedloadValueError(\n        'Value could not be loaded into %s' % type_,\n        value=value,\n        type_=type_\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _noneload(l: Loader, value, type_) -> None:\n    if value is None:\n        return None\n    raise TypedloadValueError('Not None', value=value, type_=type_)", "response": "Load a value that can only be None so it fails"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index(self, type_: Type[T]) -> int:\n        for i, cond in ((q[0], q[1][0]) for q in enumerate(self.handlers)):\n            try:\n                match = cond(type_)\n            except:\n                if self.raiseconditionerrors:\n                    raise\n                match = False\n            if match:\n                return i\n        raise ValueError('No matching condition found')", "response": "Returns the index in the handlers list that matches the given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, value: Any, type_: Type[T], *, annotation: Optional[Annotation] = None) -> T:\n        try:\n            index = self.index(type_)\n        except ValueError:\n            raise TypedloadTypeError(\n                'Cannot deal with value of type %s' % type_,\n                value=value,\n                type_=type_\n            )\n\n        # Add type to known types, to resolve ForwardRef later on\n        if self.frefs is not None and hasattr(type_, '__name__'):\n            tname = type_.__name__\n            if tname not in self.frefs:\n                self.frefs[tname] = type_\n\n        func = self.handlers[index][1]\n        try:\n            return func(self, value, type_)\n        except Exception as e:\n            assert isinstance(e, TypedloadException)\n            e.trace.insert(0, TraceItem(value, type_, annotation))\n            raise e", "response": "Loads value into the typed data structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget data from Yahoo weather API.", "response": "def get_data(city: Optional[str]) -> Dict[str, Any]:\n    \"\"\"\n    Use the Yahoo weather API to get weather information\n    \"\"\"\n    req = urllib.request.Request(get_url(city))\n    with urllib.request.urlopen(req) as f:\n        response = f.read()\n    answer = response.decode('ascii')\n    data = json.loads(answer)\n    r = data['query']['results']['channel']  # Remove some useless nesting\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(value: Any, type_: Type[T], **kwargs) -> T:\n    from . import dataloader\n    loader = dataloader.Loader(**kwargs)\n    return loader.load(value, type_)", "response": "Load data into a type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(value: Any, **kwargs) -> Any:\n    from . import datadumper\n    dumper = datadumper.Dumper(**kwargs)\n    return dumper.dump(value)", "response": "Quick function to dump a data structure into a\n    object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attrload(value: Any, type_: Type[T], **kwargs) -> T:\n    from . import dataloader\n    from .plugins import attrload as loadplugin\n    loader = dataloader.Loader(**kwargs)\n    loadplugin.add2loader(loader)\n    return loader.load(value, type_)", "response": "Load data supporting the attr module\n    in addition to the default ones."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nselect the first case that becomes ready.", "response": "def select(*cases):\n    \"\"\"\n    Select the first case that becomes ready.\n    If a default case (:class:`goless.dcase`) is present,\n    return that if no other cases are ready.\n    If there is no default case and no case is ready,\n    block until one becomes ready.\n\n    See Go's ``reflect.Select`` method for an analog\n    (http://golang.org/pkg/reflect/#Select).\n\n    :param cases: List of case instances, such as\n      :class:`goless.rcase`, :class:`goless.scase`, or :class:`goless.dcase`.\n    :return: ``(chosen case, received value)``.\n      If the chosen case is not an :class:`goless.rcase`, it will be None.\n    \"\"\"\n    if len(cases) == 0:\n        return\n    # If the first argument is a list, it should be the only argument\n    if isinstance(cases[0], list):\n        if len(cases) != 1:\n            raise TypeError('Select can be called either with a list of cases '\n                            'or multiple case arguments, but not both.')\n        cases = cases[0]\n        if not cases:\n            # Handle the case of an empty list as an argument,\n            # and prevent the raising of a SystemError by libev.\n            return\n\n    default = None\n    for c in cases:\n        if c.ready():\n            return c, c.exec_()\n        if isinstance(c, dcase):\n            assert default is None, 'Only one default case is allowd.'\n            default = c\n    if default is not None:\n        # noinspection PyCallingNonCallable\n        return default, None\n\n    # We need to check for deadlocks before selecting.\n    # We can't rely on the underlying backend to do it,\n    # as we do for channels, since we don't do an actual send or recv here.\n    # It's possible to still have a deadlock unless we move the check into\n    # the loop, but since the check is slow\n    # (gevent doesn't provide a fast way), let's leave it out here.\n    if _be.would_deadlock():\n        raise _Deadlock('No other tasklets running, cannot select.')\n    while True:\n        for c in cases:\n            if c.ready():\n                return c, c.exec_()\n        _be.yield_()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the backend for a given name from the environment variable.", "response": "def calculate_backend(name_from_env, backends=None):\n    \"\"\"\n    Calculates which backend to use with the following algorithm:\n\n    - Try to read the GOLESS_BACKEND environment variable.\n      Usually 'gevent' or 'stackless'.\n      If a value is set but no backend is available or it fails to be created,\n      this function will error.\n    - Determine the default backend (gevent for PyPy, stackless for Python).\n      If no default can be determined or created, continue.\n    - Try to create all the runtimes and choose the first one to create\n      successfully.\n    - If no runtime can be created, return a NullBackend,\n      which will error when accessed.\n\n    The \"default\" backend is the less-easy backend for a runtime.\n    Since PyPy has stackless by default, gevent is intentional.\n    Since Stackless is a separate interpreter for CPython,\n    that is more intentional than gevent.\n    We feel this is a good default behavior.\n    \"\"\"\n    if backends is None:\n        backends = _default_backends\n    if name_from_env:\n        if name_from_env not in backends:\n            raise RuntimeError(\n                'Invalid backend %r specified. Valid backends are: %s'\n                % (name_from_env, _default_backends.keys()))\n        # Allow this to raise, since it was explicitly set from the environment\n        # noinspection PyCallingNonCallable\n        return backends[name_from_env]()\n    try:\n        return _calc_default(backends)\n    except SystemError:\n        pass\n    for maker in backends.values():\n        # noinspection PyBroadException\n        try:\n            return maker()\n        except Exception:\n            pass\n    return NullBackend()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreceiving a value from the channel.", "response": "def recv(self):\n        \"\"\"\n        Receive a value from the channel.\n        Receiving will always block if no value is available.\n        If the channel is already closed,\n        :class:`goless.ChannelClosed` will be raised.\n        If the channel closes during a blocking ``recv``,\n        :class:`goless.ChannelClosed` will be raised. (#TODO)\n        \"\"\"\n        if self._closed and not self.recv_ready():\n            raise ChannelClosed()\n        got = self._recv()\n        return got"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when an unhandled error occurs in a coroutine.", "response": "def on_panic(etype, value, tb):\n    \"\"\"\n    Called when there is an unhandled error in a goroutine.\n    By default, logs and exits the process.\n    \"\"\"\n    _logging.critical(_traceback.format_exception(etype, value, tb))\n    _be.propagate_exc(SystemExit, 1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a function in a new tasklet like a goroutine.", "response": "def go(func, *args, **kwargs):\n    \"\"\"\n    Run a function in a new tasklet, like a goroutine.\n    If the goroutine raises an unhandled exception (*panics*),\n    the :func:`goless.on_panic` will be called,\n    which by default logs the error and exits the process.\n\n    :param args: Positional arguments to ``func``.\n    :param kwargs: Keyword arguments to ``func``.\n    \"\"\"\n\n    def safe_wrapped(f):\n        # noinspection PyBroadException\n        try:\n            f(*args, **kwargs)\n        except:\n            on_panic(*_sys.exc_info())\n\n    _be.start(safe_wrapped, func)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nturns the multi - line output of a benchmark process into a sequence of BenchmarkResult instances.", "response": "def stdout_to_results(s):\n    \"\"\"Turns the multi-line output of a benchmark process into\n    a sequence of BenchmarkResult instances.\"\"\"\n    results = s.strip().split('\\n')\n    return [BenchmarkResult(*r.split()) for r in results]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning BenchmarkResults for a given executable and backend.", "response": "def benchmark_process_and_backend(exe, backend):\n    \"\"\"Returns BenchmarkResults for a given executable and backend.\"\"\"\n    env = dict(os.environ)\n    env['GOLESS_BACKEND'] = backend\n    args = [exe, '-m', 'benchmark']\n    return get_benchproc_results(args, env=env)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning all platforms backends and benchmarks and returns as list of", "response": "def collect_results():\n    \"\"\"Runs all platforms/backends/benchmarks and returns as list of\n    BenchmarkResults, sorted by benchmark and time taken.\n    \"\"\"\n    results = []\n    for exe, backendname in EXE_BACKEND_MATRIX:\n        results.extend(benchmark_process_and_backend(exe, backendname))\n    results.extend(benchmark_go())\n\n    results.sort(\n        key=lambda br: (br.benchmark, float(br.time), br.platform, br.backend))\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a sequence of BenchmarkResults and return a new sequence where a seperator BenchmarkResult has been placed between differing benchmarks to provide a visual difference.", "response": "def insert_seperator_results(results):\n    \"\"\"Given a sequence of BenchmarkResults,\n    return a new sequence where a \"seperator\" BenchmarkResult has been placed\n    between differing benchmarks to provide a visual difference.\"\"\"\n    sepbench = BenchmarkResult(*[' ' * w for w in COLUMN_WIDTHS])\n    last_bm = None\n    for r in results:\n        if last_bm is None:\n            last_bm = r.benchmark\n        elif last_bm != r.benchmark:\n            yield sepbench\n            last_bm = r.benchmark\n        yield r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the BYML and get the root node with all children.", "response": "def parse(self) -> typing.Union[list, dict, None]:\n        \"\"\"Parse the BYML and get the root node with all children.\"\"\"\n        root_node_offset = self._read_u32(12)\n        if root_node_offset == 0:\n            return None\n\n        node_type = self._data[root_node_offset]\n        if not _is_container_type(node_type):\n            raise ValueError(\"Invalid root node: expected array or dict, got type 0x%x\" % node_type)\n        return self._parse_node(node_type, 12)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether or not this field s permissions can be shown.", "response": "def check_permission(self, request):\n        \"\"\"\n        Check this field's permissions to determine whether or not it may be\n        shown.\n        \"\"\"\n        return all((permission.has_permission(request) for permission in self.permission_classes))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fields(self):\n        ret = super(FieldPermissionSerializerMixin, self).fields\n        request = self._context[\"request\"]\n\n        forbidden_field_names = [\n            field_name for field_name, field in ret.items() if hasattr(field, 'check_permission') and (not field.check_permission(request))\n        ]\n\n        for field_name in forbidden_field_names:\n            ret.pop(field_name)\n\n        return ret", "response": "Supercedes drf s serializers. ModelSerializer s fields property\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_github_url(\n    repo,\n    branch=None,\n    path='requirements.txt',\n    token=None\n):\n    \"\"\"\n    Builds a URL to a file inside a Github repository.\n    \"\"\"\n\n    repo = re.sub(r\"^http(s)?://github.com/\", \"\", repo).strip('/')\n\n    # args come is as 'None' instead of not being provided\n    if not path:\n        path = 'requirements.txt'\n\n    if not branch:\n        branch = get_default_branch(repo)\n\n    url = 'https://raw.githubusercontent.com/{}/{}/{}'.format(\n        repo, branch, path\n    )\n\n    if token:\n        url = '{}?token={}'.format(url, token)\n\n    return url", "response": "Builds a URL to a file inside a Github repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_branch(repo):\n    url = \"{}/repos/{}\".format(GITHUB_API_BASE, repo)\n    response = requests.get(url)\n    if response.status_code == 200:\n        api_response = json.loads(response.text)\n        return api_response['default_branch']\n    else:\n        return 'master'", "response": "returns the name of the default branch of the repo"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_requirements_file_from_url(url):\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        return StringIO(response.text)\n    else:\n        return StringIO(\"\")", "response": "fetches the requiremets from the url"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the PyPI url for a given requirement and optional version number.", "response": "def get_pypi_url(requirement, version=None, base_url=PYPI_BASE_URL):\n    \"\"\"\n    Get the PyPI url for a given requirement and optional version number and\n    PyPI base URL. The default base url is 'https://pypi.python.org/pypi'\n    \"\"\"\n    if version:\n        return '{base}/{req}/{version}/json'.format(base=base_url,\n                                                    req=requirement,\n                                                    version=version)\n    else:\n        return '{base}/{req}/json'.format(base=base_url, req=requirement)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake a file and return a list of tuples of ( requirement versions ignore ) based on the files requirements specs.", "response": "def parse_req_file(req_file, verbatim=False):\n    \"\"\"Take a file and return a dict of (requirement, versions, ignore) based\n    on the files requirements specs.\n    \"\"\"\n    req_list = []\n    requirements = req_file.readlines()\n    for requirement in requirements:\n        requirement_no_comments = requirement.split('#')[0].strip()\n\n        # if matching requirement line (Thing==1.2.3), update dict, continue\n        req_match = re.match(\n            r'\\s*(?P<package>[^\\s\\[\\]]+)(?P<extras>\\[\\S+\\])?==(?P<version>\\S+)',\n            requirement_no_comments\n        )\n        req_ignore = requirement.strip().endswith('  # norot')\n\n        if req_match:\n            req_list.append((req_match.group('package'),\n                             req_match.group('version'),\n                             req_ignore))\n        elif requirement_no_comments.startswith('-r'):\n            try:\n                base_dir = os.path.dirname(os.path.abspath(req_file.name))\n            except AttributeError:\n                print(\n                    'Recursive requirements are not supported in URL based '\n                    'lookups'\n                )\n                continue\n\n            # replace the -r and ensure there are no leading spaces\n            file_name = requirement_no_comments.replace('-r', '').strip()\n            new_path = os.path.join(base_dir, file_name)\n            try:\n                if verbatim:\n                    req_list.append((None, requirement, req_ignore))\n                req_list.extend(\n                    parse_req_file(\n                        open(new_path),\n                        verbatim=verbatim\n                    )\n                )\n            except IOError:\n                print('Failed to import {}'.format(file_name))\n        elif verbatim:\n            req_list.append((None, requirement, req_ignore))\n    return req_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_version_and_release_date(requirement, version=None,\n                                 verbose=False, response=None):\n    \"\"\"Given a requirement and optional version returns a (version, releasedate)\n    tuple. Defaults to the latest version. Prints to stdout if verbose is True.\n    Optional response argument is the response from PyPI to be used for\n    asyncronous lookups.\n    \"\"\"\n    try:\n        if not response:\n            url = get_pypi_url(requirement, version)\n            response = requests.get(url)\n\n        # see if the url is 404'ing because it has been redirected\n        if response.status_code == 404:\n            root_url = url.rpartition('/')[0]\n            res = requests.head(root_url)\n            if res.status_code == 301:\n                new_location = res.headers['location'] + '/json'\n                response = requests.get(new_location)\n\n        response = response.json()\n    except requests.HTTPError:\n        if version:\n            if verbose:\n                print('{} ({}) isn\\'t available on PyPI '\n                      'anymore!'.format(requirement, version))\n        else:\n            if verbose:\n                print('{} isn\\'t on PyPI. Check that the project '\n                      'still exists!'.format(requirement))\n        return None, None\n    except ValueError:\n        if verbose:\n            print('Decoding the JSON response for {} ({}) '\n                  'failed'.format(requirement, version))\n        return None, None\n\n    try:\n        if version:\n            if version in response['releases']:\n                release_date = response['releases'][version][0]['upload_time']\n            else:\n                return None, None\n        else:\n            version = response['info'].get('stable_version')\n\n            if not version:\n                versions = {\n                    v: parse_version(v) for v in response['releases'].keys()\n                    if not parse_version(v).is_prerelease()\n                }\n\n                # if we still don't have a version, let's pick up a prerelease one\n                if not versions:\n                    versions = {\n                        v: parse_version(v) for v in response['releases'].keys()\n                    }\n\n                if versions:\n                    version = max(versions.items(), key=operator.itemgetter(1))[0]\n                    release_date = (\n                        response['releases'][str(version)][0]['upload_time']\n                    )\n                else:\n                    return None, None\n\n        return version, datetime.fromtimestamp(time.mktime(\n            time.strptime(release_date, '%Y-%m-%dT%H:%M:%S')\n        ))\n    except IndexError:\n        if verbose:\n            print('{} ({}) didn\\'t return a date property'.format(requirement,\n                                                                  version))\n        return None, None", "response": "Given a requirement and optional version returns a tuple containing the latest version and release date."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a list of requirements files reports which requirements are out of date. Everything is rather somewhat obvious: - verbose makes things a little louder - outdated forces piprot to only report out of date packages - latest outputs the requirements line with the latest version - verbatim outputs the requirements file as-is - with comments showing the latest versions (can be used with latest to output the latest with the old version in the comment) - delay specifies a timerange during an outdated package is allowed", "response": "def main(\n    req_files,\n    verbose=False,\n    outdated=False,\n    latest=False,\n    verbatim=False,\n    repo=None,\n    path='requirements.txt',\n    token=None,\n    branch='master',\n    url=None,\n    delay=None,\n):\n    \"\"\"Given a list of requirements files reports which requirements are out\n    of date.\n\n    Everything is rather somewhat obvious:\n    - verbose makes things a little louder\n    - outdated forces piprot to only report out of date packages\n    - latest outputs the requirements line with the latest version\n    - verbatim outputs the requirements file as-is - with comments showing the\n      latest versions (can be used with latest to output the latest with the\n      old version in the comment)\n    - delay specifies a timerange during an outdated package is allowed\n    \"\"\"\n    requirements = []\n\n    if repo:\n        github_url = build_github_url(repo, branch, path, token)\n        req_file = get_requirements_file_from_url(github_url)\n        requirements.extend(parse_req_file(req_file))\n    elif url:\n        req_file = get_requirements_file_from_url(url)\n        requirements.extend(parse_req_file(req_file))\n    else:\n        for req_file in req_files:\n            requirements.extend(parse_req_file(req_file, verbatim=verbatim))\n            req_file.close()\n\n    total_time_delta = 0\n    max_outdated_time = 0\n    session = FuturesSession()\n    results = []\n\n    for req, version, ignore in requirements:\n        if verbatim and not req:\n            results.append(version)\n        elif req:\n            results.append({\n                'req': req,\n                'version': version,\n                'ignore': ignore,\n                'latest': session.get(get_pypi_url(req)),\n                'specified': session.get(get_pypi_url(req, version))\n            })\n\n    for result in results:\n        if isinstance(result, str):\n            print(result.replace('\\n', ''))\n            continue\n\n        if result['ignore']:\n            if verbatim:\n                print('{}=={}  # norot'.format(result['req'], result['version']))\n            else:\n                print('Ignoring updates for {}. '.format(result['req']))\n            continue\n\n        req = result['req']\n        version = result['version']\n\n        latest_version, latest_release_date = get_version_and_release_date(\n            req, verbose=verbose, response=result['latest'].result()\n        )\n        specified_version, specified_release_date = \\\n            get_version_and_release_date(\n                req, version, response=result['specified'].result()\n            )\n\n        if latest_release_date and specified_release_date:\n            time_delta = (latest_release_date - specified_release_date).days\n            total_time_delta = total_time_delta + time_delta\n            max_outdated_time = max(time_delta, max_outdated_time)\n\n            if verbose:\n                if time_delta > 0:\n                    print('{} ({}) is {} days out of date. '\n                          'Latest is {}'.format(req, version, time_delta,\n                                                latest_version))\n                elif version != latest_version:\n                    print('{} ({}) is out of date. '\n                          'Latest is {}'.format(req, version, latest_version))\n                elif not outdated:\n                    print('{} ({}) is up to date'.format(req, version))\n\n            if latest and latest_version != specified_version:\n                print('{}=={}  # Updated from {}'.format(req, latest_version,\n                                                        specified_version))\n            elif verbatim and latest_version != specified_version:\n                print('{}=={}  # Latest {}'.format(req, specified_version,\n                                                  latest_version))\n            elif verbatim:\n                print('{}=={}'.format(req, specified_version))\n\n        elif verbatim:\n            print(\n                '{}=={}  # Error checking latest version'.format(req, version)\n            )\n\n    verbatim_str = \"\"\n    if verbatim:\n        verbatim_str = \"# Generated with piprot {}\\n# \".format(VERSION)\n\n    if total_time_delta > 0 and delay is None:\n        print(\"{}Your requirements are {} \"\n              \"days out of date\".format(verbatim_str, total_time_delta))\n        sys.exit(1)\n    elif delay is not None and max_outdated_time > int(delay):\n        print(\"{}At least one of your dependancies is {} \"\n              \"days out of date which is more than the allowed\"\n              \"{} days.\".format(verbatim_str, max_outdated_time, delay))\n        sys.exit(1)\n    elif delay is not None and max_outdated_time <= int(delay):\n        print(\"{}All of your dependancies are at most {} \"\n              \"days out of date.\".format(verbatim_str, delay))\n        sys.exit(1)\n    else:\n        print(\"{}Looks like you've been keeping up to date, \"\n              \"time for a delicious beverage!\".format(verbatim_str))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the command line arguments and jump into the piprot function.", "response": "def piprot():\n    \"\"\"Parse the command line arguments and jump into the piprot() function\n    (unless the user just wants the post request hook).\n    \"\"\"\n    cli_parser = argparse.ArgumentParser(\n        epilog=\"Here's hoping your requirements are nice and fresh!\"\n    )\n    cli_parser.add_argument(\n        '-v', '--verbose', action='store_true',\n        help='verbosity, can be supplied more than once '\n             '(enabled by default, use --quiet to disable)'\n    )\n    cli_parser.add_argument('-l', '--latest', action='store_true',\n                            help='print the lastest available version for out '\n                                 'of date requirements')\n    cli_parser.add_argument('-x', '--verbatim', action='store_true',\n                            help='output the full requirements file, with '\n                                 'added comments with potential updates')\n    cli_parser.add_argument('-q', '--quiet', action='store_true',\n                            help='be a little less verbose with the output '\n                                 '(<0.3 behaviour)')\n    cli_parser.add_argument('-o', '--outdated', action='store_true',\n                            help='only list outdated requirements')\n\n    cli_parser.add_argument('-g', '--github',\n                            help='Test the requirements from a GitHub repo. '\n                                 'Requires that a `requirements.txt` file '\n                                 'exists in the root of the repository.')\n\n    cli_parser.add_argument(\n        '-b', '--branch',\n        help='The branch to test requirements from, used with '\n             'the Github URL support.')\n\n    cli_parser.add_argument(\n        '-t', '--token',\n        help='Github personal access token to be used with '\n             'the Github URL support.')\n\n    cli_parser.add_argument(\n        '-p', '--path',\n        help='Path to requirements file in remote repository.')\n\n    cli_parser.add_argument(\n        '-d', '--delay',\n        help='Delay before an outdated package triggers an error.'\n             '(in days, default to 1).')\n\n    cli_parser.add_argument('-u', '--url',\n                            help='URL to requirements file.')\n\n    # if there is a requirements.txt file, use it by default. Otherwise print\n    # usage if there are no arguments.\n    nargs = '+'\n\n    if (\n        '--github' in sys.argv\n        or '-g' in sys.argv\n        or '-u' in sys.argv\n        or '--url' in sys.argv\n    ):\n        nargs = \"*\"\n\n    default = None\n    if os.path.isfile('requirements.txt'):\n        nargs = \"*\"\n        default = [open('requirements.txt')]\n\n    cli_parser.add_argument('file', nargs=nargs, type=argparse.FileType(),\n                            default=default, help='requirements file(s), use '\n                                                  '`-` for stdin')\n\n    cli_args = cli_parser.parse_args()\n\n    if len(cli_args.file) > 1 and cli_args.verbatim:\n        sys.exit('--verbatim only allowed for single requirements files')\n\n    verbose = True\n    if cli_args.quiet:\n        verbose = False\n    elif cli_args.verbatim:\n        verbose = False\n\n    # call the main function to kick off the real work\n    main(req_files=cli_args.file, verbose=verbose, outdated=cli_args.outdated,\n         latest=cli_args.latest, verbatim=cli_args.verbatim,\n         repo=cli_args.github, branch=cli_args.branch, path=cli_args.path,\n         token=cli_args.token, url=cli_args.url, delay=cli_args.delay)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the features corresponding to a segment as list of ( value feature ) tuples.", "response": "def fts(self, segment):\n        \"\"\"Return features corresponding to segment as list of (value,\n        feature) tuples\n\n        Args:\n            segment (unicode): segment for which features are to be returned as\n                               Unicode string\n\n        Returns:\n            list: None if `segment` cannot be parsed; otherwise, a list of the\n                  features of `segment` as (value, feature) pairs\n        \"\"\"\n        match = self.seg_regex.match(segment)\n        if match:\n            pre, base, post = match.group('pre'), match.group('base'), match.group('post')\n            seg = copy.deepcopy(self.bases[base])\n            for m in reversed(pre):\n                seg = update_ft_set(seg, self.prefix_dias[m])\n            for m in post:\n                seg = update_ft_set(seg, self.postfix_dias[m])\n            return set(seg)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates whether a set of features match a segment", "response": "def fts_match(self, fts_mask, segment):\n        \"\"\"Evaluates whether a set of features 'match' a segment (are a subset\n        of that segment's features)\n\n        Args:\n            fts_mask (list): list of (value, feature) tuples\n            segment (unicode): IPA string corresponding to segment (consonant or\n                               vowel)\n        Returns:\n            bool: None if `segment` cannot be parsed; True if the feature values\n                  of `fts_mask` are a subset of those for `segment`\n        \"\"\"\n        fts_mask = set(fts_mask)\n        fts_seg = self.fts(segment)\n        if fts_seg:\n            return fts_seg <= fts_mask\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns longest single - segment prefix of word", "response": "def longest_one_seg_prefix(self, word):\n        \"\"\"Return longest IPA Unicode prefix of `word`\n\n        Args:\n            word (unicode): word as IPA string\n\n        Returns:\n            unicode: longest single-segment prefix of `word`\n        \"\"\"\n        match = self.seg_regex.match(word)\n        if match:\n            return match.group(0)\n        else:\n            return ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_segs(self, segs):\n        def whole_seg(seg):\n            m = self.seg_regex.match(seg)\n            if m and m.group(0) == seg:\n                return True\n            else:\n                return False\n        return list(filter(whole_seg, segs))", "response": "Given list of strings return only those which are valid segments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_line(self, line):\n        line0 = line\n        pos = 0\n        while line:\n            seg_m = self.ft.seg_regex.match(line)\n            wsp_m = self.ws_punc_regex.match(line)\n            if seg_m:\n                length = len(seg_m.group(0))\n                line = line[length:]\n                pos += length\n            elif wsp_m:\n                length = len(wsp_m.group(0))\n                line = line[length:]\n                pos += length\n            else:\n                msg = 'IPA not valid at position {} in \"{}\".'.format(pos, line0.strip())\n                # msg = msg.decode('utf-8')\n                print(msg, file=sys.stderr)\n                line = line[1:]\n                pos += 1", "response": "Validate Unicode IPA string relative to panphon."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an iterator of segments in the input text.", "response": "def segment_text(text, seg_regex=SEG_REGEX):\n    \"\"\"Return an iterator of segments in the text.\n\n    Args:\n        text (unicode): string of IPA Unicode text\n        seg_regex (_regex.Pattern): compiled regex defining a segment (base +\n                                    modifiers)\n\n    Return:\n        generator: segments in the input text\n    \"\"\"\n    for m in seg_regex.finditer(text):\n        yield m.group(0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pat(p):\n    pattern = []\n    for matrix in [m.group(0) for m in MT_REGEX.finditer(p)]:\n        segment = set([m.groups() for m in FT_REGEX.finditer(matrix)])\n        pattern.append(segment)\n    return pattern", "response": "Given a string p with feature matrices return a list of sets of value feature tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a list of lists of lists of sets of values to a NumPy array where each row is a segment and each column is a feature.", "response": "def word2array(ft_names, word):\n    \"\"\"Converts `word` [[(value, feature),...],...] to a NumPy array\n\n    Given a word consisting of lists of lists/sets of (value, feature) tuples,\n    return a NumPy array where each row is a segment and each column is a\n    feature.\n\n    Args:\n        ft_names (list): list of feature names (as strings) in order; this\n                         argument controls what features are included in the\n                         array that is output and their order vis-a-vis the\n                         columns of the array\n        word (list): list of lists of feature tuples (output by\n                     FeatureTable.word_fts)\n\n    Returns:\n        ndarray: array in which each row is a segment and each column\n                         is a feature\n    \"\"\"\n    vdict = {'+': 1, '-': -1, '0': 0}\n\n    def seg2col(seg):\n        seg = dict([(k, v) for (v, k) in seg])\n        return [vdict[seg[ft]] for ft in ft_names]\n    return numpy.array([seg2col(s) for s in word], order='F')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the data from data / ipa_all. csv into self. segments seg_dict and names.", "response": "def _read_table(self, filename):\n        \"\"\"Read the data from data/ipa_all.csv into self.segments, a\n        list of 2-tuples of unicode strings and sets of feature tuples and\n        self.seg_dict, a dictionary mapping from unicode segments and sets of\n        feature tuples.\n        \"\"\"\n        filename = pkg_resources.resource_filename(\n            __name__, filename)\n        segments = []\n        with open(filename, 'rb') as f:\n            reader = csv.reader(f, encoding='utf-8')\n            header = next(reader)\n            names = header[1:]\n            for row in reader:\n                seg = row[0]\n                vals = row[1:]\n                specs = set(zip(vals, names))\n                segments.append((seg, specs))\n        seg_dict = dict(segments)\n        return segments, seg_dict, names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nanswering question are all features a subset of ft_seg?", "response": "def fts_match(self, features, segment):\n        \"\"\"Answer question \"are `ft_mask`'s features a subset of ft_seg?\"\n\n        This is like `FeatureTable.match` except that it checks whether a\n        segment is valid and returns None if it is not.\n\n        Args:\n            features (set): pattern defined as set of (value, feature) tuples\n            segment (set): segment defined as a set of (value, feature) tuples\n\n        Returns:\n            bool: True iff all features in `ft_mask` are also in `ft_seg`; None\n                  if segment is not valid\n        \"\"\"\n        features = set(features)\n        if self.seg_known(segment):\n            return features <= self.fts(segment)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns longest Unicode IPA prefix of a word in database", "response": "def longest_one_seg_prefix(self, word):\n        \"\"\"Return longest Unicode IPA prefix of a word\n\n        Args:\n            word (unicode): input word as Unicode IPA string\n\n        Returns:\n            unicode: longest single-segment prefix of `word` in database\n        \"\"\"\n        for i in range(self.longest_seg, 0, -1):\n            if word[:i] in self.seg_dict:\n                return word[:i]\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_word(self, word):\n        while word:\n            match = self.seg_regex.match(word)\n            if match:\n                word = word[len(match.group(0)):]\n            else:\n                # print('{}\\t->\\t{}\\t'.format(orig, word).encode('utf-8'), file=sys.stderr)\n                return False\n        return True", "response": "Returns True if word consists exhaustively of valid IPA segments\n                  is True otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of segments from a word", "response": "def segs(self, word):\n        \"\"\"Returns a list of segments from a word\n\n        Args:\n            word (unicode): input word as Unicode IPA string\n\n        Returns:\n            list: list of strings corresponding to segments found in `word`\n        \"\"\"\n        return [m.group('all') for m in self.seg_regex.finditer(word)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef word_fts(self, word):\n        return list(map(self.fts, self.segs(word)))", "response": "Return featural analysis of word"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of segments that are not valid in a word.", "response": "def segs_safe(self, word):\n        \"\"\"Return a list of segments (as strings) from a word\n\n        Characters that are not valid segments are included in the list as\n        individual characters.\n\n        Args:\n            word (unicode): word as an IPA string\n\n        Returns:\n            list: list of Unicode IPA strings corresponding to segments in\n                  `word`\n        \"\"\"\n        segs = []\n        while word:\n            m = self.seg_regex.match(word)\n            if m:\n                segs.append(m.group(1))\n                word = word[len(m.group(1)):]\n            else:\n                segs.append(word[0])\n                word = word[1:]\n        return segs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string like the input but containing only legal IPA segments with invalid IPA segments", "response": "def filter_string(self, word):\n        \"\"\"Return a string like the input but containing only legal IPA segments\n\n        Args:\n            word (unicode): input string to be filtered\n\n        Returns:\n            unicode: string identical to `word` but with invalid IPA segments\n                     absent\n\n        \"\"\"\n        segs = [m.group(0) for m in self.seg_regex.finditer(word)]\n        return ''.join(segs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the features shared by segs", "response": "def fts_intersection(self, segs):\n        \"\"\"Return the features shared by `segs`\n\n        Args:\n            segs (list): list of Unicode IPA segments\n\n        Returns:\n            set: set of (value, feature) tuples shared by the valid segments in\n                 `segs`\n        \"\"\"\n        fts_vecs = [self.fts(s) for s in self.filter_segs(segs)]\n        return reduce(lambda a, b: a & b, fts_vecs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if any segment in fts in inv matches the features in the IPA segments represented as Unicode strings", "response": "def fts_match_any(self, fts, inv):\n        \"\"\"Return `True` if any segment in `inv` matches the features in `fts`\n\n        Args:\n            fts (list): a collection of (value, feature) tuples\n            inv (list): a collection of IPA segments represented as Unicode\n                        strings\n\n        Returns:\n            bool: `True` if any segment in `inv` matches the features in `fts`\n        \"\"\"\n        return any([self.fts_match(fts, s) for s in inv])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fts_match_all(self, fts, inv):\n        return all([self.fts_match(fts, s) for s in inv])", "response": "Return True if all segments in fts in inv match the features in fts in fts\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fts_contrast2(self, fs, ft_name, inv):\n        inv_fts = [self.fts(x) for x in inv if set(fs) <= self.fts(x)]\n        for a in inv_fts:\n            for b in inv_fts:\n                if a != b:\n                    diff = a ^ b\n                    if len(diff) == 2:\n                        if all([nm == ft_name for (_, nm) in diff]):\n                            return True\n        return False", "response": "Return True if there is a segment in inv that contrasts in feature\n        ft_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the count of segments in an inventory matching a given feature mask.", "response": "def fts_count(self, fts, inv):\n        \"\"\"Return the count of segments in an inventory matching a given\n        feature mask.\n\n        Args:\n            fts (set): feature mask given as a set of (value, feature) tuples\n            inv (set): inventory of segments (as Unicode IPA strings)\n\n        Returns:\n            int: number of segments in `inv` that match feature mask `fts`\n        \"\"\"\n        return len(list(filter(lambda s: self.fts_match(fts, s), inv)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements fixed - width pattern matching.", "response": "def match_pattern(self, pat, word):\n        \"\"\"Implements fixed-width pattern matching.\n\n        Matches just in case pattern is the same length (in segments) as the\n        word and each of the segments in the pattern is a featural subset of the\n        corresponding segment in the word. Matches return the corresponding list\n        of feature sets; failed matches return None.\n\n        Args:\n           pat (list): pattern consisting of a sequence of sets of (value,\n                       feature) tuples\n           word (unicode): a Unicode IPA string consisting of zero or more\n                          segments\n\n        Returns:\n            list: corresponding list of feature sets or, if there is no match,\n                  None\n        \"\"\"\n        segs = self.word_fts(word)\n        if len(pat) != len(segs):\n            return None\n        else:\n            if all([set(p) <= s for (p, s) in zip(pat, segs)]):\n                return segs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match_pattern_seq(self, pat, const):\n        segs = [self.fts(s) for s in const]\n        if len(pat) != len(segs):\n            return False\n        else:\n            return all([set(p) <= s for (p, s) in zip(pat, segs)])", "response": "Implements limited pattern matching."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_segs_matching_fts(self, fts):\n        matching_segs = []\n        for seg, pairs in self.segments:\n            if set(fts) <= set(pairs):\n                matching_segs.append(seg)\n        return sorted(matching_segs, key=lambda x: len(x), reverse=True)", "response": "Return a list of all segments matching a feature mask."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a string describing features masks for a sequence of segments return a regular expression pattern matching the corresponding strings.", "response": "def compile_regex_from_str(self, ft_str):\n        \"\"\"Given a string describing features masks for a sequence of segments,\n        return a regex matching the corresponding strings.\n\n        Args:\n            ft_str (str): feature masks, each enclosed in square brackets, in\n            which the features are delimited by any standard delimiter.\n\n        Returns:\n           Pattern: regular expression pattern equivalent to `ft_str`\n        \"\"\"\n\n        sequence = []\n        for m in re.finditer(r'\\[([^]]+)\\]', ft_str):\n            ft_mask = fts(m.group(1))\n            segs = self.all_segs_matching_fts(ft_mask)\n            sub_pat = '({})'.format('|'.join(segs))\n            sequence.append(sub_pat)\n        pattern = ''.join(sequence)\n        regex = re.compile(pattern)\n        return regex"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a Unicode IPA segment return a list of feature specificiations in cannonical order.", "response": "def segment_to_vector(self, seg):\n        \"\"\"Given a Unicode IPA segment, return a list of feature specificiations\n        in cannonical order.\n\n        Args:\n            seg (unicode): IPA consonant or vowel\n\n        Returns:\n            list: feature specifications ('+'/'-'/'0') in the order from\n            `FeatureTable.names`\n        \"\"\"\n        ft_dict = {ft: val for (val, ft) in self.fts(seg)}\n        return [ft_dict[name] for name in self.names]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef word_to_vector_list(self, word, numeric=False, xsampa=False):\n        if xsampa:\n            word = self.xsampa.convert(word)\n        tensor = list(map(self.segment_to_vector, self.segs(word)))\n        if numeric:\n            return self.tensor_to_numeric(tensor)\n        else:\n            return tensor", "response": "Return a list of feature vectors given a Unicode IPA word."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n\n    parser = ArgumentParser()\n    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\n    parser.add_argument('-i', '--ioc', dest='ioc', default=None,\n                        help='[OPTIONAL] An IoC to attribute.')\n    parser.add_argument('-m', '--md5', dest='md5', default=None,\n                        help='[OPTIONAL] An MD5 hash.')\n    parser.add_argument('--maltego', dest='maltego', default=False, action='store_true',\n                        help='[OPTIONAL] Run in Maltego compatibility mode.')\n    args, _ = parser.parse_known_args()\n\n    tb = ThreatButt(args.maltego)\n\n    if args.ioc:\n        tb.clown_strike_ioc(args.ioc)\n\n    elif args.md5:\n        tb.bespoke_md5(args.md5)\n\n    else:\n        parser.print_help()", "response": "This is the main entry point for the ThreatButt API tool. It is used to create a ThreatButt object for the current site."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clown_strike_ioc(self, ioc):\n        r = requests.get('http://threatbutt.io/api', data='ioc={0}'.format(ioc))\n        self._output(r.text)", "response": "Performs Clown Strike lookup on an IoC."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bespoke_md5(self, md5):\n        r = requests.post('http://threatbutt.io/api/md5/{0}'.format(md5))\n        self._output(r.text)", "response": "Performs Bespoke MD5 lookup on an MD5."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a segment as features returns the sonority on a scale of 1 to 9.", "response": "def sonority_from_fts(self, seg):\n        \"\"\"Given a segment as features, returns the sonority on a scale of 1\n           to 9.\n\n        Args:\n            seg (list): collection of (value, feature) pairs representing\n                        a segment (vowel or consonant)\n\n        Returns:\n           int: sonority of `seg` between 1 and 9\n        \"\"\"\n\n        def match(m):\n            return self.fm.match(fts(m), seg)\n\n        minusHi = BoolTree(match('-hi'), 9, 8)\n        minusNas = BoolTree(match('-nas'), 6, 5)\n        plusVoi1 = BoolTree(match('+voi'), 4, 3)\n        plusVoi2 = BoolTree(match('+voi'), 2, 1)\n        plusCont = BoolTree(match('+cont'), plusVoi1, plusVoi2)\n        plusSon = BoolTree(match('+son'), minusNas, plusCont)\n        minusCons = BoolTree(match('-cons'), 7, plusSon)\n        plusSyl = BoolTree(match('+syl'), minusHi, minusCons)\n        return plusSyl.get_value()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new cache hierarchy from a dictionary.", "response": "def from_dict(cls, d):\n        \"\"\"Create cache hierarchy from dictionary.\"\"\"\n        main_memory = MainMemory()\n        caches = {}\n\n        referred_caches = set()\n\n        # First pass, create all named caches and collect references\n        for name, conf in d.items():\n            caches[name] = Cache(name=name,\n                                 **{k: v for k, v in conf.items()\n                                    if k not in ['store_to', 'load_from', 'victims_to']})\n            if 'store_to' in conf:\n                referred_caches.add(conf['store_to'])\n            if 'load_from' in conf:\n                referred_caches.add(conf['load_from'])\n            if 'victims_to' in conf:\n                referred_caches.add(conf['victims_to'])\n\n        # Second pass, connect caches\n        for name, conf in d.items():\n            if 'store_to' in conf and conf['store_to'] is not None:\n                caches[name].set_store_to(caches[conf['store_to']])\n            if 'load_from' in conf and conf['load_from'] is not None:\n                caches[name].set_load_from(caches[conf['load_from']])\n            if 'victims_to' in conf and conf['victims_to'] is not None:\n                caches[name].set_victims_to(caches[conf['victims_to']])\n\n        # Find first level (not target of any load_from or store_to)\n        first_level = set(d.keys()) - referred_caches\n        assert len(first_level) == 1, \"Unable to find first cache level.\"\n        first_level = caches[list(first_level)[0]]\n\n        # Find last level caches (has no load_from or store_to target)\n        last_level_load = c = first_level\n        while c is not None:\n            last_level_load = c\n            c = c.load_from\n        assert last_level_load is not None, \"Unable to find last cache level.\"\n        last_level_store = c = first_level\n        while c is not None:\n            last_level_store = c\n            c = c.store_to\n        assert last_level_store is not None, \"Unable to find last cache level.\"\n\n        # Set main memory connections\n        main_memory.load_to(last_level_load)\n        main_memory.store_from(last_level_store)\n\n        return cls(first_level, main_memory), caches, main_memory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, addr, length=1):\n        if addr is None:\n            return\n        elif not isinstance(addr, Iterable):\n            self.first_level.load(addr, length=length)\n        else:\n            self.first_level.iterload(addr, length=length)", "response": "Load one or more addresses."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstores one or more adresses.", "response": "def store(self, addr, length=1, non_temporal=False):\n        \"\"\"\n        Store one or more adresses.\n\n        :param addr: byte address of store location\n        :param length: All address from addr until addr+length (exclusive) are\n                       stored (default: 1)\n        :param non_temporal: if True, no write-allocate will be issued, but cacheline will be zeroed\n        \"\"\"\n        if non_temporal:\n            raise ValueError(\"non_temporal stores are not yet supported\")\n\n        if addr is None:\n            return\n        elif not isinstance(addr, Iterable):\n            self.first_level.store(addr, length=length)\n        else:\n            self.first_level.iterstore(addr, length=length)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading and store all bytes between addr and length.", "response": "def loadstore(self, addrs, length=1):\n        \"\"\"\n        Load and store address in order given.\n\n        :param addrs: iteratable of address tuples: [(loads, stores), ...]\n        :param length: will load and store all bytes between addr and\n                       addr+length (for each address)\n        \"\"\"\n        if not isinstance(addrs, Iterable):\n            raise ValueError(\"addr must be iteratable\")\n\n        self.first_level.loadstore(addrs, length=length)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator that yields all cache levels.", "response": "def levels(self, with_mem=True):\n        \"\"\"Return cache levels, optionally including main memory.\"\"\"\n        p = self.first_level\n        while p is not None:\n            yield p\n            # FIXME bad hack to include victim caches, need a more general solution, probably\n            # involving recursive tree walking\n            if p.victims_to is not None and p.victims_to != p.load_from:\n                yield p.victims_to\n            if p.store_to is not None and p.store_to != p.load_from and p.store_to != p.victims_to:\n                yield p.store_to\n            p = p.load_from\n\n        if with_mem:\n            yield self.main_memory"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count_invalid_entries(self):\n        return sum([c.count_invalid_entries() for c in self.levels(with_mem=False)])", "response": "Sum of all invalid entry counts from cache levels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cl_start(self, addr):\n        return addr >> self.backend.cl_bits << self.backend.cl_bits", "response": "Return first address belonging to the same cacheline as addr."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_load_from(self, load_from):\n        assert load_from is None or isinstance(load_from, Cache), \\\n            \"load_from needs to be None or a Cache object.\"\n        assert load_from is None or load_from.cl_size <= self.cl_size, \\\n            \"cl_size may only increase towards main memory.\"\n        self.load_from = load_from\n        self.backend.load_from = load_from.backend", "response": "Update load_from in Cache and backend."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating store_to in Cache and backend.", "response": "def set_store_to(self, store_to):\n        \"\"\"Update store_to in Cache and backend.\"\"\"\n        assert store_to is None or isinstance(store_to, Cache), \\\n            \"store_to needs to be None or a Cache object.\"\n        assert store_to is None or store_to.cl_size <= self.cl_size, \\\n            \"cl_size may only increase towards main memory.\"\n        self.store_to = store_to\n        self.backend.store_to = store_to.backend"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates victims_to in Cache and backend.", "response": "def set_victims_to(self, victims_to):\n        \"\"\"Update victims_to in Cache and backend.\"\"\"\n        assert victims_to is None or isinstance(victims_to, Cache), \\\n            \"store_to needs to be None or a Cache object.\"\n        assert victims_to is None or victims_to.cl_size == self.cl_size, \\\n            \"cl_size may only increase towards main memory.\"\n        self.victims_to = victims_to\n        self.backend.victims_to = victims_to.backend"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stats(self):\n        assert self.backend.LOAD_count >= 0, \"LOAD_count < 0\"\n        assert self.backend.LOAD_byte >= 0, \"LOAD_byte < 0\"\n        assert self.backend.STORE_count >= 0, \"STORE_count < 0\"\n        assert self.backend.STORE_byte >= 0, \"STORE_byte < 0\"\n        assert self.backend.HIT_count >= 0, \"HIT_count < 0\"\n        assert self.backend.HIT_byte >= 0, \"HIT_byte < 0\"\n        assert self.backend.MISS_count >= 0, \"MISS_count < 0\"\n        assert self.backend.MISS_byte >= 0, \"MISS_byte < 0\"\n        assert self.backend.EVICT_count >= 0, \"EVICT_count < 0\"\n        assert self.backend.EVICT_byte >= 0, \"EVICT_byte < 0\"\n        return {'name': self.name,\n                'LOAD_count': self.backend.LOAD_count,\n                'LOAD_byte': self.backend.LOAD_byte,\n                'STORE_count': self.backend.STORE_count,\n                'STORE_byte': self.backend.STORE_byte,\n                'HIT_count': self.backend.HIT_count,\n                'HIT_byte': self.backend.HIT_byte,\n                'MISS_count': self.backend.MISS_count,\n                'MISS_byte': self.backend.MISS_byte,\n                'EVICT_count': self.backend.EVICT_count,\n                'EVICT_byte': self.backend.EVICT_byte}", "response": "Return dictionay with all stats at this level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset level where to load from.", "response": "def load_to(self, last_level_load):\n        \"\"\"Set level where to load from.\"\"\"\n        assert isinstance(last_level_load, Cache), \\\n            \"last_level needs to be a Cache object.\"\n        assert last_level_load.load_from is None, \\\n            \"last_level_load must be a last level cache (.load_from is None).\"\n        self.last_level_load = last_level_load"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef store_from(self, last_level_store):\n        assert isinstance(last_level_store, Cache), \\\n            \"last_level needs to be a Cache object.\"\n        assert last_level_store.store_to is None, \\\n            \"last_level_store must be a last level cache (.store_to is None).\"\n        self.last_level_store = last_level_store", "response": "Set level where to store to."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn dictionay with all stats at this level.", "response": "def stats(self):\n        \"\"\"Return dictionay with all stats at this level.\"\"\"\n        load_count = self.last_level_load.MISS_count\n        load_byte = self.last_level_load.MISS_byte\n\n        if self.last_level_load.victims_to is not None:\n            # If there is a victim cache between last_level and memory, subtract all victim hits\n            load_count -= self.last_level_load.victims_to.HIT_count\n            load_byte -= self.last_level_load.victims_to.HIT_byte\n\n        return {'name': self.name,\n                'LOAD_count': load_count,\n                'LOAD_byte': load_byte,\n                'HIT_count': load_count,\n                'HIT_byte': load_byte,\n                'STORE_count': self.last_level_store.EVICT_count,\n                'STORE_byte': self.last_level_store.EVICT_byte,\n                'EVICT_count': 0,\n                'EVICT_byte': 0,\n                'MISS_count': 0,\n                'MISS_byte': 0}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the list representation of this Key.", "response": "def list(self):\n    '''Returns the `list` representation of this Key.\n\n    Note that this method assumes the key is immutable.\n    '''\n    if not self._list:\n      self._list = map(Namespace, self._string.split('/'))\n    return self._list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef instance(self, other):\n    '''Returns an instance Key, by appending a name to the namespace.'''\n    assert '/' not in str(other)\n    return Key(str(self) + ':' + str(other))", "response": "Returns an instance Key by appending a name to the namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the parent Key of the current Key.", "response": "def parent(self):\n    '''Returns the parent Key (all namespaces except the last).\n\n        >>> Key('/Comedy/MontyPython/Actor:JohnCleese').parent\n        Key('/Comedy/MontyPython')\n\n    '''\n    if '/' in self._string:\n      return Key(self.list[:-1])\n    raise ValueError('%s is base key (it has no parent)' % repr(self))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isAncestorOf(self, other):\n    '''Returns whether this Key is an ancestor of `other`.\n\n        >>> john = Key('/Comedy/MontyPython/Actor:JohnCleese')\n        >>> Key('/Comedy').isAncestorOf(john)\n        True\n\n    '''\n    if isinstance(other, Key):\n      return other._string.startswith(self._string + '/')\n    raise TypeError('%s is not of type %s' % (other, Key))", "response": "Returns whether this Key is an ancestor of other."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn whether this Key is a descendant of other.", "response": "def isDescendantOf(self, other):\n    '''Returns whether this Key is a descendant of `other`.\n\n        >>> Key('/Comedy/MontyPython').isDescendantOf(Key('/Comedy'))\n        True\n\n    '''\n    if isinstance(other, Key):\n      return other.isAncestorOf(self)\n    raise TypeError('%s is not of type %s' % (other, Key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ensure_directory_exists(directory):\n  '''Ensures `directory` exists. May make `directory` and intermediate dirs.\n  Raises RuntimeError if `directory` is a file.\n  '''\n  if not os.path.exists(directory):\n    os.makedirs(directory)\n  elif os.path.isfile(directory):\n    raise RuntimeError('Path %s is a file, not a directory.' % directory)", "response": "Ensures directory exists. May make directory and intermediate dirs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef relative_path(self, key):\n    '''Returns the relative path for given `key`'''\n    key = str(key)                # stringify\n    key = key.replace(':', '/')   # turn namespace delimiters into slashes\n    key = key[1:]                 # remove first slash (absolute)\n    if not self.case_sensitive:\n      key = key.lower()           # coerce to lowercase\n    return os.path.normpath(key)", "response": "Returns the relative path for given key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef path(self, key):\n    '''Returns the `path` for given `key`'''\n    return os.path.join(self.root_path, self.relative_path(key))", "response": "Returns the path for given key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef object_path(self, key):\n    '''return the object path for `key`.'''\n    return os.path.join(self.root_path, self.relative_object_path(key))", "response": "return the object path for key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite out object to file at path", "response": "def _write_object(self, path, value):\n    '''write out `object` to file at `path`'''\n    ensure_directory_exists(os.path.dirname(path))\n\n    with open(path, 'w') as f:\n      f.write(value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_object(self, path):\n    '''read in object from file at `path`'''\n    if not os.path.exists(path):\n      return None\n\n    if os.path.isdir(path):\n      raise RuntimeError('%s is a directory, not a file.' % path)\n\n    with open(path) as f:\n      file_contents = f.read()\n\n    return file_contents", "response": "read in object from file at path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the object named by key or None if it does not exist.", "response": "def get(self, key):\n    '''Return the object named by key or None if it does not exist.\n\n    Args:\n      key: Key naming the object to retrieve\n\n    Returns:\n      object or None\n    '''\n    path = self.object_path(key)\n    return self._read_object(path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring the object value named by key.", "response": "def put(self, key, value):\n    '''Stores the object `value` named by `key`.\n\n    Args:\n      key: Key naming `value`\n      value: the object to store.\n    '''\n    path = self.object_path(key)\n    self._write_object(path, value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, key):\n    '''Removes the object named by `key`.\n\n    Args:\n      key: Key naming the object to remove.\n    '''\n    path = self.object_path(key)\n    if os.path.exists(path):\n      os.remove(path)", "response": "Removes the object named by key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(self, query):\n    '''Returns an iterable of objects matching criteria expressed in `query`\n    FSDatastore.query queries all the `.obj` files within the directory\n    specified by the query.key.\n\n    Args:\n      query: Query object describing the objects to return.\n\n    Raturns:\n      Cursor with all objects matching criteria\n    '''\n    path = self.path(query.key)\n\n    if os.path.exists(path):\n      filenames = os.listdir(path)\n      filenames = list(set(filenames) - set(self.ignore_list))\n      filenames = map(lambda f: os.path.join(path, f), filenames)\n      iterable = self._read_object_gen(filenames)\n    else:\n      iterable = list()\n\n    return query(iterable)", "response": "Queries all the. obj files within the directory specified by the query. key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning whether the object named by key exists.", "response": "def contains(self, key):\n    '''Returns whether the object named by `key` exists.\n    Optimized to only check whether the file object exists.\n\n    Args:\n      key: Key naming the object to check.\n\n    Returns:\n      boalean whether the object exists\n    '''\n    path = self.object_path(key)\n    return os.path.exists(path) and os.path.isfile(path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _collection(self, key):\n    '''Returns the namespace collection for `key`.'''\n    collection = str(key.path)\n    if not collection in self._items:\n      self._items[collection] = dict()\n    return self._items[collection]", "response": "Returns the namespace collection for key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put(self, key, value):\n    '''Stores the object `value` named by `key`.\n\n    Stores the object in the collection corresponding to ``key.path``.\n\n    Args:\n      key: Key naming `value`\n      value: the object to store.\n    '''\n    if value is None:\n      self.delete(key)\n    else:\n      self._collection(key)[key] = value", "response": "Stores the object value named by key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, key):\n    '''Removes the object named by `key`.\n\n    Removes the object from the collection corresponding to ``key.path``.\n\n    Args:\n      key: Key naming the object to remove.\n    '''\n    try:\n      del self._collection(key)[key]\n\n      if len(self._collection(key)) == 0:\n        del self._items[str(key.path)]\n    except KeyError, e:\n      pass", "response": "Removes the object named by key from the collection corresponding to key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef query(self, query):\n    '''Returns an iterable of objects matching criteria expressed in `query`\n\n    Naively applies the query operations on the objects within the namespaced\n    collection corresponding to ``query.key.path``.\n\n    Args:\n      query: Query object describing the objects to return.\n\n    Raturns:\n      iterable cursor with all objects matching criteria\n    '''\n\n    # entire dataset already in memory, so ok to apply query naively\n    if str(query.key) in self._items:\n      return query(self._items[str(query.key)].values())\n    else:\n      return query([])", "response": "Returns an iterable of objects matching criteria expressed in query"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, key):\n    '''Return the object in `service` named by `key` or None.\n\n    Args:\n      key: Key naming the object to retrieve.\n\n    Returns:\n      object or None\n    '''\n    key = self._service_key(key)\n    return self._service_ops['get'](key)", "response": "Returns the object in service named by key or None."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstores the object value named by key in service.", "response": "def put(self, key, value):\n    '''Stores the object `value` named by `key` in `service`.\n\n    Args:\n      key: Key naming `value`.\n      value: the object to store.\n    '''\n    key = self._service_key(key)\n    self._service_ops['put'](key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, key):\n    '''Removes the object named by `key` in `service`.\n\n    Args:\n      key: Key naming the object to remove.\n    '''\n    key = self._service_key(key)\n    self._service_ops['delete'](key)", "response": "Removes the object named by key in service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, key):\n    '''Return the object named by key or None if it does not exist.\n       CacheShimDatastore first checks its ``cache_datastore``.\n    '''\n    value = self.cache_datastore.get(key)\n    return value if value is not None else self.child_datastore.get(key)", "response": "Return the object named by key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstores the object value named by key self. Writes to both cache_datastore and child_datastore", "response": "def put(self, key, value):\n    '''Stores the object `value` named by `key`self.\n       Writes to both ``cache_datastore`` and ``child_datastore``.\n    '''\n    self.cache_datastore.put(key, value)\n    self.child_datastore.put(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, key):\n    '''Removes the object named by `key`.\n       Writes to both ``cache_datastore`` and ``child_datastore``.\n    '''\n    self.cache_datastore.delete(key)\n    self.child_datastore.delete(key)", "response": "Removes the object named by key. Another method to delete is to both cache_datastore and child_datastore."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contains(self, key):\n    '''Returns whether the object named by `key` exists.\n       First checks ``cache_datastore``.\n    '''\n    return self.cache_datastore.contains(key) \\\n        or self.child_datastore.contains(key)", "response": "Returns whether the object named by key exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, key):\n    '''Return the object named by key or None if it does not exist.\n       LoggingDatastore logs the access.\n    '''\n    self.logger.info('%s: get %s' % (self, key))\n    value = super(LoggingDatastore, self).get(key)\n    self.logger.debug('%s: %s' % (self, value))\n    return value", "response": "Return the object named by key or None if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, key):\n    '''Removes the object named by `key`.\n       LoggingDatastore logs the access.\n    '''\n    self.logger.info('%s: delete %s' % (self, key))\n    super(LoggingDatastore, self).delete(key)", "response": "Removes the object named by key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains(self, key):\n    '''Returns whether the object named by `key` exists.\n       LoggingDatastore logs the access.\n    '''\n    self.logger.info('%s: contains %s' % (self, key))\n    return super(LoggingDatastore, self).contains(key)", "response": "Returns whether the object named by key exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an iterable of objects matching criteria expressed in query.", "response": "def query(self, query):\n    '''Returns an iterable of objects matching criteria expressed in `query`.\n       LoggingDatastore logs the access.\n    '''\n    self.logger.info('%s: query %s' % (self, query))\n    return super(LoggingDatastore, self).query(query)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put(self, key, value):\n    '''Stores the object names by keytransform(key).'''\n    return self.child_datastore.put(self._transform(key), value)", "response": "Stores the object names by keytransform."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a sequence of objects matching criteria expressed in query", "response": "def query(self, query):\n    '''Returns a sequence of objects matching criteria expressed in `query`'''\n    query = query.copy()\n    query.key = self._transform(query.key)\n    return self.child_datastore.query(query)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nestKey(self, key):\n    '''Returns a nested `key`.'''\n\n    nest = self.nest_keyfn(key)\n\n    # if depth * length > len(key.name), we need to pad.\n    mult = 1 + int(self.nest_depth * self.nest_length / len(nest))\n    nest = nest * mult\n\n    pref = Key(self.nestedPath(nest, self.nest_depth, self.nest_length))\n    return pref.child(key)", "response": "Returns a nested key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nestedPath(path, depth, length):\n    '''returns a nested version of `basename`, using the starting characters.\n      For example:\n\n        >>> NestedPathDatastore.nested_path('abcdefghijk', 3, 2)\n        'ab/cd/ef'\n        >>> NestedPathDatastore.nested_path('abcdefghijk', 4, 2)\n        'ab/cd/ef/gh'\n        >>> NestedPathDatastore.nested_path('abcdefghijk', 3, 4)\n        'abcd/efgh/ijk'\n        >>> NestedPathDatastore.nested_path('abcdefghijk', 1, 4)\n        'abcd'\n        >>> NestedPathDatastore.nested_path('abcdefghijk', 3, 10)\n        'abcdefghij/k'\n    '''\n    components = [path[n:n+length] for n in xrange(0, len(path), length)]\n    components = components[:depth]\n    return '/'.join(components)", "response": "returns a nested version of basename using the starting characters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the linked key if value is a link or None.", "response": "def _link_for_value(self, value):\n    '''Returns the linked key if `value` is a link, or None.'''\n    try:\n      key = Key(value)\n      if key.name == self.sentinel:\n        return key.parent\n    except:\n      pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns given value or if it is a symlink the value it names.", "response": "def _follow_link(self, value):\n    '''Returns given `value` or, if it is a symlink, the `value` it names.'''\n    seen_keys = set()\n    while True:\n      link_key = self._link_for_value(value)\n      if not link_key:\n        return value\n\n      assert link_key not in seen_keys, 'circular symlink reference'\n      seen_keys.add(link_key)\n      value = super(SymlinkDatastore, self).get(link_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef link(self, source_key, target_key):\n    '''Creates a symbolic link key pointing from `target_key` to `source_key`'''\n    link_value = self._link_value_for_key(source_key)\n\n    # put straight into the child, to avoid following previous links.\n    self.child_datastore.put(target_key, link_value)\n\n    # exercise the link. ensure there are no cycles.\n    self.get(target_key)", "response": "Creates a symbolic link key pointing from source_key to target_key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the object named by key. Follows links.", "response": "def get(self, key):\n    '''Return the object named by `key. Follows links.'''\n    value = super(SymlinkDatastore, self).get(key)\n    return self._follow_link(value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring the object named by key. Follows links.", "response": "def put(self, key, value):\n    '''Stores the object named by `key`. Follows links.'''\n    # if value is a link, don't follow links\n    if self._link_for_value(value):\n      super(SymlinkDatastore, self).put(key, value)\n      return\n\n    # if `key` points to a symlink, need to follow it.\n    current_value = super(SymlinkDatastore, self).get(key)\n    link_key = self._link_for_value(current_value)\n    if link_key:\n      self.put(link_key, value) # self.put: could be another link.\n    else:\n      super(SymlinkDatastore, self).put(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self, query):\n    '''Returns objects matching criteria expressed in `query`. Follows links.'''\n    results = super(SymlinkDatastore, self).query(query)\n    return self._follow_link_gen(results)", "response": "Returns objects matching criteria expressed in query. Follows links."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef directory(self, dir_key):\n    '''Initializes directory at dir_key.'''\n    dir_items = self.get(dir_key)\n    if not isinstance(dir_items, list):\n      self.put(dir_key, [])", "response": "Initializes directory at dir_key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a directory entry key to the directory at dir_key.", "response": "def directoryAdd(self, dir_key, key):\n    '''Adds directory entry `key` to directory at `dir_key`.\n\n    If the directory `dir_key` does not exist, it is created.\n    '''\n    key = str(key)\n\n    dir_items = self.get(dir_key) or []\n    if key not in dir_items:\n      dir_items.append(key)\n      self.put(dir_key, dir_items)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef directoryRemove(self, dir_key, key):\n    '''Removes directory entry `key` from directory at `dir_key`.\n\n    If either the directory `dir_key` or the directory entry `key` don't exist,\n    this method is a no-op.\n    '''\n    key = str(key)\n\n    dir_items = self.get(dir_key) or []\n    if key in dir_items:\n      dir_items = [k for k in dir_items if k != key]\n      self.put(dir_key, dir_items)", "response": "Removes a directory entry from the directory at dir_key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstores the object value named by key self. DirectoryTreeDatastore stores a directory entry.", "response": "def put(self, key, value):\n    '''Stores the object `value` named by `key`self.\n       DirectoryTreeDatastore stores a directory entry.\n    '''\n    super(DirectoryTreeDatastore, self).put(key, value)\n\n    str_key = str(key)\n\n    # ignore root\n    if str_key == '/':\n      return\n\n    # retrieve directory, to add entry\n    dir_key = key.parent.instance('directory')\n    directory = self.directory(dir_key)\n\n    # ensure key is in directory\n    if str_key not in directory:\n      directory.append(str_key)\n      super(DirectoryTreeDatastore, self).put(dir_key, directory)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the object named by key. DirectoryTreeDatastore removes the directory entry.", "response": "def delete(self, key):\n    '''Removes the object named by `key`.\n       DirectoryTreeDatastore removes the directory entry.\n    '''\n    super(DirectoryTreeDatastore, self).delete(key)\n\n    str_key = str(key)\n\n    # ignore root\n    if str_key == '/':\n      return\n\n    # retrieve directory, to remove entry\n    dir_key = key.parent.instance('directory')\n    directory = self.directory(dir_key)\n\n    # ensure key is not in directory\n    if directory and str_key in directory:\n      directory.remove(str_key)\n      if len(directory) > 0:\n        super(DirectoryTreeDatastore, self).put(dir_key, directory)\n      else:\n        super(DirectoryTreeDatastore, self).delete(dir_key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves directory entries for given key.", "response": "def directory(self, key):\n    '''Retrieves directory entries for given key.'''\n    if key.name != 'directory':\n      key = key.instance('directory')\n    return self.get(key) or []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving directory values for given key.", "response": "def directory_values_generator(self, key):\n    '''Retrieve directory values for given key.'''\n    directory = self.directory(key)\n    for key in directory:\n      yield self.get(Key(key))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending datastore store to this collection.", "response": "def appendDatastore(self, store):\n    '''Appends datastore `store` to this collection.'''\n    if not isinstance(store, Datastore):\n      raise TypeError(\"stores must be of type %s\" % Datastore)\n\n    self._stores.append(store)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insertDatastore(self, index, store):\n    '''Inserts datastore `store` into this collection at `index`.'''\n    if not isinstance(store, Datastore):\n      raise TypeError(\"stores must be of type %s\" % Datastore)\n\n    self._stores.insert(index, store)", "response": "Inserts datastore store into this collection at index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, key):\n    '''Return the object named by key. Checks each datastore in order.'''\n    value = None\n    for store in self._stores:\n      value = store.get(key)\n      if value is not None:\n        break\n\n    # add model to lower stores only\n    if value is not None:\n      for store2 in self._stores:\n        if store == store2:\n          break\n        store2.put(key, value)\n\n    return value", "response": "Return the object named by key. Checks each datastore in order."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put(self, key, value):\n    '''Stores the object in all underlying datastores.'''\n    for store in self._stores:\n      store.put(key, value)", "response": "Stores the object in all underlying datastores."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn whether the object is in this datastore.", "response": "def contains(self, key):\n    '''Returns whether the object is in this datastore.'''\n    for store in self._stores:\n      if store.contains(key):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore the object to the corresponding datastore.", "response": "def put(self, key, value):\n    '''Stores the object to the corresponding datastore.'''\n    self.shardDatastore(key).put(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a sequence of objects matching criteria expressed in query", "response": "def query(self, query):\n    '''Returns a sequence of objects matching criteria expressed in `query`'''\n    cursor = Cursor(query, self.shard_query_generator(query))\n    cursor.apply_order()  # ordering sharded queries is expensive (no generator)\n    return cursor"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef monkey_patch_bson(bson=None):\n  '''Patch bson in pymongo to use loads and dumps interface.'''\n  if not bson:\n    import bson\n\n  if not hasattr(bson, 'loads'):\n    bson.loads = lambda bsondoc: bson.BSON(bsondoc).decode()\n\n  if not hasattr(bson, 'dumps'):\n    bson.dumps = lambda document: bson.BSON.encode(document)", "response": "Patch bson in pymongo to use loads and dumps interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(cls, value):\n    '''Returns mapping type deserialized `value`.'''\n    if len(value) == 1 and cls.sentinel in value:\n      value = value[cls.sentinel]\n    return value", "response": "Returns mapping type deserialized value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn mapping typed serialized value.", "response": "def dumps(cls, value):\n    '''returns mapping typed serialized `value`.'''\n    if not hasattr(value, '__getitem__') or not hasattr(value, 'iteritems'):\n      value = {cls.sentinel: value}\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, key):\n    '''Return the object named by key or None if it does not exist.\n    Retrieves the value from the ``child_datastore``, and de-serializes\n    it on the way out.\n\n    Args:\n      key: Key naming the object to retrieve\n\n    Returns:\n      object or None\n    '''\n\n    ''''''\n    value = self.child_datastore.get(key)\n    return self.deserializedValue(value)", "response": "Retrieves the object named by key from the child_datastore and de - serializes it on the way out."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef put(self, key, value):\n    '''Stores the object `value` named by `key`.\n    Serializes values on the way in, and stores the serialized data into the\n    ``child_datastore``.\n\n    Args:\n      key: Key naming `value`\n      value: the object to store.\n    '''\n\n    value = self.serializedValue(value)\n    self.child_datastore.put(key, value)", "response": "Stores the object value named by key. Serializes values on the way in and stores the serialized data into the child_datastore."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query(self, query):\n    '''Returns an iterable of objects matching criteria expressed in `query`\n    De-serializes values on the way out, using a :ref:`deserialized_gen` to\n    avoid incurring the cost of de-serializing all data at once, or ever, if\n    iteration over results does not finish (subject to order generator\n    constraint).\n\n    Args:\n      query: Query object describing the objects to return.\n\n    Raturns:\n      iterable cursor with all objects matching criteria\n    '''\n\n    # run the query on the child datastore\n    cursor = self.child_datastore.query(query)\n\n    # chain the deserializing generator to the cursor's result set iterable\n    cursor._iterable = deserialized_gen(self.serializer, cursor._iterable)\n\n    return cursor", "response": "Returns an iterable of objects matching criteria expressed in query"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattributing getter for the objects to operate on.", "response": "def _object_getattr(obj, field):\n  '''Attribute getter for the objects to operate on.\n\n  This function can be overridden in classes or instances of Query, Filter, and\n  Order. Thus, a custom function to extract values to attributes can be\n  specified, and the system can remain agnostic to the client's data model,\n  without loosing query power.\n\n  For example, the default implementation works with attributes and items::\n\n    def _object_getattr(obj, field):\n      # check whether this key is an attribute\n      if hasattr(obj, field):\n        value = getattr(obj, field)\n\n      # if not, perhaps it is an item (raw dicts, etc)\n      elif field in obj:\n        value = obj[field]\n\n      # return whatever we've got.\n      return value\n\n  Or consider a more complex, application-specific structure::\n\n    def _object_getattr(version, field):\n\n      if field in ['key', 'committed', 'created', 'hash']:\n        return getattr(version, field)\n\n      else:\n        return version.attributes[field]['value']\n\n\n  '''\n\n  # TODO: consider changing this to raise an exception if no value is found.\n  value = None\n\n  # check whether this key is an attribute\n  if hasattr(obj, field):\n    value = getattr(obj, field)\n\n  # if not, perhaps it is an item (raw dicts, etc)\n  elif field in obj:\n    value = obj[field]\n\n  # return whatever we've got.\n  return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef valuePasses(self, value):\n    '''Returns whether this value passes this filter'''\n    return self._conditional_cmp[self.op](value, self.value)", "response": "Returns whether this value passes this filter"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the elements in iterable that pass given filters", "response": "def filter(cls, filters, iterable):\n    '''Returns the elements in `iterable` that pass given `filters`'''\n    if isinstance(filters, Filter):\n      filters = [filters]\n\n    for filter in filters:\n      iterable = filter.generator(iterable)\n\n    return iterable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function that will compare two items according to orders", "response": "def multipleOrderComparison(cls, orders):\n    '''Returns a function that will compare two items according to `orders`'''\n    comparers = [ (o.keyfn, 1 if o.isAscending() else -1) for o in orders]\n\n    def cmpfn(a, b):\n      for keyfn, ascOrDesc in comparers:\n        comparison = cmp(keyfn(a), keyfn(b)) * ascOrDesc\n        if comparison is not 0:\n          return comparison\n      return 0\n\n    return cmpfn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sorted(cls, items, orders):\n    '''Returns the elements in `items` sorted according to `orders`'''\n    return sorted(items, cmp=cls.multipleOrderComparison(orders))", "response": "Returns the elements in items sorted according to orders."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef order(self, order):\n    '''Adds an Order to this query.\n\n    Args:\n      see :py:class:`Order <datastore.query.Order>` constructor\n\n    Returns self for JS-like method chaining::\n\n      query.order('+age').order('-home')\n\n    '''\n    order = order if isinstance(order, Order) else Order(order)\n\n    # ensure order gets attr values the same way the rest of the query does.\n    order.object_getattr = self.object_getattr\n    self.orders.append(order)\n    return self", "response": "Adds an Order to this query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a Filter to this query.", "response": "def filter(self, *args):\n    '''Adds a Filter to this query.\n\n    Args:\n      see :py:class:`Filter <datastore.query.Filter>` constructor\n\n    Returns self for JS-like method chaining::\n\n      query.filter('age', '>', 18).filter('sex', '=', 'Female')\n\n    '''\n    if len(args) == 1 and isinstance(args[0], Filter):\n      filter = args[0]\n    else:\n      filter = Filter(*args)\n\n    # ensure filter gets attr values the same way the rest of the query does.\n    filter.object_getattr = self.object_getattr\n    self.filters.append(filter)\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a copy of this query.", "response": "def copy(self):\n    '''Returns a copy of this query.'''\n    if self.object_getattr is Query.object_getattr:\n      other = Query(self.key)\n    else:\n      other = Query(self.key, object_getattr=self.object_getattr)\n    other.limit = self.limit\n    other.offset = self.offset\n    other.offset_key = self.offset_key\n    other.filters = self.filters\n    other.orders = self.orders\n    return other"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary representing this query.", "response": "def dict(self):\n    '''Returns a dictionary representing this query.'''\n    d = dict()\n    d['key'] = str(self.key)\n\n    if self.limit is not None:\n      d['limit'] = self.limit\n    if self.offset > 0:\n      d['offset'] = self.offset\n    if self.offset_key:\n      d['offset_key'] = str(self.offset_key)\n    if len(self.filters) > 0:\n      d['filter'] = [[f.field, f.op, f.value] for f in self.filters]\n    if len(self.orders) > 0:\n      d['order'] = [str(o) for o in self.orders]\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_dict(cls, dictionary):\n    '''Constructs a query from a dictionary.'''\n    query = cls(Key(dictionary['key']))\n\n    for key, value in dictionary.items():\n\n      if key == 'order':\n        for order in value:\n          query.order(order)\n\n      elif key == 'filter':\n        for filter in value:\n          if not isinstance(filter, Filter):\n            filter = Filter(*filter)\n          query.filter(filter)\n\n      elif key in ['limit', 'offset', 'offset_key']:\n        setattr(query, key, value)\n    return query", "response": "Constructs a query from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next(self):\n    '''Iterator next. Build up count of returned elements during iteration.'''\n\n    # if iteration has not begun, begin it.\n    if not self._iterator:\n      self.__iter__()\n\n    next = self._iterator.next()\n    if next is not StopIteration:\n      self._returned_inc(next)\n    return next", "response": "Iterator next. Build up count of returned elements during iteration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_filter(self):\n    '''Naively apply query filters.'''\n    self._ensure_modification_is_safe()\n\n    if len(self.query.filters) > 0:\n      self._iterable = Filter.filter(self.query.filters, self._iterable)", "response": "Naively apply query filters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_limit(self):\n    '''Naively apply query limit.'''\n    self._ensure_modification_is_safe()\n\n    if self.query.limit is not None:\n      self._iterable = limit_gen(self.query.limit, self._iterable)", "response": "Naively apply query limit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a transaction with retries.", "response": "def run_transaction(transactor, callback):\n    \"\"\"Run a transaction with retries.\n\n    ``callback()`` will be called with one argument to execute the\n    transaction. ``callback`` may be called more than once; it should have\n    no side effects other than writes to the database on the given\n    connection. ``callback`` should not call ``commit()` or ``rollback()``;\n    these will be called automatically.\n\n    The ``transactor`` argument may be one of the following types:\n    * `sqlalchemy.engine.Connection`: the same connection is passed to the callback.\n    * `sqlalchemy.engine.Engine`: a connection is created and passed to the callback.\n    * `sqlalchemy.orm.sessionmaker`: a session is created and passed to the callback.\n    \"\"\"\n    if isinstance(transactor, sqlalchemy.engine.Connection):\n        return _txn_retry_loop(transactor, callback)\n    elif isinstance(transactor, sqlalchemy.engine.Engine):\n        with transactor.connect() as connection:\n            return _txn_retry_loop(connection, callback)\n    elif isinstance(transactor, sqlalchemy.orm.sessionmaker):\n        session = transactor(autocommit=True)\n        return _txn_retry_loop(session, callback)\n    else:\n        raise TypeError(\"don't know how to run a transaction on %s\", type(transactor))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get(self, pos):\n        res = None, None\n        if pos is not None:\n            try:\n                res = self[pos], pos\n            except (IndexError, KeyError):\n                pass\n        return res", "response": "loads widget at given position"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _next_of_kin(self, pos):\n        candidate = None\n        parent = self.parent_position(pos)\n        if parent is not None:\n            candidate = self.next_sibling_position(parent)\n            if candidate is None:\n                candidate = self._next_of_kin(parent)\n        return candidate", "response": "Searches up the next sibling of the closest ancestor with not - None next\n            siblings."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove in the tree in given direction and return the last position.", "response": "def _last_in_direction(starting_pos, direction):\n        \"\"\"\n        move in the tree in given direction and return the last position.\n\n        :param starting_pos: position to start at\n        :param direction: callable that transforms a position into a position.\n        \"\"\"\n        cur_pos = None\n        next_pos = starting_pos\n        while next_pos is not None:\n            cur_pos = next_pos\n            next_pos = direction(cur_pos)\n        return cur_pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef depth(self, pos):\n        parent = self.parent_position(pos)\n        if parent is None:\n            return 0\n        else:\n            return self.depth(parent) + 1", "response": "determine depth of node at pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the next position in depth - first order", "response": "def next_position(self, pos):\n        \"\"\"returns the next position in depth-first order\"\"\"\n        candidate = None\n        if pos is not None:\n            candidate = self.first_child_position(pos)\n            if candidate is None:\n                candidate = self.next_sibling_position(pos)\n                if candidate is None:\n                    candidate = self._next_of_kin(pos)\n        return candidate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prev_position(self, pos):\n        candidate = None\n        if pos is not None:\n            prevsib = self.prev_sibling_position(pos)  # is None if first\n            if prevsib is not None:\n                candidate = self.last_decendant(prevsib)\n            else:\n                parent = self.parent_position(pos)\n                if parent is not None:\n                    candidate = parent\n        return candidate", "response": "returns the previous position in depth - first order"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a generator that walks the positions of this tree in DFO", "response": "def positions(self, reverse=False):\n        \"\"\"returns a generator that walks the positions of this tree in DFO\"\"\"\n        def Posgen(reverse):\n            if reverse:\n                lastrootsib = self.last_sibling_position(self.root)\n                current = self.last_decendant(lastrootsib)\n                while current is not None:\n                    yield current\n                    current = self.prev_position(current)\n            else:\n                current = self.root\n                while current is not None:\n                    yield current\n                    current = self.next_position(current)\n        return Posgen(reverse)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_substructure(self, treelist, pos):\n        subtree = None\n        if len(pos) > 1:\n            subtree = self._get_substructure(treelist[pos[0]][1], pos[1:])\n        else:\n            try:\n                subtree = treelist[pos[0]]\n            except (IndexError, TypeError):\n                pass\n        return subtree", "response": "recursive helper to look up node - tuple for pos in treelist"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_node(self, treelist, pos):\n        node = None\n        if pos is not None:\n            subtree = self._get_substructure(treelist, pos)\n            if subtree is not None:\n                node = subtree[0]\n        return node", "response": "get node from treelist at pos"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _confirm_pos(self, pos):\n        candidate = None\n        if self._get_node(self._treelist, pos) is not None:\n            candidate = pos\n        return candidate", "response": "look up widget for pos and default to None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning absolute paths for all entries in a directory", "response": "def _list_dir(self, path):\n        \"\"\"returns absolute paths for all entries in a directory\"\"\"\n        try:\n            elements = [\n                os.path.join(path, x) for x in os.listdir(path)\n            ] if os.path.isdir(path) else []\n            elements.sort()\n        except OSError:\n            elements = None\n        return elements"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_siblings(self, pos):\n        parent = self.parent_position(pos)\n        siblings = [pos]\n        if parent is not None:\n            siblings = self._list_dir(parent)\n        return siblings", "response": "lists the parent directory of pos"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reorder_view(self, request):\n        model = self.model\n\n        if not self.has_change_permission(request):\n            raise PermissionDenied\n\n        if request.method == \"POST\":\n            object_pks = request.POST.getlist('neworder[]')\n            model.objects.set_orders(object_pks)\n\n        return HttpResponse(\"OK\")", "response": "The reorder admin view for this model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if given position is currently collapsed", "response": "def is_collapsed(self, pos):\n        \"\"\"checks if given position is currently collapsed\"\"\"\n        collapsed = self._initially_collapsed(pos)\n        if pos in self._divergent_positions:\n            collapsed = not collapsed\n        return collapsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decorate(self, pos, widget, is_first=True):\n        void = urwid.SolidFill(' ')\n        line = None\n        cols = []\n        depth = self._tree.depth(pos)\n\n        # add spacer filling all but the last indent\n        if depth > 0:\n            cols.append((depth * self._indent, void)),  # spacer\n\n        # construct last indent\n        # TODO\n        iwidth, icon = self._construct_collapse_icon(pos)\n        available_space = self._indent\n        firstindent_width = self._icon_offset + iwidth\n\n        # stop if indent is too small for this decoration\n        if firstindent_width > available_space:\n            raise NoSpaceError()\n\n        # add icon only for non-leafs\n        is_leaf = self._tree.is_leaf(pos)\n        if not is_leaf:\n            if icon is not None:\n                # space to the left\n                cols.append((available_space - firstindent_width,\n                             urwid.SolidFill(' ')))\n                # icon\n                icon_pile = urwid.Pile([('pack', icon), void])\n                cols.append((iwidth, icon_pile))\n                # spacer until original widget\n                available_space = self._icon_offset\n            cols.append((available_space, urwid.SolidFill(' ')))\n        else:  # otherwise just add another spacer\n            cols.append((self._indent, urwid.SolidFill(' ')))\n\n        cols.append(widget)  # original widget ]\n        # construct a Columns, defining all spacer as Box widgets\n        line = urwid.Columns(cols, box_columns=range(len(cols))[:-1])\n\n        return line", "response": "Builds a list element for given position in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a spacer that occupies the horizontally indented space between pos's parent and the root node. It will return a list of tuples to be fed into a Columns widget.", "response": "def _construct_spacer(self, pos, acc):\n        \"\"\"\n        build a spacer that occupies the horizontally indented space between\n        pos's parent and the root node. It will return a list of tuples to be\n        fed into a Columns widget.\n        \"\"\"\n        parent = self._tree.parent_position(pos)\n        if parent is not None:\n            grandparent = self._tree.parent_position(parent)\n            if self._indent > 0 and grandparent is not None:\n                parent_sib = self._tree.next_sibling_position(parent)\n                draw_vbar = parent_sib is not None and \\\n                    self._arrow_vbar_char is not None\n                space_width = self._indent - 1 * (draw_vbar) - self._childbar_offset\n                if space_width > 0:\n                    void = urwid.AttrMap(urwid.SolidFill(' '), self._arrow_att)\n                    acc.insert(0, ((space_width, void)))\n                if draw_vbar:\n                    barw = urwid.SolidFill(self._arrow_vbar_char)\n                    bar = urwid.AttrMap(barw, self._arrow_vbar_att or\n                                        self._arrow_att)\n                    acc.insert(0, ((1, bar)))\n            return self._construct_spacer(parent, acc)\n        else:\n            return acc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct the connector widget for the current position", "response": "def _construct_connector(self, pos):\n        \"\"\"\n        build widget to be used as \"connector\" bit between the vertical bar\n        between siblings and their respective horizontal bars leading to the\n        arrow tip\n        \"\"\"\n        # connector symbol, either L or |- shaped.\n        connectorw = None\n        connector = None\n        if self._tree.next_sibling_position(pos) is not None:  # |- shaped\n            if self._arrow_connector_tchar is not None:\n                connectorw = urwid.Text(self._arrow_connector_tchar)\n        else:  # L shaped\n            if self._arrow_connector_lchar is not None:\n                connectorw = urwid.Text(self._arrow_connector_lchar)\n        if connectorw is not None:\n            att = self._arrow_connector_att or self._arrow_att\n            connector = urwid.AttrMap(connectorw, att)\n        return connector"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct arrow tip as ( width widget", "response": "def _construct_arrow_tip(self, pos):\n        \"\"\"returns arrow tip as (width, widget)\"\"\"\n        arrow_tip = None\n        width = 0\n        if self._arrow_tip_char:\n            txt = urwid.Text(self._arrow_tip_char)\n            arrow_tip = urwid.AttrMap(\n                txt, self._arrow_tip_att or self._arrow_att)\n            width = len(self._arrow_tip_char)\n        return width, arrow_tip"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _construct_first_indent(self, pos):\n        cols = []\n        void = urwid.AttrMap(urwid.SolidFill(' '), self._arrow_att)\n        available_width = self._indent\n\n        if self._tree.depth(pos) > 0:\n            connector = self._construct_connector(pos)\n            if connector is not None:\n                width = connector.pack()[0]\n                if width > available_width:\n                    raise NoSpaceError()\n                available_width -= width\n                if self._tree.next_sibling_position(pos) is not None:\n                    barw = urwid.SolidFill(self._arrow_vbar_char)\n                    below = urwid.AttrMap(barw, self._arrow_vbar_att or\n                                          self._arrow_att)\n                else:\n                    below = void\n                # pile up connector and bar\n                spacer = urwid.Pile([('pack', connector), below])\n                cols.append((width, spacer))\n\n            #arrow tip\n            awidth, at = self._construct_arrow_tip(pos)\n            if at is not None:\n                if awidth > available_width:\n                    raise NoSpaceError()\n                available_width -= awidth\n                at_spacer = urwid.Pile([('pack', at), void])\n                cols.append((awidth, at_spacer))\n\n            # bar between connector and arrow tip\n            if available_width > 0:\n                barw = urwid.SolidFill(self._arrow_hbar_char)\n                bar = urwid.AttrMap(\n                    barw, self._arrow_hbar_att or self._arrow_att)\n                hb_spacer = urwid.Pile([(1, bar), void])\n                cols.insert(1, (available_width, hb_spacer))\n        return cols", "response": "constructs a list of columns to occupy the first indentation level from pos to the last indentation level. This is separate as it adds arrowtip and sibling connector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decorate(self, pos, widget, is_first=True):\n        line = None\n        if pos is not None:\n            original_widget = widget\n            cols = self._construct_spacer(pos, [])\n\n            # Construct arrow leading from parent here,\n            # if we have a parent and indentation is turned on\n            if self._indent > 0:\n                if is_first:\n                    indent = self._construct_first_indent(pos)\n                    if indent is not None:\n                        cols = cols + indent\n                else:\n                    parent = self._tree.parent_position(pos)\n                    if self._indent > 0 and parent is not None:\n                        parent_sib = self._tree.next_sibling_position(pos)\n                        draw_vbar = parent_sib is not None\n                        void = urwid.AttrMap(urwid.SolidFill(' '),\n                                             self._arrow_att)\n                        if self._childbar_offset > 0:\n                            cols.append((self._childbar_offset, void))\n                        if draw_vbar:\n                            barw = urwid.SolidFill(self._arrow_vbar_char)\n                            bar = urwid.AttrMap(\n                                barw, self._arrow_vbar_att or self._arrow_att)\n                            rspace_width = self._indent - \\\n                                1 - self._childbar_offset\n                            cols.append((1, bar))\n                            cols.append((rspace_width, void))\n                        else:\n                            cols.append((self._indent, void))\n\n            # add the original widget for this line\n            cols.append(original_widget)\n            # construct a Columns, defining all spacer as Box widgets\n            line = urwid.Columns(cols, box_columns=range(len(cols))[:-1])\n        return line", "response": "Creates a list element for given position in the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collapse_focussed(self):\n        if implementsCollapseAPI(self._tree):\n            w, focuspos = self.get_focus()\n            self._tree.collapse(focuspos)\n            self._walker.clear_cache()\n            self.refresh()", "response": "Collapse currently focussed position ; works only if the underlying\n        tree allows it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand currently focussed position ; works only if the underlying tree allows it.", "response": "def expand_focussed(self):\n        \"\"\"\n        Expand currently focussed position; works only if the underlying\n        tree allows it.\n        \"\"\"\n        if implementsCollapseAPI(self._tree):\n            w, focuspos = self.get_focus()\n            self._tree.expand(focuspos)\n            self._walker.clear_cache()\n            self.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collapse_all(self):\n        if implementsCollapseAPI(self._tree):\n            self._tree.collapse_all()\n            self.set_focus(self._tree.root)\n            self._walker.clear_cache()\n            self.refresh()", "response": "Collapse all positions ; works only if the underlying tree allows it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexpand all positions in the tree.", "response": "def expand_all(self):\n        \"\"\"\n        Expand all positions; works only if the underlying tree allows it.\n        \"\"\"\n        if implementsCollapseAPI(self._tree):\n            self._tree.expand_all()\n            self._walker.clear_cache()\n            self.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmoves focus to parent node of currently focussed one", "response": "def focus_parent(self):\n        \"\"\"move focus to parent node of currently focussed one\"\"\"\n        w, focuspos = self.get_focus()\n        parent = self._tree.parent_position(focuspos)\n        if parent is not None:\n            self.set_focus(parent)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove focus to first child of currently focussed one", "response": "def focus_first_child(self):\n        \"\"\"move focus to first child of currently focussed one\"\"\"\n        w, focuspos = self.get_focus()\n        child = self._tree.first_child_position(focuspos)\n        if child is not None:\n            self.set_focus(child)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove focus to last child of currently focussed one", "response": "def focus_last_child(self):\n        \"\"\"move focus to last child of currently focussed one\"\"\"\n        w, focuspos = self.get_focus()\n        child = self._tree.last_child_position(focuspos)\n        if child is not None:\n            self.set_focus(child)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove focus to next sibling of currently focussed one", "response": "def focus_next_sibling(self):\n        \"\"\"move focus to next sibling of currently focussed one\"\"\"\n        w, focuspos = self.get_focus()\n        sib = self._tree.next_sibling_position(focuspos)\n        if sib is not None:\n            self.set_focus(sib)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmove focus to previous sibling of currently focussed one", "response": "def focus_prev_sibling(self):\n        \"\"\"move focus to previous sibling of currently focussed one\"\"\"\n        w, focuspos = self.get_focus()\n        sib = self._tree.prev_sibling_position(focuspos)\n        if sib is not None:\n            self.set_focus(sib)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef focus_next(self):\n        w, focuspos = self.get_focus()\n        next = self._tree.next_position(focuspos)\n        if next is not None:\n            self.set_focus(next)", "response": "move focus to next position ( DFO"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef focus_prev(self):\n        w, focuspos = self.get_focus()\n        prev = self._tree.prev_position(focuspos)\n        if prev is not None:\n            self.set_focus(prev)", "response": "move focus to previous position ( DFO"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting field names that are unique_together with sort_order.", "response": "def get_unique_fields(self):\n        \"\"\"List field names that are unique_together with `sort_order`.\"\"\"\n        for unique_together in self._meta.unique_together:\n            if 'sort_order' in unique_together:\n                unique_fields = list(unique_together)\n                unique_fields.remove('sort_order')\n                return ['%s_id' % f for f in unique_fields]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_sort_order_unique_together_with_something(self):\n        unique_together = self._meta.unique_together\n        for fields in unique_together:\n            if 'sort_order' in fields and len(fields) > 1:\n                return True\n        return False", "response": "Is the sort_order field unique_together with something?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the sort_order of the objects in a queryset.", "response": "def _update(qs):\n        \"\"\"\n        Increment the sort_order in a queryset.\n\n        Handle IntegrityErrors caused by unique constraints.\n        \"\"\"\n        try:\n            with transaction.atomic():\n                qs.update(sort_order=models.F('sort_order') + 1)\n        except IntegrityError:\n            for obj in qs.order_by('-sort_order'):\n                qs.filter(pk=obj.pk).update(sort_order=models.F('sort_order') + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the order of the objects in the order specified by old_pos and new_pos.", "response": "def _save(self, objects, old_pos, new_pos):\n        \"\"\"WARNING: Intensive giggery-pokery zone.\"\"\"\n        to_shift = objects.exclude(pk=self.pk) if self.pk else objects\n\n        # If not set, insert at end.\n        if self.sort_order is None:\n            self._move_to_end(objects)\n\n        # New insert.\n        elif not self.pk and not old_pos:\n            # Increment `sort_order` on objects with:\n            #     sort_order > new_pos.\n            to_shift = to_shift.filter(sort_order__gte=self.sort_order)\n            self._update(to_shift)\n            self.sort_order = new_pos\n\n        # self.sort_order decreased.\n        elif old_pos and new_pos < old_pos:\n            self._move_to_end(objects)\n            super(Orderable, self).save()\n            # Increment `sort_order` on objects with:\n            #     sort_order >= new_pos and sort_order < old_pos\n            to_shift = to_shift.filter(sort_order__gte=new_pos, sort_order__lt=old_pos)\n            self._update(to_shift)\n            self.sort_order = new_pos\n\n        # self.sort_order increased.\n        elif old_pos and new_pos > old_pos:\n            self._move_to_end(objects)\n            super(Orderable, self).save()\n            # Decrement sort_order on objects with:\n            #     sort_order <= new_pos and sort_order > old_pos.\n            to_shift = to_shift.filter(sort_order__lte=new_pos, sort_order__gt=old_pos)\n            to_shift.update(sort_order=models.F('sort_order') - 1)\n            self.sort_order = new_pos"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nkeep the unique order in sync.", "response": "def save(self, *args, **kwargs):\n        \"\"\"Keep the unique order in sync.\"\"\"\n        objects = self.get_filtered_manager()\n        old_pos = getattr(self, '_original_sort_order', None)\n        new_pos = self.sort_order\n\n        if old_pos is None and self._unique_togethers_changed():\n            self.sort_order = None\n            new_pos = None\n\n        try:\n            with transaction.atomic():\n                self._save(objects, old_pos, new_pos)\n        except IntegrityError:\n            with transaction.atomic():\n                old_pos = objects.filter(pk=self.pk).values_list(\n                    'sort_order', flat=True)[0]\n                self._save(objects, old_pos, new_pos)\n\n        # Call the \"real\" save() method.\n        super(Orderable, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sanitize_position(self, pos, tree=None):\n        if pos is not None:\n            tree = tree or self._tree\n            entry = self._lookup_entry(tree, pos)\n            if isinstance(entry, Tree):\n                pos = pos + self._sanitize_position((entry.root,), tree=entry)\n        return pos", "response": "Ensure a position tuple until the result does not contain a tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_subtree_for(self, pos):\n        res = self._tree\n        candidate = self._lookup_entry(self._tree, pos[:-1])\n        if isinstance(candidate, Tree):\n            res = candidate\n        return res", "response": "returns Tree that manages pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_orders(self, object_pks):\n        objects_to_sort = self.filter(pk__in=object_pks)\n        max_value = self.model.objects.all().aggregate(\n            models.Max('sort_order')\n        )['sort_order__max']\n\n        # Call list() on the values right away, so they don't get affected by the\n        # update() later (since values_list() is lazy).\n        orders = list(objects_to_sort.values_list('sort_order', flat=True))\n\n        # Check there are no unrecognised entries in the object_pks list. If so,\n        # throw an error. We only have to check that they're the same length because\n        # orders is built using only entries in object_pks, and all the pks are unique,\n        # so if their lengths are the same, the elements must match up exactly.\n        if len(orders) != len(object_pks):\n            pks = set(objects_to_sort.values_list('pk', flat=True))\n            message = 'The following object_pks are not in this queryset: {}'.format(\n                [pk for pk in object_pks if pk not in pks]\n            )\n            raise TypeError(message)\n\n        with transaction.atomic():\n            objects_to_sort.update(sort_order=models.F('sort_order') + max_value)\n            for pk, order in zip(object_pks, orders):\n                # Use update() to save a query per item and dodge the insertion sort\n                # code in save().\n                self.filter(pk=pk).update(sort_order=order)\n\n        # Return the operated-on queryset for convenience.\n        return objects_to_sort", "response": "Sets the sort orders for the related objects in the base queryset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new QuerySet with the values of the specified json_path in the specified language_code.", "response": "def order_by_json_path(self, json_path, language_code=None, order='asc'):\n        \"\"\"\n        Orders a queryset by the value of the specified `json_path`.\n\n        More about the `#>>` operator and the `json_path` arg syntax:\n        https://www.postgresql.org/docs/current/static/functions-json.html\n\n        More about Raw SQL expressions:\n        https://docs.djangoproject.com/en/dev/ref/models/expressions/#raw-sql-expressions\n\n        Usage example:\n            MyModel.objects.language('en_us').filter(is_active=True).order_by_json_path('title')\n        \"\"\"\n        language_code = (language_code\n                            or self._language_code\n                            or self.get_language_key(language_code))\n        json_path = '{%s,%s}' % (language_code, json_path)\n        # Our jsonb field is named `translations`.\n        raw_sql_expression = RawSQL(\"translations#>>%s\", (json_path,))\n        if order == 'desc':\n            raw_sql_expression = raw_sql_expression.desc()\n        return self.order_by(raw_sql_expression)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef order_by_json_path(self, json_path, language_code=None, order='asc'):\n        return self.get_queryset(language_code).order_by_json_path(\n            json_path, language_code=language_code, order=order)", "response": "Makes the method available through the manager."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_percent(self):\n        if not (self.votes and self.score):\n            return 0\n        return 100 * (self.get_rating() / self.field.range)", "response": "Returns the weighted percentage of the score from min - max values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_real_percent(self):\n        if not (self.votes and self.score):\n            return 0\n        return 100 * (self.get_real_rating() / self.field.range)", "response": "get_real_percent - Returns the unmodified percentage of the score based on a 0 - point scale"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ratings(self):\n        return Vote.objects.filter(content_type=self.get_content_type(), object_id=self.instance.pk, key=self.field.key)", "response": "get_ratings - Returns a QuerySet for this rating field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_real_rating(self):\n        if not (self.votes and self.score):\n            return 0\n        return float(self.score)/self.votes", "response": "get_rating - Returns the unmodified average rating."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_rating_for_user(self, user, ip_address=None, cookies={}):\n        kwargs = dict(\n            content_type    = self.get_content_type(),\n            object_id       = self.instance.pk,\n            key             = self.field.key,\n        )\n\n        if not (user and user.is_authenticated()):\n            if not ip_address:\n                raise ValueError('``user`` or ``ip_address`` must be present.')\n            kwargs['user__isnull'] = True\n            kwargs['ip_address'] = ip_address\n        else:\n            kwargs['user'] = user\n        \n        use_cookies = (self.field.allow_anonymous and self.field.use_cookies)\n        if use_cookies:\n            # TODO: move 'vote-%d.%d.%s' to settings or something\n            cookie_name = 'vote-%d.%d.%s' % (kwargs['content_type'].pk, kwargs['object_id'], kwargs['key'][:6],) # -> md5_hexdigest?\n            cookie = cookies.get(cookie_name)\n            if cookie:    \n                kwargs['cookie'] = cookie\n            else:\n                kwargs['cookie__isnull'] = True\n            \n        try:\n            rating = Vote.objects.get(**kwargs)\n            return rating.score\n        except Vote.MultipleObjectsReturned:\n            pass\n        except Vote.DoesNotExist:\n            pass\n        return", "response": "Returns the rating for a user or anonymous IP."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a new vote to an object.", "response": "def add(self, score, user, ip_address, cookies={}, commit=True):\n        \"\"\"add(score, user, ip_address)\n        \n        Used to add a rating to an object.\"\"\"\n        try:\n            score = int(score)\n        except (ValueError, TypeError):\n            raise InvalidRating(\"%s is not a valid choice for %s\" % (score, self.field.name))\n        \n        delete = (score == 0)\n        if delete and not self.field.allow_delete:\n            raise CannotDeleteVote(\"you are not allowed to delete votes for %s\" % (self.field.name,))\n            # ... you're also can't delete your vote if you haven't permissions to change it. I leave this case for CannotChangeVote\n        \n        if score < 0 or score > self.field.range:\n            raise InvalidRating(\"%s is not a valid choice for %s\" % (score, self.field.name))\n\n        is_anonymous = (user is None or not user.is_authenticated())\n        if is_anonymous and not self.field.allow_anonymous:\n            raise AuthRequired(\"user must be a user, not '%r'\" % (user,))\n        \n        if is_anonymous:\n            user = None\n        \n        defaults = dict(\n            score = score,\n            ip_address = ip_address,\n        )\n        \n        kwargs = dict(\n            content_type    = self.get_content_type(),\n            object_id       = self.instance.pk,\n            key             = self.field.key,\n            user            = user,\n        )\n        if not user:\n            kwargs['ip_address'] = ip_address\n        \n        use_cookies = (self.field.allow_anonymous and self.field.use_cookies)\n        if use_cookies:\n            defaults['cookie'] = now().strftime('%Y%m%d%H%M%S%f') # -> md5_hexdigest?\n            # TODO: move 'vote-%d.%d.%s' to settings or something\n            cookie_name = 'vote-%d.%d.%s' % (kwargs['content_type'].pk, kwargs['object_id'], kwargs['key'][:6],) # -> md5_hexdigest?\n            cookie = cookies.get(cookie_name) # try to get existent cookie value\n            if not cookie:\n                kwargs['cookie__isnull'] = True\n            kwargs['cookie'] = cookie\n\n        try:\n            rating, created = Vote.objects.get(**kwargs), False\n        except Vote.DoesNotExist:\n            if delete:\n                raise CannotDeleteVote(\"attempt to find and delete your vote for %s is failed\" % (self.field.name,))\n            if getattr(settings, 'RATINGS_VOTES_PER_IP', RATINGS_VOTES_PER_IP):\n                num_votes = Vote.objects.filter(\n                    content_type=kwargs['content_type'],\n                    object_id=kwargs['object_id'],\n                    key=kwargs['key'],\n                    ip_address=ip_address,\n                ).count()\n                if num_votes >= getattr(settings, 'RATINGS_VOTES_PER_IP', RATINGS_VOTES_PER_IP):\n                    raise IPLimitReached()\n            kwargs.update(defaults)\n            if use_cookies:\n                # record with specified cookie was not found ...\n                cookie = defaults['cookie'] # ... thus we need to replace old cookie (if presented) with new one\n                kwargs.pop('cookie__isnull', '') # ... and remove 'cookie__isnull' (if presented) from .create()'s **kwargs\n            rating, created = Vote.objects.create(**kwargs), True\n            \n        has_changed = False\n        if not created:\n            if self.field.can_change_vote:\n                has_changed = True\n                self.score -= rating.score\n                # you can delete your vote only if you have permission to change your vote\n                if not delete:\n                    rating.score = score\n                    rating.save()\n                else:\n                    self.votes -= 1\n                    rating.delete()\n            else:\n                raise CannotChangeVote()\n        else:\n            has_changed = True\n            self.votes += 1\n        if has_changed:\n            if not delete:\n                self.score += rating.score\n            if commit:\n                self.instance.save()\n            #setattr(self.instance, self.field.name, Rating(score=self.score, votes=self.votes))\n            \n            defaults = dict(\n                score   = self.score,\n                votes   = self.votes,\n            )\n            \n            kwargs = dict(\n                content_type    = self.get_content_type(),\n                object_id       = self.instance.pk,\n                key             = self.field.key,\n            )\n            \n            try:\n                score, created = Score.objects.get(**kwargs), False\n            except Score.DoesNotExist:\n                kwargs.update(defaults)\n                score, created = Score.objects.create(**kwargs), True\n            \n            if not created:\n                score.__dict__.update(defaults)\n                score.save()\n        \n        # return value\n        adds = {}\n        if use_cookies:\n            adds['cookie_name'] = cookie_name\n            adds['cookie'] = cookie\n        if delete:\n            adds['deleted'] = True\n        return adds"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update(self, commit=False):\n        votes = Vote.objects.filter(\n            content_type    = self.get_content_type(),\n            object_id       = self.instance.pk,\n            key             = self.field.key,\n        )\n        obj_score = sum([v.score for v in votes])\n        obj_votes = len(votes)\n\n        score, created = Score.objects.get_or_create(\n            content_type    = self.get_content_type(),\n            object_id       = self.instance.pk,\n            key             = self.field.key,\n            defaults        = dict(\n                score       = obj_score,\n                votes       = obj_votes,\n            )\n        )\n        if not created:\n            score.score = obj_score\n            score.votes = obj_votes\n            score.save()\n        self.score = obj_score\n        self.votes = obj_votes\n        if commit:\n            self.instance.save()", "response": "Forces an update of this rating."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, *args, **kwargs):\n        # XXX: circular import\n        from fields import RatingField\n\n        qs = self.distinct().values_list('content_type', 'object_id').order_by('content_type')\n    \n        to_update = []\n        for content_type, objects in itertools.groupby(qs, key=lambda x: x[0]):\n            model_class = ContentType.objects.get(pk=content_type).model_class()\n            if model_class:\n                to_update.extend(list(model_class.objects.filter(pk__in=list(objects)[0])))\n        \n        retval = super(VoteQuerySet, self).delete(*args, **kwargs)\n        \n        # TODO: this could be improved\n        for obj in to_update:\n            for field in getattr(obj, '_djangoratings', []):\n                getattr(obj, field.name)._update(commit=False)\n            obj.save()\n        \n        return retval", "response": "Handles updating the related votes and score fields attached to the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the ``Vote`` cast by a user on a particular object and stores it in a context variable. If the user has not voted, the context variable will be 0. Example usage:: {% rating_by_request request on instance as vote %}", "response": "def do_rating_by_request(parser, token):\n    \"\"\"\n    Retrieves the ``Vote`` cast by a user on a particular object and\n    stores it in a context variable. If the user has not voted, the\n    context variable will be 0.\n    \n    Example usage::\n    \n        {% rating_by_request request on instance as vote %}\n    \"\"\"\n    \n    bits = token.contents.split()\n    if len(bits) != 6:\n        raise template.TemplateSyntaxError(\"'%s' tag takes exactly five arguments\" % bits[0])\n    if bits[2] != 'on':\n        raise template.TemplateSyntaxError(\"second argument to '%s' tag must be 'on'\" % bits[0])\n    if bits[4] != 'as':\n        raise template.TemplateSyntaxError(\"fourth argument to '%s' tag must be 'as'\" % bits[0])\n    return RatingByRequestNode(bits[1], bits[3], bits[5])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_rating_by_user(parser, token):\n    \n    bits = token.contents.split()\n    if len(bits) != 6:\n        raise template.TemplateSyntaxError(\"'%s' tag takes exactly five arguments\" % bits[0])\n    if bits[2] != 'on':\n        raise template.TemplateSyntaxError(\"second argument to '%s' tag must be 'on'\" % bits[0])\n    if bits[4] != 'as':\n        raise template.TemplateSyntaxError(\"fourth argument to '%s' tag must be 'as'\" % bits[0])\n    return RatingByUserNode(bits[1], bits[3], bits[5])", "response": "This function returns the Vote cast by a user on a particular object and stores it in a context variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_revision():\n    package_dir = os.path.dirname(__file__)\n    checkout_dir = os.path.normpath(os.path.join(package_dir, '..'))\n    path = os.path.join(checkout_dir, '.git')\n    if os.path.exists(path):\n        return _get_git_revision(path)\n    return None", "response": "Returns the revision number of this branch or None if no revision number can be determined."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging min and max pulse widths.", "response": "def set_pulse_width_range(self, min_pulse=750, max_pulse=2250):\n        \"\"\"Change min and max pulse widths.\"\"\"\n        self._min_duty = int((min_pulse * self._pwm_out.frequency) / 1000000 * 0xffff)\n        max_duty = (max_pulse * self._pwm_out.frequency) / 1000000 * 0xffff\n        self._duty_range = int(max_duty - self._min_duty)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef onestep(self, *, direction=FORWARD, style=SINGLE):\n        # Adjust current steps based on the direction and type of step.\n        step_size = 0\n        if style == MICROSTEP:\n            step_size = 1\n        else:\n            half_step = self._microsteps // 2\n            full_step = self._microsteps\n            # Its possible the previous steps were MICROSTEPS so first align with the interleave\n            # pattern.\n            additional_microsteps = self._current_microstep % half_step\n            if additional_microsteps != 0:\n                # We set _current_microstep directly because our step size varies depending on the\n                # direction.\n                if direction == FORWARD:\n                    self._current_microstep += half_step - additional_microsteps\n                else:\n                    self._current_microstep -= additional_microsteps\n                step_size = 0\n            elif style == INTERLEAVE:\n                step_size = half_step\n\n            current_interleave = self._current_microstep // half_step\n            if ((style == SINGLE and current_interleave % 2 == 1) or\n                    (style == DOUBLE and current_interleave % 2 == 0)):\n                step_size = half_step\n            elif style in (SINGLE, DOUBLE):\n                step_size = full_step\n\n        if direction == FORWARD:\n            self._current_microstep += step_size\n        else:\n            self._current_microstep -= step_size\n\n        # Now that we know our target microstep we can determine how to energize the four coils.\n        self._update_coils(microstepping=style == MICROSTEP)\n\n        return self._current_microstep", "response": "Performs one step of a particular style."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _adjust_merged_values_orm(env, model_name, record_ids, target_record_id,\n                              field_spec):\n    \"\"\"This method deals with the values on the records to be merged +\n    the target record, performing operations that makes sense on the meaning\n    of the model.\n\n    :param field_spec: Dictionary with field names as keys and forced operation\n      to perform as values. If a field is not present here, default operation\n      will be performed.\n\n      Possible operations by field types:\n\n      * Char, Text and Html fields:\n        - 'merge' (default for Text and Html): content is concatenated\n          with an ' | ' as separator\n        - other value (default for Char): content on target record is preserved\n      * Integer, Float and Monetary fields:\n        - 'sum' (default for Float and Monetary): Sum all the values of\n          the records.\n        - 'avg': Perform the arithmetic average of the values of the records.\n        - 'max': Put the maximum of all the values.\n        - 'min': Put the minimum of all the values.\n        - other value (default for Integer): content on target record\n          is preserved\n      * Binary field:\n        - 'merge' (default): apply first not null value of the records if\n        value of target record is null, preserve target value otherwise.\n        - other value: content on target record is preserved\n      * Boolean field:\n        - 'and': Perform a logical AND over all values.\n        - 'or': Perform a logical OR over all values.\n        - other value (default): content on target record is preserved\n      * Date and Datetime fields:\n        - 'max': Put the maximum of all the values.\n        - 'min': Put the minimum of all the values.\n        - other value (default): content on target record is preserved\n      * Many2one fields:\n        - 'merge' (default): apply first not null value of the records if\n        value of target record is null, preserve target value otherwise.\n        - other value: content on target record is preserved\n      * Many2many fields:\n        - 'merge' (default): combine all the values\n        - other value: content on target record is preserved\n      * One2many fields:\n        - 'merge' (default): combine all the values\n        - other value: content on target record is preserved\n      * Reference fields:\n        - any value: content on target record is preserved\n      * Selection fields:\n        - any value: content on target record is preserved\n    \"\"\"\n    model = env[model_name]\n    fields = model._fields.values()\n    all_records = model.browse(tuple(record_ids) + (target_record_id, ))\n    target_record = model.browse(target_record_id)\n    vals = {}\n    o2m_changes = 0\n    for field in fields:\n        if not field.store or field.compute or field.related:\n            continue  # don't do anything on these cases\n        op = field_spec.get(field.name, False)\n        l = all_records.mapped(field.name)\n        if field.type in ('char', 'text', 'html'):\n            if not op:\n                op = 'other' if field.type == 'char' else 'merge'\n            if op == 'merge':\n                l = filter(lambda x: x is not False, l)\n                vals[field.name] = ' | '.join(l)\n        elif field.type in ('integer', 'float', 'monetary'):\n            if not op:\n                op = 'other' if field.type == 'integer' else 'sum'\n            if op == 'sum':\n                vals[field.name] = sum(l)\n            elif op == 'avg':\n                vals[field.name] = sum(l) / len(l)\n            elif op == 'max':\n                vals[field.name] = max(l)\n            elif op == 'min':\n                vals[field.name] = min(l)\n        elif field.type == 'boolean':\n            op = op or 'other'\n            if op == 'and':\n                vals[field.name] = functools.reduce(lambda x, y: x & y, l)\n            elif op == 'or':\n                vals[field.name] = functools.reduce(lambda x, y: x | y, l)\n        elif field.type in ('date', 'datetime'):\n            if op:\n                l = filter(lambda x: x is not False, l)\n            op = op or 'other'\n            if op == 'max':\n                vals[field.name] = max(l)\n            elif op == 'min':\n                vals[field.name] = min(l)\n        elif field.type == 'many2many':\n            op = op or 'merge'\n            if op == 'merge':\n                l = filter(lambda x: x is not False, l)\n                vals[field.name] = [(4, x.id) for x in l]\n        elif field.type == 'one2many':\n            op = op or 'merge'\n            if op == 'merge':\n                o2m_changes += 1\n                l.write({field.inverse_name: target_record_id})\n        elif field.type == 'binary':\n            op = op or 'merge'\n            if op == 'merge':\n                l = [x for x in l if x]\n                if not getattr(target_record, field.name) and l:\n                    vals[field.name] = l[0]\n        elif field.type == 'many2one':\n            op = op or 'merge'\n            if op == 'merge':\n                if not getattr(target_record, field.name) and l:\n                    vals[field.name] = l[0]\n    # Curate values that haven't changed\n    new_vals = {}\n    for f in vals:\n        if model._fields[f].type != 'many2many':\n            if vals[f] != getattr(target_record, f):\n                new_vals[f] = vals[f]\n        else:\n            if [x[1] for x in vals[f]] not in getattr(target_record, f).ids:\n                new_vals[f] = vals[f]\n    if new_vals:\n        target_record.write(new_vals)\n        logger.debug(\n            \"Write %s value(s) in target record '%s' of model '%s'\",\n            len(new_vals) + o2m_changes, target_record_id, model_name,\n        )", "response": "This method applies the values on the records to be merged and performs the normal operations on the target record."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates known generic style res_id or res_model references", "response": "def _change_generic(env, model_name, record_ids, target_record_id,\n                    exclude_columns, method='orm'):\n    \"\"\" Update known generic style res_id/res_model references \"\"\"\n    for model_to_replace, res_id_column, model_column in [\n            ('calendar.event', 'res_id', 'res_model'),\n            ('ir.attachment', 'res_id', 'res_model'),\n            ('mail.activity', 'res_id', 'res_model'),\n            ('mail.followers', 'res_id', 'res_model'),\n            ('mail.message', 'res_id', 'model'),\n            ('rating.rating', 'res_id', 'res_model'),\n            ]:\n        try:\n            model = env[model_to_replace].with_context(active_test=False)\n        except KeyError:\n            continue\n        if (model._table, res_id_column) in exclude_columns:\n            continue\n        if method == 'orm':\n            records = model.search([\n                (model_column, '=', model_name),\n                (res_id_column, 'in', record_ids)])\n            if records:\n                records.write({res_id_column: target_record_id})\n                logger.debug(\n                    \"Changed %s record(s) of model '%s'\",\n                    len(records), model_to_replace)\n        else:\n            logged_query(\n                env.cr,\n                \"\"\" UPDATE %(table)s\n                    SET %(res_id_column)s = %(target_record_id)s\n                    WHERE %(model_column)s = %(model_name)s\n                    AND %(res_id_column)s in %(record_ids)s\n                \"\"\",\n                {\n                    'table': AsIs(model._table),\n                    'res_id_column': AsIs(res_id_column),\n                    'model_column': AsIs(model_column),\n                    'model_name': model_name,\n                    'target_record_id': target_record_id,\n                    'record_ids': record_ids,\n                }, skip_no_result=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_records(env, model_name, record_ids, target_record_id,\n                  field_spec=None, method='orm', delete=True,\n                  exclude_columns=None):\n    \"\"\"Merge several records into the target one.\n\n    NOTE: This should be executed in end migration scripts for assuring that\n    all the possible relations are loaded and changed. Tested on v10/v11.\n\n    :param env: Environment variable\n    :param model_name: Name of the model of the records to merge\n    :param record_ids: List of IDS of records that are going to be merged.\n    :param target_record_id: ID of the record where the rest records are going\n      to be merge in.\n    :param field_spec: Dictionary with field names as keys and forced operation\n      to perform as values. If a field is not present here, default operation\n      will be performed. See _adjust_merged_values_orm method doc for all the\n      available operators.\n    :param method: Specify how to perform operations. By default or specifying\n      'orm', operations will be performed with ORM, maybe slower, but safer, as\n      related and computed fields will be recomputed on changes, and all\n      constraints will be checked.\n    :param delete: If set, the source ids will be unlinked.\n    :exclude_columns: list of tuples (table, column) that will be ignored.\n    \"\"\"\n    if exclude_columns is None:\n        exclude_columns = []\n    if field_spec is None:\n        field_spec = {}\n    if isinstance(record_ids, list):\n        record_ids = tuple(record_ids)\n    args0 = (env, model_name, record_ids, target_record_id)\n    args = args0 + (exclude_columns, )\n    args2 = args0 + (field_spec, )\n    if target_record_id in record_ids:\n        raise Exception(\"You can't put the target record in the list or \"\n                        \"records to be merged.\")\n    # Check which records to be merged exist\n    record_ids = env[model_name].browse(record_ids).exists().ids\n    if not record_ids:\n        return\n    _change_generic(*args, method=method)\n    if method == 'orm':\n        _change_many2one_refs_orm(*args)\n        _change_many2many_refs_orm(*args)\n        _change_reference_refs_orm(*args)\n        _change_translations_orm(*args)\n        # TODO: serialized fields\n        with env.norecompute():\n            _adjust_merged_values_orm(*args2)\n        env[model_name].recompute()\n        if delete:\n            _delete_records_orm(env, model_name, record_ids, target_record_id)\n    else:\n        _change_foreign_key_refs(*args)\n        _change_reference_refs_sql(*args)\n        _change_translations_sql(*args)\n        # TODO: Adjust values of the merged records through SQL\n        if delete:\n            _delete_records_sql(env, model_name, record_ids, target_record_id)", "response": "Merge several records into a single record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef allow_pgcodes(cr, *codes):\n    try:\n        with cr.savepoint():\n            with core.tools.mute_logger('odoo.sql_db'):\n                yield\n    except (ProgrammingError, IntegrityError) as error:\n        msg = \"Code: {code}. Class: {class_}. Error: {error}.\".format(\n            code=error.pgcode,\n            class_=errorcodes.lookup(error.pgcode[:2]),\n            error=errorcodes.lookup(error.pgcode))\n        if error.pgcode in codes or error.pgcode[:2] in codes:\n            logger.info(msg)\n        else:\n            logger.exception(msg)\n            raise", "response": "Context manager that will ignore specified error codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_values_selection_field(cr, table_name, field_name, allowed_values):\n    res = True\n    cr.execute(\"SELECT %s, count(*) FROM %s GROUP BY %s;\" %\n               (field_name, table_name, field_name))\n    for row in cr.fetchall():\n        if row[0] not in allowed_values:\n            logger.error(\n                \"Invalid value '%s' in the table '%s' \"\n                \"for the field '%s'. (%s rows).\",\n                row[0], table_name, field_name, row[1])\n            res = False\n    return res", "response": "Check if the field selection field_name of the table table_name field_name has only the values allowed_values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_data(cr, module_name, filename, idref=None, mode='init'):\n\n    if idref is None:\n        idref = {}\n    logger.info('%s: loading %s' % (module_name, filename))\n    _, ext = os.path.splitext(filename)\n    pathname = os.path.join(module_name, filename)\n    fp = tools.file_open(pathname)\n    try:\n        if ext == '.csv':\n            noupdate = True\n            tools.convert_csv_import(\n                cr, module_name, pathname, fp.read(), idref, mode, noupdate)\n        elif ext == '.yml':\n            yaml_import(cr, module_name, fp, None, idref=idref, mode=mode)\n        elif mode == 'init_no_create':\n            for fp2 in _get_existing_records(cr, fp, module_name):\n                tools.convert_xml_import(\n                    cr, module_name, fp2, idref, mode='init',\n                )\n        else:\n            tools.convert_xml_import(cr, module_name, fp, idref, mode=mode)\n    finally:\n        fp.close()", "response": "Load an xml csv or yml file from a migration script."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_existing_records(cr, fp, module_name):\n    def yield_element(node, path=None):\n        if node.tag not in ['openerp', 'odoo', 'data']:\n            if node.tag == 'record':\n                xmlid = node.attrib['id']\n                if '.' not in xmlid:\n                    module = module_name\n                else:\n                    module, xmlid = xmlid.split('.', 1)\n                cr.execute(\n                    'select id from ir_model_data where module=%s and name=%s',\n                    (module, xmlid)\n                )\n                if not cr.rowcount:\n                    return\n            result = StringIO(etree.tostring(path, encoding='unicode'))\n            result.name = None\n            yield result\n        else:\n            for child in node:\n                for value in yield_element(\n                        child,\n                        etree.SubElement(path, node.tag, node.attrib)\n                        if path else etree.Element(node.tag, node.attrib)\n                ):\n                    yield value\n    return yield_element(etree.parse(fp).getroot())", "response": "Yields file like objects per leaf node in the xml file that exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy columns from a table to another table.", "response": "def copy_columns(cr, column_spec):\n    \"\"\"\n    Copy table columns. Typically called in the pre script.\n\n    :param column_spec: a hash with table keys, with lists of tuples as\n        values. Tuples consist of (old_name, new_name, type). Use None for\n        new_name to trigger a conversion of old_name using get_legacy_name()\n        Use None for type to use type of old field.\n        Make sure to quote properly, if your column name coincides with a\n        SQL directive. eg. '\"column\"'\n\n    .. versionadded:: 8.0\n    \"\"\"\n    for table_name in column_spec.keys():\n        for (old, new, field_type) in column_spec[table_name]:\n            if new is None:\n                new = get_legacy_name(old)\n            if field_type is None:\n                cr.execute(\"\"\"\n                    SELECT data_type\n                    FROM information_schema.columns\n                    WHERE table_name=%s\n                        AND column_name = %s;\n                    \"\"\", (table_name, old))\n                field_type = cr.fetchone()[0]\n            logged_query(cr, \"\"\"\n                ALTER TABLE %(table_name)s\n                ADD COLUMN %(new)s %(field_type)s;\n                UPDATE %(table_name)s SET %(new)s=%(old)s;\n                \"\"\" % {\n                'table_name': table_name,\n                'old': old,\n                'field_type': field_type,\n                'new': new,\n            })"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenames columns in the sequence of tables.", "response": "def rename_columns(cr, column_spec):\n    \"\"\"\n    Rename table columns. Typically called in the pre script.\n\n    :param column_spec: a hash with table keys, with lists of tuples as \\\n    values. Tuples consist of (old_name, new_name). Use None for new_name \\\n    to trigger a conversion of old_name using get_legacy_name()\n    \"\"\"\n    for table in column_spec.keys():\n        for (old, new) in column_spec[table]:\n            if new is None:\n                new = get_legacy_name(old)\n            logger.info(\"table %s, column %s: renaming to %s\",\n                        table, old, new)\n            cr.execute(\n                'ALTER TABLE \"%s\" RENAME \"%s\" TO \"%s\"' % (table, old, new,))\n            cr.execute('DROP INDEX IF EXISTS \"%s_%s_index\"' % (table, old))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rename_fields(env, field_spec, no_deep=False):\n    cr = env.cr\n    for model, table, old_field, new_field in field_spec:\n        if column_exists(cr, table, old_field):\n            rename_columns(cr, {table: [(old_field, new_field)]})\n        # Rename corresponding field entry\n        cr.execute(\"\"\"\n            UPDATE ir_model_fields\n            SET name = %s\n            WHERE name = %s\n                AND model = %s\n            \"\"\", (new_field, old_field, model),\n        )\n        # Rename translations\n        cr.execute(\"\"\"\n            UPDATE ir_translation\n            SET name = %s\n            WHERE name = %s\n                AND type = 'model'\n            \"\"\", (\n                \"%s,%s\" % (model, old_field),\n                \"%s,%s\" % (model, new_field),\n            ),\n        )\n        # Rename appearances on export profiles\n        # TODO: Rename when the field is part of a submodel (ex. m2one.field)\n        cr.execute(\"\"\"\n            UPDATE ir_exports_line\n            SET name = %s\n            WHERE name = %s\n            \"\"\", (old_field, new_field),\n        )\n        # Rename appearances on filters\n        # Example of replaced domain: [['field', '=', self], ...]\n        # TODO: Rename when the field is part of a submodel (ex. m2one.field)\n        cr.execute(\"\"\"\n            UPDATE ir_filters\n            SET domain = replace(domain, %(old_pattern)s, %(new_pattern)s)\n            WHERE model_id = %%s\n                AND domain ~ %(old_pattern)s\n            \"\"\" % {\n                'old_pattern': \"$$'%s'$$\" % old_field,\n                'new_pattern': \"$$'%s'$$\" % new_field,\n            }, (model, ),\n        )\n        # Examples of replaced contexts:\n        # {'group_by': ['field', 'other_field'], 'other_key':value}\n        # {'group_by': ['date_field:month']}\n        # {'other_key': value, 'group_by': ['other_field', 'field']}\n        # {'group_by': ['other_field'],'col_group_by': ['field']}\n        cr.execute(r\"\"\"\n            UPDATE ir_filters\n            SET context = regexp_replace(\n                context, %(old_pattern)s, %(new_pattern)s\n            )\n            WHERE model_id = %%s\n                AND context ~ %(old_pattern)s\n            \"\"\" % {\n                'old_pattern': (\n                    r\"$$('group_by'|'col_group_by'):([^\\]]*)\"\n                    r\"'%s(:day|:week|:month|:year){0,1}'(.*?\\])$$\"\n                ) % old_field,\n                'new_pattern': r\"$$\\1:\\2'%s\\3'\\4$$\" % new_field,\n            }, (model, ),\n        )\n        if table_exists(env.cr, 'mail_alias'):\n            # Rename appearances on mail alias\n            cr.execute(\"\"\"\n                UPDATE mail_alias ma\n                SET alias_defaults =\n                    replace(alias_defaults, %(old_pattern)s, %(new_pattern)s)\n                FROM ir_model im\n                WHERE ma.alias_model_id = im.id\n                    AND im.model = %%s\n                    AND ma.alias_defaults ~ %(old_pattern)s\n                \"\"\" % {\n                    'old_pattern': \"$$'%s'$$\" % old_field,\n                    'new_pattern': \"$$'%s'$$\" % new_field,\n                }, (model, ),\n            )", "response": "Rename fields in the specified base module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrenaming tables. Typically called in the pre script.", "response": "def rename_tables(cr, table_spec):\n    \"\"\"\n    Rename tables. Typically called in the pre script.\n    This function also renames the id sequence if it exists and if it is\n    not modified in the same run.\n\n    :param table_spec: a list of tuples (old table name, new table name). Use \\\n    None for new_name to trigger a conversion of old_name to the result of \\\n    get_legacy_name()\n    \"\"\"\n    # Append id sequences\n    to_rename = [x[0] for x in table_spec]\n    for old, new in list(table_spec):\n        if new is None:\n            new = get_legacy_name(old)\n        if (table_exists(cr, old + '_id_seq') and\n                old + '_id_seq' not in to_rename):\n            table_spec.append((old + '_id_seq', new + '_id_seq'))\n    for (old, new) in table_spec:\n        if new is None:\n            new = get_legacy_name(old)\n        logger.info(\"table %s: renaming to %s\",\n                    old, new)\n        cr.execute('ALTER TABLE \"%s\" RENAME TO \"%s\"' % (old, new,))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrename models in the current node.", "response": "def rename_models(cr, model_spec):\n    \"\"\"\n    Rename models. Typically called in the pre script.\n    :param model_spec: a list of tuples (old model name, new model name).\n\n    Use case: if a model changes name, but still implements equivalent\n    functionality you will want to update references in for instance\n    relation fields.\n\n    WARNING: This method doesn't rename the associated tables. For that,\n    you need to call `rename_tables` method.\n    \"\"\"\n    for (old, new) in model_spec:\n        logger.info(\"model %s: renaming to %s\",\n                    old, new)\n        _old = old.replace('.', '_')\n        _new = new.replace('.', '_')\n        logged_query(\n            cr,\n            'UPDATE ir_model SET model = %s '\n            'WHERE model = %s', (new, old,),\n        )\n        logged_query(\n            cr,\n            'UPDATE ir_model_data SET model = %s '\n            'WHERE model = %s', (new, old,),\n        )\n        logged_query(\n            cr,\n            \"UPDATE ir_model_data SET name=%s \"\n            \"WHERE name=%s AND model = 'ir.model'\",\n            ('model_' + _new, 'model_' + _old,),\n        )\n        logged_query(\n            cr, \"\"\"UPDATE ir_model_data imd\n            SET name = 'field_' || '%s' || '_' || imf.name\n            FROM ir_model_fields imf\n            WHERE imd.model = 'ir.model.fields'\n                AND imd.name = 'field_' || '%s' || '_' || imf.name\n                AND imf.model = %s\"\"\",\n            (AsIs(_new), AsIs(_old), old),\n        )\n        logged_query(\n            cr,\n            'UPDATE ir_attachment SET res_model = %s '\n            'WHERE res_model = %s', (new, old,),\n        )\n        logged_query(\n            cr,\n            'UPDATE ir_model_fields SET model = %s '\n            'WHERE model = %s', (new, old,),\n        )\n        logged_query(\n            cr,\n            \"UPDATE ir_translation SET \"\n            \"name=%s || substr(name, strpos(name, ',')) \"\n            \"WHERE name LIKE %s\",\n            (new, old + ',%'),\n        )\n        # Handle properties that reference to this model\n        logged_query(\n            cr,\n            \"SELECT id FROM ir_model_fields \"\n            \"WHERE relation = %s AND ttype = 'many2one'\", (old, ),\n        )\n        field_ids = [x[0] for x in cr.fetchall()]\n        logged_query(\n            cr,\n            'UPDATE ir_model_fields SET relation = %s '\n            'WHERE relation = %s', (new, old,),\n        )\n        if field_ids:\n            logged_query(\n                cr, \"\"\"\n                UPDATE ir_property\n                SET value_reference = regexp_replace(\n                    value_reference, %(old_pattern)s, %(new_pattern)s\n                )\n                WHERE fields_id IN %(field_ids)s\n                AND value_reference ~ %(old_pattern)s\"\"\", {\n                    'field_ids': tuple(field_ids),\n                    'old_pattern': r\"^%s,[ ]*([0-9]*)\" % old,\n                    'new_pattern': r\"%s,\\1\" % new,\n                },\n            )\n        if column_exists(cr, 'ir_act_server', 'model_name'):\n            # model_name is a related field that in v11 becomes stored\n            logged_query(\n                cr,\n                'UPDATE ir_act_server SET model_name = %s '\n                'WHERE model_name = %s', (new, old,),\n            )\n        if is_module_installed(cr, 'email_template'):\n            if table_exists(cr, 'email_template') and column_exists(\n                    cr, 'email_template', 'model'):\n                logged_query(\n                    cr,\n                    'UPDATE email_template SET model=%s'\n                    'where model=%s', (new, old),\n                )\n        if is_module_installed(cr, 'mail'):\n            # fortunately, the data model didn't change up to now\n            logged_query(\n                cr,\n                'UPDATE mail_message SET model=%s where model=%s', (new, old),\n            )\n            if table_exists(cr, 'mail_template'):\n                logged_query(\n                    cr,\n                    'UPDATE mail_template SET model=%s'\n                    'where model=%s', (new, old),\n                )\n            if table_exists(cr, 'mail_followers'):\n                logged_query(\n                    cr,\n                    'UPDATE mail_followers SET res_model=%s '\n                    'where res_model=%s', (new, old),\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_xmlids(cr, xmlids_spec):\n    for (old, new) in xmlids_spec:\n        if '.' not in old or '.' not in new:\n            logger.error(\n                'Cannot rename XMLID %s to %s: need the module '\n                'reference to be specified in the IDs' % (old, new))\n        else:\n            query = (\"UPDATE ir_model_data SET module = %s, name = %s \"\n                     \"WHERE module = %s and name = %s\")\n            logged_query(cr, query, tuple(new.split('.') + old.split('.')))", "response": "This function is called by the pre script to rename XML IDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_xmlid(cr, module, xmlid, model, res_id, noupdate=False):\n    # Check if the XMLID doesn't already exists\n    cr.execute(\n        \"SELECT id FROM ir_model_data WHERE module=%s AND name=%s \"\n        \"AND model=%s\",\n        (module, xmlid, model))\n    already_exists = cr.fetchone()\n    if already_exists:\n        return False\n    else:\n        logged_query(\n            cr,\n            \"INSERT INTO ir_model_data (create_uid, create_date, \"\n            \"write_uid, write_date, date_init, date_update, noupdate, \"\n            \"name, module, model, res_id) \"\n            \"VALUES (%s, (now() at time zone 'UTC'), %s, \"\n            \"(now() at time zone 'UTC'), (now() at time zone 'UTC'), \"\n            \"(now() at time zone 'UTC'), %s, %s, %s, %s, %s)\", (\n                SUPERUSER_ID, SUPERUSER_ID, noupdate,\n                xmlid, module, model, res_id))\n        return True", "response": "Add an entry in the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndrop columns from the database.", "response": "def drop_columns(cr, column_spec):\n    \"\"\"\n    Drop columns but perform an additional check if a column exists.\n    This covers the case of function fields that may or may not be stored.\n    Consider that this may not be obvious: an additional module can govern\n    a function fields' store properties.\n\n    :param column_spec: a list of (table, column) tuples\n    \"\"\"\n    for (table, column) in column_spec:\n        logger.info(\"table %s: drop column %s\",\n                    table, column)\n        if column_exists(cr, table, column):\n            cr.execute('ALTER TABLE \"%s\" DROP COLUMN \"%s\"' %\n                       (table, column))\n        else:\n            logger.warn(\"table %s: column %s did not exist\",\n                        table, column)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_workflow_workitems(cr, pool, ref_spec_actions):\n    workflow_workitems = pool['workflow.workitem']\n    ir_model_data_model = pool['ir.model.data']\n\n    for (target_external_id, fallback_external_id) in ref_spec_actions:\n        target_activity = ir_model_data_model.get_object(\n            cr, SUPERUSER_ID,\n            target_external_id.split(\".\")[0],\n            target_external_id.split(\".\")[1],\n        )\n        fallback_activity = ir_model_data_model.get_object(\n            cr, SUPERUSER_ID,\n            fallback_external_id.split(\".\")[0],\n            fallback_external_id.split(\".\")[1],\n        )\n        ids = workflow_workitems.search(\n            cr, SUPERUSER_ID, [('act_id', '=', target_activity.id)]\n        )\n        if ids:\n            logger.info(\n                \"Moving %d items in the removed workflow action (%s) to a \"\n                \"fallback action (%s): %s\",\n                len(ids), target_activity.name, fallback_activity.name, ids\n            )\n            workflow_workitems.write(\n                cr, SUPERUSER_ID, ids, {'act_id': fallback_activity.id}\n            )", "response": "Update the list of items in the target state to the wanted state."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses that function in the following case: if a field of a model was moved from a 'A' module to a 'B' module. ('B' depend on 'A'), This function will test if 'B' is installed. If not, count the number of different value and possibly warn the user. Use orm, so call from the post script. :param old_module: name of the old module :param fields: list of dictionary with the following keys: 'table' : name of the table where the field is. 'field' : name of the field that are moving. 'new_module' : name of the new module .. versionadded:: 7.0", "response": "def warn_possible_dataloss(cr, pool, old_module, fields):\n    \"\"\"\n    Use that function in the following case:\n    if a field of a model was moved from a 'A' module to a 'B' module.\n    ('B' depend on 'A'),\n    This function will test if 'B' is installed.\n    If not, count the number of different value and possibly warn the user.\n    Use orm, so call from the post script.\n\n    :param old_module: name of the old module\n    :param fields: list of dictionary with the following keys:\n        'table' : name of the table where the field is.\n        'field' : name of the field that are moving.\n        'new_module' : name of the new module\n\n    .. versionadded:: 7.0\n    \"\"\"\n    module_obj = pool.get('ir.module.module')\n    for field in fields:\n        module_ids = module_obj.search(\n            cr, SUPERUSER_ID, [\n                ('name', '=', field['new_module']),\n                ('state', 'in', ['installed', 'to upgrade', 'to install'])\n            ])\n        if not module_ids:\n            cr.execute(\n                \"SELECT count(*) FROM (SELECT %s from %s group by %s) \"\n                \"as tmp\" % (\n                    field['field'], field['table'], field['field']))\n            row = cr.fetchone()\n            if row[0] == 1:\n                # not a problem, that field wasn't used.\n                # Just a loss of functionality\n                logger.info(\n                    \"Field '%s' from module '%s' was moved to module \"\n                    \"'%s' which is not installed: \"\n                    \"No dataloss detected, only loss of functionality\"\n                    % (field['field'], old_module, field['new_module']))\n            else:\n                # there is data loss after the migration.\n                message(\n                    cr, old_module, None, None,\n                    \"Field '%s' was moved to module \"\n                    \"'%s' which is not installed: \"\n                    \"There were %s distinct values in this field.\",\n                    field['field'], field['new_module'], row[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_defaults(cr, pool, default_spec, force=False, use_orm=False):\n\n    def write_value(ids, field, value):\n        logger.debug(\n            \"model %s, field %s: setting default value of resources %s to %s\",\n            model, field, ids, unicode(value))\n        if use_orm:\n            if version_info[0] <= 7:\n                for res_id in ids:\n                    # Iterating over ids here as a workaround for lp:1131653\n                    obj.write(cr, SUPERUSER_ID, [res_id], {field: value})\n            else:\n                if api and isinstance(pool, api.Environment):\n                    obj.browse(ids).write({field: value})\n                else:\n                    obj.write(cr, SUPERUSER_ID, ids, {field: value})\n        else:\n            query, params = \"UPDATE %s SET %s = %%s WHERE id IN %%s\" % (\n                obj._table, field), (value, tuple(ids))\n            # handle fields inherited from somewhere else\n            if version_info[0] >= 10:\n                columns = obj._fields\n            else:\n                columns = obj._columns\n            if field not in columns:\n                query, params = None, None\n                for model_name in obj._inherits:\n                    if obj._inherit_fields[field][0] != model_name:\n                        continue\n                    col = obj._inherits[model_name]\n                    # this is blatantly stolen and adapted from\n                    # https://github.com/OCA/OCB/blob/def7db0b93e45eda7b51b3b61\n                    # bae1e975d07968b/openerp/osv/orm.py#L4307\n                    nids = []\n                    for sub_ids in cr.split_for_in_conditions(ids):\n                        cr.execute(\n                            'SELECT DISTINCT %s FROM %s WHERE id IN %%s' % (\n                                col, obj._table), (sub_ids,))\n                        nids.extend(x for x, in cr.fetchall())\n                    query, params = \"UPDATE %s SET %s = %%s WHERE id IN %%s\" %\\\n                        (pool[model_name]._table, field), (value, tuple(nids))\n            if not query:\n                do_raise(\"Can't set default for %s on %s!\" % (\n                    field, obj._name))\n            # cope with really big tables\n            for sub_ids in cr.split_for_in_conditions(params[1]):\n                cr.execute(query, (params[0], sub_ids))\n\n    for model in default_spec.keys():\n        try:\n            obj = pool[model]\n        except KeyError:\n            do_raise(\n                \"Migration: error setting default, no such model: %s\" % model)\n\n        for field, value in default_spec[model]:\n            domain = not force and [(field, '=', False)] or []\n            if api and isinstance(pool, api.Environment):\n                ids = obj.search(domain).ids\n            else:\n                ids = obj.search(cr, SUPERUSER_ID, domain)\n            if not ids:\n                continue\n            if value is None:\n                if version_info[0] > 7:\n                    if api and isinstance(pool, api.Environment):\n                        value = obj.default_get([field]).get(field)\n                    else:\n                        value = obj.default_get(\n                            cr, SUPERUSER_ID, [field]).get(field)\n                    if value:\n                        write_value(ids, field, value)\n                else:\n                    # For older versions, compute defaults per user anymore\n                    # if the default is a method. If we need this in newer\n                    # versions, make it a parameter.\n                    if field in obj._defaults:\n                        if not callable(obj._defaults[field]):\n                            write_value(ids, field, obj._defaults[field])\n                        else:\n                            cr.execute(\n                                \"SELECT id, COALESCE(create_uid, 1) FROM %s \" %\n                                obj._table + \"WHERE id in %s\", (tuple(ids),))\n                            # Execute the function once per user_id\n                            user_id_map = {}\n                            for row in cr.fetchall():\n                                user_id_map.setdefault(row[1], []).append(\n                                    row[0])\n                            for user_id in user_id_map:\n                                write_value(\n                                    user_id_map[user_id], field,\n                                    obj._defaults[field](\n                                        obj, cr, user_id, None))\n                    else:\n                        error = (\n                            \"OpenUpgrade: error setting default, field %s \"\n                            \"with None default value not in %s' _defaults\" % (\n                                field, model))\n                        logger.error(error)\n                        # this exc. seems to get lost in a higher up try block\n                        except_orm(\"OpenUpgrade\", error)\n            else:\n                write_value(ids, field, value)", "response": "Set the default value of the resource in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logged_query(cr, query, args=None, skip_no_result=False):\n    if args is None:\n        args = ()\n    args = tuple(args) if type(args) == list else args\n    try:\n        cr.execute(query, args)\n    except (ProgrammingError, IntegrityError):\n        logger.error('Error running %s' % cr.mogrify(query, args))\n        raise\n    if not skip_no_result or cr.rowcount:\n        logger.debug('Running %s', query % args)\n        logger.debug('%s rows affected', cr.rowcount)\n    return cr.rowcount", "response": "Logs a query and affected rows at level DEBUG."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_module_names(cr, namespec, merge_modules=False):\n    for (old_name, new_name) in namespec:\n        if merge_modules:\n            # Delete meta entries, that will avoid the entry removal\n            # They will be recreated by the new module anyhow.\n            query = \"SELECT id FROM ir_module_module WHERE name = %s\"\n            cr.execute(query, [old_name])\n            row = cr.fetchone()\n            if row:\n                old_id = row[0]\n                query = \"DELETE FROM ir_model_constraint WHERE module = %s\"\n                logged_query(cr, query, [old_id])\n                query = \"DELETE FROM ir_model_relation WHERE module = %s\"\n                logged_query(cr, query, [old_id])\n        else:\n            query = \"UPDATE ir_module_module SET name = %s WHERE name = %s\"\n            logged_query(cr, query, (new_name, old_name))\n            query = (\"UPDATE ir_model_data SET name = %s \"\n                     \"WHERE name = %s AND module = 'base' AND \"\n                     \"model='ir.module.module' \")\n            logged_query(cr, query,\n                         (\"module_%s\" % new_name, \"module_%s\" % old_name))\n        # The subselect allows to avoid duplicated XML-IDs\n        query = (\"UPDATE ir_model_data SET module = %s \"\n                 \"WHERE module = %s AND name NOT IN \"\n                 \"(SELECT name FROM ir_model_data WHERE module = %s)\")\n        logged_query(cr, query, (new_name, old_name, new_name))\n        # Rename the remaining occurrences for let Odoo's update process\n        # to auto-remove related resources\n        query = (\"UPDATE ir_model_data \"\n                 \"SET name = name || '_openupgrade_' || id, \"\n                 \"module = %s \"\n                 \"WHERE module = %s\")\n        logged_query(cr, query, (new_name, old_name))\n        query = (\"UPDATE ir_module_module_dependency SET name = %s \"\n                 \"WHERE name = %s\")\n        logged_query(cr, query, (new_name, old_name))\n        if version_info[0] > 7:\n            query = (\"UPDATE ir_translation SET module = %s \"\n                     \"WHERE module = %s\")\n            logged_query(cr, query, (new_name, old_name))\n        if merge_modules:\n            # Conserve old_name's state if new_name is uninstalled\n            logged_query(\n                cr,\n                \"UPDATE ir_module_module m1 SET state=m2.state \"\n                \"FROM ir_module_module m2 WHERE m1.name=%s AND \"\n                \"m2.name=%s AND m1.state='uninstalled'\",\n                (new_name, old_name),\n            )\n            query = \"DELETE FROM ir_module_module WHERE name = %s\"\n            logged_query(cr, query, [old_name])\n            logged_query(\n                cr,\n                \"DELETE FROM ir_model_data WHERE module = 'base' \"\n                \"AND model='ir.module.module' AND name = %s\",\n                ('module_%s' % old_name,),\n            )", "response": "Deal with changed module names making all the needed changes on the tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds new columns on ir_model_fields to the base model.", "response": "def add_ir_model_fields(cr, columnspec):\n    \"\"\"\n    Typically, new columns on ir_model_fields need to be added in a very\n    early stage in the upgrade process of the base module, in raw sql\n    as they need to be in place before any model gets initialized.\n    Do not use for fields with additional SQL constraints, such as a\n    reference to another table or the cascade constraint, but craft your\n    own statement taking them into account.\n\n    :param columnspec: tuple of (column name, column type)\n    \"\"\"\n    for column in columnspec:\n        query = 'ALTER TABLE ir_model_fields ADD COLUMN %s %s' % (\n            column)\n        logged_query(cr, query, [])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef m2o_to_x2m(cr, model, table, field, source_field):\n    columns = getattr(model, '_columns', False) or getattr(model, '_fields')\n    if not columns.get(field):\n        do_raise(\"m2o_to_x2m: field %s doesn't exist in model %s\" % (\n            field, model._name))\n    m2m_types = []\n    if many2many:\n        m2m_types.append(many2many)\n    if Many2many:\n        m2m_types.append(Many2many)\n    o2m_types = []\n    if one2many:\n        o2m_types.append(one2many)\n    if One2many:\n        o2m_types.append(One2many)\n    if isinstance(columns[field], tuple(m2m_types)):\n        column = columns[field]\n        if hasattr(many2many, '_sql_names'):  # >= 6.1 and < 10.0\n            rel, id1, id2 = many2many._sql_names(column, model)\n        elif hasattr(column, 'relation'):  # >= 10.0\n            rel, id1, id2 = column.relation, column.column1, column.column2\n        else:  # <= 6.0\n            rel, id1, id2 = column._rel, column._id1, column._id2\n        logged_query(\n            cr,\n            \"\"\"\n            INSERT INTO %s (%s, %s)\n            SELECT id, %s\n            FROM %s\n            WHERE %s is not null\n            \"\"\" %\n            (rel, id1, id2, source_field, table, source_field))\n    elif isinstance(columns[field], tuple(o2m_types)):\n        if isinstance(columns[field], One2many):  # >= 8.0\n            target_table = model.env[columns[field].comodel_name]._table\n            target_field = columns[field].inverse_name\n        else:\n            target_table = model.pool[columns[field]._obj]._table\n            target_field = columns[field]._fields_id\n        logged_query(\n            cr,\n            \"\"\"\n            UPDATE %(target_table)s AS target\n            SET %(target_field)s=source.id\n            FROM %(source_table)s AS source\n            WHERE source.%(source_field)s=target.id\n            \"\"\" % {'target_table': target_table,\n                   'target_field': target_field,\n                   'source_field': source_field,\n                   'source_table': table})\n    else:\n        do_raise(\n            \"m2o_to_x2m: field %s of model %s is not a \"\n            \"many2many/one2many one\" % (field, model._name))", "response": "Transform many2one relations into one2many or many2many."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef m2o_to_m2m(cr, model, table, field, source_field):\n    return m2o_to_x2m(cr, model, table, field, source_field)", "response": "Recreate relations in many2many fields that were formerly\n    many2one fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap old values to new values within the same model or table.", "response": "def map_values(\n        cr, source_column, target_column, mapping,\n        model=None, table=None, write='sql'):\n    \"\"\"\n    Map old values to new values within the same model or table. Old values\n    presumably come from a legacy column.\n    You will typically want to use it in post-migration scripts.\n\n    :param cr: The database cursor\n    :param source_column: the database column that contains old values to be \\\n    mapped\n    :param target_column: the database column, or model field (if 'write' is \\\n    'orm') that the new values are written to\n    :param mapping: list of tuples [(old value, new value)]\n        Old value True represents \"is set\", False \"is not set\".\n    :param model: used for writing if 'write' is 'orm', or to retrieve the \\\n    table if 'table' is not given.\n    :param table: the database table used to query the old values, and write \\\n    the new values (if 'write' is 'sql')\n    :param write: Either 'orm' or 'sql'. Note that old ids are always \\\n    identified by an sql read.\n\n    This method does not support mapping m2m, o2m or property fields. \\\n    For o2m you can migrate the inverse field's column instead.\n\n    .. versionadded:: 8.0\n    \"\"\"\n\n    if write not in ('sql', 'orm'):\n        logger.exception(\n            \"map_values is called with unknown value for write param: %s\",\n            write)\n    if not table:\n        if not model:\n            logger.exception(\"map_values is called with no table and no model\")\n        table = model._table\n    if source_column == target_column:\n        logger.exception(\n            \"map_values is called with the same value for source and old\"\n            \" columns : %s\",\n            source_column)\n    for old, new in mapping:\n        new = \"'%s'\" % new\n\n        if old is True:\n            old = 'NOT NULL'\n            op = 'IS'\n        elif old is False:\n            old = 'NULL'\n            op = 'IS'\n        else:\n            old = \"'%s'\" % old\n            op = '='\n\n        values = {\n            'table': table,\n            'source': source_column,\n            'target': target_column,\n            'old': old,\n            'new': new,\n            'op': op,\n        }\n\n        if write == 'sql':\n            query = \"\"\"UPDATE %(table)s\n                       SET %(target)s = %(new)s\n                       WHERE %(source)s %(op)s %(old)s\"\"\" % values\n        else:\n            query = \"\"\"SELECT id FROM %(table)s\n                       WHERE %(source)s %(op)s %(old)s\"\"\" % values\n        logged_query(cr, query, values)\n        if write == 'orm':\n            model.write(\n                cr, SUPERUSER_ID,\n                [row[0] for row in cr.fetchall()],\n                {target_column: new})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog handler for non - critical notifications about the upgrade.", "response": "def message(cr, module, table, column,\n            message, *args, **kwargs):\n    \"\"\"\n    Log handler for non-critical notifications about the upgrade.\n    To be extended with logging to a table for reporting purposes.\n\n    :param module: the module name that the message concerns\n    :param table: the model that this message concerns (may be False, \\\n    but preferably not if 'column' is defined)\n    :param column: the column that this message concerns (may be False)\n\n    .. versionadded:: 7.0\n    \"\"\"\n    argslist = list(args or [])\n    prefix = ': '\n    if column:\n        argslist.insert(0, column)\n        prefix = ', column %s' + prefix\n    if table:\n        argslist.insert(0, table)\n        prefix = ', table %s' + prefix\n    argslist.insert(0, module)\n    prefix = 'Module %s' + prefix\n\n    logger.warn(prefix + message, *argslist, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deactivate_workflow_transitions(cr, model, transitions=None):\n    transition_ids = []\n    if transitions:\n        data_obj = RegistryManager.get(cr.dbname)['ir.model.data']\n        for module, name in transitions:\n            try:\n                transition_ids.append(\n                    data_obj.get_object_reference(\n                        cr, SUPERUSER_ID, module, name)[1])\n            except ValueError:\n                continue\n    else:\n        cr.execute(\n            '''select distinct t.id\n            from wkf w\n            join wkf_activity a on a.wkf_id=w.id\n            join wkf_transition t\n                on t.act_from=a.id or t.act_to=a.id\n            where w.osv=%s''', (model,))\n        transition_ids = [i for i, in cr.fetchall()]\n    cr.execute(\n        'select id, condition from wkf_transition where id in %s',\n        (tuple(transition_ids),))\n    transition_conditions = dict(cr.fetchall())\n    cr.execute(\n        \"update wkf_transition set condition = 'False' WHERE id in %s\",\n        (tuple(transition_ids),))\n    return transition_conditions", "response": "Deactivate workflow transitions for a given model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreactivating workflow transitions previously deactivated by deactivate_workflow_transitions.", "response": "def reactivate_workflow_transitions(cr, transition_conditions):\n    \"\"\"\n    Reactivate workflow transition previously deactivated by\n    deactivate_workflow_transitions.\n\n    :param transition_conditions: a dictionary returned by \\\n    deactivate_workflow_transitions\n\n    .. versionadded:: 7.0\n    .. deprecated:: 11.0\n       Workflows were removed from Odoo as of version 11.0\n    \"\"\"\n    for transition_id, condition in transition_conditions.iteritems():\n        cr.execute(\n            'update wkf_transition set condition = %s where id = %s',\n            (condition, transition_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logging(args_details=False, step=False):\n    def wrap(func):\n        def wrapped_function(*args, **kwargs):\n            to_log = True\n            msg = \"Executing method %s\" % func.__name__\n\n            # Count calls\n            if step:\n                # Compute unique name\n                unique_name = '%s.%s' % (func.__module__, func.__name__)\n                if unique_name not in openupgrade_call_logging:\n                    openupgrade_call_logging[unique_name] = 0\n                openupgrade_call_logging[unique_name] += 1\n                current = openupgrade_call_logging[unique_name]\n                if current == 1 or current % step == 0:\n                    msg += \" ; Calls quantity : %d\" % current\n                    if current == 1:\n                        msg += \" ; Logging Step : %d\" % step\n                else:\n                    to_log = False\n\n            # Log Args\n            if args_details and to_log:\n                if args:\n                    msg += \" ; args : %s\" % str(args)\n                if kwargs:\n                    msg += \" ; kwargs : %s\" % str(kwargs)\n\n            if to_log:\n                logger.info(msg)\n\n            return func(*args, **kwargs)\n        return wrapped_function\n    return wrap", "response": "Decorator for functions that can take time or for debug purposes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses that function in the following case: A field moves from a model A to the model B with : A -> m2o -> B. (For exemple product_product -> product_template) This function manage the migration of this field. available on post script migration. :param registry_old_model: registry of the model A; :param field_old_model: name of the field to move in model A; :param m2o_field_old_model: name of the field of the table of the model A \\ that link model A to model B; :param registry_new_model: registry of the model B; :param field_new_model: name of the field to move in model B; :param quick_request: Set to False, if you want to use write function to \\ update value; Otherwise, the function will use UPDATE SQL request; :param compute_func: This a function that receives 4 parameters: \\ cr, pool: common args;\\ id: id of the instance of Model B\\ vals: list of different values.\\ This function must return a unique value that will be set to the\\ instance of Model B which id is 'id' param;\\ If compute_func is not set, the algorithm will take the value that\\ is the most present in vals.\\ :binary_field: Set to True if the migrated field is a binary field .. versionadded:: 8.0", "response": "def move_field_m2o(\n        cr, pool,\n        registry_old_model, field_old_model, m2o_field_old_model,\n        registry_new_model, field_new_model,\n        quick_request=True, compute_func=None, binary_field=False):\n    \"\"\"\n    Use that function in the following case:\n    A field moves from a model A to the model B with : A -> m2o -> B.\n    (For exemple product_product -> product_template)\n    This function manage the migration of this field.\n    available on post script migration.\n    :param registry_old_model: registry of the model A;\n    :param field_old_model: name of the field to move in model A;\n    :param m2o_field_old_model: name of the field of the table of the model A \\\n    that link model A to model B;\n    :param registry_new_model: registry of the model B;\n    :param field_new_model: name of the field to move in model B;\n    :param quick_request: Set to False, if you want to use write function to \\\n    update value; Otherwise, the function will use UPDATE SQL request;\n    :param compute_func: This a function that receives 4 parameters: \\\n    cr, pool: common args;\\\n    id: id of the instance of Model B\\\n    vals:  list of different values.\\\n    This function must return a unique value that will be set to the\\\n    instance of Model B which id is 'id' param;\\\n    If compute_func is not set, the algorithm will take the value that\\\n    is the most present in vals.\\\n    :binary_field: Set to True if the migrated field is a binary field\n\n    .. versionadded:: 8.0\n    \"\"\"\n    def default_func(cr, pool, id, vals):\n        \"\"\"This function return the value the most present in vals.\"\"\"\n        quantity = {}.fromkeys(set(vals), 0)\n        for val in vals:\n            quantity[val] += 1\n        res = vals[0]\n        for val in vals:\n            if quantity[res] < quantity[val]:\n                res = val\n        return res\n\n    logger.info(\"Moving data from '%s'.'%s' to '%s'.'%s'\" % (\n        registry_old_model, field_old_model,\n        registry_new_model, field_new_model))\n\n    table_old_model = pool[registry_old_model]._table\n    table_new_model = pool[registry_new_model]._table\n    # Manage regular case (all the value are identical)\n    cr.execute(\n        \" SELECT %s\"\n        \" FROM %s\"\n        \" GROUP BY %s\"\n        \" HAVING count(*) = 1;\" % (\n            m2o_field_old_model, table_old_model, m2o_field_old_model\n        ))\n    ok_ids = [x[0] for x in cr.fetchall()]\n    if quick_request:\n        query = (\n            \" UPDATE %s as new_table\"\n            \" SET %s=(\"\n            \"    SELECT old_table.%s\"\n            \"    FROM %s as old_table\"\n            \"    WHERE old_table.%s=new_table.id\"\n            \"    LIMIT 1) \"\n            \" WHERE id in %%s\" % (\n                table_new_model, field_new_model, field_old_model,\n                table_old_model, m2o_field_old_model))\n        logged_query(cr, query, [tuple(ok_ids)])\n    else:\n        query = (\n            \" SELECT %s, %s\"\n            \" FROM %s \"\n            \" WHERE %s in %%s\"\n            \" GROUP BY %s, %s\" % (\n                m2o_field_old_model, field_old_model, table_old_model,\n                m2o_field_old_model, m2o_field_old_model, field_old_model))\n        cr.execute(query, [tuple(ok_ids)])\n        for res in cr.fetchall():\n            if res[1] and binary_field:\n                pool[registry_new_model].write(\n                    cr, SUPERUSER_ID, res[0],\n                    {field_new_model: res[1][:]})\n            else:\n                pool[registry_new_model].write(\n                    cr, SUPERUSER_ID, res[0],\n                    {field_new_model: res[1]})\n\n    # Manage non-determinist case (some values are different)\n    func = compute_func if compute_func else default_func\n    cr.execute(\n        \" SELECT %s \"\n        \" FROM %s \"\n        \" GROUP BY %s having count(*) != 1;\" % (\n            m2o_field_old_model, table_old_model, m2o_field_old_model\n        ))\n    ko_ids = [x[0] for x in cr.fetchall()]\n    for ko_id in ko_ids:\n        query = (\n            \" SELECT %s\"\n            \" FROM %s\"\n            \" WHERE %s = %s;\" % (\n                field_old_model, table_old_model, m2o_field_old_model, ko_id))\n        cr.execute(query)\n        if binary_field:\n            vals = [str(x[0][:]) if x[0] else False for x in cr.fetchall()]\n        else:\n            vals = [x[0] for x in cr.fetchall()]\n        value = func(cr, pool, ko_id, vals)\n        if quick_request:\n            query = (\n                \" UPDATE %s\"\n                \" SET %s=%%s\"\n                \" WHERE id = %%s\" % (table_new_model, field_new_model))\n            logged_query(\n                cr, query, (value, ko_id))\n        else:\n            pool[registry_new_model].write(\n                cr, SUPERUSER_ID, [ko_id],\n                {field_new_model: value})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts field value to HTML value.", "response": "def convert_field_to_html(cr, table, field_name, html_field_name):\n    \"\"\"\n    Convert field value to HTML value.\n\n    .. versionadded:: 7.0\n    \"\"\"\n    if version_info[0] < 7:\n        logger.error(\"You cannot use this method in an OpenUpgrade version \"\n                     \"prior to 7.0.\")\n        return\n    cr.execute(\n        \"SELECT id, %(field)s FROM %(table)s WHERE %(field)s IS NOT NULL\" % {\n            'field': field_name,\n            'table': table,\n        }\n    )\n    for row in cr.fetchall():\n        logged_query(\n            cr, \"UPDATE %(table)s SET %(field)s = %%s WHERE id = %%s\" % {\n                'field': html_field_name,\n                'table': table,\n            }, (plaintext2html(row[1]), row[0])\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking the related user s timezone into account when converting a date field to datetime in a given table.", "response": "def date_to_datetime_tz(\n        cr, table_name, user_field_name, date_field_name, datetime_field_name):\n    \"\"\" Take the related user's timezone into account when converting\n    date field to datetime in a given table.\n    This function must be call in post migration script.\n\n    :param table_name : Name of the table where the field is;\n    :param user_field_name : The name of the user field (res.users);\n    :param date_field_name : The name of the old date field; \\\n    (Typically a legacy name, set in pre-migration script)\n    :param datetime_field_name : The name of the new date field;\n\n    .. versionadded:: 8.0\n    \"\"\"\n    cr.execute(\n        \"\"\"\n        SELECT distinct(rp.tz)\n        FROM %s my_table, res_users ru, res_partner rp\n        WHERE rp.tz IS NOT NULL\n            AND my_table.%s=ru.id\n            AND ru.partner_id=rp.id\n        \"\"\" % (table_name, user_field_name,))\n    for timezone, in cr.fetchall():\n        cr.execute(\"SET TIMEZONE=%s\", (timezone,))\n        values = {\n            'table_name': table_name,\n            'date_field_name': date_field_name,\n            'datetime_field_name': datetime_field_name,\n            'timezone': timezone,\n        }\n        logged_query(\n            cr,\n            \"\"\"\n            UPDATE %(table_name)s my_table\n            SET %(datetime_field_name)s =\n                my_table.%(date_field_name)s::TIMESTAMP AT TIME ZONE 'UTC'\n            FROM res_partner rp, res_users ru\n            WHERE my_table.%(date_field_name)s IS NOT NULL\n                AND my_table.user_id=ru.id\n                AND ru.partner_id=rp.id\n                AND rp.tz='%(timezone)s';\n            \"\"\" % values)\n    cr.execute(\"RESET TIMEZONE\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlifts all constraints on column in table.", "response": "def lift_constraints(cr, table, column):\n    \"\"\"Lift all constraints on column in table.\n    Typically, you use this in a pre-migrate script where you adapt references\n    for many2one fields with changed target objects.\n    If everything went right, the constraints will be recreated\"\"\"\n    cr.execute(\n        'select relname, array_agg(conname) from '\n        '(select t1.relname, c.conname '\n        'from pg_constraint c '\n        'join pg_attribute a '\n        'on c.confrelid=a.attrelid and a.attnum=any(c.conkey) '\n        'join pg_class t on t.oid=a.attrelid '\n        'join pg_class t1 on t1.oid=c.conrelid '\n        'where t.relname=%(table)s and attname=%(column)s '\n        'union select t.relname, c.conname '\n        'from pg_constraint c '\n        'join pg_attribute a '\n        'on c.conrelid=a.attrelid and a.attnum=any(c.conkey) '\n        'join pg_class t on t.oid=a.attrelid '\n        'where relname=%(table)s and attname=%(column)s) in_out '\n        'group by relname',\n        {\n            'table': table,\n            'column': column,\n        })\n    for table, constraints in cr.fetchall():\n        cr.execute(\n            'alter table %s drop constraint %s',\n            (AsIs(table), AsIs(', drop constraint '.join(constraints)))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef savepoint(cr):\n    if hasattr(cr, 'savepoint'):\n        with cr.savepoint():\n            yield\n    else:\n        name = uuid.uuid1().hex\n        cr.execute('SAVEPOINT \"%s\"' % name)\n        try:\n            yield\n            cr.execute('RELEASE SAVEPOINT \"%s\"' % name)\n        except:\n            cr.execute('ROLLBACK TO SAVEPOINT \"%s\"' % name)", "response": "return a context manager wrapping postgres savepoints"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrename property old_name owned by model to new_name. This should happen in a pre - migration script.", "response": "def rename_property(cr, model, old_name, new_name):\n    \"\"\"Rename property old_name owned by model to new_name. This should happen\n    in a pre-migration script.\"\"\"\n    cr.execute(\n        \"update ir_model_fields f set name=%s \"\n        \"from ir_model m \"\n        \"where m.id=f.model_id and m.model=%s and f.name=%s \"\n        \"returning f.id\",\n        (new_name, model, old_name))\n    field_ids = tuple(i for i, in cr.fetchall())\n    cr.execute(\n        \"update ir_model_data set name=%s where model='ir.model.fields' and \"\n        \"res_id in %s\",\n        ('%s,%s' % (model, new_name), field_ids))\n    cr.execute(\n        \"update ir_property set name=%s where fields_id in %s\",\n        (new_name, field_ids))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_record_translations(cr, module, xml_ids):\n    if not isinstance(xml_ids, (list, tuple)):\n        do_raise(\"XML IDs %s must be a tuple or list!\" % (xml_ids))\n\n    cr.execute(\"\"\"\n        SELECT model, res_id\n        FROM ir_model_data\n        WHERE module = %s AND name in %s\n    \"\"\", (module, tuple(xml_ids),))\n    for row in cr.fetchall():\n        query = (\"\"\"\n            DELETE FROM ir_translation\n            WHERE module = %s AND name LIKE %s AND res_id = %s;\n        \"\"\")\n        logged_query(cr, query, (module, row[0] + ',%', row[1],))", "response": "Cleanup translations of specific records in a module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_fields(env, field_spec):\n    sql_type_mapping = {\n        'binary': 'bytea',  # If there's attachment, no SQL. Force it manually\n        'boolean': 'bool',\n        'char': 'varchar',  # Force it manually if there's size limit\n        'date': 'date',\n        'datetime': 'timestamp',\n        'float': 'numeric',  # Force manually to double precision if no digits\n        'html': 'text',\n        'integer': 'int4',\n        'many2many': False,  # No need to create SQL column\n        'many2one': 'int4',\n        'monetary': 'numeric',\n        'one2many': False,  # No need to create SQL column\n        'reference': 'varchar',\n        'selection': 'varchar',  # Can be sometimes integer. Force it manually\n        'text': 'text',\n        'serialized': 'text',\n    }\n    for vals in field_spec:\n        field_name, model_name, table_name, field_type, sql_type, module = vals\n        # Add SQL column\n        if not table_name:\n            table_name = env[model_name]._table\n        sql_type = sql_type_mapping.get(field_type)\n        if sql_type:\n            logged_query(\n                env.cr, \"\"\"ALTER TABLE %s ADD COLUMN %s %s\"\"\",\n                (AsIs(table_name), AsIs(field_name), AsIs(sql_type)),\n            )\n        # Add ir.model.fields entry\n        env.cr.execute(\n            \"SELECT id FROM ir_model WHERE model = %s\", (model_name, ),\n        )\n        row = env.cr.fetchone()\n        if not row:\n            continue\n        model_id = row[0]\n        logged_query(\n            env.cr, \"\"\"\n            INSERT INTO ir_model_fields (\n                model_id, model, name, field_description, ttype, state\n            ) VALUES (\n                %s, %s, %s, %s, %s, %s\n            ) RETURNING id\"\"\",\n            (model_id, model_name, field_name, 'OU', field_type, 'base'),\n        )\n        field_id = env.cr.fetchone()[0]\n        # Add ir.model.data entry\n        if not module:\n            continue\n        name1 = 'field_%s_%s' % (model_name.replace('.', '_'), field_name)\n        logged_query(\n            env.cr, \"\"\"\n            INSERT INTO ir_model_data (\n                name, date_init, date_update, module, model, res_id\n            ) VALUES (\n                %s, (now() at time zone 'UTC'), (now() at time zone 'UTC'),\n                %s, %s, %s\n            )\"\"\", (name1, module, 'ir.model.fields', field_id),\n        )", "response": "This method adds all the needed stuff for having a new field populated in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the module for fields that have been moved from one module to another.", "response": "def update_module_moved_fields(\n        cr, model, moved_fields, old_module, new_module):\n    \"\"\"Update module for field definition in general tables that have been moved\n    from one module to another.\n\n    NOTE: This is not needed in >=v12, as now Odoo always add the XML-ID entry:\n    https://github.com/odoo/odoo/blob/9201f92a4f29a53a014b462469f27b32dca8fc5a/\n    odoo/addons/base/models/ir_model.py#L794-L802\n\n    :param cr: Database cursor\n    :param model: model name\n    :param moved_fields: list of moved fields\n    :param old_module: previous module of the fields\n    :param new_module: new module of the fields\n    \"\"\"\n    if version_info[0] <= 7:\n        do_raise(\"This only works for Odoo version >=v8\")\n    if version_info[0] >= 12:\n        do_raise(\"This should be used only for Odoo version <v12\")\n    if not isinstance(moved_fields, (list, tuple)):\n        do_raise(\"moved_fields %s must be a tuple or list!\" % moved_fields)\n    logger.info(\n        \"Moving fields %s in model %s from module '%s' to module '%s'\",\n        ', '.join(moved_fields), model, old_module, new_module,\n    )\n    vals = {\n        'new_module': new_module,\n        'old_module': old_module,\n        'model': model,\n        'fields': tuple(moved_fields),\n    }\n    # update xml-id entries\n    logged_query(\n        cr, \"\"\"\n        UPDATE ir_model_data imd\n        SET module = %(new_module)s\n        FROM ir_model_fields imf\n        WHERE\n            imf.model = %(model)s AND\n            imf.name IN %(fields)s AND\n            imd.module = %(old_module)s AND\n            imd.model = 'ir.model.fields' AND\n            imd.res_id = imf.id AND\n            imd.id NOT IN (\n               SELECT id FROM ir_model_data WHERE module = %(new_module)s\n            )\n        \"\"\", vals,\n    )\n    # update ir_translation - it covers both <=v8 through type='field' and\n    # >=v9 through type='model' + name\n    logged_query(\n        cr, \"\"\"\n        UPDATE ir_translation it\n        SET module = %(new_module)s\n        FROM ir_model_fields imf\n        WHERE\n            imf.model = %(model)s AND\n            imf.name IN %(fields)s AND\n            it.res_id = imf.id AND\n            it.module = %(old_module)s AND ((\n                it.name LIKE 'ir.model.fields,field_%%' AND\n                it.type = 'model'\n            ) OR (\n                it.type = 'field'\n            ))\n        \"\"\", vals,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_records_safely_by_xml_id(env, xml_ids):\n    for xml_id in xml_ids:\n        logger.debug('Deleting record for XML-ID %s', xml_id)\n        try:\n            with env.cr.savepoint():\n                env.ref(xml_id).exists().unlink()\n        except Exception as e:\n            logger.error('Error deleting XML-ID %s: %s', xml_id, repr(e))", "response": "This removes in the safest possible way the records whose XML - IDs are xml_ids passed as argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chunked(records, single=True):\n    if version_info[0] > 10:\n        invalidate = records.env.cache.invalidate\n    elif version_info[0] > 7:\n        invalidate = records.env.invalidate_all\n    else:\n        raise Exception('Not supported Odoo version for this method.')\n    size = core.models.PREFETCH_MAX\n    model = records._name\n    ids = records.with_context(prefetch_fields=False).ids\n    for i in range(0, len(ids), size):\n        invalidate()\n        chunk = records.env[model].browse(ids[i:i + size])\n        if single:\n            for record in chunk:\n                yield record\n            continue\n        yield chunk", "response": "Yields a chunk of the recordset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_binary_field_to_attachment(env, field_spec):\n    logger = logging.getLogger('OpenUpgrade')\n    attachment_model = env['ir.attachment']\n    for model_name in field_spec:\n        model = env[model_name]\n        for field, column in field_spec[model_name]:\n            if column is None:\n                column = openupgrade.get_legacy_name(field)\n            logger.info(\n                \"Converting to attachment field {} from model {} stored in \"\n                \"column {}\".format(field, model_name, column)\n            )\n            last_id = 0\n            while True:\n                env.cr.execute(\n                    \"\"\"SELECT id, {0} FROM {1} WHERE {0} IS NOT NULL AND id > {2}\n                    ORDER BY id LIMIT 500;\n                    \"\"\".format(column, model._table, last_id)\n                )\n                rows = env.cr.fetchall()\n                if not rows:\n                    break\n                logger.info(\n                    \"  converting {0} items starting after {1}...\"\n                    \"\".format(len(rows), last_id))\n                for row in rows:\n                    last_id = row[0]\n                    data = bytes(row[1])\n                    if data and data != 'None':\n                        attachment_model.create({\n                            'name': field,\n                            'res_model': model_name,\n                            'res_field': field,\n                            'res_id': last_id,\n                            'type': 'binary',\n                            'datas': data,\n                        })\n            # Remove source column for cleaning the room\n            env.cr.execute(\"ALTER TABLE {} DROP COLUMN {}\".format(\n                model._table, column,\n            ))", "response": "This method converts binary fields to attachments like Odoo 9. 0. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreplaces old account types with their replacements.", "response": "def replace_account_types(env, type_spec, unlink=True):\n    \"\"\" Replace old account types with their replacements. The old account\n    type is allowed not to exist anymore, except when running unit tests.\n    :param type_spec: list of tuples (xmlid of old account.account.type, \\\nxmlid of new account.account.type)\n    :param unlink: attempt to unlink the old account type\n    \"\"\"\n    logger = logging.getLogger('OpenUpgrade')\n    for old_type, new_type in type_spec:\n        try:\n            type8 = env.ref(old_type)\n        except ValueError:\n            if getattr(threading.currentThread(), 'testing', False):\n                raise\n            continue\n\n        type9 = env.ref(new_type)\n        for table in ('account_account',\n                      'account_account_template',\n                      'account_move_line'):\n            env.cr.execute(\n                \"UPDATE %s SET user_type_id = %s WHERE user_type_id = %s\",\n                (AsIs(table), type9.id, type8.id))\n        if unlink:\n            with env.cr.savepoint():\n                try:\n                    type8.unlink()\n                except Exception as e:\n                    logger.info(\n                        'Could not remove account type %s: %s',\n                        old_type, e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_partner_id_from_partner_address_id(\n        cr, pool, model_name, partner_field, address_field, table=None):\n    \"\"\"\n    Set the new partner_id on any table with migrated contact ids\n\n    :param model_name: the model name of the target table\n    :param partner_field: the column in the target model's table \\\n                          that will store the new partner when found\n    :param address_field: the legacy field in the model's table \\\n                    that contains the old address in the model's table\n    :param table: override the target model's table name in case it was renamed\n    :returns: nothing\n    \"\"\"\n    model = pool.get(model_name)\n    table = table or model._table\n    openupgrade.logged_query(\n        cr,\n        \"\"\"\n        UPDATE %(table)s\n        SET %(partner_field)s=address.openupgrade_7_migrated_to_partner_id\n        FROM res_partner_address address\n        WHERE %(table)s.%(address_field)s=address.id\n        \"\"\" % {'table': table,\n               'partner_field': partner_field,\n               'address_field': address_field})", "response": "Set the new partner id on any table with migrated contact ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a set of ids and a model pool return a dict of each object ids with their latest message date as a value.", "response": "def get_last_post_for_model(cr, uid, ids, model_pool):\n    \"\"\"\n    Given a set of ids and a model pool, return a dict of each object ids with\n    their latest message date as a value.\n    To be called in post-migration scripts\n\n    :param cr: database cursor\n    :param uid: user id, assumed to be openerp.SUPERUSER_ID\n    :param ids: ids of the model in question to retrieve ids\n    :param model_pool: orm model pool, assumed to be from pool.get()\n    :return: a dict with ids as keys and with dates as values\n    \"\"\"\n    if type(ids) is not list:\n        ids = [ids]\n    res = {}\n    for obj in model_pool.browse(cr, uid, ids):\n        message_ids = obj.message_ids\n        if message_ids:\n            res[obj.id] = sorted(\n                message_ids, key=lambda x: x.date, reverse=True)[0].date\n        else:\n            res[obj.id] = False\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a list of models set their message_last_post fields to an analyzed object.", "response": "def set_message_last_post(cr, uid, pool, models):\n    \"\"\"\n    Given a list of models, set their 'message_last_post' fields to an\n    estimated last post datetime.\n    To be called in post-migration scripts\n\n    :param cr: database cursor\n    :param uid: user id, assumed to be openerp.SUPERUSER_ID\n    :param pool: orm pool, assumed to be openerp.pooler.get_pool(cr.dbname)\n    :param models: a list of model names for which 'message_last_post' needs \\\n    to be filled\n    :return:\n    \"\"\"\n    if type(models) is not list:\n        models = [models]\n    for model in models:\n        model_pool = pool[model]\n        cr.execute(\n            \"UPDATE {table} \"\n            \"SET message_last_post=(SELECT max(mm.date) \"\n            \"FROM mail_message mm \"\n            \"WHERE mm.model=%s \"\n            \"AND mm.date IS NOT NULL \"\n            \"AND mm.res_id={table}.id)\".format(\n                table=model_pool._table), (model,)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the aliases of a resource in a specific model.", "response": "def update_aliases(\n        cr, registry, model_name, set_parent_thread_id,\n        alias_defaults=None, defaults_id_key=False):\n    \"\"\"\n    Update a model's aliases according to how they are configured\n    in the model's create() method.\n\n    :param model_name: The name of the model whose aliases are to be updated. \\\n    The model_id is also set as the aliases' alias_parent_model_id.\n    :param set_parent_thread_id': When set, set the ids of the resources as \\\n    their alias' alias_parent_thread_id\n    :param alias_defaults: Static dictionary, recorded as a string on each \\\n    alias\n    :param defaults_id_key: When defined, add this key to each alias' \\\n    defaults dictionary with the resource id as its value.\n    \"\"\"\n    model_id = registry['ir.model'].search(\n        cr, SUPERUSER_ID, [('model', '=', model_name)])[0]\n    vals = {'alias_parent_model_id': model_id}\n    if defaults_id_key and alias_defaults is None:\n        alias_defaults = {}\n    res_ids = registry[model_name].search(\n        cr, SUPERUSER_ID, [], context={'active_test': False})\n    for res in registry[model_name].browse(\n            cr, SUPERUSER_ID, res_ids):\n        if set_parent_thread_id:\n            vals['alias_parent_thread_id'] = res.id\n        if defaults_id_key:\n            alias_defaults[defaults_id_key] = res.id\n        if alias_defaults is not None:\n            vals['alias_defaults'] = str(alias_defaults)\n        res.alias_id.write(vals)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether a certain column exists in the given table.", "response": "def column_exists(cr, table, column):\n    \"\"\" Check whether a certain column exists \"\"\"\n    cr.execute(\n        'SELECT count(attname) FROM pg_attribute '\n        'WHERE attrelid = '\n        '( SELECT oid FROM pg_class WHERE relname = %s ) '\n        'AND attname = %s',\n        (table, column))\n    return cr.fetchone()[0] == 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nguesses the type of this stream and return whether or not it needs encoding.", "response": "def guess_stream_needs_encoding(fileobj, default=True):\n    \"\"\"\n    Guess the type (bytes/unicode) of this stream, and return whether or not it\n    requires text to be encoded before written into it.\n    \"\"\"\n    # XXX: Unicode\n    # On Python 2, stdout is bytes. However, we can't wrap it in a\n    # TextIOWrapper, as it's not from IOBase, so it doesn't have .seekable.\n    # It does, however, have a mode, and we can cheese it base on that.\n    # On Python 3, stdout is a TextIOWrapper, and so we can safely write\n    # str to it, and it will encode it correctly for the target terminal or\n    # whatever.\n    # If it's a io.BytesIO or StringIO, then it won't have a mode, but it\n    # is a read/write stream, so we can get its type by reading 0 bytes and\n    # checking the type.\n    try:\n        # If it's a r/w stream, this will give us the type of it\n        t = type(fileobj.read(0))\n\n        if t is bytes:\n            return True\n        elif t is unicode:\n            return False\n\n    except Exception:\n        pass\n\n    try:\n        mode = fileobj.mode\n\n        if PY2 and mode == \"w\":\n            mode = \"wb\"\n\n        if \"b\" in mode:\n            return True\n        else:\n            return False\n    except Exception:\n        pass\n\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an instance of the asyncio API with the given configuration.", "response": "def with_config(loop=None):\n    \"\"\"\n    :return: an instance of the txaio API with the given\n        configuration. This won't affect anything using the 'gloabl'\n        config nor other instances created using this function.\n\n    If you need to customize txaio configuration separately (e.g. to\n    use multiple event-loops in asyncio), you can take code like this:\n\n        import txaio\n\n\n        class FunTimes(object):\n\n            def something_async(self):\n                return txaio.call_later(1, lambda: 'some result')\n\n    and instead do this:\n\n        import txaio\n\n\n        class FunTimes(object):\n            txaio = txaio\n\n            def something_async(self):\n                # this will run in the local/new event loop created in the constructor\n                return self.txaio.call_later(1, lambda: 'some result')\n\n        fun0 = FunTimes()\n        fun1 = FunTimes()\n        fun1.txaio = txaio.with_config(loop=asyncio.new_event_loop())\n\n    So `fun1` will run its futures on the newly-created event loop,\n    while `fun0` will work just as it did before this `with_config`\n    method was introduced (after 2.6.2).\n    \"\"\"\n    cfg = _Config()\n    if loop is not None:\n        cfg.loop = loop\n    return _AsyncioApi(cfg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_logging(out=_stdout, level='info'):\n    global _log_level, _loggers, _started_logging\n    if level not in log_levels:\n        raise RuntimeError(\n            \"Invalid log level '{0}'; valid are: {1}\".format(\n                level, ', '.join(log_levels)\n            )\n        )\n\n    if _started_logging:\n        return\n\n    _started_logging = True\n    _log_level = level\n\n    handler = _TxaioFileHandler(out)\n    logging.getLogger().addHandler(handler)\n    # note: Don't need to call basicConfig() or similar, because we've\n    # now added at least one handler to the root logger\n    logging.raiseExceptions = True  # FIXME\n    level_to_stdlib = {\n        'critical': logging.CRITICAL,\n        'error': logging.ERROR,\n        'warn': logging.WARNING,\n        'info': logging.INFO,\n        'debug': logging.DEBUG,\n        'trace': logging.DEBUG,\n    }\n    logging.getLogger().setLevel(level_to_stdlib[level])\n    # make sure any loggers we created before now have their log-level\n    # set (any created after now will get it from _log_level\n    for logger in _loggers:\n        logger._set_log_level(level)", "response": "Begin logging.\n\n    :param out: if provided, a file-like object to log to. By default, this is\n                stdout.\n    :param level: the maximum log-level to emit (a string)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the global log level on all loggers in the main log hierarchy.", "response": "def set_global_log_level(level):\n    \"\"\"\n    Set the global log level on all loggers instantiated by txaio.\n    \"\"\"\n    for logger in _loggers:\n        logger._set_log_level(level)\n    global _log_level\n    _log_level = level"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a unicode error - message for a failed exception", "response": "def failure_message(self, fail):\n        \"\"\"\n        :param fail: must be an IFailedFuture\n        returns a unicode error-message\n        \"\"\"\n        try:\n            return u'{0}: {1}'.format(\n                fail._value.__class__.__name__,\n                str(fail._value),\n            )\n        except Exception:\n            return u'Failed to produce failure message for \"{0}\"'.format(fail)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef failure_format_traceback(self, fail):\n        try:\n            f = six.StringIO()\n            traceback.print_exception(\n                fail._type,\n                fail.value,\n                fail._traceback,\n                file=f,\n            )\n            return f.getvalue()\n        except Exception:\n            return u\"Failed to format failure traceback for '{0}'\".format(fail)", "response": "Formats the traceback of a failed future."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and returns a new _BatchedTimer object that will be called every bucket_seconds seconds.", "response": "def make_batched_timer(self, bucket_seconds, chunk_size=100):\n        \"\"\"\n        Creates and returns an object implementing\n        :class:`txaio.IBatchedTimer`.\n\n        :param bucket_seconds: the number of seconds in each bucket. That\n            is, a value of 5 means that any timeout within a 5 second\n            window will be in the same bucket, and get notified at the\n            same time. This is only accurate to \"milliseconds\".\n\n        :param chunk_size: when \"doing\" the callbacks in a particular\n            bucket, this controls how many we do at once before yielding to\n            the reactor.\n        \"\"\"\n\n        def get_seconds():\n            return self._config.loop.time()\n\n        return _BatchedTimer(\n            bucket_seconds * 1000.0, chunk_size,\n            seconds_provider=get_seconds,\n            delayed_call_creator=self.call_later,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_failure(self, exception=None):\n        if exception:\n            return FailedFuture(type(exception), exception, None)\n        return FailedFuture(*sys.exc_info())", "response": "Create a failed future."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding callbacks to the given future.", "response": "def add_callbacks(self, future, callback, errback):\n        \"\"\"\n        callback or errback may be None, but at least one must be\n        non-None.\n        \"\"\"\n        def done(f):\n            try:\n                res = f.result()\n                if callback:\n                    callback(res)\n            except Exception:\n                if errback:\n                    errback(create_failure())\n        return future.add_done_callback(done)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gather(self, futures, consume_exceptions=True):\n\n        # from the asyncio docs: \"If return_exceptions is True, exceptions\n        # in the tasks are treated the same as successful results, and\n        # gathered in the result list; otherwise, the first raised\n        # exception will be immediately propagated to the returned\n        # future.\"\n        return asyncio.gather(*futures, return_exceptions=consume_exceptions)", "response": "This returns a Future that waits for all the Futures in the list\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_logging(out=_stdout, level='info'):\n    global _loggers, _observer, _log_level, _started_logging\n\n    if level not in log_levels:\n        raise RuntimeError(\n            \"Invalid log level '{0}'; valid are: {1}\".format(\n                level, ', '.join(log_levels)\n            )\n        )\n\n    if _started_logging:\n        return\n\n    _started_logging = True\n\n    _log_level = level\n    set_global_log_level(_log_level)\n\n    if out:\n        _observer = _LogObserver(out)\n\n    if _NEW_LOGGER:\n        _observers = []\n        if _observer:\n            _observers.append(_observer)\n        globalLogBeginner.beginLoggingTo(_observers)\n    else:\n        assert out, \"out needs to be given a value if using Twisteds before 15.2\"\n        from twisted.python import log\n        log.startLogging(out)", "response": "Start logging to the file - like object in out. By default this is stdout. By default this is stdout. By default this is stdout. By default this is stdout. By default this is stdout. By default this is stdout. By default this is info."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_global_log_level(level):\n    for item in _loggers:\n        if not item._log_level_set_explicitly:\n            item._set_log_level(level)\n    global _log_level\n    _log_level = level", "response": "Sets the global log level on all loggers in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_log_level(self, level, keep=True):\n        self._set_log_level(level)\n        self._log_level_set_explicitly = keep", "response": "Set the log level."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef failure_message(self, fail):\n        try:\n            return u'{0}: {1}'.format(\n                fail.value.__class__.__name__,\n                fail.getErrorMessage(),\n            )\n        except Exception:\n            return 'Failed to produce failure message for \"{0}\"'.format(fail)", "response": "returns a unicode error - message for a failed exception"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef failure_format_traceback(self, fail):\n        try:\n            f = six.StringIO()\n            fail.printTraceback(file=f)\n            return f.getvalue()\n        except Exception:\n            return u\"Failed to format failure traceback for '{0}'\".format(fail)", "response": "Formats the traceback of a failed future."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_batched_timer(self, bucket_seconds, chunk_size=100):\n\n        def get_seconds():\n            return self._get_loop().seconds()\n\n        def create_delayed_call(delay, fun, *args, **kwargs):\n            return self._get_loop().callLater(delay, fun, *args, **kwargs)\n\n        return _BatchedTimer(\n            bucket_seconds * 1000.0, chunk_size,\n            seconds_provider=get_seconds,\n            delayed_call_creator=create_delayed_call,\n        )", "response": "Creates and returns a new _BatchedTimer object that will be used to send the next batch of callbacks to the reactor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_callbacks(self, future, callback, errback):\n        assert future is not None\n        if callback is None:\n            assert errback is not None\n            future.addErrback(errback)\n        else:\n            # Twisted allows errback to be None here\n            future.addCallbacks(callback, errback)\n        return future", "response": "Add a callback or errback to a future."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sleep(self, delay):\n        d = Deferred()\n        self._get_loop().callLater(delay, d.callback, None)\n        return d", "response": "Inline sleep for use in co - routines."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the internal helper s loop name.", "response": "def _get_loop(self):\n        \"\"\"\n        internal helper\n        \"\"\"\n        # we import and assign the default here (and not, e.g., when\n        # making Config) so as to delay importing reactor as long as\n        # possible in case someone is installing a custom one.\n        if self._config.loop is None:\n            from twisted.internet import reactor\n            self._config.loop = reactor\n        return self._config.loop"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall the function func at the next time delay seconds.", "response": "def call_later(self, delay, func, *args, **kwargs):\n        \"\"\"\n        IBatchedTimer API\n        \"\"\"\n        # \"quantize\" the delay to the nearest bucket\n        now = self._get_seconds()\n        real_time = int(now + delay) * 1000\n        real_time -= int(real_time % self._bucket_milliseconds)\n        call = _BatchedCall(self, real_time, lambda: func(*args, **kwargs))\n        try:\n            self._buckets[real_time][1].append(call)\n        except KeyError:\n            # new bucket; need to add \"actual\" underlying IDelayedCall\n            diff = (real_time / 1000.0) - now\n            # we need to clamp this because if we quantized to the\n            # nearest second, but that second is actually (slightly)\n            # less than the current time 'diff' will be negative.\n            delayed_call = self._create_delayed_call(\n                max(0.0, diff),\n                self._notify_bucket, real_time,\n            )\n            self._buckets[real_time] = (delayed_call, [call])\n        return call"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _remove_call(self, real_time, call):\n        try:\n            (delayed_call, calls) = self._buckets[real_time]\n        except KeyError:\n            # no such bucket ... error? swallow?\n            return\n        # remove call; if we're empty, cancel underlying\n        # bucket-timeout IDelayedCall\n        calls.remove(call)\n        if not calls:\n            del self._buckets[real_time]\n            delayed_call.cancel()", "response": "Internal helper to remove a call from the a\n        bucket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_bipole(inp, name):\n\n    def chck_dipole(inp, name):\n        r\"\"\"Check inp for shape and type.\"\"\"\n        # Check x\n        inp[0] = _check_var(inp[0], float, 1, name+'-x')\n\n        # Check y and ensure it has same dimension as x\n        inp[1] = _check_var(inp[1], float, 1, name+'-y', inp[0].shape)\n\n        # Check z\n        inp[2] = _check_var(inp[2], float, 1, name+'-z', (1,), inp[0].shape)\n\n        # Check if all depths are the same, if so replace by one value\n        if np.all(np.isclose(inp[2]-inp[2][0], 0)):\n            inp[2] = np.array([inp[2][0]])\n\n        return inp\n\n    # Check length of inp.\n    narr = len(inp)\n    if narr not in [5, 6]:\n        print('* ERROR   :: Parameter ' + name + ' has wrong length! : ' +\n              str(narr) + ' instead of 5 (dipole) or 6 (bipole).')\n        raise ValueError(name)\n\n    # Flag if it is a dipole or not\n    isdipole = narr == 5\n\n    if isdipole:  # dipole checks\n        # Check x, y, and z\n        inp = chck_dipole(inp, name)\n\n        # Check azimuth and dip (must be floats, otherwise use ``bipole``)\n        inp[3] = _check_var(inp[3], float, 1, 'azimuth', (1,))\n        inp[4] = _check_var(inp[4], float, 1, 'dip', (1,))\n\n        # How many different depths\n        inpz = inp[2].size\n\n    else:         # bipole checks\n        # Check each pole for x, y, and z\n        inp0 = chck_dipole(inp[::2], name+'-1')   # [x0, y0, z0]\n        inp1 = chck_dipole(inp[1::2], name+'-2')  # [x1, y1, z1]\n\n        # If one pole has a single depth, but the other has various\n        # depths, we have to repeat the single depth, as we will have\n        # to loop over them.\n        if inp0[2].size != inp1[2].size:\n            if inp0[2].size == 1:\n                inp0[2] = np.repeat(inp0[2], inp1[2].size)\n            else:\n                inp1[2] = np.repeat(inp1[2], inp0[2].size)\n\n        # Check if inp is a dipole instead of a bipole\n        # (This is a problem, as we would could not define the angles then.)\n        if not np.all((inp0[0] != inp1[0]) + (inp0[1] != inp1[1]) +\n                      (inp0[2] != inp1[2])):\n            print(\"* ERROR   :: At least one of <\" + name + \"> is a point \" +\n                  \"dipole, use the format [x, y, z, azimuth, dip] instead \" +\n                  \"of [x0, x1, y0, y1, z0, z1].\")\n            raise ValueError('Bipole: bipole-' + name)\n\n        # Collect elements\n        inp = [inp0[0], inp1[0], inp0[1], inp1[1], inp0[2], inp1[2]]\n\n        # How many different depths\n        inpz = inp[4].size\n\n    return inp, inp[0].size, inpz, isdipole", "response": "r Check di - and bipole parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_dipole(inp, name, verb):\n\n    # Check inp for x, y, and z; x & y must have same length, z is a float\n    _check_shape(np.squeeze(inp), name, (3,))\n    inp[0] = _check_var(inp[0], float, 1, name+'-x')\n    inp[1] = _check_var(inp[1], float, 1, name+'-y', inp[0].shape)\n    inp[2] = _check_var(inp[2], float, 1, name+'-z', (1,))\n\n    # Print spatial parameters\n    if verb > 2:\n        # Pole-type: src or rec\n        if name == 'src':\n            longname = '   Source(s)       : '\n        else:\n            longname = '   Receiver(s)     : '\n\n        print(longname, str(inp[0].size), 'dipole(s)')\n        tname = ['x  ', 'y  ', 'z  ']\n        for i in range(3):\n            text = \"     > \" + tname[i] + \"     [m] : \"\n            _prnt_min_max_val(inp[i], text, verb)\n\n    return inp, inp[0].size", "response": "r Check dipole parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_frequency(freq, res, aniso, epermH, epermV, mpermH, mpermV, verb):\n    global _min_freq\n\n    # Check if the user provided a model for etaH/etaV/zetaH/zetaV\n    if isinstance(res, dict):\n        res = res['res']\n\n    # Check frequency\n    freq = _check_var(freq, float, 1, 'freq')\n\n    # Minimum frequency to avoid division by zero at freq = 0 Hz.\n    # => min_freq can be set with utils.set_min\n    freq = _check_min(freq, _min_freq, 'Frequencies', 'Hz', verb)\n    if verb > 2:\n        _prnt_min_max_val(freq, \"   frequency  [Hz] : \", verb)\n\n    # Calculate eta and zeta (horizontal and vertical)\n    c = 299792458              # Speed of light m/s\n    mu_0 = 4e-7*np.pi          # Magn. permeability of free space [H/m]\n    epsilon_0 = 1./(mu_0*c*c)  # Elec. permittivity of free space [F/m]\n\n    etaH = 1/res + np.outer(2j*np.pi*freq, epermH*epsilon_0)\n    etaV = 1/(res*aniso*aniso) + np.outer(2j*np.pi*freq, epermV*epsilon_0)\n    zetaH = np.outer(2j*np.pi*freq, mpermH*mu_0)\n    zetaV = np.outer(2j*np.pi*freq, mpermV*mu_0)\n\n    return freq, etaH, etaV, zetaH, zetaV", "response": "r Check the frequency of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_model(depth, res, aniso, epermH, epermV, mpermH, mpermV, xdirect,\n                verb):\n    r\"\"\"Check the model: depth and corresponding layer parameters.\n\n    This check-function is called from one of the modelling routines in\n    :mod:`model`.  Consult these modelling routines for a detailed description\n    of the input parameters.\n\n    Parameters\n    ----------\n    depth : list\n        Absolute layer interfaces z (m); #depth = #res - 1\n        (excluding +/- infinity).\n\n    res : array_like\n        Horizontal resistivities rho_h (Ohm.m); #res = #depth + 1.\n\n    aniso : array_like\n        Anisotropies lambda = sqrt(rho_v/rho_h) (-); #aniso = #res.\n\n    epermH, epermV : array_like\n        Relative horizontal/vertical electric permittivities\n        epsilon_h/epsilon_v (-);\n        #epermH = #epermV = #res.\n\n    mpermH, mpermV : array_like\n        Relative horizontal/vertical magnetic permeabilities mu_h/mu_v (-);\n        #mpermH = #mpermV = #res.\n\n    xdirect : bool, optional\n        If True and source and receiver are in the same layer, the direct field\n        is calculated analytically in the frequency domain, if False it is\n        calculated in the wavenumber domain.\n\n    verb : {0, 1, 2, 3, 4}\n        Level of verbosity.\n\n\n    Returns\n    -------\n    depth : array\n        Depths of layer interfaces, adds -infty at beginning if not present.\n\n    res : array\n        As input, checked for size.\n\n    aniso : array\n        As input, checked for size. If None, defaults to an array of ones.\n\n    epermH, epermV : array_like\n        As input, checked for size. If None, defaults to an array of ones.\n\n    mpermH, mpermV : array_like\n        As input, checked for size. If None, defaults to an array of ones.\n\n    isfullspace : bool\n        If True, the model is a fullspace (res, aniso, epermH, epermV, mpermM,\n        and mpermV are in all layers the same).\n\n    \"\"\"\n    global _min_res\n\n    # Check depth\n    if depth is None:\n        depth = []\n    depth = _check_var(depth, float, 1, 'depth')\n\n    # Add -infinity at the beginning\n    # => The top-layer (-infinity to first interface) is layer 0.\n    if depth.size == 0:\n        depth = np.array([-np.infty, ])\n    elif depth[0] != -np.infty:\n        depth = np.insert(depth, 0, -np.infty)\n\n    # Ensure depth is increasing\n    if np.any(depth[1:] - depth[:-1] < 0):\n        print('* ERROR   :: <depth> must be increasing;' +\n              ' <depth> provided: ' + _strvar(depth))\n        raise ValueError('depth')\n\n    # Check if the user provided a model for etaH/etaV/zetaH/zetaV\n    if isinstance(res, dict):\n        res_dict, res = res, res['res']\n    else:\n        res_dict = False\n\n    # Cast and check resistivity\n    res = _check_var(res, float, 1, 'res', depth.shape)\n    # => min_res can be set with utils.set_min\n    res = _check_min(res, _min_res, 'Resistivities', 'Ohm.m', verb)\n\n    # Check optional parameters anisotropy, electric permittivity, and magnetic\n    # permeability\n    def check_inp(var, name, min_val):\n        r\"\"\"Param-check function. Default to ones if not provided\"\"\"\n        if var is None:\n            return np.ones(depth.size)\n        else:\n            param = _check_var(var, float, 1, name, depth.shape)\n            if name == 'aniso':  # Convert aniso into vertical resistivity\n                param = param**2*res\n            param = _check_min(param, min_val, 'Parameter ' + name, '', verb)\n            if name == 'aniso':  # Convert vert. resistivity back to aniso\n                param = np.sqrt(param/res)\n            return param\n\n    # => min_res can be set with utils.set_min\n    aniso = check_inp(aniso, 'aniso', _min_res)\n    epermH = check_inp(epermH, 'epermH', 0.0)\n    epermV = check_inp(epermV, 'epermV', 0.0)\n    mpermH = check_inp(mpermH, 'mpermH', 0.0)\n    mpermV = check_inp(mpermV, 'mpermV', 0.0)\n\n    # Print model parameters\n    if verb > 2:\n        print(\"   depth       [m] : \", _strvar(depth[1:]))\n        print(\"   res     [Ohm.m] : \", _strvar(res))\n        print(\"   aniso       [-] : \", _strvar(aniso))\n        print(\"   epermH      [-] : \", _strvar(epermH))\n        print(\"   epermV      [-] : \", _strvar(epermV))\n        print(\"   mpermH      [-] : \", _strvar(mpermH))\n        print(\"   mpermV      [-] : \", _strvar(mpermV))\n\n    # Check if medium is a homogeneous full-space. If that is the case, the\n    # EM-field is computed analytically directly in the frequency-domain.\n    # Note: Also a stack of layers with the same material parameters is treated\n    #       as a homogeneous full-space.\n    isores = (res - res[0] == 0).all()*(aniso - aniso[0] == 0).all()\n    isoep = (epermH - epermH[0] == 0).all()*(epermV - epermV[0] == 0).all()\n    isomp = (mpermH - mpermH[0] == 0).all()*(mpermV - mpermV[0] == 0).all()\n    isfullspace = isores*isoep*isomp\n\n    # Check parameters of user-provided parameters\n    if res_dict:\n        # Switch off fullspace-option\n        isfullspace = False\n\n        # Loop over key, value pair and check\n        for key, value in res_dict.items():\n            if key not in ['res', 'func_eta', 'func_zeta']:\n                res_dict[key] = check_inp(value, key, None)\n\n        # Put res back\n        res_dict['res'] = res\n\n        # store res_dict back to res\n        res = res_dict\n\n    # Print fullspace info\n    if verb > 2 and isfullspace:\n        if xdirect:\n            print(\"\\n>  MODEL IS A FULLSPACE; returning analytical \" +\n                  \"frequency-domain solution\")\n        else:\n            print(\"\\n>  MODEL IS A FULLSPACE\")\n\n    # Print xdirect info\n    if verb > 2:\n        if xdirect is None:\n            print(\"   direct field    :  Not calculated (secondary field)\")\n        elif xdirect:\n            print(\"   direct field    :  Calc. in frequency domain\")\n        else:\n            print(\"   direct field    :  Calc. in wavenumber domain\")\n\n    return depth, res, aniso, epermH, epermV, mpermH, mpermV, isfullspace", "response": "r Check the model for the depth and corresponding layer parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_opt(opt, loop, ht, htarg, verb):\n\n    # Check optimization flag\n    use_ne_eval = False\n    if opt == 'parallel':\n        if numexpr:\n            use_ne_eval = numexpr.evaluate\n        elif verb > 0:\n            print(numexpr_msg)\n\n    # Define if to loop over frequencies or over offsets\n    lagged_splined_fht = False\n    if ht == 'fht':\n        if htarg[1] != 0:\n            lagged_splined_fht = True\n\n    if ht in ['hqwe', 'hquad'] or lagged_splined_fht:\n        loop_freq = True\n        loop_off = False\n    else:\n        loop_off = loop == 'off'\n        loop_freq = loop == 'freq'\n\n    # If verbose, print optimization information\n    if verb > 2:\n        if use_ne_eval:\n            print(\"   Kernel Opt.     :  Use parallel\")\n        else:\n            print(\"   Kernel Opt.     :  None\")\n\n        if loop_off:\n            print(\"   Loop over       :  Offsets\")\n        elif loop_freq:\n            print(\"   Loop over       :  Frequencies\")\n        else:\n            print(\"   Loop over       :  None (all vectorized)\")\n\n    return use_ne_eval, loop_freq, loop_off", "response": "r Check optimization parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_time(time, signal, ft, ftarg, verb):\n\n    # Check time and input signal\n    time = check_time_only(time, signal, verb)\n\n    # Ensure ft is all lowercase\n    ft = ft.lower()\n\n    if ft in ['cos', 'sin', 'ffht']:  # Fourier-FHT (Sine/Cosine-filters)\n\n        # If `ft='ffht'`, we assume that it run the check before, and get\n        # sin/cos from ftarg. If not, defaults to 'sin'. To ensure that this\n        # check can be re-run without failing.\n        if ft == 'ffht':\n            try:\n                ft = ftarg[2]\n            except VariableCatch:\n                ft = 'sin'\n\n        # If switch-off/on is required, ensure ft is cosine/sine\n        if signal > 0:\n            ft = 'sin'\n        elif signal < 0:\n            ft = 'cos'\n\n        # Check Input\n        ftarg = _check_targ(ftarg, ['fftfilt', 'pts_per_dec', 'ft'])\n\n        # Check filter; defaults to key_201_CosSin_2012\n        try:\n            fftfilt = ftarg['fftfilt']\n            if not hasattr(fftfilt, 'base'):\n                fftfilt = getattr(filters, fftfilt)()\n        except VariableCatch:\n            fftfilt = filters.key_201_CosSin_2012()\n\n        # Check pts_per_dec; defaults to -1\n        try:\n            pts_per_dec = _check_var(ftarg['pts_per_dec'], float, 0,\n                                     ft + 'pts_per_dec', ())\n        except VariableCatch:\n            pts_per_dec = -1.0\n\n        # Assemble ftarg\n        ftarg = (fftfilt, pts_per_dec, ft)\n\n        # If verbose, print Fourier transform information\n        if verb > 2:\n            if ft == 'sin':\n                print(\"   Fourier         :  DLF (Sine-Filter)\")\n            else:\n                print(\"   Fourier         :  DLF (Cosine-Filter)\")\n            print(\"     > Filter      :  \" + fftfilt.name)\n            pstr = \"     > DLF type    :  \"\n            if pts_per_dec < 0:\n                print(pstr + \"Lagged Convolution\")\n            elif pts_per_dec > 0:\n                print(pstr + \"Splined, \" + str(pts_per_dec) + \" pts/dec\")\n            else:\n                print(pstr + \"Standard\")\n\n        # Get required frequencies\n        # (multiply time by 2Pi, as calculation is done in angular frequencies)\n        freq, _ = transform.get_spline_values(ftarg[0], 2*np.pi*time, ftarg[1])\n        freq = np.squeeze(freq)\n\n        # Rename ft\n        ft = 'ffht'\n\n    elif ft in ['qwe', 'fqwe']:       # QWE (using sine and imag-part)\n        # Rename ft\n        ft = 'fqwe'\n\n        # Get and check input or set defaults\n        ftarg = _check_targ(ftarg, ['rtol', 'atol', 'nquad', 'maxint',\n                                    'pts_per_dec', 'diff_quad', 'a', 'b',\n                                    'limit', 'sincos'])\n\n        # If switch-off is required, use cosine, else sine\n        if signal >= 0:\n            sincos = np.sin\n        elif signal < 0:\n            sincos = np.cos\n\n        try:  # rtol\n            rtol = _check_var(ftarg['rtol'], float, 0, 'qwe: rtol', ())\n        except VariableCatch:\n            rtol = np.array(1e-8, dtype=float)\n\n        try:  # atol\n            atol = _check_var(ftarg['atol'], float, 0, 'qwe: atol', ())\n        except VariableCatch:\n            atol = np.array(1e-20, dtype=float)\n\n        try:  # nquad\n            nquad = _check_var(ftarg['nquad'], int, 0, 'qwe: nquad', ())\n        except VariableCatch:\n            nquad = np.array(21, dtype=int)\n\n        try:  # maxint\n            maxint = _check_var(ftarg['maxint'], int, 0, 'qwe: maxint', ())\n        except VariableCatch:\n            maxint = np.array(200, dtype=int)\n\n        try:  # pts_per_dec\n            pts_per_dec = _check_var(ftarg['pts_per_dec'], int, 0,\n                                     'qwe: pts_per_dec', ())\n            pts_per_dec = _check_min(pts_per_dec, 1, 'pts_per_dec', '', verb)\n        except VariableCatch:\n            pts_per_dec = np.array(20, dtype=int)\n\n        # diff_quad : 100\n        try:\n            diff_quad = _check_var(ftarg['diff_quad'], int, 0,\n                                   'qwe: diff_quad', ())\n        except VariableCatch:\n            diff_quad = np.array(100, dtype=int)\n\n        # a : None\n        try:\n            a = _check_var(ftarg['a'], float, 0, 'qwe: a (quad)', ())\n        except VariableCatch:\n            a = None\n\n        # b : None\n        try:\n            b = _check_var(ftarg['b'], float, 0, 'qwe: b (quad)', ())\n        except VariableCatch:\n            b = None\n\n        # limit : None\n        try:\n            limit = _check_var(ftarg['limit'], float, 0, 'qwe: limit (quad)',\n                               ())\n        except VariableCatch:\n            limit = None\n\n        # Assemble ftarg\n        ftarg = (rtol, atol, nquad, maxint, pts_per_dec, diff_quad, a, b,\n                 limit, sincos)\n\n        # If verbose, print Fourier transform information\n        if verb > 2:\n            print(\"   Fourier         :  Quadrature-with-Extrapolation\")\n            print(\"     > rtol        :  \" + str(rtol))\n            print(\"     > atol        :  \" + str(atol))\n            print(\"     > nquad       :  \" + str(nquad))\n            print(\"     > maxint      :  \" + str(maxint))\n            print(\"     > pts_per_dec :  \" + str(pts_per_dec))\n            print(\"     > diff_quad   :  \" + str(diff_quad))\n            if a:\n                print(\"     > a     (quad):  \" + str(a))\n            if b:\n                print(\"     > b     (quad):  \" + str(b))\n            if limit:\n                print(\"     > limit (quad):  \" + str(limit))\n\n        # Get required frequencies\n        g_x, _ = special.p_roots(nquad)\n        minf = np.floor(10*np.log10((g_x.min() + 1)*np.pi/2/time.max()))/10\n        maxf = np.ceil(10*np.log10(maxint*np.pi/time.min()))/10\n        freq = np.logspace(minf, maxf, (maxf-minf)*pts_per_dec + 1)\n\n    elif ft == 'fftlog':              # FFTLog (using sine and imag-part)\n\n        # Get and check input or set defaults\n        ftarg = _check_targ(ftarg, ['pts_per_dec', 'add_dec', 'q', 'mu',\n                                    'tcalc', 'dlnr', 'kr', 'rk'])\n\n        try:  # pts_per_dec\n            pts_per_dec = _check_var(ftarg['pts_per_dec'], int, 0,\n                                     'fftlog: pts_per_dec', ())\n            pts_per_dec = _check_min(pts_per_dec, 1, 'pts_per_dec', '', verb)\n        except VariableCatch:\n            pts_per_dec = np.array(10, dtype=int)\n\n        try:  # add_dec\n            add_dec = _check_var(ftarg['add_dec'], float, 1, 'fftlog: add_dec',\n                                 (2,))\n        except VariableCatch:\n            add_dec = np.array([-2, 1], dtype=float)\n\n        try:  # q\n            q = _check_var(ftarg['q'], float, 0, 'fftlog: q', ())\n            # Restrict q to +/- 1\n            if np.abs(q) > 1:\n                q = np.sign(q)\n        except VariableCatch:\n            q = np.array(0, dtype=float)\n\n        # If switch-off is required, use cosine, else sine\n        if signal >= 0:\n            mu = 0.5\n        elif signal < 0:\n            mu = -0.5\n\n        # If verbose, print Fourier transform information\n        if verb > 2:\n            print(\"   Fourier         :  FFTLog\")\n            print(\"     > pts_per_dec :  \" + str(pts_per_dec))\n            print(\"     > add_dec     :  \" + str(add_dec))\n            print(\"     > q           :  \" + str(q))\n\n        # Calculate minimum and maximum required frequency\n        minf = np.log10(1/time.max()) + add_dec[0]\n        maxf = np.log10(1/time.min()) + add_dec[1]\n        n = np.int(maxf - minf)*pts_per_dec\n\n        # Initialize FFTLog, get required parameters\n        freq, tcalc, dlnr, kr, rk = transform.fhti(minf, maxf, n, q, mu)\n\n        # Assemble ftarg\n        # Keep first 3 entries, so re-running this check is stable\n        ftarg = (pts_per_dec, add_dec, q, mu, tcalc, dlnr, kr, rk)\n\n    elif ft == 'fft':                 # FFT\n\n        # Get and check input or set defaults\n        ftarg = _check_targ(ftarg, ['dfreq', 'nfreq', 'ntot', 'pts_per_dec',\n                                    'fftfreq'])\n\n        try:  # dfreq\n            dfreq = _check_var(ftarg['dfreq'], float, 0, 'fft: dfreq', ())\n        except VariableCatch:\n            dfreq = np.array(0.002, dtype=float)\n\n        try:  # nfreq\n            nfreq = _check_var(ftarg['nfreq'], int, 0, 'fft: nfreq', ())\n        except VariableCatch:\n            nfreq = np.array(2048, dtype=int)\n\n        nall = 2**np.arange(30)\n        try:  # ntot\n            ntot = _check_var(ftarg['ntot'], int, 0, 'fft: ntot', ())\n        except VariableCatch:\n            # We could use here fftpack.next_fast_len, but tests have shown\n            # that powers of two yield better results in this case.\n            ntot = nall[np.argmax(nall >= nfreq)]\n        else:  # Assure that input ntot is not bigger than nfreq\n            if nfreq > ntot:\n                ntot = nall[np.argmax(nall >= nfreq)]\n\n        # Check pts_per_dec; defaults to None\n        try:\n            pts_per_dec = _check_var(ftarg['pts_per_dec'], int, 0,\n                                     'fft: pts_per_dec', ())\n            pts_per_dec = _check_min(pts_per_dec, 1, 'pts_per_dec', '', verb)\n        except VariableCatch:\n            pts_per_dec = None\n\n        # Get required frequencies\n        if pts_per_dec:  # Space actually calculated freqs logarithmically.\n            start = np.log10(dfreq)\n            stop = np.log10(nfreq*dfreq)\n            freq = np.logspace(start, stop, (stop-start)*pts_per_dec + 1)\n        else:\n            freq = np.arange(1, nfreq+1)*dfreq\n\n        # Assemble ftarg\n        ftarg = (dfreq, nfreq, ntot, pts_per_dec)\n\n        # If verbose, print Fourier transform information\n        if verb > 2:\n            print(\"   Fourier         :  Fast Fourier Transform FFT\")\n            print(\"     > dfreq       :  \" + str(ftarg[0]))\n            print(\"     > nfreq       :  \" + str(ftarg[1]))\n            print(\"     > ntot        :  \" + str(ftarg[2]))\n            if pts_per_dec:\n                print(\"     > pts_per_dec :  \" + str(ftarg[3]))\n            else:\n                print(\"     > pts_per_dec :  (linear)\")\n\n    else:\n        print(\"* ERROR   :: <ft> must be one of: ['cos', 'sin', 'qwe', \" +\n              \"'fftlog', 'fft']; <ft> provided: \"+str(ft))\n        raise ValueError('ft')\n\n    return time, freq, ft, ftarg", "response": "r Check time and signal for valid time domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_time_only(time, signal, verb):\n    global _min_time\n\n    # Check input signal\n    if int(signal) not in [-1, 0, 1]:\n        print(\"* ERROR   :: <signal> must be one of: [None, -1, 0, 1]; \" +\n              \"<signal> provided: \"+str(signal))\n        raise ValueError('signal')\n\n    # Check time\n    time = _check_var(time, float, 1, 'time')\n\n    # Minimum time to avoid division by zero  at time = 0 s.\n    # => min_time can be set with utils.set_min\n    time = _check_min(time, _min_time, 'Times', 's', verb)\n    if verb > 2:\n        _prnt_min_max_val(time, \"   time        [s] : \", verb)\n\n    return time", "response": "r Check time and signal parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_solution(solution, signal, ab, msrc, mrec):\n\n    # Ensure valid solution.\n    if solution not in ['fs', 'dfs', 'dhs', 'dsplit', 'dtetm']:\n        print(\"* ERROR   :: Solution must be one of ['fs', 'dfs', 'dhs', \" +\n              \"'dsplit', 'dtetm']; <solution> provided: \" + solution)\n        raise ValueError('solution')\n\n    # If diffusive solution is required, ensure EE-field.\n    if solution[0] == 'd' and (msrc or mrec):\n        print('* ERROR   :: Diffusive solution is only implemented for ' +\n              'electric sources and electric receivers, <ab> provided: ' +\n              str(ab))\n        raise ValueError('ab')\n\n    # If full solution is required, ensure frequency-domain.\n    if solution == 'fs' and signal is not None:\n        print('* ERROR   :: Full fullspace solution is only implemented for ' +\n              'the frequency domain, <signal> provided: ' + str(signal))\n        raise ValueError('signal')", "response": "r Check required solution with parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_abs(msrc, mrec, srcazm, srcdip, recazm, recdip, verb):\n\n    # Get required ab's (9 at most)\n    ab_calc = np.array([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n    if msrc:\n        ab_calc += 3\n    if mrec:\n        ab_calc += 30\n\n        # Switch <ab> using reciprocity.\n        if msrc:\n            # G^mm_ab(s, r, e, z) = -G^ee_ab(s, r, -z, -e)\n            ab_calc -= 33  # -30 : mrec->erec; -3: msrc->esrc\n        else:\n            # G^me_ab(s, r, e, z) = -G^em_ba(r, s, e, z)\n            ab_calc = ab_calc % 10*10 + ab_calc // 10  # Swap alpha/beta\n\n    # Remove unnecessary ab's\n    bab = np.asarray(ab_calc*0+1, dtype=bool)\n\n    # Remove if source is x- or y-directed\n    check = np.atleast_1d(srcazm)[0]\n    if np.allclose(srcazm % (np.pi/2), 0):  # if all angles are multiples of 90\n        if np.isclose(check // (np.pi/2) % 2, 0):  # Multiples of pi (180)\n            bab[:, 1] *= False        # x-directed source, remove y\n        else:                                      # Multiples of pi/2 (90)\n            bab[:, 0] *= False        # y-directed source, remove x\n\n    # Remove if source is vertical\n    check = np.atleast_1d(srcdip)[0]\n    if np.allclose(srcdip % (np.pi/2), 0):  # if all angles are multiples of 90\n        if np.isclose(check // (np.pi/2) % 2, 0):  # Multiples of pi (180)\n            bab[:, 2] *= False        # Horizontal, remove z\n        else:                                      # Multiples of pi/2 (90)\n            bab[:, :2] *= False       # Vertical, remove x/y\n\n    # Remove if receiver is x- or y-directed\n    check = np.atleast_1d(recazm)[0]\n    if np.allclose(recazm % (np.pi/2), 0):  # if all angles are multiples of 90\n        if np.isclose(check // (np.pi/2) % 2, 0):  # Multiples of pi (180)\n            bab[1, :] *= False        # x-directed receiver, remove y\n        else:                                      # Multiples of pi/2 (90)\n            bab[0, :] *= False        # y-directed receiver, remove x\n\n    # Remove if receiver is vertical\n    check = np.atleast_1d(recdip)[0]\n    if np.allclose(recdip % (np.pi/2), 0):  # if all angles are multiples of 90\n        if np.isclose(check // (np.pi/2) % 2, 0):  # Multiples of pi (180)\n            bab[2, :] *= False        # Horizontal, remove z\n        else:                                      # Multiples of pi/2 (90)\n            bab[:2, :] *= False       # Vertical, remove x/y\n\n    # Reduce\n    ab_calc = ab_calc[bab].ravel()\n\n    # Print actual calculated <ab>\n    if verb > 2:\n        print(\"   Required ab's   : \", _strvar(ab_calc))\n\n    return ab_calc", "response": "r Return the required abs for given angles."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef printstartfinish(verb, inp=None, kcount=None):\n    if inp:\n        if verb > 1:\n            ttxt = str(timedelta(seconds=default_timer() - inp))\n            ktxt = ' '\n            if kcount:\n                ktxt += str(kcount) + ' kernel call(s)'\n            print('\\n:: empymod END; runtime = ' + ttxt + ' ::' + ktxt + '\\n')\n    else:\n        t0 = default_timer()\n        if verb > 2:\n            print(\"\\n:: empymod START  ::\\n\")\n        return t0", "response": "r Print start and finish with time measure and kernel count."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_minimum(min_freq=None, min_time=None, min_off=None, min_res=None,\n                min_angle=None):\n    r\"\"\"\n    Set minimum values of parameters.\n\n    The given parameters are set to its minimum value if they are smaller.\n\n    Parameters\n    ----------\n    min_freq : float, optional\n        Minimum frequency [Hz] (default 1e-20 Hz).\n    min_time : float, optional\n        Minimum time [s] (default 1e-20 s).\n    min_off : float, optional\n        Minimum offset [m] (default 1e-3 m).\n        Also used to round src- & rec-coordinates.\n    min_res : float, optional\n        Minimum horizontal and vertical resistivity [Ohm.m] (default 1e-20).\n    min_angle : float, optional\n        Minimum angle [-] (default 1e-10).\n\n    Note\n    ----\n    set_minimum and get_minimum are derived after set_printoptions and\n    get_printoptions from arrayprint.py in numpy.\n\n    \"\"\"\n\n    global _min_freq, _min_time, _min_off, _min_res, _min_angle\n\n    if min_freq is not None:\n        _min_freq = min_freq\n    if min_time is not None:\n        _min_time = min_time\n    if min_off is not None:\n        _min_off = min_off\n    if min_res is not None:\n        _min_res = min_res\n    if min_angle is not None:\n        _min_angle = min_angle", "response": "r Sets the minimum value of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_var(var, dtype, ndmin, name, shape=None, shape2=None):\n    if var is None:\n        raise ValueError\n    var = np.array(var, dtype=dtype, copy=True, ndmin=ndmin)\n    if shape:\n        _check_shape(var, name, shape, shape2)\n    return var", "response": "r Return variable as array of dtype ndmin ; shape - checked."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _strvar(a, prec='{:G}'):\n    return ' '.join([prec.format(i) for i in np.atleast_1d(a)])", "response": "r Return variable as a string to print with given precision."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prnt_min_max_val(var, text, verb):\n    if var.size > 3:\n        print(text, _strvar(var.min()), \"-\", _strvar(var.max()),\n              \":\", _strvar(var.size), \" [min-max; #]\")\n        if verb > 3:\n            print(\"                   : \", _strvar(var))\n    else:\n        print(text, _strvar(np.atleast_1d(var)))", "response": "r Print variable ; if more than three just min - max unless verb > 3"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bipole(src, rec, depth, res, freqtime, signal=None, aniso=None,\n           epermH=None, epermV=None, mpermH=None, mpermV=None, msrc=False,\n           srcpts=1, mrec=False, recpts=1, strength=0, xdirect=False,\n           ht='fht', htarg=None, ft='sin', ftarg=None, opt=None, loop=None,\n           verb=2):\n    r\"\"\"Return the electromagnetic field due to an electromagnetic source.\n\n    Calculate the electromagnetic frequency- or time-domain field due to\n    arbitrary finite electric or magnetic bipole sources, measured by arbitrary\n    finite electric or magnetic bipole receivers. By default, the\n    electromagnetic response is normalized to to source and receiver of 1 m\n    length, and source strength of 1 A.\n\n\n    See Also\n    --------\n    fem : Electromagnetic frequency-domain response.\n    tem : Electromagnetic time-domain response.\n\n\n    Parameters\n    ----------\n    src, rec : list of floats or arrays\n        Source and receiver coordinates (m):\n            - [x0, x1, y0, y1, z0, z1] (bipole of finite length)\n            - [x, y, z, azimuth, dip]  (dipole, infinitesimal small)\n\n        Dimensions:\n            - The coordinates x, y, and z (dipole) or x0, x1, y0, y1, z0, and\n              z1 (bipole) can be single values or arrays.\n            - The variables x and y (dipole) or x0, x1, y0, and y1 (bipole)\n              must have the same dimensions.\n            - The variable z (dipole) or z0 and z1 (bipole) must either be\n              single values or having the same dimension as the other\n              coordinates.\n            - The variables azimuth and dip must be single values. If they have\n              different angles, you have to use the bipole-method (with\n              srcpts/recpts = 1, so it is calculated as dipoles).\n\n        Angles (coordinate system is left-handed, positive z down\n        (East-North-Depth):\n\n            - azimuth (\u00b0): horizontal deviation from x-axis, anti-clockwise.\n            - dip (\u00b0): vertical deviation from xy-plane downwards.\n\n        Sources or receivers placed on a layer interface are considered in the\n        upper layer.\n\n    depth : list\n        Absolute layer interfaces z (m); #depth = #res - 1\n        (excluding +/- infinity).\n\n    res : array_like\n        Horizontal resistivities rho_h (Ohm.m); #res = #depth + 1.\n\n        Alternatively, res can be a dictionary. See the main manual of empymod\n        too see how to exploit this hook to re-calculate etaH, etaV, zetaH, and\n        zetaV, which can be used to, for instance, use the Cole-Cole model for\n        IP.\n\n    freqtime : array_like\n        Frequencies f (Hz) if ``signal`` == None, else times t (s); (f, t > 0).\n\n    signal : {None, 0, 1, -1}, optional\n        Source signal, default is None:\n            - None: Frequency-domain response\n            - -1 : Switch-off time-domain response\n            - 0 : Impulse time-domain response\n            - +1 : Switch-on time-domain response\n\n    aniso : array_like, optional\n        Anisotropies lambda = sqrt(rho_v/rho_h) (-); #aniso = #res.\n        Defaults to ones.\n\n    epermH, epermV : array_like, optional\n        Relative horizontal/vertical electric permittivities\n        epsilon_h/epsilon_v (-);\n        #epermH = #epermV = #res. Default is ones.\n\n    mpermH, mpermV : array_like, optional\n        Relative horizontal/vertical magnetic permeabilities mu_h/mu_v (-);\n        #mpermH = #mpermV = #res. Default is ones.\n\n    msrc, mrec : boolean, optional\n        If True, source/receiver (msrc/mrec) is magnetic, else electric.\n        Default is False.\n\n    srcpts, recpts : int, optional\n        Number of integration points for bipole source/receiver, default is 1:\n            - srcpts/recpts < 3  : bipole, but calculated as dipole at centre\n            - srcpts/recpts >= 3 : bipole\n\n    strength : float, optional\n        Source strength (A):\n          - If 0, output is normalized to source and receiver of 1 m length,\n            and source strength of 1 A.\n          - If != 0, output is returned for given source and receiver length,\n            and source strength.\n\n        Default is 0.\n\n    xdirect : bool or None, optional\n        Direct field calculation (only if src and rec are in the same layer):\n          - If True, direct field is calculated analytically in the frequency\n            domain.\n          - If False, direct field is calculated in the wavenumber domain.\n          - If None, direct field is excluded from the calculation, and only\n            reflected fields are returned (secondary field).\n\n        Defaults to False.\n\n    ht : {'fht', 'qwe', 'quad'}, optional\n        Flag to choose either the *Digital Linear Filter* method (FHT, *Fast\n        Hankel Transform*), the *Quadrature-With-Extrapolation* (QWE), or a\n        simple *Quadrature* (QUAD) for the Hankel transform.  Defaults to\n        'fht'.\n\n    htarg : dict or list, optional\n        Depends on the value for ``ht``:\n            - If ``ht`` = 'fht': [fhtfilt, pts_per_dec]:\n\n                - fhtfilt: string of filter name in ``empymod.filters`` or\n                           the filter method itself.\n                           (default: ``empymod.filters.key_201_2009()``)\n                - pts_per_dec: points per decade; (default: 0)\n                    - If 0: Standard DLF.\n                    - If < 0: Lagged Convolution DLF.\n                    - If > 0: Splined DLF\n\n            - If ``ht`` = 'qwe': [rtol, atol, nquad, maxint, pts_per_dec,\n                                diff_quad, a, b, limit]:\n\n                - rtol: relative tolerance (default: 1e-12)\n                - atol: absolute tolerance (default: 1e-30)\n                - nquad: order of Gaussian quadrature (default: 51)\n                - maxint: maximum number of partial integral intervals\n                          (default: 40)\n                - pts_per_dec: points per decade; (default: 0)\n                    - If 0, no interpolation is used.\n                    - If > 0, interpolation is used.\n\n                - diff_quad: criteria when to swap to QUAD (only relevant if\n                  opt='spline') (default: 100)\n                - a: lower limit for QUAD (default: first interval from QWE)\n                - b: upper limit for QUAD (default: last interval from QWE)\n                - limit: limit for quad (default: maxint)\n\n            - If ``ht`` = 'quad': [atol, rtol, limit, lmin, lmax, pts_per_dec]:\n\n                - rtol: relative tolerance (default: 1e-12)\n                - atol: absolute tolerance (default: 1e-20)\n                - limit: An upper bound on the number of subintervals used in\n                  the adaptive algorithm (default: 500)\n                - lmin: Minimum wavenumber (default 1e-6)\n                - lmax: Maximum wavenumber (default 0.1)\n                - pts_per_dec: points per decade (default: 40)\n\n        The values can be provided as dict with the keywords, or as list.\n        However, if provided as list, you have to follow the order given above.\n        A few examples, assuming ``ht`` = ``qwe``:\n\n            - Only changing rtol:\n                {'rtol': 1e-4} or [1e-4] or 1e-4\n            - Changing rtol and nquad:\n                {'rtol': 1e-4, 'nquad': 101} or [1e-4, '', 101]\n            - Only changing diff_quad:\n                {'diffquad': 10} or ['', '', '', '', '', 10]\n\n    ft : {'sin', 'cos', 'qwe', 'fftlog', 'fft'}, optional\n        Only used if ``signal`` != None. Flag to choose either the Digital\n        Linear Filter method (Sine- or Cosine-Filter), the\n        Quadrature-With-Extrapolation (QWE), the FFTLog, or the FFT for the\n        Fourier transform.  Defaults to 'sin'.\n\n    ftarg : dict or list, optional\n        Only used if ``signal`` !=None. Depends on the value for ``ft``:\n            - If ``ft`` = 'sin' or 'cos': [fftfilt, pts_per_dec]:\n\n                - fftfilt: string of filter name in ``empymod.filters`` or\n                           the filter method itself.\n                           (Default: ``empymod.filters.key_201_CosSin_2012()``)\n                - pts_per_dec: points per decade; (default: -1)\n                    - If 0: Standard DLF.\n                    - If < 0: Lagged Convolution DLF.\n                    - If > 0: Splined DLF\n\n\n            - If ``ft`` = 'qwe': [rtol, atol, nquad, maxint, pts_per_dec]:\n\n                - rtol: relative tolerance (default: 1e-8)\n                - atol: absolute tolerance (default: 1e-20)\n                - nquad: order of Gaussian quadrature (default: 21)\n                - maxint: maximum number of partial integral intervals\n                          (default: 200)\n                - pts_per_dec: points per decade (default: 20)\n                - diff_quad: criteria when to swap to QUAD (default: 100)\n                - a: lower limit for QUAD (default: first interval from QWE)\n                - b: upper limit for QUAD (default: last interval from QWE)\n                - limit: limit for quad (default: maxint)\n\n            - If ``ft`` = 'fftlog': [pts_per_dec, add_dec, q]:\n\n                - pts_per_dec: sampels per decade (default: 10)\n                - add_dec: additional decades [left, right] (default: [-2, 1])\n                - q: exponent of power law bias (default: 0); -1 <= q <= 1\n\n            - If ``ft`` = 'fft': [dfreq, nfreq, ntot]:\n\n                - dfreq: Linear step-size of frequencies (default: 0.002)\n                - nfreq: Number of frequencies (default: 2048)\n                - ntot:  Total number for FFT; difference between nfreq and\n                         ntot is padded with zeroes. This number is ideally a\n                         power of 2, e.g. 2048 or 4096 (default: nfreq).\n                - pts_per_dec : points per decade (default: None)\n\n                Padding can sometimes improve the result, not always. The\n                default samples from 0.002 Hz - 4.096 Hz. If pts_per_dec is set\n                to an integer, calculated frequencies are logarithmically\n                spaced with the given number per decade, and then interpolated\n                to yield the required frequencies for the FFT.\n\n        The values can be provided as dict with the keywords, or as list.\n        However, if provided as list, you have to follow the order given above.\n        See ``htarg`` for a few examples.\n\n    opt : {None, 'parallel'}, optional\n        Optimization flag. Defaults to None:\n            - None: Normal case, no parallelization nor interpolation is used.\n            - If 'parallel', the package ``numexpr`` is used to evaluate the\n              most expensive statements. Always check if it actually improves\n              performance for a specific problem. It can speed up the\n              calculation for big arrays, but will most likely be slower for\n              small arrays. It will use all available cores for these specific\n              statements, which all contain ``Gamma`` in one way or another,\n              which has dimensions (#frequencies, #offsets, #layers, #lambdas),\n              therefore can grow pretty big. The module ``numexpr`` uses by\n              default all available cores up to a maximum of 8. You can change\n              this behaviour to your desired number of threads ``nthreads``\n              with ``numexpr.set_num_threads(nthreads)``.\n            - The value 'spline' is deprecated and will be removed. See\n              ``htarg`` instead for the interpolated versions.\n\n        The option 'parallel' only affects speed and memory usage, whereas\n        'spline' also affects precision!  Please read the note in the *README*\n        documentation for more information.\n\n    loop : {None, 'freq', 'off'}, optional\n        Define if to calculate everything vectorized or if to loop over\n        frequencies ('freq') or over offsets ('off'), default is None. It\n        always loops over frequencies if ``ht = 'qwe'`` or if ``opt =\n        'spline'``. Calculating everything vectorized is fast for few offsets\n        OR for few frequencies. However, if you calculate many frequencies for\n        many offsets, it might be faster to loop over frequencies. Only\n        comparing the different versions will yield the answer for your\n        specific problem at hand!\n\n    verb : {0, 1, 2, 3, 4}, optional\n        Level of verbosity, default is 2:\n            - 0: Print nothing.\n            - 1: Print warnings.\n            - 2: Print additional runtime and kernel calls\n            - 3: Print additional start/stop, condensed parameter information.\n            - 4: Print additional full parameter information\n\n\n    Returns\n    -------\n    EM : ndarray, (nfreq, nrec, nsrc)\n        Frequency- or time-domain EM field (depending on ``signal``):\n            - If rec is electric, returns E [V/m].\n            - If rec is magnetic, returns B [T] (not H [A/m]!).\n\n        However, source and receiver are normalised (unless strength != 0). So\n        for instance in the electric case the source strength is 1 A and its\n        length is 1 m. So the electric field could also be written as\n        [V/(A.m2)].\n\n        In the magnetic case the source strength is given by\n        :math:`i\\omega\\mu_0 A I^e`, where A is the loop area (m2), and\n        :math:`I^e` the electric source strength. For the normalized magnetic\n        source :math:`A=1m^2` and :math:`I^e=1 Ampere`. A magnetic source is\n        therefore frequency dependent.\n\n        The shape of EM is (nfreq, nrec, nsrc). However, single dimensions\n        are removed.\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from empymod import bipole\n    >>> # x-directed bipole source: x0, x1, y0, y1, z0, z1\n    >>> src = [-50, 50, 0, 0, 100, 100]\n    >>> # x-directed dipole source-array: x, y, z, azimuth, dip\n    >>> rec = [np.arange(1, 11)*500, np.zeros(10), 200, 0, 0]\n    >>> # layer boundaries\n    >>> depth = [0, 300, 1000, 1050]\n    >>> # layer resistivities\n    >>> res = [1e20, .3, 1, 50, 1]\n    >>> # Frequency\n    >>> freq = 1\n    >>> # Calculate electric field due to an electric source at 1 Hz.\n    >>> # [msrc = mrec = True (default)]\n    >>> EMfield = bipole(src, rec, depth, res, freq, verb=4)\n    :: empymod START  ::\n    ~\n       depth       [m] :  0 300 1000 1050\n       res     [Ohm.m] :  1E+20 0.3 1 50 1\n       aniso       [-] :  1 1 1 1 1\n       epermH      [-] :  1 1 1 1 1\n       epermV      [-] :  1 1 1 1 1\n       mpermH      [-] :  1 1 1 1 1\n       mpermV      [-] :  1 1 1 1 1\n       frequency  [Hz] :  1\n       Hankel          :  DLF (Fast Hankel Transform)\n         > Filter      :  Key 201 (2009)\n         > DLF type    :  Standard\n       Kernel Opt.     :  None\n       Loop over       :  None (all vectorized)\n       Source(s)       :  1 bipole(s)\n         > intpts      :  1 (as dipole)\n         > length  [m] :  100\n         > x_c     [m] :  0\n         > y_c     [m] :  0\n         > z_c     [m] :  100\n         > azimuth [\u00b0] :  0\n         > dip     [\u00b0] :  0\n       Receiver(s)     :  10 dipole(s)\n         > x       [m] :  500 - 5000 : 10  [min-max; #]\n                       :  500 1000 1500 2000 2500 3000 3500 4000 4500 5000\n         > y       [m] :  0 - 0 : 10  [min-max; #]\n                       :  0 0 0 0 0 0 0 0 0 0\n         > z       [m] :  200\n         > azimuth [\u00b0] :  0\n         > dip     [\u00b0] :  0\n       Required ab's   :  11\n    ~\n    :: empymod END; runtime = 0:00:00.005536 :: 1 kernel call(s)\n    ~\n    >>> print(EMfield)\n    [  1.68809346e-10 -3.08303130e-10j  -8.77189179e-12 -3.76920235e-11j\n      -3.46654704e-12 -4.87133683e-12j  -3.60159726e-13 -1.12434417e-12j\n       1.87807271e-13 -6.21669759e-13j   1.97200208e-13 -4.38210489e-13j\n       1.44134842e-13 -3.17505260e-13j   9.92770406e-14 -2.33950871e-13j\n       6.75287598e-14 -1.74922886e-13j   4.62724887e-14 -1.32266600e-13j]\n\n    \"\"\"\n\n    # === 1.  LET'S START ============\n    t0 = printstartfinish(verb)\n\n    # === 2.  CHECK INPUT ============\n\n    # Backwards compatibility\n    htarg, opt = spline_backwards_hankel(ht, htarg, opt)\n\n    # Check times and Fourier Transform arguments and get required frequencies\n    if signal is None:\n        freq = freqtime\n    else:\n        time, freq, ft, ftarg = check_time(freqtime, signal, ft, ftarg, verb)\n\n    # Check layer parameters\n    model = check_model(depth, res, aniso, epermH, epermV, mpermH, mpermV,\n                        xdirect, verb)\n    depth, res, aniso, epermH, epermV, mpermH, mpermV, isfullspace = model\n\n    # Check frequency => get etaH, etaV, zetaH, and zetaV\n    frequency = check_frequency(freq, res, aniso, epermH, epermV, mpermH,\n                                mpermV, verb)\n    freq, etaH, etaV, zetaH, zetaV = frequency\n\n    # Update etaH/etaV and zetaH/zetaV according to user-provided model\n    if isinstance(res, dict) and 'func_eta' in res:\n        etaH, etaV = res['func_eta'](res, locals())\n    if isinstance(res, dict) and 'func_zeta' in res:\n        zetaH, zetaV = res['func_zeta'](res, locals())\n\n    # Check Hankel transform parameters\n    ht, htarg = check_hankel(ht, htarg, verb)\n\n    # Check optimization\n    use_ne_eval, loop_freq, loop_off = check_opt(opt, loop, ht, htarg, verb)\n\n    # Check src and rec, get flags if dipole or not\n    # nsrcz/nrecz are number of unique src/rec-pole depths\n    src, nsrc, nsrcz, srcdipole = check_bipole(src, 'src')\n    rec, nrec, nrecz, recdipole = check_bipole(rec, 'rec')\n\n    # === 3. EM-FIELD CALCULATION ============\n\n    # Pre-allocate output EM array\n    EM = np.zeros((freq.size, nrec*nsrc), dtype=complex)\n\n    # Initialize kernel count, conv (only for QWE)\n    # (how many times the wavenumber-domain kernel was calld)\n    kcount = 0\n    conv = True\n\n    # Define some indeces\n    isrc = int(nsrc/nsrcz)  # this is either 1 or nsrc\n    irec = int(nrec/nrecz)  # this is either 1 or nrec\n    isrz = int(isrc*irec)   # this is either 1, nsrc, nrec, or nsrc*nrec\n\n    # The kernel handles only 1 ab with one srcz-recz combination at once.\n    # Hence we have to loop over every different depth of src or rec, and\n    # over all required ab's.\n    for isz in range(nsrcz):  # Loop over source depths\n\n        # Get this source\n        srcazmdip = get_azm_dip(src, isz, nsrcz, srcpts, srcdipole, strength,\n                                'src', verb)\n        tsrc, srcazm, srcdip, srcg_w, srcpts, src_w = srcazmdip\n\n        for irz in range(nrecz):  # Loop over receiver depths\n\n            # Get this receiver\n            recazmdip = get_azm_dip(rec, irz, nrecz, recpts, recdipole,\n                                    strength, 'rec', verb)\n            trec, recazm, recdip, recg_w, recpts, rec_w = recazmdip\n\n            # Get required ab's\n            ab_calc = get_abs(msrc, mrec, srcazm, srcdip, recazm, recdip, verb)\n\n            # Pre-allocate temporary source-EM array for integration loop\n            sEM = np.zeros((freq.size, isrz), dtype=complex)\n\n            for isg in range(srcpts):  # Loop over src integration points\n\n                # This integration source\n                tisrc = [tsrc[0][isg::srcpts], tsrc[1][isg::srcpts],\n                         tsrc[2][isg]]\n\n                # Get layer number in which src resides\n                lsrc, zsrc = get_layer_nr(tisrc, depth)\n\n                # Pre-allocate temporary receiver EM arrays for integr. loop\n                rEM = np.zeros((freq.size, isrz), dtype=complex)\n\n                for irg in range(recpts):  # Loop over rec integration pts\n                    # Note, if source or receiver is a bipole, but horizontal\n                    # (dip=0), then calculation could be sped up by not looping\n                    # over the bipole elements, but calculate it all in one go.\n\n                    # This integration receiver\n                    tirec = [trec[0][irg::recpts], trec[1][irg::recpts],\n                             trec[2][irg]]\n\n                    # Get src-rec offsets and angles\n                    off, angle = get_off_ang(tisrc, tirec, isrc, irec, verb)\n\n                    # Get layer number in which rec resides\n                    lrec, zrec = get_layer_nr(tirec, depth)\n\n                    # Gather variables\n                    finp = (off, angle, zsrc, zrec, lsrc, lrec, depth, freq,\n                            etaH, etaV, zetaH, zetaV, xdirect, isfullspace, ht,\n                            htarg, use_ne_eval, msrc, mrec, loop_freq,\n                            loop_off, conv)\n\n                    # Pre-allocate temporary EM array for ab-loop\n                    abEM = np.zeros((freq.size, isrz), dtype=complex)\n\n                    for iab in ab_calc:  # Loop over required ab's\n\n                        # Carry-out the frequency-domain calculation\n                        out = fem(iab, *finp)\n\n                        # Get geometrical scaling factor\n                        tfact = get_geo_fact(iab, srcazm, srcdip, recazm,\n                                             recdip, msrc, mrec)\n\n                        # Add field to EM with geometrical factor\n                        abEM += out[0]*np.squeeze(tfact)\n\n                        # Update kernel count\n                        kcount += out[1]\n\n                        # Update conv (QWE convergence)\n                        conv *= out[2]\n\n                    # Add this receiver element, with weight from integration\n                    rEM += abEM*recg_w[irg]\n\n                # Add this source element, with weight from integration\n                sEM += rEM*srcg_w[isg]\n\n            # Scale signal for src-strength and src/rec-lengths\n            src_rec_w = 1\n            if strength > 0:\n                src_rec_w *= np.repeat(src_w, irec)\n                src_rec_w *= np.tile(rec_w, isrc)\n            sEM *= src_rec_w\n\n            # Add this src-rec signal\n            if nrec == nrecz:\n                if nsrc == nsrcz:  # Case 1: Looped over each src and each rec\n                    EM[:, isz*nrec+irz:isz*nrec+irz+1] = sEM\n                else:              # Case 2: Looped over each rec\n                    EM[:, irz:nsrc*nrec:nrec] = sEM\n            else:\n                if nsrc == nsrcz:  # Case 3: Looped over each src\n                    EM[:, isz*nrec:nrec*(isz+1)] = sEM\n                else:              # Case 4: All in one go\n                    EM = sEM\n\n    # In case of QWE/QUAD, print Warning if not converged\n    conv_warning(conv, htarg, 'Hankel', verb)\n\n    # Do f->t transform if required\n    if signal is not None:\n        EM, conv = tem(EM, EM[0, :], freq, time, signal, ft, ftarg)\n\n        # In case of QWE/QUAD, print Warning if not converged\n        conv_warning(conv, ftarg, 'Fourier', verb)\n\n    # Reshape for number of sources\n    EM = np.squeeze(EM.reshape((-1, nrec, nsrc), order='F'))\n\n    # === 4.  FINISHED ============\n    printstartfinish(verb, t0, kcount)\n\n    return EM", "response": "r Return the electromagnetic field due to an arbitrary source and receiver."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analytical(src, rec, res, freqtime, solution='fs', signal=None, ab=11,\n               aniso=None, epermH=None, epermV=None, mpermH=None, mpermV=None,\n               verb=2):\n    r\"\"\"Return the analytical full- or half-space solution.\n\n    Calculate the electromagnetic frequency- or time-domain field due to\n    infinitesimal small electric or magnetic dipole source(s), measured by\n    infinitesimal small electric or magnetic dipole receiver(s); sources and\n    receivers are directed along the principal directions x, y, or z, and all\n    sources are at the same depth, as well as all receivers are at the same\n    depth.\n\n    In the case of a halfspace the air-interface is located at z = 0 m.\n\n    You can call the functions ``fullspace`` and ``halfspace`` in ``kernel.py``\n    directly. This interface is just to provide a consistent interface with the\n    same input parameters as for instance for ``dipole``.\n\n    This function yields the same result if ``solution='fs'`` as ``dipole``, if\n    the model is a fullspace.\n\n    Included are:\n      - Full fullspace solution (``solution='fs'``) for ee-, me-, em-,\n        mm-fields, only frequency domain, [HuTS15]_.\n      - Diffusive fullspace solution (``solution='dfs'``) for ee-fields,\n        [SlHM10]_.\n      - Diffusive halfspace solution (``solution='dhs'``) for ee-fields,\n        [SlHM10]_.\n      - Diffusive direct- and reflected field and airwave\n        (``solution='dsplit'``) for ee-fields, [SlHM10]_.\n      - Diffusive direct- and reflected field and airwave\n        (``solution='dtetm'``) for ee-fields, split into TE and TM mode\n        [SlHM10]_.\n\n    Parameters\n    ----------\n    src, rec : list of floats or arrays\n        Source and receiver coordinates (m): [x, y, z].\n        The x- and y-coordinates can be arrays, z is a single value.\n        The x- and y-coordinates must have the same dimension.\n\n    res : float\n        Horizontal resistivity rho_h (Ohm.m).\n\n        Alternatively, res can be a dictionary. See the main manual of empymod\n        too see how to exploit this hook to re-calculate etaH, etaV, zetaH, and\n        zetaV, which can be used to, for instance, use the Cole-Cole model for\n        IP.\n\n    freqtime : array_like\n        Frequencies f (Hz) if ``signal`` == None, else times t (s); (f, t > 0).\n\n    solution : str, optional\n      Defines which solution is returned:\n        - 'fs' : Full fullspace solution (ee-, me-, em-, mm-fields); f-domain.\n        - 'dfs' : Diffusive fullspace solution (ee-fields only).\n        - 'dhs' : Diffusive halfspace solution (ee-fields only).\n        - 'dsplit' : Diffusive direct- and reflected field and airwave\n                     (ee-fields only).\n        - 'dtetm' : as dsplit, but direct fielt TE, TM; reflected field TE, TM,\n                    and airwave (ee-fields only).\n\n    signal : {None, 0, 1, -1}, optional\n        Source signal, default is None:\n            - None: Frequency-domain response\n            - -1 : Switch-off time-domain response\n            - 0 : Impulse time-domain response\n            - +1 : Switch-on time-domain response\n\n    ab : int, optional\n        Source-receiver configuration, defaults to 11.\n\n        +---------------+-------+------+------+------+------+------+------+\n        |                       | electric  source   | magnetic source    |\n        +===============+=======+======+======+======+======+======+======+\n        |                       | **x**| **y**| **z**| **x**| **y**| **z**|\n        +---------------+-------+------+------+------+------+------+------+\n        |               | **x** |  11  |  12  |  13  |  14  |  15  |  16  |\n        + **electric**  +-------+------+------+------+------+------+------+\n        |               | **y** |  21  |  22  |  23  |  24  |  25  |  26  |\n        + **receiver**  +-------+------+------+------+------+------+------+\n        |               | **z** |  31  |  32  |  33  |  34  |  35  |  36  |\n        +---------------+-------+------+------+------+------+------+------+\n        |               | **x** |  41  |  42  |  43  |  44  |  45  |  46  |\n        + **magnetic**  +-------+------+------+------+------+------+------+\n        |               | **y** |  51  |  52  |  53  |  54  |  55  |  56  |\n        + **receiver**  +-------+------+------+------+------+------+------+\n        |               | **z** |  61  |  62  |  63  |  64  |  65  |  66  |\n        +---------------+-------+------+------+------+------+------+------+\n\n    aniso : float, optional\n        Anisotropy lambda = sqrt(rho_v/rho_h) (-); defaults to one.\n\n    epermH, epermV : float, optional\n        Relative horizontal/vertical electric permittivity epsilon_h/epsilon_v\n        (-); default is one. Ignored for the diffusive solution.\n\n    mpermH, mpermV : float, optional\n        Relative horizontal/vertical magnetic permeability mu_h/mu_v (-);\n        default is one. Ignored for the diffusive solution.\n\n    verb : {0, 1, 2, 3, 4}, optional\n        Level of verbosity, default is 2:\n            - 0: Print nothing.\n            - 1: Print warnings.\n            - 2: Print additional runtime\n            - 3: Print additional start/stop, condensed parameter information.\n            - 4: Print additional full parameter information\n\n    Returns\n    -------\n    EM : ndarray, (nfreq, nrec, nsrc)\n        Frequency- or time-domain EM field (depending on ``signal``):\n            - If rec is electric, returns E [V/m].\n            - If rec is magnetic, returns B [T] (not H [A/m]!).\n\n        However, source and receiver are normalised. So for instance in the\n        electric case the source strength is 1 A and its length is 1 m. So the\n        electric field could also be written as [V/(A.m2)].\n\n        The shape of EM is (nfreq, nrec, nsrc). However, single dimensions\n        are removed.\n\n        If ``solution='dsplit'``, three ndarrays are returned: direct, reflect,\n        air.\n\n        If ``solution='dtetm'``, five ndarrays are returned: direct_TE,\n        direct_TM, reflect_TE, reflect_TM, air.\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from empymod import analytical\n    >>> src = [0, 0, 0]\n    >>> rec = [np.arange(1, 11)*500, np.zeros(10), 200]\n    >>> res = 50\n    >>> EMfield = analytical(src, rec, res, freqtime=1, verb=0)\n    >>> print(EMfield)\n    [  4.03091405e-08 -9.69163818e-10j   6.97630362e-09 -4.88342150e-10j\n       2.15205979e-09 -2.97489809e-10j   8.90394459e-10 -1.99313433e-10j\n       4.32915802e-10 -1.40741644e-10j   2.31674165e-10 -1.02579391e-10j\n       1.31469130e-10 -7.62770461e-11j   7.72342470e-11 -5.74534125e-11j\n       4.61480481e-11 -4.36275540e-11j   2.76174038e-11 -3.32860932e-11j]\n    \"\"\"\n\n    # === 1.  LET'S START ============\n    t0 = printstartfinish(verb)\n\n    # === 2.  CHECK INPUT ============\n\n    # Check times or frequencies\n    if signal is not None:\n        freqtime = check_time_only(freqtime, signal, verb)\n\n    # Check layer parameters\n    model = check_model([], res, aniso, epermH, epermV, mpermH, mpermV, True,\n                        verb)\n    depth, res, aniso, epermH, epermV, mpermH, mpermV, _ = model\n\n    # Check frequency => get etaH, etaV, zetaH, and zetaV\n    frequency = check_frequency(freqtime, res, aniso, epermH, epermV, mpermH,\n                                mpermV, verb)\n    freqtime, etaH, etaV, zetaH, zetaV = frequency\n\n    # Update etaH/etaV and zetaH/zetaV according to user-provided model\n    if isinstance(res, dict) and 'func_eta' in res:\n        etaH, etaV = res['func_eta'](res, locals())\n    if isinstance(res, dict) and 'func_zeta' in res:\n        zetaH, zetaV = res['func_zeta'](res, locals())\n\n    # Check src-rec configuration\n    # => Get flags if src or rec or both are magnetic (msrc, mrec)\n    ab_calc, msrc, mrec = check_ab(ab, verb)\n\n    # Check src and rec\n    src, nsrc = check_dipole(src, 'src', verb)\n    rec, nrec = check_dipole(rec, 'rec', verb)\n\n    # Get offsets and angles (off, angle)\n    off, angle = get_off_ang(src, rec, nsrc, nrec, verb)\n\n    # Get layer number in which src and rec reside (lsrc/lrec)\n    _, zsrc = get_layer_nr(src, depth)\n    _, zrec = get_layer_nr(rec, depth)\n\n    # Check possibilities\n    check_solution(solution, signal, ab, msrc, mrec)\n\n    # === 3. EM-FIELD CALCULATION ============\n\n    if solution[0] == 'd':\n        EM = kernel.halfspace(off, angle, zsrc, zrec, etaH, etaV,\n                              freqtime[:, None], ab_calc, signal, solution)\n    else:\n        if ab_calc not in [36, ]:\n            EM = kernel.fullspace(off, angle, zsrc, zrec, etaH, etaV, zetaH,\n                                  zetaV, ab_calc, msrc, mrec)\n        else:\n            # If <ab> = 36 (or 63), field is zero\n            # In `bipole` and in `dipole`, this is taken care of in `fem`. Here\n            # we have to take care of it separately\n            EM = np.zeros((freqtime.size*nrec*nsrc), dtype=complex)\n\n    # Squeeze\n    if solution[1:] == 'split':\n        EM = (np.squeeze(EM[0].reshape((-1, nrec, nsrc), order='F')),\n              np.squeeze(EM[1].reshape((-1, nrec, nsrc), order='F')),\n              np.squeeze(EM[2].reshape((-1, nrec, nsrc), order='F')))\n    elif solution[1:] == 'tetm':\n        EM = (np.squeeze(EM[0].reshape((-1, nrec, nsrc), order='F')),\n              np.squeeze(EM[1].reshape((-1, nrec, nsrc), order='F')),\n              np.squeeze(EM[2].reshape((-1, nrec, nsrc), order='F')),\n              np.squeeze(EM[3].reshape((-1, nrec, nsrc), order='F')),\n              np.squeeze(EM[4].reshape((-1, nrec, nsrc), order='F')))\n    else:\n        EM = np.squeeze(EM.reshape((-1, nrec, nsrc), order='F'))\n\n    # === 4.  FINISHED ============\n    printstartfinish(verb, t0)\n\n    return EM", "response": "r Returns an analytical full - space solution for a source and receiver."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dipole_k(src, rec, depth, res, freq, wavenumber, ab=11, aniso=None,\n             epermH=None, epermV=None, mpermH=None, mpermV=None, verb=2):\n    r\"\"\"Return the electromagnetic wavenumber-domain field.\n\n    Calculate the electromagnetic wavenumber-domain field due to infinitesimal\n    small electric or magnetic dipole source(s), measured by infinitesimal\n    small electric or magnetic dipole receiver(s); sources and receivers are\n    directed along the principal directions x, y, or z, and all sources are at\n    the same depth, as well as all receivers are at the same depth.\n\n\n    See Also\n    --------\n    dipole : Electromagnetic field due to an electromagnetic source (dipoles).\n    bipole : Electromagnetic field due to an electromagnetic source (bipoles).\n    fem : Electromagnetic frequency-domain response.\n    tem : Electromagnetic time-domain response.\n\n\n    Parameters\n    ----------\n    src, rec : list of floats or arrays\n        Source and receiver coordinates (m): [x, y, z].\n        The x- and y-coordinates can be arrays, z is a single value.\n        The x- and y-coordinates must have the same dimension.\n        The x- and y-coordinates only matter for the angle-dependent factor.\n\n        Sources or receivers placed on a layer interface are considered in the\n        upper layer.\n\n    depth : list\n        Absolute layer interfaces z (m); #depth = #res - 1\n        (excluding +/- infinity).\n\n    res : array_like\n        Horizontal resistivities rho_h (Ohm.m); #res = #depth + 1.\n\n    freq : array_like\n        Frequencies f (Hz), used to calculate etaH/V and zetaH/V.\n\n    wavenumber : array\n        Wavenumbers lambda (1/m)\n\n    ab : int, optional\n        Source-receiver configuration, defaults to 11.\n\n        +---------------+-------+------+------+------+------+------+------+\n        |                       | electric  source   | magnetic source    |\n        +===============+=======+======+======+======+======+======+======+\n        |                       | **x**| **y**| **z**| **x**| **y**| **z**|\n        +---------------+-------+------+------+------+------+------+------+\n        |               | **x** |  11  |  12  |  13  |  14  |  15  |  16  |\n        + **electric**  +-------+------+------+------+------+------+------+\n        |               | **y** |  21  |  22  |  23  |  24  |  25  |  26  |\n        + **receiver**  +-------+------+------+------+------+------+------+\n        |               | **z** |  31  |  32  |  33  |  34  |  35  |  36  |\n        +---------------+-------+------+------+------+------+------+------+\n        |               | **x** |  41  |  42  |  43  |  44  |  45  |  46  |\n        + **magnetic**  +-------+------+------+------+------+------+------+\n        |               | **y** |  51  |  52  |  53  |  54  |  55  |  56  |\n        + **receiver**  +-------+------+------+------+------+------+------+\n        |               | **z** |  61  |  62  |  63  |  64  |  65  |  66  |\n        +---------------+-------+------+------+------+------+------+------+\n\n    aniso : array_like, optional\n        Anisotropies lambda = sqrt(rho_v/rho_h) (-); #aniso = #res.\n        Defaults to ones.\n\n    epermH, epermV : array_like, optional\n        Relative horizontal/vertical electric permittivities\n        epsilon_h/epsilon_v (-);\n        #epermH = #epermV = #res. Default is ones.\n\n    mpermH, mpermV : array_like, optional\n        Relative horizontal/vertical magnetic permeabilities mu_h/mu_v (-);\n        #mpermH = #mpermV = #res. Default is ones.\n\n    verb : {0, 1, 2, 3, 4}, optional\n        Level of verbosity, default is 2:\n            - 0: Print nothing.\n            - 1: Print warnings.\n            - 2: Print additional runtime and kernel calls\n            - 3: Print additional start/stop, condensed parameter information.\n            - 4: Print additional full parameter information\n\n\n    Returns\n    -------\n    PJ0, PJ1 : array\n        Wavenumber-domain EM responses:\n            - PJ0: Wavenumber-domain solution for the kernel with a Bessel\n              function of the first kind of order zero.\n            - PJ1: Wavenumber-domain solution for the kernel with a Bessel\n              function of the first kind of order one.\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from empymod.model import dipole_k\n    >>> src = [0, 0, 100]\n    >>> rec = [5000, 0, 200]\n    >>> depth = [0, 300, 1000, 1050]\n    >>> res = [1e20, .3, 1, 50, 1]\n    >>> freq = 1\n    >>> wavenrs = np.logspace(-3.7, -3.6, 10)\n    >>> PJ0, PJ1 = dipole_k(src, rec, depth, res, freq, wavenrs, verb=0)\n    >>> print(PJ0)\n    [ -1.02638329e-08 +4.91531529e-09j  -1.05289724e-08 +5.04222413e-09j\n      -1.08009148e-08 +5.17238608e-09j  -1.10798310e-08 +5.30588284e-09j\n      -1.13658957e-08 +5.44279805e-09j  -1.16592877e-08 +5.58321732e-09j\n      -1.19601897e-08 +5.72722830e-09j  -1.22687889e-08 +5.87492067e-09j\n      -1.25852765e-08 +6.02638626e-09j  -1.29098481e-08 +6.18171904e-09j]\n    >>> print(PJ1)\n    [  1.79483705e-10 -6.59235332e-10j   1.88672497e-10 -6.93749344e-10j\n       1.98325814e-10 -7.30068377e-10j   2.08466693e-10 -7.68286748e-10j\n       2.19119282e-10 -8.08503709e-10j   2.30308887e-10 -8.50823701e-10j\n       2.42062030e-10 -8.95356636e-10j   2.54406501e-10 -9.42218177e-10j\n       2.67371420e-10 -9.91530051e-10j   2.80987292e-10 -1.04342036e-09j]\n    \"\"\"\n\n    # === 1.  LET'S START ============\n    t0 = printstartfinish(verb)\n\n    # === 2.  CHECK INPUT ============\n\n    # Check layer parameters (isfullspace not required)\n    modl = check_model(depth, res, aniso, epermH, epermV, mpermH, mpermV,\n                       False, verb)\n    depth, res, aniso, epermH, epermV, mpermH, mpermV, _ = modl\n\n    # Check frequency => get etaH, etaV, zetaH, and zetaV\n    f = check_frequency(freq, res, aniso, epermH, epermV, mpermH, mpermV, verb)\n    freq, etaH, etaV, zetaH, zetaV = f\n\n    # Check src-rec configuration\n    # => Get flags if src or rec or both are magnetic (msrc, mrec)\n    ab_calc, msrc, mrec = check_ab(ab, verb)\n\n    # Check src and rec\n    src, nsrc = check_dipole(src, 'src', verb)\n    rec, nrec = check_dipole(rec, 'rec', verb)\n\n    # Get angle-dependent factor\n    off, angle = get_off_ang(src, rec, nsrc, nrec, verb)\n    factAng = kernel.angle_factor(angle, ab, msrc, mrec)\n\n    # Get layer number in which src and rec reside (lsrc/lrec)\n    lsrc, zsrc = get_layer_nr(src, depth)\n    lrec, zrec = get_layer_nr(rec, depth)\n\n    # === 3. EM-FIELD CALCULATION ============\n\n    # Pre-allocate\n    if off.size == 1 and np.ndim(wavenumber) == 2:\n        PJ0 = np.zeros((freq.size, wavenumber.shape[0], wavenumber.shape[1]),\n                       dtype=complex)\n        PJ1 = np.zeros((freq.size, wavenumber.shape[0], wavenumber.shape[1]),\n                       dtype=complex)\n    else:\n        PJ0 = np.zeros((freq.size, off.size, wavenumber.size), dtype=complex)\n        PJ1 = np.zeros((freq.size, off.size, wavenumber.size), dtype=complex)\n\n    # If <ab> = 36 (or 63), field is zero\n    # In `bipole` and in `dipole`, this is taken care of in `fem`. Here we\n    # have to take care of it separately\n    if ab_calc not in [36, ]:\n\n        # Calculate wavenumber response\n        J0, J1, J0b = kernel.wavenumber(zsrc, zrec, lsrc, lrec, depth, etaH,\n                                        etaV, zetaH, zetaV,\n                                        np.atleast_2d(wavenumber), ab_calc,\n                                        False, msrc, mrec, False)\n\n        # Collect output\n        if J1 is not None:\n            PJ1 += factAng[:, np.newaxis]*J1\n            if ab in [11, 12, 21, 22, 14, 24, 15, 25]:  # Because of J2\n                # J2(kr) = 2/(kr)*J1(kr) - J0(kr)\n                PJ1 /= off[:, None]\n        if J0 is not None:\n            PJ0 += J0\n        if J0b is not None:\n            PJ0 += factAng[:, np.newaxis]*J0b\n\n    # === 4.  FINISHED ============\n    printstartfinish(verb, t0, 1)\n\n    return np.squeeze(PJ0), np.squeeze(PJ1)", "response": "r Return the electromagnetic dipole - domain field for a given source and receiver."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dipole(src, rec, depth, res, freqtime, aniso=None, eperm=None, mperm=None,\n           verb=2):\n    r\"\"\"Return the electromagnetic field due to a dipole source.\n\n    This is a modified version of ``empymod.model.dipole()``. It returns the\n    separated contributions of TM--, TM-+, TM+-, TM++, TMdirect, TE--, TE-+,\n    TE+-, TE++, and TEdirect.\n\n    Parameters\n    ----------\n    src, rec : list of floats or arrays\n        Source and receiver coordinates (m): [x, y, z].\n        The x- and y-coordinates can be arrays, z is a single value.\n        The x- and y-coordinates must have the same dimension.\n\n        Sources or receivers placed on a layer interface are considered in the\n        upper layer.\n\n        Sources and receivers must be in the same layer.\n\n    depth : list\n        Absolute layer interfaces z (m); #depth = #res - 1\n        (excluding +/- infinity).\n\n    res : array_like\n        Horizontal resistivities rho_h (Ohm.m); #res = #depth + 1.\n\n    freqtime : float\n        Frequency f (Hz). (The name ``freqtime`` is kept for consistency with\n        ``empymod.model.dipole()``. Only one frequency at once.\n\n    aniso : array_like, optional\n        Anisotropies lambda = sqrt(rho_v/rho_h) (-); #aniso = #res.\n        Defaults to ones.\n\n    eperm : array_like, optional\n        Relative electric permittivities epsilon (-);\n        #eperm = #res. Default is ones.\n\n    mperm : array_like, optional\n        Relative magnetic permeabilities mu (-);\n        #mperm = #res. Default is ones.\n\n    verb : {0, 1, 2, 3, 4}, optional\n        Level of verbosity, default is 2:\n            - 0: Print nothing.\n            - 1: Print warnings.\n            - 2: Print additional runtime and kernel calls\n            - 3: Print additional start/stop, condensed parameter information.\n            - 4: Print additional full parameter information\n\n\n    Returns\n    -------\n    TM, TE : list of ndarrays, (nfreq, nrec, nsrc)\n        Frequency-domain EM field [V/m], separated into\n        TM = [TM--, TM-+, TM+-, TM++, TMdirect]\n        and\n        TE = [TE--, TE-+, TE+-, TE++, TEdirect].\n\n        However, source and receiver are normalised. So the source strength is\n        1 A and its length is 1 m. Therefore the electric field could also be\n        written as [V/(A.m2)].\n\n        The shape of EM is (nfreq, nrec, nsrc). However, single dimensions\n        are removed.\n\n    \"\"\"\n\n    # === 1. LET'S START ============\n    t0 = printstartfinish(verb)\n\n    # === 2. CHECK INPUT ============\n    # Check layer parameters\n    model = check_model(depth, res, aniso, eperm, eperm, mperm, mperm, False,\n                        verb)\n    depth, res, aniso, epermH, epermV, mpermH, mpermV, _ = model\n\n    # Check frequency => get etaH, etaV, zetaH, and zetaV\n    frequency = check_frequency(freqtime, res, aniso, epermH, epermV, mpermH,\n                                mpermV, verb)\n    freq, etaH, etaV, zetaH, zetaV = frequency\n\n    # Check src and rec\n    src, nsrc = check_dipole(src, 'src', verb)\n    rec, nrec = check_dipole(rec, 'rec', verb)\n\n    # Get offsets\n    off, ang = get_off_ang(src, rec, nsrc, nrec, verb)\n\n    # Get layer number in which src and rec reside (lsrc/lrec)\n    lsrc, zsrc = get_layer_nr(src, depth)\n    lrec, zrec = get_layer_nr(rec, depth)\n\n    # Check limitations of this routine compared to the standard ``dipole``\n    if lsrc != lrec:                           # src and rec in same layer\n        print(\"* ERROR   :: src and rec must be in the same layer; \" +\n              \"<lsrc>/<lrec> provided: \"+str(lsrc)+\"/\"+str(lrec))\n        raise ValueError('src-z/rec-z')\n\n    if depth.size < 2:                         # at least two layers\n        print(\"* ERROR   :: model must have more than one layer; \" +\n              \"<depth> provided: \"+_strvar(depth[1:]))\n        raise ValueError('depth')\n\n    if freq.size > 1:                          # only 1 frequency\n        print(\"* ERROR   :: only one frequency permitted; \" +\n              \"<freqtime> provided: \"+_strvar(freqtime))\n        raise ValueError('frequency')\n\n    # === 3. EM-FIELD CALCULATION ============\n    # This part is a simplification of:\n    # - model.fem()\n    # - transform.dlf()\n    # - kernel.wavenumber()\n\n    # DLF filter we use\n    filt = key_201_2012()\n\n    # 3.1. COMPUTE REQUIRED LAMBDAS for given hankel-filter-base\n    lambd = filt.base/off[:, None]\n\n    # 3.2. CALL THE KERNEL\n    PTM, PTE = greenfct(zsrc, zrec, lsrc, lrec, depth, etaH, etaV, zetaH,\n                        zetaV, lambd)\n\n    # 3.3. CARRY OUT THE HANKEL TRANSFORM WITH DLF\n    factAng = angle_factor(ang, 11, False, False)\n    zmfactAng = (factAng[:, np.newaxis]-1)/2\n    zpfactAng = (factAng[:, np.newaxis]+1)/2\n    fact = 4*np.pi*off\n\n    # TE [uu, ud, du, dd, df]\n    for i, val in enumerate(PTE):\n        PTE[i] = (factAng*np.dot(-val, filt.j1)/off +\n                  np.dot(zmfactAng*val*lambd, filt.j0))/fact\n\n    # TM [uu, ud, du, dd, df]\n    for i, val in enumerate(PTM):\n        PTM[i] = (factAng*np.dot(-val, filt.j1)/off +\n                  np.dot(zpfactAng*val*lambd, filt.j0))/fact\n\n    # 3.4. Remove non-physical contributions\n\n    # (Note: The T*dd corrections differ slightly from the equations given in\n    # the accompanying pdf, due to the way the direct field is accounted for\n    # in the book.)\n\n    # General parameters\n    Gam = np.sqrt((zetaH*etaH)[:, None, :, None])  # Gam for lambd=0\n    iGam = Gam[:, :, lsrc, 0]\n    lgam = np.sqrt(zetaH[:, lsrc]*etaH[:, lsrc])\n    ddepth = np.r_[depth, np.inf]\n    ds = ddepth[lsrc+1] - ddepth[lsrc]\n\n    def get_rp_rm(z_eta):\n        r\"\"\"Return Rp, Rm.\"\"\"\n\n        # Get Rp/Rm for lambd=0\n        Rp, Rm = reflections(depth, z_eta, Gam, lrec, lsrc, False)\n\n        # Depending on model Rp/Rm have 3 or 4 dimensions. Last two are\n        # wavenumbers and layers btw src and rec, which both are 1.\n        if Rp.ndim == 4:\n            Rp = np.squeeze(Rp, axis=3)\n        if Rm.ndim == 4:\n            Rm = np.squeeze(Rm, axis=3)\n        Rp = np.squeeze(Rp, axis=2)\n        Rm = np.squeeze(Rm, axis=2)\n\n        # Calculate reverberation M and general factor npfct\n        Ms = 1 - Rp*Rm*np.exp(-2*iGam*ds)\n        npfct = factAng*zetaH[:, lsrc]/(fact*off*lgam*Ms)\n\n        return Rp, Rm, npfct\n\n    # TE modes TE[uu, ud, du, dd]\n    Rp, Rm, npfct = get_rp_rm(zetaH)\n\n    PTE[0] += npfct*Rp*Rm*np.exp(-lgam*(2*ds - zrec + zsrc))\n    PTE[1] += npfct*Rp*np.exp(-lgam*(2*ddepth[lrec+1] - zrec - zsrc))\n    PTE[2] += npfct*Rm*np.exp(-lgam*(zrec + zsrc))\n    PTE[3] += npfct*Rp*Rm*np.exp(-lgam*(2*ds + zrec - zsrc))\n\n    # TM modes TM[uu, ud, du, dd]\n    Rp, Rm, npfct = get_rp_rm(etaH)\n\n    PTM[0] -= npfct*Rp*Rm*np.exp(-lgam*(2*ds - zrec + zsrc))\n    PTM[1] += npfct*Rp*np.exp(-lgam*(2*ddepth[lrec+1] - zrec - zsrc))\n    PTM[2] += npfct*Rm*np.exp(-lgam*(zrec + zsrc))\n    PTM[3] -= npfct*Rp*Rm*np.exp(-lgam*(2*ds + zrec - zsrc))\n\n    # 3.5 Reshape for number of sources\n    for i, val in enumerate(PTE):\n        PTE[i] = np.squeeze(val.reshape((-1, nrec, nsrc), order='F'))\n\n    for i, val in enumerate(PTM):\n        PTM[i] = np.squeeze(val.reshape((-1, nrec, nsrc), order='F'))\n\n    # === 4. FINISHED ============\n    printstartfinish(verb, t0)\n\n    # return [TMuu, TMud, TMdu, TMdd, TMdf], [TEuu, TEud, TEdu, TEdd, TEdf]\n    return PTM, PTE", "response": "r Returns the electromagnetic field due to a dipole source."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_filter(name, filt, full=None, path='filters'):\n\n    # First we'll save the filter using its internal routine.\n    # This will create the directory ./filters if it doesn't exist already.\n    filt.tofile(path)\n\n    # If full, we store the inversion output\n    if full:\n\n        # Get file name\n        path = os.path.abspath(path)\n        if len(name.split('.')) == 2:\n            suffix = '.gz'\n        else:\n            suffix = ''\n        fullfile = os.path.join(path, name.split('.')[0]+'_full.txt' + suffix)\n\n        # Get number of spacing and shift values\n        nspace, nshift = full[3].shape\n\n        # Create header\n        header = 'Full inversion output from empymod.fdesign.design\\n'\n        header += 'Line 11: Nr of spacing values\\n'\n        header += 'Line 12: Nr of shift values\\n'\n        header += 'Line 13: Best spacing value\\n'\n        header += 'Line 14: Best shift value\\n'\n        header += 'Line 15: Min amplitude or max offset\\n'\n\n        header += 'Lines 16-{}: Spacing matrix '.format(nspace+15)\n        header += '({} x {})\\n'.format(nspace, nshift)\n\n        header += 'Lines {}-{}: Spacing matrix '.format(nspace+16, 2*nspace+15)\n        header += '({} x {})\\n'.format(nspace, nshift)\n\n        header += 'Lines {}-{}: Spacing '.format(2*nspace+16, 3*nspace+15)\n        header += 'matrix ({} x {})\\n'.format(nspace, nshift)\n\n        header += 'Line {}: Integer: 0: min amp, 1: max r'.format(3*nspace+16)\n\n        # Create arrays; put single values in arrays of nshift values\n        nr_spacing = np.r_[nspace, np.zeros(nshift-1)]\n        nr_shift = np.r_[nshift, np.zeros(nshift-1)]\n        best_spacing = np.r_[full[0][0], np.zeros(nshift-1)]\n        best_shift = np.r_[full[0][1], np.zeros(nshift-1)]\n        min_value = np.r_[np.atleast_1d(full[1]), np.zeros(nshift-1)]\n        min_max = np.r_[full[4], np.zeros(nshift-1)]\n\n        # Collect all in one array\n        fullsave = np.vstack((nr_spacing, nr_shift, best_spacing, best_shift,\n                              min_value, full[2][0], full[2][1], full[3],\n                              min_max))\n\n        # Save array\n        np.savetxt(fullfile, fullsave, header=header)", "response": "r Save DLF - filter and inversion output to plain text files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_filter(name, full=False, path='filters'):\n\n    # First we'll get the filter using its internal routine.\n    filt = DigitalFilter(name.split('.')[0])\n    filt.fromfile(path)\n\n    # If full, we get the inversion output\n    if full:\n        # Try to get the inversion result. If files are not found, most likely\n        # because they were not stored, we only return the filter\n        try:\n            # Get file name\n            path = os.path.abspath(path)\n            if len(name.split('.')) == 2:\n                suffix = '.gz'\n            else:\n                suffix = ''\n            fullfile = os.path.join(path, name.split('.')[0] +\n                                    '_full.txt' + suffix)\n\n            # Read data\n            out = np.loadtxt(fullfile)\n\n        except IOError:\n            return filt\n\n        # Collect inversion-result tuple\n        nspace = int(out[0][0])\n        nshift = int(out[1][0])\n\n        space_shift_matrix = np.zeros((2, nspace, nshift))\n        space_shift_matrix[0, :, :] = out[5:nspace+5, :]\n        space_shift_matrix[1, :, :] = out[nspace+5:2*nspace+5, :]\n\n        out = (np.array([out[2][0], out[3][0]]), out[4][0], space_shift_matrix,\n               out[2*nspace+5:3*nspace+5, :], int(out[3*nspace+5, 0]))\n\n        return filt, out\n    else:\n        return filt", "response": "r Load saved DLF - filter and inversion output from text files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_result(filt, full, prntres=True):\n    # Check matplotlib (soft dependency)\n    if not plt:\n        print(plt_msg)\n        return\n\n    if prntres:\n        print_result(filt, full)\n\n    # Get spacing and shift values from full output of brute\n    spacing = full[2][0, :, 0]\n    shift = full[2][1, 0, :]\n\n    # Get minimum field values from full output of brute\n    minfield = np.squeeze(full[3])\n\n    plt.figure(\"Brute force result\", figsize=(9.5, 4.5))\n    plt.subplots_adjust(wspace=.4, bottom=0.2)\n\n    # Figure 1: Only if more than 1 spacing or more than 1 shift\n    # Figure of minfield, depending if spacing/shift are vectors or floats\n    if spacing.size > 1 or shift.size > 1:\n        plt.subplot(121)\n        if full[4] == 0:  # Min amp\n            plt.title(\"Minimal recovered fields\")\n            ylabel = 'Minimal recovered amplitude (log10)'\n            field = np.log10(minfield)\n            cmap = plt.cm.viridis\n        else:  # Max r\n            plt.title(\"Maximum recovered r\")\n            ylabel = 'Maximum recovered r'\n            field = 1/minfield\n            cmap = plt.cm.viridis_r\n\n        if shift.size == 1:    # (a) if only one shift value,\n            plt.plot(spacing, field)\n            plt.xlabel('Spacing')\n            plt.ylabel(ylabel)\n\n        elif spacing.size == 1:  # (b) if only one spacing value\n            plt.plot(shift, field)\n            plt.xlabel('Shift')\n            plt.ylabel(ylabel)\n\n        else:   # (c) if several spacing and several shift values\n            field = np.ma.masked_where(np.isinf(minfield), field)\n            plt.pcolormesh(shift, spacing, field, cmap=cmap)\n            plt.ylabel('Spacing')\n            plt.xlabel('Shift')\n            plt.colorbar()\n\n    # Figure 2: Filter values\n    if spacing.size > 1 or shift.size > 1:\n        plt.subplot(122)\n    plt.title('Filter values of best filter')\n    for attr in ['j0', 'j1', 'sin', 'cos']:\n        if hasattr(filt, attr):\n            plt.plot(np.log10(filt.base),\n                     np.log10(np.abs(getattr(filt, attr))), '.-', lw=.5,\n                     label='abs('+attr+')')\n            plt.plot(np.log10(filt.base), np.log10(-getattr(filt, attr)), '.',\n                     color='k', ms=4)\n    plt.plot(np.inf, 0, '.', color='k', ms=4, label='Neg. values')\n    plt.xlabel('Base (log10)')\n    plt.ylabel('Abs(Amplitude) (log10)')\n    plt.legend(loc='best')\n    plt.gcf().canvas.draw()  # To force draw in notebook while running\n    plt.show()", "response": "r Plot the result of a filter and full result of a brute force."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_result(filt, full=None):\n    print('   Filter length   : %d' % filt.base.size)\n    print('   Best filter')\n\n    if full:  # If full provided, we have more information\n        if full[4] == 0:  # Min amp\n            print('   > Min field     : %g' % full[1])\n        else:  # Max amp\n            r = 1/full[1]\n            print('   > Max r         : %g' % r)\n        spacing = full[0][0]\n        shift = full[0][1]\n    else:  # Print what we can without full\n        n = filt.base.size\n        a = filt.base[-1]\n        b = filt.base[-2]\n        spacing = np.log(a)-np.log(b)\n        shift = np.log(a)-spacing*(n//2)\n    print('   > Spacing       : %1.10g' % spacing)\n    print('   > Shift         : %1.10g' % shift)\n    print('   > Base min/max  : %e / %e' % (filt.base.min(), filt.base.max()))", "response": "r Print best filter information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _call_qc_transform_pairs(n, ispacing, ishift, fI, fC, r, r_def, reim):\n    print('* QC: Input transform-pairs:')\n    print('  fC: x-range defined through ``n``, ``spacing``, ``shift``, and ' +\n          '``r``-parameters; b-range defined through ``r``-parameter.')\n    print('  fI: x- and b-range defined through ``n``, ``spacing``' +\n          ', ``shift``, and ``r_def``-parameters.')\n\n    # Calculate min/max k, from minimum and maximum spacing/shift\n    minspace = np.arange(*ispacing).min()\n    maxspace = np.arange(*ispacing).max()\n    minshift = np.arange(*ishift).min()\n    maxshift = np.arange(*ishift).max()\n\n    maxbase = np.exp(maxspace*(n//2) + maxshift)\n    minbase = np.exp(maxspace*(-n//2+1) + minshift)\n\n    # For fC-r  (k defined with same amount of points as r)\n    kmax = maxbase/r.min()\n    kmin = minbase/r.max()\n    k = np.logspace(np.log10(kmin), np.log10(kmax) + minspace, r.size)\n\n    # For fI-r\n    rI = np.logspace(np.log10(1/maxbase) - r_def[0],\n                     np.log10(1/minbase) + r_def[1], r_def[2]*n)\n    kmaxI = maxbase/rI.min()\n    kminI = minbase/rI.max()\n    kI = np.logspace(np.log10(kminI), np.log10(kmaxI) + minspace,\n                     r_def[2]*n)\n\n    # Plot QC\n    fig, axs = plt.subplots(figsize=(9.5, 6), nrows=2, ncols=2,\n                            num=\"Transform pairs\")\n    axs = axs.ravel()\n    plt.subplots_adjust(wspace=.3, hspace=.4)\n\n    _plot_transform_pairs(fC, r, k, axs[:2], 'fC')\n    if reim == np.real:\n        tit = 'RE(fI)'\n    else:\n        tit = 'IM(fI)'\n    _plot_transform_pairs(fI, rI, kI, axs[2:], tit)\n\n    fig.canvas.draw()  # To force draw in notebook while running\n    plt.show()", "response": "r QC the input transform pairs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _plot_inversion(f, rhs, r, k, imin, spacing, shift, cvar):\n\n    # Check matplotlib (soft dependency)\n    if not plt:\n        print(plt_msg)\n        return\n\n    plt.figure(\"Inversion result \"+f.name, figsize=(9.5, 4))\n    plt.subplots_adjust(wspace=.3, bottom=0.2)\n    plt.clf()\n\n    tk = np.logspace(np.log10(k.min()), np.log10(k.max()), r.size)\n\n    plt.suptitle(f.name+'; Spacing ::'+str(spacing)+'; Shift ::'+str(shift))\n\n    # Plot lhs\n    plt.subplot(121)\n    plt.title('|lhs|')\n    if f.name == 'j2':\n        lhs = f.lhs(tk)\n        plt.loglog(tk, np.abs(lhs[0]), lw=2, label='Theoretical J0')\n        plt.loglog(tk, np.abs(lhs[1]), lw=2, label='Theoretical J1')\n    else:\n        plt.loglog(tk, np.abs(f.lhs(tk)), lw=2, label='Theoretical')\n    plt.xlabel('l')\n    plt.legend(loc='best')\n\n    # Plot rhs\n    plt.subplot(122)\n    plt.title('|rhs|')\n\n    # Transform pair rhs\n    plt.loglog(r, np.abs(f.rhs), lw=2, label='Theoretical')\n\n    # Transform with filter\n    plt.loglog(r, np.abs(rhs), '-.', lw=2, label='This filter')\n\n    # Plot minimum amplitude or max r, respectively\n    if cvar == 'amp':\n        label = 'Min. Amp'\n    else:\n        label = 'Max. r'\n    plt.loglog(r[imin], np.abs(rhs[imin]), 'go', label=label)\n\n    plt.xlabel('r')\n    plt.legend(loc='best')\n    plt.gcf().canvas.draw()  # To force draw in notebook while running\n    plt.show()", "response": "r Plot the inversion of a filter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef j0_1(a=1):\n\n    def lhs(x):\n        return x*np.exp(-a*x**2)\n\n    def rhs(b):\n        return np.exp(-b**2/(4*a))/(2*a)\n\n    return Ghosh('j0', lhs, rhs)", "response": "rHankel transform pair J0_1 [ Ande75_ ] _"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef j0_2(a=1):\n\n    def lhs(x):\n        return np.exp(-a*x)\n\n    def rhs(b):\n        return 1/np.sqrt(b**2 + a**2)\n\n    return Ghosh('j0', lhs, rhs)", "response": "rHankel transform pair J0_2 [ Ande75_ ] _"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef j1_2(a=1):\n\n    def lhs(x):\n        return np.exp(-a*x)\n\n    def rhs(b):\n        return (np.sqrt(b**2 + a**2) - a)/(b*np.sqrt(b**2 + a**2))\n\n    return Ghosh('j1', lhs, rhs)", "response": "rHankel transform pair J1_2 [ Ande75_ ] _"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef empy_hankel(ftype, zsrc, zrec, res, freqtime, depth=None, aniso=None,\n                epermH=None, epermV=None, mpermH=None, mpermV=None,\n                htarg=None, verblhs=0, verbrhs=0):\n    r\"\"\"Numerical transform pair with empymod.\n\n    All parameters except ``ftype``, ``verblhs``, and ``verbrhs`` correspond to\n    the input parameters to ``empymod.dipole``. See there for more information.\n\n    Note that if depth=None or [], the analytical full-space solutions will be\n    used (much faster).\n\n    Parameters\n    ----------\n    ftype : str or list of strings\n        Either of: {'j0', 'j1', 'j2', ['j0', 'j1']}\n\n        - 'j0': Analyze J0-term with ab=11, angle=45\u00b0\n        - 'j1': Analyze J1-term with ab=31, angle=0\u00b0\n        - 'j2': Analyze J0- and J1-terms jointly with ab=12, angle=45\u00b0\n        - ['j0', 'j1']: Same as calling empy_hankel twice, once with 'j0' and\n                        one with 'j1'; can be provided like this to\n                        fdesign.design.\n\n    verblhs, verbrhs: int\n        verb-values provided to empymod for lhs and rhs.\n\n    Note that ftype='j2' only works for fC, not for fI.\n\n    \"\"\"\n\n    # Loop over ftypes, if there are several\n    if isinstance(ftype, list):\n        out = []\n        for f in ftype:\n            out.append(empy_hankel(f, zsrc, zrec, res, freqtime, depth, aniso,\n                                   epermH, epermV, mpermH, mpermV, htarg,\n                                   verblhs, verbrhs))\n        return out\n\n    # Collect model\n    model = {'src': [0, 0, zsrc],\n             'depth': depth,\n             'res': res,\n             'aniso': aniso,\n             'epermH': epermH,\n             'epermV': epermV,\n             'mpermH': mpermH,\n             'mpermV': mpermV}\n\n    # Finalize model depending on ftype\n    if ftype == 'j0':  # J0: 11, 45\u00b0\n        model['ab'] = 11\n        x = 1/np.sqrt(2)\n        y = 1/np.sqrt(2)\n\n    elif ftype == 'j1':  # J1: 31, 0\u00b0\n        model['ab'] = 31\n        x = 1\n        y = 0\n\n    elif ftype == 'j2':  # J2: 12, 45\u00b0\n        model['ab'] = 12\n        x = 1/np.sqrt(2)\n        y = 1/np.sqrt(2)\n\n    # rhs: empymod.model.dipole\n    # If depth=[], the analytical full-space solution will be used internally\n    def rhs(r):\n        out = dipole(rec=[r*x, r*y, zrec], ht='qwe', xdirect=True,\n                     verb=verbrhs, htarg=htarg, freqtime=freqtime, **model)\n        return out\n\n    # lhs: empymod.model.dipole_k\n    def lhs(k):\n        lhs0, lhs1 = dipole_k(rec=[x, y, zrec], wavenumber=k, verb=verblhs,\n                              freq=freqtime, **model)\n        if ftype == 'j0':\n            return lhs0\n        elif ftype == 'j1':\n            return lhs1\n        elif ftype == 'j2':\n            return (lhs0, lhs1)\n\n    return Ghosh(ftype, lhs, rhs)", "response": "r This function returns a sequence of the numerical transform pairs for a given frequency time and frequency pair."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_min_val(spaceshift, *params):\n\n    # Get parameters from tuples\n    spacing, shift = spaceshift\n    n, fI, fC, r, r_def, error, reim, cvar, verb, plot, log = params\n\n    # Get filter for these parameters\n    dlf = _calculate_filter(n, spacing, shift, fI, r_def, reim, 'filt')\n\n    # Calculate rhs-response with this filter\n    k = dlf.base/r[:, None]\n\n    # Loop over transforms\n    for i, f in enumerate(fC):\n        # Calculate lhs and rhs; rhs depends on ftype\n        lhs = f.lhs(k)\n        if f.name == 'j2':\n            rhs0 = np.dot(lhs[0], getattr(dlf, 'j0'))/r\n            rhs1 = np.dot(lhs[1], getattr(dlf, 'j1'))/r**2\n            rhs = rhs0 + rhs1\n        else:\n            rhs = np.dot(lhs, getattr(dlf, f.name))/r\n\n        # Get relative error\n        rel_error = np.abs((rhs - f.rhs)/f.rhs)\n\n        # Get indices where relative error is bigger than error\n        imin0 = np.where(rel_error > error)[0]\n\n        # Find first occurrence of failure\n        if np.all(rhs == 0) or np.all(np.isnan(rhs)):\n            # if all rhs are zeros or nans, the filter is useless\n            imin0 = 0\n\n        elif imin0.size == 0:\n            # if imin0.size == 0:  # empty array, all rel_error < error.\n            imin0 = rhs.size-1  # set to last r\n            if verb > 0 and log['warn-r'] == 0:\n                print('* WARNING :: all data have error < ' + str(error) +\n                      '; choose larger r or set error-level higher.')\n                log['warn-r'] = 1  # Only do this once\n\n        else:\n            # Kind of a dirty hack: Permit to jump up to four bad values,\n            # resulting for instance from high rel_error from zero crossings\n            # of the transform pair. Should be made an input argument or\n            # generally improved.\n            if imin0.size > 4:\n                imin0 = np.max([0, imin0[4]-5])\n            else:  # just take the first one (no jumping allowed; normal case)\n                imin0 = np.max([0, imin0[0]-1])\n            # Note that both version yield the same result if the failure is\n            # consistent.\n\n        # Depending on cvar, store minimum amplitude or 1/maxr\n        if cvar == 'amp':\n            min_val0 = np.abs(rhs[imin0])\n        else:\n            min_val0 = 1/r[imin0]\n\n        # Check if this inversion is better than previous ones\n        if i == 0:  # First run, store these values\n            imin = dc(imin0)\n            min_val = dc(min_val0)\n        else:  # Replace imin, min_val if this one is better\n            if min_val0 > min_val:\n                min_val = dc(min_val0)\n                imin = dc(imin0)\n\n        # QC plot\n        if plot > 2:\n            _plot_inversion(f, rhs, r, k, imin0, spacing, shift, cvar)\n\n    # If verbose, print progress\n    if verb > 1:\n        log = _print_count(log)\n\n    # If there is no point with rel_error < error (imin=0) it returns np.inf.\n    return np.where(imin == 0, np.inf, min_val)", "response": "r Calculate minimum resolved amplitude or maximum r."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wavenumber(zsrc, zrec, lsrc, lrec, depth, etaH, etaV, zetaH, zetaV, lambd,\n               ab, xdirect, msrc, mrec, use_ne_eval):\n    r\"\"\"Calculate wavenumber domain solution.\n\n    Return the wavenumber domain solutions ``PJ0``, ``PJ1``, and ``PJ0b``,\n    which have to be transformed with a Hankel transform to the frequency\n    domain. ``PJ0``/``PJ0b`` and ``PJ1`` have to be transformed with Bessel\n    functions of order 0 (:math:`J_0`) and 1 (:math:`J_1`), respectively.\n\n    This function corresponds loosely to equations 105--107, 111--116,\n    119--121, and 123--128 in [HuTS15]_, and equally loosely to the file\n    ``kxwmod.c``.\n\n    [HuTS15]_ uses Bessel functions of orders 0, 1, and 2 (:math:`J_0, J_1,\n    J_2`). The implementations of the *Fast Hankel Transform* and the\n    *Quadrature-with-Extrapolation* in ``transform`` are set-up with Bessel\n    functions of order 0 and 1 only. This is achieved by applying the\n    recurrence formula\n\n    .. math:: J_2(kr) = \\frac{2}{kr} J_1(kr) - J_0(kr) \\ .\n\n\n    .. note::\n\n        ``PJ0`` and ``PJ0b`` could theoretically be added here into one, and\n        then be transformed in one go.  However, ``PJ0b`` has to be multiplied\n        by ``factAng`` later. This has to be done after the Hankel transform\n        for methods which make use of spline interpolation, in order to work\n        for offsets that are not in line with each other.\n\n    This function is called from one of the Hankel functions in\n    :mod:`transform`.  Consult the modelling routines in :mod:`model` for a\n    description of the input and output parameters.\n\n    If you are solely interested in the wavenumber-domain solution you can call\n    this function directly. However, you have to make sure all input arguments\n    are correct, as no checks are carried out here.\n\n    \"\"\"\n\n    # ** CALCULATE GREEN'S FUNCTIONS\n    # Shape of PTM, PTE: (nfreq, noffs, nfilt)\n    PTM, PTE = greenfct(zsrc, zrec, lsrc, lrec, depth, etaH, etaV, zetaH,\n                        zetaV, lambd, ab, xdirect, msrc, mrec, use_ne_eval)\n\n    # ** AB-SPECIFIC COLLECTION OF PJ0, PJ1, AND PJ0b\n\n    # Pre-allocate output\n    PJ0 = None\n    PJ1 = None\n    PJ0b = None\n\n    # Calculate Ptot which is used in all cases\n    Ptot = (PTM + PTE)/(4*np.pi)\n\n    # If rec is magnetic switch sign (reciprocity MM/ME => EE/EM).\n    if mrec:\n        sign = -1\n    else:\n        sign = 1\n\n    # Group into PJ0 and PJ1 for J0/J1 Hankel Transform\n    if ab in [11, 12, 21, 22, 14, 24, 15, 25]:    # Eqs 105, 106, 111, 112,\n        # J2(kr) = 2/(kr)*J1(kr) - J0(kr)         #     119, 120, 123, 124\n        if ab in [14, 22]:\n            sign *= -1\n        PJ0b = sign/2*Ptot*lambd\n        PJ1 = -sign*Ptot\n        if ab in [11, 22, 24, 15]:\n            if ab in [22, 24]:\n                sign *= -1\n            PJ0 = sign*(PTM - PTE)/(8*np.pi)*lambd\n\n    elif ab in [13, 23, 31, 32, 34, 35, 16, 26]:  # Eqs 107, 113, 114, 115,\n        PJ1 = sign*Ptot*lambd*lambd               # .   121, 125, 126, 127\n        if ab in [34, 26]:\n            PJ1 *= -1\n\n    elif ab in [33, ]:                            # Eq 116\n        PJ0 = sign*Ptot*lambd*lambd*lambd\n\n    # Return PJ0, PJ1, PJ0b\n    return PJ0, PJ1, PJ0b", "response": "r Calculates the wavenumber domain solution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef greenfct(zsrc, zrec, lsrc, lrec, depth, etaH, etaV, zetaH, zetaV, lambd,\n             ab, xdirect, msrc, mrec, use_ne_eval):\n    r\"\"\"Calculate Green's function for TM and TE.\n\n    .. math:: \\tilde{g}^{tm}_{hh}, \\tilde{g}^{tm}_{hz},\n              \\tilde{g}^{tm}_{zh}, \\tilde{g}^{tm}_{zz},\n              \\tilde{g}^{te}_{hh}, \\tilde{g}^{te}_{zz}\n\n    This function corresponds to equations 108--110, 117/118, 122; 89--94,\n    A18--A23, B13--B15; 97--102 A26--A31, and B16--B18 in [HuTS15]_, and\n    loosely to the corresponding files ``Gamma.F90``, ``Wprop.F90``,\n    ``Ptotalx.F90``, ``Ptotalxm.F90``, ``Ptotaly.F90``, ``Ptotalym.F90``,\n    ``Ptotalz.F90``, and ``Ptotalzm.F90``.\n\n    The Green's functions are multiplied according to Eqs 105-107, 111-116,\n    119-121, 123-128; with the factors inside the integrals.\n\n    This function is called from the function :mod:`kernel.wavenumber`.\n\n    \"\"\"\n    # GTM/GTE have shape (frequency, offset, lambda).\n    # gamTM/gamTE have shape (frequency, offset, layer, lambda):\n\n    # Reciprocity switches for magnetic receivers\n    if mrec:\n        if msrc:  # If src is also magnetic, switch eta and zeta (MM => EE).\n            # G^mm_ab(s, r, e, z) = -G^ee_ab(s, r, -z, -e)\n            etaH, zetaH = -zetaH, -etaH\n            etaV, zetaV = -zetaV, -etaV\n        else:  # If src is electric, swap src and rec (ME => EM).\n            # G^me_ab(s, r, e, z) = -G^em_ba(r, s, e, z)\n            zsrc, zrec = zrec, zsrc\n            lsrc, lrec = lrec, lsrc\n\n    for TM in [True, False]:\n\n        # Continue if Green's function not required\n        if TM and ab in [16, 26]:\n            continue\n        elif not TM and ab in [13, 23, 31, 32, 33, 34, 35]:\n            continue\n\n        # Define eta/zeta depending if TM or TE\n        if TM:\n            e_zH, e_zV, z_eH = etaH, etaV, zetaH   # TM: zetaV not used\n        else:\n            e_zH, e_zV, z_eH = zetaH, zetaV, etaH  # TE: etaV not used\n\n        # Uppercase gamma\n        if use_ne_eval:\n            ez_ratio = (e_zH/e_zV)[:, None, :, None]  # NOQA\n            ez_prod = (z_eH*e_zH)[:, None, :, None]  # NOQA\n            lambd2 = use_ne_eval(\"lambd*lambd\")[None, :, None, :]  # NOQA\n            Gam = use_ne_eval(\"sqrt(ez_ratio*lambd2 + ez_prod)\")\n        else:\n            Gam = np.sqrt((e_zH/e_zV)[:, None, :, None] *\n                          (lambd*lambd)[None, :, None, :] +\n                          (z_eH*e_zH)[:, None, :, None])\n\n        # Gamma in receiver layer\n        lrecGam = Gam[:, :, lrec, :]\n\n        # Reflection (coming from below (Rp) and above (Rm) rec)\n        if depth.size > 1:  # Only if more than 1 layer\n            Rp, Rm = reflections(depth, e_zH, Gam, lrec, lsrc, use_ne_eval)\n\n            # Field propagators\n            # (Up- (Wu) and downgoing (Wd), in rec layer); Eq 74\n            if lrec != depth.size-1:  # No upgoing field prop. if rec in last\n                ddepth = depth[lrec + 1] - zrec\n                if use_ne_eval:\n                    Wu = use_ne_eval(\"exp(-lrecGam*ddepth)\")\n                else:\n                    Wu = np.exp(-lrecGam*ddepth)\n            else:\n                Wu = np.zeros(lrecGam.shape, dtype=complex)\n            if lrec != 0:     # No downgoing field propagator if rec in first\n                ddepth = zrec - depth[lrec]\n                if use_ne_eval:\n                    Wd = use_ne_eval(\"exp(-lrecGam*ddepth)\")\n                else:\n                    Wd = np.exp(-lrecGam*ddepth)\n            else:\n                Wd = np.zeros(lrecGam.shape, dtype=complex)\n\n            # Field at rec level (coming from below (Pu) and above (Pd) rec)\n            Pu, Pd = fields(depth, Rp, Rm, Gam, lrec, lsrc, zsrc, ab, TM,\n                            use_ne_eval)\n\n        # Green's functions\n        if lsrc == lrec:  # Rec in src layer; Eqs 108, 109, 110, 117, 118, 122\n\n            # Green's function depending on <ab>\n            if depth.size == 1:  # If only one layer, no reflections/fields\n                green = np.zeros(lrecGam.shape, dtype=complex)\n            elif ab in [13, 23, 31, 32, 14, 24, 15, 25]:\n                green = Pu*Wu - Pd*Wd\n            else:\n                green = Pu*Wu + Pd*Wd\n\n            # Direct field, if it is computed in the wavenumber domain\n            if not xdirect:\n                # Direct field\n                directf = np.exp(-lrecGam*abs(zsrc - zrec))\n\n                # Swap TM for certain <ab>\n                if TM and ab in [11, 12, 13, 14, 15, 21, 22, 23, 24, 25]:\n                    directf *= -1\n\n                # Multiply by zrec-zsrc-sign for certain <ab>\n                if ab in [13, 14, 15, 23, 24, 25, 31, 32]:\n                    directf *= np.sign(zrec - zsrc)\n\n                # Add direct field to Green's function\n                green += directf\n\n        else:\n\n            # Calculate exponential factor\n            if lrec == depth.size-1:\n                ddepth = 0\n            else:\n                ddepth = depth[lrec+1] - depth[lrec]\n            if use_ne_eval:\n                fexp = use_ne_eval(\"exp(-lrecGam*ddepth)\")\n            else:\n                fexp = np.exp(-lrecGam*ddepth)\n\n            # Sign-switch for Green calculation\n            if TM and ab in [11, 12, 13, 21, 22, 23, 14, 24, 15, 25]:\n                pmw = -1\n            else:\n                pmw = 1\n\n            if lrec < lsrc:  # Rec above src layer: Pd not used\n                #              Eqs 89-94, A18-A23, B13-B15\n                green = Pu*(Wu + pmw*Rm[:, :, 0, :]*fexp*Wd)\n\n            elif lrec > lsrc:  # rec below src layer: Pu not used\n                #                Eqs 97-102 A26-A30, B16-B18\n                green = Pd*(pmw*Wd + Rp[:, :, abs(lsrc-lrec), :]*fexp*Wu)\n\n        # Store in corresponding variable\n        if TM:\n            gamTM, GTM = Gam, green\n        else:\n            gamTE, GTE = Gam, green\n\n    # ** AB-SPECIFIC FACTORS AND CALCULATION OF PTOT'S\n    # These are the factors inside the integrals\n    # Eqs 105-107, 111-116, 119-121, 123-128\n\n    # PTM, PTE\n    if ab in [11, 12, 21, 22]:\n        PTM = GTM*gamTM[:, :, lrec, :]/etaH[:, None, lrec, None]\n        PTE = zetaH[:, None, lsrc, None]*GTE/gamTE[:, :, lsrc, :]\n    elif ab in [14, 15, 24, 25]:\n        PTM = ((etaH[:, lsrc]/etaH[:, lrec])[:, None, None] *\n               GTM*gamTM[:, :, lrec, :]/gamTM[:, :, lsrc, :])\n        PTE = GTE\n    elif ab in [13, 23]:\n        PTM = -((etaH[:, lsrc]/etaH[:, lrec]/etaV[:, lsrc])[:, None, None] *\n                GTM*gamTM[:, :, lrec, :]/gamTM[:, :, lsrc, :])\n        PTE = 0\n    elif ab in [31, 32]:\n        PTM = GTM/etaV[:, None, lrec, None]\n        PTE = 0\n    elif ab in [34, 35]:\n        PTM = ((etaH[:, lsrc]/etaV[:, lrec])[:, None, None] *\n               GTM/gamTM[:, :, lsrc, :])\n        PTE = 0\n    elif ab in [16, 26]:\n        PTM = 0\n        PTE = ((zetaH[:, lsrc]/zetaV[:, lsrc])[:, None, None] *\n               GTE/gamTE[:, :, lsrc, :])\n    elif ab in [33, ]:\n        PTM = ((etaH[:, lsrc]/etaV[:, lsrc]/etaV[:, lrec])[:, None, None] *\n               GTM/gamTM[:, :, lsrc, :])\n        PTE = 0\n\n    # Return Green's functions\n    return PTM, PTE", "response": "r Calculates the green fct function for the given source and recusivity."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef angle_factor(angle, ab, msrc, mrec):\n\n    # 33/66 are completely symmetric and hence independent of angle\n    if ab in [33, ]:\n        return np.ones(angle.size)\n\n    # Evaluation angle\n    eval_angle = angle.copy()\n\n    # Add pi if receiver is magnetic (reciprocity), but not if source is\n    # electric, because then source and receiver are swapped, ME => EM:\n    # G^me_ab(s, r, e, z) = -G^em_ba(r, s, e, z).\n    if mrec and not msrc:\n        eval_angle += np.pi\n\n    # Define fct (cos/sin) and angles to be tested\n    if ab in [11, 22, 15, 24, 13, 31, 26, 35]:\n        fct = np.cos\n        test_ang_1 = np.pi/2\n        test_ang_2 = 3*np.pi/2\n    else:\n        fct = np.sin\n        test_ang_1 = np.pi\n        test_ang_2 = 2*np.pi\n\n    if ab in [11, 22, 15, 24, 12, 21, 14, 25]:\n        eval_angle *= 2\n\n    # Get factor\n    factAng = fct(eval_angle)\n\n    # Ensure cos([pi/2, 3pi/2]) and sin([pi, 2pi]) are zero (floating pt issue)\n    factAng[np.isclose(np.abs(eval_angle), test_ang_1, 1e-10, 1e-14)] = 0\n    factAng[np.isclose(np.abs(eval_angle), test_ang_2, 1e-10, 1e-14)] = 0\n\n    return factAng", "response": "r Return the angle - dependent factor of the given angle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _repr_html_(self):\n        # Check ncol\n        ncol = int(self.ncol)\n\n        # Define html-styles\n        border = \"border: 2px solid #fff;'\"\n\n        def colspan(html, txt, ncol, nrow):\n            r\"\"\"Print txt in a row spanning whole table.\"\"\"\n            html += \"  <tr>\\n\"\n            html += \"     <td style='text-align: center; \"\n            if nrow == 0:\n                html += \"font-weight: bold; font-size: 1.2em; \"\n            elif nrow % 2 == 0:\n                html += \"background-color: #ddd;\"\n            html += border + \" colspan='\"\n            html += str(2*ncol)+\"'>%s</td>\\n\" % txt\n            html += \"  </tr>\\n\"\n            return html\n\n        def cols(html, version, name, ncol, i):\n            r\"\"\"Print package information in two cells.\"\"\"\n\n            # Check if we have to start a new row\n            if i > 0 and i % ncol == 0:\n                html += \"  </tr>\\n\"\n                html += \"  <tr>\\n\"\n\n            html += \"    <td style='text-align: right; background-color: \"\n            html += \"#ccc; \" + border + \">%s</td>\\n\" % version\n\n            html += \"    <td style='text-align: left; \"\n            html += border + \">%s</td>\\n\" % name\n\n            return html, i+1\n\n        # Start html-table\n        html = \"<table style='border: 3px solid #ddd;'>\\n\"\n\n        # Date and time info as title\n        html = colspan(html, time.strftime('%a %b %d %H:%M:%S %Y %Z'), ncol, 0)\n\n        # OS and CPUs\n        html += \"  <tr>\\n\"\n        html, i = cols(html, platform.system(), 'OS', ncol, 0)\n        html, i = cols(html, multiprocessing.cpu_count(), 'CPU(s)', ncol, i)\n\n        # Loop over packages\n        for pckg in self._get_packages(self.add_pckg):\n            html, i = cols(html, pckg.__version__, pckg.__name__, ncol, i)\n        # Fill up the row\n        while i % ncol != 0:\n            html += \"    <td style= \" + border + \"></td>\\n\"\n            html += \"    <td style= \" + border + \"></td>\\n\"\n            i += 1\n        # Finish row\n        html += \"  </tr>\\n\"\n\n        # sys.version\n        html = colspan(html, sys.version, ncol, 1)\n\n        # mkl version\n        if mklinfo:\n            html = colspan(html, mklinfo, ncol, 2)\n\n        # Finish table\n        html += \"</table>\"\n\n        return html", "response": "r Represents the HTML - rendered versions information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kong_61_2007():\n\n    dlf = DigitalFilter('Kong 61', 'kong_61_2007')\n\n    dlf.base = np.array([\n            2.3517745856009100e-02, 2.6649097336355482e-02,\n            3.0197383422318501e-02, 3.4218118311666032e-02,\n            3.8774207831722009e-02, 4.3936933623407420e-02,\n            4.9787068367863938e-02, 5.6416139503777350e-02,\n            6.3927861206707570e-02, 7.2439757034251456e-02,\n            8.2084998623898800e-02, 9.3014489210663506e-02,\n            1.0539922456186430e-01, 1.1943296826671961e-01,\n            1.3533528323661270e-01, 1.5335496684492850e-01,\n            1.7377394345044520e-01, 1.9691167520419400e-01,\n            2.2313016014842979e-01, 2.5283959580474641e-01,\n            2.8650479686019009e-01, 3.2465246735834979e-01,\n            3.6787944117144239e-01, 4.1686201967850839e-01,\n            4.7236655274101469e-01, 5.3526142851899028e-01,\n            6.0653065971263342e-01, 6.8728927879097224e-01,\n            7.7880078307140488e-01, 8.8249690258459546e-01,\n            1.0000000000000000e+00, 1.1331484530668261e+00,\n            1.2840254166877421e+00, 1.4549914146182010e+00,\n            1.6487212707001280e+00, 1.8682459574322221e+00,\n            2.1170000166126748e+00, 2.3988752939670981e+00,\n            2.7182818284590451e+00, 3.0802168489180310e+00,\n            3.4903429574618419e+00, 3.9550767229205772e+00,\n            4.4816890703380636e+00, 5.0784190371800806e+00,\n            5.7546026760057307e+00, 6.5208191203301116e+00,\n            7.3890560989306504e+00, 8.3728974881272649e+00,\n            9.4877358363585262e+00, 1.0751013186076360e+01,\n            1.2182493960703470e+01, 1.3804574186067100e+01,\n            1.5642631884188170e+01, 1.7725424121461639e+01,\n            2.0085536923187671e+01, 2.2759895093526730e+01,\n            2.5790339917193059e+01, 2.9224283781234941e+01,\n            3.3115451958692312e+01, 3.7524723159601002e+01,\n            4.2521082000062783e+01])\n\n    dlf.factor = np.array([1.1331484530668261])\n\n    dlf.j0 = np.array([\n            1.4463210615326699e+02, -1.1066222143752420e+03,\n            3.7030010025325978e+03, -6.8968188464424520e+03,\n            7.1663544112656937e+03, -2.4507884783377681e+03,\n            -4.0166567754046082e+03, 6.8623845298546094e+03,\n            -5.0013321011775661e+03, 2.1291291365196648e+03,\n            -1.3845222435542289e+03, 2.1661554291595580e+03,\n            -2.2260393789657141e+03, 8.0317156013986391e+02,\n            1.0142221718890841e+03, -1.9350455051432630e+03,\n            1.6601169447226580e+03, -7.5159684285420133e+02,\n            -9.0315984178183285e+01, 5.0705574889546148e+02,\n            -5.1207646422722519e+02, 2.9722959494490038e+02,\n            -5.0248319908072993e+01, -1.2290725861955920e+02,\n            1.9695244755899429e+02, -1.9175679966946601e+02,\n            1.4211755630338590e+02, -7.7463216543224149e+01,\n            1.7638009334931201e+01, 2.8855056499202671e+01,\n            -5.9225643887809561e+01, 7.5987941373668960e+01,\n            -8.1687962781233580e+01, 8.0599209238447102e+01,\n            -7.4895905328771619e+01, 6.7516291538794434e+01,\n            -5.9325033647358048e+01, 5.1617042242841528e+01,\n            -4.4664967446820263e+01, 3.8366152052928278e+01,\n            -3.3308787868993100e+01, 2.8278671651033459e+01,\n            -2.4505863388620480e+01, 2.0469632532079750e+01,\n            -1.7074034940700429e+01, 1.4206119215530070e+01,\n            -1.0904435643084650e+01, 8.7518389425802283e+00,\n            -6.7721665239085622e+00, 4.5096884588095891e+00,\n            -3.2704247166629590e+00, 2.6827195063720430e+00,\n            -1.8406031821386459e+00, 9.1586697140412443e-01,\n            -3.2436011485890798e-01, 8.0675176189581893e-02,\n            -1.2881307195759690e-02, 7.0489137468452920e-04,\n            2.3846917590855061e-04, -6.9102205995825531e-05,\n            6.7792635718095777e-06])\n\n    dlf.j1 = np.array([\n            4.6440396425864918e+01, -4.5034239857914162e+02,\n            1.7723440076223640e+03, -3.7559735516994660e+03,\n            4.4736494009764137e+03, -2.2476603569606068e+03,\n            -1.5219842155931799e+03, 3.4904608559273802e+03,\n            -2.4814243247472318e+03, 5.7328164634108396e+02,\n            5.3132044837659631e-01, 6.8895205008006235e+02,\n            -1.2012013872160269e+03, 7.9679138423597340e+02,\n            4.9874460187939818e+01, -5.6367338332457007e+02,\n            4.7971936503711203e+02, -5.8979702298044558e+01,\n            -3.1935800954986922e+02, 4.5762551999442371e+02,\n            -3.7239927283248380e+02, 1.8255852885279569e+02,\n            -2.3504740340815669e-01, -1.1588151583545380e+02,\n            1.5740956677133170e+02, -1.4334746114883359e+02,\n            9.9857411013284818e+01, -4.8246322019171487e+01,\n            2.0371404343057380e+00, 3.3003938094974323e+01,\n            -5.5476151884197712e+01, 6.7354852323852583e+01,\n            -7.0735403363284121e+01, 6.8872932663164747e+01,\n            -6.3272750944993042e+01, 5.6501568721817442e+01,\n            -4.8706577819918110e+01, 4.1737211284663481e+01,\n            -3.4776621242200903e+01, 2.9161717578906430e+01,\n            -2.3886749056000909e+01, 1.9554007583544220e+01,\n            -1.5966397353366460e+01, 1.2429310210239199e+01,\n            -1.0139180791868180e+01, 7.4716493393871861e+00,\n            -5.5509479014742613e+00, 4.3380799768234208e+00,\n            -2.5911516181746550e+00, 1.6300524630626780e+00,\n            -1.4041567266387460e+00, 7.5225141726873213e-01,\n            4.6808777208492733e-02, -3.6630197849601159e-01,\n            2.8948389902792782e-01, -1.3705521898064801e-01,\n            4.6292091649913013e-02, -1.1721281347435180e-02,\n            2.2002397354029149e-03, -2.8146036357227600e-04,\n            1.8788896009128770e-05])\n\n    return dlf", "response": "Return a base filter for Kong 61 pt Hankel filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef key_81_CosSin_2009():\n\n    dlf = DigitalFilter('Key 81 CosSin (2009)', 'key_81_CosSin_2009')\n\n    dlf.base = np.array([\n        3.354626279025119e-04, 4.097349789797864e-04,  5.004514334406104e-04,\n        6.112527611295723e-04, 7.465858083766792e-04,  9.118819655545162e-04,\n        1.113775147844802e-03, 1.360368037547893e-03,  1.661557273173934e-03,\n        2.029430636295734e-03, 2.478752176666358e-03,  3.027554745375813e-03,\n        3.697863716482929e-03, 4.516580942612666e-03,  5.516564420760772e-03,\n        6.737946999085467e-03, 8.229747049020023e-03,  1.005183574463358e-02,\n        1.227733990306844e-02, 1.499557682047770e-02,  1.831563888873418e-02,\n        2.237077185616559e-02, 2.732372244729256e-02,  3.337326996032607e-02,\n        4.076220397836620e-02, 4.978706836786394e-02,  6.081006262521795e-02,\n        7.427357821433388e-02, 9.071795328941247e-02,  1.108031583623339e-01,\n        1.353352832366127e-01, 1.652988882215865e-01,  2.018965179946554e-01,\n        2.465969639416064e-01, 3.011942119122020e-01,  3.678794411714423e-01,\n        4.493289641172216e-01, 5.488116360940264e-01,  6.703200460356393e-01,\n        8.187307530779818e-01, 1e0,  1.221402758160170e+00,\n        1.491824697641270e+00, 1.822118800390509e+00,  2.225540928492468e+00,\n        2.718281828459046e+00, 3.320116922736548e+00,  4.055199966844675e+00,\n        4.953032424395115e+00, 6.049647464412947e+00,  7.389056098930650e+00,\n        9.025013499434122e+00, 1.102317638064160e+01,  1.346373803500169e+01,\n        1.644464677109706e+01, 2.008553692318767e+01,  2.453253019710935e+01,\n        2.996410004739703e+01, 3.659823444367799e+01,  4.470118449330084e+01,\n        5.459815003314424e+01, 6.668633104092515e+01,  8.145086866496814e+01,\n        9.948431564193386e+01, 1.215104175187350e+02,  1.484131591025766e+02,\n        1.812722418751512e+02, 2.214064162041872e+02,  2.704264074261528e+02,\n        3.302995599096489e+02, 4.034287934927351e+02,  4.927490410932563e+02,\n        6.018450378720822e+02, 7.350951892419732e+02,  8.978472916504184e+02,\n        1.096633158428459e+03, 1.339430764394418e+03,  1.635984429995927e+03,\n        1.998195895104119e+03, 2.440601977624501e+03,  2.980957987041728e+03])\n\n    dlf.factor = np.array([1.2214027581601701])\n\n    dlf.cos = np.array([\n        1.746412733678043e-02, -7.658725022064888e-02, 1.761673907472465e-01,\n        -2.840940679113589e-01,  3.680388960144733e-01, -4.115498161707958e-01,\n        4.181209762362728e-01, -3.967204599348831e-01, 3.608829691008270e-01,\n        -3.171870084102961e-01,  2.744932842186247e-01, -2.324673650676961e-01,\n        1.971144816936984e-01, -1.634915360178986e-01, 1.381406405905393e-01,\n        -1.125728533897677e-01,  9.619580319372194e-02, -7.640431432353632e-02,\n        6.748891657821673e-02, -5.097864570224415e-02, 4.853609305288441e-02,\n        -3.293272689265632e-02,  3.677175984620380e-02, -1.969323595300588e-02,\n        3.053726798991684e-02, -9.301135480582538e-03, 2.895215492109734e-02,\n        -1.875526095801418e-04,  3.181452657662026e-02, 9.025726238227111e-03,\n        3.955376604096631e-02,  1.966766645672513e-02, 5.318782805621459e-02,\n        3.300575875620110e-02,  7.409212944640006e-02, 4.972863917303501e-02,\n        1.029344264288086e-01,  6.776855697600163e-02, 1.357865756912759e-01,\n        7.511614666518443e-02,  1.522218287240260e-01, 3.034571997381229e-02,\n        8.802563675323094e-02, -1.689255322598353e-01, -1.756581788680092e-01,\n        -6.123863775740898e-01, -5.098359641153184e-01, -6.736869803920745e-01,\n        4.599561125225532e-01,  8.907010262082216e-01, 1.039153770711999e+00,\n        -2.178135931072732e+00,  8.040971159674268e-01, 5.659848584656202e-01,\n        -9.349050336534268e-01,  8.006099486213468e-01, -5.944960111930493e-01,\n        4.369614304892440e-01, -3.292566347310282e-01, 2.547426420681868e-01,\n        -2.010899026277397e-01,  1.609467208423519e-01, -1.299975550484158e-01,\n        1.056082501090365e-01, -8.608337452556068e-02, 7.027252107999236e-02,\n        -5.735742622053085e-02,  4.673270108060494e-02, -3.793635725863799e-02,\n        3.060786160620013e-02, -2.446220554726340e-02, 1.927399223200865e-02,\n        -1.486843016804444e-02,  1.111747692371507e-02, -7.939442960305236e-03,\n        5.298852472637883e-03, -3.200104589830043e-03, 1.665382777953919e-03,\n        -6.913074254614758e-04,  1.999065225130592e-04,\n        -2.955159288961187e-05])\n\n    dlf.sin = np.array([\n        7.478326513505658e-07, -2.572850425065560e-06, 5.225955618519281e-06,\n        -7.352539610140040e-06,  8.768819961093828e-06, -8.560004370841340e-06,\n        8.101932279460349e-06, -5.983552716117552e-06, 5.036792825138655e-06,\n        -1.584355068233649e-06,  1.426050228179462e-06, 3.972863429067356e-06,\n        -1.903788077376088e-06,  1.144652944379527e-05, -4.327773998196030e-06,\n        2.297298998355334e-05, -4.391227697686659e-06, 4.291202395830839e-05,\n        1.760279032167125e-06,  8.017887907026914e-05, 2.364651853689879e-05,\n        1.535031685829202e-04,  8.375427119939347e-05, 3.030115685600468e-04,\n        2.339455351760637e-04,  6.157392107422657e-04, 5.921808556382737e-04,\n        1.281873037121434e-03,  1.424276189020714e-03, 2.718506171172064e-03,\n        3.324504626808429e-03,  5.839859904586436e-03, 7.608663600764702e-03,\n        1.263571470998938e-02,  1.714199295539484e-02, 2.735013970005427e-02,\n        3.794840483226463e-02,  5.858519896601026e-02, 8.166914231915734e-02,\n        1.215508018998907e-01,  1.658946642767184e-01, 2.324389477118542e-01,\n        2.938956625118840e-01,  3.572525844816433e-01, 3.479235360502319e-01,\n        2.294314115090992e-01, -1.250412450354792e-01, -6.340986743027450e-01,\n        -9.703404081656508e-01, -2.734109755210948e-01, 1.321852608494946e+00,\n        6.762199721133603e-01, -2.093257651144232e+00, 1.707842350925794e+00,\n        -8.844618831465598e-01,  3.720792781726873e-01, -1.481509947473694e-01,\n        6.124339615448667e-02, -2.726194382687923e-02, 1.307668436907975e-02,\n        -6.682101544475918e-03,  3.599101395415812e-03, -2.030735143712865e-03,\n        1.197624324158372e-03, -7.382202519234128e-04, 4.756906961407787e-04,\n        -3.199977708080284e-04,  2.238628518300115e-04, -1.618377502708346e-04,\n        1.199233854156409e-04, -9.025345928219504e-05, 6.830860296946832e-05,\n        -5.143409372298764e-05,  3.804574823200909e-05, -2.720604959632104e-05,\n        1.839913059679674e-05, -1.140157702141663e-05, 6.172802138985788e-06,\n        -2.706562852604888e-06,  8.403636781016683e-07,\n        -1.356300450956746e-07])\n\n    return dlf", "response": "rA simple key 81 pt CosSin filter as published in [ Key09 ]."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ffht(fEM, time, freq, ftarg):\n    # Get ffhtargs\n    ffhtfilt = ftarg[0]\n    pts_per_dec = ftarg[1]\n    kind = ftarg[2]  # Sine (`sin`) or cosine (`cos`)\n\n    # Cast into Standard DLF format\n    if pts_per_dec == 0:\n        fEM = fEM.reshape(time.size, -1)\n\n    # Carry out DLF\n    tEM = dlf(fEM, 2*np.pi*freq, time, ffhtfilt, pts_per_dec, kind=kind)\n\n    # Return the electromagnetic time domain field\n    # (Second argument is only for QWE)\n    return tEM, True", "response": "r Fourier Transform using Digital Linear Filter method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fqwe(fEM, time, freq, qweargs):\n    # Get rtol, atol, nquad, maxint, diff_quad, a, b, and limit\n    rtol, atol, nquad, maxint, _, diff_quad, a, b, limit, sincos = qweargs\n\n    # Calculate quadrature intervals for all offset\n    xint = np.concatenate((np.array([1e-20]), np.arange(1, maxint+1)*np.pi))\n    if sincos == np.cos:  # Adjust zero-crossings if cosine-transform\n        xint[1:] -= np.pi/2\n    intervals = xint/time[:, None]\n\n    # Get Gauss Quadrature Weights\n    g_x, g_w = special.p_roots(nquad)\n\n    # Pre-compute the Bessel functions at fixed quadrature points, multiplied\n    # by the corresponding Gauss quadrature weight.\n    dx = np.repeat(np.diff(xint)/2, nquad)\n    Bx = dx*(np.tile(g_x, maxint) + 1) + np.repeat(xint[:-1], nquad)\n    SS = sincos(Bx)*np.tile(g_w, maxint)\n\n    # Interpolate in frequency domain\n    tEM_rint = iuSpline(np.log(2*np.pi*freq), fEM.real)\n    tEM_iint = iuSpline(np.log(2*np.pi*freq), -fEM.imag)\n\n    # Check if we use QWE or SciPy's QUAD\n    # If there are any steep decays within an interval we have to use QUAD, as\n    # QWE is not designed for these intervals.\n    check0 = np.log(intervals[:, :-1])\n    check1 = np.log(intervals[:, 1:])\n    doqwe = np.all((np.abs(tEM_rint(check0) + 1j*tEM_iint(check0)) /\n                   np.abs(tEM_rint(check1) + 1j*tEM_iint(check1)) < diff_quad),\n                   1)\n\n    # Choose imaginary part if sine-transform, else real part\n    if sincos == np.sin:\n        tEM_int = tEM_iint\n    else:\n        tEM_int = tEM_rint\n\n    # Set quadargs if not given:\n    if not limit:\n        limit = maxint\n    if not a:\n        a = intervals[:, 0]\n    else:\n        a = a*np.ones(time.shape)\n    if not b:\n        b = intervals[:, -1]\n    else:\n        b = b*np.ones(time.shape)\n\n    # Pre-allocate output array\n    tEM = np.zeros(time.size)\n    conv = True\n\n    # Carry out SciPy's Quad if required\n    if np.any(~doqwe):\n        def sEMquad(w, t):\n            r\"\"\"Return scaled, interpolated value of tEM_int for ``w``.\"\"\"\n            return tEM_int(np.log(w))*sincos(w*t)\n\n        # Loop over times that require QUAD\n        for i in np.where(~doqwe)[0]:\n            out = integrate.quad(sEMquad, a[i], b[i], (time[i],), 1, atol,\n                                 rtol, limit)\n            tEM[i] = out[0]\n\n            # If there is a fourth output from QUAD, it means it did not conv.\n            if len(out) > 3:\n                conv *= False\n\n    # Carry out QWE for 'well-behaved' intervals\n    if np.any(doqwe):\n        sEM = tEM_int(np.log(Bx/time[doqwe, None]))*SS\n        tEM[doqwe], _, tc = qwe(rtol, atol, maxint, sEM, intervals[doqwe, :])\n        conv *= tc\n\n    return tEM, conv", "response": "r Fourier Transform using Quadrature - With - Extrapolation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fftlog(fEM, time, freq, ftarg):\n    # Get tcalc, dlnr, kr, rk, q; a and n\n    _, _, q, mu, tcalc, dlnr, kr, rk = ftarg\n    if mu > 0:  # Sine\n        a = -fEM.imag\n    else:       # Cosine\n        a = fEM.real\n    n = a.size\n\n    # 1. Amplitude and Argument of kr^(-2 i y) U_mu(q + 2 i y)\n    ln2kr = np.log(2.0/kr)\n    d = np.pi/(n*dlnr)\n    m = np.arange(1, (n+1)/2)\n    y = m*d  # y = m*pi/(n*dlnr)\n\n    if q == 0:  # unbiased case (q = 0)\n        zp = special.loggamma((mu + 1)/2.0 + 1j*y)\n        arg = 2.0*(ln2kr*y + zp.imag)\n\n    else:       # biased case (q != 0)\n        xp = (mu + 1.0 + q)/2.0\n        xm = (mu + 1.0 - q)/2.0\n\n        zp = special.loggamma(xp + 0j)\n        zm = special.loggamma(xm + 0j)\n\n        # Amplitude and Argument of U_mu(q)\n        amp = np.exp(np.log(2.0)*q + zp.real - zm.real)\n        # note +Im(zm) to get conjugate value below real axis\n        arg = zp.imag + zm.imag\n\n        # first element: cos(arg) = \u00b11, sin(arg) = 0\n        argcos1 = amp*np.cos(arg)\n\n        # remaining elements\n        zp = special.loggamma(xp + 1j*y)\n        zm = special.loggamma(xm + 1j*y)\n\n        argamp = np.exp(np.log(2.0)*q + zp.real - zm.real)\n        arg = 2*ln2kr*y + zp.imag + zm.imag\n\n    argcos = np.cos(arg)\n    argsin = np.sin(arg)\n\n    # 2. Centre point of array\n    jc = np.array((n + 1)/2.0)\n    j = np.arange(n)+1\n\n    # 3. a(r) = A(r) (r/rc)^[-dir*(q-.5)]\n    a *= np.exp(-(q - 0.5)*(j - jc)*dlnr)\n\n    # 4. transform a(r) -> \u00e3(k)\n\n    # 4.a normal FFT\n    a = fftpack.rfft(a)\n\n    # 4.b\n    m = np.arange(1, n/2, dtype=int)  # index variable\n    if q == 0:  # unbiased (q = 0) transform\n        # multiply by (kr)^[- i 2 m pi/(n dlnr)] U_mu[i 2 m pi/(n dlnr)]\n        ar = a[2*m-1]\n        ai = a[2*m]\n        a[2*m-1] = ar*argcos[:-1] - ai*argsin[:-1]\n        a[2*m] = ar*argsin[:-1] + ai*argcos[:-1]\n        # problematical last element, for even n\n        if np.mod(n, 2) == 0:\n            ar = argcos[-1]\n            a[-1] *= ar\n\n    else:  # biased (q != 0) transform\n        # multiply by (kr)^[- i 2 m pi/(n dlnr)] U_mu[q + i 2 m pi/(n dlnr)]\n        # phase\n        ar = a[2*m-1]\n        ai = a[2*m]\n        a[2*m-1] = ar*argcos[:-1] - ai*argsin[:-1]\n        a[2*m] = ar*argsin[:-1] + ai*argcos[:-1]\n\n        a[0] *= argcos1\n        a[2*m-1] *= argamp[:-1]\n        a[2*m] *= argamp[:-1]\n\n        # problematical last element, for even n\n        if np.mod(n, 2) == 0:\n            m = int(n/2)-3\n            ar = argcos[m-1]*argamp[m-1]\n            a[-1] *= ar\n\n    # 4.c normal FFT back\n    a = fftpack.irfft(a)\n\n    # \u00c3(k) = \u00e3(k) k^[-dir*(q+.5)] rc^[-dir*(q-.5)]\n    #      = \u00e3(k) (k/kc)^[-dir*(q+.5)] (kc rc)^(-dir*q) (rc/kc)^(dir*.5)\n    a = a[::-1]*np.exp(-((q + 0.5)*(j - jc)*dlnr + q*np.log(kr) -\n                       np.log(rk)/2.0))\n\n    # Interpolate for the desired times\n    ttEM = iuSpline(np.log(tcalc), a)\n    tEM = ttEM(np.log(time))\n\n    # (Second argument is only for QWE)\n    return tEM, True", "response": "r This function returns the logarithmic Fourier Transform of the given time - domain EM response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dlf(signal, points, out_pts, filt, pts_per_dec, kind=None, factAng=None,\n        ab=None, int_pts=None):\n    r\"\"\"Digital Linear Filter method.\n\n    This is the kernel of the DLF method, used for the Hankel (``fht``) and the\n    Fourier (``ffht``) Transforms. See ``fht`` for an extensive description.\n\n    For the Hankel transform, `signal` contains 3 complex wavenumber-domain\n    signals: (PJ0, PJ1, PJ0b), as returned from `kernel.wavenumber`. The Hankel\n    DLF has two additional, optional parameters: `factAng`, as returned from\n    `kernel.angle_factor`, and `ab`. The PJ0-kernel is the part of the\n    wavenumber-domain calculation which contains a zeroth-order Bessel function\n    and does NOT depend on the angle between source and receiver, only on\n    offset. PJ0b and PJ1 are the parts of the wavenumber-domain calculation\n    which contain a zeroth- and first-order Bessel function, respectively, and\n    can depend on the angle between source and receiver. PJ0, PJ1, or PJ0b can\n    also be None, if they are not used.\n\n    For the Fourier transform, `signal` is a complex frequency-domain signal.\n    The Fourier DLF requires one additional parameter, `kind`, which will be\n    'cos' or 'sin'.\n\n    \"\"\"\n    # 0. HANKEL/FOURIER-DEPENDING SETTINGS\n    if isinstance(signal, tuple):\n        # Hankel transform: 3 complex signals; respects `factAng` and `ab`\n        hankel = True\n\n        # Check if all angles are the same\n        if factAng is None:\n            has_angle_factors = False\n        else:\n            one_angle = factAng.min() == factAng.max()\n            if one_angle:\n                has_angle_factors = factAng[0] != 1.0\n                factAng = factAng[0]\n            else:\n                has_angle_factors = True\n\n        # Cast to list\n        signal = list(signal)\n\n        # Get kernels with information, to avoid unnecessary calculation\n        k_used = [True, True, True]\n        for i, val in enumerate(signal):\n            if val is None:\n                k_used[i] = False\n            else:  # Index of a kernel that is not None\n                inp_index = i\n\n        # Set has_angle_factors to False if no angle-dep. kernel is used\n        has_angle_factors *= bool(sum(k_used[1:]))\n\n    else:\n        # Fourier transform: 1 complex signal; needs kind\n        hankel = False\n\n        # Fourier independent of Angle\n        one_angle = True\n\n        # Real or -Imag part depending on kind (sine/cosine)\n        if kind == 'sin':\n            signal = -signal.imag\n        else:\n            signal = signal.real\n\n        # Cast to list\n        signal = [signal, ]\n\n        k_used = [True, ]\n\n    # 1. PREPARE SIGNALS\n\n    # Interpolation function\n    def spline(values, points, int_pts):\n        r\"\"\"Return `values` at `points` interpolated in log at `int_pts`.\"\"\"\n        out = iuSpline(np.log(points), values.real)(np.log(int_pts))\n        if hankel:\n            out = out+1j*iuSpline(np.log(points), values.imag)(np.log(int_pts))\n        return out\n\n    # Re-arranging and interpolation before DLF\n    if pts_per_dec < 0:  # Lagged Convolution DLF: interp. in output domain\n        # Lagged Convolution DLF: re-arrange signal\n\n        # Get interpolation points, if not provided # (backwards compatibility)\n        if int_pts is None:\n            _, int_pts = get_spline_values(filt, out_pts, pts_per_dec)\n\n        # Re-arrange signal\n        for i, val in enumerate(signal):\n            if k_used[i]:  # Only if kernel contains info\n                tmp_sig = np.concatenate((np.tile(val, int_pts.size).squeeze(),\n                                         np.zeros(int_pts.size)))\n                signal[i] = tmp_sig.reshape(int_pts.size, -1)[:,\n                                                              :filt.base.size]\n\n    elif pts_per_dec > 0:  # Splined DLF: interpolate in input domain\n        # Splined DLF; interpolate in input domain\n\n        # Get interpolation points, if not provided (backwards compatibility)\n        if int_pts is None:\n            int_pts = filt.base/out_pts[:, None]\n\n        for i, val in enumerate(signal):\n            if k_used[i]:  # Only if kernel contains info\n                signal[i] = spline(val, points, int_pts)\n\n    # 2. APPLY DLF\n    if hankel:  # Hankel transform\n        inp_PJ0, inp_PJ1, inp_PJ0b = signal\n\n        # If Kernel is all zeroes we just put zeroes instead of carrying out\n        # the DLF\n        alt_pre = np.zeros(signal[inp_index].shape[:-1], dtype=complex)\n\n        if pts_per_dec != 0 and not one_angle:\n            # Varying angle with either lagged or splined DLF.\n            # If not all offsets are in one line from the source, hence do not\n            # have the same angle, the DLF has to be done separately for\n            # angle-dependent and angle-independent parts.\n            out_angle = alt_pre.copy()\n            out_noang = alt_pre.copy()\n\n            # Do transform for the used kernels\n            if k_used[0]:  # J0\n                np.dot(inp_PJ0, filt.j0, out=out_noang)\n\n            if k_used[1]:  # J1\n                np.dot(inp_PJ1, filt.j1, out=out_angle)\n                if ab in [11, 12, 21, 22, 14, 24, 15, 25]:  # Because of J2\n                    # J2(kr) = 2/(kr)*J1(kr) - J0(kr)\n                    if pts_per_dec < 0:  # Lagged Convolution\n                        out_angle /= int_pts\n                    else:  # Splined\n                        out_angle /= out_pts\n\n            if k_used[2]:  # J0b\n                out_angle += np.dot(inp_PJ0b, filt.j0)\n\n            if pts_per_dec > 0:\n                # If splined we can add them here, as the interpolation\n                # is already done.\n\n                # Angle dependency\n                if has_angle_factors:\n                    out_angle *= factAng\n                out_signal = out_angle + out_noang\n\n        else:\n            # With the standard DLF (one_angle or not), and with the splined\n            # DLF but one_angle, we can combine PJ0 and PJ0b to save one DLF.\n            out_signal = alt_pre.copy()\n\n            # Do transform for the used kernels\n            if k_used[1]:  # J1\n                np.dot(inp_PJ1, filt.j1, out=out_signal)\n                if ab in [11, 12, 21, 22, 14, 24, 15, 25]:  # Because of J2\n                    # J2(kr) = 2/(kr)*J1(kr) - J0(kr)\n                    if pts_per_dec < 0:  # Lagged Convolution\n                        out_signal /= int_pts\n                    else:  # Splined\n                        out_signal /= out_pts\n\n            if k_used[2]:  # J0b\n                out_signal += np.dot(inp_PJ0b, filt.j0)\n\n            # Angle dependency\n            if has_angle_factors:\n                out_signal *= factAng\n\n            if k_used[0]:  # J0\n                out_signal += np.dot(inp_PJ0, filt.j0)\n\n    else:  # Fourier transform\n        out_signal = np.dot(signal[0], getattr(filt, kind))\n\n    # 3. IF LAGGED CONVOLUTION, INTERPOLATE NOW TO OUTPUT DOMAIN POINTS\n    if pts_per_dec < 0:\n\n        if not one_angle:  # Separately on out_noang and out_angle\n\n            # J1 or J2 are always used except for ab=33; however ab=33 is\n            # angle-independent, so we don't have to check here.\n            out_signal = spline(out_angle[::-1], int_pts[::-1], out_pts)\n\n            # Angle dependency\n            if has_angle_factors:\n                out_signal *= factAng\n\n            if k_used[0]:  # Only if kernel contains info\n                out_signal += spline(out_noang[::-1], int_pts[::-1], out_pts)\n\n        else:  # If only one angle or Fourier\n            out_signal = spline(out_signal[::-1], int_pts[::-1], out_pts)\n\n    # Return the signal in the output domain\n    return out_signal/out_pts", "response": "r Function to perform a DLF transform on a frequency domain signal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quad(sPJ0r, sPJ0i, sPJ1r, sPJ1i, sPJ0br, sPJ0bi, ab, off, factAng, iinp):\n\n    # Define the quadrature kernels\n    def quad_PJ0(klambd, sPJ0, koff):\n        r\"\"\"Quadrature for PJ0.\"\"\"\n        return sPJ0(np.log(klambd))*special.j0(koff*klambd)\n\n    def quad_PJ1(klambd, sPJ1, ab, koff, kang):\n        r\"\"\"Quadrature for PJ1.\"\"\"\n\n        tP1 = kang*sPJ1(np.log(klambd))\n        if ab in [11, 12, 21, 22, 14, 24, 15, 25]:  # Because of J2\n            # J2(kr) = 2/(kr)*J1(kr) - J0(kr)\n            tP1 /= koff\n\n        return tP1*special.j1(koff*klambd)\n\n    def quad_PJ0b(klambd, sPJ0b, koff, kang):\n        r\"\"\"Quadrature for PJ0b.\"\"\"\n        return kang*sPJ0b(np.log(klambd))*special.j0(koff*klambd)\n\n    # Pre-allocate output\n    conv = True\n    out = np.array(0.0+0.0j)\n\n    # Carry out quadrature for required kernels\n    iinp['full_output'] = 1\n\n    if sPJ0r is not None:\n        re = integrate.quad(quad_PJ0, args=(sPJ0r, off), **iinp)\n        im = integrate.quad(quad_PJ0, args=(sPJ0i, off), **iinp)\n        out += re[0] + 1j*im[0]\n        # If there is a fourth output from QUAD, it means it did not converge\n        if (len(re) or len(im)) > 3:\n            conv = False\n\n    if sPJ1r is not None:\n        re = integrate.quad(quad_PJ1, args=(sPJ1r, ab, off, factAng), **iinp)\n        im = integrate.quad(quad_PJ1, args=(sPJ1i, ab, off, factAng), **iinp)\n        out += re[0] + 1j*im[0]\n        # If there is a fourth output from QUAD, it means it did not converge\n        if (len(re) or len(im)) > 3:\n            conv = False\n\n    if sPJ0br is not None:\n        re = integrate.quad(quad_PJ0b, args=(sPJ0br, off, factAng), **iinp)\n        im = integrate.quad(quad_PJ0b, args=(sPJ0bi, off, factAng), **iinp)\n        out += re[0] + 1j*im[0]\n        # If there is a fourth output from QUAD, it means it did not converge\n        if (len(re) or len(im)) > 3:\n            conv = False\n\n    # Collect the results\n    return out, conv", "response": "This function calculates the quadrature of a single class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fhti(rmin, rmax, n, q, mu):\n\n    # Central point log10(r_c) of periodic interval\n    logrc = (rmin + rmax)/2\n\n    # Central index (1/2 integral if n is even)\n    nc = (n + 1)/2.\n\n    # Log spacing of points\n    dlogr = (rmax - rmin)/n\n    dlnr = dlogr*np.log(10.)\n\n    # Get low-ringing kr\n    y = 1j*np.pi/(2.0*dlnr)\n    zp = special.loggamma((mu + 1.0 + q)/2.0 + y)\n    zm = special.loggamma((mu + 1.0 - q)/2.0 + y)\n    arg = np.log(2.0)/dlnr + (zp.imag + zm.imag)/np.pi\n    kr = np.exp((arg - np.round(arg))*dlnr)\n\n    # Calculate required input x-values (freq); angular freq -> freq\n    freq = 10**(logrc + (np.arange(1, n+1) - nc)*dlogr)/(2*np.pi)\n\n    # Calculate tcalc with adjusted kr\n    logkc = np.log10(kr) - logrc\n    tcalc = 10**(logkc + (np.arange(1, n+1) - nc)*dlogr)\n\n    # rk = r_c/k_r; adjust for Fourier transform scaling\n    rk = 10**(logrc - logkc)*np.pi/2\n\n    return freq, tcalc, dlnr, kr, rk", "response": "r Return parameters required for FFTLog."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the CPU info gathered by querying the X86 cpuid register in a new process.", "response": "def _get_cpu_info_from_cpuid():\n\t'''\n\tReturns the CPU info gathered by querying the X86 cpuid register in a new process.\n\tReturns {} on non X86 cpus.\n\tReturns {} if SELinux is in enforcing mode.\n\t'''\n\tfrom multiprocessing import Process, Queue\n\n\t# Return {} if can't cpuid\n\tif not DataSource.can_cpuid:\n\t\treturn {}\n\n\t# Get the CPU arch and bits\n\tarch, bits = _parse_arch(DataSource.arch_string_raw)\n\n\t# Return {} if this is not an X86 CPU\n\tif not arch in ['X86_32', 'X86_64']:\n\t\treturn {}\n\n\ttry:\n\t\t# Start running the function in a subprocess\n\t\tqueue = Queue()\n\t\tp = Process(target=_actual_get_cpu_info_from_cpuid, args=(queue,))\n\t\tp.start()\n\n\t\t# Wait for the process to end, while it is still alive\n\t\twhile p.is_alive():\n\t\t\tp.join(0)\n\n\t\t# Return {} if it failed\n\t\tif p.exitcode != 0:\n\t\t\treturn {}\n\n\t\t# Return the result, only if there is something to read\n\t\tif not queue.empty():\n\t\t\toutput = queue.get()\n\t\t\treturn _b64_to_obj(output)\n\texcept:\n\t\tpass\n\n\t# Return {} if everything failed\n\treturn {}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_cpu_info_from_proc_cpuinfo():\n\t'''\n\tReturns the CPU info gathered from /proc/cpuinfo.\n\tReturns {} if /proc/cpuinfo is not found.\n\t'''\n\ttry:\n\t\t# Just return {} if there is no cpuinfo\n\t\tif not DataSource.has_proc_cpuinfo():\n\t\t\treturn {}\n\n\t\treturncode, output = DataSource.cat_proc_cpuinfo()\n\t\tif returncode != 0:\n\t\t\treturn {}\n\n\t\t# Various fields\n\t\tvendor_id = _get_field(False, output, None, '', 'vendor_id', 'vendor id', 'vendor')\n\t\tprocessor_brand = _get_field(True, output, None, None, 'model name','cpu', 'processor')\n\t\tcache_size = _get_field(False, output, None, '', 'cache size')\n\t\tstepping = _get_field(False, output, int, 0, 'stepping')\n\t\tmodel = _get_field(False, output, int, 0, 'model')\n\t\tfamily = _get_field(False, output, int, 0, 'cpu family')\n\t\thardware = _get_field(False, output, None, '', 'Hardware')\n\t\t# Flags\n\t\tflags = _get_field(False, output, None, None, 'flags', 'Features')\n\t\tif flags:\n\t\t\tflags = flags.split()\n\t\t\tflags.sort()\n\n\t\t# Convert from MHz string to Hz\n\t\thz_actual = _get_field(False, output, None, '', 'cpu MHz', 'cpu speed', 'clock')\n\t\thz_actual = hz_actual.lower().rstrip('mhz').strip()\n\t\thz_actual = _to_decimal_string(hz_actual)\n\n\t\t# Convert from GHz/MHz string to Hz\n\t\thz_advertised, scale = (None, 0)\n\t\ttry:\n\t\t\thz_advertised, scale = _parse_cpu_brand_string(processor_brand)\n\t\texcept Exception:\n\t\t\tpass\n\n\t\tinfo = {\n\t\t'hardware_raw' : hardware,\n\t\t'brand_raw' : processor_brand,\n\n\t\t'l3_cache_size' : _to_friendly_bytes(cache_size),\n\t\t'flags' : flags,\n\t\t'vendor_id_raw' : vendor_id,\n\t\t'stepping' : stepping,\n\t\t'model' : model,\n\t\t'family' : family,\n\t\t}\n\n\t\t# Make the Hz the same for actual and advertised if missing any\n\t\tif not hz_advertised or hz_advertised == '0.0':\n\t\t\thz_advertised = hz_actual\n\t\t\tscale = 6\n\t\telif not hz_actual or hz_actual == '0.0':\n\t\t\thz_actual = hz_advertised\n\n\t\t# Add the Hz if there is one\n\t\tif _hz_short_to_full(hz_advertised, scale) > (0, 0):\n\t\t\tinfo['hz_advertised_friendly'] = _hz_short_to_friendly(hz_advertised, scale)\n\t\t\tinfo['hz_advertised'] = _hz_short_to_full(hz_advertised, scale)\n\t\tif _hz_short_to_full(hz_actual, scale) > (0, 0):\n\t\t\tinfo['hz_actual_friendly'] = _hz_short_to_friendly(hz_actual, 6)\n\t\t\tinfo['hz_actual'] = _hz_short_to_full(hz_actual, 6)\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\t#raise # NOTE: To have this throw on error, uncomment this line\n\t\treturn {}", "response": "Returns the CPU info gathered from proc_cpuinfo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the CPU info gathered from cpufreq - info.", "response": "def _get_cpu_info_from_cpufreq_info():\n\t'''\n\tReturns the CPU info gathered from cpufreq-info.\n\tReturns {} if cpufreq-info is not found.\n\t'''\n\ttry:\n\t\thz_brand, scale = '0.0', 0\n\n\t\tif not DataSource.has_cpufreq_info():\n\t\t\treturn {}\n\n\t\treturncode, output = DataSource.cpufreq_info()\n\t\tif returncode != 0:\n\t\t\treturn {}\n\n\t\thz_brand = output.split('current CPU frequency is')[1].split('\\n')[0]\n\t\ti = hz_brand.find('Hz')\n\t\tassert(i != -1)\n\t\thz_brand = hz_brand[0 : i+2].strip().lower()\n\n\t\tif hz_brand.endswith('mhz'):\n\t\t\tscale = 6\n\t\telif hz_brand.endswith('ghz'):\n\t\t\tscale = 9\n\t\thz_brand = hz_brand.rstrip('mhz').rstrip('ghz').strip()\n\t\thz_brand = _to_decimal_string(hz_brand)\n\n\t\tinfo = {\n\t\t\t'hz_advertised_friendly' : _hz_short_to_friendly(hz_brand, scale),\n\t\t\t'hz_actual_friendly' : _hz_short_to_friendly(hz_brand, scale),\n\t\t\t'hz_advertised' : _hz_short_to_full(hz_brand, scale),\n\t\t\t'hz_actual' : _hz_short_to_full(hz_brand, scale),\n\t\t}\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\t#raise # NOTE: To have this throw on error, uncomment this line\n\t\treturn {}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_cpu_info_from_lscpu():\n\t'''\n\tReturns the CPU info gathered from lscpu.\n\tReturns {} if lscpu is not found.\n\t'''\n\ttry:\n\t\tif not DataSource.has_lscpu():\n\t\t\treturn {}\n\n\t\treturncode, output = DataSource.lscpu()\n\t\tif returncode != 0:\n\t\t\treturn {}\n\n\t\tinfo = {}\n\n\t\tnew_hz = _get_field(False, output, None, None, 'CPU max MHz', 'CPU MHz')\n\t\tif new_hz:\n\t\t\tnew_hz = _to_decimal_string(new_hz)\n\t\t\tscale = 6\n\t\t\tinfo['hz_advertised_friendly'] = _hz_short_to_friendly(new_hz, scale)\n\t\t\tinfo['hz_actual_friendly'] = _hz_short_to_friendly(new_hz, scale)\n\t\t\tinfo['hz_advertised'] = _hz_short_to_full(new_hz, scale)\n\t\t\tinfo['hz_actual'] = _hz_short_to_full(new_hz, scale)\n\n\t\tvendor_id = _get_field(False, output, None, None, 'Vendor ID')\n\t\tif vendor_id:\n\t\t\tinfo['vendor_id_raw'] = vendor_id\n\n\t\tbrand = _get_field(False, output, None, None, 'Model name')\n\t\tif brand:\n\t\t\tinfo['brand_raw'] = brand\n\n\t\tfamily = _get_field(False, output, None, None, 'CPU family')\n\t\tif family and family.isdigit():\n\t\t\tinfo['family'] = int(family)\n\n\t\tstepping = _get_field(False, output, None, None, 'Stepping')\n\t\tif stepping and stepping.isdigit():\n\t\t\tinfo['stepping'] = int(stepping)\n\n\t\tmodel = _get_field(False, output, None, None, 'Model')\n\t\tif model and model.isdigit():\n\t\t\tinfo['model'] = int(model)\n\n\t\tl1_data_cache_size = _get_field(False, output, None, None, 'L1d cache')\n\t\tif l1_data_cache_size:\n\t\t\tinfo['l1_data_cache_size'] = _to_friendly_bytes(l1_data_cache_size)\n\n\t\tl1_instruction_cache_size = _get_field(False, output, None, None, 'L1i cache')\n\t\tif l1_instruction_cache_size:\n\t\t\tinfo['l1_instruction_cache_size'] = _to_friendly_bytes(l1_instruction_cache_size)\n\n\t\tl2_cache_size = _get_field(False, output, None, None, 'L2 cache')\n\t\tif l2_cache_size:\n\t\t\tinfo['l2_cache_size'] = _to_friendly_bytes(l2_cache_size)\n\n\t\tl3_cache_size = _get_field(False, output, None, None, 'L3 cache')\n\t\tif l3_cache_size:\n\t\t\tinfo['l3_cache_size'] = _to_friendly_bytes(l3_cache_size)\n\n\t\t# Flags\n\t\tflags = _get_field(False, output, None, None, 'flags', 'Features')\n\t\tif flags:\n\t\t\tflags = flags.split()\n\t\t\tflags.sort()\n\t\t\tinfo['flags'] = flags\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\t#raise # NOTE: To have this throw on error, uncomment this line\n\t\treturn {}", "response": "Returns the CPU info gathered from the LSCPU."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the CPU info gathered from dmesg.", "response": "def _get_cpu_info_from_dmesg():\n\t'''\n\tReturns the CPU info gathered from dmesg.\n\tReturns {} if dmesg is not found or does not have the desired info.\n\t'''\n\t# Just return {} if there is no dmesg\n\tif not DataSource.has_dmesg():\n\t\treturn {}\n\n\t# If dmesg fails return {}\n\treturncode, output = DataSource.dmesg_a()\n\tif output == None or returncode != 0:\n\t\treturn {}\n\n\treturn _parse_dmesg_output(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_cpu_info_from_ibm_pa_features():\n\t'''\n\tReturns the CPU info gathered from lsprop /proc/device-tree/cpus/*/ibm,pa-features\n\tReturns {} if lsprop is not found or ibm,pa-features does not have the desired info.\n\t'''\n\ttry:\n\t\t# Just return {} if there is no lsprop\n\t\tif not DataSource.has_ibm_pa_features():\n\t\t\treturn {}\n\n\t\t# If ibm,pa-features fails return {}\n\t\treturncode, output = DataSource.ibm_pa_features()\n\t\tif output == None or returncode != 0:\n\t\t\treturn {}\n\n\t\t# Filter out invalid characters from output\n\t\tvalue = output.split(\"ibm,pa-features\")[1].lower()\n\t\tvalue = [s for s in value if s in list('0123456789abcfed')]\n\t\tvalue = ''.join(value)\n\n\t\t# Get data converted to Uint32 chunks\n\t\tleft = int(value[0 : 8], 16)\n\t\tright = int(value[8 : 16], 16)\n\n\t\t# Get the CPU flags\n\t\tflags = {\n\t\t\t# Byte 0\n\t\t\t'mmu' : _is_bit_set(left, 0),\n\t\t\t'fpu' : _is_bit_set(left, 1),\n\t\t\t'slb' : _is_bit_set(left, 2),\n\t\t\t'run' : _is_bit_set(left, 3),\n\t\t\t#'reserved' : _is_bit_set(left, 4),\n\t\t\t'dabr' : _is_bit_set(left, 5),\n\t\t\t'ne' : _is_bit_set(left, 6),\n\t\t\t'wtr' : _is_bit_set(left, 7),\n\n\t\t\t# Byte 1\n\t\t\t'mcr' : _is_bit_set(left, 8),\n\t\t\t'dsisr' : _is_bit_set(left, 9),\n\t\t\t'lp' : _is_bit_set(left, 10),\n\t\t\t'ri' : _is_bit_set(left, 11),\n\t\t\t'dabrx' : _is_bit_set(left, 12),\n\t\t\t'sprg3' : _is_bit_set(left, 13),\n\t\t\t'rislb' : _is_bit_set(left, 14),\n\t\t\t'pp' : _is_bit_set(left, 15),\n\n\t\t\t# Byte 2\n\t\t\t'vpm' : _is_bit_set(left, 16),\n\t\t\t'dss_2.05' : _is_bit_set(left, 17),\n\t\t\t#'reserved' : _is_bit_set(left, 18),\n\t\t\t'dar' : _is_bit_set(left, 19),\n\t\t\t#'reserved' : _is_bit_set(left, 20),\n\t\t\t'ppr' : _is_bit_set(left, 21),\n\t\t\t'dss_2.02' : _is_bit_set(left, 22),\n\t\t\t'dss_2.06' : _is_bit_set(left, 23),\n\n\t\t\t# Byte 3\n\t\t\t'lsd_in_dscr' : _is_bit_set(left, 24),\n\t\t\t'ugr_in_dscr' : _is_bit_set(left, 25),\n\t\t\t#'reserved' : _is_bit_set(left, 26),\n\t\t\t#'reserved' : _is_bit_set(left, 27),\n\t\t\t#'reserved' : _is_bit_set(left, 28),\n\t\t\t#'reserved' : _is_bit_set(left, 29),\n\t\t\t#'reserved' : _is_bit_set(left, 30),\n\t\t\t#'reserved' : _is_bit_set(left, 31),\n\n\t\t\t# Byte 4\n\t\t\t'sso_2.06' : _is_bit_set(right, 0),\n\t\t\t#'reserved' : _is_bit_set(right, 1),\n\t\t\t#'reserved' : _is_bit_set(right, 2),\n\t\t\t#'reserved' : _is_bit_set(right, 3),\n\t\t\t#'reserved' : _is_bit_set(right, 4),\n\t\t\t#'reserved' : _is_bit_set(right, 5),\n\t\t\t#'reserved' : _is_bit_set(right, 6),\n\t\t\t#'reserved' : _is_bit_set(right, 7),\n\n\t\t\t# Byte 5\n\t\t\t'le' : _is_bit_set(right, 8),\n\t\t\t'cfar' : _is_bit_set(right, 9),\n\t\t\t'eb' : _is_bit_set(right, 10),\n\t\t\t'lsq_2.07' : _is_bit_set(right, 11),\n\t\t\t#'reserved' : _is_bit_set(right, 12),\n\t\t\t#'reserved' : _is_bit_set(right, 13),\n\t\t\t#'reserved' : _is_bit_set(right, 14),\n\t\t\t#'reserved' : _is_bit_set(right, 15),\n\n\t\t\t# Byte 6\n\t\t\t'dss_2.07' : _is_bit_set(right, 16),\n\t\t\t#'reserved' : _is_bit_set(right, 17),\n\t\t\t#'reserved' : _is_bit_set(right, 18),\n\t\t\t#'reserved' : _is_bit_set(right, 19),\n\t\t\t#'reserved' : _is_bit_set(right, 20),\n\t\t\t#'reserved' : _is_bit_set(right, 21),\n\t\t\t#'reserved' : _is_bit_set(right, 22),\n\t\t\t#'reserved' : _is_bit_set(right, 23),\n\n\t\t\t# Byte 7\n\t\t\t#'reserved' : _is_bit_set(right, 24),\n\t\t\t#'reserved' : _is_bit_set(right, 25),\n\t\t\t#'reserved' : _is_bit_set(right, 26),\n\t\t\t#'reserved' : _is_bit_set(right, 27),\n\t\t\t#'reserved' : _is_bit_set(right, 28),\n\t\t\t#'reserved' : _is_bit_set(right, 29),\n\t\t\t#'reserved' : _is_bit_set(right, 30),\n\t\t\t#'reserved' : _is_bit_set(right, 31),\n\t\t}\n\n\t\t# Get a list of only the flags that are true\n\t\tflags = [k for k, v in flags.items() if v]\n\t\tflags.sort()\n\n\t\tinfo = {\n\t\t\t'flags' : flags\n\t\t}\n\t\tinfo = {k: v for k, v in info.items() if v}\n\n\t\treturn info\n\texcept:\n\t\treturn {}", "response": "Get the CPU info gathered from the lsprop or the pa - features."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_cpu_info_from_cat_var_run_dmesg_boot():\n\t'''\n\tReturns the CPU info gathered from /var/run/dmesg.boot.\n\tReturns {} if dmesg is not found or does not have the desired info.\n\t'''\n\t# Just return {} if there is no /var/run/dmesg.boot\n\tif not DataSource.has_var_run_dmesg_boot():\n\t\treturn {}\n\n\t# If dmesg.boot fails return {}\n\treturncode, output = DataSource.cat_var_run_dmesg_boot()\n\tif output == None or returncode != 0:\n\t\treturn {}\n\n\treturn _parse_dmesg_output(output)", "response": "Returns the CPU info gathered from dmesg. boot."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_cpu_info_from_sysctl():\n\t'''\n\tReturns the CPU info gathered from sysctl.\n\tReturns {} if sysctl is not found.\n\t'''\n\ttry:\n\t\t# Just return {} if there is no sysctl\n\t\tif not DataSource.has_sysctl():\n\t\t\treturn {}\n\n\t\t# If sysctl fails return {}\n\t\treturncode, output = DataSource.sysctl_machdep_cpu_hw_cpufrequency()\n\t\tif output == None or returncode != 0:\n\t\t\treturn {}\n\n\t\t# Various fields\n\t\tvendor_id = _get_field(False, output, None, None, 'machdep.cpu.vendor')\n\t\tprocessor_brand = _get_field(True, output, None, None, 'machdep.cpu.brand_string')\n\t\tcache_size = _get_field(False, output, None, None, 'machdep.cpu.cache.size')\n\t\tstepping = _get_field(False, output, int, 0, 'machdep.cpu.stepping')\n\t\tmodel = _get_field(False, output, int, 0, 'machdep.cpu.model')\n\t\tfamily = _get_field(False, output, int, 0, 'machdep.cpu.family')\n\n\t\t# Flags\n\t\tflags = _get_field(False, output, None, '', 'machdep.cpu.features').lower().split()\n\t\tflags.extend(_get_field(False, output, None, '', 'machdep.cpu.leaf7_features').lower().split())\n\t\tflags.extend(_get_field(False, output, None, '', 'machdep.cpu.extfeatures').lower().split())\n\t\tflags.sort()\n\n\t\t# Convert from GHz/MHz string to Hz\n\t\thz_advertised, scale = _parse_cpu_brand_string(processor_brand)\n\t\thz_actual = _get_field(False, output, None, None, 'hw.cpufrequency')\n\t\thz_actual = _to_decimal_string(hz_actual)\n\n\t\tinfo = {\n\t\t'vendor_id_raw' : vendor_id,\n\t\t'brand_raw' : processor_brand,\n\n\t\t'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),\n\t\t'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, 0),\n\t\t'hz_advertised' : _hz_short_to_full(hz_advertised, scale),\n\t\t'hz_actual' : _hz_short_to_full(hz_actual, 0),\n\n\t\t'l2_cache_size' : _to_friendly_bytes(cache_size),\n\n\t\t'stepping' : stepping,\n\t\t'model' : model,\n\t\t'family' : family,\n\t\t'flags' : flags\n\t\t}\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\treturn {}", "response": "Returns the CPU info gathered from sysctl."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the CPU info gathered from sysinfo.", "response": "def _get_cpu_info_from_sysinfo_v1():\n\t'''\n\tReturns the CPU info gathered from sysinfo.\n\tReturns {} if sysinfo is not found.\n\t'''\n\ttry:\n\t\t# Just return {} if there is no sysinfo\n\t\tif not DataSource.has_sysinfo():\n\t\t\treturn {}\n\n\t\t# If sysinfo fails return {}\n\t\treturncode, output = DataSource.sysinfo_cpu()\n\t\tif output == None or returncode != 0:\n\t\t\treturn {}\n\n\t\t# Various fields\n\t\tvendor_id = '' #_get_field(False, output, None, None, 'CPU #0: ')\n\t\tprocessor_brand = output.split('CPU #0: \"')[1].split('\"\\n')[0].strip()\n\t\tcache_size = '' #_get_field(False, output, None, None, 'machdep.cpu.cache.size')\n\t\tstepping = int(output.split(', stepping ')[1].split(',')[0].strip())\n\t\tmodel = int(output.split(', model ')[1].split(',')[0].strip())\n\t\tfamily = int(output.split(', family ')[1].split(',')[0].strip())\n\n\t\t# Flags\n\t\tflags = []\n\t\tfor line in output.split('\\n'):\n\t\t\tif line.startswith('\\t\\t'):\n\t\t\t\tfor flag in line.strip().lower().split():\n\t\t\t\t\tflags.append(flag)\n\t\tflags.sort()\n\n\t\t# Convert from GHz/MHz string to Hz\n\t\thz_advertised, scale = _parse_cpu_brand_string(processor_brand)\n\t\thz_actual = hz_advertised\n\n\t\tinfo = {\n\t\t'vendor_id_raw' : vendor_id,\n\t\t'brand_raw' : processor_brand,\n\n\t\t'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),\n\t\t'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, scale),\n\t\t'hz_advertised' : _hz_short_to_full(hz_advertised, scale),\n\t\t'hz_actual' : _hz_short_to_full(hz_actual, scale),\n\n\t\t'l2_cache_size' : _to_friendly_bytes(cache_size),\n\n\t\t'stepping' : stepping,\n\t\t'model' : model,\n\t\t'family' : family,\n\t\t'flags' : flags\n\t\t}\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\t#raise # NOTE: To have this throw on error, uncomment this line\n\t\treturn {}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_cpu_info_from_sysinfo_v2():\n\t'''\n\tReturns the CPU info gathered from sysinfo.\n\tReturns {} if sysinfo is not found.\n\t'''\n\ttry:\n\t\t# Just return {} if there is no sysinfo\n\t\tif not DataSource.has_sysinfo():\n\t\t\treturn {}\n\n\t\t# If sysinfo fails return {}\n\t\treturncode, output = DataSource.sysinfo_cpu()\n\t\tif output == None or returncode != 0:\n\t\t\treturn {}\n\n\t\t# Various fields\n\t\tvendor_id = '' #_get_field(False, output, None, None, 'CPU #0: ')\n\t\tprocessor_brand = output.split('CPU #0: \"')[1].split('\"\\n')[0].strip()\n\t\tcache_size = '' #_get_field(False, output, None, None, 'machdep.cpu.cache.size')\n\t\tsignature = output.split('Signature:')[1].split('\\n')[0].strip()\n\t\t#\n\t\tstepping = int(signature.split('stepping ')[1].split(',')[0].strip())\n\t\tmodel = int(signature.split('model ')[1].split(',')[0].strip())\n\t\tfamily = int(signature.split('family ')[1].split(',')[0].strip())\n\n\t\t# Flags\n\t\tdef get_subsection_flags(output):\n\t\t\tretval = []\n\t\t\tfor line in output.split('\\n')[1:]:\n\t\t\t\tif not line.startswith('                ') and not line.startswith('\t\t'): break\n\t\t\t\tfor entry in line.strip().lower().split(' '):\n\t\t\t\t\tretval.append(entry)\n\t\t\treturn retval\n\n\t\tflags = get_subsection_flags(output.split('Features: ')[1]) + \\\n\t\t\t\tget_subsection_flags(output.split('Extended Features (0x00000001): ')[1]) + \\\n\t\t\t\tget_subsection_flags(output.split('Extended Features (0x80000001): ')[1])\n\t\tflags.sort()\n\n\t\t# Convert from GHz/MHz string to Hz\n\t\tlines = [n for n in output.split('\\n') if n]\n\t\traw_hz = lines[0].split('running at ')[1].strip().lower()\n\t\thz_advertised = raw_hz.rstrip('mhz').rstrip('ghz').strip()\n\t\thz_advertised = _to_decimal_string(hz_advertised)\n\t\thz_actual = hz_advertised\n\n\t\tscale = 0\n\t\tif raw_hz.endswith('mhz'):\n\t\t\tscale = 6\n\t\telif raw_hz.endswith('ghz'):\n\t\t\tscale = 9\n\n\t\tinfo = {\n\t\t'vendor_id_raw' : vendor_id,\n\t\t'brand_raw' : processor_brand,\n\n\t\t'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),\n\t\t'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, scale),\n\t\t'hz_advertised' : _hz_short_to_full(hz_advertised, scale),\n\t\t'hz_actual' : _hz_short_to_full(hz_actual, scale),\n\n\t\t'l2_cache_size' : _to_friendly_bytes(cache_size),\n\n\t\t'stepping' : stepping,\n\t\t'model' : model,\n\t\t'family' : family,\n\t\t'flags' : flags\n\t\t}\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\t#raise # NOTE: To have this throw on error, uncomment this line\n\t\treturn {}", "response": "Get the CPU info gathered from sysinfo."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the CPU info gathered from WMI.", "response": "def _get_cpu_info_from_wmic():\n\t'''\n\tReturns the CPU info gathered from WMI.\n\tReturns {} if not on Windows, or wmic is not installed.\n\t'''\n\n\ttry:\n\t\t# Just return {} if not Windows or there is no wmic\n\t\tif not DataSource.is_windows or not DataSource.has_wmic():\n\t\t\treturn {}\n\n\t\treturncode, output = DataSource.wmic_cpu()\n\t\tif output == None or returncode != 0:\n\t\t\treturn {}\n\n\t\t# Break the list into key values pairs\n\t\tvalue = output.split(\"\\n\")\n\t\tvalue = [s.rstrip().split('=') for s in value if '=' in s]\n\t\tvalue = {k: v for k, v in value if v}\n\n\t\t# Get the advertised MHz\n\t\tprocessor_brand = value.get('Name')\n\t\thz_advertised, scale_advertised = _parse_cpu_brand_string(processor_brand)\n\n\t\t# Get the actual MHz\n\t\thz_actual = value.get('CurrentClockSpeed')\n\t\tscale_actual = 6\n\t\tif hz_actual:\n\t\t\thz_actual = _to_decimal_string(hz_actual)\n\n\t\t# Get cache sizes\n\t\tl2_cache_size = value.get('L2CacheSize')\n\t\tif l2_cache_size:\n\t\t\tl2_cache_size = l2_cache_size + ' KB'\n\n\t\tl3_cache_size = value.get('L3CacheSize')\n\t\tif l3_cache_size:\n\t\t\tl3_cache_size = l3_cache_size + ' KB'\n\n\t\t# Get family, model, and stepping\n\t\tfamily, model, stepping = '', '', ''\n\t\tdescription = value.get('Description') or value.get('Caption')\n\t\tentries = description.split(' ')\n\n\t\tif 'Family' in entries and entries.index('Family') < len(entries)-1:\n\t\t\ti = entries.index('Family')\n\t\t\tfamily = int(entries[i + 1])\n\n\t\tif 'Model' in entries and entries.index('Model') < len(entries)-1:\n\t\t\ti = entries.index('Model')\n\t\t\tmodel = int(entries[i + 1])\n\n\t\tif 'Stepping' in entries and entries.index('Stepping') < len(entries)-1:\n\t\t\ti = entries.index('Stepping')\n\t\t\tstepping = int(entries[i + 1])\n\n\t\tinfo = {\n\t\t\t'vendor_id_raw' : value.get('Manufacturer'),\n\t\t\t'brand_raw' : processor_brand,\n\n\t\t\t'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale_advertised),\n\t\t\t'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, scale_actual),\n\t\t\t'hz_advertised' : _hz_short_to_full(hz_advertised, scale_advertised),\n\t\t\t'hz_actual' : _hz_short_to_full(hz_actual, scale_actual),\n\n\t\t\t'l2_cache_size' : l2_cache_size,\n\t\t\t'l3_cache_size' : l3_cache_size,\n\n\t\t\t'stepping' : stepping,\n\t\t\t'model' : model,\n\t\t\t'family' : family,\n\t\t}\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\t#raise # NOTE: To have this throw on error, uncomment this line\n\t\treturn {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_cpu_info_from_registry():\n\t'''\n\tFIXME: Is missing many of the newer CPU flags like sse3\n\tReturns the CPU info gathered from the Windows Registry.\n\tReturns {} if not on Windows.\n\t'''\n\ttry:\n\t\t# Just return {} if not on Windows\n\t\tif not DataSource.is_windows:\n\t\t\treturn {}\n\n\t\t# Get the CPU name\n\t\tprocessor_brand = DataSource.winreg_processor_brand().strip()\n\n\t\t# Get the CPU vendor id\n\t\tvendor_id = DataSource.winreg_vendor_id_raw()\n\n\t\t# Get the CPU arch and bits\n\t\tarch_string_raw = DataSource.winreg_arch_string_raw()\n\t\tarch, bits = _parse_arch(arch_string_raw)\n\n\t\t# Get the actual CPU Hz\n\t\thz_actual = DataSource.winreg_hz_actual()\n\t\thz_actual = _to_decimal_string(hz_actual)\n\n\t\t# Get the advertised CPU Hz\n\t\thz_advertised, scale = _parse_cpu_brand_string(processor_brand)\n\n\t\t# If advertised hz not found, use the actual hz\n\t\tif hz_advertised == '0.0':\n\t\t\tscale = 6\n\t\t\thz_advertised = _to_decimal_string(hz_actual)\n\n\t\t# Get the CPU features\n\t\tfeature_bits = DataSource.winreg_feature_bits()\n\n\t\tdef is_set(bit):\n\t\t\tmask = 0x80000000 >> bit\n\t\t\tretval = mask & feature_bits > 0\n\t\t\treturn retval\n\n\t\t# http://en.wikipedia.org/wiki/CPUID\n\t\t# http://unix.stackexchange.com/questions/43539/what-do-the-flags-in-proc-cpuinfo-mean\n\t\t# http://www.lohninger.com/helpcsuite/public_constants_cpuid.htm\n\t\tflags = {\n\t\t\t'fpu' : is_set(0), # Floating Point Unit\n\t\t\t'vme' : is_set(1), # V86 Mode Extensions\n\t\t\t'de' : is_set(2), # Debug Extensions - I/O breakpoints supported\n\t\t\t'pse' : is_set(3), # Page Size Extensions (4 MB pages supported)\n\t\t\t'tsc' : is_set(4), # Time Stamp Counter and RDTSC instruction are available\n\t\t\t'msr' : is_set(5), # Model Specific Registers\n\t\t\t'pae' : is_set(6), # Physical Address Extensions (36 bit address, 2MB pages)\n\t\t\t'mce' : is_set(7), # Machine Check Exception supported\n\t\t\t'cx8' : is_set(8), # Compare Exchange Eight Byte instruction available\n\t\t\t'apic' : is_set(9), # Local APIC present (multiprocessor operation support)\n\t\t\t'sepamd' : is_set(10), # Fast system calls (AMD only)\n\t\t\t'sep' : is_set(11), # Fast system calls\n\t\t\t'mtrr' : is_set(12), # Memory Type Range Registers\n\t\t\t'pge' : is_set(13), # Page Global Enable\n\t\t\t'mca' : is_set(14), # Machine Check Architecture\n\t\t\t'cmov' : is_set(15), # Conditional MOVe instructions\n\t\t\t'pat' : is_set(16), # Page Attribute Table\n\t\t\t'pse36' : is_set(17), # 36 bit Page Size Extensions\n\t\t\t'serial' : is_set(18), # Processor Serial Number\n\t\t\t'clflush' : is_set(19), # Cache Flush\n\t\t\t#'reserved1' : is_set(20), # reserved\n\t\t\t'dts' : is_set(21), # Debug Trace Store\n\t\t\t'acpi' : is_set(22), # ACPI support\n\t\t\t'mmx' : is_set(23), # MultiMedia Extensions\n\t\t\t'fxsr' : is_set(24), # FXSAVE and FXRSTOR instructions\n\t\t\t'sse' : is_set(25), # SSE instructions\n\t\t\t'sse2' : is_set(26), # SSE2 (WNI) instructions\n\t\t\t'ss' : is_set(27), # self snoop\n\t\t\t#'reserved2' : is_set(28), # reserved\n\t\t\t'tm' : is_set(29), # Automatic clock control\n\t\t\t'ia64' : is_set(30), # IA64 instructions\n\t\t\t'3dnow' : is_set(31) # 3DNow! instructions available\n\t\t}\n\n\t\t# Get a list of only the flags that are true\n\t\tflags = [k for k, v in flags.items() if v]\n\t\tflags.sort()\n\n\t\tinfo = {\n\t\t'vendor_id_raw' : vendor_id,\n\t\t'brand_raw' : processor_brand,\n\n\t\t'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),\n\t\t'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, 6),\n\t\t'hz_advertised' : _hz_short_to_full(hz_advertised, scale),\n\t\t'hz_actual' : _hz_short_to_full(hz_actual, 6),\n\n\t\t'flags' : flags\n\t\t}\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\treturn {}", "response": "Get the CPU info from the Windows Registry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the CPU info gathered from isainfo and kstat.", "response": "def _get_cpu_info_from_kstat():\n\t'''\n\tReturns the CPU info gathered from isainfo and kstat.\n\tReturns {} if isainfo or kstat are not found.\n\t'''\n\ttry:\n\t\t# Just return {} if there is no isainfo or kstat\n\t\tif not DataSource.has_isainfo() or not DataSource.has_kstat():\n\t\t\treturn {}\n\n\t\t# If isainfo fails return {}\n\t\treturncode, flag_output = DataSource.isainfo_vb()\n\t\tif flag_output == None or returncode != 0:\n\t\t\treturn {}\n\n\t\t# If kstat fails return {}\n\t\treturncode, kstat = DataSource.kstat_m_cpu_info()\n\t\tif kstat == None or returncode != 0:\n\t\t\treturn {}\n\n\t\t# Various fields\n\t\tvendor_id = kstat.split('\\tvendor_id ')[1].split('\\n')[0].strip()\n\t\tprocessor_brand = kstat.split('\\tbrand ')[1].split('\\n')[0].strip()\n\t\tstepping = int(kstat.split('\\tstepping ')[1].split('\\n')[0].strip())\n\t\tmodel = int(kstat.split('\\tmodel ')[1].split('\\n')[0].strip())\n\t\tfamily = int(kstat.split('\\tfamily ')[1].split('\\n')[0].strip())\n\n\t\t# Flags\n\t\tflags = flag_output.strip().split('\\n')[-1].strip().lower().split()\n\t\tflags.sort()\n\n\t\t# Convert from GHz/MHz string to Hz\n\t\tscale = 6\n\t\thz_advertised = kstat.split('\\tclock_MHz ')[1].split('\\n')[0].strip()\n\t\thz_advertised = _to_decimal_string(hz_advertised)\n\n\t\t# Convert from GHz/MHz string to Hz\n\t\thz_actual = kstat.split('\\tcurrent_clock_Hz ')[1].split('\\n')[0].strip()\n\t\thz_actual = _to_decimal_string(hz_actual)\n\n\t\tinfo = {\n\t\t'vendor_id_raw' : vendor_id,\n\t\t'brand_raw' : processor_brand,\n\n\t\t'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),\n\t\t'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, 0),\n\t\t'hz_advertised' : _hz_short_to_full(hz_advertised, scale),\n\t\t'hz_actual' : _hz_short_to_full(hz_actual, 0),\n\n\t\t'stepping' : stepping,\n\t\t'model' : model,\n\t\t'family' : family,\n\t\t'flags' : flags\n\t\t}\n\n\t\tinfo = {k: v for k, v in info.items() if v}\n\t\treturn info\n\texcept:\n\t\treturn {}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_cpu_info_internal():\n\t'''\n\tReturns the CPU info by using the best sources of information for your OS.\n\tReturns {} if nothing is found.\n\t'''\n\n\t# Get the CPU arch and bits\n\tarch, bits = _parse_arch(DataSource.arch_string_raw)\n\n\tfriendly_maxsize = { 2**31-1: '32 bit', 2**63-1: '64 bit' }.get(sys.maxsize) or 'unknown bits'\n\tfriendly_version = \"{0}.{1}.{2}.{3}.{4}\".format(*sys.version_info)\n\tPYTHON_VERSION = \"{0} ({1})\".format(friendly_version, friendly_maxsize)\n\n\tinfo = {\n\t\t'python_version' : PYTHON_VERSION,\n\t\t'cpuinfo_version' : CPUINFO_VERSION,\n\t\t'cpuinfo_version_string' : CPUINFO_VERSION_STRING,\n\t\t'arch' : arch,\n\t\t'bits' : bits,\n\t\t'count' : DataSource.cpu_count,\n\t\t'arch_string_raw' : DataSource.arch_string_raw,\n\t}\n\n\t# Try the Windows wmic\n\t_copy_new_fields(info, _get_cpu_info_from_wmic())\n\n\t# Try the Windows registry\n\t_copy_new_fields(info, _get_cpu_info_from_registry())\n\n\t# Try /proc/cpuinfo\n\t_copy_new_fields(info, _get_cpu_info_from_proc_cpuinfo())\n\n\t# Try cpufreq-info\n\t_copy_new_fields(info, _get_cpu_info_from_cpufreq_info())\n\n\t# Try LSCPU\n\t_copy_new_fields(info, _get_cpu_info_from_lscpu())\n\n\t# Try sysctl\n\t_copy_new_fields(info, _get_cpu_info_from_sysctl())\n\n\t# Try kstat\n\t_copy_new_fields(info, _get_cpu_info_from_kstat())\n\n\t# Try dmesg\n\t_copy_new_fields(info, _get_cpu_info_from_dmesg())\n\n\t# Try /var/run/dmesg.boot\n\t_copy_new_fields(info, _get_cpu_info_from_cat_var_run_dmesg_boot())\n\n\t# Try lsprop ibm,pa-features\n\t_copy_new_fields(info, _get_cpu_info_from_ibm_pa_features())\n\n\t# Try sysinfo\n\t_copy_new_fields(info, _get_cpu_info_from_sysinfo())\n\n\t# Try querying the CPU cpuid register\n\t_copy_new_fields(info, _get_cpu_info_from_cpuid())\n\n\t# Try platform.uname\n\t_copy_new_fields(info, _get_cpu_info_from_platform_uname())\n\n\treturn info", "response": "Get the CPU info by using the best sources of information for your OS."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cpu_info_json():\n\t'''\n\tReturns the CPU info by using the best sources of information for your OS.\n\tReturns the result in a json string\n\t'''\n\n\timport json\n\n\toutput = None\n\n\t# If running under pyinstaller, run normally\n\tif getattr(sys, 'frozen', False):\n\t\tinfo = _get_cpu_info_internal()\n\t\toutput = json.dumps(info)\n\t\toutput = \"{0}\".format(output)\n\t# if not running under pyinstaller, run in another process.\n\t# This is done because multiprocesing has a design flaw that\n\t# causes non main programs to run multiple times on Windows.\n\telse:\n\t\tfrom subprocess import Popen, PIPE\n\n\t\tcommand = [sys.executable, __file__, '--json']\n\t\tp1 = Popen(command, stdout=PIPE, stderr=PIPE, stdin=PIPE)\n\t\toutput = p1.communicate()[0]\n\n\t\tif p1.returncode != 0:\n\t\t\treturn \"{}\"\n\n\t\tif not IS_PY2:\n\t\t\toutput = output.decode(encoding='UTF-8')\n\n\treturn output", "response": "Returns the CPU info by using the best sources of information for your OS."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the CPU info by using the best sources of information for your OS.", "response": "def get_cpu_info():\n\t'''\n\tReturns the CPU info by using the best sources of information for your OS.\n\tReturns the result in a dict\n\t'''\n\n\timport json\n\n\toutput = get_cpu_info_json()\n\n\t# Convert JSON to Python with non unicode strings\n\toutput = json.loads(output, object_hook = _utf_to_str)\n\n\treturn output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef createTransactionLookupConnector(chain=Chain.bitcoin_mainnet, options=None):\n    if chain == Chain.mockchain or chain == Chain.bitcoin_regtest:\n        return MockConnector(chain)\n    elif chain.blockchain_type == BlockchainType.ethereum:\n        if options and 'etherscan_api_token' in options:\n            etherscan_api_token = options['etherscan_api_token']\n        else:\n            etherscan_api_token = ''\n        return EtherscanConnector(chain, etherscan_api_token)\n    return FallbackConnector(chain)", "response": "create a connector for looking up transactions in a single chain"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a spacy document return the verbs that have subjects", "response": "def _verbs_with_subjects(doc):\n    \"\"\"Given a spacy document return the verbs that have subjects\"\"\"\n    # TODO: UNUSED\n    verb_subj = []\n    for possible_subject in doc:\n        if (possible_subject.dep_ == 'nsubj' and possible_subject.head.pos_ ==\n                'VERB'):\n            verb_subj.append([possible_subject.head, possible_subject])\n    return verb_subj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mangle_sentences_from_file(input_file):\n    try:\n        with open(input_file, 'r') as f:\n            # final sentence may not be a complete sentence, save and prepend to next chunk\n            leftovers = ''\n            sentence_no = 0\n            for chunk in read_in_chunks(f): # lazy way of reading our file in case it's large\n                # prepend leftovers to chunk\n                chunk = chunk.decode('utf8')\n                chunk = leftovers + chunk\n                chunk = chunk.replace(';', '.') # replace semi colons with periods \n                doc = nlp(chunk)\n\n                # last sentence may not be sentence, move to next chunk\n                sents = [sent.string.strip() for sent in doc.sents]\n                if len(sents) > 1:\n                    leftovers = sents[-1] + chunk.rpartition(sents[-1])[-1]\n                    sents = sents[:-1]\n\n                for sent in sents:\n                    sent = sent.replace('\\n', ' ')\n                    sent = sent.replace('\\r', ' ')\n                    sent = re.sub( '\\s+', ' ', sent ).strip()\n                    if len(sent) < 5:\n                        continue # skip tiny sentences\n                    # add original sentence to database\n                    cursor.execute('''insert into orignal_sentences (sentence,\n                    flesch_reading_ease, flesch_kincaid_grade_level) values (?,\n                    ?, ?)''', (sent, textstat.flesch_reading_ease(sent),\n                        textstat.flesch_kincaid_grade(sent)))\n                    og_sent_id = cursor.lastrowid\n                    for mangled_sentence in mangle_agreement(sent):\n                        # add mangled sentence to database\n                        cursor.execute('''insert into mangled_sentences\n                        (original_sentence_id, sentence) values (?, ?)''',\n                        (og_sent_id, mangled_sentence))\n                conn.commit() # commit all sentences in this chunk\n\n    except Exception as e:\n        print('error on {}'.format(input_file))\n        print(e)", "response": "Read a participle phrase file and mangle all sentences in it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a correct sentence return a sentence or sentences with a subject - verb agreement error", "response": "def mangle_agreement(correct_sentence):\n    \"\"\"Given a correct sentence, return a sentence or sentences with a subject\n    verb agreement error\"\"\"\n    # # Examples\n    #\n    # Back in the 1800s, people were much shorter and much stronger.\n    # This sentence begins with the introductory phrase, 'back in the 1800s'\n    # which means that it should have the past tense verb. Any other verb would\n    # be incorrect. \n    #\n    #\n    # Jack and jill went up the hill. \n    # This sentence is different; 'go' would also be correct. If it began with\n    # 'Yesterday', a single-word introductory phrase requiring no comma, only\n    # 'went' would be acceptable.\n    #\n    #  \n    # The man in the checkered shirt danced his warrior dance to show that\n    # he was the most dominant male in the room.\n    # This sentence has multiple verbs. If the sentence ended at the word dance,\n    # changing 'danced' to 'dances' would be acceptable, but since the sentence\n    # continues we cannot make this change -- 'was' agrees with 'danced' but not\n    # with 'dances'.  This is a shifty tense error, a classic subject verb\n    # agreement error.\n    # \n    # # Our Method\n    # \n    # Right now, we will assume that any change in verb form of a single verb in\n    # a sentence is incorrect.  As demonstrated above, this is not always true.\n    # We hope that since any model created off of this data will use a\n    # confidence interval to determine likelihood of a subject-verb agreement\n    # error, that some number can be found for which the model excels.\n    # \n    # It would also be possible to use a rule based learner to evaluate single\n    # verb sentences, and only evaluating more complex sentences with the\n    # tensorflow model.\n\n    bad_sents = []\n    doc = nlp(correct_sentence)\n    verbs = [(i, v) for (i, v) in enumerate(doc) if v.tag_.startswith('VB')]\n    for i, v in verbs:\n        for alt_verb in lexeme(doc[i].text):\n            if alt_verb == doc[i].text:\n                continue # Same as the original, skip it\n            if (tenses(alt_verb) == tenses(v.text) or\n                    (alt_verb.startswith(v.text) and alt_verb.endswith(\"n't\"))):\n                continue # Negated version of the original, skip it\n            new_sent = str(doc[:i]) + \" {} \".format(alt_verb) + str(doc[i+1:]) \n            new_sent = new_sent.replace(' ,', ',') # fix space before comma\n            bad_sents.append(new_sent)\n    return bad_sents"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_trigram_indices(trigram_index):\n    result = {}\n    trigram_count = 0\n    for key, val in csv.reader(open(trigram_index)):\n        result[key] = int(val)\n        trigram_count += 1\n    return result, trigram_count", "response": "Build a dictionary of trigrams and their indices from a csv file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the sentence or fragment begins with one of the parts ofspeech otherwise False", "response": "def _begins_with_one_of(sentence, parts_of_speech):\n    \"\"\"Return True if the sentence or fragment begins with one of the parts of\n    speech in the list, else False\"\"\"\n    doc = nlp(sentence)\n    if doc[0].tag_ in parts_of_speech:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_language_tool_feedback(sentence):\n    payload = {'language':'en-US', 'text':sentence}\n    try:\n        r = requests.post(LT_SERVER, data=payload)\n    except requests.exceptions.ConnectionError as e:\n        raise requests.exceptions.ConnectionError('''The languagetool server is\n        not running.  Try starting it with \"ltserver\" ''')\n    if r.status_code >= 200 and r.status_code < 300:\n        return r.json().get('matches', [])\n    return []", "response": "Get matches from languagetool"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_participle_clause_fragment(sentence):\n    # short circuit if sentence or fragment doesn't start with a participle\n    # past participles can sometimes look like adjectives -- ie, Tired\n    if not _begins_with_one_of(sentence, ['VBG', 'VBN', 'JJ']):\n        return 0.0\n\n    if _begins_with_one_of(sentence, ['JJ']):\n        doc = nlp(sentence)\n        fw = [w for w in doc][0]\n        # Beautiful toy birds\n        if fw.dep_ == 'amod':\n            return 0.0\n\n    # short circuit if sentence starts with a gerund and the gerund is the\n    # subject.\n    \n    if _begins_with_one_of(sentence, ['VBG']):\n        doc = nlp(sentence)\n        fw = [w for w in doc][0]\n        # Running is fun\n        if fw.dep_.endswith('subj'):\n            return 0.0\n\n        fc = [c for c in doc.noun_chunks]\n        # Dancing boys can never sing\n        if str(fw) in str(fc):\n            return 0.0\n\n    positive_prob = models['participle'].predict([_text_to_vector(sentence,\n        trigram2idx['participle'], trigram_count['participle'])])[0][1]\n    return float(positive_prob)", "response": "Supply a sentence or fragment and recieve a confidence interval"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsupplies a sentence or fragment and recieve feedback", "response": "def check(sentence):\n    \"\"\"Supply a sentence or fragment and recieve feedback\"\"\"\n\n    # How we decide what to put as the human readable feedback\n    #\n    # Our order of prefence is,\n    #\n    # 1. Spelling errors.\n    #   - A spelling error can change the sentence meaning\n    # 2. Subject-verb agreement errors\n    # 3. Subordinate conjunction starting a sentence\n    # 4. Participle phrase fragment\n    # 5. Other errors\n\n    result = Feedback()\n    is_missing_verb = detect_missing_verb(sentence)\n    is_infinitive = detect_infinitive_phrase(sentence)\n    is_participle = is_participle_clause_fragment(sentence)\n    lang_tool_feedback = get_language_tool_feedback(sentence)\n    subject_and_verb_agree = get_subject_verb_agreement_feedback(sentence)\n    ####\n    if is_missing_verb: # Lowest priority\n        result.matches['missing_verb'] = True \n        result.human_readable = MISSING_VERB_ADVICE.replace('\\n', '') \n        result.primary_error = 'MISSING_VERB_ERROR'\n        result.specific_error = 'MISSING_VERB'\n    if is_participle > .5: \n        result.matches['participle_phrase'] = is_participle\n        result.human_readable = PARTICIPLE_FRAGMENT_ADVICE.replace('\\n', '')\n        result.primary_error = 'FRAGMENT_ERROR'\n        result.specific_error = 'PARTICIPLE_PHRASE'\n    if lang_tool_feedback:\n        result.matches['lang_tool'] = lang_tool_feedback\n        for ltf in lang_tool_feedback:\n            if ltf['rule']['id'] == 'SENTENCE_FRAGMENT':\n                result.human_readable = lang_tool_feedback[0]['message']\n                result.primary_error = 'FRAGMENT_ERROR'\n                result.specific_error = 'SUBORDINATE_CLAUSE'\n    if is_infinitive:\n        result.matches['infinitive_phrase'] = True \n        result.human_readable = INFINITIVE_PHRASE_ADVICE.replace('\\n', '') \n        result.primary_error = 'INFINITIVE_PHRASE_ERROR'\n        result.specific_error = 'INFINITIVE_PHRASE'\n    if not subject_and_verb_agree:\n        result.matches['subject_verb_agreement'] = subject_and_verb_agree\n        result.human_readable = SUBJECT_VERB_AGREEMENT_ADVICE.replace('\\n', '') \n        result.primary_error = 'SUBJECT_VERB_AGREEMENT_ERROR'\n        result.specific_error = 'SUBJECT_VERB_AGREEMENT'\n    if lang_tool_feedback: # Highest priority (spelling, other lang tool errors)\n        result.matches['lang_tool'] = lang_tool_feedback\n        for ltf in lang_tool_feedback:\n            if ltf['rule']['id'] == 'MORFOLOGIK_RULE_EN_US':\n                result.human_readable = ltf['message']\n                result.primary_error = 'SPELLING_ERROR'\n                result.specific_error = 'SPELLING_ERROR'\n        if not result.primary_error:\n            result.human_readable = ltf['message']\n            result.primary_error = 'OTHER_ERROR'\n            result.specific_error = ltf['rule']['id']\n\n         \n    ####\n    if not result.matches:\n        result.human_readable = STRONG_SENTENCE_ADVICE.replace('\\n', '')\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the past submissions with information about them", "response": "def list_submissions():\n    \"\"\"List the past submissions with information about them\"\"\"\n    submissions = []\n    try:\n        submissions = session.query(Submission).all()\n    except SQLAlchemyError as e:\n        session.rollback()\n    return render_template('list_submissions.html', submissions=submissions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nraise an error if the modal auxilary has an issue with it", "response": "def raise_modal_error(verb_phrase_doc):\n    \"\"\"Given a verb phrase, raise an error if the modal auxilary has an issue\n    with it\"\"\"\n    verb_phrase = verb_phrase_doc.text.lower()\n    bad_strings = ['should had', 'should has', 'could had', 'could has', 'would '\n            'had', 'would has'] [\"should\", \"could\", \"would\"]\n    for bs in bad_strings:\n        if bs in verb_phrase:\n            raise('ShouldCouldWouldError')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubstitute infinitives as subjects.", "response": "def substitute_infinitives_as_subjects(sent_str):\n    \"\"\"If an infinitive is used as a subject, substitute the gerund.\"\"\"\n    sent_doc = textacy.Doc(sent_str, lang='en_core_web_lg')\n    #inf_pattern = r'<PART><VERB>+' # To aux/auxpass* csubj\n    inf_pattern = r'<PART><VERB>' # To aux/auxpass* csubj\n    infinitives = textacy.extract.pos_regex_matches(sent_doc, inf_pattern)\n    inf_subjs = [] # => [[0,1],...]\n    for inf in infinitives:\n        if inf[0].text.lower() != 'to':\n            continue\n        if ('csubj' not in [w.dep_ for w in inf] and sent_doc[inf[-1].i + 1].dep_\n                != 'csubj'):\n            continue\n        if inf[-1].tag_ != 'VB':\n            continue\n        inf_subj = []\n        for v in inf:\n            inf_subj.append(v.i)\n        inf_subjs.append(inf_subj) \n    new_sent_str = sent_str\n    unusual_char = '\u5f62'\n    for inf_subj in inf_subjs:\n        start_inf = sent_doc[inf_subj[0]].idx \n        end_inf = sent_doc[inf_subj[-1]].idx + len(sent_doc[inf_subj[-1]])\n        inf_len = end_inf - start_inf\n        sub = (unusual_char * inf_len)\n        new_sent_str = new_sent_str[:start_inf] + sub + new_sent_str[end_inf:]\n    new_sent_str = re.sub('\u5f62+', '{}', new_sent_str)\n    repl = [conjugate(sent_doc[i_s[-1]].text, tense='presentparticiple') for i_s in inf_subjs]\n    return new_sent_str.format(*repl)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simplify_compound_subjects(sentence_str):\n\n    sentence_doc = textacy.Doc(sentence_str, lang='en_core_web_lg')\n    \n    cs_patterns = \\\n            [r'((<DET>?(<NOUN|PROPN>|<PRON>)+<PUNCT>)+<DET>?(<NOUN|PROPN>|<PRON>)+<PUNCT>?<CCONJ><DET>?(<NOUN|PROPN>|<PRON>)+)|'\\\n            '(<DET>?(<NOUN|PROPN>|<PRON>)<CCONJ><DET>?(<NOUN|PROPN>|<PRON>))']\n\n    for cs_pattern in cs_patterns:\n    \n        compound_subjects = textacy.extract.pos_regex_matches(sentence_doc,\n                cs_pattern)\n\n        chars_to_repl = [] # [(start_repl, end_repl, replacement), (start_repl,\n        # end_repl, replacement), ...] \n        \n        for cs in compound_subjects:\n            for w in cs:\n                if w.pos_ == 'CCONJ' and w.text.lower() == 'and':\n                    # replace with they\n                    repl = 'they'.ljust(len(cs.text), '\u6587') # pad w unexpected char \n                    chars_to_repl.append([cs[0].idx, cs[-1].idx + len(cs[-1].text), repl])\n\n                elif w.pos_ == 'CCONJ' and w.text.lower() != 'and':\n                    # replace with final <DET>?(<NOUN|PROPN>|<PRON>)\n                    repl = cs[-1:].text\n                    if cs[-2].pos_ == 'DET':\n                        repl = cs[-2:].text\n                    repl = repl.ljust(len(cs.text), '\u6587') # pad w unexpected char \n                    \n                    chars_to_repl.append([cs[0].idx, cs[-1].idx + len(cs[-1].text), repl])\n        \n        new_sent_str = sentence_doc.text\n        for replacement in chars_to_repl:\n            new_sent_str = new_sent_str[:replacement[0]] + replacement[2] + \\\n                    new_sent_str[replacement[1]:]\n        new_sent_str = new_sent_str.replace('\u6587', '')\n        new_sent_str = re.sub( '\\s+', ' ', new_sent_str ).strip()\n\n    sentence_doc = textacy.Doc(new_sent_str, lang='en_core_web_lg')\n    return sentence_doc", "response": "Given a sentence doc return a new sentence doc with compound subjects reduced to their simplest forms."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a split infinitive warning if a split infinitive warning is returned else None", "response": "def split_infinitive_warning(sentence_str):\n    \"\"\"Return a warning for a split infinitive, else, None\"\"\"\n    sent_doc = textacy.Doc(sentence_str, lang='en_core_web_lg')\n    inf_pattern = r'<PART><ADV><VERB>' # To aux/auxpass* csubj\n    infinitives = textacy.extract.pos_regex_matches(sent_doc, inf_pattern)\n    for inf in infinitives:\n        if inf[0].text.lower() != 'to':\n            continue\n        if inf[-1].tag_ != 'VB':\n           continue \n        return 'SplitInfinitiveWarning'"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a string check that all infinitives are properly formatted and raise an exception if not.", "response": "def raise_infinitive_error(sentence_str):\n    \"\"\"Given a string, check that all infinitives are properly formatted\"\"\"\n    sent_doc = textacy.Doc(sentence_str, lang='en_core_web_lg')\n    inf_pattern = r'<PART|ADP><VERB>' # To aux/auxpass* csubj\n    infinitives = textacy.extract.pos_regex_matches(sent_doc, inf_pattern)\n    for inf in infinitives:\n        if inf[0].text.lower() != 'to':\n            continue\n        if inf[-1].tag_ != 'VB':\n            raise Exception('InfinitivePhraseError')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a string drop the modifiers and return a string with them", "response": "def drop_modifiers(sentence_str):\n    \"\"\"Given a string, drop the modifiers and return a string \n    without them\"\"\"\n    tdoc = textacy.Doc(sentence_str, lang='en_core_web_lg')\n    new_sent = tdoc.text \n    unusual_char = '\u5f62'\n    for tag in tdoc:\n        if tag.dep_.endswith('mod'): \n            # Replace the tag\n            new_sent = new_sent[:tag.idx] + unusual_char * len(tag.text) +\\\n                    new_sent[tag.idx + len(tag.text):]\n    new_sent = new_sent.replace(unusual_char, '')\n    new_sent = textacy.preprocess.normalize_whitespace(new_sent)\n    return new_sent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_verb_phrases(sentence_doc):\n    pattern =  r'<VERB>*<ADV>*<VERB>+' #  r'<VERB>?<ADV>*<VERB>+' is suggested by textacy site\n    verb_phrases = textacy.extract.pos_regex_matches(sentence_doc, pattern)\n\n    result = [] # [(1), (5,6,7)] => 2 verb phrases. a single verb at index 1, another verb phrase 5,6,7\n    for vp in verb_phrases:\n        word_numbers = []\n        # return the index of 'could have been happily eating' from 'She could have been happily eating chowder'\n        first_word = vp.start\n\n        x = first_word\n        if len(vp) > 1:\n            for verb_or_adverb in vp:\n                # filter out adverbs\n                if not verb_or_adverb.pos_ == 'ADV':\n                    word_numbers.append(x)\n                x += 1\n        else:\n            word_numbers.append(first_word)\n        \n        # filter out infinitive phrases\n        if ( (word_numbers[0] - 1) < 0) or (sentence_doc[word_numbers[0] - 1].text.lower() != 'to'):\n            result.append(word_numbers)\n    \n    return result", "response": "Returns a list of verb phrases that are part of the sentence."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclustering a list of texts into a predefined number of clusters.", "response": "def cluster(list_of_texts, num_clusters=3):\n    \"\"\"\n    Cluster a list of texts into a predefined number of clusters.\n\n    :param list_of_texts: a list of untokenized texts\n    :param num_clusters: the predefined number of clusters\n    :return: a list with the cluster id for each text, e.g. [0,1,0,0,2,2,1]\n    \"\"\"\n    pipeline = Pipeline([\n        (\"vect\", CountVectorizer()),\n        (\"tfidf\", TfidfTransformer()),\n        (\"clust\", KMeans(n_clusters=num_clusters))\n    ])\n\n    try:\n        clusters = pipeline.fit_predict(list_of_texts)\n    except ValueError:\n        clusters = list(range(len(list_of_texts)))\n\n    return clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_topics(token_lists, num_topics=10):\n    dictionary = Dictionary(token_lists)\n    print('Number of unique words in original documents:', len(dictionary))\n\n    dictionary.filter_extremes(no_below=2, no_above=0.7)\n    print('Number of unique words after removing rare and common words:', len(dictionary))\n\n    corpus = [dictionary.doc2bow(tokens) for tokens in token_lists]\n    model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, chunksize=100, passes=5, random_state=1)\n\n    print_topics(model)\n\n    return model, dictionary", "response": "Find the topics in a list of texts with Latent Dirichlet Allocation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_bookshelf(start_url, output_dir):\n    # make output directory\n    try:\n        os.mkdir(OUTPUT_DIR + output_dir)\n    except OSError as e:\n        raise(e)\n\n    # fetch page\n    r = requests.get(start_url)\n\n    # extract links\n    soup = bs(r.text, 'html.parser')\n    book_links = soup.find_all(class_=re.compile(\"extiw\"))\n    new_links = []\n    for el in book_links:\n        link = el['href']\n        title = el.text\n        bookid = link.split('/')[-1]\n        if bookid.isdigit():\n            new_link = NEW_LINK_BASE.format(bookid, bookid)\n            new_links.append([title, new_link])\n\n    # save links as books\n    for link_tup in new_links:\n        time.sleep(.10) # be nice to project gutenberg\n        r1 = requests.get(link_tup[1])\n        new_filename = link_tup[0].lower().replace(' ', '-').replace('\\n',\n                '-')\n        new_new_filename = ''\n        for char in new_filename:\n            if char in 'abcdefghijklmnopqrstuvwxyz-':\n                new_new_filename += char\n        new_filename = new_new_filename[:MAX_FILENAME_LEN] + '.txt'\n        with open(OUTPUT_DIR + output_dir + '/' + new_filename, 'w+') as output_file:\n            output_file.write(r1.text)\n\n    return None", "response": "Fetch all the books off of a gutenberg project bookshelf page and save them to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the lemmas of the tokens in a text.", "response": "def lemmatize(text, lowercase=True, remove_stopwords=True):\n    \"\"\" Return the lemmas of the tokens in a text. \"\"\"\n    doc = nlp(text)\n    if lowercase and remove_stopwords:\n        lemmas = [t.lemma_.lower() for t in doc if not (t.is_stop or t.orth_.lower() in STOPWORDS)]\n    elif lowercase:\n        lemmas = [t.lemma_.lower() for t in doc]\n    elif remove_stopwords:\n        lemmas = [t.lemma_ for t in doc if not (t.is_stop or t.orth_.lower() in STOPWORDS)]\n    else:\n        lemmas = [t.lemma_ for t in doc]\n\n    return lemmas"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a defalated vector inflate it into a np array and return it", "response": "def inflate(deflated_vector):\n    \"\"\"Given a defalated vector, inflate it into a np array and return it\"\"\"\n    dv = json.loads(deflated_vector)\n    #result = np.zeros(dv['reductions']) # some claim vector length 5555, others\n    #5530. this could have occurred doing remote computations? or something.\n    # anyhow, we will use 5555.  Let's just hard code it.  Gosh darnit.\n    result = np.zeros(5555) # some claim vector length 5555, others\n    for n in dv['indices']:\n        result[int(n)] = dv['indices'][n]\n    #print(\"Inflated vector. Length\", len(result))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a string get it s defalted vector then inflate it then return the inflated vector", "response": "def text_to_vector(sent_str):\n    \"\"\"Given a string, get it's defalted vector, inflate it, then return the\n    inflated vector\"\"\"\n    r = requests.get(\"{}/sva/vector\".format(VECTORIZE_API), params={'s':sent_str})\n    return inflate(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_agreement2(sentence):\n    correct = False\n\n    # see number of verb phrases\n    pattern = r'<VERB>?<ADV>*<VERB>+'\n    doc = textacy.Doc(sentence, lang='en_core_web_lg')\n    vps = textacy.extract.pos_regex_matches(doc, pattern)\n\n    #if len([x for x in vps]) < 2:\n    if (METHOD == 'COMBINED' and len([x for x in vps]) < 2) or METHOD == 'RULE_BASED':\n        print(\"Simple sentence, using rule based checker\")\n        return check_agreement(sentence)\n    \n    # Use ML on more complex sentences\n    positive_prob = model.predict([text_to_vector(sentence)])[0][1]\n    return positive_prob <= 0.61", "response": "Returns False when subject and verb disagree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef detect_missing_verb(sentence):\n    # TODO: should this be relocated?\n    doc = nlp(sentence)\n    for w in doc:\n        if w.tag_.startswith('VB') and w.dep_ == 'ROOT':\n            return False # looks like there is at least 1 main verb\n    return True", "response": "Return True if the sentence appears to be missing a main verb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef detect_infinitive_phrase(sentence):\n\n    # eliminate sentences without to\n    if not 'to' in sentence.lower():\n        return False\n\n    doc = nlp(sentence)\n    prev_word = None\n    for w in doc:\n        # if statement will execute exactly once\n        if prev_word == 'to':\n            if w.dep_ == 'ROOT' and w.tag_.startswith('VB'): \n                return True # this is quite likely to be an infinitive phrase\n            else:\n                return False\n        prev_word = w.text.lower()", "response": "Given a string return true if it is an infinitive phrase fragment"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef perform_srl(responses, prompt):\n\n    predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz\")\n\n    sentences = [{\"sentence\": prompt + \" \" + response} for response in responses]\n    output = predictor.predict_batch_json(sentences)\n\n    full_output = [{\"sentence\": prompt + response,\n                    \"response\": response,\n                    \"srl\": srl} for (response, srl) in zip(responses, output)]\n\n    return full_output", "response": "Perform semantic role labeling on a list of responses given a prompt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef training_data(job_id):\n    '''Returns training_examples for a given job_id from offset to limit\n    If full_info parameter is greater than 0, will return extra architecture\n    info,\n    GET /jobs/139/vectors?offset=0&limit=10&full_info=1\n    {\n    \"labeled_vectors\": [{\"vector\":{\"indices\": {\"0\": 1}, \"reductions\": 3}, \"label\":0},\n                        {\"vector\":{\"indices\": {\"1\": 1}, \"reductions\": 3}, \"label\":1},\n                        ...],\n    \"vector_length\": 3, # non-negative int or -1 if vector length is inconsistent\n    \"num_labeled_vectors\": 1600000, # non-negative int\n    \"num_classes\": 2, # pos integer, probably 2 or more\n    }\n    '''\n    offset = request.args.get('offset', 0)\n    limit = request.args.get('limit', 0)\n    cur.execute('SELECT vector,label FROM vectors WHERE job_id=%s OFFSET %s LIMIT %s',\n            (job_id, offset, limit))\n    training_examples = [{'vector':v,'label':l} for v,l in cur]\n    data = { 'labeled_vectors': training_examples }\n\n    if int(request.args.get('full_info', 0)) > 0:\n        cur.execute(\"SELECT vector->>'reductions' AS num_reductions FROM vectors WHERE job_id=%s GROUP BY num_reductions\",\n                (job_id,))\n        unique_num_reductions = cur.fetchall() # [[5000]]\n        if len(unique_num_reductions) > 1:\n            # the vector length for this job is inconsistent! set vector_length\n            # to -1\n            data['vector_length'] = -1\n        else:\n            data['vector_length'] = unique_num_reductions[0][0]\n\n        cur.execute(\"SELECT count(*) FROM vectors WHERE job_id=%s\",\n                (job_id,))\n        data['num_labeled_vectors'] = cur.fetchone()[0]\n\n        cur.execute(\"SELECT count(*) FROM (SELECT label FROM vectors WHERE job_id=%s GROUP BY label) AS all_vecs_for_job\",\n                (job_id,))\n        data['num_classes'] = cur.fetchone()[0]\n\n    return jsonify(data)", "response": "Returns training_examples for a given job_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a sentence is a valid verb or plural subject.", "response": "def check_agreement(sentence):\n    \"\"\"Singular subject takes a singular verb, plural subject takes a plural\n    verb\"\"\"\n    feedback = ''\n    feedback += sentence + '\\n'\n    doc = nlp(sentence)\n    feedback += \"{}\\n\".format( [(d.text, d.dep_, d.tag_) for d in doc] )\n    subject_structure, verb_structure, auxilary_structure = '', '', ''\n    auxilary_text, subject_text, verb_text = '', '', ''\n    verb_base, auxilary_base = '', ''\n    auxilaries = []\n    prev_dep = ''\n    has_direct_object = False\n    acomp_following_root = False\n    for w in doc:\n        if prev_dep.endswith('subj') and w.text.upper() == 'AND':\n            subject_structure = 'COMPOUND_SUBJ'\n        elif w.dep_.endswith('subj') and not subject_structure:\n            if w.dep_.endswith('subj') and w.tag_ != 'PRP':\n                subject_structure = w.tag_ \n                subject_text = w.text.upper()\n            elif w.dep_.endswith('subj') and w.tag_ == 'PRP':\n                subject_structure = 'I/YOU/WE/THEY'\n                subject_text = w.text.upper()\n                if subject_text in ['HE', 'SHE', 'IT']:\n                    subject_structure = 'HE/SHE/IT'\n        elif w.dep_ == 'ROOT' and w.tag_ in ['VB', 'VBD', 'VBG', 'VBN', 'VBP',\n                'VBZ']:\n            verb_structure = w.tag_\n            verb_base = w.lemma_\n            verb_text = w.text.upper()\n        elif w.dep_.startswith('aux') and not verb_text:\n            # auxilaries before main verb\n            auxilary_structure = w.tag_\n            auxilary_base = w.lemma_\n            auxilary_text = w.text.upper()\n            auxilaries.append([auxilary_text, auxilary_structure,\n                auxilary_base])\n        elif w.dep_ == 'dobj':\n            has_direct_object = True\n        elif prev_dep == 'ROOT' and w.dep_ == 'acomp':\n            acomp_following_root = True\n            acomp_structure = w.tag_\n            # The scientists will be stir\n            # The scientists have been stir\n            if acomp_structure == 'VB':\n                return False\n\n        prev_dep = w.dep_\n\n    if not verb_text:\n        return True # if there is no main verb, no need to grade this\n\n    # Ensure auxilaries have correct number (modal auxilaries never change form)\n    have_precedes = False\n    previous_aux = None\n    for aux in auxilaries:\n        if aux[2] == 'be':\n            # if be form is preceded by modal auxilary it should be base form\n            if previous_aux and previous_aux[2] not in ['be', 'do', 'have']:\n                if aux[0] != 'BE':\n                    return False \n\n            # be form is not preceded by modal auxilary\n            elif not _to_be_agreement(subject_text, subject_structure, aux[0],\n                        have_precedes):\n                return False\n\n        elif (aux[2] == 'do' and not\n                _to_do_agreement(subject_text,subject_structure, aux[0])):\n            return False\n        elif (aux[2] == 'have' and not _to_have_agreement(subject_text,\n            subject_structure, aux[0])):\n            return False\n        elif aux[2] == 'have':\n            have_precedes = True\n\n        if aux[2] != 'have':\n            have_precedes = False\n\n        previous_aux = aux\n\n\n    # Deal with main verb edge cases\n    if verb_base == 'have':\n        if not _to_have_agreement(subject_text,subject_structure, verb_text):\n            return False\n    elif verb_base == 'be':\n        # be following modal auxilary does not change form\n        if auxilaries and auxilaries[-1][2] not in ['be', 'do', 'have']:\n            if verb_text != 'BE':\n                return False\n        # if be doesn't follow a modal auxilary,\n        elif not _to_be_agreement(subject_text, subject_structure, verb_text,\n                have_precedes):\n            return False\n    elif verb_base == 'do':\n        if not _to_do_agreement(subject_text, subject_structure, verb_text):\n            return False\n\n    # If auxilaries are used, main verb should be participle, else, check\n    # structure\n    if auxilaries:\n        if auxilaries[-1][2] in ['have', 'be']: \n\n            # Gerund never follows a form of have\n            if have_precedes:\n                # we have stirred the potion\n                # she has cooked dinner\n                return verb_structure in ['VBN']\n            elif has_direct_object: \n                # the scientist and the boy stir the potion\n                # we have been stirring [the potion](D.O.)\n                # she has been cooking dinner\n                # we should be cooking dinner\n                # we should be worried\n                return verb_structure == 'VBG'\n            \n            # we should have been worried [about you](object of the prep.)\n            # we should have been worrying about you \n            # The races have been run\n            # Have mixed the potion\n            return verb_structure in ['VBG', 'VBN']\n        else: # did or modal auxilary\n            return verb_structure == 'VB'\n            \n    if verb_base in ['have', 'be', 'do']:\n        return True\n    structure = '{}--{}'.format(subject_structure, verb_structure)\n    return (structure in ACCEPTABLE_STRUCTURES or structure in\n            NON_SENTENCE_STRUCTURE)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detokenize(s):\n    print(s)\n    s = re.sub(\"\\s+([;:,\\.\\?!])\", \"\\\\1\", s)\n    s = re.sub(\"\\s+(n't)\", \"\\\\1\", s)\n    return s", "response": "Detokenize a string by removing spaces before punctuation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls this with the matplotlib Figure object.", "response": "def set_figure(self, figure):\n        \"\"\"Call this with the matplotlib Figure() object.\"\"\"\n        self.figure = figure\n\n        ax = self.figure.add_axes((0, 0, 1, 1), frame_on=False,\n                                  #viewer=self,\n                                  #projection='ginga'\n                                  )\n        #ax = fig.add_subplot(111)\n        self.ax_img = ax\n        # We don't want the axes cleared every time plot() is called\n        if MPL_V1:\n            # older versions of matplotlib\n            ax.hold(False)\n        # TODO: is this needed, since frame_on == False?\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        #ax.patch.set_alpha(0.0)\n        ax.patch.set_visible(False)\n        #ax.autoscale(enable=True, tight=True)\n        ax.autoscale(enable=False)\n\n        # Add an overlapped axis for drawing graphics\n        newax = self.figure.add_axes(self.ax_img.get_position(),\n                                     sharex=ax, sharey=ax,\n                                     frameon=False,\n                                     #viewer=self,\n                                     #projection='ginga'\n                                     )\n        if MPL_V1:\n            newax.hold(True)\n        newax.autoscale(enable=False)\n        newax.get_xaxis().set_visible(False)\n        newax.get_yaxis().set_visible(False)\n        self.ax_util = newax\n\n        # Create timers\n        self._msg_timer = None\n        self._defer_timer = None\n\n        if hasattr(figure.canvas, 'new_timer'):\n            self._msg_timer = Timer(mplcanvas=figure.canvas)\n            self._msg_timer.add_callback('expired',\n                                         lambda timer: self.onscreen_message(None))\n\n            self._defer_timer = Timer(mplcanvas=figure.canvas)\n            self._defer_timer.add_callback('expired',\n                                           lambda timer: self.delayed_redraw())\n\n        canvas = figure.canvas\n        if hasattr(canvas, 'viewer'):\n            canvas.set_viewer(self)\n        else:\n            canvas.mpl_connect(\"resize_event\", self._resize_cb)\n\n        # Because we don't know if resize callback works with all backends\n        left, bottom, wd, ht = self.ax_img.bbox.bounds\n        self.configure_window(wd, ht)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_image1(self, rgbobj, dst_x, dst_y):\n        self.logger.debug(\"redraw surface\")\n        if self.figure is None:\n            return\n        ## left, bottom, width, height = self.ax_img.bbox.bounds\n        ## self._imgwin_wd, self._imgwin_ht = width, height\n\n        # Grab the RGB array for the current image and place it in the\n        # matplotlib figure axis\n        data = self.getwin_array(order=self.rgb_order, dtype=np.uint8)\n\n        dst_x = dst_y = 0\n\n        # fill background color\n        ## rect = self.figure.patch\n        ## rect.set_facecolor(self.img_bg)\n\n        # attempt 1: using a FigureImage (non-scaled)\n        if self.mpimage is None:\n            self.mpimage = self.figure.figimage(data, xo=dst_x, yo=dst_y,\n                                                origin='upper')\n\n        else:\n            # note: this is not a typo--these offsets have a different\n            # attribute name than in the constructor ('ox' vs. 'xo')\n            self.mpimage.ox = dst_x\n            self.mpimage.oy = dst_y\n            self.mpimage.set_data(data)", "response": "Render the image represented by rgbobj at dst_x dst_y in the pixel space."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_image2(self, rgbobj, dst_x, dst_y):\n        self.logger.debug(\"redraw surface\")\n        if self.figure is None:\n            return\n\n        ## left, bottom, width, height = self.ax_img.bbox.bounds\n        ## self._imgwin_wd, self._imgwin_ht = width, height\n\n        # Grab the RGB array for the current image and place it in the\n        # matplotlib figure axis\n        arr = self.getwin_array(order=self.rgb_order, dtype=np.uint8)\n\n        # Get the data extents\n        x0, y0 = 0, 0\n        y1, x1 = arr.shape[:2]\n        flipx, flipy, swapxy = self.get_transforms()\n        if swapxy:\n            x0, x1, y0, y1 = y0, y1, x0, x1\n\n        extent = (x0, x1, y1, y0)\n        self.logger.debug(\"extent=%s\" % (str(extent)))\n\n        # Calculate aspect ratio\n        aspect = self.calculate_aspect(arr.shape, extent)\n\n        if self.mpimage is None:\n            img = self.ax_img.imshow(arr, interpolation='none',\n                                     origin=\"upper\",\n                                     vmin=0, vmax=255,\n                                     extent=extent,\n                                     aspect=aspect)\n            self.mpimage = img\n\n        else:\n            self.mpimage.set_data(arr)\n            self.ax_img.set_aspect(aspect)\n            self.mpimage.set_extent(extent)", "response": "Render the image represented by rgbobj at dst_x dst_y in the pixel space."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_tasker(func):\n    def anonFunc(*args, **kwdargs):\n        class anonTask(Task):\n            def execute(self):\n                self.logger.debug(\"Executing fn %s\" % func)\n                try:\n                    val = func(*args, **kwdargs)\n\n                    self.logger.debug(\"Done executing fn %s\" % func)\n                    return val\n\n                except Exception as e:\n                    # Log error message and re-raise exception.\n                    self.logger.error(\"fn %s raised exception: %s\" % (\n                        func, str(e)))\n                    raise e\n\n        return anonTask()\n    return anonFunc", "response": "This function takes a callable function and returns a new factory function that returns a task that is executed on the specified arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self, taskParent, override=None):\n        # For now, punt if we have no apparent parent\n        if taskParent and hasattr(taskParent, 'shares'):\n            # Copy some variables from our parent task, unless they are being\n            # overridden explicitly.  Using this general \"contagion\" mechanism,\n            # a task can cause it's children to have values available to them\n            # without passing them explicitly.\n            for var in taskParent.shares:\n                if override and var in override:\n                    self.__dict__[var] = override[var]\n                else:\n                    #print \"COPYING VAR FROM PARENT: %s(%s)\" % (var, str(taskParent.__dict__[var]))\n                    self.__dict__[var] = taskParent.__dict__[var]\n\n        else:\n            #raise TaskError(\"Cannot initialize task without a taskParent!\")\n            pass\n\n        # Generate our own unique tag.  'tagger' should have been transmitted\n        # from the parent task\n        if not self.tag:\n            try:\n                self.tag = str(taskParent) + '.' + self.tagger.get_tag(self)\n            except Exception:\n                # Failed--fall back to internal tagger\n                self.tag = get_tag(taskParent)\n\n        # Some per-task specific initialization\n        self.ev_done.clear()\n        self.starttime = time.time()\n        self.endtime = 0\n        self.totaltime = 0\n        self.result = None\n\n        return self.tag", "response": "This method initializes a task for re - use."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_and_start(self, taskParent, override={}):\n        tag = self.initialize(taskParent, override=override)\n        self.start()\n\n        return tag", "response": "Convenience method to initialize and start a task.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait(self, timeout=None):\n        self.ev_done.wait(timeout=timeout)\n\n        if not self.ev_done.is_set():\n            raise TaskTimeout(\"Task %s timed out.\" % self)\n\n        # --> self.result is set\n        # If it is an exception, then raise it in this waiter\n        if isinstance(self.result, Exception):\n            raise self.result\n\n        # Release waiters and perform callbacks\n        # done() has already been called, because of self.ev_done check\n        # \"asynchronous\" tasks should could call done() here\n        #self.done(self.result)\n\n        return self.result", "response": "This method waits for an executing task to finish."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef runTask(self, task, timeout=None):\n        # Initialize the task.\n        task.initialize(self)\n\n        # Start the task.\n        task.start()\n\n        # Lets other threads run\n        time.sleep(0)\n\n        # Wait for it to finish.\n        res = task.wait(timeout=timeout)\n\n        # Now we're done\n        return res", "response": "Runs a child task to completion. Returns the result of\n        the child task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the next child task and wait for completion.", "response": "def step(self):\n        \"\"\"Run the next child task and wait for completion (no timeout).\"\"\"\n        if self.index >= len(self.tasklist):\n            raise TaskError(\"step(): sequential compound task %s finished\" % self)\n\n        self.check_state()\n\n        # Select next task from the set and advance the index\n        self.task = self.tasklist[self.index]\n        self.index += 1\n\n        return self.runTask(self.task)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self):\n        while self.index < len(self.tasklist):\n            res = self.step()\n            self.logger.debug('SeqSet task %i has completed with result %s' %\n                              (self.index, res))\n\n        # Returns result of last task to quit\n        return res", "response": "Run all child tasks in order waiting for completion of each."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninterrupts or cancel execution but will allow current child task to complete.", "response": "def stop(self):\n        \"\"\"Interrupt/cancel execution, but will allow current child task\n        to complete.\"\"\"\n        #self.ev_intr.set()\n\n        try:\n            self.task.stop()\n\n        except TaskError as e:\n            self.logger.error(\"Error cancelling child task: %s\" % (str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting all child tasks concurrently in separate threads. Return 0 after all child tasks have completed execution. Return 1 after all child tasks have completed execution. Return 0 after all child tasks have completed execution.", "response": "def execute(self):\n        \"\"\"Run all child tasks concurrently in separate threads.\n        Return 0 after all child tasks have completed execution.\n        \"\"\"\n        self.count = 0\n        self.taskset = []\n        self.results = {}\n        self.totaltime = time.time()\n\n        # Register termination callbacks for all my child tasks.\n        for task in list(self.taskseq):\n            self.taskset.append(task)\n            task.add_callback('resolved', self.child_done, self.count)\n            self.count += 1\n\n        self.numtasks = self.count\n\n        # Now start each child task.\n        with self.regcond:\n            for task in list(self.taskset):\n                task.initialize(self)\n\n                task.start()\n\n            # Account for time needed to start subtasks\n            self.totaltime = time.time() - self.totaltime\n\n            # Now give up the critical section and wait for last child\n            # task to terminate.\n            while self.count > 0:\n                self.regcond.wait()\n\n        # Scan results for errors (exceptions) and raise the first one we find\n        for key in self.results.keys():\n            value = self.results[key]\n            if isinstance(value, Exception):\n                (count, task) = key\n                self.logger.error(\"Child task %s terminated with exception: %s\" % (\n                    task.tag, str(value)))\n                raise value\n\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef child_done(self, task, result, count):\n        with self.regcond:\n            self.logger.debug('Concurrent task %d/%d has completed' % (\n                self.count, self.numtasks))\n            self.count -= 1\n            self.taskset.remove(task)\n            self.totaltime += task.getExecutionTime()\n            self.results[(count, task)] = result\n            if self.count <= 0:\n                self.regcond.notifyAll()", "response": "Called when a child task is done."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addTask(self, task):\n        with self.regcond:\n            self.taskset.append(task)\n            task.add_callback('resolved', self.child_done, self.numtasks)\n            self.numtasks += 1\n            self.count += 1\n\n        task.initialize(self)\n        task.start()", "response": "Add a task to the task set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self):\n\n        with self._lock_c:\n            self.count = 0\n            self.numtasks = 0\n            self.taskset = []\n            self.results = {}\n            self.totaltime = time.time()\n            # Start all tasks\n            for task in self.taskseq:\n                self.taskset.append(task)\n                self.numtasks += 1\n                task.init_and_start(self)\n\n        num_tasks = self.getNumTasks()\n        # Wait on each task to clean up results\n        while num_tasks > 0:\n\n            self.check_state()\n\n            for i in range(num_tasks):\n                try:\n                    try:\n                        task = self.getTask(i)\n                    except IndexError:\n                        # A task got deleted from the set.  Jump back out\n                        # to outer loop and repoll the number of tasks\n                        break\n\n                    #self.logger.debug(\"waiting on %s\" % task)\n                    res = task.wait(timeout=self.idletime)\n\n                    #self.logger.debug(\"finished: %s\" % task)\n                    self.child_done(res, task)\n\n                except TaskTimeout:\n                    continue\n\n                except Exception as e:\n                    #self.logger.warning(\"Subtask propagated exception: %s\" % str(e))\n                    self.child_done(e, task)\n                    continue\n\n            # wait a bit and try again\n            #self.ev_quit.wait(self.idletime)\n\n            # re-get number of tasks, in case some were added or deleted\n            num_tasks = self.getNumTasks()\n\n        # Scan results for errors (exceptions) and raise the first one we find\n        for key in self.results.keys():\n            value = self.results[key]\n            if isinstance(value, Exception):\n                (count, task) = key\n                self.logger.error(\"Child task %s terminated with exception: %s\" % (\n                    task.tag, str(value)))\n                raise value\n\n        # Return value of last child to complete\n        return value", "response": "Execute all child tasks in separate threads. Return last result after all child tasks have completed execution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(self):\n        with self._lock_c:\n            for task in self.taskset:\n                try:\n                    task.stop()\n\n                except TaskError as e:\n                    # Task does not have a way to stop it.\n                    # TODO: notify who?\n                    pass", "response": "Call stop on all child tasks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a task to the task set.", "response": "def addTask(self, task):\n        \"\"\"Add a task to the task set.\n        \"\"\"\n        # Try to start task first.  If it fails then we don't need to\n        # undo adding it to taskset\n        task.initialize(self)\n        task.start()\n\n        with self._lock_c:\n            self.numtasks += 1\n            self.taskset.append(task)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart all of the threads in the thread pool.", "response": "def startall(self, wait=False, **kwdargs):\n        \"\"\"Start all of the threads in the thread pool.  If _wait_ is True\n        then don't return until all threads are up and running.  Any extra\n        keyword arguments are passed to the worker thread constructor.\n        \"\"\"\n        self.logger.debug(\"startall called\")\n        with self.regcond:\n            while self.status != 'down':\n                if self.status in ('start', 'up') or self.ev_quit.is_set():\n                    # For now, abandon additional request to start\n                    self.logger.error(\"ignoring duplicate request to start thread pool\")\n                    return\n\n                self.logger.debug(\"waiting for threads: count=%d\" %\n                                  self.runningcount)\n                self.regcond.wait()\n\n            #assert(self.status == 'down')\n            if self.ev_quit.is_set():\n                return\n\n            self.runningcount = 0\n            self.status = 'start'\n            self.workers = []\n            if wait:\n                tpool = self\n            else:\n                tpool = None\n\n            # Start all worker threads\n            self.logger.debug(\"starting threads in thread pool\")\n            for i in range(self.numthreads):\n                t = self.workerClass(self.queue, logger=self.logger,\n                                     ev_quit=self.ev_quit, tpool=tpool,\n                                     **kwdargs)\n                self.workers.append(t)\n                t.start()\n\n            # if started with wait=True, then expect that threads will register\n            # themselves and last one up will set status to \"up\"\n            if wait:\n                # Threads are on the way up.  Wait until last one starts.\n                while self.status != 'up' and not self.ev_quit.is_set():\n                    self.logger.debug(\"waiting for threads: count=%d\" %\n                                      self.runningcount)\n                    self.regcond.wait()\n            else:\n                # otherwise, we just assume the pool is up\n                self.status = 'up'\n            self.logger.debug(\"startall done\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stopall(self, wait=False):\n        self.logger.debug(\"stopall called\")\n        with self.regcond:\n            while self.status != 'up':\n                if self.status in ('stop', 'down') or self.ev_quit.is_set():\n                    # For now, silently abandon additional request to stop\n                    self.logger.warning(\"ignoring duplicate request to stop thread pool.\")\n                    return\n\n                self.logger.debug(\"waiting for threads: count=%d\" %\n                                  self.runningcount)\n                self.regcond.wait()\n\n            #assert(self.status == 'up')\n            self.logger.debug(\"stopping threads in thread pool\")\n            self.status = 'stop'\n            # Signal to all threads to terminate.\n            self.ev_quit.set()\n\n            if wait:\n                # Threads are on the way down.  Wait until last one quits.\n                while self.status != 'down':\n                    self.logger.debug(\"waiting for threads: count=%d\" %\n                                      self.runningcount)\n                    self.regcond.wait()\n\n            self.logger.debug(\"stopall done\")", "response": "Stop all threads in the worker pool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling by the worker threads to register their own threads.", "response": "def register_up(self):\n        \"\"\"Called by WorkerThread objects to register themselves.\n\n        Acquire the condition variable for the WorkerThread objects.\n        Increment the running-thread count.  If we are the last thread to\n        start, set status to 'up'.  This allows startall() to complete\n        if it was called with wait=True.\n        \"\"\"\n        with self.regcond:\n            self.runningcount += 1\n            tid = thread.get_ident()\n            self.tids.append(tid)\n            self.logger.debug(\"register_up: (%d) count is %d\" %\n                              (tid, self.runningcount))\n            if self.runningcount == self.numthreads:\n                self.status = 'up'\n            self.regcond.notify()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_dn(self):\n        with self.regcond:\n            self.runningcount -= 1\n            tid = thread.get_ident()\n            self.tids.remove(tid)\n            self.logger.debug(\"register_dn: count_dn is %d\" % self.runningcount)\n            self.logger.debug(\"register_dn: remaining: %s\" % str(self.tids))\n            if self.runningcount == 0:\n                self.status = 'down'\n            self.regcond.notify()", "response": "Called by the worker threads to register their DN."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wcs_pix_transform(ct, i, format=0):\n    z1 = float(ct.z1)\n    z2 = float(ct.z2)\n    i = float(i)\n\n    yscale = 128.0 / (z2 - z1)\n    if (format == 'T' or format == 't'):\n        format = 1\n\n    if (i == 0):\n        t = 0.\n    else:\n        if (ct.zt == W_LINEAR):\n            t = ((i - 1) * (z2 - z1) / 199.0) + z1\n            t = max(z1, min(z2, t))\n        else:\n            t = float(i)\n    if (format > 1):\n        t = (z2 - t) * yscale\n    return (t)", "response": "Computes the WCS corrected pixel value given a coordinate transformation and the raw pixel value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the RA and Dec coordinates for a given coordinate transformation and the screen coordinates x and y.", "response": "def wcs_coord_transform(ct, x, y):\n    \"\"\"Computes tha WCS corrected pixel coordinates (RA and Dec\n    in degrees) given a coordinate transformation and the screen\n    coordinates (x and y, in pixels).\n\n    Input:\n    ct      coordinate transformation. instance of coord_tran.\n    x       x coordinate in pixels.\n    y       y coordinate in pixels.\n\n    Returns:\n    (RA, Dec) in degrees (as floats).\n    \"\"\"\n    x = float(x)\n    y = float(y)\n    if (ct.valid):\n        # The imtool WCS assumes that the center of the first display\n        # pixel is at (0,0) but actually it is at (0.5,0.5).\n        #x -= 0.5\n        #y -= 0.5\n\n        if (abs(ct.a) > .001):\n            ra = ct.a * x + ct.c * y + ct.tx\n        if (abs(ct.d) > .001):\n            dec = ct.b * x + ct.d * y + ct.ty\n    else:\n        ra = x\n        dec = y\n    return ((ra, dec))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_request(self):\n        try:\n            (request, client_address) = self.get_request()\n\n        except socket.error as e:\n            # Error handling goes here.\n            self.logger.error(\"error opening the connection: %s\" % (\n                str(e)))\n            for exctn in sys.exc_info():\n                print(exctn)\n            return\n\n        try:\n            self.RequestHandlerClass(request, client_address, self)\n        except Exception as e:\n            # Error handling goes here.\n            self.logger.error('error handling the request: %s' % (\n                str(e)))\n            for exctn in sys.exc_info():\n                print(exctn)\n            return", "response": "Handles incoming requests one at the time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mainloop(self):\n        try:\n            while (not self.ev_quit.is_set()):\n                try:\n                    self.handle_request()\n\n                except socketTimeout:\n                    continue\n        finally:\n            self.socket.close()", "response": "main loop for the event loop."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wcs_update(self, wcs_text, fb=None):\n        if (fb):\n            ct = fb.ct\n        else:\n            ct = coord_tran()\n        if (not ct.valid):\n            ct.zt = W_UNITARY\n\n            # read wcs_text\n            data = string.split(wcs_text, '\\n')\n            ct.imtitle = data[0]\n            # we are expecting 8 floats and 1 int\n            try:\n                (ct.a, ct.b, ct.c, ct.d,\n                 ct.tx, ct.ty, ct.z1, ct.z2,\n                 ct.zt) = string.split(data[1])\n                ct.a = float(ct.a)\n                ct.b = float(ct.b)\n                ct.c = float(ct.c)\n                ct.d = float(ct.d)\n                ct.tx = float(ct.tx)\n                ct.ty = float(ct.ty)\n                ct.z1 = float(ct.z1)\n                ct.z2 = float(ct.z2)\n                ct.zt = int(ct.zt)\n            except Exception:\n                ct.imtitle = \"[NO WCS]\"\n                ct.a = 1\n                ct.d = 1\n                ct.b = 0\n                ct.c = 0\n                ct.tx = 0\n                ct.ty = 0\n                ct.zt = W_UNITARY\n            ct.valid += 1\n\n            # determine the best format for WCS output\n            if (ct.valid and ct.zt == W_LINEAR):\n                z1 = ct.z1\n                z2 = ct.z2\n                zrange = abs(z1 - z2)\n                zavg = (abs(z1) + abs(z2)) / 2.0\n                if (zrange < 100.0 and zavg < 200.0):\n                    ct.format = \" %7.2f %7.2f %7.3f%c\"\n                elif (zrange > 99999.0 or zavg > 99999.0):\n                    ct.format = \" %7.2f %7.2f %7.3g%c\"\n                else:\n                    ct.format = W_DEFFORMAT\n            else:\n                ct.format = \" %7.2f %7.2f %7.0f%c\"\n\n            # add_mapping, if we can\n            if (len(data) < 4):\n                return(ct)\n\n            # we are expecting 1 string, 2 floats, and 6 int\n            try:\n                print(\"updating WCS: %s\" % str(data[2]))\n                (ct.region, ct.sx, ct.sy, ct.snx,\n                 ct.sny, ct.dx, ct.dy, ct.dnx,\n                 ct.dny) = string.split(data[2])\n                ct.sx = float(ct.sx)\n                ct.sy = float(ct.sy)\n                ct.snx = int(ct.snx)\n                ct.sny = int(ct.sny)\n                # dx, dy: offset into frame where actual data starts\n                ct.dx = int(ct.dx)\n                ct.dy = int(ct.dy)\n                # dnx, dny: length of actual data in frame from offsets\n                ct.dnx = int(ct.dnx)\n                ct.dny = int(ct.dny)\n                ct.ref = string.strip(data[3])\n                # if this works, we also have the real size of the image\n                fb.img_width = ct.dnx + 1  # for some reason, the width is always 1 pixel smaller...\n                fb.img_height = ct.dny\n            except Exception:\n                ct.region = 'none'\n                ct.sx = 1.0\n                ct.sy = 1.0\n                ct.snx = fb.width\n                ct.sny = fb.height\n                ct.dx = 1\n                ct.dy = 1\n                ct.dnx = fb.width\n                ct.dny = fb.height\n                ct.ref = 'none'\n        return (ct)", "response": "Parse the WCS text and populate the fields of the current object with the values from the input WCS text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef return_cursor(self, dataout, sx, sy, frame, wcs, key, strval=''):\n        #print \"RETURN CURSOR\"\n        wcscode = (frame + 1) * 100 + wcs\n        if (key == '\\32'):\n            curval = \"EOF\"\n        else:\n            if (key in string.printable and key not in string.whitespace):\n                keystr = key\n            else:\n                keystr = \"\\\\%03o\" % (ord(key))\n\n        # send the necessary infor to the client\n        curval = \"%10.3f %10.3f %d %s %s\\n\" % (sx, sy, wcscode, keystr, strval)\n        dataout.write(right_pad(curval, SZ_IMCURVAL))", "response": "Writes the cursor position to dataout."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_wcs(self, pkt):\n        self.logger.debug(\"handle wcs\")\n        if pkt.tid & IIS_READ:\n            self.logger.debug(\"iis read\")\n            # Return the WCS for the referenced frame.\n            if (pkt.x & 0o17777) and (pkt.y & 0o17777):\n                # return IIS version number\n                text = \"version=\" + str(IIS_VERSION)\n                text = right_pad(text, SZ_OLD_WCSBUF)\n            else:\n                frame = self.decode_frameno(pkt.z & 0o177777) - 1\n                try:\n                    fb = self.server.controller.get_frame(frame)\n                except KeyError:\n                    fb = None\n                self.logger.debug(\"frame=%d fb=%s\" % (frame, fb))\n\n                if (pkt.x & 0o17777) and (pkt.t & 0o17777):\n                    self.frame = frame\n                    if (fb and fb.ct.a is not None):\n                        wcs = \"%s\\n%f %f %f %f %f %f %f %f %d\\n\" % (\n                            fb.ct.imtitle, fb.ct.a, fb.ct.b, fb.ct.c, fb.ct.d,\n                            fb.ct.tx, fb.ct.ty, fb.ct.z1, fb.ct.z2, fb.ct.zt)\n                    else:\n                        wcs = \"[NOSUCHWCS]\\n\"\n                    if (fb and fb.ct.sx is not None):\n                        mapping = \"%s %f %f %d %d %d %d %d %d\\n%s\\n\" % (\n                            fb.ct.region, fb.ct.sx, fb.ct.sy, fb.ct.snx, fb.ct.sny,\n                            fb.ct.dx, fb.ct.dy, fb.ct.dnx, fb.ct.dny, fb.ct.ref)\n                    else:\n                        mapping = \"\"\n                    text = wcs + mapping\n                    text = right_pad(text, SZ_WCSBUF)\n                else:\n                    if ((frame < 0) or (fb is None) or (fb.buffer is None) or\n                            (len(fb.buffer) == 0)):\n                        text = \"[NOSUCHFRAME]\"\n                    else:\n                        text = fb.wcs\n\n                    # old style or new style?\n                    if pkt.x & 0o777:\n                        text = right_pad(text, SZ_WCSBUF)\n                    else:\n                        text = right_pad(text, SZ_OLD_WCSBUF)\n            self.logger.debug(\"WCS: \" + text)\n            pkt.dataout.write(text)\n\n        else:\n            self.logger.debug(\"iis write\")\n            # Read the WCS information from the client\n            # frames start from 1, we start from 0\n            self.frame = self.decode_frameno(pkt.z & 0o7777) - 1\n\n            try:\n                fb = self.server.controller.get_frame(self.frame)\n\n            except KeyError:\n                # the selected frame does not exist, create it\n                fb = self.server.controller.init_frame(self.frame)\n\n            # set the width and height of the framebuffer\n            fb_config = (pkt.t & 0o777) + 1\n            try:\n                (nframes, fb.width, fb.height) = fbconfigs[fb_config]\n\n            except KeyError:\n                self.logger.warning('Non existing framebuffer config (%s)' % (\n                    str(fb_config)))\n                self.logger.info('Adding a new framebuffer config (%s)' % (\n                    str(fb_config)))\n                fbconfigs[fb_config] = [1, None, None]\n                fb.width = None\n                fb.height = None\n\n            # do we have to deal with the new WCS format? (not used, for now)\n            #new_wcs = (pkt.x & 0o777)\n\n            # read the WCS info\n            line = pkt.datain.read(pkt.nbytes)\n\n            # paste it in the frame buffer\n            fb.wcs = line\n\n            fb.ct.format = W_DEFFORMAT\n            fb.ct.imtitle = ''\n            fb.ct.valid = 0\n            fb.ct = self.wcs_update(line, fb)", "response": "This function handles the WCS command."}
